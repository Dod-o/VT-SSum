{
    "id": "3e5nx7zphguzhkhmecc7kzxwywtu4klr",
    "title": "Classification with Asymmetric Label Noise: Consistency and Maximal Denoising",
    "info": {
        "author": [
            "Gilles Blanchard, Institut f\u00fcr Mathematik, University of Potsdam"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_blanchard_noise/",
    "segmentation": [
        [
            "Thank you so well Yeah's.",
            "Analysis is joint work with Kate and Scott angry handy from.",
            "Michigan, so I'll be talking about.",
            "Contamination models, which is called here as symmetric cable noise.",
            "So I think in the."
        ],
        [
            "This talk, so this is the overview.",
            "First, talk about the model.",
            "Then one contaminated glass case and then."
        ],
        [
            "Mutual contamination so OK, so I think we've seen this in the previous talk just just to fix things.",
            "So many in this talk I'll be using a generative point of view, which is instead of saying PXY is generated from generated from a common probability, I focus on the class conditional probability P. And assume they have two separate samples.",
            "Of size in zero and one from the from both classes.",
            "So in the classical case we're just observe the sample from the probability 1P1P0 and we want to.",
            "To minimize a certain performance measure, we can which can be values.",
            "One of those things like such like such as average classification error, min Max, her own name in person."
        ],
        [
            "And usually the way to do it is to look at empirical versions of of the errors and.",
            "Have a uniform control of this area of a class of fixed VC dimension and use this to do for example empirical risk minimization and eventually many structure or model selection.",
            "Using this classical tools from EC theory."
        ],
        [
            "So here.",
            "Will be considering model where we do not observe PO and P1 or samples from 51 P one but from a mixture of two mixtures of PO and P1.",
            "So we assume they are two underlying distribution P1P1.",
            "And.",
            "The.",
            "The contaminated distribution of physio children P1 children.",
            "They are weights, contamination, weights of Kappa zero in P0 and the first sample and capital in the second sample.",
            "So the only thing in principle Now we can access our empirical risk on these contaminated distributions.",
            "And the question is, what can we do so?"
        ],
        [
            "I know an equivalent way to see it is too.",
            "If we go back to the pxy model is to assume that there is a noise flipping a label, sleeping noise.",
            "Probability that the examples are drawn according to the true distribution and then the label is randomly flip, which is maybe something which is more classical in the running literature.",
            "To have this table flipping noise.",
            "So here I insist that this beeping noise is not symmetric between the two classes.",
            "And OK, there is an easy equivalence between the two of two formulations.",
            "Just a reprioritization, but the one with the text with the we take is maybe more suitable for this.",
            "What we'll be doing.",
            "So here, yeah, so maybe from this from this slide.",
            "I need to clarify something that basically.",
            "It's assumed so.",
            "Label noise in this sense is introduced to.",
            "It was introduced early in the learning literature to deal with noisy labels and underlying so true.",
            "Concept which is deterministic and then assume that we have learning problems with sleeping noise.",
            "So here you can you can you can ask.",
            "So what's the idea?",
            "First you have already a noisy problem with PO and P1 and then you add.",
            "Additionally add some flipping noise so it seems redundant.",
            "So how can we distinguish the two?",
            "So just to motivate where it's not just twice in the same thing?"
        ],
        [
            "So here's a practical motivation problem for this.",
            "For this problem where there's a particle detector and we want to determine neutrons from.",
            "From gamma Rays and basically you have sources and which you know is that, so you have signatures of each particle.",
            "So the pure party called there have the P1P1 have overlapping overlapping supports, which means it is already a noisy problem.",
            "But then Additionally when you observe sources you know they are not pure sources, so you have a source one which is mainly a neutron source and source 2 menu gamma Ray source, but there is a contamination proportion and that's what you want to determine."
        ],
        [
            "So of course it's a long standing standing standing topic in learning, so over various POV diesel in particular from the point of view of label flipping.",
            "And it has to do with the.",
            "OK, so there's a lot of literature, so when only one one sample is corrupted, you can draw a parallel to which is called and learning and positive and unlabeled data.",
            "In which case you have one sample which is pure and the other one which is a mixture of.",
            "The phrase distribution and P0.",
            "In the paper and Co training by Blue Mitchell they consider something which is, which is a comperable.",
            "As a part of the paper, which is a quality lopsided lopsided classification noise somehow?",
            "So there's also some recent work on.",
            "Slow PD labeled data.",
            "So there is a longstanding flow feature, unusually so, so you assume one of the following.",
            "Either the PO and P1R non overlapping support, which is to say that the underlying problem is is the deterministic, so that I mean target concept is deterministic.",
            "All the label noise is symmetric.",
            "Or maybe if it's not symmetric, then the noise proportions are known.",
            "In particular, for example, the paper of stempfle in high level.",
            "A high level, yeah.",
            "Ann, usually you focus features on a specific criterion for error.",
            "So here we don't want to make as little assumptions as possible, and what we saw in symmetric label noise, but it's not, it's not adversarial."
        ],
        [
            "So just to understand what happens if you shift the issue, if you shift the.",
            "If you have this contamination proportion, so optimal decisions in general are given as a likelihood ratios label given by the true or the true densities in this, in this case, and if you introduce label noise well, you will have the same error error C curve, because if you look at the two curves for pure problem, contaminated problems will correspond to the same same curve but not parameterized the same way.",
            "So if you look for an optimal decision.",
            "In some sense, and you train on using the without without care, using the same data using the same same principle, you will not necessarily converge with the optimal decision."
        ],
        [
            "So just three 3 examples.",
            "One is, we take misclassification probability.",
            "Then if you train just a regular classifier on the contaminated data, it will only converge.",
            "Be consistent to optimal decision for the pure, for the uncontaminated distribution.",
            "If the label noise is symmetric.",
            "In the case."
        ],
        [
            "Of the Max error it will.",
            "Again if you train on the contaminated data it will only converge to the true optimal decision consistently.",
            "If since I'm the alternative, all permutations of capezio and Capital One are equal."
        ],
        [
            "And strangely enough, well miss surprising at first if you take the balanced error, then in that case that's the only case where you can train on the contaminated data.",
            "So if you look at the.",
            "Balance error.",
            "It is something which is already actually notice to implicit in the paper of Bloom and Mitchell that then even if you have a symmetric noise because you have this nice equality between the contaminated and uncontaminated risk, it's just a linear.",
            "You know equality then.",
            "Then in that case you will be convergent.",
            "So except in."
        ],
        [
            "Some specific cases.",
            "So for example this one, but for the criteria you're not, you're not consistent if you just train on the.",
            "So if you train a regular classifier on the contaminated data."
        ],
        [
            "So basically what's important here is to determine the estimate capital and Capital One the contamination proportions.",
            "If you know, then you can estimate the risks.",
            "I just re scaling appropriately, but if you don't know capacity and Capital One you don't know how to estimate that, so this is what we want to focus in.",
            "And here's the basic, really basic.",
            "It's about consistent estimation of this."
        ],
        [
            "So the case of one point immunity distribution is based on earlier work.",
            "So the big question is identifiability.",
            "So of course if you don't know, observe just the contaminated distribution and P1."
        ],
        [
            "The Troop 0 so PO~ is a combination of some PO that you don't observe on P1, and of course it could be."
        ],
        [
            "It could be there, so there is an Internet dominasi so there is no way so that you can in general hope to be consistent.",
            "So but of course so this is the probability simplex right in three points here there is a space."
        ],
        [
            "Solution, which is the maximum one, the maximum one that you can.",
            "Pull away from pesero~ and this week and we call irreducible."
        ],
        [
            "So this is the case where you have only one class which is.",
            "So then contaminated and then you can define this quantity KK star which is the maximum proportion of of 1 distribution.",
            "One order to maximum Kappa such that F can be written as a mixture of Kappa times H plus something.",
            "And what's interesting is that.",
            "So here you can estimate the mixture of P1 and P0.",
            "So the maximum proportion of P1 and PO~ and the maximum proportion is your portion kept at zero exactly when PO is irreducible to respect to P1, which means that there is a.",
            "You cannot put any proportion of P1."
        ],
        [
            "And this you can estimate consistently because an alternative representation of pasta is that it's the essential is in film overall measurable sets of the ratio of F / H for two distributions.",
            "So if you can estimate F&H by empirically empirically, and if you replace the info by an info VC class for example and you will introduce some slack.",
            "Like variables, then.",
            "First of all, this estimator is by construction greater than the case star with high probability.",
            "And then if you do this over a set of VC classes of increasing complexity and can approximate any set, then you have a consistent estimator.",
            "So in this case, if you are irreducible class.",
            "Bution you can estimate the maximum proportions.",
            "And you can estimate the capacity so."
        ],
        [
            "Do we do when you have no mutual?"
        ],
        [
            "Nation, so we have two.",
            "Two to observe distribution which are two mixtures of.",
            "Of unknown P0 and P1."
        ],
        [
            "So this is the contamination model with the two crises.",
            "And this is coupled in the sense that you have two unknowns here and PO children P0 and P1~ So basic idea is to say, well, I don't like when it's coupled.",
            "I would prefer to use to use the case of 1 contaminated distribution to look at the maximum proportion of P1~ in PO~ and vice versa.",
            "So for this basically you can go to this alternative representation and represent PO~ is a mixture of P0 and P1~ contaminating distribution that you observe and vice versa.",
            "So this is nothing more than less than 4 million 2 by two matrix inversion, right?",
            "You express differently the variables.",
            "And here the only important point is to check that indeed this new new proportions are between zero and one.",
            "So this is indeed a mixture of presentation that it's not so, so it is still a mixture and this holds if the total noise or it should be kept as human Capital One here not cap on Cap 2 if the total noise is less than one, so the total level is this and one.",
            "Then you have these.",
            "Alternative representation is also is also mixture representation.",
            "So now we can see the road map.",
            "You can say, OK, I can estimate capezio children Kappa one~ by using the case of 1 continuity distribution.",
            "Because here for example for the first line I observe you wanted, I could look at the maximum proportion of P1 children P zero children's children and vice a versa for the other one.",
            "So."
        ],
        [
            "So this is the Cooper presentation, the original representation, and we see that here the advantages that yeah, it's the couple so that the box constraint on capacity and Capital One.",
            "Today it's just a box constraints so they can vary between zero and the maximum proportion, and in the original representation we have like a modified possible domain of solution capezio and Capital One and."
        ],
        [
            "Came back to this.",
            "So the next point is identifiability or irreducibility, so we needed irreducibility in the contaminated case case case, so here.",
            "Basically we need that P1~ PO~ is reducing respect to P1.",
            "Sorry, P0 is either disabled respect to P1~ and vice a versa.",
            "And it's not very difficult to see that it's equivalent.",
            "We say that actually P0 and P1, so here's a receiver with respect to each other.",
            "So under this assumption that PNP one are really simple to respect to each other, you can estimate the.",
            "You can."
        ],
        [
            "The proportions.",
            "So this."
        ],
        [
            "What it looks like you basically pull apart as much as you can.",
            "P0 from P1.",
            "To solve the solved."
        ],
        [
            "Questions.",
            "So basically these are OK, so these are examples of what it looks like to be."
        ],
        [
            "Visible, so I skip this receivable means that if you look at the probability of being one of the conditional probability of Y in one given X, it will attain the bounds one and zero over the whole space.",
            "This means that you cannot.",
            "You cannot write each for each conditional probability as a mixture of the other."
        ],
        [
            "And also the alternative representation allows us to understand what's what's.",
            "What are the properties of these irreducible solution?",
            "So basically we have seen through this transformation that we have a correspondence between the original solution Capezio Capital One of these mixture equations.",
            "Under the alternative representation, the D Cooper representation.",
            "And this tells us that this irreducible solution, which is the one you obtain when you assume your disability, has very special properties so."
        ],
        [
            "An extremal point of these feasible set of old proportion capezio and Capital One compatible with the with the model.",
            "It's the one, so it's an extremal point, an it's it's one which corresponds to the largest.",
            "Level of total noise."
        ],
        [
            "Compatible with this model and so you can also interpret this model as a.",
            "Kind of maximal denoising.",
            "So it looked for this solution.",
            "You look at the solution underlying POP one such that the noise, so I don't know it is maximal or such that the sources PO and P1 are separated as possible.",
            "As you see here, and which is still compatible with your model."
        ],
        [
            "So this this solution is something that has that is interesting.",
            "And also I think relevant for many."
        ],
        [
            "Practical impractical case.",
            "You can assume that irreducible is a random number assumption.",
            "OK, so once you've done that, basically we can use the one announced.",
            "The one class contaminated case to basically apply first the estimator of Kappa Zero Tilda and then of Kappa, one~ If you want, you can then estimate the original proportions cap at zero and Capital One by just inverting the relation.",
            "And these are universally consistent estimators."
        ],
        [
            "And then OK based on that because you have.",
            "Because then you can.",
            "You have an estimate of this proportion, which is what we needed to to estimate the risk.",
            "Then you can estimate the risk is announced initially and actually OK.",
            "So one nice.",
            "Observation is that you don't.",
            "Even if you want to estimate the risk, you don't even need to go back to the original proportion capital and Capital One.",
            "You can achieve so simple simple reworking directly.",
            "Use you estimated proportions in this day Cooper representation.",
            "So the DQ presentation is actually more practical.",
            "So and once you have a again uniform estimation, consistent estimation of the risk over service class, then you can for the risk.",
            "So that's so risk error of class one, an error of class of your classified under distribution, the true distribution 0 and under the two distribution one.",
            "Then you can also find using usual arguments the optimal decision.",
            "Or you can be a consistent towards the optimal decision.",
            "For.",
            "For values, error criteria.",
            "So Min Max error probability.",
            "Basically you can deal with the.",
            "With many different error code, I'll."
        ],
        [
            "So to conclude.",
            "So here we have basically tried to clarify it.",
            "What it means in this contamination model to solve and which condition you can.",
            "You can have identified video.",
            "The model, so under the special Fishel assumption of irreducibility, which is the only.",
            "Only assumption on the source distribution that we make.",
            "Otherwise it's completely agnostic.",
            "You can indeed estimate consistently disproportions.",
            "And OK so further works are apart from race, so the multiclass case is more complicated because you have several challenges that arise if you have more than two distribute source distributions, and so how do you generalize the?",
            "The irreducibility and the assumption of recovering the spatial solutions so it's actually the geometry is little bit more complex."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so well Yeah's.",
                    "label": 0
                },
                {
                    "sent": "Analysis is joint work with Kate and Scott angry handy from.",
                    "label": 0
                },
                {
                    "sent": "Michigan, so I'll be talking about.",
                    "label": 0
                },
                {
                    "sent": "Contamination models, which is called here as symmetric cable noise.",
                    "label": 0
                },
                {
                    "sent": "So I think in the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk, so this is the overview.",
                    "label": 0
                },
                {
                    "sent": "First, talk about the model.",
                    "label": 0
                },
                {
                    "sent": "Then one contaminated glass case and then.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mutual contamination so OK, so I think we've seen this in the previous talk just just to fix things.",
                    "label": 0
                },
                {
                    "sent": "So many in this talk I'll be using a generative point of view, which is instead of saying PXY is generated from generated from a common probability, I focus on the class conditional probability P. And assume they have two separate samples.",
                    "label": 0
                },
                {
                    "sent": "Of size in zero and one from the from both classes.",
                    "label": 0
                },
                {
                    "sent": "So in the classical case we're just observe the sample from the probability 1P1P0 and we want to.",
                    "label": 0
                },
                {
                    "sent": "To minimize a certain performance measure, we can which can be values.",
                    "label": 0
                },
                {
                    "sent": "One of those things like such like such as average classification error, min Max, her own name in person.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And usually the way to do it is to look at empirical versions of of the errors and.",
                    "label": 0
                },
                {
                    "sent": "Have a uniform control of this area of a class of fixed VC dimension and use this to do for example empirical risk minimization and eventually many structure or model selection.",
                    "label": 0
                },
                {
                    "sent": "Using this classical tools from EC theory.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Will be considering model where we do not observe PO and P1 or samples from 51 P one but from a mixture of two mixtures of PO and P1.",
                    "label": 0
                },
                {
                    "sent": "So we assume they are two underlying distribution P1P1.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The contaminated distribution of physio children P1 children.",
                    "label": 0
                },
                {
                    "sent": "They are weights, contamination, weights of Kappa zero in P0 and the first sample and capital in the second sample.",
                    "label": 0
                },
                {
                    "sent": "So the only thing in principle Now we can access our empirical risk on these contaminated distributions.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what can we do so?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I know an equivalent way to see it is too.",
                    "label": 0
                },
                {
                    "sent": "If we go back to the pxy model is to assume that there is a noise flipping a label, sleeping noise.",
                    "label": 0
                },
                {
                    "sent": "Probability that the examples are drawn according to the true distribution and then the label is randomly flip, which is maybe something which is more classical in the running literature.",
                    "label": 0
                },
                {
                    "sent": "To have this table flipping noise.",
                    "label": 0
                },
                {
                    "sent": "So here I insist that this beeping noise is not symmetric between the two classes.",
                    "label": 0
                },
                {
                    "sent": "And OK, there is an easy equivalence between the two of two formulations.",
                    "label": 0
                },
                {
                    "sent": "Just a reprioritization, but the one with the text with the we take is maybe more suitable for this.",
                    "label": 0
                },
                {
                    "sent": "What we'll be doing.",
                    "label": 0
                },
                {
                    "sent": "So here, yeah, so maybe from this from this slide.",
                    "label": 0
                },
                {
                    "sent": "I need to clarify something that basically.",
                    "label": 0
                },
                {
                    "sent": "It's assumed so.",
                    "label": 0
                },
                {
                    "sent": "Label noise in this sense is introduced to.",
                    "label": 0
                },
                {
                    "sent": "It was introduced early in the learning literature to deal with noisy labels and underlying so true.",
                    "label": 0
                },
                {
                    "sent": "Concept which is deterministic and then assume that we have learning problems with sleeping noise.",
                    "label": 0
                },
                {
                    "sent": "So here you can you can you can ask.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "First you have already a noisy problem with PO and P1 and then you add.",
                    "label": 0
                },
                {
                    "sent": "Additionally add some flipping noise so it seems redundant.",
                    "label": 0
                },
                {
                    "sent": "So how can we distinguish the two?",
                    "label": 0
                },
                {
                    "sent": "So just to motivate where it's not just twice in the same thing?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a practical motivation problem for this.",
                    "label": 0
                },
                {
                    "sent": "For this problem where there's a particle detector and we want to determine neutrons from.",
                    "label": 0
                },
                {
                    "sent": "From gamma Rays and basically you have sources and which you know is that, so you have signatures of each particle.",
                    "label": 0
                },
                {
                    "sent": "So the pure party called there have the P1P1 have overlapping overlapping supports, which means it is already a noisy problem.",
                    "label": 0
                },
                {
                    "sent": "But then Additionally when you observe sources you know they are not pure sources, so you have a source one which is mainly a neutron source and source 2 menu gamma Ray source, but there is a contamination proportion and that's what you want to determine.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So of course it's a long standing standing standing topic in learning, so over various POV diesel in particular from the point of view of label flipping.",
                    "label": 0
                },
                {
                    "sent": "And it has to do with the.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot of literature, so when only one one sample is corrupted, you can draw a parallel to which is called and learning and positive and unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "In which case you have one sample which is pure and the other one which is a mixture of.",
                    "label": 0
                },
                {
                    "sent": "The phrase distribution and P0.",
                    "label": 0
                },
                {
                    "sent": "In the paper and Co training by Blue Mitchell they consider something which is, which is a comperable.",
                    "label": 0
                },
                {
                    "sent": "As a part of the paper, which is a quality lopsided lopsided classification noise somehow?",
                    "label": 1
                },
                {
                    "sent": "So there's also some recent work on.",
                    "label": 0
                },
                {
                    "sent": "Slow PD labeled data.",
                    "label": 1
                },
                {
                    "sent": "So there is a longstanding flow feature, unusually so, so you assume one of the following.",
                    "label": 1
                },
                {
                    "sent": "Either the PO and P1R non overlapping support, which is to say that the underlying problem is is the deterministic, so that I mean target concept is deterministic.",
                    "label": 0
                },
                {
                    "sent": "All the label noise is symmetric.",
                    "label": 0
                },
                {
                    "sent": "Or maybe if it's not symmetric, then the noise proportions are known.",
                    "label": 0
                },
                {
                    "sent": "In particular, for example, the paper of stempfle in high level.",
                    "label": 0
                },
                {
                    "sent": "A high level, yeah.",
                    "label": 1
                },
                {
                    "sent": "Ann, usually you focus features on a specific criterion for error.",
                    "label": 0
                },
                {
                    "sent": "So here we don't want to make as little assumptions as possible, and what we saw in symmetric label noise, but it's not, it's not adversarial.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to understand what happens if you shift the issue, if you shift the.",
                    "label": 0
                },
                {
                    "sent": "If you have this contamination proportion, so optimal decisions in general are given as a likelihood ratios label given by the true or the true densities in this, in this case, and if you introduce label noise well, you will have the same error error C curve, because if you look at the two curves for pure problem, contaminated problems will correspond to the same same curve but not parameterized the same way.",
                    "label": 0
                },
                {
                    "sent": "So if you look for an optimal decision.",
                    "label": 0
                },
                {
                    "sent": "In some sense, and you train on using the without without care, using the same data using the same same principle, you will not necessarily converge with the optimal decision.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just three 3 examples.",
                    "label": 0
                },
                {
                    "sent": "One is, we take misclassification probability.",
                    "label": 1
                },
                {
                    "sent": "Then if you train just a regular classifier on the contaminated data, it will only converge.",
                    "label": 1
                },
                {
                    "sent": "Be consistent to optimal decision for the pure, for the uncontaminated distribution.",
                    "label": 1
                },
                {
                    "sent": "If the label noise is symmetric.",
                    "label": 0
                },
                {
                    "sent": "In the case.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the Max error it will.",
                    "label": 0
                },
                {
                    "sent": "Again if you train on the contaminated data it will only converge to the true optimal decision consistently.",
                    "label": 0
                },
                {
                    "sent": "If since I'm the alternative, all permutations of capezio and Capital One are equal.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And strangely enough, well miss surprising at first if you take the balanced error, then in that case that's the only case where you can train on the contaminated data.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Balance error.",
                    "label": 1
                },
                {
                    "sent": "It is something which is already actually notice to implicit in the paper of Bloom and Mitchell that then even if you have a symmetric noise because you have this nice equality between the contaminated and uncontaminated risk, it's just a linear.",
                    "label": 0
                },
                {
                    "sent": "You know equality then.",
                    "label": 0
                },
                {
                    "sent": "Then in that case you will be convergent.",
                    "label": 0
                },
                {
                    "sent": "So except in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some specific cases.",
                    "label": 0
                },
                {
                    "sent": "So for example this one, but for the criteria you're not, you're not consistent if you just train on the.",
                    "label": 1
                },
                {
                    "sent": "So if you train a regular classifier on the contaminated data.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what's important here is to determine the estimate capital and Capital One the contamination proportions.",
                    "label": 0
                },
                {
                    "sent": "If you know, then you can estimate the risks.",
                    "label": 0
                },
                {
                    "sent": "I just re scaling appropriately, but if you don't know capacity and Capital One you don't know how to estimate that, so this is what we want to focus in.",
                    "label": 0
                },
                {
                    "sent": "And here's the basic, really basic.",
                    "label": 0
                },
                {
                    "sent": "It's about consistent estimation of this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the case of one point immunity distribution is based on earlier work.",
                    "label": 0
                },
                {
                    "sent": "So the big question is identifiability.",
                    "label": 0
                },
                {
                    "sent": "So of course if you don't know, observe just the contaminated distribution and P1.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Troop 0 so PO~ is a combination of some PO that you don't observe on P1, and of course it could be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It could be there, so there is an Internet dominasi so there is no way so that you can in general hope to be consistent.",
                    "label": 0
                },
                {
                    "sent": "So but of course so this is the probability simplex right in three points here there is a space.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution, which is the maximum one, the maximum one that you can.",
                    "label": 0
                },
                {
                    "sent": "Pull away from pesero~ and this week and we call irreducible.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the case where you have only one class which is.",
                    "label": 0
                },
                {
                    "sent": "So then contaminated and then you can define this quantity KK star which is the maximum proportion of of 1 distribution.",
                    "label": 0
                },
                {
                    "sent": "One order to maximum Kappa such that F can be written as a mixture of Kappa times H plus something.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting is that.",
                    "label": 0
                },
                {
                    "sent": "So here you can estimate the mixture of P1 and P0.",
                    "label": 0
                },
                {
                    "sent": "So the maximum proportion of P1 and PO~ and the maximum proportion is your portion kept at zero exactly when PO is irreducible to respect to P1, which means that there is a.",
                    "label": 1
                },
                {
                    "sent": "You cannot put any proportion of P1.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this you can estimate consistently because an alternative representation of pasta is that it's the essential is in film overall measurable sets of the ratio of F / H for two distributions.",
                    "label": 1
                },
                {
                    "sent": "So if you can estimate F&H by empirically empirically, and if you replace the info by an info VC class for example and you will introduce some slack.",
                    "label": 0
                },
                {
                    "sent": "Like variables, then.",
                    "label": 0
                },
                {
                    "sent": "First of all, this estimator is by construction greater than the case star with high probability.",
                    "label": 1
                },
                {
                    "sent": "And then if you do this over a set of VC classes of increasing complexity and can approximate any set, then you have a consistent estimator.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you are irreducible class.",
                    "label": 0
                },
                {
                    "sent": "Bution you can estimate the maximum proportions.",
                    "label": 0
                },
                {
                    "sent": "And you can estimate the capacity so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we do when you have no mutual?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation, so we have two.",
                    "label": 0
                },
                {
                    "sent": "Two to observe distribution which are two mixtures of.",
                    "label": 0
                },
                {
                    "sent": "Of unknown P0 and P1.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the contamination model with the two crises.",
                    "label": 0
                },
                {
                    "sent": "And this is coupled in the sense that you have two unknowns here and PO children P0 and P1~ So basic idea is to say, well, I don't like when it's coupled.",
                    "label": 0
                },
                {
                    "sent": "I would prefer to use to use the case of 1 contaminated distribution to look at the maximum proportion of P1~ in PO~ and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So for this basically you can go to this alternative representation and represent PO~ is a mixture of P0 and P1~ contaminating distribution that you observe and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So this is nothing more than less than 4 million 2 by two matrix inversion, right?",
                    "label": 0
                },
                {
                    "sent": "You express differently the variables.",
                    "label": 0
                },
                {
                    "sent": "And here the only important point is to check that indeed this new new proportions are between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So this is indeed a mixture of presentation that it's not so, so it is still a mixture and this holds if the total noise or it should be kept as human Capital One here not cap on Cap 2 if the total noise is less than one, so the total level is this and one.",
                    "label": 0
                },
                {
                    "sent": "Then you have these.",
                    "label": 0
                },
                {
                    "sent": "Alternative representation is also is also mixture representation.",
                    "label": 0
                },
                {
                    "sent": "So now we can see the road map.",
                    "label": 0
                },
                {
                    "sent": "You can say, OK, I can estimate capezio children Kappa one~ by using the case of 1 continuity distribution.",
                    "label": 0
                },
                {
                    "sent": "Because here for example for the first line I observe you wanted, I could look at the maximum proportion of P1 children P zero children's children and vice a versa for the other one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the Cooper presentation, the original representation, and we see that here the advantages that yeah, it's the couple so that the box constraint on capacity and Capital One.",
                    "label": 0
                },
                {
                    "sent": "Today it's just a box constraints so they can vary between zero and the maximum proportion, and in the original representation we have like a modified possible domain of solution capezio and Capital One and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Came back to this.",
                    "label": 0
                },
                {
                    "sent": "So the next point is identifiability or irreducibility, so we needed irreducibility in the contaminated case case case, so here.",
                    "label": 0
                },
                {
                    "sent": "Basically we need that P1~ PO~ is reducing respect to P1.",
                    "label": 0
                },
                {
                    "sent": "Sorry, P0 is either disabled respect to P1~ and vice a versa.",
                    "label": 0
                },
                {
                    "sent": "And it's not very difficult to see that it's equivalent.",
                    "label": 0
                },
                {
                    "sent": "We say that actually P0 and P1, so here's a receiver with respect to each other.",
                    "label": 1
                },
                {
                    "sent": "So under this assumption that PNP one are really simple to respect to each other, you can estimate the.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proportions.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What it looks like you basically pull apart as much as you can.",
                    "label": 0
                },
                {
                    "sent": "P0 from P1.",
                    "label": 0
                },
                {
                    "sent": "To solve the solved.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So basically these are OK, so these are examples of what it looks like to be.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visible, so I skip this receivable means that if you look at the probability of being one of the conditional probability of Y in one given X, it will attain the bounds one and zero over the whole space.",
                    "label": 0
                },
                {
                    "sent": "This means that you cannot.",
                    "label": 0
                },
                {
                    "sent": "You cannot write each for each conditional probability as a mixture of the other.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also the alternative representation allows us to understand what's what's.",
                    "label": 0
                },
                {
                    "sent": "What are the properties of these irreducible solution?",
                    "label": 0
                },
                {
                    "sent": "So basically we have seen through this transformation that we have a correspondence between the original solution Capezio Capital One of these mixture equations.",
                    "label": 0
                },
                {
                    "sent": "Under the alternative representation, the D Cooper representation.",
                    "label": 0
                },
                {
                    "sent": "And this tells us that this irreducible solution, which is the one you obtain when you assume your disability, has very special properties so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An extremal point of these feasible set of old proportion capezio and Capital One compatible with the with the model.",
                    "label": 0
                },
                {
                    "sent": "It's the one, so it's an extremal point, an it's it's one which corresponds to the largest.",
                    "label": 0
                },
                {
                    "sent": "Level of total noise.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compatible with this model and so you can also interpret this model as a.",
                    "label": 0
                },
                {
                    "sent": "Kind of maximal denoising.",
                    "label": 0
                },
                {
                    "sent": "So it looked for this solution.",
                    "label": 0
                },
                {
                    "sent": "You look at the solution underlying POP one such that the noise, so I don't know it is maximal or such that the sources PO and P1 are separated as possible.",
                    "label": 0
                },
                {
                    "sent": "As you see here, and which is still compatible with your model.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this solution is something that has that is interesting.",
                    "label": 0
                },
                {
                    "sent": "And also I think relevant for many.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Practical impractical case.",
                    "label": 0
                },
                {
                    "sent": "You can assume that irreducible is a random number assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you've done that, basically we can use the one announced.",
                    "label": 0
                },
                {
                    "sent": "The one class contaminated case to basically apply first the estimator of Kappa Zero Tilda and then of Kappa, one~ If you want, you can then estimate the original proportions cap at zero and Capital One by just inverting the relation.",
                    "label": 0
                },
                {
                    "sent": "And these are universally consistent estimators.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then OK based on that because you have.",
                    "label": 0
                },
                {
                    "sent": "Because then you can.",
                    "label": 0
                },
                {
                    "sent": "You have an estimate of this proportion, which is what we needed to to estimate the risk.",
                    "label": 0
                },
                {
                    "sent": "Then you can estimate the risk is announced initially and actually OK.",
                    "label": 0
                },
                {
                    "sent": "So one nice.",
                    "label": 0
                },
                {
                    "sent": "Observation is that you don't.",
                    "label": 0
                },
                {
                    "sent": "Even if you want to estimate the risk, you don't even need to go back to the original proportion capital and Capital One.",
                    "label": 0
                },
                {
                    "sent": "You can achieve so simple simple reworking directly.",
                    "label": 0
                },
                {
                    "sent": "Use you estimated proportions in this day Cooper representation.",
                    "label": 0
                },
                {
                    "sent": "So the DQ presentation is actually more practical.",
                    "label": 0
                },
                {
                    "sent": "So and once you have a again uniform estimation, consistent estimation of the risk over service class, then you can for the risk.",
                    "label": 1
                },
                {
                    "sent": "So that's so risk error of class one, an error of class of your classified under distribution, the true distribution 0 and under the two distribution one.",
                    "label": 0
                },
                {
                    "sent": "Then you can also find using usual arguments the optimal decision.",
                    "label": 0
                },
                {
                    "sent": "Or you can be a consistent towards the optimal decision.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For values, error criteria.",
                    "label": 0
                },
                {
                    "sent": "So Min Max error probability.",
                    "label": 0
                },
                {
                    "sent": "Basically you can deal with the.",
                    "label": 0
                },
                {
                    "sent": "With many different error code, I'll.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "So here we have basically tried to clarify it.",
                    "label": 0
                },
                {
                    "sent": "What it means in this contamination model to solve and which condition you can.",
                    "label": 0
                },
                {
                    "sent": "You can have identified video.",
                    "label": 0
                },
                {
                    "sent": "The model, so under the special Fishel assumption of irreducibility, which is the only.",
                    "label": 0
                },
                {
                    "sent": "Only assumption on the source distribution that we make.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's completely agnostic.",
                    "label": 0
                },
                {
                    "sent": "You can indeed estimate consistently disproportions.",
                    "label": 0
                },
                {
                    "sent": "And OK so further works are apart from race, so the multiclass case is more complicated because you have several challenges that arise if you have more than two distribute source distributions, and so how do you generalize the?",
                    "label": 0
                },
                {
                    "sent": "The irreducibility and the assumption of recovering the spatial solutions so it's actually the geometry is little bit more complex.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}