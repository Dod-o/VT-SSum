{
    "id": "o4533zc2rc2tim3t3mwnhbkqvcn5nudy",
    "title": "Identifying graph-structured activation patterns in networks",
    "info": {
        "author": [
            "James Sharpnack, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/nips2010_sharpnack_igs/",
    "segmentation": [
        [
            "Alright, so this is work that I've been doing with my advisor, Aarti Singh."
        ],
        [
            "So networks occur in the real world all the time, and the task that we're going to be considering is estimating activation patterns in this network.",
            "And so one example of this is we have routers in the Internet.",
            "Connections mean that they they can pass messages and the activation pattern that we're interested in for this is localising congestion in this network so that we can alleviate this congestion congestion in some way.",
            "And experimentally we can get at this by computing roundtrip times, things like that.",
            "Another example is we have sensors in a body of water and the activation pattern here.",
            "That we would like to localize in some senses contamination.",
            "So we'd like to detect where is the oil.",
            "And in this body of water."
        ],
        [
            "So.",
            "But now without structure, the thing that we're considering would be the normal means problem.",
            "This is a statistical classical statistical problem.",
            "So first we believe that there's some underlying vector P dimensional an in our situation, we're generally going to be thinking that these coordinates are either zero or one.",
            "For the most part here, Red represents one and blue represents 0.",
            "And of course we are not going to observe this signal."
        ],
        [
            "We're going to observe some noisy version of it.",
            "And the noise that we're considering is additive Gaussian I. Identically distributed and independent, Zero Sigma squared, where Sigma squared variance.",
            "Now the task is to reconstruct X from Y.",
            "These noisy versions in this normal means problem, and generally we're going to be suffering from high dimensionality if P is large."
        ],
        [
            "Now, what happens when we incorporate structure?",
            "So a priority this vector?",
            "The ordering of the components has no meaning, right?",
            "In the in the normal means problem."
        ],
        [
            "Now we suppose that we have some underlying network where the components of X are indexed by vertices of a graph.",
            "Edges between these vertices specify some dependence.",
            "A priore between the variables in these components of the vector X."
        ],
        [
            "Now, how does this help?",
            "How does it help to know that there's some underlying structure?",
            "So if we were to use the noisy observations at face value and try to estimate which are active, which are red and which are not blue.",
            "You can see these highlighted nodes.",
            "They're kind of purplish right, and so we might assume incorrectly that they are blue and in fact, but so if we were to incorporate this structure uses structure wisely, we would note that they are adjacent to a red node as something that's obviously red, and then we might infer, correctly that these two nodes are in fact read, so this is how using the dependencies given by this graph we can reconstruct X from YA little more efficiently.",
            "Alright."
        ],
        [
            "So I'm going to be very specific about the generative statistical model that I'm going to be using here.",
            "So first nature will draw a graph.",
            "This is what we assume we believe to be true with P nodes."
        ],
        [
            "Then nature draws a signal, an activation pattern X.",
            "Where the density is proportionate to E to the negative X transpose LX, where L is the unnormalized graph Laplacian.",
            "So if the dominating measure here is labagh and L is the precision matrix, then we get back our usual Gaussian graphical models.",
            "This isn't as interesting to us what we're going to be considering when, when the dominating measure is the counting measure.",
            "We get back binary signals, and this is the Ising model, which is a well studied thing in physics, actually.",
            "So a little note about the graph Laplacian is defined as the diagonal degree vector minus the weight matrix for the graph, and then X transpose.",
            "Alex this is a good thing to be looking at because what it is is this weighted sum of squared differences between adjacent components of our vector, right?",
            "So it penalizes against these differences between adjacent nodes."
        ],
        [
            "And then of course we draw that same Gaussian IID Zero Sigma squared noise and this is what we get to observe."
        ],
        [
            "Alright, so why isn't this exactly a solved problem?",
            "So first of all, since we have a generative model, we think that we should be using Bayes optimal rules for Mean Square and Hamming distance.",
            "The Bayes optimal rules of the posterior mean in the posterior centroid respectively.",
            "These are one hard to implement and two very hard to analyze.",
            "And so.",
            "Now, if we had 01 loss, then the Bayes optimal rule is the map estimate and this is efficiently implemented via graph min cut.",
            "We know this the issue is that the 01 loss isn't terribly appropriate.",
            "I think in this case because there's no notion of closeness between our estimator and the truth.",
            "Either you're correct or you're not correct in the 01 loss, so we're considering the Hamming distance in the mean square error."
        ],
        [
            "There there is work that uses the Unnormalized Laplacian and of course the normalized Laplacian.",
            "But most of these are justified in the embedded setting and there's this.",
            "There's this sort of focused importance on the second eigenvalue of Laplacian.",
            "But this is.",
            "These aren't always the case, but."
        ],
        [
            "So the estimated that we're proposing is very simple.",
            "So first you take your unnormalized Laplacian, your graph Laplacian and then you compute its eigen system.",
            "Right eigenvalue eigenvector pairs, land is use.",
            "Then you have them increasing from zero.",
            "So the estimator that we use we take our our noisy observations and we project them onto the span of the first K eigenvectors, right?",
            "There's two reasons why we like this one.",
            "It's easy to analyze the astronautic risk, and two, it's easy to implement if you have an SVD, so."
        ],
        [
            "Over it's just three lines of code."
        ],
        [
            "Alright, So what is this giving us for graphs that we're accustomed to write for the hierarchical graph?",
            "It's just specified by this tree structure where we observe the leaf nodes.",
            "The graph Laplacian is the unnormalized Laplacian.",
            "Has this block matrix form?",
            "Now what ends up happening is that the Laplacian eigenvectors give us these Haar wavelets where this bottom row is the constant and then we have increasing resolution wavelets as we go up.",
            "And this is a localized basis.",
            "Now if we if we were to look at the lattice graph on a tourist in dimensions first of all, the lattice Laplacian has a circulant form and the Laplacian eigen basis gives us back the Fourier basis where this is constant and these are increasing frequency sinusoids as we go up."
        ],
        [
            "Alright, so let's look at a couple of these simulations that we were looking at before, so we take this network activation pattern which is actually drawn from an Ising model here."
        ],
        [
            "And then we add noise.",
            "Sigma squared is 1/2 in this case."
        ],
        [
            "And then we use the Eigen Maps estimator where our eigen dimensions is 3 projection onto a 3 dimensional space.",
            "We have near perfect recovery in this example."
        ],
        [
            "Now let's look at a larger real world type graph.",
            "With 100"
        ],
        [
            "Nodes and then we add even more noise.",
            "Sigma squared is 4/5."
        ],
        [
            "Then the Eigen Maps estimator again with this is with ten.",
            "I can dimension does a pretty good job of recovering the signal and."
        ],
        [
            "Furthermore, if we were to threshold this estimator.",
            "And then we get in your perfect recovery where if we had thresholded the observations we would get pretty poor recovery.",
            "So this gives us this intuition that we can do even better for larger graphs because there's more connectivity in larger graphs alot of times."
        ],
        [
            "Alright, so.",
            "Alright, so the big picture that we're trying to say is where do we get consistent estimation?",
            "This is the sort of asymptotically statistical statements that we want to be making right?",
            "An algorithm is not good enough for us.",
            "Alright, so if we define the Bayes, the Bayes risk to be the expected average mean square error loss, then we say that consistent estimation is when this is driven to 0.",
            "And in the unstructured case, what we need to happen?",
            "What needs to happen is that the Sigma squared.",
            "The noise variance has to decrease to get consistent estimation as the network size grows."
        ],
        [
            "Now what we find for a lot of graph models is that for the structure for these structural cases, we can tolerate increasing noise as the graph grows, and I'd also argue that some of these models are very natural, if not pedagogical.",
            "Specifically, if Sigma squared is like little low P to the gamma, we find that below that is consistency regime.",
            "So now a lot of what we want to be doing is figuring out what gamma is for these networks."
        ],
        [
            "So the main machinery that we used to get at this is this bound on the Bayes risk."
        ],
        [
            "It's decomposed into 3."
        ],
        [
            "Terms the first term is a concentration bound."
        ],
        [
            "The next is is like a bias term which is inversely proportionate to the Lambda K plus one.",
            "The K plus first eigenvalue."
        ],
        [
            "And this variance term, which is linear in K. If."
        ],
        [
            "You note that Lambda K plus one is like a quantile of the eigenvalue distribution.",
            "If we select eigenvalues uniformly at random from its set and K / P is which quantile this corresponds to, then we end up getting a trade off a bias variance tradeoff for choosing K."
        ],
        [
            "Alright, so.",
            "So I'll give you some intuition first about why this is true.",
            "Note that our estimator is the projection of X plus the projection of the noise.",
            "And First off, we can show we show using a turn off type bound that X with high probability is within this ellipsoid which is given by X transpose.",
            "LX is bounded by 2P or Gamma P."
        ],
        [
            "Furthermore, the axes of this ellipse are given.",
            "The major axis is given by the 1st K eigenvectors and the minor axes are given by the remainder of the eigenvectors.",
            "And then the one we Project X on to this first case, the span of the first K eigenvectors.",
            "The projection loss is at most the radius of this minor axis, which which ends up being like the reciprocal Lambda K plus."
        ],
        [
            "1.",
            "Um?",
            "Just because this is a projection onto a K dimensional space, it naturally will reduce the isotropic noise by a factor of therapy.",
            "So I'd like to argue that this is very simple."
        ],
        [
            "Alright.",
            "So the big picture is now that we have this machinery in place, this this bound on the Bayes risk.",
            "We can show that for some simple graph models we get this consistency regime."
        ],
        [
            "Alright, so the first model that we're going to be looking at is this hierarchical structure where we have these latent nodes and then we observe the leaf nodes in this tree.",
            "What we find is that if we plot the eigenvalue histogram for for this graph for large P and large large depth true depth.",
            "We find that there's some possibly 0 eigenvalues with some multiplicity, and then as the eigen values increase we get more and more multiplicity.",
            "This structure has these parameters beta, which is the interaction strength and and also this distance between leaves where there's still a non zero interaction given by two L star.",
            "And so L star is kind of a measure of the connectivity as it increases the connectivity of the graph increases, right?",
            "Um?"
        ],
        [
            "Alright, so so we can.",
            "We can get A at our bias variance tradeoff.",
            "For this example using this eigenvalue histogram are using using the sequence of eigenvalues.",
            "We see that the bias term, which is the reciprocal of Lambda K plus one, is decreasing sharply while the variance is increasing linearly in K. And so we get the familiar U shaped risk curve."
        ],
        [
            "Now we can further get at what this consistency regime should be for our estimator by having by showing that well, we do show where we find that.",
            "And the number of Eigen values that are small is bounded above by this term here.",
            "So what we're able to do is set K the eigen dimension to be this value here and then we define L star to be proportionate to this gamma.",
            "What ends up happening is that if Sigma squared is litleo pizza the gamma, then we get this Bayes risk consistency.",
            "So we can see how kind of concentration results, concentration type results for the eigenvalue histogram.",
            "The density gives us this consistency region an what gamma should be."
        ],
        [
            "So let's do this for the lattice graph in D dimensions, where along each dimension there's N vertices.",
            "Using Hoeffding's bound we prove that the eigen values tend to concentrate above the dimensions the lattice dimensions D exponentially."
        ],
        [
            "And again we get this bias variance tradeoff with RU shaped risk curve and we."
        ],
        [
            "See as the lattice."
        ],
        [
            "Dimensions increases we get faster or lower and lower bounds for this bazra."
        ],
        [
            "Again, using that same concentration bound for the Lambda eyes were able to specify the lattice dimension to be gamma or proportionate to gamma.",
            "We set K to be this right hand side here for this concentration bound.",
            "And then again we see that Sigma squared, if it's little low piece of the gamma then we get risk consistency.",
            "As as the number of nodes increases in the."
        ],
        [
            "We do the same.",
            "A similar thing for the orders running graph.",
            "What the years radiograph is is you take 2 nodes.",
            "And you connect them, you connect them with an edge with probability P to the gamma minus one.",
            "We again get a similar concentration bound and we're able to show the same sort of risk consistency region."
        ],
        [
            "So in total what we see is that for the tree graph, the interaction distance can be proportionate to gamma.",
            "The dimensions can be proportionate to gamma for the lattice and for the orders.",
            "For any graph, the edge probability is proportionate to gamma.",
            "So we see that to get risk consistency in this regime where you have below P to the gamma.",
            "We need that gamma is some sort of a measure of the connectivity of the graph, and this satisfies our intuition of a graph.",
            "Is Morris connected?",
            "Then it's going to be more useful in structured estimate."
        ],
        [
            "And.",
            "So, so we have some simulations for the tree graph.",
            "The Eigen Maps estimator does nearly as well as the Bayes rule, but all outperform significantly the unstructured case."
        ],
        [
            "For the lattice graph, as the dimensions D increases, we continually get the Eigen Maps estimator.",
            "Does better in terms of MSE than the naive."
        ],
        [
            "Motor assembly for the order.",
            "Any graph as the network size increases."
        ],
        [
            "And for we also did this on the small world graph.",
            "What strogatz model?",
            "Can we see the same?",
            "The same, much better performance of the Eigen map system?"
        ],
        [
            "So, in summary, nature draws in X from an Ising model and then we get observations which are these noisy versions of it using our Eigen Maps estimator were able to show that for these simple graph models that we get consistency in our regime where the noise variance can increase in the number of nodes.",
            "So thank you.",
            "So one question I had was.",
            "So these quest these this estimator works if there's a decay in the eigenvalues etc.",
            "So would this correspond to?",
            "An increase in the eigenvalues because it's the reciprocal of them.",
            "OK, so in such settings with the map estimator, the typical typically intractable map estimators would they also perform well?",
            "So did you compare to?",
            "The typical map estimators say using belief propagation and so on.",
            "So so I guess, so I guess.",
            "You could.",
            "Yeah, I I can repeat it so.",
            "So I think the main main part is.",
            "Did we compare this to the typical map estimator and can we get similar results for the map estimate?",
            "Um?",
            "The issue with the map estimate, since it's solved by this graph cut, it doesn't have this nice closed form.",
            "Then we can't get this or I have.",
            "I don't know how to get the same sort of consistency results as we do for our estimator.",
            "But we didn't empirically compare to the map, so there have been some recent results where if.",
            "The eigenvalues of this power law behavior.",
            "Then these map estimates do have guarantees.",
            "OK, and this is for this is still based like Bayes risk bounds on 01 loss.",
            "So yeah yeah.",
            "I'll look into that.",
            "Any other we have time for couple more questions.",
            "Speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is work that I've been doing with my advisor, Aarti Singh.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So networks occur in the real world all the time, and the task that we're going to be considering is estimating activation patterns in this network.",
                    "label": 1
                },
                {
                    "sent": "And so one example of this is we have routers in the Internet.",
                    "label": 0
                },
                {
                    "sent": "Connections mean that they they can pass messages and the activation pattern that we're interested in for this is localising congestion in this network so that we can alleviate this congestion congestion in some way.",
                    "label": 0
                },
                {
                    "sent": "And experimentally we can get at this by computing roundtrip times, things like that.",
                    "label": 0
                },
                {
                    "sent": "Another example is we have sensors in a body of water and the activation pattern here.",
                    "label": 0
                },
                {
                    "sent": "That we would like to localize in some senses contamination.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to detect where is the oil.",
                    "label": 0
                },
                {
                    "sent": "And in this body of water.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But now without structure, the thing that we're considering would be the normal means problem.",
                    "label": 1
                },
                {
                    "sent": "This is a statistical classical statistical problem.",
                    "label": 0
                },
                {
                    "sent": "So first we believe that there's some underlying vector P dimensional an in our situation, we're generally going to be thinking that these coordinates are either zero or one.",
                    "label": 0
                },
                {
                    "sent": "For the most part here, Red represents one and blue represents 0.",
                    "label": 0
                },
                {
                    "sent": "And of course we are not going to observe this signal.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to observe some noisy version of it.",
                    "label": 0
                },
                {
                    "sent": "And the noise that we're considering is additive Gaussian I. Identically distributed and independent, Zero Sigma squared, where Sigma squared variance.",
                    "label": 0
                },
                {
                    "sent": "Now the task is to reconstruct X from Y.",
                    "label": 1
                },
                {
                    "sent": "These noisy versions in this normal means problem, and generally we're going to be suffering from high dimensionality if P is large.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what happens when we incorporate structure?",
                    "label": 0
                },
                {
                    "sent": "So a priority this vector?",
                    "label": 0
                },
                {
                    "sent": "The ordering of the components has no meaning, right?",
                    "label": 0
                },
                {
                    "sent": "In the in the normal means problem.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we suppose that we have some underlying network where the components of X are indexed by vertices of a graph.",
                    "label": 0
                },
                {
                    "sent": "Edges between these vertices specify some dependence.",
                    "label": 0
                },
                {
                    "sent": "A priore between the variables in these components of the vector X.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, how does this help?",
                    "label": 0
                },
                {
                    "sent": "How does it help to know that there's some underlying structure?",
                    "label": 0
                },
                {
                    "sent": "So if we were to use the noisy observations at face value and try to estimate which are active, which are red and which are not blue.",
                    "label": 0
                },
                {
                    "sent": "You can see these highlighted nodes.",
                    "label": 0
                },
                {
                    "sent": "They're kind of purplish right, and so we might assume incorrectly that they are blue and in fact, but so if we were to incorporate this structure uses structure wisely, we would note that they are adjacent to a red node as something that's obviously red, and then we might infer, correctly that these two nodes are in fact read, so this is how using the dependencies given by this graph we can reconstruct X from YA little more efficiently.",
                    "label": 1
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to be very specific about the generative statistical model that I'm going to be using here.",
                    "label": 0
                },
                {
                    "sent": "So first nature will draw a graph.",
                    "label": 0
                },
                {
                    "sent": "This is what we assume we believe to be true with P nodes.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then nature draws a signal, an activation pattern X.",
                    "label": 0
                },
                {
                    "sent": "Where the density is proportionate to E to the negative X transpose LX, where L is the unnormalized graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So if the dominating measure here is labagh and L is the precision matrix, then we get back our usual Gaussian graphical models.",
                    "label": 0
                },
                {
                    "sent": "This isn't as interesting to us what we're going to be considering when, when the dominating measure is the counting measure.",
                    "label": 0
                },
                {
                    "sent": "We get back binary signals, and this is the Ising model, which is a well studied thing in physics, actually.",
                    "label": 0
                },
                {
                    "sent": "So a little note about the graph Laplacian is defined as the diagonal degree vector minus the weight matrix for the graph, and then X transpose.",
                    "label": 0
                },
                {
                    "sent": "Alex this is a good thing to be looking at because what it is is this weighted sum of squared differences between adjacent components of our vector, right?",
                    "label": 0
                },
                {
                    "sent": "So it penalizes against these differences between adjacent nodes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course we draw that same Gaussian IID Zero Sigma squared noise and this is what we get to observe.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so why isn't this exactly a solved problem?",
                    "label": 0
                },
                {
                    "sent": "So first of all, since we have a generative model, we think that we should be using Bayes optimal rules for Mean Square and Hamming distance.",
                    "label": 0
                },
                {
                    "sent": "The Bayes optimal rules of the posterior mean in the posterior centroid respectively.",
                    "label": 1
                },
                {
                    "sent": "These are one hard to implement and two very hard to analyze.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Now, if we had 01 loss, then the Bayes optimal rule is the map estimate and this is efficiently implemented via graph min cut.",
                    "label": 0
                },
                {
                    "sent": "We know this the issue is that the 01 loss isn't terribly appropriate.",
                    "label": 0
                },
                {
                    "sent": "I think in this case because there's no notion of closeness between our estimator and the truth.",
                    "label": 1
                },
                {
                    "sent": "Either you're correct or you're not correct in the 01 loss, so we're considering the Hamming distance in the mean square error.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There there is work that uses the Unnormalized Laplacian and of course the normalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "But most of these are justified in the embedded setting and there's this.",
                    "label": 1
                },
                {
                    "sent": "There's this sort of focused importance on the second eigenvalue of Laplacian.",
                    "label": 1
                },
                {
                    "sent": "But this is.",
                    "label": 0
                },
                {
                    "sent": "These aren't always the case, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the estimated that we're proposing is very simple.",
                    "label": 0
                },
                {
                    "sent": "So first you take your unnormalized Laplacian, your graph Laplacian and then you compute its eigen system.",
                    "label": 0
                },
                {
                    "sent": "Right eigenvalue eigenvector pairs, land is use.",
                    "label": 1
                },
                {
                    "sent": "Then you have them increasing from zero.",
                    "label": 0
                },
                {
                    "sent": "So the estimator that we use we take our our noisy observations and we project them onto the span of the first K eigenvectors, right?",
                    "label": 0
                },
                {
                    "sent": "There's two reasons why we like this one.",
                    "label": 0
                },
                {
                    "sent": "It's easy to analyze the astronautic risk, and two, it's easy to implement if you have an SVD, so.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over it's just three lines of code.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, So what is this giving us for graphs that we're accustomed to write for the hierarchical graph?",
                    "label": 1
                },
                {
                    "sent": "It's just specified by this tree structure where we observe the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "The graph Laplacian is the unnormalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Has this block matrix form?",
                    "label": 0
                },
                {
                    "sent": "Now what ends up happening is that the Laplacian eigenvectors give us these Haar wavelets where this bottom row is the constant and then we have increasing resolution wavelets as we go up.",
                    "label": 0
                },
                {
                    "sent": "And this is a localized basis.",
                    "label": 1
                },
                {
                    "sent": "Now if we if we were to look at the lattice graph on a tourist in dimensions first of all, the lattice Laplacian has a circulant form and the Laplacian eigen basis gives us back the Fourier basis where this is constant and these are increasing frequency sinusoids as we go up.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's look at a couple of these simulations that we were looking at before, so we take this network activation pattern which is actually drawn from an Ising model here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we add noise.",
                    "label": 0
                },
                {
                    "sent": "Sigma squared is 1/2 in this case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we use the Eigen Maps estimator where our eigen dimensions is 3 projection onto a 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "We have near perfect recovery in this example.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at a larger real world type graph.",
                    "label": 0
                },
                {
                    "sent": "With 100",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nodes and then we add even more noise.",
                    "label": 0
                },
                {
                    "sent": "Sigma squared is 4/5.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the Eigen Maps estimator again with this is with ten.",
                    "label": 0
                },
                {
                    "sent": "I can dimension does a pretty good job of recovering the signal and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furthermore, if we were to threshold this estimator.",
                    "label": 0
                },
                {
                    "sent": "And then we get in your perfect recovery where if we had thresholded the observations we would get pretty poor recovery.",
                    "label": 0
                },
                {
                    "sent": "So this gives us this intuition that we can do even better for larger graphs because there's more connectivity in larger graphs alot of times.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the big picture that we're trying to say is where do we get consistent estimation?",
                    "label": 1
                },
                {
                    "sent": "This is the sort of asymptotically statistical statements that we want to be making right?",
                    "label": 0
                },
                {
                    "sent": "An algorithm is not good enough for us.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if we define the Bayes, the Bayes risk to be the expected average mean square error loss, then we say that consistent estimation is when this is driven to 0.",
                    "label": 0
                },
                {
                    "sent": "And in the unstructured case, what we need to happen?",
                    "label": 0
                },
                {
                    "sent": "What needs to happen is that the Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "The noise variance has to decrease to get consistent estimation as the network size grows.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what we find for a lot of graph models is that for the structure for these structural cases, we can tolerate increasing noise as the graph grows, and I'd also argue that some of these models are very natural, if not pedagogical.",
                    "label": 0
                },
                {
                    "sent": "Specifically, if Sigma squared is like little low P to the gamma, we find that below that is consistency regime.",
                    "label": 0
                },
                {
                    "sent": "So now a lot of what we want to be doing is figuring out what gamma is for these networks.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main machinery that we used to get at this is this bound on the Bayes risk.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's decomposed into 3.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Terms the first term is a concentration bound.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next is is like a bias term which is inversely proportionate to the Lambda K plus one.",
                    "label": 0
                },
                {
                    "sent": "The K plus first eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this variance term, which is linear in K. If.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You note that Lambda K plus one is like a quantile of the eigenvalue distribution.",
                    "label": 0
                },
                {
                    "sent": "If we select eigenvalues uniformly at random from its set and K / P is which quantile this corresponds to, then we end up getting a trade off a bias variance tradeoff for choosing K.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you some intuition first about why this is true.",
                    "label": 0
                },
                {
                    "sent": "Note that our estimator is the projection of X plus the projection of the noise.",
                    "label": 0
                },
                {
                    "sent": "And First off, we can show we show using a turn off type bound that X with high probability is within this ellipsoid which is given by X transpose.",
                    "label": 0
                },
                {
                    "sent": "LX is bounded by 2P or Gamma P.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Furthermore, the axes of this ellipse are given.",
                    "label": 0
                },
                {
                    "sent": "The major axis is given by the 1st K eigenvectors and the minor axes are given by the remainder of the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And then the one we Project X on to this first case, the span of the first K eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "The projection loss is at most the radius of this minor axis, which which ends up being like the reciprocal Lambda K plus.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just because this is a projection onto a K dimensional space, it naturally will reduce the isotropic noise by a factor of therapy.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to argue that this is very simple.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So the big picture is now that we have this machinery in place, this this bound on the Bayes risk.",
                    "label": 1
                },
                {
                    "sent": "We can show that for some simple graph models we get this consistency regime.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the first model that we're going to be looking at is this hierarchical structure where we have these latent nodes and then we observe the leaf nodes in this tree.",
                    "label": 0
                },
                {
                    "sent": "What we find is that if we plot the eigenvalue histogram for for this graph for large P and large large depth true depth.",
                    "label": 0
                },
                {
                    "sent": "We find that there's some possibly 0 eigenvalues with some multiplicity, and then as the eigen values increase we get more and more multiplicity.",
                    "label": 0
                },
                {
                    "sent": "This structure has these parameters beta, which is the interaction strength and and also this distance between leaves where there's still a non zero interaction given by two L star.",
                    "label": 1
                },
                {
                    "sent": "And so L star is kind of a measure of the connectivity as it increases the connectivity of the graph increases, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so so we can.",
                    "label": 0
                },
                {
                    "sent": "We can get A at our bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "For this example using this eigenvalue histogram are using using the sequence of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "We see that the bias term, which is the reciprocal of Lambda K plus one, is decreasing sharply while the variance is increasing linearly in K. And so we get the familiar U shaped risk curve.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can further get at what this consistency regime should be for our estimator by having by showing that well, we do show where we find that.",
                    "label": 0
                },
                {
                    "sent": "And the number of Eigen values that are small is bounded above by this term here.",
                    "label": 0
                },
                {
                    "sent": "So what we're able to do is set K the eigen dimension to be this value here and then we define L star to be proportionate to this gamma.",
                    "label": 1
                },
                {
                    "sent": "What ends up happening is that if Sigma squared is litleo pizza the gamma, then we get this Bayes risk consistency.",
                    "label": 1
                },
                {
                    "sent": "So we can see how kind of concentration results, concentration type results for the eigenvalue histogram.",
                    "label": 1
                },
                {
                    "sent": "The density gives us this consistency region an what gamma should be.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's do this for the lattice graph in D dimensions, where along each dimension there's N vertices.",
                    "label": 0
                },
                {
                    "sent": "Using Hoeffding's bound we prove that the eigen values tend to concentrate above the dimensions the lattice dimensions D exponentially.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again we get this bias variance tradeoff with RU shaped risk curve and we.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See as the lattice.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensions increases we get faster or lower and lower bounds for this bazra.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, using that same concentration bound for the Lambda eyes were able to specify the lattice dimension to be gamma or proportionate to gamma.",
                    "label": 0
                },
                {
                    "sent": "We set K to be this right hand side here for this concentration bound.",
                    "label": 0
                },
                {
                    "sent": "And then again we see that Sigma squared, if it's little low piece of the gamma then we get risk consistency.",
                    "label": 0
                },
                {
                    "sent": "As as the number of nodes increases in the.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do the same.",
                    "label": 0
                },
                {
                    "sent": "A similar thing for the orders running graph.",
                    "label": 0
                },
                {
                    "sent": "What the years radiograph is is you take 2 nodes.",
                    "label": 0
                },
                {
                    "sent": "And you connect them, you connect them with an edge with probability P to the gamma minus one.",
                    "label": 1
                },
                {
                    "sent": "We again get a similar concentration bound and we're able to show the same sort of risk consistency region.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in total what we see is that for the tree graph, the interaction distance can be proportionate to gamma.",
                    "label": 1
                },
                {
                    "sent": "The dimensions can be proportionate to gamma for the lattice and for the orders.",
                    "label": 0
                },
                {
                    "sent": "For any graph, the edge probability is proportionate to gamma.",
                    "label": 1
                },
                {
                    "sent": "So we see that to get risk consistency in this regime where you have below P to the gamma.",
                    "label": 0
                },
                {
                    "sent": "We need that gamma is some sort of a measure of the connectivity of the graph, and this satisfies our intuition of a graph.",
                    "label": 0
                },
                {
                    "sent": "Is Morris connected?",
                    "label": 0
                },
                {
                    "sent": "Then it's going to be more useful in structured estimate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So, so we have some simulations for the tree graph.",
                    "label": 1
                },
                {
                    "sent": "The Eigen Maps estimator does nearly as well as the Bayes rule, but all outperform significantly the unstructured case.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the lattice graph, as the dimensions D increases, we continually get the Eigen Maps estimator.",
                    "label": 0
                },
                {
                    "sent": "Does better in terms of MSE than the naive.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motor assembly for the order.",
                    "label": 0
                },
                {
                    "sent": "Any graph as the network size increases.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for we also did this on the small world graph.",
                    "label": 0
                },
                {
                    "sent": "What strogatz model?",
                    "label": 0
                },
                {
                    "sent": "Can we see the same?",
                    "label": 0
                },
                {
                    "sent": "The same, much better performance of the Eigen map system?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, in summary, nature draws in X from an Ising model and then we get observations which are these noisy versions of it using our Eigen Maps estimator were able to show that for these simple graph models that we get consistency in our regime where the noise variance can increase in the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "So one question I had was.",
                    "label": 0
                },
                {
                    "sent": "So these quest these this estimator works if there's a decay in the eigenvalues etc.",
                    "label": 0
                },
                {
                    "sent": "So would this correspond to?",
                    "label": 0
                },
                {
                    "sent": "An increase in the eigenvalues because it's the reciprocal of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so in such settings with the map estimator, the typical typically intractable map estimators would they also perform well?",
                    "label": 0
                },
                {
                    "sent": "So did you compare to?",
                    "label": 0
                },
                {
                    "sent": "The typical map estimators say using belief propagation and so on.",
                    "label": 0
                },
                {
                    "sent": "So so I guess, so I guess.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I can repeat it so.",
                    "label": 0
                },
                {
                    "sent": "So I think the main main part is.",
                    "label": 0
                },
                {
                    "sent": "Did we compare this to the typical map estimator and can we get similar results for the map estimate?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The issue with the map estimate, since it's solved by this graph cut, it doesn't have this nice closed form.",
                    "label": 0
                },
                {
                    "sent": "Then we can't get this or I have.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to get the same sort of consistency results as we do for our estimator.",
                    "label": 0
                },
                {
                    "sent": "But we didn't empirically compare to the map, so there have been some recent results where if.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues of this power law behavior.",
                    "label": 0
                },
                {
                    "sent": "Then these map estimates do have guarantees.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is for this is still based like Bayes risk bounds on 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I'll look into that.",
                    "label": 0
                },
                {
                    "sent": "Any other we have time for couple more questions.",
                    "label": 0
                },
                {
                    "sent": "Speaker.",
                    "label": 0
                }
            ]
        }
    }
}