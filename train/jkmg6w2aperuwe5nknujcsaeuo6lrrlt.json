{
    "id": "jkmg6w2aperuwe5nknujcsaeuo6lrrlt",
    "title": "NLP data cleansing based on Linguistic Ontology constraints",
    "info": {
        "author": [
            "Dimitris Kontokostas, Agile Knowledge Engineering and Semantic Web (AKSW), University of Leipzig"
        ],
        "published": "July 30, 2014",
        "recorded": "May 2014",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2014_kontokostas_nlp/",
    "segmentation": [
        [
            "Paper titled NLP Data Cleasing based on linguistic mythology constraints."
        ],
        [
            "Let's start with the classic image.",
            "The only reason I'm showing it becausw it's from 2011 and three years ago."
        ],
        [
            "Low linguistic data were filed only under crossdomain category."
        ],
        [
            "But the last few years we've seen a big rise in linguistic communities.",
            "We've seen the open linguistics from OK fan, and will be glad to have Community link data for language, technology, WC, Community Leader, Project, and many other communities that I'm missing."
        ],
        [
            "And also a lot of linguistic related workshops and conferences like and will be in the pedia web of linked entity with legal linked open data for prizes, LDL and Elrick.",
            "In for a."
        ],
        [
            "Rick, although it's a core linguistic conference this year, her topic was four linked open data."
        ],
        [
            "So what the linguist tried to do is they created their own alicloud VLD cloud which is quite big already.",
            "The green bubbles are for dictionaries, orange for repositories, Brown for databases, Blues for corpus and grace are any other data set that could be used from for linguistic purposes.",
            "For example, the Pedia is a great resource for linguists."
        ],
        [
            "The problem definition linguistic data is not a correct term.",
            "It's actually a purpose driven definition and we define it as all the datasets that could be used for any linguistic purpose.",
            "And we've seen a big rise in data and all of the similarities, and it's hard for newcomers to follow up their dates or learn all the neurologist.",
            "And validation is an essential part especially for NLP 'cause there are many different pipelines like parsing, annotation, situation and so on and errors in a single step can be easily propagated to all the computational thing.",
            "So maintainers already foreseen that it have some validation options, but we think it's incomplete.",
            "And in this working focus on lemon, and if, as a proof of concept."
        ],
        [
            "Before we go to details will give an overview of Lemon.",
            "If Lemon is models lexicons and machine readable dictionaries, it's native RDF form.",
            "It's linguistically sound structure and has quite a loose ontology to make it more usable by more people.",
            "The basic idea is that we have a lexicon which has entries which are lexical entries, and every entry must have lexical form and can have Alexa concerns that link Synology."
        ],
        [
            "Here's an example we have lexicon with.",
            "Sizzling Lexicon has two entries with tender Tia.",
            "Pizza is a lexical entry that links to the Pedia pizza and tortilla is an entry that links to the pediatrician.",
            "Although this looks very simple and complete, it's actually wrong."
        ],
        [
            "Because there are many things missing.",
            "Every lexical must have a language and it must not have a language tag.",
            "Every lexical entry must have a Canonical form.",
            "And so on."
        ],
        [
            "NIF stands for NLP interchange format.",
            "It tries to provide a logical transformation formalisation for strengthening locations in order to make interoperability between different NLP tools, language resources and annotations.",
            "It's built from standards like RDF Graph, RFC 5147.",
            "And tries to reuse the whole RDF stack of tools and to decrease the development cost for integration.",
            "It's already degraded in the Pedia spotlight.",
            "In cell phone cord and LP.",
            "Open NLP are the face validator, kernel converter and many other tools."
        ],
        [
            "The basic idea is that the top.",
            "We have different tools and services like war stand for cordon, LP folks, DPS Spotlight.",
            "And if they all have nice Roberts, what Nick does, it provides structural, conceptual and access interoperability not only between these tools, but also to the whole of the cloud as a background knowledge for cross linking and query Federation."
        ],
        [
            "And now an example.",
            "We have this more complicated.",
            "We have a context, which is the string.",
            "My dog likes pizza.",
            "It's from string zero to 17.",
            "And wouldn't this context from another here I with the uncle of dog from characters 2 to 7 and we say dog is an animal."
        ],
        [
            "Again, many errors in this case can exist.",
            "For example a typo in the type.",
            "The actual length is 18 characters, not 17.",
            "An IF data type begin index must have XSD nonnegative integer data types, and in the RFC string must have at least begin at an index defined."
        ],
        [
            "So they were maintainers seen these errors in the use of their abilities, so they try to provide some scripts and validators.",
            "It can be used.",
            "Lemon has a Python script that contains 24 structural criteria, but it's too slow for big datasets and doesn't have good reporting.",
            "An if you sparking to test this test cases and they have 11 sparkle test cases to find common errors."
        ],
        [
            "So what we do is we build on previous work.",
            "It's a paper called test driven evaluation of linked data quality where we did horizontal in domain data quality assessment of five large scale datasets and detect a massive number of errors and apply the same technique to almost 300 calories independent of their domain or purpose.",
            "Then you contributions here is that we relate to all regionals providing ontology on top of our methodology and especially focused on domain specific validation.",
            "And we saw that we can very quickly improve existing validation options that are provided by ontology maintainers."
        ],
        [
            "We briefly go through the methodology.",
            "It's called test driven development methodology and inspired from test driven software development.",
            "And the same notion.",
            "We define the same terms.",
            "We say that this case is a data constraint that involves one or more triples.",
            "But this one is a set of test cases for testing data set and every this case can have a status of success failed timeout in case of complexity or error.",
            "For example network error.",
            "Any data to fail is not always a hard fail.",
            "It can be an actual error warning or maybe a notice that something that could be added to make it better but doesn't hurt if it doesn't exist.",
            "The good thing in RDF compared to software development is that we have the same model.",
            "RDF for both the data and the schema, and this unified model facilitates automatic this case generation.",
            "And we use Sparkle as our test case definition language."
        ],
        [
            "Let's look at an example.",
            "This case.",
            "We say that and if RFC 5147 string should never have a beginning that's greater than an index and we have the following sparkle query.",
            "Select this where need beginning next V1, and if we ended X V2 filter V1 greater than, we do.",
            "So what we do is that we query for errors and we have a success when we get an empty result set and fail whenever we have a result.",
            "And for every delegate we have a violation instance.",
            "In case of timeout or error we need to further investigate the complex of the query or the query syntax or the sparkle engine capabilities."
        ],
        [
            "Sparkles and easy to master language and can easily write Spartan queries, but this is error prone and not very productive.",
            "So we also define data quality test patterns which are abstract sparkle queries that can be further refined into concrete test cases using proper bindings.",
            "Have library of the patterns and one of these is the following.",
            "And if we define the binding.",
            "Nick beginning next to P1 and if any next to P2 and greater drop what we do is instantiate our previously previously defined example.",
            "So patterns is a very easy way to rapidly instantiate new test cases that based on very common validation errors."
        ],
        [
            "So we have the manual.",
            "This case definition and the pattern base second manual and we also have auto generators that right provide automatic test case generation.",
            "We call them tags and what they do is they get an ontology as an input and query the ontology for support actions.",
            "For example, this query gets all the old stone with definitions.",
            "And for every result we get we create a binding to pattern and instantiate this case so.",
            "At the moment we support our defense domain and range from all cardinality constraints, functionality, intersectionality, class and property disjointness.",
            "Symmetricity an old deprecated so this is a way to rapidly instantiate.",
            "Make this cases without a."
        ],
        [
            "Any user interaction.",
            "The workload is like this.",
            "We have two inputs.",
            "We have the community feedback that's taken by the knowledge maintainers or the Datsun maintainers.",
            "That get common errors from their community and generate manual sparkle queries or based on the pattern.",
            "And we have the schema input that using the tags.",
            "We automatically instantiate new test case based on the pattern library and existing tags.",
            "As a third step that is described in the main paper, we have an optional enrichment phase with the learning, for example, that using that data set we generate new actions.",
            "Automatically and feed them into the tags and get new test cases automatically again."
        ],
        [
            "So what's the relation to the reasoners?",
            "At the moment, sparkling our approach detects only a subset of the things that original could detect, and we probably will never reach 100%.",
            "And we are limited by the sparkling point reasoning support because we use extensively property paths to get transitive relations.",
            "And we also limited from the old Sparkle translation.",
            "On the other hand, sparkle can detect more errors and all can be cause.",
            "For example, we can do sex on language which is not supported.",
            "Noel, an all listeners, are not easily visible on larger datasets.",
            "And using sparkling now is very easy because everything is behind an endpoint, so using this technology seems to deploy on the whole of the cloud.",
            "And finally, reporting library is more user friendly than writing, writing manual whole actions, but still requires similarity and non common errors will need to be done manually again."
        ],
        [
            "So our ontology we try to do some dogfooding and have whole input and output in RDF and we model everything in all like their suitcases.",
            "Patterns of the generators and so on.",
            "And we made it quite strict in order to serve as a validation layer for our input and output, because using our tool can check the inputs and the outputs of our tool again, and we also have 4 levels of reporting to based on test case, reporting status and two based on violation instance level."
        ],
        [
            "Here, here's the class diagram.",
            "We have a test which is approved collection that has a set of test cases as a member.",
            "At this case, can be either a manual, this case where we write the Magna Sparkle query, or a pattern based this case partner based.",
            "This case is based on a pattern may be generated by this generator and has a set of bindings that buying a value to a pattern parameter.",
            "This case dependencies are used for defining this execution scenes.",
            "Because for example, when we have a domain tech that fails, then there is no reason to take for katnal concerns because it will fail as well.",
            "So this way we can skip error that will fail anyway.",
            "And don't run them.",
            "And finally, these other locations are used for the reporting phase where we allocate the result with extra properties for filtering."
        ],
        [
            "Which is there is a presentation class diagram we have at this execution that uses.",
            "The suit and has some statistics like start anytime and the source with this run failed, succeeded and so on.",
            "And contains a set of test case results, and this part is the we have a test case status where we have success fail timeout or error and can be further.",
            "It's with result counts or prevalence and in this part we have violation instance instance reporting where for every violation instance we have an outlook entry which contains a message.",
            "The local level like log error, log warning or log notice and the resource that is based on this violation and this is further enriched.",
            "With this class that we can have more properties for enriching the results that could be used for navigating on the errors using the Facebook browser."
        ],
        [
            "Now back to lemon.",
            "If we implemented this school methodology in a tool called RDF Unit Suite.",
            "And we run our run it with lemon and if our tax generated a set of test case but not hard in some corner cases like old enough always from some values from herself, in some cases of our DFS sub property of.",
            "And what we did is we use the existing validation tools and everything that was not handled by the tags.",
            "We created a manual sparkle query for every option developed.",
            "The other validators had.",
            "And this is the total we have for lemon.",
            "We created 182 test cases and 96 for lemon and these are aggregated for domain range data type, cardinality, disjointness, functional endurance functional.",
            "And we had 10 manual test cases for lemon and for NIF, and these were not all only errors.",
            "We talk them either as actual errors as warnings or R as notices."
        ],
        [
            "Some example this case for lemon are for example memo lemon error where we asked for this property to never be symmetric or contain cycles, and this is that this case we used to detect cycles and some interesting or lemon language must not have a language tag which can be checked by this this case where we check if the language is not empty."
        ],
        [
            "An example of this case for MIF is this, where we take if the beginner decks are correct, where we calculate the substrings that are used for the beginning, an index and secure.",
            "The strings are equal.",
            "And if they're not, we report an error."
        ],
        [
            "This is how the valuation data set.",
            "We have 11 datasets.",
            "Five are based on lemon and six on leaf.",
            "For Lemon we have lemon obituary for English and for them, and we have the ordnet data set.",
            "The DPI visionary and create cell which is a multilingual translation graph for more than 5050 lexicons.",
            "And for now, if we have wikilinks corpus, which example of the whole WikiLeaks corpus that was published by Google, it's converted in if.",
            "58 annotated languages from the spotlight.",
            "50 sentences from the Ida Corpus and the next three are annotated manually annotated articles from various sources, like their menus, articles, or RSS feeds, or generally news articles."
        ],
        [
            "Not my results here.",
            "We have all the datasets sorted by by size.",
            "And next we have the successful test cases that this case is that failed timeout or return an error.",
            "And this column is all the errors that are produced from the ontology from that generators.",
            "And the following three are from manual errors that are actually rose warnings or notices information.",
            "The last column is not actual errors are just suggestions that could be included in the data set, but doesn't hurt if it doesn't be fixed, although we see that the lemon datasets seem to have more errors, the scale is very big, so if we scale them to the same size it's more or less the same.",
            "An interesting part here is to notice that this errors that are generated from myth begin at any decks, because when we try to compute substrings, some some indexes contain letters and this produces a failing the query and then an error.",
            "So this could be prevented by providing a dependency change between the test cases."
        ],
        [
            "Yeah.",
            "And now to conclude what we did here, we extended previous introduced methodology for test driven quality assessment.",
            "We presented engineering methodology and we devise a total of almost 300 this cases for NLP datasets using lemon and if.",
            "And our valuation revealed a substantial number of errors again.",
            "And for the future, what we want to do is to extend this to model datasets like model nerd or ideas.",
            "RDF.",
            "Provide automatic dependencies generation between test cases and Rob are the unit as a service that can be used for Destin LB datasets.",
            "We are already working on integrating if it's almost complete, so yeah.",
            "That we are happy to collaborate."
        ],
        [
            "And thank you would like things are great for helping us with lemon model and you can check out our this unit and the code in the following links."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper titled NLP Data Cleasing based on linguistic mythology constraints.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start with the classic image.",
                    "label": 0
                },
                {
                    "sent": "The only reason I'm showing it becausw it's from 2011 and three years ago.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Low linguistic data were filed only under crossdomain category.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the last few years we've seen a big rise in linguistic communities.",
                    "label": 0
                },
                {
                    "sent": "We've seen the open linguistics from OK fan, and will be glad to have Community link data for language, technology, WC, Community Leader, Project, and many other communities that I'm missing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also a lot of linguistic related workshops and conferences like and will be in the pedia web of linked entity with legal linked open data for prizes, LDL and Elrick.",
                    "label": 0
                },
                {
                    "sent": "In for a.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rick, although it's a core linguistic conference this year, her topic was four linked open data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what the linguist tried to do is they created their own alicloud VLD cloud which is quite big already.",
                    "label": 0
                },
                {
                    "sent": "The green bubbles are for dictionaries, orange for repositories, Brown for databases, Blues for corpus and grace are any other data set that could be used from for linguistic purposes.",
                    "label": 0
                },
                {
                    "sent": "For example, the Pedia is a great resource for linguists.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem definition linguistic data is not a correct term.",
                    "label": 0
                },
                {
                    "sent": "It's actually a purpose driven definition and we define it as all the datasets that could be used for any linguistic purpose.",
                    "label": 0
                },
                {
                    "sent": "And we've seen a big rise in data and all of the similarities, and it's hard for newcomers to follow up their dates or learn all the neurologist.",
                    "label": 0
                },
                {
                    "sent": "And validation is an essential part especially for NLP 'cause there are many different pipelines like parsing, annotation, situation and so on and errors in a single step can be easily propagated to all the computational thing.",
                    "label": 0
                },
                {
                    "sent": "So maintainers already foreseen that it have some validation options, but we think it's incomplete.",
                    "label": 0
                },
                {
                    "sent": "And in this working focus on lemon, and if, as a proof of concept.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before we go to details will give an overview of Lemon.",
                    "label": 0
                },
                {
                    "sent": "If Lemon is models lexicons and machine readable dictionaries, it's native RDF form.",
                    "label": 0
                },
                {
                    "sent": "It's linguistically sound structure and has quite a loose ontology to make it more usable by more people.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is that we have a lexicon which has entries which are lexical entries, and every entry must have lexical form and can have Alexa concerns that link Synology.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example we have lexicon with.",
                    "label": 0
                },
                {
                    "sent": "Sizzling Lexicon has two entries with tender Tia.",
                    "label": 0
                },
                {
                    "sent": "Pizza is a lexical entry that links to the Pedia pizza and tortilla is an entry that links to the pediatrician.",
                    "label": 0
                },
                {
                    "sent": "Although this looks very simple and complete, it's actually wrong.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because there are many things missing.",
                    "label": 0
                },
                {
                    "sent": "Every lexical must have a language and it must not have a language tag.",
                    "label": 0
                },
                {
                    "sent": "Every lexical entry must have a Canonical form.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "NIF stands for NLP interchange format.",
                    "label": 1
                },
                {
                    "sent": "It tries to provide a logical transformation formalisation for strengthening locations in order to make interoperability between different NLP tools, language resources and annotations.",
                    "label": 1
                },
                {
                    "sent": "It's built from standards like RDF Graph, RFC 5147.",
                    "label": 1
                },
                {
                    "sent": "And tries to reuse the whole RDF stack of tools and to decrease the development cost for integration.",
                    "label": 0
                },
                {
                    "sent": "It's already degraded in the Pedia spotlight.",
                    "label": 0
                },
                {
                    "sent": "In cell phone cord and LP.",
                    "label": 0
                },
                {
                    "sent": "Open NLP are the face validator, kernel converter and many other tools.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic idea is that the top.",
                    "label": 0
                },
                {
                    "sent": "We have different tools and services like war stand for cordon, LP folks, DPS Spotlight.",
                    "label": 0
                },
                {
                    "sent": "And if they all have nice Roberts, what Nick does, it provides structural, conceptual and access interoperability not only between these tools, but also to the whole of the cloud as a background knowledge for cross linking and query Federation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now an example.",
                    "label": 0
                },
                {
                    "sent": "We have this more complicated.",
                    "label": 0
                },
                {
                    "sent": "We have a context, which is the string.",
                    "label": 0
                },
                {
                    "sent": "My dog likes pizza.",
                    "label": 0
                },
                {
                    "sent": "It's from string zero to 17.",
                    "label": 0
                },
                {
                    "sent": "And wouldn't this context from another here I with the uncle of dog from characters 2 to 7 and we say dog is an animal.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, many errors in this case can exist.",
                    "label": 0
                },
                {
                    "sent": "For example a typo in the type.",
                    "label": 0
                },
                {
                    "sent": "The actual length is 18 characters, not 17.",
                    "label": 0
                },
                {
                    "sent": "An IF data type begin index must have XSD nonnegative integer data types, and in the RFC string must have at least begin at an index defined.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they were maintainers seen these errors in the use of their abilities, so they try to provide some scripts and validators.",
                    "label": 0
                },
                {
                    "sent": "It can be used.",
                    "label": 0
                },
                {
                    "sent": "Lemon has a Python script that contains 24 structural criteria, but it's too slow for big datasets and doesn't have good reporting.",
                    "label": 1
                },
                {
                    "sent": "An if you sparking to test this test cases and they have 11 sparkle test cases to find common errors.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is we build on previous work.",
                    "label": 1
                },
                {
                    "sent": "It's a paper called test driven evaluation of linked data quality where we did horizontal in domain data quality assessment of five large scale datasets and detect a massive number of errors and apply the same technique to almost 300 calories independent of their domain or purpose.",
                    "label": 1
                },
                {
                    "sent": "Then you contributions here is that we relate to all regionals providing ontology on top of our methodology and especially focused on domain specific validation.",
                    "label": 1
                },
                {
                    "sent": "And we saw that we can very quickly improve existing validation options that are provided by ontology maintainers.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We briefly go through the methodology.",
                    "label": 0
                },
                {
                    "sent": "It's called test driven development methodology and inspired from test driven software development.",
                    "label": 0
                },
                {
                    "sent": "And the same notion.",
                    "label": 0
                },
                {
                    "sent": "We define the same terms.",
                    "label": 0
                },
                {
                    "sent": "We say that this case is a data constraint that involves one or more triples.",
                    "label": 1
                },
                {
                    "sent": "But this one is a set of test cases for testing data set and every this case can have a status of success failed timeout in case of complexity or error.",
                    "label": 0
                },
                {
                    "sent": "For example network error.",
                    "label": 0
                },
                {
                    "sent": "Any data to fail is not always a hard fail.",
                    "label": 0
                },
                {
                    "sent": "It can be an actual error warning or maybe a notice that something that could be added to make it better but doesn't hurt if it doesn't exist.",
                    "label": 1
                },
                {
                    "sent": "The good thing in RDF compared to software development is that we have the same model.",
                    "label": 0
                },
                {
                    "sent": "RDF for both the data and the schema, and this unified model facilitates automatic this case generation.",
                    "label": 0
                },
                {
                    "sent": "And we use Sparkle as our test case definition language.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "This case.",
                    "label": 0
                },
                {
                    "sent": "We say that and if RFC 5147 string should never have a beginning that's greater than an index and we have the following sparkle query.",
                    "label": 1
                },
                {
                    "sent": "Select this where need beginning next V1, and if we ended X V2 filter V1 greater than, we do.",
                    "label": 1
                },
                {
                    "sent": "So what we do is that we query for errors and we have a success when we get an empty result set and fail whenever we have a result.",
                    "label": 1
                },
                {
                    "sent": "And for every delegate we have a violation instance.",
                    "label": 0
                },
                {
                    "sent": "In case of timeout or error we need to further investigate the complex of the query or the query syntax or the sparkle engine capabilities.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sparkles and easy to master language and can easily write Spartan queries, but this is error prone and not very productive.",
                    "label": 0
                },
                {
                    "sent": "So we also define data quality test patterns which are abstract sparkle queries that can be further refined into concrete test cases using proper bindings.",
                    "label": 1
                },
                {
                    "sent": "Have library of the patterns and one of these is the following.",
                    "label": 0
                },
                {
                    "sent": "And if we define the binding.",
                    "label": 0
                },
                {
                    "sent": "Nick beginning next to P1 and if any next to P2 and greater drop what we do is instantiate our previously previously defined example.",
                    "label": 0
                },
                {
                    "sent": "So patterns is a very easy way to rapidly instantiate new test cases that based on very common validation errors.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have the manual.",
                    "label": 0
                },
                {
                    "sent": "This case definition and the pattern base second manual and we also have auto generators that right provide automatic test case generation.",
                    "label": 0
                },
                {
                    "sent": "We call them tags and what they do is they get an ontology as an input and query the ontology for support actions.",
                    "label": 0
                },
                {
                    "sent": "For example, this query gets all the old stone with definitions.",
                    "label": 0
                },
                {
                    "sent": "And for every result we get we create a binding to pattern and instantiate this case so.",
                    "label": 1
                },
                {
                    "sent": "At the moment we support our defense domain and range from all cardinality constraints, functionality, intersectionality, class and property disjointness.",
                    "label": 0
                },
                {
                    "sent": "Symmetricity an old deprecated so this is a way to rapidly instantiate.",
                    "label": 0
                },
                {
                    "sent": "Make this cases without a.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any user interaction.",
                    "label": 0
                },
                {
                    "sent": "The workload is like this.",
                    "label": 0
                },
                {
                    "sent": "We have two inputs.",
                    "label": 0
                },
                {
                    "sent": "We have the community feedback that's taken by the knowledge maintainers or the Datsun maintainers.",
                    "label": 0
                },
                {
                    "sent": "That get common errors from their community and generate manual sparkle queries or based on the pattern.",
                    "label": 0
                },
                {
                    "sent": "And we have the schema input that using the tags.",
                    "label": 0
                },
                {
                    "sent": "We automatically instantiate new test case based on the pattern library and existing tags.",
                    "label": 1
                },
                {
                    "sent": "As a third step that is described in the main paper, we have an optional enrichment phase with the learning, for example, that using that data set we generate new actions.",
                    "label": 0
                },
                {
                    "sent": "Automatically and feed them into the tags and get new test cases automatically again.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the relation to the reasoners?",
                    "label": 0
                },
                {
                    "sent": "At the moment, sparkling our approach detects only a subset of the things that original could detect, and we probably will never reach 100%.",
                    "label": 1
                },
                {
                    "sent": "And we are limited by the sparkling point reasoning support because we use extensively property paths to get transitive relations.",
                    "label": 0
                },
                {
                    "sent": "And we also limited from the old Sparkle translation.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, sparkle can detect more errors and all can be cause.",
                    "label": 0
                },
                {
                    "sent": "For example, we can do sex on language which is not supported.",
                    "label": 0
                },
                {
                    "sent": "Noel, an all listeners, are not easily visible on larger datasets.",
                    "label": 1
                },
                {
                    "sent": "And using sparkling now is very easy because everything is behind an endpoint, so using this technology seems to deploy on the whole of the cloud.",
                    "label": 0
                },
                {
                    "sent": "And finally, reporting library is more user friendly than writing, writing manual whole actions, but still requires similarity and non common errors will need to be done manually again.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our ontology we try to do some dogfooding and have whole input and output in RDF and we model everything in all like their suitcases.",
                    "label": 0
                },
                {
                    "sent": "Patterns of the generators and so on.",
                    "label": 0
                },
                {
                    "sent": "And we made it quite strict in order to serve as a validation layer for our input and output, because using our tool can check the inputs and the outputs of our tool again, and we also have 4 levels of reporting to based on test case, reporting status and two based on violation instance level.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, here's the class diagram.",
                    "label": 0
                },
                {
                    "sent": "We have a test which is approved collection that has a set of test cases as a member.",
                    "label": 0
                },
                {
                    "sent": "At this case, can be either a manual, this case where we write the Magna Sparkle query, or a pattern based this case partner based.",
                    "label": 0
                },
                {
                    "sent": "This case is based on a pattern may be generated by this generator and has a set of bindings that buying a value to a pattern parameter.",
                    "label": 0
                },
                {
                    "sent": "This case dependencies are used for defining this execution scenes.",
                    "label": 0
                },
                {
                    "sent": "Because for example, when we have a domain tech that fails, then there is no reason to take for katnal concerns because it will fail as well.",
                    "label": 0
                },
                {
                    "sent": "So this way we can skip error that will fail anyway.",
                    "label": 0
                },
                {
                    "sent": "And don't run them.",
                    "label": 0
                },
                {
                    "sent": "And finally, these other locations are used for the reporting phase where we allocate the result with extra properties for filtering.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is there is a presentation class diagram we have at this execution that uses.",
                    "label": 0
                },
                {
                    "sent": "The suit and has some statistics like start anytime and the source with this run failed, succeeded and so on.",
                    "label": 0
                },
                {
                    "sent": "And contains a set of test case results, and this part is the we have a test case status where we have success fail timeout or error and can be further.",
                    "label": 0
                },
                {
                    "sent": "It's with result counts or prevalence and in this part we have violation instance instance reporting where for every violation instance we have an outlook entry which contains a message.",
                    "label": 0
                },
                {
                    "sent": "The local level like log error, log warning or log notice and the resource that is based on this violation and this is further enriched.",
                    "label": 0
                },
                {
                    "sent": "With this class that we can have more properties for enriching the results that could be used for navigating on the errors using the Facebook browser.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now back to lemon.",
                    "label": 0
                },
                {
                    "sent": "If we implemented this school methodology in a tool called RDF Unit Suite.",
                    "label": 0
                },
                {
                    "sent": "And we run our run it with lemon and if our tax generated a set of test case but not hard in some corner cases like old enough always from some values from herself, in some cases of our DFS sub property of.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we use the existing validation tools and everything that was not handled by the tags.",
                    "label": 0
                },
                {
                    "sent": "We created a manual sparkle query for every option developed.",
                    "label": 0
                },
                {
                    "sent": "The other validators had.",
                    "label": 0
                },
                {
                    "sent": "And this is the total we have for lemon.",
                    "label": 0
                },
                {
                    "sent": "We created 182 test cases and 96 for lemon and these are aggregated for domain range data type, cardinality, disjointness, functional endurance functional.",
                    "label": 1
                },
                {
                    "sent": "And we had 10 manual test cases for lemon and for NIF, and these were not all only errors.",
                    "label": 1
                },
                {
                    "sent": "We talk them either as actual errors as warnings or R as notices.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some example this case for lemon are for example memo lemon error where we asked for this property to never be symmetric or contain cycles, and this is that this case we used to detect cycles and some interesting or lemon language must not have a language tag which can be checked by this this case where we check if the language is not empty.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example of this case for MIF is this, where we take if the beginner decks are correct, where we calculate the substrings that are used for the beginning, an index and secure.",
                    "label": 0
                },
                {
                    "sent": "The strings are equal.",
                    "label": 0
                },
                {
                    "sent": "And if they're not, we report an error.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is how the valuation data set.",
                    "label": 0
                },
                {
                    "sent": "We have 11 datasets.",
                    "label": 0
                },
                {
                    "sent": "Five are based on lemon and six on leaf.",
                    "label": 0
                },
                {
                    "sent": "For Lemon we have lemon obituary for English and for them, and we have the ordnet data set.",
                    "label": 0
                },
                {
                    "sent": "The DPI visionary and create cell which is a multilingual translation graph for more than 5050 lexicons.",
                    "label": 1
                },
                {
                    "sent": "And for now, if we have wikilinks corpus, which example of the whole WikiLeaks corpus that was published by Google, it's converted in if.",
                    "label": 0
                },
                {
                    "sent": "58 annotated languages from the spotlight.",
                    "label": 0
                },
                {
                    "sent": "50 sentences from the Ida Corpus and the next three are annotated manually annotated articles from various sources, like their menus, articles, or RSS feeds, or generally news articles.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not my results here.",
                    "label": 0
                },
                {
                    "sent": "We have all the datasets sorted by by size.",
                    "label": 0
                },
                {
                    "sent": "And next we have the successful test cases that this case is that failed timeout or return an error.",
                    "label": 0
                },
                {
                    "sent": "And this column is all the errors that are produced from the ontology from that generators.",
                    "label": 0
                },
                {
                    "sent": "And the following three are from manual errors that are actually rose warnings or notices information.",
                    "label": 0
                },
                {
                    "sent": "The last column is not actual errors are just suggestions that could be included in the data set, but doesn't hurt if it doesn't be fixed, although we see that the lemon datasets seem to have more errors, the scale is very big, so if we scale them to the same size it's more or less the same.",
                    "label": 0
                },
                {
                    "sent": "An interesting part here is to notice that this errors that are generated from myth begin at any decks, because when we try to compute substrings, some some indexes contain letters and this produces a failing the query and then an error.",
                    "label": 0
                },
                {
                    "sent": "So this could be prevented by providing a dependency change between the test cases.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And now to conclude what we did here, we extended previous introduced methodology for test driven quality assessment.",
                    "label": 0
                },
                {
                    "sent": "We presented engineering methodology and we devise a total of almost 300 this cases for NLP datasets using lemon and if.",
                    "label": 1
                },
                {
                    "sent": "And our valuation revealed a substantial number of errors again.",
                    "label": 1
                },
                {
                    "sent": "And for the future, what we want to do is to extend this to model datasets like model nerd or ideas.",
                    "label": 0
                },
                {
                    "sent": "RDF.",
                    "label": 0
                },
                {
                    "sent": "Provide automatic dependencies generation between test cases and Rob are the unit as a service that can be used for Destin LB datasets.",
                    "label": 0
                },
                {
                    "sent": "We are already working on integrating if it's almost complete, so yeah.",
                    "label": 0
                },
                {
                    "sent": "That we are happy to collaborate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And thank you would like things are great for helping us with lemon model and you can check out our this unit and the code in the following links.",
                    "label": 0
                }
            ]
        }
    }
}