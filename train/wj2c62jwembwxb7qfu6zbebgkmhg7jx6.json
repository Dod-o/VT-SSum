{
    "id": "wj2c62jwembwxb7qfu6zbebgkmhg7jx6",
    "title": "An Experimental Comparison of Click Position-Bias Models",
    "info": {
        "author": [
            "Nick Craswell, Microsoft Research"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Web Mining",
            "Top->Computer Science->Web Search",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/wsdm08_craswell_eccp/",
    "segmentation": [
        [
            "So this isn't a learning to rank paper, this is just a study of.",
            "Of how users interact with the search results list.",
            "And apologies to Bill Ramsey, I couldn't find a photo of him online at the last minute, but I know and Michael here today.",
            "By the way, we just heard from Michael."
        ],
        [
            "OK, so position bias.",
            "This is just the top ranked results in, let's say a rank top.",
            "Ten lists tend to get more clicks, so the question here is why is this?",
            "Is it because some users don't even look at the results and just click on rank one?",
            "For example, is it that people look less at rank 10, so therefore they're less likely to click it?",
            "Is it that users go through in order and click the first good thing they see?",
            "Like what's a good explanation for this bias that we see?",
            "I think this is, you know, not claiming to have solved this problem.",
            "I still think it's it's.",
            "It's an important problem, so a fundamental question of importance is, given some interactions with the result list, what can we say about individual documents?",
            "So can we remove this bias and then the reverse question is if we know something about documents, what can we say with how we think about how we think users will interact with the list?",
            "So can we go from a list to individual documents and we go?",
            "Can we go from a document to a list?",
            "How can we understand?",
            "How users view results."
        ],
        [
            "So the approach here is to come up with some simple hypothesis for explaining the position bias, including one that we like called the Cascade Model A.",
            "The next thing we do is a large data gathering effort, so we gather sort of many observations from a search engine, and then we're just going to test which of the hypothesis best explains the position bias that we see."
        ],
        [
            "OK."
        ],
        [
            "So.",
            "The baseline is that there is no position bias.",
            "OK, so before I talk about what it is, I'm just going to say, you know why?",
            "Why even bother with this hypothesis?",
            "Well, we know that some of the explanation from clicks of how clicks happen is from relevance, or is from the perceived attractiveness of a search result.",
            "So we know that that's an important factor, right?",
            "So our baseline is just to say that that's the only factor.",
            "And maybe between ranks nine and ten were saying how likely are people to click on a document in rank 9 versus if it was in rank 10?",
            "Maybe that's mostly determined by the quality of the document, or the attractiveness of the of the results summary.",
            "Maybe there's very little rank bias between 9:00 and 10:00, so at least in some some positions there may be very little position bias effect, so it's a reasonable baseline to have, but we think it's not going to work near rank one.",
            "We know that rank one is special, that people are a lot more likely to click rank one.",
            "So just a bit of notation is that we're going to say CDI is the.",
            "The probability that we're going to click a result document D if it's in position I and and with kind of assuming there's this underlying thing ID, which is that Geno.",
            "That's just the attractiveness of the result.",
            "There's kind of a.",
            "Here we're not considering the relevance of documents as in relevance judgments or usefulness or any of this.",
            "We're just saying, how likely are we to see a click, and we assume this is associated with the perceived relevance or attractive."
        ],
        [
            "Server of a summary.",
            "OK, so another hypothesis is that user is sometimes just click on a rank without really looking at it, or they just take a take a guess that this is going to be relevant.",
            "So we say with some probability Lambda they we get a user who's going to actually look at the results and click based on relevance and with some probability 1 minus Lambda we're going to get a user who clicks just based on rank, so maybe there's some users who you know just like to click on rank one regardless of what's there.",
            "That's one hypothesis for why we see more clicks on rank one.",
            "This could also be called the the OR model in that clicks whenever we see a click, it either arises because they they perceive the document was going to be relevant or just based on their position.",
            "And we're going to share these B.",
            "We're going to have 10 B parameters, which is the chance of blindly clicking each of the 10 ranks and.",
            "This is shared across all.",
            "Can I get some water please?",
            "This is shared across.",
            "I got a cold on the plane on the way over.",
            "And this is shared across all queries, so we're going to have these shared parameters."
        ],
        [
            "Thanks, the third hypothesis is that is to do with examination.",
            "So what we're saying is that in order to observe a click, they have to both look at a result and decide that it was relevant.",
            "So the we've again in all of these.",
            "We've had this ID, which is how relevant the document was perceived to be.",
            "But in this case we're saying it has.",
            "That has to happen and the document has to be examined.",
            "So we might say people always look at rank one, maybe only half of them look at rank two and so on.",
            "And this explains the drop off we see in clicks over ranks.",
            "Uh.",
            "And in this model, the probability of looking at rank 2 doesn't depend on what's in rank one.",
            "OK, so it's it's.",
            "It's just a fixed examination gave across all queries regardless of what was in the other ranks."
        ],
        [
            "OK, and I forth the the the the kind of main one that we actually like in this study is the Cascade model.",
            "OK, so in this one the probability of looking at rank two does depend on what's in rank one, so I'll just go through how this works.",
            "The user is going to view results in order from top to bottom.",
            "So let's say there is a, so the the previous models, by the way didn't assume any.",
            "I've used 10 things here, but you could have any number of positions.",
            "You could have a grid of results.",
            "You could, as long as you can assign a position ID.",
            "There's no assumption made about what order they look at them in, we just have a probability of examining each of these positions, whereas here we are assuming that reversing the results in order and when they see a document, they'll click it.",
            "With probability ID and only if they don't click it, they will proceed to the next result so.",
            "Basically, in order to get down to the 5th result, you have to not click on one, not click on to not click on 3, not click on for an, then click on five.",
            "So the.",
            "The the basically I'll just give a quick example."
        ],
        [
            "My apologies for the voice.",
            "So imagine 500 users typed a particular query and none of them clicked on the top result, which will call 100 clicked on the 2nd result, which is B and hundred clicked on C in rank 3.",
            "The The Cascade model says the relevance of a is zero.",
            "We observed no clicks.",
            "The relevance of B is 20%.",
            "There's kind of a 20% chance that people will click B and there's a 25% chance that people will click see.",
            "So we're basically saying at at B we lost 100 people, and so the 100 clicks in rank three are worth more because we've lost some people by that stage.",
            "So that's the that's the.",
            "That's the model."
        ],
        [
            "OK."
        ],
        [
            "So how are we going to test this?",
            "We're going to do.",
            "We're going to flip adjacent ranks in in a top 10 list of a search engine.",
            "So that means we've got nine types of flips.",
            "We can flip one and two.",
            "We can flip two and three, and so on down to flipping 9 and 10.",
            "And then we're going to define an experiment as these as a group of four things, a query URL, a URL B and rank which will just call M. So the idea is that you know URLs A&B were originally in position one and two.",
            "It might have been originally AB or BA or we don't.",
            "We don't mind.",
            "We're not assuming that they always came first, but what we're saying is we're going to show, with equal probability, A B&B A.",
            "So all we're doing is varying the position in which we show the results, and we have 108 thousand such experiments, and you might wonder, well, you know, isn't it interesting to know how you know what would happen if we flip between one and three, or between one and 10 or something like that?",
            "Well, you know that kind of thing is quite dangerous when you're using real users, and in fact, flipping one and two is quite extreme given how important rank one is.",
            "So we.",
            "We're kind of limited in how much we did when we went through this UI at some.",
            "At some users, when we when we did these flips with some users, we only flipped adjacent ones.",
            "And notice that.",
            "We are using flips in our experiments, but none of our models rely on flips, so we're not proposing a system where the search engine will always be randomizing its results in order to learn about relevance.",
            "We just saying, you know, for our evaluation we are going to do some."
        ],
        [
            "Flips OK, so all of the plots that I'm going to show are in kind of lohgad space which is.",
            "The logo to the probability as it shows in the top right there is the log of the probability over 1 minus the probability.",
            "And the reason we do this is because there's lots of very low probabilities, so you can see what's going on better in log space.",
            "But this is the only place for using log odds.",
            "We're not using it in any of the any of the models.",
            "OK, so this is this is the full data set except we have not shown any of the.",
            "Times when there is zero clicks because that goes off the off the plot.",
            "Here we were just showing like so if you saw.",
            "10 clicks for.",
            "So what we're showing here is the chances of being clicked when you're in the lower position and what my position plus one versus the low guards are being clicked in position M for the same document and the black line in the middle is the mean.",
            "Of of it shows that basically you have slightly higher probability of being clicked in the upper position, 'cause it's above the diagonal and the the error bias there show the middle 50%.",
            "So it's basically showing that like 50% of the data lies within this kind of black to belong here above the diagonal.",
            "But there's also a lot of noise, so there's a lot of cases where it was below the diagonal, which means that we move something up up A notch and it actually got clicked less, and so on.",
            "So there's all kinds of strangeness in the data, but.",
            "But we're basically showing that you know the the.",
            "Majority of the data lies in this in this kind of black dark black sort of band."
        ],
        [
            "There.",
            "OK, so when we collected the data though.",
            "Covered something kind of surprising for us, which was that two of our hypothesis are kind of broken.",
            "So the one of the hypothesis was that if users that users will click blindly on rank one, but we see a number of cases where rank one has no clicks for a large number of observations.",
            "So if you put something bad enough in rank 1, users won't click it, so it's not the case that users click blindly.",
            "This is a key point.",
            "We're kind of surprised by this and also you know if you say well, only half the people look at position to.",
            "This explains why we get fewer clicks in position 2, but sometimes we observed like almost all the clicks happening in position 2.",
            "So it's also not the case that you know users will will will.",
            "You know that there's this fixed decay of examination over ranks so.",
            "If we tried to learn parameters of the model and stay within bounds the the so we tried to not say there's 150% chance of click or say that this negative 20% chance of click but tried to stay within bounds, we found that you know the blind click hypothesis was completely bound bound up and couldn't couldn't make any adjustment and the examination hypothesis also made less adjustment and near the top then it did lower down so that there are problems with this.",
            "We need some way of staying within bounds.",
            "Um?"
        ],
        [
            "So we're out of our nice hypothesis.",
            "Now.",
            "This is a logistic hypothesis, and what this is saying is notice that this black thing that we saw earlier looks a bit like a straight line and logod space.",
            "So you know what we want.",
            "I mean, we want to shift this thing away of shifting.",
            "It is to just take the log odds, add some weight and then remove the log odds.",
            "And now you have adjusted the probability.",
            "And this is related to logistic regression.",
            "So this is just you know.",
            "Something that we observe from the data rather than hypothesis.",
            "It's related I guess to the examination model also.",
            "But basically it's it's.",
            "It's a way of staying within bounds because when you when you transform when you're in law, God space, you're in the space of negative Infinity, positive Infinity.",
            "You transform back to a probability which is between zero and one, and you're guaranteed not to be out of."
        ],
        [
            "Once.",
            "OK, So what are we going to measure?",
            "We're going to measure the.",
            "Cross entropy what we're going to do is we're going to say, given how many clicks we observed when it was a above B now going to predict how many, what's the probability of click the probability of different events in the in the in the order BBA.",
            "Once we do a flip.",
            "And there's actually 4 events that you'll click document B in order to be a.",
            "You'll click, you'll click a.",
            "You'll click both, so you'll click neither, so we're going to use this cross entropy error.",
            "Lower is better.",
            "And we're going to use 1010 fold cross validation across our our old 100 and 8000 observations we had."
        ],
        [
            "OK, so here the main."
        ],
        [
            "Results we we have something here called best possible, which is actually what would happen if we cheated and we knew the counts.",
            "So if we knew what happens in BA precisely what would be you know and use that for our prediction?",
            "We get an error of .141.",
            "And none of our methods can get anywhere near that sort of cheating.",
            "You know performance, but the best performance was from the Cascade Model and the baseline, which you know beats the baseline quite well.",
            "The person minus is 2 standard deviations.",
            "Across the different folds.",
            "So we're seeing that the Cascade Model works best.",
            "The logistic model, you know it looks better than the baseline."
        ],
        [
            "And here's a breakdown by rank of the same numbers.",
            "So we're seeing.",
            "My apologies, so we're seeing the.",
            "What we've done is normalized the cross entropy error here just to make it easier to view.",
            "So zero is the best possible system where we cheated and one is where we make no adjustment at all and we can see that particularly in ranks one and two.",
            "The Cascade model is doing a good job of getting pretty close to the best possible formance at rank one, for example, compared to the other approaches, but.",
            "It's in fact doing worse than no adjustment down down lower.",
            "So this is it seems like the Cascade model is a particularly good explanation for presentation bias.",
            "When you're flipping ranks one and two or two and three.",
            "And.",
            "And none of them are particularly good at lower ranks.",
            "It seems like the baseline of May of just saying it's purely explained by relevance of the of.",
            "The result seems to be the best explanation that we have lowered."
        ],
        [
            "Down the list and the reason this might be occurring is well, so here's the same scatter plot I showed earlier, but broken down by rank as well.",
            "So what we're saying is, you know our best guess.",
            "In this plot here, this is a flip M = 9, So we're flipping ranks nine and 10, and this is in the lower position.",
            "This is in the upper position we're actually seeing.",
            "It's pretty much along the diagonal.",
            "The line that we see, and it's pretty tight here as well.",
            "So it seems like you know our best guess of how likely something is going to is to be clicked.",
            "Remove it from 8:50.",
            "Is this well, it will be clicked the same amount that it was before, whereas way up in the top left there when we flip from 2 to one, there's going to be quite a change."
        ],
        [
            "OK. D and this is just showing.",
            "In in the.",
            "The story on the It's not well labeled here on the left is is the baseline where we make no adjustment.",
            "But now we've broken it down into shifting up and shifting down again.",
            "These bars are showing the middle 50% of the data and on the right we are seeing the Cascade model.",
            "So we're kind of seeing that we're getting both closer to that to the diagonal.",
            "And also we've got kind of tighter, slightly tighter error bars in some cases.",
            "But it's still.",
            "It's still not, you know, right on the diagonal with very narrow bias."
        ],
        [
            "OK, so I guess the main take home thing is that we have these hypothesis that users click at random on rank one 'cause they just don't even look at the results and this is just not supported by the data.",
            "And we had this other hypothesis that you know, maybe you know you can guarantee that users, not many, not as many users are going to look at ranks two and three and so on, as look at one.",
            "And this is also not true 'cause you might see lots of clicks on rank two, so it doesn't seem that these simple explanations that we had are actually very good explanations of the data we collected.",
            "The.",
            "The good explanation was the Cascade model that says you know how many people look at rank two depends on what's in rank one and how many people look at rank three depends on what was in ranks one and two, and so on.",
            "So.",
            "That's pretty much it the so, except to note that the Cascade model is very basic, so all it's doing is saying users will keep going down the ranks until they you know until they hit something that's relevant.",
            "We don't let them abandon, for example, an click.",
            "Nothing under this model, which is kind of strange.",
            "We we since we're only considering two documents, we didn't have to worry about that here.",
            "We just said all.",
            "We assume they look like something below, but in reality we should let them abandon their search if they're not finding anything good.",
            "Also, if they click something, they might come back and click something else under our model uses always click at most users can click at most once.",
            "Once they're gone, they never come back.",
            "So so I think there's there's.",
            "There's more to be done there, but.",
            "But overall, out of out of those, we considered the the Cascade model worked best, thank you."
        ],
        [
            "I was wondering if you've looked at the distribution of.",
            "I'm sure you must have looked at the distribution of clicks just raw without worrying about anything else and.",
            "My intuition and I could could be wrong is that the cascade and probably several of the other models would predict an exponential.",
            "Distribution, Ryan, this would be one way to validate the models 'cause you could also just simulate your model and or perhaps even analytically look at the distribution that it predicts.",
            "I wonder if that would right say anything interesting.",
            "We haven't done that with these models when I started out the work I started that was my starting point, but I haven't.",
            "I haven't revisited that that say thanks.",
            "I'll take a look at that.",
            "Did you consider clicks on ads at the North position?",
            "OK, so in our so.",
            "Because we were so we applied this to a real search engine and in that real search engine there's ads.",
            "There's also things that get put above rank one.",
            "There's also such as you know, the phone book or some other thing, and what we did was we in these experiments we actually ignore it.",
            "All those cases we just looked at cases where a list was displayed without extra things, and so I think that sorry, I probably should have mentioned that when I was describing the data collection.",
            "The reason we did that is 'cause we didn't want to have to model the ads and so on, but that probably biases the data set and in some way we have a large amount of data, but it's only for cases where there were no ads.",
            "Concerning a model about users not looking at something clicks and you say this is a broken model.",
            "Like a lot of user queries get just second clicks or something.",
            "I was wondering perhaps you just hit the wrong waiting, like perhaps nobody looks after some higher number like nobody looks on 7 eight at 9th and 10th, but everybody looks on the 1st fifth.",
            "Well we gave the parameters to full.",
            "Possible range I guess.",
            "The point is that if you assume that and so.",
            "I guess the point is you can always see no clicks in any of our positions, so the idea that some users click at random just didn't seem to fit with the data.",
            "And similarly, there's there's some cases where you see tons of clicks on something that's not rank one, which suggested last.",
            "So yeah, we can talk more about that.",
            "You said there were 108 thousand experiments.",
            "How many distinct queries were involved?",
            "I'm not sure, but I guess at at most 10 per ten.",
            "I mean, nine peccary.",
            "Sorry, because there were nine.",
            "None of that could be even more than that, because it's URLs as well.",
            "I don't know.",
            "Well, I assume that it's.",
            "On I assume that a very very head queries that are very popular would appear a lot of times because you could have you know.",
            "An experiment ranked one and two with these two URLs and then an experiment ranks what I do with those two.",
            "You derive the odds of clicking.",
            "You need some statistics on each query, right?",
            "So and total was one 100,000.",
            "So how many?",
            "That's what I'm saying is I don't know.",
            "I'm saying that the forhead queries the same that they could appear in a lot of experiments and for tail queries you would expect them to appear in very few experiments.",
            "Work which is given from the actual stream or they were pre selected.",
            "Just picked at random from so so some portion of users had these flips out into their right, thanks.",
            "Thank you, I was wondering if you tried to switch one and four and.",
            "Yeah, we we.",
            "We avoided that because we were worried about messing too much with our users.",
            "But don't you think you will get more conclusive results if you do that kind of experience?",
            "Yes, but we just we can't.",
            "Any other questions?",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this isn't a learning to rank paper, this is just a study of.",
                    "label": 0
                },
                {
                    "sent": "Of how users interact with the search results list.",
                    "label": 0
                },
                {
                    "sent": "And apologies to Bill Ramsey, I couldn't find a photo of him online at the last minute, but I know and Michael here today.",
                    "label": 1
                },
                {
                    "sent": "By the way, we just heard from Michael.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so position bias.",
                    "label": 0
                },
                {
                    "sent": "This is just the top ranked results in, let's say a rank top.",
                    "label": 0
                },
                {
                    "sent": "Ten lists tend to get more clicks, so the question here is why is this?",
                    "label": 1
                },
                {
                    "sent": "Is it because some users don't even look at the results and just click on rank one?",
                    "label": 1
                },
                {
                    "sent": "For example, is it that people look less at rank 10, so therefore they're less likely to click it?",
                    "label": 0
                },
                {
                    "sent": "Is it that users go through in order and click the first good thing they see?",
                    "label": 1
                },
                {
                    "sent": "Like what's a good explanation for this bias that we see?",
                    "label": 0
                },
                {
                    "sent": "I think this is, you know, not claiming to have solved this problem.",
                    "label": 0
                },
                {
                    "sent": "I still think it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's an important problem, so a fundamental question of importance is, given some interactions with the result list, what can we say about individual documents?",
                    "label": 1
                },
                {
                    "sent": "So can we remove this bias and then the reverse question is if we know something about documents, what can we say with how we think about how we think users will interact with the list?",
                    "label": 0
                },
                {
                    "sent": "So can we go from a list to individual documents and we go?",
                    "label": 0
                },
                {
                    "sent": "Can we go from a document to a list?",
                    "label": 0
                },
                {
                    "sent": "How can we understand?",
                    "label": 0
                },
                {
                    "sent": "How users view results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the approach here is to come up with some simple hypothesis for explaining the position bias, including one that we like called the Cascade Model A.",
                    "label": 0
                },
                {
                    "sent": "The next thing we do is a large data gathering effort, so we gather sort of many observations from a search engine, and then we're just going to test which of the hypothesis best explains the position bias that we see.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The baseline is that there is no position bias.",
                    "label": 0
                },
                {
                    "sent": "OK, so before I talk about what it is, I'm just going to say, you know why?",
                    "label": 0
                },
                {
                    "sent": "Why even bother with this hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Well, we know that some of the explanation from clicks of how clicks happen is from relevance, or is from the perceived attractiveness of a search result.",
                    "label": 1
                },
                {
                    "sent": "So we know that that's an important factor, right?",
                    "label": 1
                },
                {
                    "sent": "So our baseline is just to say that that's the only factor.",
                    "label": 0
                },
                {
                    "sent": "And maybe between ranks nine and ten were saying how likely are people to click on a document in rank 9 versus if it was in rank 10?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's mostly determined by the quality of the document, or the attractiveness of the of the results summary.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's very little rank bias between 9:00 and 10:00, so at least in some some positions there may be very little position bias effect, so it's a reasonable baseline to have, but we think it's not going to work near rank one.",
                    "label": 1
                },
                {
                    "sent": "We know that rank one is special, that people are a lot more likely to click rank one.",
                    "label": 0
                },
                {
                    "sent": "So just a bit of notation is that we're going to say CDI is the.",
                    "label": 0
                },
                {
                    "sent": "The probability that we're going to click a result document D if it's in position I and and with kind of assuming there's this underlying thing ID, which is that Geno.",
                    "label": 0
                },
                {
                    "sent": "That's just the attractiveness of the result.",
                    "label": 0
                },
                {
                    "sent": "There's kind of a.",
                    "label": 0
                },
                {
                    "sent": "Here we're not considering the relevance of documents as in relevance judgments or usefulness or any of this.",
                    "label": 0
                },
                {
                    "sent": "We're just saying, how likely are we to see a click, and we assume this is associated with the perceived relevance or attractive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Server of a summary.",
                    "label": 0
                },
                {
                    "sent": "OK, so another hypothesis is that user is sometimes just click on a rank without really looking at it, or they just take a take a guess that this is going to be relevant.",
                    "label": 0
                },
                {
                    "sent": "So we say with some probability Lambda they we get a user who's going to actually look at the results and click based on relevance and with some probability 1 minus Lambda we're going to get a user who clicks just based on rank, so maybe there's some users who you know just like to click on rank one regardless of what's there.",
                    "label": 1
                },
                {
                    "sent": "That's one hypothesis for why we see more clicks on rank one.",
                    "label": 1
                },
                {
                    "sent": "This could also be called the the OR model in that clicks whenever we see a click, it either arises because they they perceive the document was going to be relevant or just based on their position.",
                    "label": 0
                },
                {
                    "sent": "And we're going to share these B.",
                    "label": 0
                },
                {
                    "sent": "We're going to have 10 B parameters, which is the chance of blindly clicking each of the 10 ranks and.",
                    "label": 0
                },
                {
                    "sent": "This is shared across all.",
                    "label": 0
                },
                {
                    "sent": "Can I get some water please?",
                    "label": 0
                },
                {
                    "sent": "This is shared across.",
                    "label": 0
                },
                {
                    "sent": "I got a cold on the plane on the way over.",
                    "label": 0
                },
                {
                    "sent": "And this is shared across all queries, so we're going to have these shared parameters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, the third hypothesis is that is to do with examination.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is that in order to observe a click, they have to both look at a result and decide that it was relevant.",
                    "label": 0
                },
                {
                    "sent": "So the we've again in all of these.",
                    "label": 0
                },
                {
                    "sent": "We've had this ID, which is how relevant the document was perceived to be.",
                    "label": 0
                },
                {
                    "sent": "But in this case we're saying it has.",
                    "label": 0
                },
                {
                    "sent": "That has to happen and the document has to be examined.",
                    "label": 0
                },
                {
                    "sent": "So we might say people always look at rank one, maybe only half of them look at rank two and so on.",
                    "label": 1
                },
                {
                    "sent": "And this explains the drop off we see in clicks over ranks.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "And in this model, the probability of looking at rank 2 doesn't depend on what's in rank one.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's it's.",
                    "label": 1
                },
                {
                    "sent": "It's just a fixed examination gave across all queries regardless of what was in the other ranks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and I forth the the the the kind of main one that we actually like in this study is the Cascade model.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this one the probability of looking at rank two does depend on what's in rank one, so I'll just go through how this works.",
                    "label": 0
                },
                {
                    "sent": "The user is going to view results in order from top to bottom.",
                    "label": 0
                },
                {
                    "sent": "So let's say there is a, so the the previous models, by the way didn't assume any.",
                    "label": 0
                },
                {
                    "sent": "I've used 10 things here, but you could have any number of positions.",
                    "label": 0
                },
                {
                    "sent": "You could have a grid of results.",
                    "label": 0
                },
                {
                    "sent": "You could, as long as you can assign a position ID.",
                    "label": 0
                },
                {
                    "sent": "There's no assumption made about what order they look at them in, we just have a probability of examining each of these positions, whereas here we are assuming that reversing the results in order and when they see a document, they'll click it.",
                    "label": 1
                },
                {
                    "sent": "With probability ID and only if they don't click it, they will proceed to the next result so.",
                    "label": 0
                },
                {
                    "sent": "Basically, in order to get down to the 5th result, you have to not click on one, not click on to not click on 3, not click on for an, then click on five.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The the basically I'll just give a quick example.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My apologies for the voice.",
                    "label": 0
                },
                {
                    "sent": "So imagine 500 users typed a particular query and none of them clicked on the top result, which will call 100 clicked on the 2nd result, which is B and hundred clicked on C in rank 3.",
                    "label": 1
                },
                {
                    "sent": "The The Cascade model says the relevance of a is zero.",
                    "label": 0
                },
                {
                    "sent": "We observed no clicks.",
                    "label": 0
                },
                {
                    "sent": "The relevance of B is 20%.",
                    "label": 0
                },
                {
                    "sent": "There's kind of a 20% chance that people will click B and there's a 25% chance that people will click see.",
                    "label": 0
                },
                {
                    "sent": "So we're basically saying at at B we lost 100 people, and so the 100 clicks in rank three are worth more because we've lost some people by that stage.",
                    "label": 0
                },
                {
                    "sent": "So that's the that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how are we going to test this?",
                    "label": 0
                },
                {
                    "sent": "We're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to flip adjacent ranks in in a top 10 list of a search engine.",
                    "label": 0
                },
                {
                    "sent": "So that means we've got nine types of flips.",
                    "label": 0
                },
                {
                    "sent": "We can flip one and two.",
                    "label": 0
                },
                {
                    "sent": "We can flip two and three, and so on down to flipping 9 and 10.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to define an experiment as these as a group of four things, a query URL, a URL B and rank which will just call M. So the idea is that you know URLs A&B were originally in position one and two.",
                    "label": 1
                },
                {
                    "sent": "It might have been originally AB or BA or we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't mind.",
                    "label": 0
                },
                {
                    "sent": "We're not assuming that they always came first, but what we're saying is we're going to show, with equal probability, A B&B A.",
                    "label": 0
                },
                {
                    "sent": "So all we're doing is varying the position in which we show the results, and we have 108 thousand such experiments, and you might wonder, well, you know, isn't it interesting to know how you know what would happen if we flip between one and three, or between one and 10 or something like that?",
                    "label": 0
                },
                {
                    "sent": "Well, you know that kind of thing is quite dangerous when you're using real users, and in fact, flipping one and two is quite extreme given how important rank one is.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We're kind of limited in how much we did when we went through this UI at some.",
                    "label": 0
                },
                {
                    "sent": "At some users, when we when we did these flips with some users, we only flipped adjacent ones.",
                    "label": 0
                },
                {
                    "sent": "And notice that.",
                    "label": 0
                },
                {
                    "sent": "We are using flips in our experiments, but none of our models rely on flips, so we're not proposing a system where the search engine will always be randomizing its results in order to learn about relevance.",
                    "label": 0
                },
                {
                    "sent": "We just saying, you know, for our evaluation we are going to do some.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flips OK, so all of the plots that I'm going to show are in kind of lohgad space which is.",
                    "label": 0
                },
                {
                    "sent": "The logo to the probability as it shows in the top right there is the log of the probability over 1 minus the probability.",
                    "label": 0
                },
                {
                    "sent": "And the reason we do this is because there's lots of very low probabilities, so you can see what's going on better in log space.",
                    "label": 0
                },
                {
                    "sent": "But this is the only place for using log odds.",
                    "label": 0
                },
                {
                    "sent": "We're not using it in any of the any of the models.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the full data set except we have not shown any of the.",
                    "label": 0
                },
                {
                    "sent": "Times when there is zero clicks because that goes off the off the plot.",
                    "label": 0
                },
                {
                    "sent": "Here we were just showing like so if you saw.",
                    "label": 0
                },
                {
                    "sent": "10 clicks for.",
                    "label": 0
                },
                {
                    "sent": "So what we're showing here is the chances of being clicked when you're in the lower position and what my position plus one versus the low guards are being clicked in position M for the same document and the black line in the middle is the mean.",
                    "label": 0
                },
                {
                    "sent": "Of of it shows that basically you have slightly higher probability of being clicked in the upper position, 'cause it's above the diagonal and the the error bias there show the middle 50%.",
                    "label": 0
                },
                {
                    "sent": "So it's basically showing that like 50% of the data lies within this kind of black to belong here above the diagonal.",
                    "label": 0
                },
                {
                    "sent": "But there's also a lot of noise, so there's a lot of cases where it was below the diagonal, which means that we move something up up A notch and it actually got clicked less, and so on.",
                    "label": 0
                },
                {
                    "sent": "So there's all kinds of strangeness in the data, but.",
                    "label": 0
                },
                {
                    "sent": "But we're basically showing that you know the the.",
                    "label": 0
                },
                {
                    "sent": "Majority of the data lies in this in this kind of black dark black sort of band.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we collected the data though.",
                    "label": 0
                },
                {
                    "sent": "Covered something kind of surprising for us, which was that two of our hypothesis are kind of broken.",
                    "label": 0
                },
                {
                    "sent": "So the one of the hypothesis was that if users that users will click blindly on rank one, but we see a number of cases where rank one has no clicks for a large number of observations.",
                    "label": 0
                },
                {
                    "sent": "So if you put something bad enough in rank 1, users won't click it, so it's not the case that users click blindly.",
                    "label": 1
                },
                {
                    "sent": "This is a key point.",
                    "label": 0
                },
                {
                    "sent": "We're kind of surprised by this and also you know if you say well, only half the people look at position to.",
                    "label": 0
                },
                {
                    "sent": "This explains why we get fewer clicks in position 2, but sometimes we observed like almost all the clicks happening in position 2.",
                    "label": 0
                },
                {
                    "sent": "So it's also not the case that you know users will will will.",
                    "label": 0
                },
                {
                    "sent": "You know that there's this fixed decay of examination over ranks so.",
                    "label": 1
                },
                {
                    "sent": "If we tried to learn parameters of the model and stay within bounds the the so we tried to not say there's 150% chance of click or say that this negative 20% chance of click but tried to stay within bounds, we found that you know the blind click hypothesis was completely bound bound up and couldn't couldn't make any adjustment and the examination hypothesis also made less adjustment and near the top then it did lower down so that there are problems with this.",
                    "label": 1
                },
                {
                    "sent": "We need some way of staying within bounds.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're out of our nice hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is a logistic hypothesis, and what this is saying is notice that this black thing that we saw earlier looks a bit like a straight line and logod space.",
                    "label": 0
                },
                {
                    "sent": "So you know what we want.",
                    "label": 0
                },
                {
                    "sent": "I mean, we want to shift this thing away of shifting.",
                    "label": 0
                },
                {
                    "sent": "It is to just take the log odds, add some weight and then remove the log odds.",
                    "label": 0
                },
                {
                    "sent": "And now you have adjusted the probability.",
                    "label": 0
                },
                {
                    "sent": "And this is related to logistic regression.",
                    "label": 1
                },
                {
                    "sent": "So this is just you know.",
                    "label": 0
                },
                {
                    "sent": "Something that we observe from the data rather than hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It's related I guess to the examination model also.",
                    "label": 0
                },
                {
                    "sent": "But basically it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's a way of staying within bounds because when you when you transform when you're in law, God space, you're in the space of negative Infinity, positive Infinity.",
                    "label": 0
                },
                {
                    "sent": "You transform back to a probability which is between zero and one, and you're guaranteed not to be out of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are we going to measure?",
                    "label": 0
                },
                {
                    "sent": "We're going to measure the.",
                    "label": 0
                },
                {
                    "sent": "Cross entropy what we're going to do is we're going to say, given how many clicks we observed when it was a above B now going to predict how many, what's the probability of click the probability of different events in the in the in the order BBA.",
                    "label": 0
                },
                {
                    "sent": "Once we do a flip.",
                    "label": 0
                },
                {
                    "sent": "And there's actually 4 events that you'll click document B in order to be a.",
                    "label": 1
                },
                {
                    "sent": "You'll click, you'll click a.",
                    "label": 1
                },
                {
                    "sent": "You'll click both, so you'll click neither, so we're going to use this cross entropy error.",
                    "label": 0
                },
                {
                    "sent": "Lower is better.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use 1010 fold cross validation across our our old 100 and 8000 observations we had.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here the main.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results we we have something here called best possible, which is actually what would happen if we cheated and we knew the counts.",
                    "label": 1
                },
                {
                    "sent": "So if we knew what happens in BA precisely what would be you know and use that for our prediction?",
                    "label": 0
                },
                {
                    "sent": "We get an error of .141.",
                    "label": 0
                },
                {
                    "sent": "And none of our methods can get anywhere near that sort of cheating.",
                    "label": 0
                },
                {
                    "sent": "You know performance, but the best performance was from the Cascade Model and the baseline, which you know beats the baseline quite well.",
                    "label": 0
                },
                {
                    "sent": "The person minus is 2 standard deviations.",
                    "label": 0
                },
                {
                    "sent": "Across the different folds.",
                    "label": 0
                },
                {
                    "sent": "So we're seeing that the Cascade Model works best.",
                    "label": 0
                },
                {
                    "sent": "The logistic model, you know it looks better than the baseline.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a breakdown by rank of the same numbers.",
                    "label": 0
                },
                {
                    "sent": "So we're seeing.",
                    "label": 0
                },
                {
                    "sent": "My apologies, so we're seeing the.",
                    "label": 0
                },
                {
                    "sent": "What we've done is normalized the cross entropy error here just to make it easier to view.",
                    "label": 0
                },
                {
                    "sent": "So zero is the best possible system where we cheated and one is where we make no adjustment at all and we can see that particularly in ranks one and two.",
                    "label": 0
                },
                {
                    "sent": "The Cascade model is doing a good job of getting pretty close to the best possible formance at rank one, for example, compared to the other approaches, but.",
                    "label": 0
                },
                {
                    "sent": "It's in fact doing worse than no adjustment down down lower.",
                    "label": 0
                },
                {
                    "sent": "So this is it seems like the Cascade model is a particularly good explanation for presentation bias.",
                    "label": 0
                },
                {
                    "sent": "When you're flipping ranks one and two or two and three.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And none of them are particularly good at lower ranks.",
                    "label": 0
                },
                {
                    "sent": "It seems like the baseline of May of just saying it's purely explained by relevance of the of.",
                    "label": 0
                },
                {
                    "sent": "The result seems to be the best explanation that we have lowered.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down the list and the reason this might be occurring is well, so here's the same scatter plot I showed earlier, but broken down by rank as well.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is, you know our best guess.",
                    "label": 0
                },
                {
                    "sent": "In this plot here, this is a flip M = 9, So we're flipping ranks nine and 10, and this is in the lower position.",
                    "label": 0
                },
                {
                    "sent": "This is in the upper position we're actually seeing.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "The line that we see, and it's pretty tight here as well.",
                    "label": 0
                },
                {
                    "sent": "So it seems like you know our best guess of how likely something is going to is to be clicked.",
                    "label": 0
                },
                {
                    "sent": "Remove it from 8:50.",
                    "label": 0
                },
                {
                    "sent": "Is this well, it will be clicked the same amount that it was before, whereas way up in the top left there when we flip from 2 to one, there's going to be quite a change.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. D and this is just showing.",
                    "label": 0
                },
                {
                    "sent": "In in the.",
                    "label": 0
                },
                {
                    "sent": "The story on the It's not well labeled here on the left is is the baseline where we make no adjustment.",
                    "label": 0
                },
                {
                    "sent": "But now we've broken it down into shifting up and shifting down again.",
                    "label": 0
                },
                {
                    "sent": "These bars are showing the middle 50% of the data and on the right we are seeing the Cascade model.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of seeing that we're getting both closer to that to the diagonal.",
                    "label": 1
                },
                {
                    "sent": "And also we've got kind of tighter, slightly tighter error bars in some cases.",
                    "label": 0
                },
                {
                    "sent": "But it's still.",
                    "label": 0
                },
                {
                    "sent": "It's still not, you know, right on the diagonal with very narrow bias.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I guess the main take home thing is that we have these hypothesis that users click at random on rank one 'cause they just don't even look at the results and this is just not supported by the data.",
                    "label": 0
                },
                {
                    "sent": "And we had this other hypothesis that you know, maybe you know you can guarantee that users, not many, not as many users are going to look at ranks two and three and so on, as look at one.",
                    "label": 0
                },
                {
                    "sent": "And this is also not true 'cause you might see lots of clicks on rank two, so it doesn't seem that these simple explanations that we had are actually very good explanations of the data we collected.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The good explanation was the Cascade model that says you know how many people look at rank two depends on what's in rank one and how many people look at rank three depends on what was in ranks one and two, and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much it the so, except to note that the Cascade model is very basic, so all it's doing is saying users will keep going down the ranks until they you know until they hit something that's relevant.",
                    "label": 1
                },
                {
                    "sent": "We don't let them abandon, for example, an click.",
                    "label": 0
                },
                {
                    "sent": "Nothing under this model, which is kind of strange.",
                    "label": 0
                },
                {
                    "sent": "We we since we're only considering two documents, we didn't have to worry about that here.",
                    "label": 0
                },
                {
                    "sent": "We just said all.",
                    "label": 0
                },
                {
                    "sent": "We assume they look like something below, but in reality we should let them abandon their search if they're not finding anything good.",
                    "label": 1
                },
                {
                    "sent": "Also, if they click something, they might come back and click something else under our model uses always click at most users can click at most once.",
                    "label": 0
                },
                {
                    "sent": "Once they're gone, they never come back.",
                    "label": 0
                },
                {
                    "sent": "So so I think there's there's.",
                    "label": 0
                },
                {
                    "sent": "There's more to be done there, but.",
                    "label": 0
                },
                {
                    "sent": "But overall, out of out of those, we considered the the Cascade model worked best, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was wondering if you've looked at the distribution of.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you must have looked at the distribution of clicks just raw without worrying about anything else and.",
                    "label": 0
                },
                {
                    "sent": "My intuition and I could could be wrong is that the cascade and probably several of the other models would predict an exponential.",
                    "label": 0
                },
                {
                    "sent": "Distribution, Ryan, this would be one way to validate the models 'cause you could also just simulate your model and or perhaps even analytically look at the distribution that it predicts.",
                    "label": 0
                },
                {
                    "sent": "I wonder if that would right say anything interesting.",
                    "label": 0
                },
                {
                    "sent": "We haven't done that with these models when I started out the work I started that was my starting point, but I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't revisited that that say thanks.",
                    "label": 0
                },
                {
                    "sent": "I'll take a look at that.",
                    "label": 0
                },
                {
                    "sent": "Did you consider clicks on ads at the North position?",
                    "label": 0
                },
                {
                    "sent": "OK, so in our so.",
                    "label": 0
                },
                {
                    "sent": "Because we were so we applied this to a real search engine and in that real search engine there's ads.",
                    "label": 0
                },
                {
                    "sent": "There's also things that get put above rank one.",
                    "label": 0
                },
                {
                    "sent": "There's also such as you know, the phone book or some other thing, and what we did was we in these experiments we actually ignore it.",
                    "label": 0
                },
                {
                    "sent": "All those cases we just looked at cases where a list was displayed without extra things, and so I think that sorry, I probably should have mentioned that when I was describing the data collection.",
                    "label": 0
                },
                {
                    "sent": "The reason we did that is 'cause we didn't want to have to model the ads and so on, but that probably biases the data set and in some way we have a large amount of data, but it's only for cases where there were no ads.",
                    "label": 0
                },
                {
                    "sent": "Concerning a model about users not looking at something clicks and you say this is a broken model.",
                    "label": 0
                },
                {
                    "sent": "Like a lot of user queries get just second clicks or something.",
                    "label": 0
                },
                {
                    "sent": "I was wondering perhaps you just hit the wrong waiting, like perhaps nobody looks after some higher number like nobody looks on 7 eight at 9th and 10th, but everybody looks on the 1st fifth.",
                    "label": 0
                },
                {
                    "sent": "Well we gave the parameters to full.",
                    "label": 0
                },
                {
                    "sent": "Possible range I guess.",
                    "label": 0
                },
                {
                    "sent": "The point is that if you assume that and so.",
                    "label": 0
                },
                {
                    "sent": "I guess the point is you can always see no clicks in any of our positions, so the idea that some users click at random just didn't seem to fit with the data.",
                    "label": 0
                },
                {
                    "sent": "And similarly, there's there's some cases where you see tons of clicks on something that's not rank one, which suggested last.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we can talk more about that.",
                    "label": 0
                },
                {
                    "sent": "You said there were 108 thousand experiments.",
                    "label": 0
                },
                {
                    "sent": "How many distinct queries were involved?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, but I guess at at most 10 per ten.",
                    "label": 0
                },
                {
                    "sent": "I mean, nine peccary.",
                    "label": 0
                },
                {
                    "sent": "Sorry, because there were nine.",
                    "label": 0
                },
                {
                    "sent": "None of that could be even more than that, because it's URLs as well.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Well, I assume that it's.",
                    "label": 0
                },
                {
                    "sent": "On I assume that a very very head queries that are very popular would appear a lot of times because you could have you know.",
                    "label": 0
                },
                {
                    "sent": "An experiment ranked one and two with these two URLs and then an experiment ranks what I do with those two.",
                    "label": 0
                },
                {
                    "sent": "You derive the odds of clicking.",
                    "label": 0
                },
                {
                    "sent": "You need some statistics on each query, right?",
                    "label": 0
                },
                {
                    "sent": "So and total was one 100,000.",
                    "label": 0
                },
                {
                    "sent": "So how many?",
                    "label": 0
                },
                {
                    "sent": "That's what I'm saying is I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm saying that the forhead queries the same that they could appear in a lot of experiments and for tail queries you would expect them to appear in very few experiments.",
                    "label": 0
                },
                {
                    "sent": "Work which is given from the actual stream or they were pre selected.",
                    "label": 0
                },
                {
                    "sent": "Just picked at random from so so some portion of users had these flips out into their right, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I was wondering if you tried to switch one and four and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we.",
                    "label": 0
                },
                {
                    "sent": "We avoided that because we were worried about messing too much with our users.",
                    "label": 0
                },
                {
                    "sent": "But don't you think you will get more conclusive results if you do that kind of experience?",
                    "label": 0
                },
                {
                    "sent": "Yes, but we just we can't.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}