{
    "id": "vyeflqhvrdckf2yfwrha47vm64hh7bo3",
    "title": "Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics",
    "info": {
        "author": [
            "Achim Rettinger, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_rettinger_word_semantic/",
    "segmentation": [
        [
            "This is joint work with Stephen Toma and far beyond bored, so I might not be a responsible for everything that's bad in this paper, but for the general idea, you can come to me and complain about it.",
            "Yeah, so we started."
        ],
        [
            "This research with this question, so what's actually captured in knowledge graph embeddings that are learned from knowledge graphs, and for those of you who are not that familiar, I guess it's not that many but very brief introduction to this knowledge graph.",
            "Embeddings area is actually some in my opinion.",
            "The best example of their Semantic web task and semantic web data has made it to the machine learning community, and there have been in the machine learning community developing for the last years a lot of different.",
            "Approaches to embedding knowledge graphs.",
            "Here just four classes, and of course a lot lot of different approaches that have been proposed.",
            "I'm only talk about tranzit because it's kind of the simplest approach and the most used so far and.",
            "How this proceeds is they basically look at some sort of triple right?",
            "In this case, let's say it's baseball and baseball is of category bat and ball game, and what they tried to do is replace it with a vector representation of this, so they try to replace the subject predicate and the object with with the vector low dimensional dense vector and for that they need to define a scoring function that scores how likely this vector is."
        ],
        [
            "And in the case of tranzit, what they do is they basically say if you have a known triple you should rank this higher as artificially corrupted triple.",
            "And in the case of trends E, that's I guess the most simple as scoring function that you could imagine.",
            "You just say in vector space if you add the predicate to the subject, you should end up at the representation of the object.",
            "Right, so that's very brief."
        ],
        [
            "What's captured, So what they do capture is the relational structure they capture kind of the network structure itself.",
            "And we ask ourself, is there may be other types of context context that you want to leverage in your embedding in your representation of your context?",
            "And if you would."
        ],
        [
            "About it as a human.",
            "If you think about the baseball, it's probably not the first thing that comes to your mind, right?",
            "This is the stuff that you find in the Knowledge Graph.",
            "It's of category Patton Pool game, and there's a union called World Baseball Softball Confederation, right?",
            "That's what you might think, and this is now maybe a little bit of extreme.",
            "But what I want to stress here is that it's really a really different type of context that you're thinking about is, for example.",
            "Oh, it's kind of visually similar to Apple or something like this, so it has the same size it's.",
            "It has the same shape and so on.",
            "So it's very different, and this is some sort of information that you can't get out of a knowledge graph, and if you continue this thinking, So what do you think about in apples?",
            "Or maybe you associate an Apple also more with Cinnamon Becausw.",
            "They're more recipes with that.",
            "Combine Apple cinnamon compared to potatoes.",
            "So it's really really a very different type of information that.",
            "USA human combined.",
            "I'm not saying that this is like the best example, I'm just saying it's really a different type of information that you're looking at.",
            "So then the question is, how do we kind of get this information?",
            "And obviously there is data out there so."
        ],
        [
            "If you look at the visual modality right at images, you exactly get this information, so you get this kind of standard common sense.",
            "Visual information like shape, color, background of baseball, and if you look at the detection modality, what what's the most interesting part?",
            "There is kind of this core currents of entities, so in language, what could occur Co occurs together which entities are mentioned together often, and this is also something that you don't.",
            "Get in in the Knowledge graph, at least not you don't get the frequency right.",
            "How often is the entity mentioned?",
            "In the real world, OK?"
        ],
        [
            "So.",
            "Yes, the context from the visual modality of natural language is there.",
            "So how do we collect this kind of information in an encoding that is compatible?",
            "And I guess you can assume."
        ],
        [
            "And what the answer to this is right, we go with embeddings right?",
            "In all of those different research areas like computer vision and NLP.",
            "They have been embedding techniques created in the last years.",
            "If more extensively than in the Knowledge Graph community."
        ],
        [
            "So just two examples that we used here that."
        ],
        [
            "The basic approach is so convolutional neural network we used here that kind of learns a lot of layers with increasing abstraction of the visual features, and we took the second to last layer and use this as the vector representation of the objects.",
            "If you average this over a big.",
            "Image classification task like image net.",
            "And."
        ],
        [
            "Same you can do with the birds, I don't.",
            "I'm not going into details.",
            "You have seen those types of slides during the conference alot so."
        ],
        [
            "What we get is we get vectors here for visual features, burden packings and knowledge graph entity embeddings, and in this case we use those methods.",
            "You can plug in whatever other methods you want right there.",
            "Much more much better embedding methods than those are."
        ],
        [
            "So now what to do with this multiple embeddings, right?",
            "The next question is how do we allow in them?",
            "And this is actually something that we from the semantic web community are doing every day, right?",
            "We match stuff to words and so on.",
            "What we want to build is a concatenated vector where we know it represents, let's say, a baseball.",
            "So we need to know this text is about baseball.",
            "This this picture is about baseball and so on, so."
        ],
        [
            "We just do this in our case on the textural level, that means we just get the most probable you or I for a for a given word and for the visual information we just take version it becausw Imagenet data set that we use is already linked to word net an from word net.",
            "Getting the words is quite obvious easy."
        ],
        [
            "OK, so those are the methods we use for establishing the alignment of those three ways of representing the concept."
        ],
        [
            "So.",
            "The next question is how do we identify complementary information?",
            "Or in if you put it the other way around, how do we get rid of redundant information?",
            "So in this vector there might be redundant information.",
            "Ah."
        ],
        [
            "And we just resort again to standard techniques from dimensionality reduction like PCA, SVD, and all of those like low rank matrix factorization techniques."
        ],
        [
            "What's very important at this step is that you first normalize.",
            "So in our experience we found this out.",
            "You need to normalize this to unit length each of the portions of the vector because you have to keep in mind like the visual vector typically is something like a couple of 1000 dimensions like 2048 and something like the knowledge graph dimensions of often just 50, right?"
        ],
        [
            "OK.",
            "So then we end up with this kind of shared crossmodal concept space.",
            "This is like the general architecture.",
            "In our case, those are the methods that we actually used to obtain this and we got in the end 1538 representations for shared concepts.",
            "This is mostly limited by the visual modality."
        ],
        [
            "OK, so.",
            "Next question is, how do we measure if those embeddings actually?",
            "Are more holistic if they capture more context than before, and the most basic task to eval."
        ],
        [
            "Get this, we could come up with the word similarity task.",
            "There are lots of datasets out there which basically.",
            "Her words are shown to elect pairs of words are showing to humans and they have to rank how similar, how related they are.",
            "So in this example, sun in sunlight is very closely related, but bakery and Zebra isn't so.",
            "If we have our embeddings, we could now just.",
            "By using something like cosine similarity of the vector space also calculated similarity for all of those pairs, rank those pairs and then compare this to some correlate this ranking to the ranking.",
            "That is basically the gold standard created by humans.",
            "So if you do this."
        ],
        [
            "You can show how this works for the different modalities, so the dark green ones on the left.",
            "This is the bird modality, and obviously this or expectedly, this is basically the best for a single modal embedding becausw here we're going forward similar similarity.",
            "It could have been interesting to see how this would perform if you would have shown to the users 2 images to rate the similarity, then maybe the second like the blue bar would have been.",
            "Better, but maybe the two interesting messages here are knowledge.",
            "Graphs are not doing very well in this kind of task, 'cause it's apparently their information that humans used the least for associating concepts to each other.",
            "But if you use this to combine all of them.",
            "Benefit so you always are considerably better than any of the single modal embedding, so that's the black bar."
        ],
        [
            "So.",
            "This obviously comes brings you to the question.",
            "That's really every modality contribute to this representation."
        ],
        [
            "And we search for all different weights that are possible and plotted this in such triangular crafts.",
            "So you have to read it like this if you would be at a corner of this triangle, this would mean you only exploit one modality.",
            "If you're on one edge, you exploit two modalities and just ignore one of them.",
            "But in all our experiments, this black cross that indicates what's the best.",
            "Best performing combination of modalities was always somewhere in the middle of the triangle, right?",
            "So this is also interesting finding."
        ],
        [
            "So every modality contributes.",
            "Next question was how do the embedding spaces differ?",
            "Can we get some intuition how they are different?",
            "For that we just plotted some of the concepts, so here we compared a birds with vehicles.",
            "Right and you see, in the textual mortality, it's already quite well separated, but knowledge graph in visual modality.",
            "It's not that clear, but if you combine this, it's pretty nicely contently represented in this case."
        ],
        [
            "Then finally, and maybe the most interesting task for this community here.",
            "Do knowledge graph tasks benefit from such representations?"
        ],
        [
            "And for this we used entity type prediction.",
            "It's some kind of a sub task of link prediction.",
            "So basically you delete an organ relation.",
            "In this case the category that baseball is of category Betton ball game."
        ],
        [
            "Should try to re predict it right.",
            "In order to do this we need to have a representation of a category in the cross modal embedding space and we also do this very naively by just averaging over all entities that we know know are in this category.",
            "And then we have like a average representation for this category.",
            "Of course, you need to take care that this entity that you want to predict where you want to measure your performance is not used to generate this average category representation.",
            "Well."
        ],
        [
            "And the results again.",
            "Quite interesting, so we compared it to two entity embedding approaches on knowledge graph embedding approaches, Rascal and transient.",
            "In the paper there are a few more and you can see that the.",
            "The cross modal representation clearly outperforms this, so you have gains over 100%.",
            "It's really, really impressive."
        ],
        [
            "So.",
            "Wrapping up.",
            "What are the lessons learned here?",
            "So first of all.",
            "It seems that visual, common sense, knowledge and distributional semantics complement entity embeddings.",
            "I mean, it's it's kind of expectable, right?",
            "I mean it's knowledge graphs are not made to represent this sort of information, but still.",
            "This is the first time that we empirically kind of show that it's really the case that they are complementing each other, and that embeddings are way to compliment them so too.",
            "Combine them and also what I just shown.",
            "There are some.",
            "Tasks that significantly better perform if you use cross modal embeddings.",
            "Then other various other benchmarks.",
            "So now."
        ],
        [
            "Our final question, so shouldn't everyone try cross modal concept embeddings now, right?",
            "Isn't this?",
            "Isn't this a better way to go and I kind of leave the answer up to you, but my opinion on this is if you have a task and there are so many tasks that kind of need this sort of machinery where you have any ranking of entities, retrieval of entities searching for entities, similarity of entities, whatever you mean.",
            "Maybe think about not tweaking your whatever algorithm that just exploits the relational structure of the Knowledge Graph.",
            "Tweak it for whatever a month and then you get a gain of 5% or whatever, but maybe think about if the task that you have at hand could benefit from this very different type of context that is out there because you could gain easily something like 100% right?",
            "It's made maybe the same effort.",
            "OK, so this is where I'd like to close, I'm."
        ],
        [
            "Not going in detail over the future challenges, I'm just showing them here because I think each of them is so extensive that we could talk easily an hour about those future challenges.",
            "And this is also why this paper is called towards.",
            "Right, so I'm happy to take any questions.",
            "Have you considered instead of doing that merger in test agnostic way, don't you think that you know this approach could actually benefit in learning through some supervision?",
            "The optimal combination of these different embeddings?",
            "Yeah, So what we already do is some kind of a week type of supervision, right?",
            "We just test all the weights and see what are the best.",
            "But I totally agree.",
            "So this type of whoever is more familiar with those kind of crossmodal research.",
            "There's lots of different ways of combining this, so there's middle of early Fusion, middle Fusion, and lead Fusion approaches.",
            "This is a type of middle Fusion approach and I.",
            "Personally, I would assume that something like a early Fusion approach so that where you right away learn a common embedding just by looking for example at parallel corpora.",
            "So text that have images and annotations in there.",
            "Try to learn embedding of the elements in there in one step, right?",
            "Not going through this separated extraction of modalities should perform better, and this would be a much nicer like end to end way of learning this.",
            "So I totally agree in this work it was just about.",
            "Let's try the most straightforward approach and see what we can get.",
            "Hey thank you second question.",
            "Could you go back to the bar chart?",
            "Which player was wondering so you have like word pairs as inputs and then you predict.",
            "And how do you do that with a visual model like?",
            "Because visual like you would put just two pictures and so visual is here also combined with text or.",
            "No, so it's it's all on avert level, right?",
            "So it's only it's the data set is pairs of words that have an associated human rating of how similar those words are, right?",
            "And you can now represent those words just with a bird embedding, right?",
            "Or you can use a cross modal embedding, and this is what we did.",
            "So you rank.",
            "Not by the vectors of the word embedding, but you rank the vectors from the crossmodal space.",
            "And this is how you get the visual information in there.",
            "So how this data sets are created is it's just showing showing 2 words to a human anti has to judge how similar they are.",
            "OK thanks.",
            "OK way back there.",
            "So there is a number of works in HI and AAAI of last year 2015 where they combine and weddings from knowledge graphs.",
            "An from text and I think they do what you mentioned as early Fusion.",
            "OK, I was wondering whether you consider those works and how you compare against them.",
            "If you did, yeah, so there is a few works is out there, so as far as I know there is none that combines three different.",
            "Three different modalities, so vision, text and knowledge graphs, so I don't know where you are, but.",
            "And.",
            "They also don't evaluate on the same tasks, right?",
            "So for us it was really the interesting question.",
            "How can entity embeddings benefit from that?",
            "But again, from the approach that we took, it's not a sophisticated approach, right?",
            "It's just a straightforward approach and this can be easily improved by a lot of work that has been done also in whatever the image captured in community, and so there's much more sophisticated, much better models.",
            "I'm just saying this is kind of a baseline approach, and already this kind of baseline approach.",
            "Gives you a lot of gain in your tasks.",
            "Potentially talk so on the word similarity task you did some kind of contribution analysis like and you found that it's really all three modalities that contribute for the link prediction task.",
            "I'm very surprised because I would expect that visual context would actually negatively contribute.",
            "Maybe that you do a similar contribution analysis on the link prediction task, and if you find the same equal contribution so in the paper there are the weights for this task but are the.",
            "Best weights I don't recall them right now, but I'm sure that all of the all of the modalities played a role.",
            "I can't tell you if the visual modality was less important, but I would assume it is, but it's in the paper, but you also have to think about the selection of concepts that you have here typically have a very nice visual representation because this is what we get from the object detection like visual object detection datasets.",
            "So there is some kind of typically visual visual similarity inside the categories.",
            "Text where you have a lot of concepts like love and hate and philosophy.",
            "There's nothing visual about those things.",
            "This could bias it very badly, right?",
            "So if you're trying to get to visual embeddings of words that have absolutely no visual, there's where visuality doesn't make actually any sense.",
            "You could pollute it, right?",
            "Yeah, problematic.",
            "The lightning fast answers, yes.",
            "It's true."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Stephen Toma and far beyond bored, so I might not be a responsible for everything that's bad in this paper, but for the general idea, you can come to me and complain about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This research with this question, so what's actually captured in knowledge graph embeddings that are learned from knowledge graphs, and for those of you who are not that familiar, I guess it's not that many but very brief introduction to this knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "Embeddings area is actually some in my opinion.",
                    "label": 0
                },
                {
                    "sent": "The best example of their Semantic web task and semantic web data has made it to the machine learning community, and there have been in the machine learning community developing for the last years a lot of different.",
                    "label": 0
                },
                {
                    "sent": "Approaches to embedding knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "Here just four classes, and of course a lot lot of different approaches that have been proposed.",
                    "label": 0
                },
                {
                    "sent": "I'm only talk about tranzit because it's kind of the simplest approach and the most used so far and.",
                    "label": 0
                },
                {
                    "sent": "How this proceeds is they basically look at some sort of triple right?",
                    "label": 0
                },
                {
                    "sent": "In this case, let's say it's baseball and baseball is of category bat and ball game, and what they tried to do is replace it with a vector representation of this, so they try to replace the subject predicate and the object with with the vector low dimensional dense vector and for that they need to define a scoring function that scores how likely this vector is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the case of tranzit, what they do is they basically say if you have a known triple you should rank this higher as artificially corrupted triple.",
                    "label": 0
                },
                {
                    "sent": "And in the case of trends E, that's I guess the most simple as scoring function that you could imagine.",
                    "label": 0
                },
                {
                    "sent": "You just say in vector space if you add the predicate to the subject, you should end up at the representation of the object.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's very brief.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's captured, So what they do capture is the relational structure they capture kind of the network structure itself.",
                    "label": 0
                },
                {
                    "sent": "And we ask ourself, is there may be other types of context context that you want to leverage in your embedding in your representation of your context?",
                    "label": 0
                },
                {
                    "sent": "And if you would.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About it as a human.",
                    "label": 0
                },
                {
                    "sent": "If you think about the baseball, it's probably not the first thing that comes to your mind, right?",
                    "label": 0
                },
                {
                    "sent": "This is the stuff that you find in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "It's of category Patton Pool game, and there's a union called World Baseball Softball Confederation, right?",
                    "label": 0
                },
                {
                    "sent": "That's what you might think, and this is now maybe a little bit of extreme.",
                    "label": 0
                },
                {
                    "sent": "But what I want to stress here is that it's really a really different type of context that you're thinking about is, for example.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's kind of visually similar to Apple or something like this, so it has the same size it's.",
                    "label": 0
                },
                {
                    "sent": "It has the same shape and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's very different, and this is some sort of information that you can't get out of a knowledge graph, and if you continue this thinking, So what do you think about in apples?",
                    "label": 0
                },
                {
                    "sent": "Or maybe you associate an Apple also more with Cinnamon Becausw.",
                    "label": 0
                },
                {
                    "sent": "They're more recipes with that.",
                    "label": 0
                },
                {
                    "sent": "Combine Apple cinnamon compared to potatoes.",
                    "label": 0
                },
                {
                    "sent": "So it's really really a very different type of information that.",
                    "label": 0
                },
                {
                    "sent": "USA human combined.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that this is like the best example, I'm just saying it's really a different type of information that you're looking at.",
                    "label": 0
                },
                {
                    "sent": "So then the question is, how do we kind of get this information?",
                    "label": 0
                },
                {
                    "sent": "And obviously there is data out there so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the visual modality right at images, you exactly get this information, so you get this kind of standard common sense.",
                    "label": 0
                },
                {
                    "sent": "Visual information like shape, color, background of baseball, and if you look at the detection modality, what what's the most interesting part?",
                    "label": 0
                },
                {
                    "sent": "There is kind of this core currents of entities, so in language, what could occur Co occurs together which entities are mentioned together often, and this is also something that you don't.",
                    "label": 0
                },
                {
                    "sent": "Get in in the Knowledge graph, at least not you don't get the frequency right.",
                    "label": 0
                },
                {
                    "sent": "How often is the entity mentioned?",
                    "label": 0
                },
                {
                    "sent": "In the real world, OK?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yes, the context from the visual modality of natural language is there.",
                    "label": 0
                },
                {
                    "sent": "So how do we collect this kind of information in an encoding that is compatible?",
                    "label": 0
                },
                {
                    "sent": "And I guess you can assume.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what the answer to this is right, we go with embeddings right?",
                    "label": 0
                },
                {
                    "sent": "In all of those different research areas like computer vision and NLP.",
                    "label": 0
                },
                {
                    "sent": "They have been embedding techniques created in the last years.",
                    "label": 0
                },
                {
                    "sent": "If more extensively than in the Knowledge Graph community.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just two examples that we used here that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic approach is so convolutional neural network we used here that kind of learns a lot of layers with increasing abstraction of the visual features, and we took the second to last layer and use this as the vector representation of the objects.",
                    "label": 0
                },
                {
                    "sent": "If you average this over a big.",
                    "label": 0
                },
                {
                    "sent": "Image classification task like image net.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same you can do with the birds, I don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into details.",
                    "label": 0
                },
                {
                    "sent": "You have seen those types of slides during the conference alot so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we get is we get vectors here for visual features, burden packings and knowledge graph entity embeddings, and in this case we use those methods.",
                    "label": 0
                },
                {
                    "sent": "You can plug in whatever other methods you want right there.",
                    "label": 0
                },
                {
                    "sent": "Much more much better embedding methods than those are.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what to do with this multiple embeddings, right?",
                    "label": 0
                },
                {
                    "sent": "The next question is how do we allow in them?",
                    "label": 0
                },
                {
                    "sent": "And this is actually something that we from the semantic web community are doing every day, right?",
                    "label": 0
                },
                {
                    "sent": "We match stuff to words and so on.",
                    "label": 0
                },
                {
                    "sent": "What we want to build is a concatenated vector where we know it represents, let's say, a baseball.",
                    "label": 0
                },
                {
                    "sent": "So we need to know this text is about baseball.",
                    "label": 0
                },
                {
                    "sent": "This this picture is about baseball and so on, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just do this in our case on the textural level, that means we just get the most probable you or I for a for a given word and for the visual information we just take version it becausw Imagenet data set that we use is already linked to word net an from word net.",
                    "label": 0
                },
                {
                    "sent": "Getting the words is quite obvious easy.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those are the methods we use for establishing the alignment of those three ways of representing the concept.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The next question is how do we identify complementary information?",
                    "label": 0
                },
                {
                    "sent": "Or in if you put it the other way around, how do we get rid of redundant information?",
                    "label": 0
                },
                {
                    "sent": "So in this vector there might be redundant information.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we just resort again to standard techniques from dimensionality reduction like PCA, SVD, and all of those like low rank matrix factorization techniques.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's very important at this step is that you first normalize.",
                    "label": 0
                },
                {
                    "sent": "So in our experience we found this out.",
                    "label": 0
                },
                {
                    "sent": "You need to normalize this to unit length each of the portions of the vector because you have to keep in mind like the visual vector typically is something like a couple of 1000 dimensions like 2048 and something like the knowledge graph dimensions of often just 50, right?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then we end up with this kind of shared crossmodal concept space.",
                    "label": 0
                },
                {
                    "sent": "This is like the general architecture.",
                    "label": 0
                },
                {
                    "sent": "In our case, those are the methods that we actually used to obtain this and we got in the end 1538 representations for shared concepts.",
                    "label": 0
                },
                {
                    "sent": "This is mostly limited by the visual modality.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Next question is, how do we measure if those embeddings actually?",
                    "label": 0
                },
                {
                    "sent": "Are more holistic if they capture more context than before, and the most basic task to eval.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get this, we could come up with the word similarity task.",
                    "label": 0
                },
                {
                    "sent": "There are lots of datasets out there which basically.",
                    "label": 0
                },
                {
                    "sent": "Her words are shown to elect pairs of words are showing to humans and they have to rank how similar, how related they are.",
                    "label": 0
                },
                {
                    "sent": "So in this example, sun in sunlight is very closely related, but bakery and Zebra isn't so.",
                    "label": 0
                },
                {
                    "sent": "If we have our embeddings, we could now just.",
                    "label": 0
                },
                {
                    "sent": "By using something like cosine similarity of the vector space also calculated similarity for all of those pairs, rank those pairs and then compare this to some correlate this ranking to the ranking.",
                    "label": 0
                },
                {
                    "sent": "That is basically the gold standard created by humans.",
                    "label": 0
                },
                {
                    "sent": "So if you do this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can show how this works for the different modalities, so the dark green ones on the left.",
                    "label": 0
                },
                {
                    "sent": "This is the bird modality, and obviously this or expectedly, this is basically the best for a single modal embedding becausw here we're going forward similar similarity.",
                    "label": 0
                },
                {
                    "sent": "It could have been interesting to see how this would perform if you would have shown to the users 2 images to rate the similarity, then maybe the second like the blue bar would have been.",
                    "label": 0
                },
                {
                    "sent": "Better, but maybe the two interesting messages here are knowledge.",
                    "label": 0
                },
                {
                    "sent": "Graphs are not doing very well in this kind of task, 'cause it's apparently their information that humans used the least for associating concepts to each other.",
                    "label": 0
                },
                {
                    "sent": "But if you use this to combine all of them.",
                    "label": 0
                },
                {
                    "sent": "Benefit so you always are considerably better than any of the single modal embedding, so that's the black bar.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This obviously comes brings you to the question.",
                    "label": 0
                },
                {
                    "sent": "That's really every modality contribute to this representation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we search for all different weights that are possible and plotted this in such triangular crafts.",
                    "label": 0
                },
                {
                    "sent": "So you have to read it like this if you would be at a corner of this triangle, this would mean you only exploit one modality.",
                    "label": 0
                },
                {
                    "sent": "If you're on one edge, you exploit two modalities and just ignore one of them.",
                    "label": 0
                },
                {
                    "sent": "But in all our experiments, this black cross that indicates what's the best.",
                    "label": 0
                },
                {
                    "sent": "Best performing combination of modalities was always somewhere in the middle of the triangle, right?",
                    "label": 0
                },
                {
                    "sent": "So this is also interesting finding.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So every modality contributes.",
                    "label": 0
                },
                {
                    "sent": "Next question was how do the embedding spaces differ?",
                    "label": 0
                },
                {
                    "sent": "Can we get some intuition how they are different?",
                    "label": 0
                },
                {
                    "sent": "For that we just plotted some of the concepts, so here we compared a birds with vehicles.",
                    "label": 0
                },
                {
                    "sent": "Right and you see, in the textual mortality, it's already quite well separated, but knowledge graph in visual modality.",
                    "label": 0
                },
                {
                    "sent": "It's not that clear, but if you combine this, it's pretty nicely contently represented in this case.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then finally, and maybe the most interesting task for this community here.",
                    "label": 0
                },
                {
                    "sent": "Do knowledge graph tasks benefit from such representations?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for this we used entity type prediction.",
                    "label": 0
                },
                {
                    "sent": "It's some kind of a sub task of link prediction.",
                    "label": 0
                },
                {
                    "sent": "So basically you delete an organ relation.",
                    "label": 0
                },
                {
                    "sent": "In this case the category that baseball is of category Betton ball game.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should try to re predict it right.",
                    "label": 0
                },
                {
                    "sent": "In order to do this we need to have a representation of a category in the cross modal embedding space and we also do this very naively by just averaging over all entities that we know know are in this category.",
                    "label": 0
                },
                {
                    "sent": "And then we have like a average representation for this category.",
                    "label": 0
                },
                {
                    "sent": "Of course, you need to take care that this entity that you want to predict where you want to measure your performance is not used to generate this average category representation.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the results again.",
                    "label": 0
                },
                {
                    "sent": "Quite interesting, so we compared it to two entity embedding approaches on knowledge graph embedding approaches, Rascal and transient.",
                    "label": 0
                },
                {
                    "sent": "In the paper there are a few more and you can see that the.",
                    "label": 0
                },
                {
                    "sent": "The cross modal representation clearly outperforms this, so you have gains over 100%.",
                    "label": 0
                },
                {
                    "sent": "It's really, really impressive.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Wrapping up.",
                    "label": 0
                },
                {
                    "sent": "What are the lessons learned here?",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "It seems that visual, common sense, knowledge and distributional semantics complement entity embeddings.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's it's kind of expectable, right?",
                    "label": 0
                },
                {
                    "sent": "I mean it's knowledge graphs are not made to represent this sort of information, but still.",
                    "label": 0
                },
                {
                    "sent": "This is the first time that we empirically kind of show that it's really the case that they are complementing each other, and that embeddings are way to compliment them so too.",
                    "label": 0
                },
                {
                    "sent": "Combine them and also what I just shown.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "Tasks that significantly better perform if you use cross modal embeddings.",
                    "label": 0
                },
                {
                    "sent": "Then other various other benchmarks.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our final question, so shouldn't everyone try cross modal concept embeddings now, right?",
                    "label": 0
                },
                {
                    "sent": "Isn't this?",
                    "label": 0
                },
                {
                    "sent": "Isn't this a better way to go and I kind of leave the answer up to you, but my opinion on this is if you have a task and there are so many tasks that kind of need this sort of machinery where you have any ranking of entities, retrieval of entities searching for entities, similarity of entities, whatever you mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe think about not tweaking your whatever algorithm that just exploits the relational structure of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Tweak it for whatever a month and then you get a gain of 5% or whatever, but maybe think about if the task that you have at hand could benefit from this very different type of context that is out there because you could gain easily something like 100% right?",
                    "label": 0
                },
                {
                    "sent": "It's made maybe the same effort.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is where I'd like to close, I'm.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not going in detail over the future challenges, I'm just showing them here because I think each of them is so extensive that we could talk easily an hour about those future challenges.",
                    "label": 0
                },
                {
                    "sent": "And this is also why this paper is called towards.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'm happy to take any questions.",
                    "label": 0
                },
                {
                    "sent": "Have you considered instead of doing that merger in test agnostic way, don't you think that you know this approach could actually benefit in learning through some supervision?",
                    "label": 0
                },
                {
                    "sent": "The optimal combination of these different embeddings?",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what we already do is some kind of a week type of supervision, right?",
                    "label": 0
                },
                {
                    "sent": "We just test all the weights and see what are the best.",
                    "label": 0
                },
                {
                    "sent": "But I totally agree.",
                    "label": 0
                },
                {
                    "sent": "So this type of whoever is more familiar with those kind of crossmodal research.",
                    "label": 0
                },
                {
                    "sent": "There's lots of different ways of combining this, so there's middle of early Fusion, middle Fusion, and lead Fusion approaches.",
                    "label": 0
                },
                {
                    "sent": "This is a type of middle Fusion approach and I.",
                    "label": 0
                },
                {
                    "sent": "Personally, I would assume that something like a early Fusion approach so that where you right away learn a common embedding just by looking for example at parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "So text that have images and annotations in there.",
                    "label": 0
                },
                {
                    "sent": "Try to learn embedding of the elements in there in one step, right?",
                    "label": 0
                },
                {
                    "sent": "Not going through this separated extraction of modalities should perform better, and this would be a much nicer like end to end way of learning this.",
                    "label": 0
                },
                {
                    "sent": "So I totally agree in this work it was just about.",
                    "label": 0
                },
                {
                    "sent": "Let's try the most straightforward approach and see what we can get.",
                    "label": 0
                },
                {
                    "sent": "Hey thank you second question.",
                    "label": 0
                },
                {
                    "sent": "Could you go back to the bar chart?",
                    "label": 0
                },
                {
                    "sent": "Which player was wondering so you have like word pairs as inputs and then you predict.",
                    "label": 0
                },
                {
                    "sent": "And how do you do that with a visual model like?",
                    "label": 0
                },
                {
                    "sent": "Because visual like you would put just two pictures and so visual is here also combined with text or.",
                    "label": 0
                },
                {
                    "sent": "No, so it's it's all on avert level, right?",
                    "label": 0
                },
                {
                    "sent": "So it's only it's the data set is pairs of words that have an associated human rating of how similar those words are, right?",
                    "label": 0
                },
                {
                    "sent": "And you can now represent those words just with a bird embedding, right?",
                    "label": 0
                },
                {
                    "sent": "Or you can use a cross modal embedding, and this is what we did.",
                    "label": 0
                },
                {
                    "sent": "So you rank.",
                    "label": 0
                },
                {
                    "sent": "Not by the vectors of the word embedding, but you rank the vectors from the crossmodal space.",
                    "label": 0
                },
                {
                    "sent": "And this is how you get the visual information in there.",
                    "label": 0
                },
                {
                    "sent": "So how this data sets are created is it's just showing showing 2 words to a human anti has to judge how similar they are.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "OK way back there.",
                    "label": 0
                },
                {
                    "sent": "So there is a number of works in HI and AAAI of last year 2015 where they combine and weddings from knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "An from text and I think they do what you mentioned as early Fusion.",
                    "label": 0
                },
                {
                    "sent": "OK, I was wondering whether you consider those works and how you compare against them.",
                    "label": 0
                },
                {
                    "sent": "If you did, yeah, so there is a few works is out there, so as far as I know there is none that combines three different.",
                    "label": 0
                },
                {
                    "sent": "Three different modalities, so vision, text and knowledge graphs, so I don't know where you are, but.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "They also don't evaluate on the same tasks, right?",
                    "label": 0
                },
                {
                    "sent": "So for us it was really the interesting question.",
                    "label": 0
                },
                {
                    "sent": "How can entity embeddings benefit from that?",
                    "label": 0
                },
                {
                    "sent": "But again, from the approach that we took, it's not a sophisticated approach, right?",
                    "label": 0
                },
                {
                    "sent": "It's just a straightforward approach and this can be easily improved by a lot of work that has been done also in whatever the image captured in community, and so there's much more sophisticated, much better models.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying this is kind of a baseline approach, and already this kind of baseline approach.",
                    "label": 0
                },
                {
                    "sent": "Gives you a lot of gain in your tasks.",
                    "label": 0
                },
                {
                    "sent": "Potentially talk so on the word similarity task you did some kind of contribution analysis like and you found that it's really all three modalities that contribute for the link prediction task.",
                    "label": 0
                },
                {
                    "sent": "I'm very surprised because I would expect that visual context would actually negatively contribute.",
                    "label": 0
                },
                {
                    "sent": "Maybe that you do a similar contribution analysis on the link prediction task, and if you find the same equal contribution so in the paper there are the weights for this task but are the.",
                    "label": 0
                },
                {
                    "sent": "Best weights I don't recall them right now, but I'm sure that all of the all of the modalities played a role.",
                    "label": 0
                },
                {
                    "sent": "I can't tell you if the visual modality was less important, but I would assume it is, but it's in the paper, but you also have to think about the selection of concepts that you have here typically have a very nice visual representation because this is what we get from the object detection like visual object detection datasets.",
                    "label": 0
                },
                {
                    "sent": "So there is some kind of typically visual visual similarity inside the categories.",
                    "label": 0
                },
                {
                    "sent": "Text where you have a lot of concepts like love and hate and philosophy.",
                    "label": 0
                },
                {
                    "sent": "There's nothing visual about those things.",
                    "label": 0
                },
                {
                    "sent": "This could bias it very badly, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to get to visual embeddings of words that have absolutely no visual, there's where visuality doesn't make actually any sense.",
                    "label": 0
                },
                {
                    "sent": "You could pollute it, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, problematic.",
                    "label": 0
                },
                {
                    "sent": "The lightning fast answers, yes.",
                    "label": 0
                },
                {
                    "sent": "It's true.",
                    "label": 0
                }
            ]
        }
    }
}