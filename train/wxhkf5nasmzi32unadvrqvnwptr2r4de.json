{
    "id": "wxhkf5nasmzi32unadvrqvnwptr2r4de",
    "title": "Learning to Predict Combinatorial Structures",
    "info": {
        "author": [
            "Shankar Vembu, Department of Computer Science, University of Illinois at Urbana-Champaign"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/siso08_vembu_ltpcs/",
    "segmentation": [
        [
            "Thanks, kosta."
        ],
        [
            "OK, so this book is about structured prediction models and I'm going to focus on models that use.",
            "Some sort of inference during learning, so I'll explain what influences in a few minutes.",
            "But for now, let's say it's an important ingredient of your training algorithm.",
            "If you can do this inference exactly in polynomial time, then you could use existing.",
            "Models for structured prediction.",
            "But if this is not the case, then you may not be able to train structured prediction models."
        ],
        [
            "Efficiently.",
            "So the question that I'm trying to ask is the following.",
            "Is it possible to learn structured prediction models without using any inference algorithm?"
        ],
        [
            "And hopefully I can convince you that the answer is indeed yes for for certain class of structures that we which we consider in this work.",
            "So they're combinatorial structures.",
            "So what are community?"
        ],
        [
            "Structures, so in this work I'll focus on on this list, so I will consider predicting partially ordered sets, permutations, cycles and other graph classes.",
            "I should also mention that various other machine learning problems like multiclass multilabel, ordinal regression, and hierarchical classification can be solved using our framework."
        ],
        [
            "OK, so this is the outline of the talk.",
            "I'll first give a very brief introduction to structured prediction and describe the limitations of existing models.",
            "I'll then talk about training combinatorial structures, which is one of our main contributions.",
            "I'll then talk about constructing combinatorial structures, so this is basically the prediction step.",
            "So in structured prediction there are two different components.",
            "One is the learning component and the other one is the prediction component, so we have contributions in both these.",
            "And finally, I will talk about some applications in machine learning."
        ],
        [
            "Alright, So what is structured prediction?",
            "So there is an input space and an output space.",
            "And you are given a bunch of training instances from from an underlying probability distribution.",
            "Now the goal is still on a joint scoring function on the inputs and outputs.",
            "OK, prediction is performed according to the following rule.",
            "So for a given X you predict this structure Y that has the higher score."
        ],
        [
            "And I'm going to focus only on discriminating structured prediction, where the goal is to minimize some cost functional of your of hedge, subject to the following constraints.",
            "So these constraints ensure that every correct input output pair gets a score higher than all the other incorrect ones.",
            "It's not difficult to see that there are exponential number of constraints here.",
            "So much work on structure, discriminative structure prediction has focused on how to handle these exponential number of constraints.",
            "So in this."
        ],
        [
            "This extent they make existing models make different assumptions.",
            "The first assumption is the decoding problem.",
            "So.",
            "If you can solve this decoding problem, what is the decoding problem?",
            "So, given your scoring function, hedge and some input X, you need to find the structure that has higher score.",
            "So if you can solve this decoding problem in polynomial time, you could use SVM struct for example.",
            "Another assumption which is weaker than the decoding assumption is the separation problem.",
            "Here you don't have to find the argmax or the structure with the higher score, but you need to find any structure with a score higher than the input output X&Y.",
            "So if you can do this in polynomial time, then you could use Michael Collins structured variant of the Perceptron to solve structured prediction models.",
            "And finally, there is under the weakest assumption that is made by one of the algorithms is what is called the optimality.",
            "So it's basically a decision problem.",
            "So in here you are asking the following question.",
            "So given HX&Y, you need to decide if there is no structure with a score higher than the given input output pair.",
            "In other words, you are asking whether my structure is the argmax or use the highest scoring structure.",
            "So if you can decide this problem in polynomial time then you could use one of Ben Taskar's.",
            "Algorithm to do a structured prediction."
        ],
        [
            "Alright, so now what I'm going to show you is for the kind of structures that we considered in this work.",
            "Even the weakest assumption.",
            "So the weakest assumption is optimality, so even the weakest assumption fails, which means that it cannot use any of the existing algorithms to train for the structures that we consider in this world.",
            "So let's get into optimality and non optimality in much more detail.",
            "As I said before, what is optimality?",
            "So given the scoring function and an input output pair, unity decide whether there is no structure with a score higher than the given.",
            "Non optimality is the complement of the problem, so you're basically asking given HX&Y, is there any structure with the score higher than the given input output?"
        ],
        [
            "OK, now let's take.",
            "Let's take an example.",
            "Let's take the example of predicting bicycles.",
            "So predicting bicycles is like for example, if you have a street network and then there is this guy who wants to perform some tasks and then come back to his home for example, and now he wants to find out which path he has to take.",
            "So it's basically a cycle you are trying to predict cycles on the street network.",
            "OK, so how would you ask the optimality?",
            "What is the decision problem of optimality?",
            "So you're asking, is there no longer cycle than the given input output?",
            "So X is some features feature vector.",
            "And why is the cycle OK?",
            "And non optimality is is there any longer cycle than the given given in?",
            "Now we show that."
        ],
        [
            "The non optimality problem for directed cycles is NP complete.",
            "The proof is in the paper.",
            "It follows from a simple reduction.",
            "When I say that the non optimality problem is NP complete, it means that the optimality problem, which is the complement of the problem, is Cohen."
        ],
        [
            "A complete.",
            "OK, so now if you look at this complexity class diagram, sorry.",
            "So as I said, the optimality problem should be in NP, right?",
            "But now I have shown that optimality is actually Co NP complete, so it's on the other side unfortunately, which means that you cannot even the weakest assumption fails.",
            "So this is for bicycles.",
            "Similarly, you could argue for other structures that we consider in this work.",
            "So that's basically the problem.",
            "Definitely the main problem."
        ],
        [
            "Alright, so now let's see how we could circumvent this problem.",
            "I'll start with training combinatorial structures so."
        ],
        [
            "Let's start with loss functions.",
            "The loss functions that we consider.",
            "I'll start with what is called the ranking loss.",
            "So in ranking loss you are basically trying to.",
            "Make sure that every correct score gets is ranked higher than all the other incorrect ones.",
            "So if this is not the case then you incur a loss there.",
            "So that's the step function over there, so it's not possible to minimize this loss, right?",
            "I have to upper bound this by a convex loss function.",
            "I use the."
        ],
        [
            "Spinach in loss.",
            "So this is very much you asked me.",
            "Actually mentioned all these losses in her in her introduction to structured prediction.",
            "Alright, unfortunately because of the exponent term, I cannot still compute the sums, but."
        ],
        [
            "I'll do here is.",
            "This is one of the main tricks.",
            "I take the 2nd order Taylor expansion at zero and then I. I use this particular.",
            "Term instead of the exponent.",
            "So I have the near term in the quadratic terms in a subsequent slide I will show you how to compute these sums efficiently.",
            "So that's basically the last function we consider."
        ],
        [
            "OK, so now we follow this regularizer risk minimization approach.",
            "So you're basically trying to minimize.",
            "So everything is in our kitchens, right?",
            "So you are basically minimizing the election of hedge.",
            "Plus you have this error term.",
            "We make two other assumptions.",
            "The first assumption is.",
            "The what it basically says is that you have a joint kernel matrix on the inputs and outputs.",
            "So I basically assume that the kernel matrix factorizes.",
            "OK, and then we make another assumption that the span of those set of vectors has a polynomial number of bases.",
            "So if these two assumptions hold, which will hold for?"
        ],
        [
            "Structures that we consider.",
            "It boils down to solving this particular minimization optimization problem.",
            "OK."
        ],
        [
            "No.",
            "Optimization with finite embedding.",
            "So for output spaces I assume that there is a map.",
            "There is a finite dimensional map.",
            "Fee given and then there is a kernel on the output space.",
            "So the main trick is to be able to compute those sums.",
            "The metrics.",
            "Sorry the vector bold fee and the metric see in polynomial time.",
            "If you can do that.",
            "And if I use the Canonical orthonormal basis of RDI, can optimize this problem?",
            "It's an unconstrained polynomial sized keeping.",
            "And that's about it."
        ],
        [
            "Alright, so now.",
            "Recipe for training communication structures.",
            "So if you have a new structure, new combination structure in your mind, you basically need to follow these following steps.",
            "The first thing that we need to come up with."
        ],
        [
            "Finite dimensional output hammering of your output space, right?"
        ],
        [
            "Thing is, you need to make sure that you are able to compute those terms in polynomial time.",
            "Vector Wolfe"
        ],
        [
            "And then this will result in a polynomial sex Panther unconstrained quadratic program.",
            "OK. Then in the final section of this talk I will show how to compute those songs for variety of computer instructions.",
            "Alright, so that."
        ],
        [
            "Training.",
            "Now let's see how we could construct community Inspector.",
            "So once you have learned or model."
        ],
        [
            "So what we have to do is approximate."
        ],
        [
            "We come up with approximation of using a different kind of measure which is."
        ],
        [
            "Anne.",
            "The motivation behind using C approximation."
        ],
        [
            "So what we show is for for sibling systems and independent systems there are approximation.",
            "We come up with a design approximation algorithm to the predictions.",
            "So what is the sibling system?",
            "So the definition is given, but basically what it says is if you take a structure where is account, there is a sibling corresponding simply for every structure.",
            "So if you consider directed cycles, the reverse of this cycle is in Sydney.",
            "And now if you take the existence of macrocystis this summer.",
            "This is a constant and therefore the properties for.",
            "So that's basically an example of signaling system."
        ],
        [
            "So we showed that there is a half actors, the approximation algorithm for decoding signal systems.",
            "The algorithm as well as the proof is in the paper.",
            "Answer."
        ],
        [
            "We also consider independent system an example of an independent system is the property of this activity.",
            "So if you take an inside game graph and you remove some of the images from it, you still get any exactly graph, so the property is preserved the independence system.",
            "So."
        ],
        [
            "Independent systems we showed that there is this factor approximation algorithm for decoding independent systems.",
            "So all the structures that we consider in this work fall into one of these categories, so you could do the prediction step using these approximation algorithms."
        ],
        [
            "OK, so that's the prediction.",
            "But now let's get into applications."
        ],
        [
            "Start with a very simple application which is a multi class classification machine learning.",
            "So as I said, you first need to come up with a finite dimensional embedding of the output space.",
            "Let's say we have four classes, so this is basically an embedding of your entire office space, so it's number one is at the position of the class and the remaining remaining weeks ago.",
            "Now it's not easy.",
            "It's not difficult to see that we can compute the board.",
            "Vector fee is just summing up all the all the outputs, right?",
            "So you get an identity if a vector with all months.",
            "And then similarly you can compute the matrix C, which is just identity vector.",
            "You take the outer product of these of these metrics and that's it.",
            "Decoding is trivial because you just need to enumerate over all classes.",
            "So this is a very simple example of how we could do multiclass classification using our approach.",
            "So once again, you need to compute.",
            "You need to come up with the finite dimensional output embedding you need to compute those sums.",
            "The bold vector fee and the metrics see.",
            "And that's it."
        ],
        [
            "How do you do multi label classification?",
            "Now the output space is exponential exponential in the number of labels that you have OK. And.",
            "I won't go through the derivation here, but then it's also possible to compute those two quantities in polynomial time.",
            "Decoding is not trivial, but still you could do it in polynomial time using this particular expression.",
            "OK, so that's how you solve multi level."
        ],
        [
            "Fordyce circles it becomes a bit more complicated, so here the output embedding is basically your edges and symmetrics.",
            "So if you have K vertices.",
            "You come out you.",
            "You compute your Carcross K adjacency matrix, and that's your output memory.",
            "So that's minus 101 edges and symmetrics."
        ],
        [
            "These equations are not straightforward, but then it is still possible to compute these two quantities.",
            "The bold, better with the bold vector fee is basically 0 because it's a sibling system.",
            "So for every edges and symmetrics you have another adjacency matrix which is it's circling.",
            "To sum them up, you get 0.",
            "That's it actually simplifies the problem or not.",
            "Cimetrix is difficult to compute your for every edge you are basically trying to.",
            "Count the number of cycles that has this particular edge and then with little bit of work you will be able to come up with those expressions.",
            "So once again, I have shown you how to how to compute.",
            "Define a dimensional embedding.",
            "How to compute those sums?",
            "The bold vector fee and the metric see.",
            "And then you plug in these quantities into the optimization problem and you're done.",
            "Alright, so that's how you do it."
        ],
        [
            "Prediction and there are a lot of other examples that you will find in the paper, so you could do ordinal regression.",
            "You could hierarchical classification, you could predict partially ordered sets.",
            "You could do predict permutations, so predicting permutations is what is called label ranking in machine learning.",
            "And then you could also predict other classes of graphs.",
            "But of course I'm not going to go through the details here.",
            "Yep."
        ],
        [
            "Alright, so let's have a quick look at the experiments."
        ],
        [
            "So we have some results on multi label classification.",
            "We made some comparisons with multi label SVM and we obtain some good results here on the Hamming loss in the ranking loss."
        ],
        [
            "We have some results on hierarchical classifications of, so we compare this office our algorithms.",
            "So that's the name of our algorithm.",
            "We compared it with the different other.",
            "Models for hierarchical classification taken from a paper, and we obtained good results on the different class functions."
        ],
        [
            "So predicting bicycles is basically an artificial simulation.",
            "We don't really have any real world datasets here, so the goal is to predict the cyclic tour of different people.",
            "As I already explained, and now we assume that there is some policy that this person takes.",
            "Another goal is to infer this policy."
        ],
        [
            "And we compared.",
            "Bicycle policy estimation with SPM struck so you cannot solve this problem.",
            "You cannot train SVM struct efficiently with this for this problem because there is no decoding.",
            "Decoding is not possible.",
            "So what we did was we used approximation algorithms for decoding.",
            "So inside SVM struct we use an approximation algorithm to compute the argmax decoding problem.",
            "In our case, we don't really need this because we don't use any approximation algorithms for learning, and then you can see the comparisons.",
            "So the cosine measure is the higher the better so.",
            "Yeah, that's the comparison with the stream structure.",
            "As expected as the number of training instances grew, the cosine measure was going up.",
            "Of course, for both the algorithms.",
            "And the results are significant.",
            "As you can see from the very fast."
        ],
        [
            "OK, So what am I currently working on?",
            "I'm actually trying to come up with probabilistic models because as I told you, I'm still minimizing a second order Taylor expansion, which is a quadratic loss which is.",
            "Which is not as good as the Max margin hinge loss, right?",
            "So it would be nice if I can, for example, minimize the negative log likelihood.",
            "That's what I'm currently working on.",
            "So if you're trying to come up with probabilistic models then you need to come up with sampling techniques, because once you have a probabilistic model, you need to do the prediction step, which is, you know the sampling techniques.",
            "So for this I will use.",
            "I'm using Markov chain Monte Carlo methods, so I'm not just interested in using some MCMC technique, but I'm also interested in coming up with provable guarantees for mixing time, so I want to know how fast my my chain is going to converge to stationary distribution, so that is what I'm working on at the moment."
        ],
        [
            "OK, that's it, I'm done, thanks.",
            "Questions.",
            "So.",
            "Books.",
            "It was hardly been used.",
            "OK, OK, but one of the main reasons of the motivation behind using now that I have time, I think I can go through the approximation."
        ],
        [
            "So from from our learned model, we cannot ensure that the weights that are given to the edges are non negative.",
            "So if the way it's given to the edges are non negative, most of the existing approximation algorithms fail.",
            "So in order to circumvent this problem, we had to use the approximation that is the main reason.",
            "And I have given some concrete examples in the paper.",
            "And then of course there are other.",
            "You know, these are the properties of the approximation, but.",
            "But that's as I said, the main reason was to circumvent this.",
            "To handle this non negative.",
            "Sorry, negative it weights on the edges.",
            "Thanks again.",
            "Move on to the next speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks, kosta.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this book is about structured prediction models and I'm going to focus on models that use.",
                    "label": 1
                },
                {
                    "sent": "Some sort of inference during learning, so I'll explain what influences in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "But for now, let's say it's an important ingredient of your training algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you can do this inference exactly in polynomial time, then you could use existing.",
                    "label": 0
                },
                {
                    "sent": "Models for structured prediction.",
                    "label": 0
                },
                {
                    "sent": "But if this is not the case, then you may not be able to train structured prediction models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "So the question that I'm trying to ask is the following.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to learn structured prediction models without using any inference algorithm?",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And hopefully I can convince you that the answer is indeed yes for for certain class of structures that we which we consider in this work.",
                    "label": 0
                },
                {
                    "sent": "So they're combinatorial structures.",
                    "label": 0
                },
                {
                    "sent": "So what are community?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structures, so in this work I'll focus on on this list, so I will consider predicting partially ordered sets, permutations, cycles and other graph classes.",
                    "label": 0
                },
                {
                    "sent": "I should also mention that various other machine learning problems like multiclass multilabel, ordinal regression, and hierarchical classification can be solved using our framework.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll first give a very brief introduction to structured prediction and describe the limitations of existing models.",
                    "label": 1
                },
                {
                    "sent": "I'll then talk about training combinatorial structures, which is one of our main contributions.",
                    "label": 0
                },
                {
                    "sent": "I'll then talk about constructing combinatorial structures, so this is basically the prediction step.",
                    "label": 0
                },
                {
                    "sent": "So in structured prediction there are two different components.",
                    "label": 0
                },
                {
                    "sent": "One is the learning component and the other one is the prediction component, so we have contributions in both these.",
                    "label": 0
                },
                {
                    "sent": "And finally, I will talk about some applications in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, So what is structured prediction?",
                    "label": 1
                },
                {
                    "sent": "So there is an input space and an output space.",
                    "label": 0
                },
                {
                    "sent": "And you are given a bunch of training instances from from an underlying probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Now the goal is still on a joint scoring function on the inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "OK, prediction is performed according to the following rule.",
                    "label": 0
                },
                {
                    "sent": "So for a given X you predict this structure Y that has the higher score.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to focus only on discriminating structured prediction, where the goal is to minimize some cost functional of your of hedge, subject to the following constraints.",
                    "label": 0
                },
                {
                    "sent": "So these constraints ensure that every correct input output pair gets a score higher than all the other incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult to see that there are exponential number of constraints here.",
                    "label": 1
                },
                {
                    "sent": "So much work on structure, discriminative structure prediction has focused on how to handle these exponential number of constraints.",
                    "label": 0
                },
                {
                    "sent": "So in this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This extent they make existing models make different assumptions.",
                    "label": 0
                },
                {
                    "sent": "The first assumption is the decoding problem.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "If you can solve this decoding problem, what is the decoding problem?",
                    "label": 1
                },
                {
                    "sent": "So, given your scoring function, hedge and some input X, you need to find the structure that has higher score.",
                    "label": 0
                },
                {
                    "sent": "So if you can solve this decoding problem in polynomial time, you could use SVM struct for example.",
                    "label": 0
                },
                {
                    "sent": "Another assumption which is weaker than the decoding assumption is the separation problem.",
                    "label": 1
                },
                {
                    "sent": "Here you don't have to find the argmax or the structure with the higher score, but you need to find any structure with a score higher than the input output X&Y.",
                    "label": 0
                },
                {
                    "sent": "So if you can do this in polynomial time, then you could use Michael Collins structured variant of the Perceptron to solve structured prediction models.",
                    "label": 0
                },
                {
                    "sent": "And finally, there is under the weakest assumption that is made by one of the algorithms is what is called the optimality.",
                    "label": 1
                },
                {
                    "sent": "So it's basically a decision problem.",
                    "label": 0
                },
                {
                    "sent": "So in here you are asking the following question.",
                    "label": 1
                },
                {
                    "sent": "So given HX&Y, you need to decide if there is no structure with a score higher than the given input output pair.",
                    "label": 0
                },
                {
                    "sent": "In other words, you are asking whether my structure is the argmax or use the highest scoring structure.",
                    "label": 0
                },
                {
                    "sent": "So if you can decide this problem in polynomial time then you could use one of Ben Taskar's.",
                    "label": 0
                },
                {
                    "sent": "Algorithm to do a structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now what I'm going to show you is for the kind of structures that we considered in this work.",
                    "label": 0
                },
                {
                    "sent": "Even the weakest assumption.",
                    "label": 0
                },
                {
                    "sent": "So the weakest assumption is optimality, so even the weakest assumption fails, which means that it cannot use any of the existing algorithms to train for the structures that we consider in this world.",
                    "label": 0
                },
                {
                    "sent": "So let's get into optimality and non optimality in much more detail.",
                    "label": 0
                },
                {
                    "sent": "As I said before, what is optimality?",
                    "label": 0
                },
                {
                    "sent": "So given the scoring function and an input output pair, unity decide whether there is no structure with a score higher than the given.",
                    "label": 0
                },
                {
                    "sent": "Non optimality is the complement of the problem, so you're basically asking given HX&Y, is there any structure with the score higher than the given input output?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now let's take.",
                    "label": 0
                },
                {
                    "sent": "Let's take an example.",
                    "label": 0
                },
                {
                    "sent": "Let's take the example of predicting bicycles.",
                    "label": 0
                },
                {
                    "sent": "So predicting bicycles is like for example, if you have a street network and then there is this guy who wants to perform some tasks and then come back to his home for example, and now he wants to find out which path he has to take.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a cycle you are trying to predict cycles on the street network.",
                    "label": 0
                },
                {
                    "sent": "OK, so how would you ask the optimality?",
                    "label": 0
                },
                {
                    "sent": "What is the decision problem of optimality?",
                    "label": 0
                },
                {
                    "sent": "So you're asking, is there no longer cycle than the given input output?",
                    "label": 1
                },
                {
                    "sent": "So X is some features feature vector.",
                    "label": 0
                },
                {
                    "sent": "And why is the cycle OK?",
                    "label": 0
                },
                {
                    "sent": "And non optimality is is there any longer cycle than the given given in?",
                    "label": 1
                },
                {
                    "sent": "Now we show that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The non optimality problem for directed cycles is NP complete.",
                    "label": 0
                },
                {
                    "sent": "The proof is in the paper.",
                    "label": 0
                },
                {
                    "sent": "It follows from a simple reduction.",
                    "label": 0
                },
                {
                    "sent": "When I say that the non optimality problem is NP complete, it means that the optimality problem, which is the complement of the problem, is Cohen.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A complete.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if you look at this complexity class diagram, sorry.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the optimality problem should be in NP, right?",
                    "label": 0
                },
                {
                    "sent": "But now I have shown that optimality is actually Co NP complete, so it's on the other side unfortunately, which means that you cannot even the weakest assumption fails.",
                    "label": 0
                },
                {
                    "sent": "So this is for bicycles.",
                    "label": 0
                },
                {
                    "sent": "Similarly, you could argue for other structures that we consider in this work.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the problem.",
                    "label": 0
                },
                {
                    "sent": "Definitely the main problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now let's see how we could circumvent this problem.",
                    "label": 0
                },
                {
                    "sent": "I'll start with training combinatorial structures so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start with loss functions.",
                    "label": 0
                },
                {
                    "sent": "The loss functions that we consider.",
                    "label": 0
                },
                {
                    "sent": "I'll start with what is called the ranking loss.",
                    "label": 0
                },
                {
                    "sent": "So in ranking loss you are basically trying to.",
                    "label": 0
                },
                {
                    "sent": "Make sure that every correct score gets is ranked higher than all the other incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "So if this is not the case then you incur a loss there.",
                    "label": 0
                },
                {
                    "sent": "So that's the step function over there, so it's not possible to minimize this loss, right?",
                    "label": 0
                },
                {
                    "sent": "I have to upper bound this by a convex loss function.",
                    "label": 0
                },
                {
                    "sent": "I use the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spinach in loss.",
                    "label": 0
                },
                {
                    "sent": "So this is very much you asked me.",
                    "label": 0
                },
                {
                    "sent": "Actually mentioned all these losses in her in her introduction to structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Alright, unfortunately because of the exponent term, I cannot still compute the sums, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll do here is.",
                    "label": 0
                },
                {
                    "sent": "This is one of the main tricks.",
                    "label": 0
                },
                {
                    "sent": "I take the 2nd order Taylor expansion at zero and then I. I use this particular.",
                    "label": 1
                },
                {
                    "sent": "Term instead of the exponent.",
                    "label": 0
                },
                {
                    "sent": "So I have the near term in the quadratic terms in a subsequent slide I will show you how to compute these sums efficiently.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the last function we consider.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we follow this regularizer risk minimization approach.",
                    "label": 0
                },
                {
                    "sent": "So you're basically trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "So everything is in our kitchens, right?",
                    "label": 0
                },
                {
                    "sent": "So you are basically minimizing the election of hedge.",
                    "label": 0
                },
                {
                    "sent": "Plus you have this error term.",
                    "label": 0
                },
                {
                    "sent": "We make two other assumptions.",
                    "label": 0
                },
                {
                    "sent": "The first assumption is.",
                    "label": 0
                },
                {
                    "sent": "The what it basically says is that you have a joint kernel matrix on the inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So I basically assume that the kernel matrix factorizes.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we make another assumption that the span of those set of vectors has a polynomial number of bases.",
                    "label": 0
                },
                {
                    "sent": "So if these two assumptions hold, which will hold for?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structures that we consider.",
                    "label": 0
                },
                {
                    "sent": "It boils down to solving this particular minimization optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Optimization with finite embedding.",
                    "label": 0
                },
                {
                    "sent": "So for output spaces I assume that there is a map.",
                    "label": 0
                },
                {
                    "sent": "There is a finite dimensional map.",
                    "label": 0
                },
                {
                    "sent": "Fee given and then there is a kernel on the output space.",
                    "label": 0
                },
                {
                    "sent": "So the main trick is to be able to compute those sums.",
                    "label": 0
                },
                {
                    "sent": "The metrics.",
                    "label": 0
                },
                {
                    "sent": "Sorry the vector bold fee and the metric see in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "If you can do that.",
                    "label": 0
                },
                {
                    "sent": "And if I use the Canonical orthonormal basis of RDI, can optimize this problem?",
                    "label": 1
                },
                {
                    "sent": "It's an unconstrained polynomial sized keeping.",
                    "label": 0
                },
                {
                    "sent": "And that's about it.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now.",
                    "label": 0
                },
                {
                    "sent": "Recipe for training communication structures.",
                    "label": 1
                },
                {
                    "sent": "So if you have a new structure, new combination structure in your mind, you basically need to follow these following steps.",
                    "label": 0
                },
                {
                    "sent": "The first thing that we need to come up with.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finite dimensional output hammering of your output space, right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing is, you need to make sure that you are able to compute those terms in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "Vector Wolfe",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then this will result in a polynomial sex Panther unconstrained quadratic program.",
                    "label": 1
                },
                {
                    "sent": "OK. Then in the final section of this talk I will show how to compute those songs for variety of computer instructions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Now let's see how we could construct community Inspector.",
                    "label": 0
                },
                {
                    "sent": "So once you have learned or model.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we have to do is approximate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We come up with approximation of using a different kind of measure which is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The motivation behind using C approximation.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we show is for for sibling systems and independent systems there are approximation.",
                    "label": 1
                },
                {
                    "sent": "We come up with a design approximation algorithm to the predictions.",
                    "label": 1
                },
                {
                    "sent": "So what is the sibling system?",
                    "label": 0
                },
                {
                    "sent": "So the definition is given, but basically what it says is if you take a structure where is account, there is a sibling corresponding simply for every structure.",
                    "label": 0
                },
                {
                    "sent": "So if you consider directed cycles, the reverse of this cycle is in Sydney.",
                    "label": 0
                },
                {
                    "sent": "And now if you take the existence of macrocystis this summer.",
                    "label": 0
                },
                {
                    "sent": "This is a constant and therefore the properties for.",
                    "label": 0
                },
                {
                    "sent": "So that's basically an example of signaling system.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we showed that there is a half actors, the approximation algorithm for decoding signal systems.",
                    "label": 1
                },
                {
                    "sent": "The algorithm as well as the proof is in the paper.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also consider independent system an example of an independent system is the property of this activity.",
                    "label": 0
                },
                {
                    "sent": "So if you take an inside game graph and you remove some of the images from it, you still get any exactly graph, so the property is preserved the independence system.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independent systems we showed that there is this factor approximation algorithm for decoding independent systems.",
                    "label": 0
                },
                {
                    "sent": "So all the structures that we consider in this work fall into one of these categories, so you could do the prediction step using these approximation algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the prediction.",
                    "label": 0
                },
                {
                    "sent": "But now let's get into applications.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with a very simple application which is a multi class classification machine learning.",
                    "label": 0
                },
                {
                    "sent": "So as I said, you first need to come up with a finite dimensional embedding of the output space.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have four classes, so this is basically an embedding of your entire office space, so it's number one is at the position of the class and the remaining remaining weeks ago.",
                    "label": 0
                },
                {
                    "sent": "Now it's not easy.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult to see that we can compute the board.",
                    "label": 0
                },
                {
                    "sent": "Vector fee is just summing up all the all the outputs, right?",
                    "label": 0
                },
                {
                    "sent": "So you get an identity if a vector with all months.",
                    "label": 0
                },
                {
                    "sent": "And then similarly you can compute the matrix C, which is just identity vector.",
                    "label": 0
                },
                {
                    "sent": "You take the outer product of these of these metrics and that's it.",
                    "label": 0
                },
                {
                    "sent": "Decoding is trivial because you just need to enumerate over all classes.",
                    "label": 1
                },
                {
                    "sent": "So this is a very simple example of how we could do multiclass classification using our approach.",
                    "label": 0
                },
                {
                    "sent": "So once again, you need to compute.",
                    "label": 0
                },
                {
                    "sent": "You need to come up with the finite dimensional output embedding you need to compute those sums.",
                    "label": 0
                },
                {
                    "sent": "The bold vector fee and the metrics see.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you do multi label classification?",
                    "label": 0
                },
                {
                    "sent": "Now the output space is exponential exponential in the number of labels that you have OK. And.",
                    "label": 0
                },
                {
                    "sent": "I won't go through the derivation here, but then it's also possible to compute those two quantities in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "Decoding is not trivial, but still you could do it in polynomial time using this particular expression.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how you solve multi level.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fordyce circles it becomes a bit more complicated, so here the output embedding is basically your edges and symmetrics.",
                    "label": 0
                },
                {
                    "sent": "So if you have K vertices.",
                    "label": 0
                },
                {
                    "sent": "You come out you.",
                    "label": 0
                },
                {
                    "sent": "You compute your Carcross K adjacency matrix, and that's your output memory.",
                    "label": 1
                },
                {
                    "sent": "So that's minus 101 edges and symmetrics.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These equations are not straightforward, but then it is still possible to compute these two quantities.",
                    "label": 0
                },
                {
                    "sent": "The bold, better with the bold vector fee is basically 0 because it's a sibling system.",
                    "label": 0
                },
                {
                    "sent": "So for every edges and symmetrics you have another adjacency matrix which is it's circling.",
                    "label": 0
                },
                {
                    "sent": "To sum them up, you get 0.",
                    "label": 0
                },
                {
                    "sent": "That's it actually simplifies the problem or not.",
                    "label": 0
                },
                {
                    "sent": "Cimetrix is difficult to compute your for every edge you are basically trying to.",
                    "label": 0
                },
                {
                    "sent": "Count the number of cycles that has this particular edge and then with little bit of work you will be able to come up with those expressions.",
                    "label": 0
                },
                {
                    "sent": "So once again, I have shown you how to how to compute.",
                    "label": 0
                },
                {
                    "sent": "Define a dimensional embedding.",
                    "label": 0
                },
                {
                    "sent": "How to compute those sums?",
                    "label": 0
                },
                {
                    "sent": "The bold vector fee and the metric see.",
                    "label": 0
                },
                {
                    "sent": "And then you plug in these quantities into the optimization problem and you're done.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's how you do it.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prediction and there are a lot of other examples that you will find in the paper, so you could do ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "You could hierarchical classification, you could predict partially ordered sets.",
                    "label": 1
                },
                {
                    "sent": "You could do predict permutations, so predicting permutations is what is called label ranking in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And then you could also predict other classes of graphs.",
                    "label": 0
                },
                {
                    "sent": "But of course I'm not going to go through the details here.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's have a quick look at the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have some results on multi label classification.",
                    "label": 0
                },
                {
                    "sent": "We made some comparisons with multi label SVM and we obtain some good results here on the Hamming loss in the ranking loss.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have some results on hierarchical classifications of, so we compare this office our algorithms.",
                    "label": 0
                },
                {
                    "sent": "So that's the name of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We compared it with the different other.",
                    "label": 0
                },
                {
                    "sent": "Models for hierarchical classification taken from a paper, and we obtained good results on the different class functions.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So predicting bicycles is basically an artificial simulation.",
                    "label": 0
                },
                {
                    "sent": "We don't really have any real world datasets here, so the goal is to predict the cyclic tour of different people.",
                    "label": 1
                },
                {
                    "sent": "As I already explained, and now we assume that there is some policy that this person takes.",
                    "label": 0
                },
                {
                    "sent": "Another goal is to infer this policy.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we compared.",
                    "label": 0
                },
                {
                    "sent": "Bicycle policy estimation with SPM struck so you cannot solve this problem.",
                    "label": 1
                },
                {
                    "sent": "You cannot train SVM struct efficiently with this for this problem because there is no decoding.",
                    "label": 0
                },
                {
                    "sent": "Decoding is not possible.",
                    "label": 0
                },
                {
                    "sent": "So what we did was we used approximation algorithms for decoding.",
                    "label": 0
                },
                {
                    "sent": "So inside SVM struct we use an approximation algorithm to compute the argmax decoding problem.",
                    "label": 0
                },
                {
                    "sent": "In our case, we don't really need this because we don't use any approximation algorithms for learning, and then you can see the comparisons.",
                    "label": 0
                },
                {
                    "sent": "So the cosine measure is the higher the better so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the comparison with the stream structure.",
                    "label": 0
                },
                {
                    "sent": "As expected as the number of training instances grew, the cosine measure was going up.",
                    "label": 0
                },
                {
                    "sent": "Of course, for both the algorithms.",
                    "label": 0
                },
                {
                    "sent": "And the results are significant.",
                    "label": 0
                },
                {
                    "sent": "As you can see from the very fast.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what am I currently working on?",
                    "label": 0
                },
                {
                    "sent": "I'm actually trying to come up with probabilistic models because as I told you, I'm still minimizing a second order Taylor expansion, which is a quadratic loss which is.",
                    "label": 0
                },
                {
                    "sent": "Which is not as good as the Max margin hinge loss, right?",
                    "label": 0
                },
                {
                    "sent": "So it would be nice if I can, for example, minimize the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm currently working on.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to come up with probabilistic models then you need to come up with sampling techniques, because once you have a probabilistic model, you need to do the prediction step, which is, you know the sampling techniques.",
                    "label": 0
                },
                {
                    "sent": "So for this I will use.",
                    "label": 0
                },
                {
                    "sent": "I'm using Markov chain Monte Carlo methods, so I'm not just interested in using some MCMC technique, but I'm also interested in coming up with provable guarantees for mixing time, so I want to know how fast my my chain is going to converge to stationary distribution, so that is what I'm working on at the moment.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's it, I'm done, thanks.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Books.",
                    "label": 0
                },
                {
                    "sent": "It was hardly been used.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, but one of the main reasons of the motivation behind using now that I have time, I think I can go through the approximation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from from our learned model, we cannot ensure that the weights that are given to the edges are non negative.",
                    "label": 0
                },
                {
                    "sent": "So if the way it's given to the edges are non negative, most of the existing approximation algorithms fail.",
                    "label": 0
                },
                {
                    "sent": "So in order to circumvent this problem, we had to use the approximation that is the main reason.",
                    "label": 0
                },
                {
                    "sent": "And I have given some concrete examples in the paper.",
                    "label": 0
                },
                {
                    "sent": "And then of course there are other.",
                    "label": 0
                },
                {
                    "sent": "You know, these are the properties of the approximation, but.",
                    "label": 0
                },
                {
                    "sent": "But that's as I said, the main reason was to circumvent this.",
                    "label": 0
                },
                {
                    "sent": "To handle this non negative.",
                    "label": 0
                },
                {
                    "sent": "Sorry, negative it weights on the edges.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                },
                {
                    "sent": "Move on to the next speaker.",
                    "label": 0
                }
            ]
        }
    }
}