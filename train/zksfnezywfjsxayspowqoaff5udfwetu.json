{
    "id": "zksfnezywfjsxayspowqoaff5udfwetu",
    "title": "Poster: Knowledge as a Constraint on Uncertainty for Unsupervised Classification: A Study in Part-of-Speech Tagging",
    "info": {
        "author": [
            "Thomas J. Murray, University of Southern California"
        ],
        "published": "Aug. 11, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/icml08_murray_kcu/",
    "segmentation": [
        [
            "OK, right, so some interest in the setting where we're doing unsupervised learning, but we have some knowledge source which is able to limit or bias the classifier choices for each given input."
        ],
        [
            "So this study is in very simple part of speech tagging.",
            "So in that setting we you know we have a full tag set and our knowledge sources able to limit into at least in some cases is able to limit the set of choices that the classifier might know.",
            "So the question that I'm really interested in here is there's the knowledge increases and were able to limit that distribution more and more.",
            "How well does our reduction in the entropy of that distribution predict the final model performance?",
            "An equivalently what that's basically asking is.",
            "As we're designing the knowledge, how precise do we have to be to get good model performance in the end?",
            "And then I also evaluate other other effects of learning in this constraint parameter space stability with respect to initialization conditions, convergence of training and also label assignment, which I'll explain explain."
        ],
        [
            "So the basic view that I'm taking is that we have a given input X.",
            "We have some prior knowledge which can join constrains the choice of label Y, and we're going to use this as a conditional distribution.",
            "PY given X, and then of course we can compute the conditional entropy and view that as a measure of task difficulty.",
            "The nice thing about this is that we don't require labeled data because all we need is the outputs of that of that knowledge source.",
            "Label you'll give him his people given extra every X. Microsoft labeling, I mean it's not like if one of the most one and the other is 0 so sure.",
            "Sure yeah, right wait.",
            "So this encompases unsupervised learning and purely supervised right?",
            "So if it's if it's one, if it's if it's positive only for one value, then it's supervised.",
            "If it's uniform, if it's uniform, which is the maximum entropy for a given cardinality, then it is pure unsupervised, right?",
            "So this is kind of in between were kind of exploring that that continuum right?",
            "OK so, but the thing to note is that we're really just looking at the different types of knowledge.",
            "We're fixing the model type, 'cause obviously I'm not this entropy of this knowledge is not going to predict the relative performance of different model families, right?",
            "So I'm just fixing a very basic.",
            "Basic model type.",
            "An interesting I just happened upon this actually recently is that.",
            "I mean, it's intuitive that if we if the model has fewer choices, it's easier to learn there's actually a result in information theory that proves this.",
            "Is that that conditional entropy?",
            "This conditional entropy is part of a lower bound on the probability of error of our classifier.",
            "So that's kind of interesting as a special case when you supervise, it's easier to learn than when unsupervised, right?",
            "Extreme case.",
            "Yeah, sure."
        ],
        [
            "Yeah, sure, right?",
            "So for the constraints, what?",
            "What I'm interested in.",
            "The reason I'm doing this is is I really want to apply unsupervised learning, but I don't want unsupervised learning performance.",
            "I want something closer to supervised learning.",
            "So So what I tried to do is I tried to simulate instead of use of cheating and using the corpus as much as possible.",
            "I tried to use general knowledge to form these constraints.",
            "The first one is just some basic punctuation numbers and capitalization.",
            "These these are a little bit more interesting because I did them as first order constraints so that a number is something that has a digit that's obvious, but also something that follows another.",
            "Also allowed it to be tagged as a number if it follows something else which might be tagged as a number, so that kind of cascades, now with vanishing probabilities we get away from the digit.",
            "The reason I do that is because when we have 123 million.",
            "123 is written as a digit and the million is still tagged as a number and so so we need to allow allow that so the closed classes, list of prepositions, and so on.",
            "But again, I got those from independent resources, not from, not from the Penn Treebank, the only the only place where I started cheating was this final top words, but I tried to make it more realistic where it was just the top, the most common 100 or 200 words which were moving to Swahili or something like that.",
            "It is still fairly feasible that we could take.",
            "We can look at the data.",
            "Get the 100 or 200 most common words and say OK, that could be a preposition or this and so on.",
            "OK so.",
            "OK, so this one is for the top 100 or 200 words that we get.",
            "Most frequent words.",
            "We give the list of possible tax, so this one is when I did this.",
            "Because it's not.",
            "Yeah, that's what that's what's commonly done and still called unsupervised learning.",
            "I mean, this is this is this is this is in purely unsupervised learning either, but what I was looking at the reason I did this is I want it to be more feasible.",
            "That's what I'm saying.",
            "It's like if you if you tell me you want me to tag Swahili, right?",
            "I'm not going to go and annotate, annotate a million words, right?",
            "But it is feasible for me to annotate 100 or 200 words, and so that's what I was trying to see is.",
            "How much does that help if we do a very limited amount of annotations?",
            "Naturally get their most frequent words occurring right.",
            "You will just get that right, but I might not get all of the tags too.",
            "So yeah, there are lots of different ways of doing it, Sir, but so I was just trying to.",
            "I was looking at these specific this specific settings.",
            "You know I wasn't trying to get into the semi supervised setting or anything like that.",
            "But yeah, I expect you're right.",
            "That is.",
            "Yeah, that's it.",
            "Doesn't different set?",
            "Yes, it's a different setting of like comparing this tradeoff between how much time I need to spend on annotating.",
            "It's kind of what you are getting at here.",
            "Talk to, right?"
        ],
        [
            "So OK, so very simple, straightforward 1st order.",
            "Hmm, I did ten runs of each one starting with the uniform distribution and then perturbing it slightly and then.",
            "As is common in this type of unsupervised tagging work, we need to map the internal states of the HMM to labels to do evaluation, and I compared both the many to one and one to one heuristics as defined by Johnson.",
            "OK, so yeah, so the many want to one allows us to map multiple states in the HMM to a single label with.",
            "That allows us to do is kind of discard the rare labels an map multiple contexts.",
            "So the problem with that is that you can view it as cheating, because if I push up the number of states.",
            "200 say and I only have 40 tags, then I can start, you know, pushing all these labels so but I help them fix the equal to them.",
            "So I don't think it's quite so much cheating, but."
        ],
        [
            "I compared the two.",
            "OK, so the first thing is that even though even this one is fairly simple, fairly simple amount of constraint, we still get granted were still at the low performance spectrum, but we still get like 30% improvement, which is pretty.",
            "I was surprised that it was so good for that.",
            "We also get a fairly high degree of correlation with respect to the entropy and the performance, and so this is what I was kind of getting at.",
            "Is how strong this is now.",
            "It's not correlated, and I wouldn't expect it fully to be, because I don't think it's accounting for everything that's going into the final performance.",
            "But but it is fairly strong, and because we don't need labeled data for this, it is kind of encouraging that we can get some idea of, you know how much more knowledge we need to put in the system to get."
        ],
        [
            "Get this is probably possibly the most general, generally applicable result because you know this is for our first order HMM and no one's actually going to use this, But what's interesting is that this is of course a huge problem for many alium methods, and lots of other unsupervised methods.",
            "Is this sensitivity to initialization conditions, and there's actually a recent paper that showed that if you choose it, use some other methods to bootstrap that initialization thing.",
            "You can get incredibly better results.",
            "You know it's very sensitive.",
            "So what's interesting is that as you start adding these constraints, it really goes down like this.",
            "One was like a 14% accuracy range between between the maximin over over 10 experiments, so you really have no idea and you really need to start having some held out set to be able to choose the right model, whereas you start to get down here with the more constrained ones were really getting a range of within a half a percentage point or so on, so it's really not so horrible if we even choose the worst one.",
            "That's that's kind of a nice result there.",
            "This is this is our convergence.",
            "How quickly converges again much more quickly for the unconstrained, you know we have this very long curve here, it's it's really converged with after only tenant iterations."
        ],
        [
            "The last thing this is.",
            "Label assignment so in in one viewpoint that the label assignment we do here is purely for evaluation, and so it's kind of an artificial problem.",
            "We don't really need to do it, but it's only to evaluate our unsupervised models.",
            "However, if we want to get output from the model that's interpretable that you know that's not State 7, but it says determiner.",
            "Or if we need to feed that into the cascading pipeline that you're talking about, we need to feed it to some other model that is trained on.",
            "DT Anne Anne, Anne Anne and all this.",
            "Then we do actually need to get this, which means we have the problem of how we actually make that label assignment right and for the result here and everything put in the literature.",
            "People are always using the full, unsupervised, unsupervised training set.",
            "The real labels to make that optimal label assignment.",
            "So the question here is, as I start taking a tiny fraction of the full data to make the label mapping how much more it is the accuracy to grade right?",
            "You don't need to major label assignment, you would just put all of your outfits features.",
            "Right, but if you want it, yeah, so it depends what you're feeding it to right?",
            "And maybe maybe it's arbitrary.",
            "But yeah, if it's if it's a cascade of purely unsupervised models, then you're right.",
            "It doesn't matter, it can.",
            "It can be trained.",
            "Supervised him doing something else.",
            "You could just give it all out, right?",
            "Yeah yeah it it depends, but I think especially for the for people needing to interpret this is less artificial.",
            "Maybe in the cascade it starts a little artificial.",
            "Maybe that you have unsupervised learning at one stage and at the higher stage have supervised learning But anyway so but it is interesting because when we're doing this label assignment to make to show these really high performance numbers on supervised learning, it is really it is really requiring that a lot of data.",
            "To make that optimal label assignment, but again with the constraints it makes for less, right?",
            "So when we have 2% of the data, the more constrained models are getting to within 90% of their best month of their best performance, right?",
            "And even even like this 10%, that's already 19,000 words that I have to annotate to be able to get a mapping that gets me to 90% of them.",
            "So that's what this is.",
            "In one sense it maybe it's a little bit of artificial problem.",
            "It's kind of interesting that you know if I do want to have some interpretable output that again constraining it because I was just I was just talking to Mark Johnson about this because it's interesting because a lot of unsupervised tagging work.",
            "They assume partial, either full or partial tagging dictionaries, right?",
            "So it's really not so unsupervised, and people are now starting to call it semi supervised, which I think is a better, better label for it.",
            "But for those works they don't have to do this label mapping.",
            "So it's almost like there's this state change.",
            "Where when you have very little information, your system is selling constrained that you do have to do this mapping, whereas when you give it enough information to constrain it then you then you can use the States as if they are labels and you don't have to re map them right because you because you are so constrained.",
            "It's kind of interesting."
        ],
        [
            "So I think that's that's the final thing is, so they take home messages that you know this view of entropy is fairly predictive, and it has lots of other training benefits, and that's it.",
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, right, so some interest in the setting where we're doing unsupervised learning, but we have some knowledge source which is able to limit or bias the classifier choices for each given input.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this study is in very simple part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "So in that setting we you know we have a full tag set and our knowledge sources able to limit into at least in some cases is able to limit the set of choices that the classifier might know.",
                    "label": 0
                },
                {
                    "sent": "So the question that I'm really interested in here is there's the knowledge increases and were able to limit that distribution more and more.",
                    "label": 0
                },
                {
                    "sent": "How well does our reduction in the entropy of that distribution predict the final model performance?",
                    "label": 1
                },
                {
                    "sent": "An equivalently what that's basically asking is.",
                    "label": 1
                },
                {
                    "sent": "As we're designing the knowledge, how precise do we have to be to get good model performance in the end?",
                    "label": 1
                },
                {
                    "sent": "And then I also evaluate other other effects of learning in this constraint parameter space stability with respect to initialization conditions, convergence of training and also label assignment, which I'll explain explain.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic view that I'm taking is that we have a given input X.",
                    "label": 1
                },
                {
                    "sent": "We have some prior knowledge which can join constrains the choice of label Y, and we're going to use this as a conditional distribution.",
                    "label": 1
                },
                {
                    "sent": "PY given X, and then of course we can compute the conditional entropy and view that as a measure of task difficulty.",
                    "label": 1
                },
                {
                    "sent": "The nice thing about this is that we don't require labeled data because all we need is the outputs of that of that knowledge source.",
                    "label": 0
                },
                {
                    "sent": "Label you'll give him his people given extra every X. Microsoft labeling, I mean it's not like if one of the most one and the other is 0 so sure.",
                    "label": 0
                },
                {
                    "sent": "Sure yeah, right wait.",
                    "label": 0
                },
                {
                    "sent": "So this encompases unsupervised learning and purely supervised right?",
                    "label": 0
                },
                {
                    "sent": "So if it's if it's one, if it's if it's positive only for one value, then it's supervised.",
                    "label": 1
                },
                {
                    "sent": "If it's uniform, if it's uniform, which is the maximum entropy for a given cardinality, then it is pure unsupervised, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of in between were kind of exploring that that continuum right?",
                    "label": 0
                },
                {
                    "sent": "OK so, but the thing to note is that we're really just looking at the different types of knowledge.",
                    "label": 0
                },
                {
                    "sent": "We're fixing the model type, 'cause obviously I'm not this entropy of this knowledge is not going to predict the relative performance of different model families, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just fixing a very basic.",
                    "label": 0
                },
                {
                    "sent": "Basic model type.",
                    "label": 0
                },
                {
                    "sent": "An interesting I just happened upon this actually recently is that.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's intuitive that if we if the model has fewer choices, it's easier to learn there's actually a result in information theory that proves this.",
                    "label": 0
                },
                {
                    "sent": "Is that that conditional entropy?",
                    "label": 0
                },
                {
                    "sent": "This conditional entropy is part of a lower bound on the probability of error of our classifier.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of interesting as a special case when you supervise, it's easier to learn than when unsupervised, right?",
                    "label": 0
                },
                {
                    "sent": "Extreme case.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, sure, right?",
                    "label": 0
                },
                {
                    "sent": "So for the constraints, what?",
                    "label": 0
                },
                {
                    "sent": "What I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "The reason I'm doing this is is I really want to apply unsupervised learning, but I don't want unsupervised learning performance.",
                    "label": 0
                },
                {
                    "sent": "I want something closer to supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So So what I tried to do is I tried to simulate instead of use of cheating and using the corpus as much as possible.",
                    "label": 1
                },
                {
                    "sent": "I tried to use general knowledge to form these constraints.",
                    "label": 1
                },
                {
                    "sent": "The first one is just some basic punctuation numbers and capitalization.",
                    "label": 0
                },
                {
                    "sent": "These these are a little bit more interesting because I did them as first order constraints so that a number is something that has a digit that's obvious, but also something that follows another.",
                    "label": 0
                },
                {
                    "sent": "Also allowed it to be tagged as a number if it follows something else which might be tagged as a number, so that kind of cascades, now with vanishing probabilities we get away from the digit.",
                    "label": 1
                },
                {
                    "sent": "The reason I do that is because when we have 123 million.",
                    "label": 0
                },
                {
                    "sent": "123 is written as a digit and the million is still tagged as a number and so so we need to allow allow that so the closed classes, list of prepositions, and so on.",
                    "label": 0
                },
                {
                    "sent": "But again, I got those from independent resources, not from, not from the Penn Treebank, the only the only place where I started cheating was this final top words, but I tried to make it more realistic where it was just the top, the most common 100 or 200 words which were moving to Swahili or something like that.",
                    "label": 0
                },
                {
                    "sent": "It is still fairly feasible that we could take.",
                    "label": 0
                },
                {
                    "sent": "We can look at the data.",
                    "label": 0
                },
                {
                    "sent": "Get the 100 or 200 most common words and say OK, that could be a preposition or this and so on.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one is for the top 100 or 200 words that we get.",
                    "label": 0
                },
                {
                    "sent": "Most frequent words.",
                    "label": 0
                },
                {
                    "sent": "We give the list of possible tax, so this one is when I did this.",
                    "label": 0
                },
                {
                    "sent": "Because it's not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what that's what's commonly done and still called unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is this is this is this is in purely unsupervised learning either, but what I was looking at the reason I did this is I want it to be more feasible.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "It's like if you if you tell me you want me to tag Swahili, right?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go and annotate, annotate a million words, right?",
                    "label": 0
                },
                {
                    "sent": "But it is feasible for me to annotate 100 or 200 words, and so that's what I was trying to see is.",
                    "label": 0
                },
                {
                    "sent": "How much does that help if we do a very limited amount of annotations?",
                    "label": 0
                },
                {
                    "sent": "Naturally get their most frequent words occurring right.",
                    "label": 0
                },
                {
                    "sent": "You will just get that right, but I might not get all of the tags too.",
                    "label": 0
                },
                {
                    "sent": "So yeah, there are lots of different ways of doing it, Sir, but so I was just trying to.",
                    "label": 0
                },
                {
                    "sent": "I was looking at these specific this specific settings.",
                    "label": 0
                },
                {
                    "sent": "You know I wasn't trying to get into the semi supervised setting or anything like that.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I expect you're right.",
                    "label": 0
                },
                {
                    "sent": "That is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's it.",
                    "label": 0
                },
                {
                    "sent": "Doesn't different set?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a different setting of like comparing this tradeoff between how much time I need to spend on annotating.",
                    "label": 0
                },
                {
                    "sent": "It's kind of what you are getting at here.",
                    "label": 0
                },
                {
                    "sent": "Talk to, right?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so very simple, straightforward 1st order.",
                    "label": 0
                },
                {
                    "sent": "Hmm, I did ten runs of each one starting with the uniform distribution and then perturbing it slightly and then.",
                    "label": 0
                },
                {
                    "sent": "As is common in this type of unsupervised tagging work, we need to map the internal states of the HMM to labels to do evaluation, and I compared both the many to one and one to one heuristics as defined by Johnson.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, so the many want to one allows us to map multiple states in the HMM to a single label with.",
                    "label": 0
                },
                {
                    "sent": "That allows us to do is kind of discard the rare labels an map multiple contexts.",
                    "label": 0
                },
                {
                    "sent": "So the problem with that is that you can view it as cheating, because if I push up the number of states.",
                    "label": 0
                },
                {
                    "sent": "200 say and I only have 40 tags, then I can start, you know, pushing all these labels so but I help them fix the equal to them.",
                    "label": 0
                },
                {
                    "sent": "So I don't think it's quite so much cheating, but.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I compared the two.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing is that even though even this one is fairly simple, fairly simple amount of constraint, we still get granted were still at the low performance spectrum, but we still get like 30% improvement, which is pretty.",
                    "label": 0
                },
                {
                    "sent": "I was surprised that it was so good for that.",
                    "label": 0
                },
                {
                    "sent": "We also get a fairly high degree of correlation with respect to the entropy and the performance, and so this is what I was kind of getting at.",
                    "label": 0
                },
                {
                    "sent": "Is how strong this is now.",
                    "label": 0
                },
                {
                    "sent": "It's not correlated, and I wouldn't expect it fully to be, because I don't think it's accounting for everything that's going into the final performance.",
                    "label": 0
                },
                {
                    "sent": "But but it is fairly strong, and because we don't need labeled data for this, it is kind of encouraging that we can get some idea of, you know how much more knowledge we need to put in the system to get.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get this is probably possibly the most general, generally applicable result because you know this is for our first order HMM and no one's actually going to use this, But what's interesting is that this is of course a huge problem for many alium methods, and lots of other unsupervised methods.",
                    "label": 0
                },
                {
                    "sent": "Is this sensitivity to initialization conditions, and there's actually a recent paper that showed that if you choose it, use some other methods to bootstrap that initialization thing.",
                    "label": 0
                },
                {
                    "sent": "You can get incredibly better results.",
                    "label": 0
                },
                {
                    "sent": "You know it's very sensitive.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting is that as you start adding these constraints, it really goes down like this.",
                    "label": 0
                },
                {
                    "sent": "One was like a 14% accuracy range between between the maximin over over 10 experiments, so you really have no idea and you really need to start having some held out set to be able to choose the right model, whereas you start to get down here with the more constrained ones were really getting a range of within a half a percentage point or so on, so it's really not so horrible if we even choose the worst one.",
                    "label": 0
                },
                {
                    "sent": "That's that's kind of a nice result there.",
                    "label": 0
                },
                {
                    "sent": "This is this is our convergence.",
                    "label": 0
                },
                {
                    "sent": "How quickly converges again much more quickly for the unconstrained, you know we have this very long curve here, it's it's really converged with after only tenant iterations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last thing this is.",
                    "label": 0
                },
                {
                    "sent": "Label assignment so in in one viewpoint that the label assignment we do here is purely for evaluation, and so it's kind of an artificial problem.",
                    "label": 0
                },
                {
                    "sent": "We don't really need to do it, but it's only to evaluate our unsupervised models.",
                    "label": 0
                },
                {
                    "sent": "However, if we want to get output from the model that's interpretable that you know that's not State 7, but it says determiner.",
                    "label": 0
                },
                {
                    "sent": "Or if we need to feed that into the cascading pipeline that you're talking about, we need to feed it to some other model that is trained on.",
                    "label": 1
                },
                {
                    "sent": "DT Anne Anne, Anne Anne and all this.",
                    "label": 0
                },
                {
                    "sent": "Then we do actually need to get this, which means we have the problem of how we actually make that label assignment right and for the result here and everything put in the literature.",
                    "label": 0
                },
                {
                    "sent": "People are always using the full, unsupervised, unsupervised training set.",
                    "label": 0
                },
                {
                    "sent": "The real labels to make that optimal label assignment.",
                    "label": 1
                },
                {
                    "sent": "So the question here is, as I start taking a tiny fraction of the full data to make the label mapping how much more it is the accuracy to grade right?",
                    "label": 0
                },
                {
                    "sent": "You don't need to major label assignment, you would just put all of your outfits features.",
                    "label": 0
                },
                {
                    "sent": "Right, but if you want it, yeah, so it depends what you're feeding it to right?",
                    "label": 0
                },
                {
                    "sent": "And maybe maybe it's arbitrary.",
                    "label": 0
                },
                {
                    "sent": "But yeah, if it's if it's a cascade of purely unsupervised models, then you're right.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter, it can.",
                    "label": 0
                },
                {
                    "sent": "It can be trained.",
                    "label": 0
                },
                {
                    "sent": "Supervised him doing something else.",
                    "label": 0
                },
                {
                    "sent": "You could just give it all out, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah it it depends, but I think especially for the for people needing to interpret this is less artificial.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the cascade it starts a little artificial.",
                    "label": 0
                },
                {
                    "sent": "Maybe that you have unsupervised learning at one stage and at the higher stage have supervised learning But anyway so but it is interesting because when we're doing this label assignment to make to show these really high performance numbers on supervised learning, it is really it is really requiring that a lot of data.",
                    "label": 0
                },
                {
                    "sent": "To make that optimal label assignment, but again with the constraints it makes for less, right?",
                    "label": 0
                },
                {
                    "sent": "So when we have 2% of the data, the more constrained models are getting to within 90% of their best month of their best performance, right?",
                    "label": 0
                },
                {
                    "sent": "And even even like this 10%, that's already 19,000 words that I have to annotate to be able to get a mapping that gets me to 90% of them.",
                    "label": 0
                },
                {
                    "sent": "So that's what this is.",
                    "label": 0
                },
                {
                    "sent": "In one sense it maybe it's a little bit of artificial problem.",
                    "label": 0
                },
                {
                    "sent": "It's kind of interesting that you know if I do want to have some interpretable output that again constraining it because I was just I was just talking to Mark Johnson about this because it's interesting because a lot of unsupervised tagging work.",
                    "label": 0
                },
                {
                    "sent": "They assume partial, either full or partial tagging dictionaries, right?",
                    "label": 0
                },
                {
                    "sent": "So it's really not so unsupervised, and people are now starting to call it semi supervised, which I think is a better, better label for it.",
                    "label": 0
                },
                {
                    "sent": "But for those works they don't have to do this label mapping.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like there's this state change.",
                    "label": 0
                },
                {
                    "sent": "Where when you have very little information, your system is selling constrained that you do have to do this mapping, whereas when you give it enough information to constrain it then you then you can use the States as if they are labels and you don't have to re map them right because you because you are so constrained.",
                    "label": 0
                },
                {
                    "sent": "It's kind of interesting.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think that's that's the final thing is, so they take home messages that you know this view of entropy is fairly predictive, and it has lots of other training benefits, and that's it.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}