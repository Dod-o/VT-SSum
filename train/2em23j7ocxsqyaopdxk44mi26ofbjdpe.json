{
    "id": "2em23j7ocxsqyaopdxk44mi26ofbjdpe",
    "title": "Spectral Clustering",
    "info": {
        "author": [
            "Arik Azran, Department of Engineering, University of Cambridge"
        ],
        "published": "March 3, 2008",
        "recorded": "October 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlcued08_azran_mcl/",
    "segmentation": [
        [
            "Thank you Ruben.",
            "OK, so to talk about bigger clustering."
        ],
        [
            "When we talk about clustering, it's it's R and there are some theoretical result details that says that clustering is something that you can't really define mathematically defined, but the kind of clustering.",
            "But I'm going to talk about is the kind of clustering that, like the topic title yourself, that you can you know if it's good when you see.",
            "Um, one kind of clustering would be the one on the left where you can model the data using some probabilistic model.",
            "For example, in this case you could use a mixture of Gaussians and you could do very nice, interesting with very nice things with with probabilistic model.",
            "But it's not always something that you can use, like for example the data set that we have on the left.",
            "We see that there are clusters.",
            "There are groups of data points.",
            "But for me it's a little more a little more hard, little more difficult to model.",
            "This data points away from the probabilistic probabilistic model, so the kind of partitioning that we're looking for.",
            "Is both basically, but we're going to focus mainly on this kind of datasets, where spectral clustering really has an edge over other methods."
        ],
        [
            "Before I begin, I want to say something about star clustering in general and what is not going to be.",
            "What I'm not going to be talking about in this tutorial, so when we talk about spectral clustering clustering, we basically have two stages in the in the in the technique.",
            "First of all, what we do is we take the data, set the input and we represent it as a graph.",
            "This is an extremely important part.",
            "If you don't do it properly, you're going to.",
            "You're not going to get any useful result.",
            "I'm not going to talk about this part.",
            "I'm going to talk about the second part though.",
            "The one that's given the given the graph we need to find a good partitioning of the graph.",
            "The reason I'm not going to talk about this this stage here is because it's largely problem dependent.",
            "For each problem, we're going to have some kind of expertise that tells you this is how we need to measure similarity between data points, for example.",
            "We can think about.",
            "How to set of images?",
            "If I'm giving you a set of images and I'm asking you to do cluster this images into groups, you could somehow put an edge between put an edge between similar points by saying the two images are similar.",
            "If for example the pixelwise difference between them is small or the Instagram for both images look similar.",
            "This is one way of doing that.",
            "But then if I told you that these images are basically taken out of a movie for film, that you have an additional information here that tells you that you basically have a time series.",
            "And you need to use this to construct your graph, so the same data set images you might want to build one graph if it's just a sequence of images, you might want.",
            "You might want to build a different graph if you know that this is a time series time series.",
            "So this is basically the stage the step of building the graph representing the data using a graph is largely problem problem dependent, and I'm not going to say too much about it just a little bit, but not too much, and I am going to focus about the problem that given the graph, how can we find good subgraph or good partition?"
        ],
        [
            "Of this graph.",
            "The way the way I see the way I planned the talk is by first I'm going to talk a little bit about graph partitioning.",
            "I'm going to talk like a couple of slides about what are the major or the most important quantities in a graph that we were going to talk about.",
            "Then I'm going to talk a little bit about the algorithms the way we actually find the partitioning.",
            "I'll try and give you some insight about why and by knowing why we can also try and understand when the algorithms are expected to work, whether or not we will then move to what I call yourself tuning algorithm.",
            "This is something that we did here last year.",
            "And I'm going to finish with the practicalities of are we going to need to or how we efficiently can implement this this."
        ],
        [
            "So this technique and I'm going to begin with some terminology about graphs.",
            "We have a graph in with OK, so when I when I'm talking about graph the data, the items that I'm given the data points are the nodes of the graph.",
            "The edges are the entity that I'm going to introduce by to build the graph and the edges are going to be weighted by some affinity function that measures the distance between or.",
            "The similarity between the data items.",
            "For example, what we see here is that if South is 1 item and SM is.",
            "Is the 2nd item.",
            "The edge between them.",
            "Is weight weighted by the radial basis function?",
            "In this case where we plug here into the radial basis function, is the difference between the two items.",
            "This means that if South and SM are close to each other similar, they have a small distance between them, then the affinity between NWM is going to be.",
            "It's going to be close to one in this case, if Sanon SM are distant from each other, then WNM is going to be close to 0.",
            "And not volume or no degree is the sum of all of the edges that are connected to the node.",
            "So DN and DN is a crucial is a critical quantity quantity and what I'm going to say.",
            "So it's important to remember it.",
            "The end is the sum of all edges that are connected to the end node.",
            "This is the this is the volume of the node volume of a cluster is to compute the volume of a cluster.",
            "What we need to do is simply.",
            "Go over all of the nodes in the cluster and sum the volumes so we basically go over each node and some the edges that are connected.",
            "To that node at the cut between two clusters of two subgraphs.",
            "If we have this is a graph and we want to partition this graph into one sub graph in the second sub graph.",
            "Then the cut is the sum of the edges that we need to take out together partitioning."
        ],
        [
            "Now we can think about different kind of graph cuts or different kind of criteria to define the graph cut or to use the graph to find the partitioning, one would simply be to find a partitioning we want to cluster data or partitioning, partitioning, graph.",
            "We can look for the partitioning or that minimizes the cut.",
            "The partitioning that if this is the graph we're looking for, the partitioning that minimizes the edges, the weights of the edges that we need to take out to get the partitioning.",
            "In this case what it means is that we need to take out this edge.",
            "And if we want to partition this data, this graph into two subgraphs, we're going to end up having one sub graph here and the other one being only the soul node at the side.",
            "This is not a very good criteria to have reasonable partitioning of graphs are good partitioning of graphs, and the reason here is that the reason is that it tends to cut out nodes that are on the edge of the data set, and if you have an outlier, if you have one point which is far away, then it will.",
            "It will end up if you implement this right here.",
            "This criteria you're going to end up separating between this outlier and the rest of the data points.",
            "The alternative would be the normalized cut.",
            "The normalized cut is using the cut itself.",
            "This is exactly the same.",
            "Cat is here, but it probably, but it also introduced a tradeoff.",
            "It multiplied the cut by this quantity here.",
            "Now this quantity is the sum of 1 over the volume of the first cluster plus one over the volume of the second cluster.",
            "Why?",
            "Why is it doing anything which we might consider good?",
            "It's because that if we look at the volume of a cluster, if this cluster is small and the number of nodes that it includes are small, and the volume of this cluster is small, then the discount is going to be large.",
            "And the tradeoff that we have here is before between having.",
            "Between taking out a small number of edges.",
            "But not ending up having clusters clusters which volume that is too small, we're going to rebalancing between taking a few edges as we can, but still having clusters which are relatively large.",
            "And if you implement for example for this data set, if you implement this criterion, if you optimize this criterion, you can end up having this kind of partitioning of data.",
            "Now finding the solution for.",
            "For minimizing or the solution that minimizes the find out good or or low normalized cut is combinatory.",
            "This is a combinatorial problem, so to find the solution is NP hard.",
            "And we know that we usually want to run away from NPR problems.",
            "And one fight in some approximation approximation that I'm going to talk about here is the spectral graph partitioning.",
            "This is basically an approximation for this problem, and we're going to see different ways of looking at spectral clustering as an approximation for minimizing the normalized cut."
        ],
        [
            "So what is it?",
            "Special clustering?",
            "I once heard somebody told me that, well, we talked about spectral clustering and said yeah, I know what's wrong is you take some affinity matrix, you find its eigenvectors and you do K means over there and that's it.",
            "And he's right.",
            "I mean, you have a lot of algorithms and you have a lot of criterias and you have a lot of different ways of initializing.",
            "There are many papers about special clustering and they have a lot of differences between them.",
            "But the basic thing the basic idea in all of these methods is that you're going to represent your data as a graph, and instead of putting edges you can simply summarize it in a matrix.",
            "And then you're going to find some other metrics.",
            "If this matrix is W, this is the metrics that the empty entry of this matrix would be the affinity between the North and the empty nodes.",
            "So we're going to take the data we're going to define some graph over it by plugging all the affinity values into a matrix.",
            "We're going to find a new matrix, A find its eigenvectors, do something which we call K means in the feature space in this class in this eigenvector space, and that's it, basically.",
            "Older, older or most of the cluster of the string algorithms are based on this idea and the main difference between them is how you define a A from W. And I'll make sense out of all of it.",
            "How can you can use it to extend this kind of basic usage?",
            "Off a.",
            "Now, and I'm going to talk about three different algorithms.",
            "They do exactly this, but with different kind of metrics, is a different kind of of partitioning the data.",
            "I'm going to show some connections between them.",
            "I'm going to try and give you some explanation about how they work, and then I'm going to show we can extend them.",
            "Into a into to make a little bit more than just that.",
            "But I'm going to begin with three different clustering algorithms, so if you have any problems at this point, this is a very good time to ask them."
        ],
        [
            "So you said you're going to focus on the clustering."
        ],
        [
            "So not the other, so there's nothing else to really say I guess.",
            "Or this sort of evidence how to build Infinity matrix mean there's nothing more you can do creatively after you have the edges of the graph.",
            "OK, so the edges of the graphs are the affinity matrix.",
            "Now you construct them is depends a lot about your about the kind of data that you're dealing with.",
            "You can say a lot about how to do that.",
            "There are different applications and in different applications, each of them you can find in a different paper.",
            "You're going to see a different way of constructing this affinity matrix.",
            "There isn't much I can say which is general about it.",
            "I mean, I'm going to use this kind of function here, which is a radial basis function.",
            "This is this is not a very good solution.",
            "I dare say it's far from being optimal, and in many cases it's even silly to do that, but still.",
            "I mean, I always almost always use that and it gives you quite satisfactory results.",
            "And it's only.",
            "I mean I can only hope that if you use a different algorithm.",
            "Different, definitely an affinity function which more which is more suitable for the data that you you can get better results but using instead of this function using a different function, a different finish function.",
            "Again, it depends on the kind of data that is diffuse biological data you might.",
            "You might want to use different kernel then if you use something else like images or if you use something else like graphs themselves.",
            "But it won't matter for our purposes what that function is.",
            "No, it's not, no, it doesn't matter.",
            "The only thing that I'm going to see.",
            "Guarding W that is symmetric, so W&M equals WM and that's it other than that.",
            "What's the reason for backing, separating how you define your affinity and this funky hey?",
            "We're going to see three different ways of defining a from W. Why is it?",
            "It seems to me like.",
            "You define your affinity based.",
            "The distance metric could just as well go into your phone.",
            "Yeah, it's fairly natural wage for them.",
            "So I'm not I didn't really mean to get too formal here when I'm saying funk, I'm just saying we take W, we do something and we get a so you could as well say look let's put your a big box and we say we have some matrix that somehow represent the pairwise similarity between data and that's it.",
            "But the way the algorithm that I'm going to present, they all are all going to be using this W and do something very simple to this W to end up with a different matrix which is a so you can.",
            "Yeah, I mean this funk thing is just.",
            "Some kind of a cartoon.",
            "I'm not going to deal too much with this kind of what exactly what function it is?",
            "Time to transform the graph into a matrix.",
            "You have to order the vertices of of the graph.",
            "Yeah, so I'm not, it doesn't really affect the results, it doesn't because we're going to look at adding vectors and eigen vectors are going to be represented.",
            "This is important, so maybe I'll say another couple of sentences about it.",
            "We're going to index each of these points.",
            "I'm going to put an index about each on each of these points.",
            "It doesn't really matter how we index them, but we assume that we have some say endpoints.",
            "Then we somehow enumerate them and that's it.",
            "It doesn't really matter how we do that, but what's important is that if this is the first point for example, then it corresponds to the first row of W, so WNM.",
            "If they were looking at the first point, W1M would be the affinity between the first point and the end point, and it's going to be summarizing the first row and for the endpoint.",
            "In general, we're going to be looking at the end row of W. Scheme how to deal with new points which arrive after young directors?",
            "So we're going to talk about transductive setting here.",
            "We're not going to.",
            "We're going to deal with unseen points.",
            "Points are different framework and we usually use them in semi supervised learning nothing clustering and then we can do something more nicer nicer than what I can think about here, but this is a good direction.",
            "I don't think I saw anybody then expect clustering and incorporate new points.",
            "I'm not very clear about the motivation for using Eigen analysis and it's just about every damn thing for no.",
            "So this is one of the this is one of the questions that I'm going to try and shed some light on during this talk."
        ],
        [
            "OK, so the first algorithm, the first algorithm was.",
            "So maybe I should say that I'm going to talk about the algorithms there are not.",
            "I'm not going to talk about them.",
            "Is there which indicates how they were the analogically, how they were probably published?",
            "So this one was published in 2001.",
            "The next algorithm was also published in 2001, and the third algorithm was published in 2000.",
            "So I'm going to say this one and that one.",
            "I'm not going.",
            "I'm not going not well, don't imply that this one came before the other two.",
            "But that one this algorithm was published in 2001 in NIPS and.",
            "What the authors did here is the following.",
            "They they."
        ],
        [
            "Find the matrix W. Using this."
        ],
        [
            "Affinity function, but they they they set each point affinity between each point and send itself to be 0 normally using."
        ],
        [
            "TBF, each point and it sells affinity here would be one, but they set it to be zero.",
            "Will see why it is."
        ],
        [
            "Important later, they don't really discuss it, but they do that they said each each of these abilities to be 0.",
            "Then they compute the just like the way we did before.",
            "The end is the volume of a node, so it simply the mass of all the edges connected to the node and they define D together agonal matrix with all of the volumes of the node edges on its diagonal.",
            "These are diagonal matrix, the matrix a that defined this is the funk that I was that I was mentioning before.",
            "Is simply due to the minus RW D minus off.",
            "They multiply W from both sides by D to the minus half.",
            "They compute the first leading K eigenvectors of a so.",
            "V1 would be the first eigenvector, V2 would be, the second NVK would be the case when I say the first second, I mean that these are the eigenvectors vectors which correspond to the largest eigenvalue, the second largest eigenvalue, and so on.",
            "This is the Matrix V. So we have here Matrix V which is N * K matrix.",
            "Then they normalize each row of this matrix.",
            "OK, they compute the eigenvectors and then they normalize each of each of the rows of this matrix and they run K means over this rose.",
            "The partitioning that they get from this K means algorithms.",
            "They apply this partitioning to the original data set and that's it.",
            "I'm going to show you this cartoon Ann and I'm going to show you a few more later.",
            "And the reason I say that is because I'm going to use this cartoon, which is simply a tree Gaussians which we for this data we could.",
            "We could very easily use K means on this data and get the same result, and it would be for this data.",
            "Again, it would be very silly to use spectral clustering.",
            "It's much more expensive.",
            "But it is useful to understand the way they explain why their algorithm is efficient or is successful at all.",
            "And then I'm going to show you how they have even nicer result in that one.",
            "But anyway, for this data we have three Gaussians here, and we compute the affinity matrix.",
            "That corresponds to these three Gaussians, so it looks roughly like block block diagonal.",
            "If you can see from where you sit.",
            "We then compute the eigenvectors.",
            "This would be the first eigenvector V1.",
            "This would be the second eigenvector and the third eigenvectors.",
            "And the Matrix V would simply be V1V2V3 packed as columns.",
            "We have the metrics with the normalized each of the rows of, which means we take the first item here in the first item in each of these vectors we normalize them to be equal to 1.",
            "We do it for each of these.",
            "Three entries and we get this three eigenvectors.",
            "If this is VV.",
            "Then after normalizing, we get you.",
            "This is you.",
            "And we now run K means on the rows of you and we get this partitioning.",
            "Now you can see already that you is perfectly aligned with the data itself, which means that the first entries for each of these eigenvectors identical, and then the next batch of entries which correspond to the next.",
            "To the next day, Gaussian are also identical in all tree in all the tree eigenvectors and so on.",
            "So running K means on the rows of you is trivial, is like finding tree clusters, where each cluster is like a point.",
            "We will see that in a minute."
        ],
        [
            "How do they explain this?",
            "How do they?",
            "I mean, they're trying to come up with this algorithm and now they're trying to explain why why it works.",
            "So to do that they're saying, let's look at the ideal case.",
            "What would be the ideal case?",
            "The ideal case would be when we have three clusters, for example, and they are far away from each other.",
            "There well separated.",
            "Now in this case, if there were separated and we take Sigma to be large enough Sigma."
        ],
        [
            "Being the.",
            "Length scale definitive function."
        ],
        [
            "If we have this kind of scenario, so we have three clusters and their separated in Sigma is large, W is approximately block diagonal, where on the diagonal with blocks of matrices each of them is 1.",
            "Hey, which is W multiplied by both sides by little minus half it's going to be like that again block diagonal.",
            "It's easy to compute for this case.",
            "Is the computer eigenvectors going to be what we see here?",
            "Eigenvectors would be simply indicator function, normalize each of them.",
            "And if we normalize the rows of each of these of each row of this matrix V, we're going to end up with you, which again is like an indicator function."
        ],
        [
            "It looks very similar to what we see here."
        ],
        [
            "This is very similar to what we see here.",
            "So we have if we look at you we have eigenvectors which are orthogonal and they can serve as indicators to the class assignment.",
            "Of the of the of the data points."
        ],
        [
            "Only problem with this explanation is that he doesn't really explain the kind of results that they get.",
            "This is from the same paper, so they show.",
            "For example, it wasn't need 2001, four.",
            "If they apply to this kind of data set, it should have an S. Here it disappeared when I copied it, but it should have an S. They showed that for this data set for example, they get 8 clusters.",
            "This this is interesting if you look here.",
            "If they tell the algorithm to find 2 clusters, this is what it gets.",
            "If they tell it to find tree clusters.",
            "This is what it gets.",
            "And this data set is.",
            "It couldn't be really more."
        ],
        [
            "Different than this kind of data set."
        ],
        [
            "A little bit, but still their algorithm works and this is the kind of results they get.",
            "So why using dragon vectors?",
            "We still don't know, right?",
            "Maybe we know a little bit later, so just to clarify, the algorithm has to take K, the number of clusters right in as an input, right?"
        ],
        [
            "So all algorithms assume that we know the number of class."
        ],
        [
            "And the number of clusters dictates how many eigenvectors we're going to find.",
            "It's given to us by the user."
        ],
        [
            "Anymore question quick question.",
            "So the initial graphs of course are fully connected.",
            "It could be both.",
            "So this is one of the things things I was going to talk about later.",
            "It could be fully connected and you could sparsified and there's not going to be an issue of some pathological cases which we can maybe think of, but you know in all realistic datasets there isn't going to be much difference between them.",
            "The condition that you need to keep in mind is that even if you specify the graph, you always.",
            "Or from if we if we want to analyze the algorithm, we always need to assume that the graph is still fully connected fully.",
            "It's not.",
            "Well, it's not disconnected.",
            "We don't buy sparsifying it we don't get two different subgraphs to disconnected subgraphs.",
            "It's not.",
            "Maybe this is what you said.",
            "It's not a fully connected in the sense that each node is connected to all of the rest of the nodes.",
            "It's not.",
            "It's not.",
            "Well, it could be.",
            "It could be the case and it could be.",
            "It could be sparse.",
            "Is going to get very similar results.",
            "I'm going to see why later, but what I am assuming is that from if we if we begin from any point or any node in the graph and we start working on the edges.",
            "I'm going to be able to reach any other node.",
            "Not directly because I'm not connected to all of the rest of the points, but if I follow that the edges, then I could theoretically get to any other notes.",
            "OK, so you're expressing surprise little bit of these results and you understand them.",
            "Obviously better better than I do, so I'm trying to catch up to your intuition here.",
            "You are expressing some surprise.",
            "Why is it?",
            "Is it that the rings that the three when the cluster of three the upper right should not have come out as cleanly as it is?"
        ],
        [
            "I'll tell you why, the reason why I am expressing surprise is then when I first looked at it and I saw that I couldn't really understand why this would do anything reasonable.",
            "When you have some affinity matrix and compute some eigenvectors and you do K means you normalized eigenvector, you do all these kind of like black magic and you get something that makes sense."
        ],
        [
            "You get this, so This is why I was surprised at the beginning.",
            "If you're not surprised that it may be better understanding of it than me.",
            "When I first saw it.",
            "But This is why I expressed surprise.",
            "And it's it's.",
            "It's the fact that it gets any anything which is reasonable hour surprised by it, not because it gets.",
            "I mean, it's not like that I'm troubled about where exactly the cluster is cut here or here.",
            "It's not what surprises me.",
            "I'm surprised it got any good results at all.",
            "OK, so that was kind of related to my question about how they're connected, because if they're just connected like Delaunay triangulation connections that those are the only edges.",
            "In the original graph, right?",
            "Then all the cuts would have to happen in sort of spatially meaningful places.",
            "Versus if all of the if each node has a weight to every other node in the entire graph, then."
        ],
        [
            "I agree with your OK, but why?",
            "Why would they do anything reasonable when we talk about cut?"
        ],
        [
            "So yes, I agree with you.",
            "I mean if you talk about cut this normalized cuts for example, which I mentioned before, then this would be a good place to cut it, right?",
            "I mean, even if it's fully connected, we know that the edges between this point and that point are much lower than the edges between this point and that point.",
            "So it is a good place to cut, but why?"
        ],
        [
            "Why would this do anything which is which is, you know, graph cut?",
            "This is what I was surprised by the function.",
            "Sorry, I mean some kind of function to make pretty metrics.",
            "Is they used they they use the RBF there."
        ],
        [
            "Radial basis function."
        ],
        [
            "So the way."
        ],
        [
            "They chose the."
        ],
        [
            "And this is not the way I would do that, but when they wrote the paper, it was the 1st paper.",
            "As far as I know, the first topic that this kind of clustering in the way they chose it is they simply took a bunch of them.",
            "And they."
        ],
        [
            "They measured for each of them how dense each clusters are.",
            "In this you matrix, so they took for each Sigma.",
            "They computed V and then the computed U and they look for EU.",
            "That gives you if you run K means on the on the rows of you they look for the Sigma that gives you the smallest distortion of the K means on this matrix.",
            "They simply chose this Sigma so it was like simply trying a lot of sigmas and picking up the one that gave you the smallest distortion here.",
            "OK, different Sigma correspond to different WS.",
            "Different WS correspond to different movies and then different use and then simply use the Sigma that ended up with you with the lowest distortion.",
            "The just traditional clustering based on pieces of space is just accepted.",
            "Using the kernel function.",
            "So is there any word meanings in using the setting required W equal to zero with their affinity?",
            "By setting the several affinity to 0 or different."
        ],
        [
            "B0 you mean here?"
        ],
        [
            "You're not going to see it in this kind of data sets.",
            "The kind of data set that it becomes really crucial is if you took this data set exactly that one, but you edit one outlier far away from it.",
            "If you had this case, if you added one point outlier right here, then you would have had a completely different results if you use W North, which is 0 or WNN, which is 1, and why this is the case, we're going to see later.",
            "Sorry, let's change my questions.",
            "OK, so if you do the clustering based on the subspace is generated by corner PC.",
            "You'd be able to get the similar result."
        ],
        [
            "The same meaning, the look at the eigenvectors of W itself.",
            "RBF corners, but you're looking at the eigenvectors of W itself.",
            "This is what you mean.",
            "What do you mean, eigenvectors produced by the kernel itself?",
            "Covariance matrix correlation matrices can be made by the affinity matrix is by using our wavefunction right?",
            "And then can analyze the PCA.",
            "OK, so you're talking about W = 8 = A = W."
        ],
        [
            "If we don't have this normalization here.",
            "If simply a = W and we do exactly the same thing with A equals W, you're asking if we're going to get the same result.",
            "No, you're not going to get the same results.",
            "You're not going to get the same results it tested.",
            "Mutation the normalization does make a difference in the main diff."
        ],
        [
            "Difference it makes is that if you look here for example.",
            "The main difference it makes is that it it gives you a different ordering of the eigenvectors going to different values for the eigenvectors, and you're also going to get a different ordering of the eigenvalues, so the eigenvectors corresponding to the to the largest eigenvalues are not necessarily going to be the same eigen vectors that correspond to the largest eigenvalues.",
            "If the matrix is normalized, or if it is not.",
            "And Bowen also diagonals are going to be different, so why this is doing anything reasonable at all?",
            "It's still not really clear.",
            "But if you do W without normalization, you're going to be at less, less intuitive results."
        ],
        [
            "OK, so this was the first algorithm.",
            "The second algorithm was.",
            "And the second algorithm is is slightly different in the way we normalize W to derive a.",
            "In this case we call it P. So instead of having having D minus half WD minus half, we use D -- 1 W. If we look at this matrix P. Then remember that D is a diagonal matrix, and on the diagonal we have the volume of each of the nodes.",
            "What this does basically is it divides this multiplication.",
            "It divides each row of W. By the sum of the elements of this row.",
            "Which means that each row of P sums to one, it's it's not negative.",
            "It might be 0, but it's not negative.",
            "Each of the elements of P and each row sums to one, so we might think about this matrix P. As a stochastic matrix as a first order Markov Markov Walk matrix, which is why they called this paper around the world view of spectral class for spectral clustering or special segmentation.",
            "So they define these metrics P and then they do exactly the same thing.",
            "The computer K leading eigenvectors of P. They put all of them one after the other.",
            "They run K means on the rows of this matrix.",
            "And they get similar results.",
            "The main difference between this algo?"
        ],
        [
            "In the previous one, is that instead of having this kind of normalization, we have data minus 1 W and we don't need to do.",
            "Normalization of flow?"
        ],
        [
            "Of V. Um?",
            "So it looks a little bit simple.",
            "And this is the kind of result that you get for this data set.",
            "If you take these three rings and you compute their first eigenvectors corresponding to the affinity matrix of this data, you get these three eigenvectors.",
            "Again, it looks like a perfect indication functions.",
            "If you plot each each.",
            "Is the matrix V If you plot the first, second, 3rd row of V in a 3 dimensional space, you're going to get these three points.",
            "I don't know if you can see this one here, but we have a Brown point where the green point and where the blue one, and each of these points basically is a pile of other points, which all of them correspond.",
            "The Brown point here correspond to all of the points on the Brown cluster.",
            "Brown ring.",
            "The Green one correspond to all of the points on the green ring, and the same for the blue one.",
            "So clustering this data set the rows of a few of the story with K means is trivial.",
            "We simply have a pile of three points in.",
            "In in these locations and we need to do K means it's immediate.",
            "This is like K means on the roads of this.",
            "Of this eigenvectors now doing the same thing with images.",
            "What they did is this is one way of defining an affinity and affinity matrix or a graph in an image is what they did is they they took this image and for this specific experiment they used an edge to edge detector to find and edges and they use different features of each pixel.",
            "For example they use the information that they got from there.",
            "And they used RGB or the grayscale information that they got from the image itself.",
            "And they built a graph sparse graph where they connected neighboring pixels.",
            "And the value of the edge depends both on the value of the pixels and the information they got from the edge detector.",
            "But the kind of segmentations they get is 4, six, and seven segments.",
            "This is the kind of results that they get when they segment images with their technique.",
            "With this with this technique.",
            "Any questions?",
            "I think it's more or less and saying look at the set P equals.",
            "Good afternoon, we find out later.",
            "Tonight is how it happened.",
            "Kia.",
            "That seems straightforward, but I do not know what the user testing method.",
            "I need to pay me back.",
            "What do you think means, oh this is something that.",
            "They just suggested and I think it was in."
        ],
        [
            "In Jordan and Weiss in this paper, they first suggested to use the K means, and then they simply kept on doing it in other papers.",
            "And if you look at the way the eigenvectors are in the in the eigenvectors, basically they call it.",
            "If you look if you actually look at adding vectors, then K means is not really a reasonable thing to do.",
            "You could do something more interesting than that, but this is simple and you don't want to complicate algorithm more than that and I'm not sure that it's after doing all this.",
            "I only say computation, not sure that it's justifiable to do other there's other other algorithms in the in the eigenvector space.",
            "Now.",
            "You could for example do like.",
            "I mean, why not doing a loop right?",
            "Or something that simply fits fits yourself if you have, like if you know that you have K clusters in the eigenvector space, you could do again spectral clustering on the eigenvector space.",
            "But what they found or what you could you could see if you experiment enough."
        ],
        [
            "Is that doing?",
            "K means often works good enough if you do something else, you're not going to get better results even though you don't really have any good explanation to why we need to do that.",
            "Play my booty.",
            "Analyze the data centers themselves digon vectors themselves.",
            "Yeah."
        ],
        [
            "So this is what this is.",
            "What we will see in a minute.",
            "The eigenvectors themselves are the ones that you need to look at, but then came in is simply a way to looking for clusters in you stack.",
            "All of the investors one after the other.",
            "You could do other things, but you could also.",
            "You could also look."
        ],
        [
            "Each of the eigenvectors by itself I'm going to.",
            "I'm going to talk about it in a second, the third one and the last one.",
            "I'm going to talk.",
            "Here is the one.",
            "This is the first one.",
            "Actually it was published in 2000.",
            "Here the metrics they use is D -- W. I am and they don't normalize it here, but I can problem that they solve.",
            "Is this one here?",
            "If this is Elder Laplacian?",
            "The man is W. When the eigenvector, the investor problem they solve is this one, and if we multiply by detail minus one, we get basically these minus one LV equals Lambda V. So even though it is presented like that, so I I presented the same here it is a normalized Laplacian problem that we're looking for.",
            "If you're looking at so they define this matrix D -- W, they compute its eigenvectors and they do pretty much the same thing as male and cheated in the in the in the in the paper.",
            "I just mentioned before, which means that they take all the K eigenvectors and then they do K means on the roads of this matrix and the kind of examples they show here is that if they take this image which has a noisy step on it and they graph here, the graph here again, each pixel is a node of the graph, and if you compute an eigenvector you basically get Eigenvector tells your value, it gives you a value to each of the nodes, so you can plot the eigenvector as an image as well.",
            "If this is the original image, this is the second largest, the second smallest.",
            "Sorry there are.",
            "They actually correspond to the second smallest eigenvalue, and if you do, K means on this kind of eigenvector.",
            "This is the kind of partitioning you get at the same thing, with three segments in this image gives you 1, two and three segments.",
            "If you look at the 1st three eigenvectors and do K means on."
        ],
        [
            "And you said that it looks to you to say pretty much the same doing this normalization or that normalization.",
            "So this is what this slide is about.",
            "They are very closely related.",
            "If we look at the first one that I introduced in Jordan and Weiss Weiss, we look we see this kind of matrix.",
            "Definitely metrics the eigenvector problem lacking lacking problems that we solve is that one form a line.",
            "She we solve this one and fortune Malik we solve the third one.",
            "Now let's see what we can do here.",
            "How can we move from different to that one?",
            "If you look at the and NJW the first one eigen problem and we multiply both sides by D to the minus half we get this expression here.",
            "And this is pretty much the same as that expression.",
            "If we define V to be.",
            "Tied to the minus Alpha you.",
            "So if we define Vita beaded minus one, which we did myself, you we get this this problem and that problem to be identical.",
            "And if we look at the Shane Malik problem, which is this normalized pricing problem and we do something similar, we get that Lambda which is the eigenvalue of melenci equals to 1 -- 1 minus mu, where mu is the eigenvalue of Shane Malik.",
            "So we simply have.",
            "Mirror image of all of Dragon values.",
            "Each eigenvalue I should say each eigenvalue.",
            "Here Lambda is between minus one and one.",
            "So taking one minus simply gives us a mirror image.",
            "Of the eigenvalues of the ordering of values.",
            "This is why taking the largest eigenvectors corresponding to the largest eigenvalues here or taking the one corresponds to the smallest.",
            "Here gives you the same eigenvectors at the end.",
            "Um?",
            "So they are very closely related and you can move from each if you know the solution for one of them, you can very easily move to the solution of the other one.",
            "The only thing that we need to keep in mind is that if you want to use in Jordan advice, you need to do this extra normalization of rose in the eigenvectors, which you don't need to do in this two, and the reason that you don't need to do that is exactly this multiplication here.",
            "If we do normalization of rows of U is like scaling each of the dimensions and you get this scaling each of the of each of the dimensions built in already.",
            "Here in the solution.",
            "And if you experiment enough, you can see that if you do use this solution for this problem.",
            "This normalization here.",
            "Saves you the need to do the normalization, extra normalization that they do in engine Jordan advice, but the picture is what I'm trying to say here is that they are all closely related and if you do one, you're very likely to get the results of another one as well.",
            "Anywhere any question about this one is is a very good time to ask now.",
            "But they're not identical.",
            "They're not there there.",
            "There there exists some datasets where you could get different.",
            "Different results, so it depends where, so we see.",
            "For example, we change Malik Malik.",
            "They basically stick about about by partitioning of data.",
            "So we talk about binary clustering and if you want if you want to get more than a binary clustering, you need to do some kind of hierarchical clustering.",
            "Even though they speak about other other methods as well.",
            "But this she is exactly that she so the if we look at the the way I describe the algorithm here and we look at this solution for Shane Malik or Melenci.",
            "We basically have the same.",
            "This is identical, this exactly the same algorithm.",
            "In both of them, the difference being."
        ],
        [
            "And I guess This is why they called their paper this way.",
            "Is that this is a random walk view of of image segmentation or separate spectral segmentation?",
            "It's not necessarily a new algorithm, but it is a new view."
        ],
        [
            "Which they didn't really realize this."
        ],
        [
            "Or um.",
            "Yeah, but they're not accept this too.",
            "They're not, they're not identical, and if you look at other algorithms they're also not identical, even though they all follow pretty much the same.",
            "Same idea.",
            "Anymore questions."
        ],
        [
            "OK, so so why?",
            "Why is it good at all?",
            "Why would looking at the eigenvectors?",
            "We do anything that corresponds to doing something like normalized cuts so insane.",
            "Malik paper from 2000 they showed basically that if you want to do that, if you want to minimize the normalized cut then you can approximate the solution for this by looking at a solution for the Laplacian eigen problem.",
            "So this is this is one way of trying to understand this is an approximation?",
            "Why would it?",
            "Why is it?",
            "Why is it any good to look at eigenvectors?",
            "Because you can show that this is it is approximating the solution for this combinatorial problem.",
            "I'm not going to talk about it more, but I'm going to talk about this to explanation.",
            "One would be to look at the graph Laplacian and related to smooth functions.",
            "And the other one would be to look at the random walks view and see how it relates to what Mailer called Marina Melody, she called it piecewise constant Eigenvectors, which are special case.",
            "Of smooth functions."
        ],
        [
            "So first television, the passion is the minus W this is the one that chain molecules and if we look at these metrics demands, W is a nice property which if you will take this quadratic form V transpose LV, then it equals this expression.",
            "VN is the NTH entry of VV being dragon vector of V being the.",
            "Vector, which we're going to use this and eigenvector, but this this quadratic form equals this expression.",
            "Now we've we solve the solution LV equals Lambda V, and we assume that V is normalized, which doesn't limit us.",
            "We can always assume that.",
            "So if we assume that the normalized and we multiply V transpose, we multiply each side by V transpose.",
            "We get that Lambda equals V transpose LV, which means that Lambda equals this expression.",
            "Now if you look at this expression, what it basically does is it computes something like a distortion.",
            "It says if WNM is I then I'm going to give a lot of weight to the difference between VM and VM.",
            "If WNM is low then I'm not going to care a lot about how different they are.",
            "So what it does?",
            "Basically, if we want to minimize if we want to find out if, well if we have a small Lambda, then we can.",
            "We can imply we can learn from it that this vector V. Is it does similar values to each other, two different entries of the of the vector?",
            "They are similar if the affinity between these entries is I.",
            "This means that if we have a graph and we know the some edges, are I then the eigenvector which corresponds to a function of this over these nodes?",
            "The functional on this graph diagram vector would be fairly smooth."
        ],
        [
            "This is, for example a smooth function, if 2.",
            "Nodes are close to each other and the edge between them is.",
            "I then the eigenvector.",
            "In the entries that correspond to these two nodes which are close to each other, it's going to be similar to each other.",
            "This is what it means."
        ],
        [
            "So if you look at it, yeah.",
            "The W's are all non negative right?",
            "Yeah 'cause if they went negative then you know this thing could could go right right right L is the minus W so the entries of L could be negative or most of them are negative except diagonal.",
            "But if you look at the quadratic form then it is positive.",
            "Yeah so all the eigenvalues of Laplacian for example are a positive or negative.",
            "It is zero.",
            "Eigenvalue is 0.",
            "If V is all equal, is all once and you can show that it has one eigenvector, which is all ones.",
            "So looking at the eigenvectors of the Laplacian itself gives us a smooth function over the graph and a smooth function of the graph.",
            "What it does is it puts similar values on nodes that are close to each other and this similar values are nodes that are far away from each other, which starts to make sense why it's any good to look at eigenvectors of this kind of matrices.",
            "I hope it starts to make sense.",
            "If you look at the normalized version of of the of the Laplacian, so you could look at the left, right normalized version, which is that you can show that it is closely related to the one that in Jordan and Wise use, and if you look at the quadratic form with this kind of normalized matrix then you get something similar to that one only, only you normalize each of the entries of the eigenvector by the volume.",
            "Of that node, this is a bad thing to do because what it says is that you don't really care about the values on each of the nodes, but you care about the ratio between the value on the nodes and the volume of the nodes, and there is no reason to think that the volume of the node have to do anything.",
            "Anything with the cluster assignment.",
            "So what we really need to look at is this kind of expression, but not that kind of expression and the way they overcome this.",
            "This this problem here that you also consider the the volume of the node is by normalizing the rows of the matrix V to get the metrics here.",
            "Different kind of normalization and less normalization, which is D -- 1 L gives you the random walk matrix.",
            "Here I don't know any way of any result that relates the quadratic form to this kind of a nice expression, but if we look at the metrics D -- 1 W and we look at the eigenvectors."
        ],
        [
            "D -- 1 W We have a result that tells us white it makes any sense to use them."
        ],
        [
            "But before we do that, maybe maybe I should pose for questions about why using the Laplacian, yeah?",
            "So the explanation is very good, but I've gotten a little confused now with respect to our initial goal of the approximating the normalized cuts.",
            "So if, for instance.",
            "You have these clusters in one cluster.",
            "That's sort of the same separation, but really lots of points.",
            "Lots of data, all landing sitting on top of itself.",
            "The eigenvectors are going to be will try to span the space, but it will be very heavily sort of biased right in the.",
            "No, the eigenvectors would be.",
            "If we look we talking about eigenvectors of L Now or we talk about which eigenvectors are you referring to."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Ruben.",
                    "label": 0
                },
                {
                    "sent": "OK, so to talk about bigger clustering.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we talk about clustering, it's it's R and there are some theoretical result details that says that clustering is something that you can't really define mathematically defined, but the kind of clustering.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to talk about is the kind of clustering that, like the topic title yourself, that you can you know if it's good when you see.",
                    "label": 0
                },
                {
                    "sent": "Um, one kind of clustering would be the one on the left where you can model the data using some probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case you could use a mixture of Gaussians and you could do very nice, interesting with very nice things with with probabilistic model.",
                    "label": 1
                },
                {
                    "sent": "But it's not always something that you can use, like for example the data set that we have on the left.",
                    "label": 0
                },
                {
                    "sent": "We see that there are clusters.",
                    "label": 0
                },
                {
                    "sent": "There are groups of data points.",
                    "label": 0
                },
                {
                    "sent": "But for me it's a little more a little more hard, little more difficult to model.",
                    "label": 0
                },
                {
                    "sent": "This data points away from the probabilistic probabilistic model, so the kind of partitioning that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "Is both basically, but we're going to focus mainly on this kind of datasets, where spectral clustering really has an edge over other methods.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I begin, I want to say something about star clustering in general and what is not going to be.",
                    "label": 1
                },
                {
                    "sent": "What I'm not going to be talking about in this tutorial, so when we talk about spectral clustering clustering, we basically have two stages in the in the in the technique.",
                    "label": 0
                },
                {
                    "sent": "First of all, what we do is we take the data, set the input and we represent it as a graph.",
                    "label": 0
                },
                {
                    "sent": "This is an extremely important part.",
                    "label": 0
                },
                {
                    "sent": "If you don't do it properly, you're going to.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get any useful result.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about this part.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the second part though.",
                    "label": 0
                },
                {
                    "sent": "The one that's given the given the graph we need to find a good partitioning of the graph.",
                    "label": 1
                },
                {
                    "sent": "The reason I'm not going to talk about this this stage here is because it's largely problem dependent.",
                    "label": 0
                },
                {
                    "sent": "For each problem, we're going to have some kind of expertise that tells you this is how we need to measure similarity between data points, for example.",
                    "label": 1
                },
                {
                    "sent": "We can think about.",
                    "label": 0
                },
                {
                    "sent": "How to set of images?",
                    "label": 0
                },
                {
                    "sent": "If I'm giving you a set of images and I'm asking you to do cluster this images into groups, you could somehow put an edge between put an edge between similar points by saying the two images are similar.",
                    "label": 0
                },
                {
                    "sent": "If for example the pixelwise difference between them is small or the Instagram for both images look similar.",
                    "label": 0
                },
                {
                    "sent": "This is one way of doing that.",
                    "label": 0
                },
                {
                    "sent": "But then if I told you that these images are basically taken out of a movie for film, that you have an additional information here that tells you that you basically have a time series.",
                    "label": 0
                },
                {
                    "sent": "And you need to use this to construct your graph, so the same data set images you might want to build one graph if it's just a sequence of images, you might want.",
                    "label": 0
                },
                {
                    "sent": "You might want to build a different graph if you know that this is a time series time series.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the stage the step of building the graph representing the data using a graph is largely problem problem dependent, and I'm not going to say too much about it just a little bit, but not too much, and I am going to focus about the problem that given the graph, how can we find good subgraph or good partition?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this graph.",
                    "label": 0
                },
                {
                    "sent": "The way the way I see the way I planned the talk is by first I'm going to talk a little bit about graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk like a couple of slides about what are the major or the most important quantities in a graph that we were going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk a little bit about the algorithms the way we actually find the partitioning.",
                    "label": 0
                },
                {
                    "sent": "I'll try and give you some insight about why and by knowing why we can also try and understand when the algorithms are expected to work, whether or not we will then move to what I call yourself tuning algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is something that we did here last year.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to finish with the practicalities of are we going to need to or how we efficiently can implement this this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this technique and I'm going to begin with some terminology about graphs.",
                    "label": 0
                },
                {
                    "sent": "We have a graph in with OK, so when I when I'm talking about graph the data, the items that I'm given the data points are the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "The edges are the entity that I'm going to introduce by to build the graph and the edges are going to be weighted by some affinity function that measures the distance between or.",
                    "label": 0
                },
                {
                    "sent": "The similarity between the data items.",
                    "label": 0
                },
                {
                    "sent": "For example, what we see here is that if South is 1 item and SM is.",
                    "label": 0
                },
                {
                    "sent": "Is the 2nd item.",
                    "label": 0
                },
                {
                    "sent": "The edge between them.",
                    "label": 0
                },
                {
                    "sent": "Is weight weighted by the radial basis function?",
                    "label": 0
                },
                {
                    "sent": "In this case where we plug here into the radial basis function, is the difference between the two items.",
                    "label": 0
                },
                {
                    "sent": "This means that if South and SM are close to each other similar, they have a small distance between them, then the affinity between NWM is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be close to one in this case, if Sanon SM are distant from each other, then WNM is going to be close to 0.",
                    "label": 0
                },
                {
                    "sent": "And not volume or no degree is the sum of all of the edges that are connected to the node.",
                    "label": 0
                },
                {
                    "sent": "So DN and DN is a crucial is a critical quantity quantity and what I'm going to say.",
                    "label": 0
                },
                {
                    "sent": "So it's important to remember it.",
                    "label": 0
                },
                {
                    "sent": "The end is the sum of all edges that are connected to the end node.",
                    "label": 0
                },
                {
                    "sent": "This is the this is the volume of the node volume of a cluster is to compute the volume of a cluster.",
                    "label": 1
                },
                {
                    "sent": "What we need to do is simply.",
                    "label": 0
                },
                {
                    "sent": "Go over all of the nodes in the cluster and sum the volumes so we basically go over each node and some the edges that are connected.",
                    "label": 1
                },
                {
                    "sent": "To that node at the cut between two clusters of two subgraphs.",
                    "label": 0
                },
                {
                    "sent": "If we have this is a graph and we want to partition this graph into one sub graph in the second sub graph.",
                    "label": 0
                },
                {
                    "sent": "Then the cut is the sum of the edges that we need to take out together partitioning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can think about different kind of graph cuts or different kind of criteria to define the graph cut or to use the graph to find the partitioning, one would simply be to find a partitioning we want to cluster data or partitioning, partitioning, graph.",
                    "label": 0
                },
                {
                    "sent": "We can look for the partitioning or that minimizes the cut.",
                    "label": 0
                },
                {
                    "sent": "The partitioning that if this is the graph we're looking for, the partitioning that minimizes the edges, the weights of the edges that we need to take out to get the partitioning.",
                    "label": 0
                },
                {
                    "sent": "In this case what it means is that we need to take out this edge.",
                    "label": 0
                },
                {
                    "sent": "And if we want to partition this data, this graph into two subgraphs, we're going to end up having one sub graph here and the other one being only the soul node at the side.",
                    "label": 0
                },
                {
                    "sent": "This is not a very good criteria to have reasonable partitioning of graphs are good partitioning of graphs, and the reason here is that the reason is that it tends to cut out nodes that are on the edge of the data set, and if you have an outlier, if you have one point which is far away, then it will.",
                    "label": 0
                },
                {
                    "sent": "It will end up if you implement this right here.",
                    "label": 0
                },
                {
                    "sent": "This criteria you're going to end up separating between this outlier and the rest of the data points.",
                    "label": 0
                },
                {
                    "sent": "The alternative would be the normalized cut.",
                    "label": 1
                },
                {
                    "sent": "The normalized cut is using the cut itself.",
                    "label": 1
                },
                {
                    "sent": "This is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Cat is here, but it probably, but it also introduced a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "It multiplied the cut by this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Now this quantity is the sum of 1 over the volume of the first cluster plus one over the volume of the second cluster.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why is it doing anything which we might consider good?",
                    "label": 0
                },
                {
                    "sent": "It's because that if we look at the volume of a cluster, if this cluster is small and the number of nodes that it includes are small, and the volume of this cluster is small, then the discount is going to be large.",
                    "label": 0
                },
                {
                    "sent": "And the tradeoff that we have here is before between having.",
                    "label": 0
                },
                {
                    "sent": "Between taking out a small number of edges.",
                    "label": 0
                },
                {
                    "sent": "But not ending up having clusters clusters which volume that is too small, we're going to rebalancing between taking a few edges as we can, but still having clusters which are relatively large.",
                    "label": 0
                },
                {
                    "sent": "And if you implement for example for this data set, if you implement this criterion, if you optimize this criterion, you can end up having this kind of partitioning of data.",
                    "label": 0
                },
                {
                    "sent": "Now finding the solution for.",
                    "label": 0
                },
                {
                    "sent": "For minimizing or the solution that minimizes the find out good or or low normalized cut is combinatory.",
                    "label": 0
                },
                {
                    "sent": "This is a combinatorial problem, so to find the solution is NP hard.",
                    "label": 0
                },
                {
                    "sent": "And we know that we usually want to run away from NPR problems.",
                    "label": 1
                },
                {
                    "sent": "And one fight in some approximation approximation that I'm going to talk about here is the spectral graph partitioning.",
                    "label": 0
                },
                {
                    "sent": "This is basically an approximation for this problem, and we're going to see different ways of looking at spectral clustering as an approximation for minimizing the normalized cut.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is it?",
                    "label": 0
                },
                {
                    "sent": "Special clustering?",
                    "label": 0
                },
                {
                    "sent": "I once heard somebody told me that, well, we talked about spectral clustering and said yeah, I know what's wrong is you take some affinity matrix, you find its eigenvectors and you do K means over there and that's it.",
                    "label": 1
                },
                {
                    "sent": "And he's right.",
                    "label": 0
                },
                {
                    "sent": "I mean, you have a lot of algorithms and you have a lot of criterias and you have a lot of different ways of initializing.",
                    "label": 0
                },
                {
                    "sent": "There are many papers about special clustering and they have a lot of differences between them.",
                    "label": 0
                },
                {
                    "sent": "But the basic thing the basic idea in all of these methods is that you're going to represent your data as a graph, and instead of putting edges you can simply summarize it in a matrix.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to find some other metrics.",
                    "label": 1
                },
                {
                    "sent": "If this matrix is W, this is the metrics that the empty entry of this matrix would be the affinity between the North and the empty nodes.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take the data we're going to define some graph over it by plugging all the affinity values into a matrix.",
                    "label": 0
                },
                {
                    "sent": "We're going to find a new matrix, A find its eigenvectors, do something which we call K means in the feature space in this class in this eigenvector space, and that's it, basically.",
                    "label": 0
                },
                {
                    "sent": "Older, older or most of the cluster of the string algorithms are based on this idea and the main difference between them is how you define a A from W. And I'll make sense out of all of it.",
                    "label": 1
                },
                {
                    "sent": "How can you can use it to extend this kind of basic usage?",
                    "label": 0
                },
                {
                    "sent": "Off a.",
                    "label": 0
                },
                {
                    "sent": "Now, and I'm going to talk about three different algorithms.",
                    "label": 0
                },
                {
                    "sent": "They do exactly this, but with different kind of metrics, is a different kind of of partitioning the data.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show some connections between them.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try and give you some explanation about how they work, and then I'm going to show we can extend them.",
                    "label": 0
                },
                {
                    "sent": "Into a into to make a little bit more than just that.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to begin with three different clustering algorithms, so if you have any problems at this point, this is a very good time to ask them.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you said you're going to focus on the clustering.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So not the other, so there's nothing else to really say I guess.",
                    "label": 0
                },
                {
                    "sent": "Or this sort of evidence how to build Infinity matrix mean there's nothing more you can do creatively after you have the edges of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so the edges of the graphs are the affinity matrix.",
                    "label": 1
                },
                {
                    "sent": "Now you construct them is depends a lot about your about the kind of data that you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "You can say a lot about how to do that.",
                    "label": 0
                },
                {
                    "sent": "There are different applications and in different applications, each of them you can find in a different paper.",
                    "label": 0
                },
                {
                    "sent": "You're going to see a different way of constructing this affinity matrix.",
                    "label": 0
                },
                {
                    "sent": "There isn't much I can say which is general about it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm going to use this kind of function here, which is a radial basis function.",
                    "label": 0
                },
                {
                    "sent": "This is this is not a very good solution.",
                    "label": 0
                },
                {
                    "sent": "I dare say it's far from being optimal, and in many cases it's even silly to do that, but still.",
                    "label": 0
                },
                {
                    "sent": "I mean, I always almost always use that and it gives you quite satisfactory results.",
                    "label": 0
                },
                {
                    "sent": "And it's only.",
                    "label": 0
                },
                {
                    "sent": "I mean I can only hope that if you use a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "Different, definitely an affinity function which more which is more suitable for the data that you you can get better results but using instead of this function using a different function, a different finish function.",
                    "label": 1
                },
                {
                    "sent": "Again, it depends on the kind of data that is diffuse biological data you might.",
                    "label": 0
                },
                {
                    "sent": "You might want to use different kernel then if you use something else like images or if you use something else like graphs themselves.",
                    "label": 0
                },
                {
                    "sent": "But it won't matter for our purposes what that function is.",
                    "label": 0
                },
                {
                    "sent": "No, it's not, no, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "The only thing that I'm going to see.",
                    "label": 0
                },
                {
                    "sent": "Guarding W that is symmetric, so W&M equals WM and that's it other than that.",
                    "label": 0
                },
                {
                    "sent": "What's the reason for backing, separating how you define your affinity and this funky hey?",
                    "label": 0
                },
                {
                    "sent": "We're going to see three different ways of defining a from W. Why is it?",
                    "label": 0
                },
                {
                    "sent": "It seems to me like.",
                    "label": 0
                },
                {
                    "sent": "You define your affinity based.",
                    "label": 0
                },
                {
                    "sent": "The distance metric could just as well go into your phone.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's fairly natural wage for them.",
                    "label": 0
                },
                {
                    "sent": "So I'm not I didn't really mean to get too formal here when I'm saying funk, I'm just saying we take W, we do something and we get a so you could as well say look let's put your a big box and we say we have some matrix that somehow represent the pairwise similarity between data and that's it.",
                    "label": 0
                },
                {
                    "sent": "But the way the algorithm that I'm going to present, they all are all going to be using this W and do something very simple to this W to end up with a different matrix which is a so you can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean this funk thing is just.",
                    "label": 0
                },
                {
                    "sent": "Some kind of a cartoon.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to deal too much with this kind of what exactly what function it is?",
                    "label": 0
                },
                {
                    "sent": "Time to transform the graph into a matrix.",
                    "label": 0
                },
                {
                    "sent": "You have to order the vertices of of the graph.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm not, it doesn't really affect the results, it doesn't because we're going to look at adding vectors and eigen vectors are going to be represented.",
                    "label": 0
                },
                {
                    "sent": "This is important, so maybe I'll say another couple of sentences about it.",
                    "label": 0
                },
                {
                    "sent": "We're going to index each of these points.",
                    "label": 0
                },
                {
                    "sent": "I'm going to put an index about each on each of these points.",
                    "label": 1
                },
                {
                    "sent": "It doesn't really matter how we index them, but we assume that we have some say endpoints.",
                    "label": 0
                },
                {
                    "sent": "Then we somehow enumerate them and that's it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter how we do that, but what's important is that if this is the first point for example, then it corresponds to the first row of W, so WNM.",
                    "label": 0
                },
                {
                    "sent": "If they were looking at the first point, W1M would be the affinity between the first point and the end point, and it's going to be summarizing the first row and for the endpoint.",
                    "label": 0
                },
                {
                    "sent": "In general, we're going to be looking at the end row of W. Scheme how to deal with new points which arrive after young directors?",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk about transductive setting here.",
                    "label": 0
                },
                {
                    "sent": "We're not going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to deal with unseen points.",
                    "label": 0
                },
                {
                    "sent": "Points are different framework and we usually use them in semi supervised learning nothing clustering and then we can do something more nicer nicer than what I can think about here, but this is a good direction.",
                    "label": 0
                },
                {
                    "sent": "I don't think I saw anybody then expect clustering and incorporate new points.",
                    "label": 0
                },
                {
                    "sent": "I'm not very clear about the motivation for using Eigen analysis and it's just about every damn thing for no.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the this is one of the questions that I'm going to try and shed some light on during this talk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first algorithm, the first algorithm was.",
                    "label": 0
                },
                {
                    "sent": "So maybe I should say that I'm going to talk about the algorithms there are not.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about them.",
                    "label": 0
                },
                {
                    "sent": "Is there which indicates how they were the analogically, how they were probably published?",
                    "label": 0
                },
                {
                    "sent": "So this one was published in 2001.",
                    "label": 0
                },
                {
                    "sent": "The next algorithm was also published in 2001, and the third algorithm was published in 2000.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say this one and that one.",
                    "label": 0
                },
                {
                    "sent": "I'm not going.",
                    "label": 0
                },
                {
                    "sent": "I'm not going not well, don't imply that this one came before the other two.",
                    "label": 0
                },
                {
                    "sent": "But that one this algorithm was published in 2001 in NIPS and.",
                    "label": 0
                },
                {
                    "sent": "What the authors did here is the following.",
                    "label": 0
                },
                {
                    "sent": "They they.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the matrix W. Using this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Affinity function, but they they they set each point affinity between each point and send itself to be 0 normally using.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "TBF, each point and it sells affinity here would be one, but they set it to be zero.",
                    "label": 0
                },
                {
                    "sent": "Will see why it is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important later, they don't really discuss it, but they do that they said each each of these abilities to be 0.",
                    "label": 0
                },
                {
                    "sent": "Then they compute the just like the way we did before.",
                    "label": 0
                },
                {
                    "sent": "The end is the volume of a node, so it simply the mass of all the edges connected to the node and they define D together agonal matrix with all of the volumes of the node edges on its diagonal.",
                    "label": 0
                },
                {
                    "sent": "These are diagonal matrix, the matrix a that defined this is the funk that I was that I was mentioning before.",
                    "label": 0
                },
                {
                    "sent": "Is simply due to the minus RW D minus off.",
                    "label": 0
                },
                {
                    "sent": "They multiply W from both sides by D to the minus half.",
                    "label": 0
                },
                {
                    "sent": "They compute the first leading K eigenvectors of a so.",
                    "label": 1
                },
                {
                    "sent": "V1 would be the first eigenvector, V2 would be, the second NVK would be the case when I say the first second, I mean that these are the eigenvectors vectors which correspond to the largest eigenvalue, the second largest eigenvalue, and so on.",
                    "label": 0
                },
                {
                    "sent": "This is the Matrix V. So we have here Matrix V which is N * K matrix.",
                    "label": 1
                },
                {
                    "sent": "Then they normalize each row of this matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, they compute the eigenvectors and then they normalize each of each of the rows of this matrix and they run K means over this rose.",
                    "label": 0
                },
                {
                    "sent": "The partitioning that they get from this K means algorithms.",
                    "label": 0
                },
                {
                    "sent": "They apply this partitioning to the original data set and that's it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you this cartoon Ann and I'm going to show you a few more later.",
                    "label": 0
                },
                {
                    "sent": "And the reason I say that is because I'm going to use this cartoon, which is simply a tree Gaussians which we for this data we could.",
                    "label": 1
                },
                {
                    "sent": "We could very easily use K means on this data and get the same result, and it would be for this data.",
                    "label": 0
                },
                {
                    "sent": "Again, it would be very silly to use spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "It's much more expensive.",
                    "label": 0
                },
                {
                    "sent": "But it is useful to understand the way they explain why their algorithm is efficient or is successful at all.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to show you how they have even nicer result in that one.",
                    "label": 0
                },
                {
                    "sent": "But anyway, for this data we have three Gaussians here, and we compute the affinity matrix.",
                    "label": 0
                },
                {
                    "sent": "That corresponds to these three Gaussians, so it looks roughly like block block diagonal.",
                    "label": 0
                },
                {
                    "sent": "If you can see from where you sit.",
                    "label": 0
                },
                {
                    "sent": "We then compute the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "This would be the first eigenvector V1.",
                    "label": 0
                },
                {
                    "sent": "This would be the second eigenvector and the third eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And the Matrix V would simply be V1V2V3 packed as columns.",
                    "label": 0
                },
                {
                    "sent": "We have the metrics with the normalized each of the rows of, which means we take the first item here in the first item in each of these vectors we normalize them to be equal to 1.",
                    "label": 0
                },
                {
                    "sent": "We do it for each of these.",
                    "label": 0
                },
                {
                    "sent": "Three entries and we get this three eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "If this is VV.",
                    "label": 0
                },
                {
                    "sent": "Then after normalizing, we get you.",
                    "label": 0
                },
                {
                    "sent": "This is you.",
                    "label": 0
                },
                {
                    "sent": "And we now run K means on the rows of you and we get this partitioning.",
                    "label": 1
                },
                {
                    "sent": "Now you can see already that you is perfectly aligned with the data itself, which means that the first entries for each of these eigenvectors identical, and then the next batch of entries which correspond to the next.",
                    "label": 0
                },
                {
                    "sent": "To the next day, Gaussian are also identical in all tree in all the tree eigenvectors and so on.",
                    "label": 0
                },
                {
                    "sent": "So running K means on the rows of you is trivial, is like finding tree clusters, where each cluster is like a point.",
                    "label": 0
                },
                {
                    "sent": "We will see that in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do they explain this?",
                    "label": 0
                },
                {
                    "sent": "How do they?",
                    "label": 0
                },
                {
                    "sent": "I mean, they're trying to come up with this algorithm and now they're trying to explain why why it works.",
                    "label": 0
                },
                {
                    "sent": "So to do that they're saying, let's look at the ideal case.",
                    "label": 0
                },
                {
                    "sent": "What would be the ideal case?",
                    "label": 0
                },
                {
                    "sent": "The ideal case would be when we have three clusters, for example, and they are far away from each other.",
                    "label": 0
                },
                {
                    "sent": "There well separated.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, if there were separated and we take Sigma to be large enough Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Being the.",
                    "label": 0
                },
                {
                    "sent": "Length scale definitive function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have this kind of scenario, so we have three clusters and their separated in Sigma is large, W is approximately block diagonal, where on the diagonal with blocks of matrices each of them is 1.",
                    "label": 0
                },
                {
                    "sent": "Hey, which is W multiplied by both sides by little minus half it's going to be like that again block diagonal.",
                    "label": 0
                },
                {
                    "sent": "It's easy to compute for this case.",
                    "label": 0
                },
                {
                    "sent": "Is the computer eigenvectors going to be what we see here?",
                    "label": 0
                },
                {
                    "sent": "Eigenvectors would be simply indicator function, normalize each of them.",
                    "label": 0
                },
                {
                    "sent": "And if we normalize the rows of each of these of each row of this matrix V, we're going to end up with you, which again is like an indicator function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It looks very similar to what we see here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is very similar to what we see here.",
                    "label": 0
                },
                {
                    "sent": "So we have if we look at you we have eigenvectors which are orthogonal and they can serve as indicators to the class assignment.",
                    "label": 0
                },
                {
                    "sent": "Of the of the of the data points.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only problem with this explanation is that he doesn't really explain the kind of results that they get.",
                    "label": 0
                },
                {
                    "sent": "This is from the same paper, so they show.",
                    "label": 0
                },
                {
                    "sent": "For example, it wasn't need 2001, four.",
                    "label": 0
                },
                {
                    "sent": "If they apply to this kind of data set, it should have an S. Here it disappeared when I copied it, but it should have an S. They showed that for this data set for example, they get 8 clusters.",
                    "label": 0
                },
                {
                    "sent": "This this is interesting if you look here.",
                    "label": 0
                },
                {
                    "sent": "If they tell the algorithm to find 2 clusters, this is what it gets.",
                    "label": 0
                },
                {
                    "sent": "If they tell it to find tree clusters.",
                    "label": 0
                },
                {
                    "sent": "This is what it gets.",
                    "label": 0
                },
                {
                    "sent": "And this data set is.",
                    "label": 0
                },
                {
                    "sent": "It couldn't be really more.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different than this kind of data set.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit, but still their algorithm works and this is the kind of results they get.",
                    "label": 0
                },
                {
                    "sent": "So why using dragon vectors?",
                    "label": 0
                },
                {
                    "sent": "We still don't know, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe we know a little bit later, so just to clarify, the algorithm has to take K, the number of clusters right in as an input, right?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all algorithms assume that we know the number of class.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the number of clusters dictates how many eigenvectors we're going to find.",
                    "label": 0
                },
                {
                    "sent": "It's given to us by the user.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anymore question quick question.",
                    "label": 0
                },
                {
                    "sent": "So the initial graphs of course are fully connected.",
                    "label": 0
                },
                {
                    "sent": "It could be both.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the things things I was going to talk about later.",
                    "label": 0
                },
                {
                    "sent": "It could be fully connected and you could sparsified and there's not going to be an issue of some pathological cases which we can maybe think of, but you know in all realistic datasets there isn't going to be much difference between them.",
                    "label": 0
                },
                {
                    "sent": "The condition that you need to keep in mind is that even if you specify the graph, you always.",
                    "label": 0
                },
                {
                    "sent": "Or from if we if we want to analyze the algorithm, we always need to assume that the graph is still fully connected fully.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not disconnected.",
                    "label": 0
                },
                {
                    "sent": "We don't buy sparsifying it we don't get two different subgraphs to disconnected subgraphs.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is what you said.",
                    "label": 0
                },
                {
                    "sent": "It's not a fully connected in the sense that each node is connected to all of the rest of the nodes.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Well, it could be.",
                    "label": 0
                },
                {
                    "sent": "It could be the case and it could be.",
                    "label": 0
                },
                {
                    "sent": "It could be sparse.",
                    "label": 0
                },
                {
                    "sent": "Is going to get very similar results.",
                    "label": 0
                },
                {
                    "sent": "I'm going to see why later, but what I am assuming is that from if we if we begin from any point or any node in the graph and we start working on the edges.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be able to reach any other node.",
                    "label": 0
                },
                {
                    "sent": "Not directly because I'm not connected to all of the rest of the points, but if I follow that the edges, then I could theoretically get to any other notes.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're expressing surprise little bit of these results and you understand them.",
                    "label": 0
                },
                {
                    "sent": "Obviously better better than I do, so I'm trying to catch up to your intuition here.",
                    "label": 0
                },
                {
                    "sent": "You are expressing some surprise.",
                    "label": 0
                },
                {
                    "sent": "Why is it?",
                    "label": 0
                },
                {
                    "sent": "Is it that the rings that the three when the cluster of three the upper right should not have come out as cleanly as it is?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll tell you why, the reason why I am expressing surprise is then when I first looked at it and I saw that I couldn't really understand why this would do anything reasonable.",
                    "label": 0
                },
                {
                    "sent": "When you have some affinity matrix and compute some eigenvectors and you do K means you normalized eigenvector, you do all these kind of like black magic and you get something that makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get this, so This is why I was surprised at the beginning.",
                    "label": 0
                },
                {
                    "sent": "If you're not surprised that it may be better understanding of it than me.",
                    "label": 0
                },
                {
                    "sent": "When I first saw it.",
                    "label": 0
                },
                {
                    "sent": "But This is why I expressed surprise.",
                    "label": 0
                },
                {
                    "sent": "And it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's the fact that it gets any anything which is reasonable hour surprised by it, not because it gets.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not like that I'm troubled about where exactly the cluster is cut here or here.",
                    "label": 0
                },
                {
                    "sent": "It's not what surprises me.",
                    "label": 0
                },
                {
                    "sent": "I'm surprised it got any good results at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was kind of related to my question about how they're connected, because if they're just connected like Delaunay triangulation connections that those are the only edges.",
                    "label": 0
                },
                {
                    "sent": "In the original graph, right?",
                    "label": 0
                },
                {
                    "sent": "Then all the cuts would have to happen in sort of spatially meaningful places.",
                    "label": 0
                },
                {
                    "sent": "Versus if all of the if each node has a weight to every other node in the entire graph, then.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I agree with your OK, but why?",
                    "label": 0
                },
                {
                    "sent": "Why would they do anything reasonable when we talk about cut?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, I agree with you.",
                    "label": 0
                },
                {
                    "sent": "I mean if you talk about cut this normalized cuts for example, which I mentioned before, then this would be a good place to cut it, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, even if it's fully connected, we know that the edges between this point and that point are much lower than the edges between this point and that point.",
                    "label": 0
                },
                {
                    "sent": "So it is a good place to cut, but why?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why would this do anything which is which is, you know, graph cut?",
                    "label": 0
                },
                {
                    "sent": "This is what I was surprised by the function.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mean some kind of function to make pretty metrics.",
                    "label": 0
                },
                {
                    "sent": "Is they used they they use the RBF there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radial basis function.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They chose the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is not the way I would do that, but when they wrote the paper, it was the 1st paper.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, the first topic that this kind of clustering in the way they chose it is they simply took a bunch of them.",
                    "label": 0
                },
                {
                    "sent": "And they.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They measured for each of them how dense each clusters are.",
                    "label": 0
                },
                {
                    "sent": "In this you matrix, so they took for each Sigma.",
                    "label": 0
                },
                {
                    "sent": "They computed V and then the computed U and they look for EU.",
                    "label": 0
                },
                {
                    "sent": "That gives you if you run K means on the on the rows of you they look for the Sigma that gives you the smallest distortion of the K means on this matrix.",
                    "label": 0
                },
                {
                    "sent": "They simply chose this Sigma so it was like simply trying a lot of sigmas and picking up the one that gave you the smallest distortion here.",
                    "label": 0
                },
                {
                    "sent": "OK, different Sigma correspond to different WS.",
                    "label": 0
                },
                {
                    "sent": "Different WS correspond to different movies and then different use and then simply use the Sigma that ended up with you with the lowest distortion.",
                    "label": 0
                },
                {
                    "sent": "The just traditional clustering based on pieces of space is just accepted.",
                    "label": 0
                },
                {
                    "sent": "Using the kernel function.",
                    "label": 0
                },
                {
                    "sent": "So is there any word meanings in using the setting required W equal to zero with their affinity?",
                    "label": 0
                },
                {
                    "sent": "By setting the several affinity to 0 or different.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "B0 you mean here?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're not going to see it in this kind of data sets.",
                    "label": 0
                },
                {
                    "sent": "The kind of data set that it becomes really crucial is if you took this data set exactly that one, but you edit one outlier far away from it.",
                    "label": 0
                },
                {
                    "sent": "If you had this case, if you added one point outlier right here, then you would have had a completely different results if you use W North, which is 0 or WNN, which is 1, and why this is the case, we're going to see later.",
                    "label": 0
                },
                {
                    "sent": "Sorry, let's change my questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you do the clustering based on the subspace is generated by corner PC.",
                    "label": 0
                },
                {
                    "sent": "You'd be able to get the similar result.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same meaning, the look at the eigenvectors of W itself.",
                    "label": 0
                },
                {
                    "sent": "RBF corners, but you're looking at the eigenvectors of W itself.",
                    "label": 0
                },
                {
                    "sent": "This is what you mean.",
                    "label": 0
                },
                {
                    "sent": "What do you mean, eigenvectors produced by the kernel itself?",
                    "label": 0
                },
                {
                    "sent": "Covariance matrix correlation matrices can be made by the affinity matrix is by using our wavefunction right?",
                    "label": 0
                },
                {
                    "sent": "And then can analyze the PCA.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're talking about W = 8 = A = W.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we don't have this normalization here.",
                    "label": 0
                },
                {
                    "sent": "If simply a = W and we do exactly the same thing with A equals W, you're asking if we're going to get the same result.",
                    "label": 0
                },
                {
                    "sent": "No, you're not going to get the same results.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get the same results it tested.",
                    "label": 0
                },
                {
                    "sent": "Mutation the normalization does make a difference in the main diff.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference it makes is that if you look here for example.",
                    "label": 0
                },
                {
                    "sent": "The main difference it makes is that it it gives you a different ordering of the eigenvectors going to different values for the eigenvectors, and you're also going to get a different ordering of the eigenvalues, so the eigenvectors corresponding to the to the largest eigenvalues are not necessarily going to be the same eigen vectors that correspond to the largest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If the matrix is normalized, or if it is not.",
                    "label": 0
                },
                {
                    "sent": "And Bowen also diagonals are going to be different, so why this is doing anything reasonable at all?",
                    "label": 0
                },
                {
                    "sent": "It's still not really clear.",
                    "label": 0
                },
                {
                    "sent": "But if you do W without normalization, you're going to be at less, less intuitive results.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was the first algorithm.",
                    "label": 0
                },
                {
                    "sent": "The second algorithm was.",
                    "label": 0
                },
                {
                    "sent": "And the second algorithm is is slightly different in the way we normalize W to derive a.",
                    "label": 0
                },
                {
                    "sent": "In this case we call it P. So instead of having having D minus half WD minus half, we use D -- 1 W. If we look at this matrix P. Then remember that D is a diagonal matrix, and on the diagonal we have the volume of each of the nodes.",
                    "label": 0
                },
                {
                    "sent": "What this does basically is it divides this multiplication.",
                    "label": 0
                },
                {
                    "sent": "It divides each row of W. By the sum of the elements of this row.",
                    "label": 0
                },
                {
                    "sent": "Which means that each row of P sums to one, it's it's not negative.",
                    "label": 1
                },
                {
                    "sent": "It might be 0, but it's not negative.",
                    "label": 1
                },
                {
                    "sent": "Each of the elements of P and each row sums to one, so we might think about this matrix P. As a stochastic matrix as a first order Markov Markov Walk matrix, which is why they called this paper around the world view of spectral class for spectral clustering or special segmentation.",
                    "label": 0
                },
                {
                    "sent": "So they define these metrics P and then they do exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "The computer K leading eigenvectors of P. They put all of them one after the other.",
                    "label": 1
                },
                {
                    "sent": "They run K means on the rows of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And they get similar results.",
                    "label": 0
                },
                {
                    "sent": "The main difference between this algo?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous one, is that instead of having this kind of normalization, we have data minus 1 W and we don't need to do.",
                    "label": 0
                },
                {
                    "sent": "Normalization of flow?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of V. Um?",
                    "label": 0
                },
                {
                    "sent": "So it looks a little bit simple.",
                    "label": 0
                },
                {
                    "sent": "And this is the kind of result that you get for this data set.",
                    "label": 0
                },
                {
                    "sent": "If you take these three rings and you compute their first eigenvectors corresponding to the affinity matrix of this data, you get these three eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Again, it looks like a perfect indication functions.",
                    "label": 0
                },
                {
                    "sent": "If you plot each each.",
                    "label": 0
                },
                {
                    "sent": "Is the matrix V If you plot the first, second, 3rd row of V in a 3 dimensional space, you're going to get these three points.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see this one here, but we have a Brown point where the green point and where the blue one, and each of these points basically is a pile of other points, which all of them correspond.",
                    "label": 0
                },
                {
                    "sent": "The Brown point here correspond to all of the points on the Brown cluster.",
                    "label": 0
                },
                {
                    "sent": "Brown ring.",
                    "label": 0
                },
                {
                    "sent": "The Green one correspond to all of the points on the green ring, and the same for the blue one.",
                    "label": 0
                },
                {
                    "sent": "So clustering this data set the rows of a few of the story with K means is trivial.",
                    "label": 0
                },
                {
                    "sent": "We simply have a pile of three points in.",
                    "label": 0
                },
                {
                    "sent": "In in these locations and we need to do K means it's immediate.",
                    "label": 0
                },
                {
                    "sent": "This is like K means on the roads of this.",
                    "label": 0
                },
                {
                    "sent": "Of this eigenvectors now doing the same thing with images.",
                    "label": 0
                },
                {
                    "sent": "What they did is this is one way of defining an affinity and affinity matrix or a graph in an image is what they did is they they took this image and for this specific experiment they used an edge to edge detector to find and edges and they use different features of each pixel.",
                    "label": 0
                },
                {
                    "sent": "For example they use the information that they got from there.",
                    "label": 0
                },
                {
                    "sent": "And they used RGB or the grayscale information that they got from the image itself.",
                    "label": 0
                },
                {
                    "sent": "And they built a graph sparse graph where they connected neighboring pixels.",
                    "label": 0
                },
                {
                    "sent": "And the value of the edge depends both on the value of the pixels and the information they got from the edge detector.",
                    "label": 0
                },
                {
                    "sent": "But the kind of segmentations they get is 4, six, and seven segments.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of results that they get when they segment images with their technique.",
                    "label": 0
                },
                {
                    "sent": "With this with this technique.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "I think it's more or less and saying look at the set P equals.",
                    "label": 0
                },
                {
                    "sent": "Good afternoon, we find out later.",
                    "label": 0
                },
                {
                    "sent": "Tonight is how it happened.",
                    "label": 0
                },
                {
                    "sent": "Kia.",
                    "label": 0
                },
                {
                    "sent": "That seems straightforward, but I do not know what the user testing method.",
                    "label": 0
                },
                {
                    "sent": "I need to pay me back.",
                    "label": 0
                },
                {
                    "sent": "What do you think means, oh this is something that.",
                    "label": 0
                },
                {
                    "sent": "They just suggested and I think it was in.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In Jordan and Weiss in this paper, they first suggested to use the K means, and then they simply kept on doing it in other papers.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the way the eigenvectors are in the in the eigenvectors, basically they call it.",
                    "label": 0
                },
                {
                    "sent": "If you look if you actually look at adding vectors, then K means is not really a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "You could do something more interesting than that, but this is simple and you don't want to complicate algorithm more than that and I'm not sure that it's after doing all this.",
                    "label": 0
                },
                {
                    "sent": "I only say computation, not sure that it's justifiable to do other there's other other algorithms in the in the eigenvector space.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "You could for example do like.",
                    "label": 0
                },
                {
                    "sent": "I mean, why not doing a loop right?",
                    "label": 0
                },
                {
                    "sent": "Or something that simply fits fits yourself if you have, like if you know that you have K clusters in the eigenvector space, you could do again spectral clustering on the eigenvector space.",
                    "label": 0
                },
                {
                    "sent": "But what they found or what you could you could see if you experiment enough.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that doing?",
                    "label": 0
                },
                {
                    "sent": "K means often works good enough if you do something else, you're not going to get better results even though you don't really have any good explanation to why we need to do that.",
                    "label": 0
                },
                {
                    "sent": "Play my booty.",
                    "label": 0
                },
                {
                    "sent": "Analyze the data centers themselves digon vectors themselves.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what this is.",
                    "label": 0
                },
                {
                    "sent": "What we will see in a minute.",
                    "label": 0
                },
                {
                    "sent": "The eigenvectors themselves are the ones that you need to look at, but then came in is simply a way to looking for clusters in you stack.",
                    "label": 0
                },
                {
                    "sent": "All of the investors one after the other.",
                    "label": 0
                },
                {
                    "sent": "You could do other things, but you could also.",
                    "label": 0
                },
                {
                    "sent": "You could also look.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each of the eigenvectors by itself I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about it in a second, the third one and the last one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk.",
                    "label": 0
                },
                {
                    "sent": "Here is the one.",
                    "label": 0
                },
                {
                    "sent": "This is the first one.",
                    "label": 0
                },
                {
                    "sent": "Actually it was published in 2000.",
                    "label": 0
                },
                {
                    "sent": "Here the metrics they use is D -- W. I am and they don't normalize it here, but I can problem that they solve.",
                    "label": 0
                },
                {
                    "sent": "Is this one here?",
                    "label": 0
                },
                {
                    "sent": "If this is Elder Laplacian?",
                    "label": 0
                },
                {
                    "sent": "The man is W. When the eigenvector, the investor problem they solve is this one, and if we multiply by detail minus one, we get basically these minus one LV equals Lambda V. So even though it is presented like that, so I I presented the same here it is a normalized Laplacian problem that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at so they define this matrix D -- W, they compute its eigenvectors and they do pretty much the same thing as male and cheated in the in the in the in the paper.",
                    "label": 0
                },
                {
                    "sent": "I just mentioned before, which means that they take all the K eigenvectors and then they do K means on the roads of this matrix and the kind of examples they show here is that if they take this image which has a noisy step on it and they graph here, the graph here again, each pixel is a node of the graph, and if you compute an eigenvector you basically get Eigenvector tells your value, it gives you a value to each of the nodes, so you can plot the eigenvector as an image as well.",
                    "label": 1
                },
                {
                    "sent": "If this is the original image, this is the second largest, the second smallest.",
                    "label": 0
                },
                {
                    "sent": "Sorry there are.",
                    "label": 0
                },
                {
                    "sent": "They actually correspond to the second smallest eigenvalue, and if you do, K means on this kind of eigenvector.",
                    "label": 1
                },
                {
                    "sent": "This is the kind of partitioning you get at the same thing, with three segments in this image gives you 1, two and three segments.",
                    "label": 0
                },
                {
                    "sent": "If you look at the 1st three eigenvectors and do K means on.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you said that it looks to you to say pretty much the same doing this normalization or that normalization.",
                    "label": 0
                },
                {
                    "sent": "So this is what this slide is about.",
                    "label": 0
                },
                {
                    "sent": "They are very closely related.",
                    "label": 0
                },
                {
                    "sent": "If we look at the first one that I introduced in Jordan and Weiss Weiss, we look we see this kind of matrix.",
                    "label": 0
                },
                {
                    "sent": "Definitely metrics the eigenvector problem lacking lacking problems that we solve is that one form a line.",
                    "label": 0
                },
                {
                    "sent": "She we solve this one and fortune Malik we solve the third one.",
                    "label": 0
                },
                {
                    "sent": "Now let's see what we can do here.",
                    "label": 0
                },
                {
                    "sent": "How can we move from different to that one?",
                    "label": 0
                },
                {
                    "sent": "If you look at the and NJW the first one eigen problem and we multiply both sides by D to the minus half we get this expression here.",
                    "label": 0
                },
                {
                    "sent": "And this is pretty much the same as that expression.",
                    "label": 0
                },
                {
                    "sent": "If we define V to be.",
                    "label": 0
                },
                {
                    "sent": "Tied to the minus Alpha you.",
                    "label": 0
                },
                {
                    "sent": "So if we define Vita beaded minus one, which we did myself, you we get this this problem and that problem to be identical.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the Shane Malik problem, which is this normalized pricing problem and we do something similar, we get that Lambda which is the eigenvalue of melenci equals to 1 -- 1 minus mu, where mu is the eigenvalue of Shane Malik.",
                    "label": 0
                },
                {
                    "sent": "So we simply have.",
                    "label": 0
                },
                {
                    "sent": "Mirror image of all of Dragon values.",
                    "label": 0
                },
                {
                    "sent": "Each eigenvalue I should say each eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Here Lambda is between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "So taking one minus simply gives us a mirror image.",
                    "label": 0
                },
                {
                    "sent": "Of the eigenvalues of the ordering of values.",
                    "label": 0
                },
                {
                    "sent": "This is why taking the largest eigenvectors corresponding to the largest eigenvalues here or taking the one corresponds to the smallest.",
                    "label": 0
                },
                {
                    "sent": "Here gives you the same eigenvectors at the end.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So they are very closely related and you can move from each if you know the solution for one of them, you can very easily move to the solution of the other one.",
                    "label": 0
                },
                {
                    "sent": "The only thing that we need to keep in mind is that if you want to use in Jordan advice, you need to do this extra normalization of rose in the eigenvectors, which you don't need to do in this two, and the reason that you don't need to do that is exactly this multiplication here.",
                    "label": 0
                },
                {
                    "sent": "If we do normalization of rows of U is like scaling each of the dimensions and you get this scaling each of the of each of the dimensions built in already.",
                    "label": 0
                },
                {
                    "sent": "Here in the solution.",
                    "label": 0
                },
                {
                    "sent": "And if you experiment enough, you can see that if you do use this solution for this problem.",
                    "label": 0
                },
                {
                    "sent": "This normalization here.",
                    "label": 0
                },
                {
                    "sent": "Saves you the need to do the normalization, extra normalization that they do in engine Jordan advice, but the picture is what I'm trying to say here is that they are all closely related and if you do one, you're very likely to get the results of another one as well.",
                    "label": 0
                },
                {
                    "sent": "Anywhere any question about this one is is a very good time to ask now.",
                    "label": 0
                },
                {
                    "sent": "But they're not identical.",
                    "label": 0
                },
                {
                    "sent": "They're not there there.",
                    "label": 0
                },
                {
                    "sent": "There there exists some datasets where you could get different.",
                    "label": 0
                },
                {
                    "sent": "Different results, so it depends where, so we see.",
                    "label": 0
                },
                {
                    "sent": "For example, we change Malik Malik.",
                    "label": 0
                },
                {
                    "sent": "They basically stick about about by partitioning of data.",
                    "label": 0
                },
                {
                    "sent": "So we talk about binary clustering and if you want if you want to get more than a binary clustering, you need to do some kind of hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "Even though they speak about other other methods as well.",
                    "label": 0
                },
                {
                    "sent": "But this she is exactly that she so the if we look at the the way I describe the algorithm here and we look at this solution for Shane Malik or Melenci.",
                    "label": 0
                },
                {
                    "sent": "We basically have the same.",
                    "label": 0
                },
                {
                    "sent": "This is identical, this exactly the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "In both of them, the difference being.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I guess This is why they called their paper this way.",
                    "label": 0
                },
                {
                    "sent": "Is that this is a random walk view of of image segmentation or separate spectral segmentation?",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily a new algorithm, but it is a new view.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which they didn't really realize this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or um.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but they're not accept this too.",
                    "label": 0
                },
                {
                    "sent": "They're not, they're not identical, and if you look at other algorithms they're also not identical, even though they all follow pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "Same idea.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so why?",
                    "label": 0
                },
                {
                    "sent": "Why is it good at all?",
                    "label": 0
                },
                {
                    "sent": "Why would looking at the eigenvectors?",
                    "label": 0
                },
                {
                    "sent": "We do anything that corresponds to doing something like normalized cuts so insane.",
                    "label": 0
                },
                {
                    "sent": "Malik paper from 2000 they showed basically that if you want to do that, if you want to minimize the normalized cut then you can approximate the solution for this by looking at a solution for the Laplacian eigen problem.",
                    "label": 0
                },
                {
                    "sent": "So this is this is one way of trying to understand this is an approximation?",
                    "label": 0
                },
                {
                    "sent": "Why would it?",
                    "label": 0
                },
                {
                    "sent": "Why is it?",
                    "label": 0
                },
                {
                    "sent": "Why is it any good to look at eigenvectors?",
                    "label": 0
                },
                {
                    "sent": "Because you can show that this is it is approximating the solution for this combinatorial problem.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about it more, but I'm going to talk about this to explanation.",
                    "label": 0
                },
                {
                    "sent": "One would be to look at the graph Laplacian and related to smooth functions.",
                    "label": 1
                },
                {
                    "sent": "And the other one would be to look at the random walks view and see how it relates to what Mailer called Marina Melody, she called it piecewise constant Eigenvectors, which are special case.",
                    "label": 0
                },
                {
                    "sent": "Of smooth functions.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first television, the passion is the minus W this is the one that chain molecules and if we look at these metrics demands, W is a nice property which if you will take this quadratic form V transpose LV, then it equals this expression.",
                    "label": 0
                },
                {
                    "sent": "VN is the NTH entry of VV being dragon vector of V being the.",
                    "label": 0
                },
                {
                    "sent": "Vector, which we're going to use this and eigenvector, but this this quadratic form equals this expression.",
                    "label": 0
                },
                {
                    "sent": "Now we've we solve the solution LV equals Lambda V, and we assume that V is normalized, which doesn't limit us.",
                    "label": 0
                },
                {
                    "sent": "We can always assume that.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the normalized and we multiply V transpose, we multiply each side by V transpose.",
                    "label": 0
                },
                {
                    "sent": "We get that Lambda equals V transpose LV, which means that Lambda equals this expression.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at this expression, what it basically does is it computes something like a distortion.",
                    "label": 0
                },
                {
                    "sent": "It says if WNM is I then I'm going to give a lot of weight to the difference between VM and VM.",
                    "label": 0
                },
                {
                    "sent": "If WNM is low then I'm not going to care a lot about how different they are.",
                    "label": 0
                },
                {
                    "sent": "So what it does?",
                    "label": 0
                },
                {
                    "sent": "Basically, if we want to minimize if we want to find out if, well if we have a small Lambda, then we can.",
                    "label": 0
                },
                {
                    "sent": "We can imply we can learn from it that this vector V. Is it does similar values to each other, two different entries of the of the vector?",
                    "label": 0
                },
                {
                    "sent": "They are similar if the affinity between these entries is I.",
                    "label": 0
                },
                {
                    "sent": "This means that if we have a graph and we know the some edges, are I then the eigenvector which corresponds to a function of this over these nodes?",
                    "label": 0
                },
                {
                    "sent": "The functional on this graph diagram vector would be fairly smooth.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is, for example a smooth function, if 2.",
                    "label": 0
                },
                {
                    "sent": "Nodes are close to each other and the edge between them is.",
                    "label": 0
                },
                {
                    "sent": "I then the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "In the entries that correspond to these two nodes which are close to each other, it's going to be similar to each other.",
                    "label": 0
                },
                {
                    "sent": "This is what it means.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at it, yeah.",
                    "label": 0
                },
                {
                    "sent": "The W's are all non negative right?",
                    "label": 0
                },
                {
                    "sent": "Yeah 'cause if they went negative then you know this thing could could go right right right L is the minus W so the entries of L could be negative or most of them are negative except diagonal.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the quadratic form then it is positive.",
                    "label": 0
                },
                {
                    "sent": "Yeah so all the eigenvalues of Laplacian for example are a positive or negative.",
                    "label": 0
                },
                {
                    "sent": "It is zero.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalue is 0.",
                    "label": 0
                },
                {
                    "sent": "If V is all equal, is all once and you can show that it has one eigenvector, which is all ones.",
                    "label": 0
                },
                {
                    "sent": "So looking at the eigenvectors of the Laplacian itself gives us a smooth function over the graph and a smooth function of the graph.",
                    "label": 0
                },
                {
                    "sent": "What it does is it puts similar values on nodes that are close to each other and this similar values are nodes that are far away from each other, which starts to make sense why it's any good to look at eigenvectors of this kind of matrices.",
                    "label": 0
                },
                {
                    "sent": "I hope it starts to make sense.",
                    "label": 0
                },
                {
                    "sent": "If you look at the normalized version of of the of the Laplacian, so you could look at the left, right normalized version, which is that you can show that it is closely related to the one that in Jordan and Wise use, and if you look at the quadratic form with this kind of normalized matrix then you get something similar to that one only, only you normalize each of the entries of the eigenvector by the volume.",
                    "label": 0
                },
                {
                    "sent": "Of that node, this is a bad thing to do because what it says is that you don't really care about the values on each of the nodes, but you care about the ratio between the value on the nodes and the volume of the nodes, and there is no reason to think that the volume of the node have to do anything.",
                    "label": 0
                },
                {
                    "sent": "Anything with the cluster assignment.",
                    "label": 0
                },
                {
                    "sent": "So what we really need to look at is this kind of expression, but not that kind of expression and the way they overcome this.",
                    "label": 0
                },
                {
                    "sent": "This this problem here that you also consider the the volume of the node is by normalizing the rows of the matrix V to get the metrics here.",
                    "label": 0
                },
                {
                    "sent": "Different kind of normalization and less normalization, which is D -- 1 L gives you the random walk matrix.",
                    "label": 0
                },
                {
                    "sent": "Here I don't know any way of any result that relates the quadratic form to this kind of a nice expression, but if we look at the metrics D -- 1 W and we look at the eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "D -- 1 W We have a result that tells us white it makes any sense to use them.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But before we do that, maybe maybe I should pose for questions about why using the Laplacian, yeah?",
                    "label": 0
                },
                {
                    "sent": "So the explanation is very good, but I've gotten a little confused now with respect to our initial goal of the approximating the normalized cuts.",
                    "label": 0
                },
                {
                    "sent": "So if, for instance.",
                    "label": 0
                },
                {
                    "sent": "You have these clusters in one cluster.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the same separation, but really lots of points.",
                    "label": 0
                },
                {
                    "sent": "Lots of data, all landing sitting on top of itself.",
                    "label": 0
                },
                {
                    "sent": "The eigenvectors are going to be will try to span the space, but it will be very heavily sort of biased right in the.",
                    "label": 0
                },
                {
                    "sent": "No, the eigenvectors would be.",
                    "label": 0
                },
                {
                    "sent": "If we look we talking about eigenvectors of L Now or we talk about which eigenvectors are you referring to.",
                    "label": 0
                }
            ]
        }
    }
}