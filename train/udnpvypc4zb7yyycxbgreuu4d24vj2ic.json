{
    "id": "udnpvypc4zb7yyycxbgreuu4d24vj2ic",
    "title": "Fast Food: Approximating Kernel Expansion in Loglinear Time",
    "info": {
        "author": [
            "Alex Smola, Amazon"
        ],
        "published": "Jan. 18, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_smola_kernel/",
    "segmentation": [
        [
            "So."
        ],
        [
            "What's the problem?",
            "If you use a kernel method, right?",
            "It's the kernel expansion.",
            "And this is usually what kills you.",
            "So if you have a fix is a linear combination, some of Alpha IKFX on X.",
            "That sounds really awesome.",
            "If you have, you know a reasonable amount of data points.",
            "But the problem is as the number of observations grows and well, we have that problem at Google.",
            "This can actually become ridiculously expensive.",
            "It's one of the reasons why at the moment this is amazing, resurgence of, shall we say, neural networks.",
            "And two.",
            "To illustrate the problem a little bit, I've just made this small table here to show a little bit what the issues are.",
            "So if you do the naive thing, then you know if you want to train a kernel method.",
            "So by the way this expansion here is essentially inevitable if you have a noisy problem.",
            "I mean it's going to be order M and not every data point is going to be picked, but at least you know if you have a certain fraction of noisy data then you won't be able to avoid this.",
            "So if you train an SVM naively then you will at least have to evaluate order M squared entries of the kernel matrix.",
            "So CPU training is order M ^2 * D dimensionality of data.",
            "A test time you know for a new observation you need to compute all the kernels.",
            "OK, so or MD.",
            "For the memory, well, you know you have to actually keep the data around and the same thing for testing.",
            "This can be improved so they are reduced set methods.",
            "I mean the states back about 1015 years.",
            "Training is just as expensive, at least in the simplest cases.",
            "In terms of memory and CPU.",
            "But then you project down to a smaller space that causes some extra computation, but you know you can somehow work around that.",
            "So afterwards it's order N * D N is the number of basis functions that you're going to pick, and that's reasonable.",
            "Then there is a number of low rank methods.",
            "So for instance, Nystrom approximation, sparse grid matrix approximation, positive diagonal pivoting.",
            "All those methods will basically do the following thing.",
            "They will slice out a part of the Matrix and use that to perform a full rank up to perform a low rank approximation of the entire kernel matrix.",
            "And since things are positive semidefinite, you can get away with a lot, so you know we've now reduced the training time to order M * N * D. There's actually an order.",
            "You know in Square D term as well, but since then is smaller than him, it's not a big deal, and sometimes you have to invert the matrix.",
            "So I'm being very generous here in the complexity bounds.",
            "CPU test I'm OK, you have this reduced set, so this gets better.",
            "And then there was this really cool algorithm.",
            "Better human raped called random kitchen sinks and if you look at it first, it doesn't look actually any better.",
            "Basically what they do is they randomly generate basis functions in those basis.",
            "Functions are behaved in such a way.",
            "That in expectation this will correspond to a proper kernel.",
            "And it's a very nice idea.",
            "They show that basically it works as well as the real thing, and it's always been bugging me as follows.",
            "So basically at some point I have to draw.",
            "These random basis functions, but they have to store all those coefficients and you basically are spending most of your CPU time multiplying a random matrix with a vector.",
            "I'll show you more details later.",
            "And this is actually very costly.",
            "First of all, you have to store the random matrix.",
            "Secondly, it's kind of awkward to multiply with, so can we replace this by something that's much cheaper?"
        ],
        [
            "I'm going to show you now is a technique we called it fast food, well as an improvement over random kitchen sinks, which gives you the following guarantees.",
            "I don't know how much you can see in the back.",
            "Maybe I shouldn't have picked green, but it's basically for training its order.",
            "M * N log D. So the big difference is that it's not linear in the dimensionality anymore, but logarithmic.",
            "I think as good as you can probably get it.",
            "Aim is the sample size.",
            "Well, OK, you need to look at every instance at least once in is the number of basis functions.",
            "Well, you need to evaluate them at least once and then log D is as cheap as you can get away with.",
            "The really cool thing is the test.",
            "I'm listing flies so it's order in number of basis functions log dimensionality.",
            "So this means your cell phone can do it.",
            "The really cool thing is that the memory footprint now is linear in the number of basis functions.",
            "No dimensionality dependence whatsoever.",
            "So this is a considerable improvement of random kitchen sinks, and I'm going to show you how to do it."
        ],
        [
            "So let's start with something that's really old from 1932 Bochner's theorem, and essentially it says that while K of X&X prime for an RBF kernel can be written as the integral over some measure, the P of Omega and I'm going to assume that it's a probability measure.",
            "Otherwise you can just re normalize it and then here you have to free basis.",
            "OK, we've all seen that at some point.",
            "For instance in RBF kernel doesn't and the really cool idea of remuneration was to say, hey, this is a distribution.",
            "So if we don't like the integral, we can sample.",
            "That's exactly what they do.",
            "They draw Omega JS from this distribution.",
            "Peer formiga.",
            "And now you have an explicit feature space representation for this kernel, and if you can compute this experience cheaply, then you're done.",
            "So why is this useful?",
            "Because now you can use basically any primal space method that you would have used.",
            "Otherwise that's a good thing.",
            "So for Gaussian RBF's, this is essentially what they described in their paper.",
            "You need to draw those Omega Jays from a normal distribution.",
            "So this is very cheap, but it's very straightforward to extend that to other districts other kernels.",
            "Basically you just draw from that Gaussian distribution, and you then draw separately a radial part, and you're done.",
            "So the problem is that you know once you do that, you still have to do essentially a matrix vector multiply, right?",
            "As you need to store all those Omega JS so we have some matrix, large capital Amiga and have to compute Omega Times X.",
            "Once I have that, I'm done right because then I just have to compute the free basis of that.",
            "That's constant operation per dimension.",
            "But this matrix vector multiply has always been bugging me.",
            "This beautiful algorithm, but that's what kills it.",
            "So can we get rid of this?",
            "Can we do something cheaper?",
            "And essentially the question is, can we design A matrix Omega that looks almost as if it were Gaussian, but it's computationally much more attractive, so that's the problem that we're solving."
        ],
        [
            "So here's what we're going to do.",
            "First of all, let's take this large matrix Omega.",
            "So it's an.",
            "Invite the matrix and let's break it up into dyd blocks.",
            "And I'm going to discuss one of those dyd blocks right now and then you can easily generate something that's larger by just, you know, generating a nobody of them.",
            "OK, so here's what I'm going to do.",
            "I'm going to approximate this matrix Omega by some matrix M, which is given by this beast here.",
            "Now that doesn't look very simple, but it actually is.",
            "This is a scaling matrix diagonal scaling.",
            "This is a harder mark matrix.",
            "The diagonal Gaussian a permutation.",
            "Harder Mart and a diagonal matrix with just plus minus one entries.",
            "So.",
            "What do those various pieces do?",
            "So this first matrixes stag.",
            "And also I can easily multiply and ordering time and this deals with the spectrum.",
            "So you can do multiple kernel learning and other things with it.",
            "So basically this is the radial component of let's say a Gaussian matrix.",
            "The harder mark matrix is this object here.",
            "So it's basically harder.",
            "So it's recursively defined as follows.",
            "Harder Mart, harder Mart, harder Mart, minus auto Mart and H1 is 1.",
            "And some people insist that they should call it a Walsh Watermark matrix, but in any case, the good news is you can multiply with this matrix in log linear time.",
            "So this matrix.",
            "So basically this matrix times a vector.",
            "I can do indeed log D time.",
            "That's the key trick.",
            "OK, geez, this random Gaussian diagonal matrix?",
            "And this is going to be enough randomness to essentially fake a full Gaussian.",
            "Piza permutation that ensures that the eigenspaces that basically the basis of H&H here are scrambled relative to each other.",
            "And then B is, well, a binary random matrix that's thrown in for good measure to decorrelate the entries OK. Now the nice thing is, as you can immediately see that multiplying this matrix is order D log D. Because the deal of the time comes in here and all the other parts is G Prime, BR or D. Now how do we get something that's order N log D?",
            "Well, remember we have to take in over the blocks, so that's what gives us in log D. And each of those blocks is drawn independently.",
            "So you can easily see that this thing is fast now.",
            "Is it correct?"
        ],
        [
            "OK.",
            "So in order to see the correct, the expectation is correct.",
            "Let's first go and ignore it.",
            "And just focus on a single row of this object here.",
            "Now a single row is a linear combination of Gaussians with zero mean and then OK.",
            "I have those plus minus ones here.",
            "But since there's a zero mean random variable, if I know multiplied with plus with plus minus one variable, it doesn't matter, it still stays Gaussian.",
            "So you can easily see that all the entries are Gaussian and by squinting at it a little bit harder you can see that by starting out with angle.",
            "Since I also end up with an independent Gaussians.",
            "'cause this was easy.",
            "Unfortunately, if you check this all the rows in this matrix here have the same links, and that's given actually by the Frobenius norm of G. So so in other words, the sum over all the diagonal entries, but it's actually not too bad, because then you can now easily go and uses to randomize those links.",
            "So if you don't like the Gaussian spectrum, but for instance like the maternal spectrum, you can easily in free space sample from the maternal spectral distribution, which is a lot easier to do rather than sampling from a basal function.",
            "Um, OK. And then there's a nice theorem, and I'm not going to go into a lot of details, but it basically says that things are well behaved where you go and workout.",
            "The covariance of the attributes, and I think they don't have a huge amount of time, but basically over about one page or so you can prove stuff."
        ],
        [
            "Now, does it actually do anything useful?",
            "OK, So what we did is we compare to random kitchen sinks.",
            "Given that random kitchen sinks are well behaved.",
            "Random kitchen sinks is the red curve.",
            "Ours is the green curve in there.",
            "Both work the same.",
            "Mind you, there's another trick that you could use, and this is what we call an 850 you Ristic, where you basically just use the.",
            "Converse of a subsample random Fourier transform, which usually goes from high to low dimensions.",
            "We use that to push from low to high.",
            "In other words, we take a free transfer matrix, pick a couple of random columns.",
            "That's also fast."
        ],
        [
            "Actually, we started Interestingly.",
            "That actually performs better.",
            "In practice, however, you can see that our method basically performs as well as random kitchen sinks, except that it's blazingly much faster."
        ],
        [
            "To drive that point home a little bit further.",
            "So here are some data sets.",
            "Basically, picked data sets that were of reasonable size.",
            "And we compared exact the Nystrom method, random kitchen sinks, fast food and then the 50 version and what you can see is that you can't really see any difference.",
            "In other words, this thing works at least as well.",
            "It works probably slightly better but not statistically significantly better, but it was never the point because we set out to approximate, you know, the proper kernel and lo and behold, we do.",
            "This is the interesting part, it's much faster.",
            "So I mean obviously.",
            "You know log D is much smaller than D for large D, right?",
            "And so you can immediately see that this gives you considerable speedups relative to random kitchen sinks, which already quite fast.",
            "And the key point here is really it requires an external memory.",
            "So you now can afford, you know, a million basis functions, and you need 4 megabytes.",
            "OK, it's not quite 4 megabytes, it's about 12 megabytes because you need to store a couple of things, but it's stuff that basically easily fits into your memory.",
            "I mean, your cell phone can do it.",
            "So."
        ],
        [
            "So where do we go from here?",
            "First of all, you know it lets you play with a lot of different spectral distributions, because if you think about it, the Gaussian RBF kernel is actually really stupid.",
            "So if you think about what happens if you have a Gaussian in high dimension, then essentially all the frequencies are the same.",
            "That's just the concentration of measure effect.",
            "That basically if you draw from a Gaussian, all the points are on the surface of a sphere, which means you're looking at the data with the same frequency in random directions.",
            "That's not necessarily what you want, so you would actually want to have a different spectral property.",
            "The other thing is, since this is so ridiculously cheap so you can afford a million dimensions, you can actually do multiple kernel learning if you want.",
            "We haven't quite figured out yet a.",
            "Nice joint Parameterisation, but you could just brute force directly optimize over that, except that this drives you very much into neural networks territory.",
            "The other thing is you can pick different localized basis functions.",
            "Basically anything that requires an inner product with a spherical isotropic matrix can be used for this, and so you can get similar expansions as a matter of fact.",
            "The original idea of the random kitchen sinks in away is actually Radford.",
            "Neal's paper from 94 where he shows that Gaussian process in neural networks with infinite number of basis functions are equivalent and you're basically driving this backwards, right?",
            "We're basically saying, OK?",
            "Here's an efficient mechanism of generating and more or less infinite number of basis functions.",
            "A similar trick works for matrix values functions.",
            "And then of course, if you want to go crazy, you can stack several layers of those things on top of it.",
            "Now this starts to look very much like a deep belief network.",
            "Mind you, back properties really easy because the inverse Hadamard can also be efficiently done.",
            "OK, it's not all just sunshine.",
            "The big problem is that.",
            "Computing the kernel itself is now really expensive, right?",
            "This is almost like an invariant theorem of difficulty.",
            "And the covariance operators that can be talked about before are also not explicitly available.",
            "So now you basically have a million by million objects and you need to come up with some low rank expansions.",
            "We have some ideas about it, but that's not work out yet.",
            "And then the obvious caveat is you never ever want to store explicit feature map.",
            "It's just so much cheaper to recompute it rather than to restore it.",
            "Basically your CPU is much faster than you memory, so never ever store those things.",
            "Don't even think of putting them on disk either, OK and that's all I wanted to say."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the problem?",
                    "label": 0
                },
                {
                    "sent": "If you use a kernel method, right?",
                    "label": 0
                },
                {
                    "sent": "It's the kernel expansion.",
                    "label": 0
                },
                {
                    "sent": "And this is usually what kills you.",
                    "label": 0
                },
                {
                    "sent": "So if you have a fix is a linear combination, some of Alpha IKFX on X.",
                    "label": 0
                },
                {
                    "sent": "That sounds really awesome.",
                    "label": 0
                },
                {
                    "sent": "If you have, you know a reasonable amount of data points.",
                    "label": 0
                },
                {
                    "sent": "But the problem is as the number of observations grows and well, we have that problem at Google.",
                    "label": 0
                },
                {
                    "sent": "This can actually become ridiculously expensive.",
                    "label": 0
                },
                {
                    "sent": "It's one of the reasons why at the moment this is amazing, resurgence of, shall we say, neural networks.",
                    "label": 0
                },
                {
                    "sent": "And two.",
                    "label": 0
                },
                {
                    "sent": "To illustrate the problem a little bit, I've just made this small table here to show a little bit what the issues are.",
                    "label": 0
                },
                {
                    "sent": "So if you do the naive thing, then you know if you want to train a kernel method.",
                    "label": 0
                },
                {
                    "sent": "So by the way this expansion here is essentially inevitable if you have a noisy problem.",
                    "label": 0
                },
                {
                    "sent": "I mean it's going to be order M and not every data point is going to be picked, but at least you know if you have a certain fraction of noisy data then you won't be able to avoid this.",
                    "label": 0
                },
                {
                    "sent": "So if you train an SVM naively then you will at least have to evaluate order M squared entries of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So CPU training is order M ^2 * D dimensionality of data.",
                    "label": 1
                },
                {
                    "sent": "A test time you know for a new observation you need to compute all the kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so or MD.",
                    "label": 0
                },
                {
                    "sent": "For the memory, well, you know you have to actually keep the data around and the same thing for testing.",
                    "label": 0
                },
                {
                    "sent": "This can be improved so they are reduced set methods.",
                    "label": 0
                },
                {
                    "sent": "I mean the states back about 1015 years.",
                    "label": 0
                },
                {
                    "sent": "Training is just as expensive, at least in the simplest cases.",
                    "label": 0
                },
                {
                    "sent": "In terms of memory and CPU.",
                    "label": 0
                },
                {
                    "sent": "But then you project down to a smaller space that causes some extra computation, but you know you can somehow work around that.",
                    "label": 0
                },
                {
                    "sent": "So afterwards it's order N * D N is the number of basis functions that you're going to pick, and that's reasonable.",
                    "label": 1
                },
                {
                    "sent": "Then there is a number of low rank methods.",
                    "label": 0
                },
                {
                    "sent": "So for instance, Nystrom approximation, sparse grid matrix approximation, positive diagonal pivoting.",
                    "label": 0
                },
                {
                    "sent": "All those methods will basically do the following thing.",
                    "label": 0
                },
                {
                    "sent": "They will slice out a part of the Matrix and use that to perform a full rank up to perform a low rank approximation of the entire kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And since things are positive semidefinite, you can get away with a lot, so you know we've now reduced the training time to order M * N * D. There's actually an order.",
                    "label": 0
                },
                {
                    "sent": "You know in Square D term as well, but since then is smaller than him, it's not a big deal, and sometimes you have to invert the matrix.",
                    "label": 0
                },
                {
                    "sent": "So I'm being very generous here in the complexity bounds.",
                    "label": 0
                },
                {
                    "sent": "CPU test I'm OK, you have this reduced set, so this gets better.",
                    "label": 1
                },
                {
                    "sent": "And then there was this really cool algorithm.",
                    "label": 0
                },
                {
                    "sent": "Better human raped called random kitchen sinks and if you look at it first, it doesn't look actually any better.",
                    "label": 0
                },
                {
                    "sent": "Basically what they do is they randomly generate basis functions in those basis.",
                    "label": 0
                },
                {
                    "sent": "Functions are behaved in such a way.",
                    "label": 0
                },
                {
                    "sent": "That in expectation this will correspond to a proper kernel.",
                    "label": 0
                },
                {
                    "sent": "And it's a very nice idea.",
                    "label": 0
                },
                {
                    "sent": "They show that basically it works as well as the real thing, and it's always been bugging me as follows.",
                    "label": 0
                },
                {
                    "sent": "So basically at some point I have to draw.",
                    "label": 0
                },
                {
                    "sent": "These random basis functions, but they have to store all those coefficients and you basically are spending most of your CPU time multiplying a random matrix with a vector.",
                    "label": 0
                },
                {
                    "sent": "I'll show you more details later.",
                    "label": 0
                },
                {
                    "sent": "And this is actually very costly.",
                    "label": 0
                },
                {
                    "sent": "First of all, you have to store the random matrix.",
                    "label": 0
                },
                {
                    "sent": "Secondly, it's kind of awkward to multiply with, so can we replace this by something that's much cheaper?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to show you now is a technique we called it fast food, well as an improvement over random kitchen sinks, which gives you the following guarantees.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much you can see in the back.",
                    "label": 0
                },
                {
                    "sent": "Maybe I shouldn't have picked green, but it's basically for training its order.",
                    "label": 0
                },
                {
                    "sent": "M * N log D. So the big difference is that it's not linear in the dimensionality anymore, but logarithmic.",
                    "label": 1
                },
                {
                    "sent": "I think as good as you can probably get it.",
                    "label": 0
                },
                {
                    "sent": "Aim is the sample size.",
                    "label": 1
                },
                {
                    "sent": "Well, OK, you need to look at every instance at least once in is the number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Well, you need to evaluate them at least once and then log D is as cheap as you can get away with.",
                    "label": 0
                },
                {
                    "sent": "The really cool thing is the test.",
                    "label": 0
                },
                {
                    "sent": "I'm listing flies so it's order in number of basis functions log dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So this means your cell phone can do it.",
                    "label": 0
                },
                {
                    "sent": "The really cool thing is that the memory footprint now is linear in the number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "No dimensionality dependence whatsoever.",
                    "label": 1
                },
                {
                    "sent": "So this is a considerable improvement of random kitchen sinks, and I'm going to show you how to do it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with something that's really old from 1932 Bochner's theorem, and essentially it says that while K of X&X prime for an RBF kernel can be written as the integral over some measure, the P of Omega and I'm going to assume that it's a probability measure.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can just re normalize it and then here you have to free basis.",
                    "label": 0
                },
                {
                    "sent": "OK, we've all seen that at some point.",
                    "label": 0
                },
                {
                    "sent": "For instance in RBF kernel doesn't and the really cool idea of remuneration was to say, hey, this is a distribution.",
                    "label": 0
                },
                {
                    "sent": "So if we don't like the integral, we can sample.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what they do.",
                    "label": 0
                },
                {
                    "sent": "They draw Omega JS from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Peer formiga.",
                    "label": 0
                },
                {
                    "sent": "And now you have an explicit feature space representation for this kernel, and if you can compute this experience cheaply, then you're done.",
                    "label": 0
                },
                {
                    "sent": "So why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Because now you can use basically any primal space method that you would have used.",
                    "label": 0
                },
                {
                    "sent": "Otherwise that's a good thing.",
                    "label": 0
                },
                {
                    "sent": "So for Gaussian RBF's, this is essentially what they described in their paper.",
                    "label": 0
                },
                {
                    "sent": "You need to draw those Omega Jays from a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is very cheap, but it's very straightforward to extend that to other districts other kernels.",
                    "label": 0
                },
                {
                    "sent": "Basically you just draw from that Gaussian distribution, and you then draw separately a radial part, and you're done.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that you know once you do that, you still have to do essentially a matrix vector multiply, right?",
                    "label": 0
                },
                {
                    "sent": "As you need to store all those Omega JS so we have some matrix, large capital Amiga and have to compute Omega Times X.",
                    "label": 0
                },
                {
                    "sent": "Once I have that, I'm done right because then I just have to compute the free basis of that.",
                    "label": 0
                },
                {
                    "sent": "That's constant operation per dimension.",
                    "label": 0
                },
                {
                    "sent": "But this matrix vector multiply has always been bugging me.",
                    "label": 0
                },
                {
                    "sent": "This beautiful algorithm, but that's what kills it.",
                    "label": 0
                },
                {
                    "sent": "So can we get rid of this?",
                    "label": 0
                },
                {
                    "sent": "Can we do something cheaper?",
                    "label": 0
                },
                {
                    "sent": "And essentially the question is, can we design A matrix Omega that looks almost as if it were Gaussian, but it's computationally much more attractive, so that's the problem that we're solving.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "First of all, let's take this large matrix Omega.",
                    "label": 0
                },
                {
                    "sent": "So it's an.",
                    "label": 0
                },
                {
                    "sent": "Invite the matrix and let's break it up into dyd blocks.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to discuss one of those dyd blocks right now and then you can easily generate something that's larger by just, you know, generating a nobody of them.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "I'm going to approximate this matrix Omega by some matrix M, which is given by this beast here.",
                    "label": 0
                },
                {
                    "sent": "Now that doesn't look very simple, but it actually is.",
                    "label": 0
                },
                {
                    "sent": "This is a scaling matrix diagonal scaling.",
                    "label": 1
                },
                {
                    "sent": "This is a harder mark matrix.",
                    "label": 1
                },
                {
                    "sent": "The diagonal Gaussian a permutation.",
                    "label": 0
                },
                {
                    "sent": "Harder Mart and a diagonal matrix with just plus minus one entries.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What do those various pieces do?",
                    "label": 0
                },
                {
                    "sent": "So this first matrixes stag.",
                    "label": 0
                },
                {
                    "sent": "And also I can easily multiply and ordering time and this deals with the spectrum.",
                    "label": 0
                },
                {
                    "sent": "So you can do multiple kernel learning and other things with it.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the radial component of let's say a Gaussian matrix.",
                    "label": 0
                },
                {
                    "sent": "The harder mark matrix is this object here.",
                    "label": 0
                },
                {
                    "sent": "So it's basically harder.",
                    "label": 0
                },
                {
                    "sent": "So it's recursively defined as follows.",
                    "label": 0
                },
                {
                    "sent": "Harder Mart, harder Mart, harder Mart, minus auto Mart and H1 is 1.",
                    "label": 0
                },
                {
                    "sent": "And some people insist that they should call it a Walsh Watermark matrix, but in any case, the good news is you can multiply with this matrix in log linear time.",
                    "label": 0
                },
                {
                    "sent": "So this matrix.",
                    "label": 0
                },
                {
                    "sent": "So basically this matrix times a vector.",
                    "label": 0
                },
                {
                    "sent": "I can do indeed log D time.",
                    "label": 0
                },
                {
                    "sent": "That's the key trick.",
                    "label": 0
                },
                {
                    "sent": "OK, geez, this random Gaussian diagonal matrix?",
                    "label": 0
                },
                {
                    "sent": "And this is going to be enough randomness to essentially fake a full Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Piza permutation that ensures that the eigenspaces that basically the basis of H&H here are scrambled relative to each other.",
                    "label": 0
                },
                {
                    "sent": "And then B is, well, a binary random matrix that's thrown in for good measure to decorrelate the entries OK. Now the nice thing is, as you can immediately see that multiplying this matrix is order D log D. Because the deal of the time comes in here and all the other parts is G Prime, BR or D. Now how do we get something that's order N log D?",
                    "label": 0
                },
                {
                    "sent": "Well, remember we have to take in over the blocks, so that's what gives us in log D. And each of those blocks is drawn independently.",
                    "label": 0
                },
                {
                    "sent": "So you can easily see that this thing is fast now.",
                    "label": 0
                },
                {
                    "sent": "Is it correct?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in order to see the correct, the expectation is correct.",
                    "label": 0
                },
                {
                    "sent": "Let's first go and ignore it.",
                    "label": 0
                },
                {
                    "sent": "And just focus on a single row of this object here.",
                    "label": 0
                },
                {
                    "sent": "Now a single row is a linear combination of Gaussians with zero mean and then OK.",
                    "label": 0
                },
                {
                    "sent": "I have those plus minus ones here.",
                    "label": 0
                },
                {
                    "sent": "But since there's a zero mean random variable, if I know multiplied with plus with plus minus one variable, it doesn't matter, it still stays Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So you can easily see that all the entries are Gaussian and by squinting at it a little bit harder you can see that by starting out with angle.",
                    "label": 0
                },
                {
                    "sent": "Since I also end up with an independent Gaussians.",
                    "label": 0
                },
                {
                    "sent": "'cause this was easy.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, if you check this all the rows in this matrix here have the same links, and that's given actually by the Frobenius norm of G. So so in other words, the sum over all the diagonal entries, but it's actually not too bad, because then you can now easily go and uses to randomize those links.",
                    "label": 0
                },
                {
                    "sent": "So if you don't like the Gaussian spectrum, but for instance like the maternal spectrum, you can easily in free space sample from the maternal spectral distribution, which is a lot easier to do rather than sampling from a basal function.",
                    "label": 0
                },
                {
                    "sent": "Um, OK. And then there's a nice theorem, and I'm not going to go into a lot of details, but it basically says that things are well behaved where you go and workout.",
                    "label": 0
                },
                {
                    "sent": "The covariance of the attributes, and I think they don't have a huge amount of time, but basically over about one page or so you can prove stuff.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, does it actually do anything useful?",
                    "label": 0
                },
                {
                    "sent": "OK, So what we did is we compare to random kitchen sinks.",
                    "label": 0
                },
                {
                    "sent": "Given that random kitchen sinks are well behaved.",
                    "label": 0
                },
                {
                    "sent": "Random kitchen sinks is the red curve.",
                    "label": 0
                },
                {
                    "sent": "Ours is the green curve in there.",
                    "label": 0
                },
                {
                    "sent": "Both work the same.",
                    "label": 0
                },
                {
                    "sent": "Mind you, there's another trick that you could use, and this is what we call an 850 you Ristic, where you basically just use the.",
                    "label": 0
                },
                {
                    "sent": "Converse of a subsample random Fourier transform, which usually goes from high to low dimensions.",
                    "label": 0
                },
                {
                    "sent": "We use that to push from low to high.",
                    "label": 0
                },
                {
                    "sent": "In other words, we take a free transfer matrix, pick a couple of random columns.",
                    "label": 0
                },
                {
                    "sent": "That's also fast.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, we started Interestingly.",
                    "label": 0
                },
                {
                    "sent": "That actually performs better.",
                    "label": 0
                },
                {
                    "sent": "In practice, however, you can see that our method basically performs as well as random kitchen sinks, except that it's blazingly much faster.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To drive that point home a little bit further.",
                    "label": 0
                },
                {
                    "sent": "So here are some data sets.",
                    "label": 0
                },
                {
                    "sent": "Basically, picked data sets that were of reasonable size.",
                    "label": 0
                },
                {
                    "sent": "And we compared exact the Nystrom method, random kitchen sinks, fast food and then the 50 version and what you can see is that you can't really see any difference.",
                    "label": 0
                },
                {
                    "sent": "In other words, this thing works at least as well.",
                    "label": 0
                },
                {
                    "sent": "It works probably slightly better but not statistically significantly better, but it was never the point because we set out to approximate, you know, the proper kernel and lo and behold, we do.",
                    "label": 0
                },
                {
                    "sent": "This is the interesting part, it's much faster.",
                    "label": 0
                },
                {
                    "sent": "So I mean obviously.",
                    "label": 0
                },
                {
                    "sent": "You know log D is much smaller than D for large D, right?",
                    "label": 0
                },
                {
                    "sent": "And so you can immediately see that this gives you considerable speedups relative to random kitchen sinks, which already quite fast.",
                    "label": 0
                },
                {
                    "sent": "And the key point here is really it requires an external memory.",
                    "label": 0
                },
                {
                    "sent": "So you now can afford, you know, a million basis functions, and you need 4 megabytes.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not quite 4 megabytes, it's about 12 megabytes because you need to store a couple of things, but it's stuff that basically easily fits into your memory.",
                    "label": 0
                },
                {
                    "sent": "I mean, your cell phone can do it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where do we go from here?",
                    "label": 0
                },
                {
                    "sent": "First of all, you know it lets you play with a lot of different spectral distributions, because if you think about it, the Gaussian RBF kernel is actually really stupid.",
                    "label": 0
                },
                {
                    "sent": "So if you think about what happens if you have a Gaussian in high dimension, then essentially all the frequencies are the same.",
                    "label": 0
                },
                {
                    "sent": "That's just the concentration of measure effect.",
                    "label": 0
                },
                {
                    "sent": "That basically if you draw from a Gaussian, all the points are on the surface of a sphere, which means you're looking at the data with the same frequency in random directions.",
                    "label": 0
                },
                {
                    "sent": "That's not necessarily what you want, so you would actually want to have a different spectral property.",
                    "label": 0
                },
                {
                    "sent": "The other thing is, since this is so ridiculously cheap so you can afford a million dimensions, you can actually do multiple kernel learning if you want.",
                    "label": 0
                },
                {
                    "sent": "We haven't quite figured out yet a.",
                    "label": 0
                },
                {
                    "sent": "Nice joint Parameterisation, but you could just brute force directly optimize over that, except that this drives you very much into neural networks territory.",
                    "label": 0
                },
                {
                    "sent": "The other thing is you can pick different localized basis functions.",
                    "label": 1
                },
                {
                    "sent": "Basically anything that requires an inner product with a spherical isotropic matrix can be used for this, and so you can get similar expansions as a matter of fact.",
                    "label": 0
                },
                {
                    "sent": "The original idea of the random kitchen sinks in away is actually Radford.",
                    "label": 0
                },
                {
                    "sent": "Neal's paper from 94 where he shows that Gaussian process in neural networks with infinite number of basis functions are equivalent and you're basically driving this backwards, right?",
                    "label": 0
                },
                {
                    "sent": "We're basically saying, OK?",
                    "label": 0
                },
                {
                    "sent": "Here's an efficient mechanism of generating and more or less infinite number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "A similar trick works for matrix values functions.",
                    "label": 1
                },
                {
                    "sent": "And then of course, if you want to go crazy, you can stack several layers of those things on top of it.",
                    "label": 0
                },
                {
                    "sent": "Now this starts to look very much like a deep belief network.",
                    "label": 0
                },
                {
                    "sent": "Mind you, back properties really easy because the inverse Hadamard can also be efficiently done.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not all just sunshine.",
                    "label": 0
                },
                {
                    "sent": "The big problem is that.",
                    "label": 1
                },
                {
                    "sent": "Computing the kernel itself is now really expensive, right?",
                    "label": 0
                },
                {
                    "sent": "This is almost like an invariant theorem of difficulty.",
                    "label": 1
                },
                {
                    "sent": "And the covariance operators that can be talked about before are also not explicitly available.",
                    "label": 1
                },
                {
                    "sent": "So now you basically have a million by million objects and you need to come up with some low rank expansions.",
                    "label": 0
                },
                {
                    "sent": "We have some ideas about it, but that's not work out yet.",
                    "label": 1
                },
                {
                    "sent": "And then the obvious caveat is you never ever want to store explicit feature map.",
                    "label": 0
                },
                {
                    "sent": "It's just so much cheaper to recompute it rather than to restore it.",
                    "label": 0
                },
                {
                    "sent": "Basically your CPU is much faster than you memory, so never ever store those things.",
                    "label": 0
                },
                {
                    "sent": "Don't even think of putting them on disk either, OK and that's all I wanted to say.",
                    "label": 0
                }
            ]
        }
    }
}