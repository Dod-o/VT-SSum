{
    "id": "lsw23a6cjzryt6ase64k63hf2575sbbi",
    "title": "Sparsity regret bounds for individual sequences in online linear regression",
    "info": {
        "author": [
            "S\u00e9bastien Gerchinovitz, Department of Mathematics and their applications, \u00c9cole normale sup\u00e9rieure Paris"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/colt2011_gerchinovitz_linear/",
    "segmentation": [
        [
            "So I'm going to speak about sparsity sparsity bounds in a deterministic online setting."
        ],
        [
            "Just for the sake of introduction, I just very briefly recall some well known riddles about sparsity in the stochastic setting, and then we will turn to the individual sequences setting in the next slide.",
            "So just consider the linear regression model with fixed or random design.",
            "So we observe random indefinite peers XD yd, where yd is a linear combination of Fiji of XTZ to some additive noise.",
            "So here with JRR base regresses on the ustariz unknown, this is a vector in Rd.",
            "And in the previous talk, the issue is about variable selection, so identification of the support of a used are here.",
            "The problem is different.",
            "We will deal with the prediction problem, which consistent estimating the non regression function which is a linear combination.",
            "Are you star of the PJ where you start is unknown, so it's now very well known that in small dimensions the Wendy's smaller than something of the order of T. Then we can estimate this regression function in Delta risk with the rate which is optimal very of D / T so.",
            "This is in particular proportional to beyond the dimension D, so that in higher dimensions, no curate prediction is possible in general.",
            "But if we make further assumptions like sparsity assumption.",
            "So if we assume that the linear combination you star is actually false, meaning that it is only a few number of nonzero coordinates, then we can basically replace in the bounds you and I mentioned we level of sparsity this year, and if you start up to some luck factor, so this is the main message in the sparsity bounced for prediction problem, we can replace the dimension D. We've level of sparsity.",
            "So I won't have time to comment on the various methods that proved such balance that we can think.",
            "For example, of the L0 regularization at one regularization, an more recently exponential weighting with spicy favoring price."
        ],
        [
            "So let's turn to our setting.",
            "We are going to consider a deterministic online setting which is known as online regression and in digital sequences and in this setting we will introduce.",
            "An equivalent deterministic kind of part of the previous bounce, which we will call sparsity regret bounds.",
            "So I first introduced such bounds, then present the main results we got with individual sequences.",
            "And finally we will apply those results to ID data to get some adaptivity results."
        ],
        [
            "So let's introduce the notion of spouse."
        ],
        [
            "Rebounds.",
            "So the setting is known as online in your regression, individual sequences, and so it is a deterministic online framework which is studied by many, many papers.",
            "But for example, Chism, Akila, Invar move into that in 96 and 0 involvement from Buck in 2001, or many other papers as well.",
            "The prediction test is sequential at each time T, the forecasters to predict the observation YT, which is real valued.",
            "From some input data XT in a given set X.",
            "And then also from some base forecasters Vijay which are.",
            "Functions from X to the real line, and there are deer.",
            "The default days forecast is.",
            "So more precisely, the game can be described as follows.",
            "First, the environment chooses between deterministic sequences, so individual sequences of observations, YT and of input data XD.",
            "But of course the forecast does not access to them and then in a sequential fashion the environment reveals input data XD and on the basis of this new XT of the base forecast, Vijay Alexion the past data.",
            "The forecast is a prediction waiting heads.",
            "And finally, the environment reveals the observation YT which is compared to the prediction through the square loss."
        ],
        [
            "So here is our setting.",
            "And in this setting, the goal of the forecaster is to predict almost as well as the best linear forecaster.",
            "So we will call linear forecaster any linear combination of Vijay.",
            "And by predicting almost as well, we mean the usual.",
            "We usually regret bound, so we want to further forecasted to achieve.",
            "Accumulative square loss, which is smaller than the smallest cumulative Squillace of any linear forecaster up to some rig better, which would be small, and in particular sub linear in T. So for example, the sequential Ridge regression forecaster than zero in Falmouth and Buck in this setting achieves such a regret bound.",
            "Well the regret term is roughly a Verity of D log of T. So you can see that this is just like in the stochastic setting.",
            "The upper bound is proportional to the end dimension D. So that if we are in small dimension, if these much smaller vent even D log of T is indeed sub linear in T, so the goal is achieved.",
            "So this is for small dimensions."
        ],
        [
            "What about the dimensions?",
            "In high dimension D, then the bound the log of Taylor Ridge Regression Forecast is no longer so linear in T. But just like in the stochastic setting, we can still do something in the sparsity scenario.",
            "Assume that there is some linear combination you star which has a low cumulative loss and which in the same time is a sparse meaning that it has only a very few number of nonzero coordinates.",
            "Then under this assumption, we could apply the same forecasters previously, but not to the whole RT space, but only to the.",
            "Subspace of already generally associated with the support of you star, and instead of getting abound of your of the log of T, we have amount of urea level of sparsity times log log of T, which this time is a sub linear into.",
            "If you started sparse."
        ],
        [
            "Of course, with support of you star is unknown, so this is only a benchmark bounce.",
            "But in this talk we prove that balance proportional to the 0 number used are all achievable up to Legarrette, make effectors.",
            "Because we derive bands of the following form, so this is a sound well.",
            "The regret term is proportional to the number of nonzero coordinates of U and wear them.",
            "Alleviate effects is logarithmic.",
            "In the time runs number of rounds T in the other dimension D and the one norm of you.",
            "And we will call such bounds sparsity regrid mouse.",
            "You can see from the stochastic in literature that such bands are.",
            "Deterministic online counterparts of the sparsity are equal inequalities.",
            "Which usually take the following form you have on the left, inside the risk of your estimator, which is a bounded by a constant times the smallest risk plus a remainder term which is a proportional to zero normal vehicle vector.",
            "So these are.",
            "Like counterparts, abounds in the deterministic setting in the online in the stochastic saying sorry."
        ],
        [
            "Recently, several papers addressed Dispersity issue in the same setting's house.",
            "So for example, the people by long thought Lee and Jung, Sha Lips, Watson, Tiwari, show and do she singer Charlotte wasn't a worry and some of us as well.",
            "And in the particular case of L1 regularization.",
            "They approved the tackle Dispersity issue, but from a quite different angle.",
            "They focus on algorithms that outputs pulse linear combinations.",
            "While we are interested in algorithms with the sparsity sparse bounce, if you if you like so we've bounds of this poem where the regret term is proportional to the zero normal view.",
            "It's quite straightforward to see that on L zero goals of small ready such bounds are optimal at 2 log factors.",
            "From the lower bounds that hold for linear regression online regression.",
            "On the contrary, the reread bounds of the above mentioned works.",
            "The worst dependence in TD or the one of you, because usually either the form of the square root of B which which is large when Wendy's large.",
            "Or a sqrt T, which is a slower rate compared to a fast rate we can get because that is in the logarithm.",
            "So this is for."
        ],
        [
            "The related works.",
            "Now we present the main results.",
            "So how to derive such sparsity rebounds with individual sequences?",
            "We will first make some assumptions on the Prairie knowledge on the data to be predicted, and then we will remove the such requirements with adaptive."
        ],
        [
            "Williams so the first version of algorithm is the following.",
            "We call it sequentials plus exponential weighting or Seca.",
            "Suitable useful shots.",
            "It has depends on free parameters of threshold be an inverse temperature Aida and a prior scale TA.",
            "And at each time round, T algorithm fredicks as a convex mixture of the linear forecasts, or more precisely, a convex mixture of a clipped linear forecast.",
            "So the quantity 2 square brackets is the standard clipping between minus B&B thresholding as well.",
            "And here the convex mixture is a computed with respect to a probability distribution PD which is defined as follows.",
            "So it has a density.",
            "With respect to a prior which we commit on later, and this density is just the standard exponential weights, so it is exponentially decreasing in the cumulative loss of the linear forecaster after clipping up to time T -- 1, which is your valuable data at time T. So this is the standard exponentially weighted average with clipping and on a continuous set of experts name ALDI.",
            "Anne, this prior title was introduced by Dell EMC back off in the stochastic setting.",
            "You can see.",
            "So this is where the definition of Pi Tau.",
            "So all the coordinates are independent and it is polynomial it freezing in each direction.",
            "So it is a detailed which favors sparsity.",
            "So one very contribution of his work was to show that there was a link between exponential exponential weighting and sparsity.",
            "And the link actually still holds in the deterministic setting."
        ],
        [
            "So here first regret bounds.",
            "So we first make two assumptions.",
            "We assume that we have access before hand to two A Prairie bounds.",
            "Byn beefy be wise in a Prairie bound on all the absolute values of your observationes.",
            "NBC is in a fairy band on the trace of the empirical Gram metrics.",
            "So you can think of the fee of the order of D * T so it can be very large an it comes just in the logarithmic factor.",
            "So if we know the one before hand and if we tune the algorithm.",
            "With those Prairie Band, so BY is just used for the clipping.",
            "The inverse temperature parameter is tuned as one of eight, the squared threshold and to just like this when we get a rig Redmond where the main regret term is proportional to the 0 number of you.",
            "And where the dependence in DT and the one of you are inside the logarithmic, so this is indeed a as positive regard.",
            "Man as we defined it earlier.",
            "And to prove it, we use the deterministic online big basin bounds of Cody Bear combined with a form of email provider which were studied within dalliances back off."
        ],
        [
            "So now.",
            "We will remove one of those two assumptions, so we still assume that we have access to some bound beefy, but we will adapt to be Y.",
            "So B.",
            "Why is the unknown maximum amplitude of the observations?",
            "And how to adapt to this, we can replace the threshold be in the inverse temperature, either by time varying thresholds and time varying.",
            "Tom Fisher Parameters so BDS chosen as something averted.",
            "The maximum amplitude up to time T -- 1.",
            "This is actually the smallest squared of two.",
            "Immediately larger than this.",
            "An eatery, just like previously, is 1 / 8 times the square threshold.",
            "And we've both choices, and we've been appropriate choice of tab.",
            "We can get the same type of sparsity, regret, bounds.",
            "So again, a proportional to be 0 normal U and a large dependencies into the logarithm.",
            "And to prove this we used an adaptive variants of the pack Bayesian lemma provided by Odie bear and this adaptive lemma is.",
            "Makes a crucial use of the form of the temperature parameter.",
            "To remove the further the second assumption, the assumption on the beefy we can carry it carry out a standard doubling trick.",
            "The price of an extra load factor."
        ],
        [
            "OK, so the last part deals with the application of those individual sequences result."
        ],
        [
            "To ID data.",
            "So we can see the usual batch setting where we observe random peers XT which are ID.",
            "And distributed as a random pair XY, and the goal is to estimate the regression function, which is F as the expectation of Y given X.",
            "We use the standard online to batch conversion, so we treat the sample in a sequential fashion an we run the algorithm of a previous slide we've.",
            "Completely known parameters.",
            "So this algorithm is a parameter free from time one to time T, and we estimate the regression function with.",
            "Which is our average FT at as so which is our average of the sequential predictions of the algorithm?",
            "So this is the standard online to batch conversion of algorithm, but not here.",
            "That's FT had does not depend on any prior knowledge on the distribution of XY, such as the variance of a noise or some Infinity.",
            "Norms of Bayes regresses or differences between the Bayes regresses on the regression function.",
            "Actually the last two quantities do not even need to be bounded."
        ],
        [
            "So here is an estimator and with this estimator we can prove this risk bound.",
            "This is very straightforward, we just take the expectation of a. Bounds on individual sequences applied Jens's inequality twice, and we get.",
            "This bound so you have the Delta risk of our estimator, which is smaller than the smallest Delta risk up to some terms.",
            "Well, the main one is proportional to the zero normal view, so this is a sparsity recording equal."
        ],
        [
            "And more importantly, the term that matches the amplitude of those remainder terms.",
            "So this is the expectation of the maximum of yd squared.",
            "This expectation can be upper bounding into various assumptions, so which we comment on in the full version of this paper?",
            "But in their particular keys, where the regression function is bounded in Infinity norm on the noises suggestion conditionally on the design with a known variance vector Sigma squared.",
            "Then we can upper bound this expectation with this term substitute.",
            "It into our sparsity Recon equality on get a band that looks very much like bonds in the recent people by Delancey back off.",
            "But we got these bounds in a more adaptive way because our bound here calls on the whole artist space instead of just being restricted to bowls of properly chosen radiate.",
            "And we do not require the pre knowledge of Sigma squared.",
            "And you cannot put down also this expectation under other assumptions to get other adaptive results.",
            "So."
        ],
        [
            "So to conclude.",
            "We introduced the notion of sparsity triggered Mom, which is a sparsity bound in the deterministic setting.",
            "We showed how to adapt to some unknown quantities here BY Ann Fruusto.",
            "Doubling trick beefy.",
            "An when applying those results to ID data we get response which are adaptive to the invariants of the noise.",
            "So individual sequences techniques are a way to adapt to.",
            "Parameters.",
            "Of course we could like to get the same type of bounds, but for algorithms which I put sparse linear combinations just like the sequential lesu, and this is actually a work in progress."
        ],
        [
            "So thank you very much for your attention.",
            "Do you have a version of the algorithm that doesn't use clipping that it would output the linear mixture of the basis functions?",
            "There is a. I know one Ultra native or this clipping version which makes use of a few more intricated way of choosing the prediction with the square loss.",
            "So with the aggregating algorithm of Q can also.",
            "Put away the clipping and use some other version.",
            "Which actually makes the bound fourth times smaller than the noise is pretty the same and I actually I choose this clipping because it was maybe a little bit easier to explain and the second question, what is the computational complexity of your algorithm?",
            "I did so it's just a fair equal result.",
            "Yeah, and the complexity is actually not.",
            "Maybe not very good.",
            "If you would like to use this in practice, there is a people by Diane.",
            "See back often called 2009 which explains how to approximate such such context mixtures through launch violent diffusion.",
            "So it would be a way to use this actually in their experimental results.",
            "It seems to work fine, so.",
            "But I won't.",
            "I won't go into further detail.",
            "Any other questions?",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to speak about sparsity sparsity bounds in a deterministic online setting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just for the sake of introduction, I just very briefly recall some well known riddles about sparsity in the stochastic setting, and then we will turn to the individual sequences setting in the next slide.",
                    "label": 1
                },
                {
                    "sent": "So just consider the linear regression model with fixed or random design.",
                    "label": 1
                },
                {
                    "sent": "So we observe random indefinite peers XD yd, where yd is a linear combination of Fiji of XTZ to some additive noise.",
                    "label": 0
                },
                {
                    "sent": "So here with JRR base regresses on the ustariz unknown, this is a vector in Rd.",
                    "label": 0
                },
                {
                    "sent": "And in the previous talk, the issue is about variable selection, so identification of the support of a used are here.",
                    "label": 0
                },
                {
                    "sent": "The problem is different.",
                    "label": 0
                },
                {
                    "sent": "We will deal with the prediction problem, which consistent estimating the non regression function which is a linear combination.",
                    "label": 0
                },
                {
                    "sent": "Are you star of the PJ where you start is unknown, so it's now very well known that in small dimensions the Wendy's smaller than something of the order of T. Then we can estimate this regression function in Delta risk with the rate which is optimal very of D / T so.",
                    "label": 1
                },
                {
                    "sent": "This is in particular proportional to beyond the dimension D, so that in higher dimensions, no curate prediction is possible in general.",
                    "label": 0
                },
                {
                    "sent": "But if we make further assumptions like sparsity assumption.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the linear combination you star is actually false, meaning that it is only a few number of nonzero coordinates, then we can basically replace in the bounds you and I mentioned we level of sparsity this year, and if you start up to some luck factor, so this is the main message in the sparsity bounced for prediction problem, we can replace the dimension D. We've level of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So I won't have time to comment on the various methods that proved such balance that we can think.",
                    "label": 0
                },
                {
                    "sent": "For example, of the L0 regularization at one regularization, an more recently exponential weighting with spicy favoring price.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's turn to our setting.",
                    "label": 0
                },
                {
                    "sent": "We are going to consider a deterministic online setting which is known as online regression and in digital sequences and in this setting we will introduce.",
                    "label": 1
                },
                {
                    "sent": "An equivalent deterministic kind of part of the previous bounce, which we will call sparsity regret bounds.",
                    "label": 1
                },
                {
                    "sent": "So I first introduced such bounds, then present the main results we got with individual sequences.",
                    "label": 0
                },
                {
                    "sent": "And finally we will apply those results to ID data to get some adaptivity results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's introduce the notion of spouse.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rebounds.",
                    "label": 0
                },
                {
                    "sent": "So the setting is known as online in your regression, individual sequences, and so it is a deterministic online framework which is studied by many, many papers.",
                    "label": 1
                },
                {
                    "sent": "But for example, Chism, Akila, Invar move into that in 96 and 0 involvement from Buck in 2001, or many other papers as well.",
                    "label": 1
                },
                {
                    "sent": "The prediction test is sequential at each time T, the forecasters to predict the observation YT, which is real valued.",
                    "label": 1
                },
                {
                    "sent": "From some input data XT in a given set X.",
                    "label": 1
                },
                {
                    "sent": "And then also from some base forecasters Vijay which are.",
                    "label": 0
                },
                {
                    "sent": "Functions from X to the real line, and there are deer.",
                    "label": 1
                },
                {
                    "sent": "The default days forecast is.",
                    "label": 0
                },
                {
                    "sent": "So more precisely, the game can be described as follows.",
                    "label": 1
                },
                {
                    "sent": "First, the environment chooses between deterministic sequences, so individual sequences of observations, YT and of input data XD.",
                    "label": 0
                },
                {
                    "sent": "But of course the forecast does not access to them and then in a sequential fashion the environment reveals input data XD and on the basis of this new XT of the base forecast, Vijay Alexion the past data.",
                    "label": 1
                },
                {
                    "sent": "The forecast is a prediction waiting heads.",
                    "label": 0
                },
                {
                    "sent": "And finally, the environment reveals the observation YT which is compared to the prediction through the square loss.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is our setting.",
                    "label": 0
                },
                {
                    "sent": "And in this setting, the goal of the forecaster is to predict almost as well as the best linear forecaster.",
                    "label": 1
                },
                {
                    "sent": "So we will call linear forecaster any linear combination of Vijay.",
                    "label": 0
                },
                {
                    "sent": "And by predicting almost as well, we mean the usual.",
                    "label": 0
                },
                {
                    "sent": "We usually regret bound, so we want to further forecasted to achieve.",
                    "label": 1
                },
                {
                    "sent": "Accumulative square loss, which is smaller than the smallest cumulative Squillace of any linear forecaster up to some rig better, which would be small, and in particular sub linear in T. So for example, the sequential Ridge regression forecaster than zero in Falmouth and Buck in this setting achieves such a regret bound.",
                    "label": 0
                },
                {
                    "sent": "Well the regret term is roughly a Verity of D log of T. So you can see that this is just like in the stochastic setting.",
                    "label": 0
                },
                {
                    "sent": "The upper bound is proportional to the end dimension D. So that if we are in small dimension, if these much smaller vent even D log of T is indeed sub linear in T, so the goal is achieved.",
                    "label": 0
                },
                {
                    "sent": "So this is for small dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about the dimensions?",
                    "label": 0
                },
                {
                    "sent": "In high dimension D, then the bound the log of Taylor Ridge Regression Forecast is no longer so linear in T. But just like in the stochastic setting, we can still do something in the sparsity scenario.",
                    "label": 1
                },
                {
                    "sent": "Assume that there is some linear combination you star which has a low cumulative loss and which in the same time is a sparse meaning that it has only a very few number of nonzero coordinates.",
                    "label": 0
                },
                {
                    "sent": "Then under this assumption, we could apply the same forecasters previously, but not to the whole RT space, but only to the.",
                    "label": 0
                },
                {
                    "sent": "Subspace of already generally associated with the support of you star, and instead of getting abound of your of the log of T, we have amount of urea level of sparsity times log log of T, which this time is a sub linear into.",
                    "label": 0
                },
                {
                    "sent": "If you started sparse.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, with support of you star is unknown, so this is only a benchmark bounce.",
                    "label": 0
                },
                {
                    "sent": "But in this talk we prove that balance proportional to the 0 number used are all achievable up to Legarrette, make effectors.",
                    "label": 1
                },
                {
                    "sent": "Because we derive bands of the following form, so this is a sound well.",
                    "label": 0
                },
                {
                    "sent": "The regret term is proportional to the number of nonzero coordinates of U and wear them.",
                    "label": 0
                },
                {
                    "sent": "Alleviate effects is logarithmic.",
                    "label": 1
                },
                {
                    "sent": "In the time runs number of rounds T in the other dimension D and the one norm of you.",
                    "label": 0
                },
                {
                    "sent": "And we will call such bounds sparsity regrid mouse.",
                    "label": 0
                },
                {
                    "sent": "You can see from the stochastic in literature that such bands are.",
                    "label": 1
                },
                {
                    "sent": "Deterministic online counterparts of the sparsity are equal inequalities.",
                    "label": 0
                },
                {
                    "sent": "Which usually take the following form you have on the left, inside the risk of your estimator, which is a bounded by a constant times the smallest risk plus a remainder term which is a proportional to zero normal vehicle vector.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "Like counterparts, abounds in the deterministic setting in the online in the stochastic saying sorry.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recently, several papers addressed Dispersity issue in the same setting's house.",
                    "label": 0
                },
                {
                    "sent": "So for example, the people by long thought Lee and Jung, Sha Lips, Watson, Tiwari, show and do she singer Charlotte wasn't a worry and some of us as well.",
                    "label": 0
                },
                {
                    "sent": "And in the particular case of L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "They approved the tackle Dispersity issue, but from a quite different angle.",
                    "label": 1
                },
                {
                    "sent": "They focus on algorithms that outputs pulse linear combinations.",
                    "label": 1
                },
                {
                    "sent": "While we are interested in algorithms with the sparsity sparse bounce, if you if you like so we've bounds of this poem where the regret term is proportional to the zero normal view.",
                    "label": 1
                },
                {
                    "sent": "It's quite straightforward to see that on L zero goals of small ready such bounds are optimal at 2 log factors.",
                    "label": 0
                },
                {
                    "sent": "From the lower bounds that hold for linear regression online regression.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, the reread bounds of the above mentioned works.",
                    "label": 0
                },
                {
                    "sent": "The worst dependence in TD or the one of you, because usually either the form of the square root of B which which is large when Wendy's large.",
                    "label": 0
                },
                {
                    "sent": "Or a sqrt T, which is a slower rate compared to a fast rate we can get because that is in the logarithm.",
                    "label": 0
                },
                {
                    "sent": "So this is for.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The related works.",
                    "label": 0
                },
                {
                    "sent": "Now we present the main results.",
                    "label": 1
                },
                {
                    "sent": "So how to derive such sparsity rebounds with individual sequences?",
                    "label": 1
                },
                {
                    "sent": "We will first make some assumptions on the Prairie knowledge on the data to be predicted, and then we will remove the such requirements with adaptive.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Williams so the first version of algorithm is the following.",
                    "label": 0
                },
                {
                    "sent": "We call it sequentials plus exponential weighting or Seca.",
                    "label": 1
                },
                {
                    "sent": "Suitable useful shots.",
                    "label": 0
                },
                {
                    "sent": "It has depends on free parameters of threshold be an inverse temperature Aida and a prior scale TA.",
                    "label": 1
                },
                {
                    "sent": "And at each time round, T algorithm fredicks as a convex mixture of the linear forecasts, or more precisely, a convex mixture of a clipped linear forecast.",
                    "label": 1
                },
                {
                    "sent": "So the quantity 2 square brackets is the standard clipping between minus B&B thresholding as well.",
                    "label": 1
                },
                {
                    "sent": "And here the convex mixture is a computed with respect to a probability distribution PD which is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "So it has a density.",
                    "label": 0
                },
                {
                    "sent": "With respect to a prior which we commit on later, and this density is just the standard exponential weights, so it is exponentially decreasing in the cumulative loss of the linear forecaster after clipping up to time T -- 1, which is your valuable data at time T. So this is the standard exponentially weighted average with clipping and on a continuous set of experts name ALDI.",
                    "label": 0
                },
                {
                    "sent": "Anne, this prior title was introduced by Dell EMC back off in the stochastic setting.",
                    "label": 1
                },
                {
                    "sent": "You can see.",
                    "label": 0
                },
                {
                    "sent": "So this is where the definition of Pi Tau.",
                    "label": 0
                },
                {
                    "sent": "So all the coordinates are independent and it is polynomial it freezing in each direction.",
                    "label": 0
                },
                {
                    "sent": "So it is a detailed which favors sparsity.",
                    "label": 0
                },
                {
                    "sent": "So one very contribution of his work was to show that there was a link between exponential exponential weighting and sparsity.",
                    "label": 0
                },
                {
                    "sent": "And the link actually still holds in the deterministic setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here first regret bounds.",
                    "label": 1
                },
                {
                    "sent": "So we first make two assumptions.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have access before hand to two A Prairie bounds.",
                    "label": 1
                },
                {
                    "sent": "Byn beefy be wise in a Prairie bound on all the absolute values of your observationes.",
                    "label": 1
                },
                {
                    "sent": "NBC is in a fairy band on the trace of the empirical Gram metrics.",
                    "label": 1
                },
                {
                    "sent": "So you can think of the fee of the order of D * T so it can be very large an it comes just in the logarithmic factor.",
                    "label": 0
                },
                {
                    "sent": "So if we know the one before hand and if we tune the algorithm.",
                    "label": 0
                },
                {
                    "sent": "With those Prairie Band, so BY is just used for the clipping.",
                    "label": 0
                },
                {
                    "sent": "The inverse temperature parameter is tuned as one of eight, the squared threshold and to just like this when we get a rig Redmond where the main regret term is proportional to the 0 number of you.",
                    "label": 0
                },
                {
                    "sent": "And where the dependence in DT and the one of you are inside the logarithmic, so this is indeed a as positive regard.",
                    "label": 0
                },
                {
                    "sent": "Man as we defined it earlier.",
                    "label": 0
                },
                {
                    "sent": "And to prove it, we use the deterministic online big basin bounds of Cody Bear combined with a form of email provider which were studied within dalliances back off.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We will remove one of those two assumptions, so we still assume that we have access to some bound beefy, but we will adapt to be Y.",
                    "label": 0
                },
                {
                    "sent": "So B.",
                    "label": 0
                },
                {
                    "sent": "Why is the unknown maximum amplitude of the observations?",
                    "label": 1
                },
                {
                    "sent": "And how to adapt to this, we can replace the threshold be in the inverse temperature, either by time varying thresholds and time varying.",
                    "label": 1
                },
                {
                    "sent": "Tom Fisher Parameters so BDS chosen as something averted.",
                    "label": 0
                },
                {
                    "sent": "The maximum amplitude up to time T -- 1.",
                    "label": 0
                },
                {
                    "sent": "This is actually the smallest squared of two.",
                    "label": 0
                },
                {
                    "sent": "Immediately larger than this.",
                    "label": 0
                },
                {
                    "sent": "An eatery, just like previously, is 1 / 8 times the square threshold.",
                    "label": 0
                },
                {
                    "sent": "And we've both choices, and we've been appropriate choice of tab.",
                    "label": 1
                },
                {
                    "sent": "We can get the same type of sparsity, regret, bounds.",
                    "label": 0
                },
                {
                    "sent": "So again, a proportional to be 0 normal U and a large dependencies into the logarithm.",
                    "label": 0
                },
                {
                    "sent": "And to prove this we used an adaptive variants of the pack Bayesian lemma provided by Odie bear and this adaptive lemma is.",
                    "label": 0
                },
                {
                    "sent": "Makes a crucial use of the form of the temperature parameter.",
                    "label": 0
                },
                {
                    "sent": "To remove the further the second assumption, the assumption on the beefy we can carry it carry out a standard doubling trick.",
                    "label": 1
                },
                {
                    "sent": "The price of an extra load factor.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the last part deals with the application of those individual sequences result.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To ID data.",
                    "label": 0
                },
                {
                    "sent": "So we can see the usual batch setting where we observe random peers XT which are ID.",
                    "label": 1
                },
                {
                    "sent": "And distributed as a random pair XY, and the goal is to estimate the regression function, which is F as the expectation of Y given X.",
                    "label": 1
                },
                {
                    "sent": "We use the standard online to batch conversion, so we treat the sample in a sequential fashion an we run the algorithm of a previous slide we've.",
                    "label": 1
                },
                {
                    "sent": "Completely known parameters.",
                    "label": 1
                },
                {
                    "sent": "So this algorithm is a parameter free from time one to time T, and we estimate the regression function with.",
                    "label": 0
                },
                {
                    "sent": "Which is our average FT at as so which is our average of the sequential predictions of the algorithm?",
                    "label": 1
                },
                {
                    "sent": "So this is the standard online to batch conversion of algorithm, but not here.",
                    "label": 0
                },
                {
                    "sent": "That's FT had does not depend on any prior knowledge on the distribution of XY, such as the variance of a noise or some Infinity.",
                    "label": 0
                },
                {
                    "sent": "Norms of Bayes regresses or differences between the Bayes regresses on the regression function.",
                    "label": 0
                },
                {
                    "sent": "Actually the last two quantities do not even need to be bounded.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an estimator and with this estimator we can prove this risk bound.",
                    "label": 0
                },
                {
                    "sent": "This is very straightforward, we just take the expectation of a. Bounds on individual sequences applied Jens's inequality twice, and we get.",
                    "label": 0
                },
                {
                    "sent": "This bound so you have the Delta risk of our estimator, which is smaller than the smallest Delta risk up to some terms.",
                    "label": 0
                },
                {
                    "sent": "Well, the main one is proportional to the zero normal view, so this is a sparsity recording equal.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more importantly, the term that matches the amplitude of those remainder terms.",
                    "label": 0
                },
                {
                    "sent": "So this is the expectation of the maximum of yd squared.",
                    "label": 0
                },
                {
                    "sent": "This expectation can be upper bounding into various assumptions, so which we comment on in the full version of this paper?",
                    "label": 1
                },
                {
                    "sent": "But in their particular keys, where the regression function is bounded in Infinity norm on the noises suggestion conditionally on the design with a known variance vector Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Then we can upper bound this expectation with this term substitute.",
                    "label": 0
                },
                {
                    "sent": "It into our sparsity Recon equality on get a band that looks very much like bonds in the recent people by Delancey back off.",
                    "label": 1
                },
                {
                    "sent": "But we got these bounds in a more adaptive way because our bound here calls on the whole artist space instead of just being restricted to bowls of properly chosen radiate.",
                    "label": 0
                },
                {
                    "sent": "And we do not require the pre knowledge of Sigma squared.",
                    "label": 1
                },
                {
                    "sent": "And you cannot put down also this expectation under other assumptions to get other adaptive results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "We introduced the notion of sparsity triggered Mom, which is a sparsity bound in the deterministic setting.",
                    "label": 1
                },
                {
                    "sent": "We showed how to adapt to some unknown quantities here BY Ann Fruusto.",
                    "label": 0
                },
                {
                    "sent": "Doubling trick beefy.",
                    "label": 0
                },
                {
                    "sent": "An when applying those results to ID data we get response which are adaptive to the invariants of the noise.",
                    "label": 1
                },
                {
                    "sent": "So individual sequences techniques are a way to adapt to.",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 1
                },
                {
                    "sent": "Of course we could like to get the same type of bounds, but for algorithms which I put sparse linear combinations just like the sequential lesu, and this is actually a work in progress.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you very much for your attention.",
                    "label": 1
                },
                {
                    "sent": "Do you have a version of the algorithm that doesn't use clipping that it would output the linear mixture of the basis functions?",
                    "label": 0
                },
                {
                    "sent": "There is a. I know one Ultra native or this clipping version which makes use of a few more intricated way of choosing the prediction with the square loss.",
                    "label": 0
                },
                {
                    "sent": "So with the aggregating algorithm of Q can also.",
                    "label": 0
                },
                {
                    "sent": "Put away the clipping and use some other version.",
                    "label": 0
                },
                {
                    "sent": "Which actually makes the bound fourth times smaller than the noise is pretty the same and I actually I choose this clipping because it was maybe a little bit easier to explain and the second question, what is the computational complexity of your algorithm?",
                    "label": 0
                },
                {
                    "sent": "I did so it's just a fair equal result.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the complexity is actually not.",
                    "label": 0
                },
                {
                    "sent": "Maybe not very good.",
                    "label": 0
                },
                {
                    "sent": "If you would like to use this in practice, there is a people by Diane.",
                    "label": 0
                },
                {
                    "sent": "See back often called 2009 which explains how to approximate such such context mixtures through launch violent diffusion.",
                    "label": 0
                },
                {
                    "sent": "So it would be a way to use this actually in their experimental results.",
                    "label": 0
                },
                {
                    "sent": "It seems to work fine, so.",
                    "label": 0
                },
                {
                    "sent": "But I won't.",
                    "label": 0
                },
                {
                    "sent": "I won't go into further detail.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}