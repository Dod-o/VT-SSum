{
    "id": "eip7qn3eziobnl34d2kazu5tiseu52ww",
    "title": "Learning from Partially Annotated Sequences",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Ulf Brefeld, Leuphana University of L\u00fcneburg"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_brefeld_annotated/",
    "segmentation": [
        [
            "Yeah, this is the focus on learning from sequences and this is joint work with a lot of analysts with right now in prison.",
            "So that although we present them.",
            "Some new technical contributions.",
            "The talk is not really about environment, something, but it's more about how we could reach out to apply more complex machine learning approaches in the real world.",
            "And."
        ],
        [
            "Therefore, I motivate my child is some really so for instance."
        ],
        [
            "So therefore.",
            "If we want to do related sequence learning, if you want to learn from sequences, that is, we have a sequence is important beyond sequences output and looking for a model, visions first, the input and the output, then a good application is always protein.",
            "Secondary structure prediction, radius given the input.",
            "But let's say the primary sequence of the of the protein and we are looking for the secondary structure."
        ],
        [
            "Another important area is natural language processing differences.",
            "Part of speech Tagging's is is a tragic where we're looking for annotating every word of a sentence with this part of speech tag."
        ],
        [
            "Another thing, and this is the running example in this talk, is named entity recognition.",
            "That is, we have a sentence and we're looking.",
            "We're aiming at detecting all the entities or the names of entities that are within the sentence.",
            "So here, for instance, we would like to detect that Enrico Palazzo isn't personal."
        ],
        [
            "So.",
            "OK, so whenever we do with sequential learning, we usually use Markov random fields and so in case of sequences we use this chain like structure where the X denote input variables.",
            "And why are the output variables?",
            "So this chain extracts are here.",
            "Is the sequential model for the sentence John is the cat, for instance, and the joint probability of India developer variables.",
            "Factorizes over their cheeks.",
            "Here we have two different types of police.",
            "That's the green clicks between transitions or for transitions, and we have the rupees for available central situations.",
            "And we can describe this clixby features.",
            "For instance, is this word a noun?",
            "Is the following verb, or is this version of it?",
            "Is the observation job?",
            "So it's taking up all these features into one large feature vector, which is called often called joint feature representation, because it depends on.",
            "Not only on the on the input, but also in the output allows us to to rewrite the conditional probability and an output sequence given input sequence as it gives distribution.",
            "And if we are not really interested in the full posterior, we can take a maximum.",
            "Approach in and we end up with a with a generalized linear model."
        ],
        [
            "The most simplest model, I guess that you work with or that tries to adapt this.",
            "This generalized linear model to data is the structure of perception.",
            "Is actually very close to the binary perception that you're very familiar with, I guess, so here's a more reminder.",
            "So the structure perception also starts with an empty model, and then it generates a sequence of models.",
            "Every time we see a new example of her, we recompute the prediction.",
            "I will come back to the prediction data and then we compare the outcome of the of this prediction with the true labeling of the sequence and then if they don't do it, they don't go inside.",
            "If they if they are not identical we need to perform an update.",
            "So we can show that it converges.",
            "Is putting on your time with the proof is very similar to the binary perception.",
            "And yeah.",
            "It has the same guarantee for Morris.",
            "So they're the only program is problem.",
            "Maybe the computation of the argument.",
            "So because now we have sequences, it's a bit more complicated, but."
        ],
        [
            "Everything turns out to be very simple because.",
            "And the macro framing feed and the graph structure that I showed you before his rise to the to the Victoria algorithm that is more or less the ticket here.",
            "I imagine some observations, observation probabilities.",
            "So if we have the sentence here, look at the record that so."
        ],
        [
            "We know that the true output would be the green path in this lettuce.",
            "That is, we know that hey look, it is not containing entity and then Enrico products.",
            "So so request the beginning of the person and policy would be inside the person.",
            "And now the red and has shows an erroneous prediction.",
            "And yeah, so the prediction says that.",
            "So is the beginning of your location, for instance, and Enrico is not detected this as an entity that order, so this update that I showed you here."
        ],
        [
            ".",
            "Is the difference between the feature vector that is generated using the true label sequence and the general feature vector generated with the?"
        ],
        [
            "Prediction.",
            "And very by subjecting them we give rise to retrieve the probability that in the next step when we see the sentence again, for instance, that we will see a path that is more likely to be the green one vendor right now."
        ],
        [
            "OK, so if we want to if we want to find a company, let's say we want to apply or machine learning techniques in the real world, we record data.",
            "We download everything.",
            "Most of the times the problem is with the label data, so the the input data is not a problem with.",
            "The labels are missing.",
            "So there.",
            "Then the the choices you have is like at the label.",
            "Everything yourself or hire people so it's time consuming, expensive.",
            "Sometimes you need effort, sometimes you you need maybe even wet lab experiments in biology.",
            "Things, for instance in English like a bigger team made it even more complex, so the labeling data is is a hard problem.",
            "If you want to apply.",
            "Things to do.",
            "New problems where you don't have that same benchmark."
        ],
        [
            "So the question that we want to address in this talk is.",
            "Can we leave parts of this annotation to laypeople?",
            "Can we go to, let's say Mechanical Turk or can we go through pro flow and say?",
            "David, those parts of our examples, way free, confidently.",
            "But if you don't know, just leave it unlabeled and.",
            "We will fill in the gaps later.",
            "So when is the formalization the problem setting that we try to solve, here's his.",
            "We want to learn from sequences that are only partially annotated.",
            "That might have guessed that are unlabeled.",
            "And where labels might be erroneous.",
            "And where the missing labels are not uniformly distribute?",
            "Across the observations."
        ],
        [
            "So in order to.",
            "To study how difficult it is in principle.",
            "We we we wanted to show that over that everyone just wants to study the most simplest approach in order to try to see whether or difficult the problem itself is.",
            "And therefore we came up with a with a simple extension of this perception that I showed you before.",
            "And so now we again we would have to do the infinite sequence theory of input examples.",
            "But now the label sequences are only partially David.",
            "So here for instance, we know a guy who annotated it.",
            "He knew that the first part of the sentence, hey, look it is the new products who does not contain an entity.",
            "But then he wasn't sure whether equal Palazzo is like maybe a person or application or whatever, so he didn't know.",
            "And the idea is.",
            "We used to use our current model to fill in the gaps in order to use the regular perception idea.",
            "Anne.",
            "And to fill in the gaps.",
            "We could use the constraint between the algorithm, which is pretty much what you what you intuitively would do, so it has a name.",
            "Is both the constraint."
        ],
        [
            "And it works like this.",
            "So if we already know that the sequence the first part of the sequence does not contain an entity, we can we can follow like we would have like a partial path in the letters."
        ],
        [
            "And the constraint material algorithm guarantees us that that this path is always followed for the for the optimal solution, and therefore the optimization problem reduces to do only the part that we have.",
            "We don't have favorite sport.",
            "Very cute."
        ],
        [
            "So what we did in the end is we we use the loss of mental perception which had margin is it's a bit more complicated than the one I showed you, but.",
            "Complicated.",
            "So the thing that we're doing now is it every time we see a new instance.",
            "We compute the constraint feature.",
            "We are ready to fill in the gaps to compute the full labeling sequence of this example, and then we use the regular victory algorithm to compute the loss of mental prediction, and then we compare both and do an update step if necessary."
        ],
        [
            "OK, so now the most interesting part of the presentation.",
            "So first of all we want to study whether.",
            "Control scenario is giving rise to this idea that we can learn from something with this partially sequences.",
            "So therefore we use the English corner data and we discarded token labels randomly and uniformly.",
            "And.",
            "Yeah, we wanted to compare the performance to that of the two baselines.",
            "That is, the discipline of perceptron and and it's only supervised varying.",
            "But of course these two guys cannot deal with partial nature of the data.",
            "So what we did is stated.",
            "We drove for them sentences at random, David them completely and we did that until the ratio of labeled and unlabeled tokens for exactly the same as in in our approach.",
            "So there is the the data is different, but the number of failures in the information that we put into it is distinct, and we repeated a couple of times to be sure that.",
            "Randomness doesn't affect."
        ],
        [
            "So here's the result.",
            "On the horizontal axis, we see that we see the number of the type of percentage of annotated tokens.",
            "And every time we train all the data, but the data is always the same old training set of coordinates.",
            "And.",
            "In so we see that we are never worse.",
            "So.",
            "And that is, we could say that learning from from from partially annotated data is is if it's uniformly distributed orbit.",
            "The missing sailor uniform.",
            "This is almost as good, or maybe a bit better.",
            "So this was a very promising result."
        ],
        [
            "We can also show that if we used a certain fraction of data is completely labeled examples and use the remaining parts as partially annotated examples of probability that.",
            "Performance.",
            "But even if we use like 20% of the data is partially annotated data, so there's not so much additional information.",
            "We can increase the performance by a couple of percentage."
        ],
        [
            "So what we did then is we we want to show that we can also.",
            "Here, with data that is very noisy, this.",
            "More realistic anyway, So what we did is we or let's say what we wanted to do is we want to use the code editor.",
            "And and we want to extract additional data from from Wikipedia.",
            "Luckily, there's an annotated resource that helped us a lot.",
            "Text version of the Wikipedia.",
            "A lot of different energy text annotated.",
            "And they say they are part of each day or the entities.",
            "I mean it is not always perfect, right?",
            "But at least they.",
            "They did a good job.",
            "And the most important thing for us is that they preserve the link structure and download it.",
            "And it's like the whole Wikipedia.",
            "So we we distributed our algorithm because in the end we will have a lot of data and the the distribution scheme wise is recording setup which is pretty much like running the person from on the subset of the data on every computer and then do a join afterwards and then distribute it again.",
            "Repeat the."
        ],
        [
            "So here's the Wikipedia data exchange.",
            "So here we have like 3 pages.",
            "It's hope it confuses Spain, Barcelona."
        ],
        [
            "And of course, Barcelona, Madrid the pages for them mentioned Spain because of disease or within Spain and."
        ],
        [
            "Office this annotated resource.",
            "We know that both are Texas locations.",
            "So this is not always the case, so there there are a lot of errors in this."
        ],
        [
            "This annotated resource, but it's that's who we have a perfect world so that we could count the distribution of that so we can count the number of times we have seen Spain.",
            "David as as a location the number of times you've seen Spain David as a person and we get it, we get this disappearing for distribution.",
            "So what we do is in order to average more things that we just take them."
        ],
        [
            "Eczema?",
            "So, and that is relatable."
        ],
        [
            "The entity of the entry of of Spain in the Wikipedia said location.",
            "And then we go back to the text.",
            "And re label all the occurrences of Spain as a reflection.",
            "So in using this example, it sounds a bit superfoods but.",
            "They are set up so that the the tokens are not always there."
        ],
        [
            "If you would actually encounter a lot of persons in a lot of organizations, are there for you."
        ],
        [
            "To do so."
        ],
        [
            "OK, so now we have a sentence that's safer soon as the characters are gone.",
            "We don't know nothing.",
            "We only know that Spain is a location.",
            "And this is the way."
        ],
        [
            "We generate the corpus.",
            "And.",
            "This is the the characteristics of the data that we extract.",
            "So in the end we have like 60,000,000 examples.",
            "Every example contains on average about 0.4 entities which is not developing, so we were missing probably a lot of entities.",
            "Entities, phrases that do not have a Wikipedia entry.",
            "Whatever noise could affect us.",
            "Also, we see that the distribution of entities is different for Francis.",
            "Persons are not very well captured, so we are we have a completely different distribution in the corner.",
            "Data is in the in the Wikipedia data and I so.",
            "To maybe to say already, I think we could if we would take this into account, which we would completely ignore that at that point I think the results could have been much better, but we didn't do that.",
            "We want to do it as simple."
        ],
        [
            "So here's the response.",
            "Um?",
            "So the curve that says Cornettos only is trained on the corner elevator on the training set.",
            "And this other curve is using the partially labeled data that we extracted automatically from Wikipedia as additional data.",
            "So.",
            "We see that we can increase the performance significantly.",
            "Although the increase is not very impressive, so 0.5 is not very.",
            "That's a very much so people are not usually at the end of it impressed.",
            "But you have to take into account that the coordinated training site is already very good for testing on coordinate.",
            "Which we did.",
            "The distributions are different and I think people have trained only on say affection of the corner.",
            "Edge training said it would have been or we would see a lot a lot more here but anyway, so it's a significant.",
            "I think we can increase the margin."
        ],
        [
            "So finally we need we repeated this.",
            "Experimenting in a slightly more complicated way.",
            "It is we want to do closely with entity recognition.",
            "That is, we.",
            "Use the first step that I explain to where we take the Wikipedia entries with.",
            "With.",
            "Yeah, we take it you using the summary over the distribution.",
            "And then we propagate the labels.",
            "And by means of the Wikipedia language, things to the to the corresponding page in another language, which will be Spanish here and then we do the labeling of the sentences in the extraction in Spanish."
        ],
        [
            "So that is, once we have the tech for the week."
        ],
        [
            "Yeah page, we propagated to the Spanish version of.",
            "Of the Wikipedia and now we have the page corresponding."
        ],
        [
            "And then related the Spanish sentences, and in this case we know that is funnier is location."
        ],
        [
            "So this leaves us with another data state which is much smaller than the English one.",
            "Because first of all, we expect Wikipedia is a lot smaller than this sort of language is missing, so we lose a lot of data points.",
            "Again, the distribution is very different, so although now person seems to be represented very well, the locations are are slightly under represented here and But again, we didn't account for for different distributions."
        ],
        [
            "So the results show pretty much the same thing.",
            "We see that we can increase the performance significantly.",
            "Although again, the the margin in order to say the difference compared to the baseline is is not really high.",
            "But I think it's a.",
            "It's a good start for for the most simplest algorithm that we could think of and."
        ],
        [
            "This is already.",
            "Almost the end, so let me conclude.",
            "So we showed or we studied whether we can learn from data that is partially annotated and perhaps automatically generated.",
            "Device the simple and straightforward constructive extension of the loss acquainted perceptual.",
            "We conducted large scale experiments using this data that I showed you, which was extracted from the Wikipedia and our results are always showing and supporting that we can.",
            "We can indeed use the partially labeled data.",
            "And.",
            "Yeah, hopefully there will be more impressive results in the future, so thanks for your attention and I'm happy to take it.",
            "Question.",
            "Even use a microphone.",
            "How much are you?",
            "Depending on checking the prosector approach.",
            "Work with any other.",
            "'cause running?",
            "Yeah, I think there's yeah of course so we could use everything.",
            "So the perception is is particularly simple.",
            "So therefore I think we we picked it because we it's easy to distribute across across the cluster.",
            "So we used to post for for distributing it, and it's it's relatively straightforward and I think if you have so many data points, like 16,000,000 examples, also is.",
            "So in my experience is not really necessary to come up with fancy approaches because the data is so.",
            "There's so much data that it approximates the underlying distribution quite well and then also the perception is very good.",
            "I mean, of course you could.",
            "You could use you can project everything and like like in the in the pig's approach, for instance.",
            "So I mean we have already the first step.",
            "If we do the projection afterwards, we will have the support vector machine or.",
            "Play some online version of the condition.",
            "I was thinking because there is also distributed in that way, yeah?",
            "Prediction and then.",
            "Then you could do it, but I thought so in order to convince people that.",
            "This might be an interesting research to try to learn from things that are only partially annotated, maybe or maybe weekly entertainer.",
            "I think if you come up with a very sophisticated approach in the beginning then looking OK, sources maybe, or it's it's over.",
            "So this I think gives gives a lot of ideas and and room for improvement so.",
            "Go ahead.",
            "It is really.",
            "I mean normally we say garbage in, garbage out.",
            "And what you are saying is, if we put enough garbage in, we get gold out of it.",
            "I mean, this is really a new claim.",
            "To convince you, took that easy to understand approach.",
            "Yeah, I think it's easier.",
            "Just wondering with it.",
            "It could make even more gold out of.",
            "I mean, this is very old dream of mankind.",
            "Yeah, I think about this.",
            "I mean this I think is obvious ways to program.",
            "So if you would print that issue with account for the for the different distributions, or if you perhaps.",
            "Also if you just using more sophisticated algorithm, I think there's there's much more in it.",
            "I think I think the most important thing is to come for the different distribution, because you can compare them before and you know the datasets already.",
            "He yeah, I think if you just for instance, if you incorporate this simple companionship approach into this I, I would expect that it's that it's better.",
            "Justin.",
            "You have a message.",
            "Data and excellent classification."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this is the focus on learning from sequences and this is joint work with a lot of analysts with right now in prison.",
                    "label": 0
                },
                {
                    "sent": "So that although we present them.",
                    "label": 0
                },
                {
                    "sent": "Some new technical contributions.",
                    "label": 0
                },
                {
                    "sent": "The talk is not really about environment, something, but it's more about how we could reach out to apply more complex machine learning approaches in the real world.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore, I motivate my child is some really so for instance.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So therefore.",
                    "label": 0
                },
                {
                    "sent": "If we want to do related sequence learning, if you want to learn from sequences, that is, we have a sequence is important beyond sequences output and looking for a model, visions first, the input and the output, then a good application is always protein.",
                    "label": 0
                },
                {
                    "sent": "Secondary structure prediction, radius given the input.",
                    "label": 1
                },
                {
                    "sent": "But let's say the primary sequence of the of the protein and we are looking for the secondary structure.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important area is natural language processing differences.",
                    "label": 0
                },
                {
                    "sent": "Part of speech Tagging's is is a tragic where we're looking for annotating every word of a sentence with this part of speech tag.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing, and this is the running example in this talk, is named entity recognition.",
                    "label": 1
                },
                {
                    "sent": "That is, we have a sentence and we're looking.",
                    "label": 0
                },
                {
                    "sent": "We're aiming at detecting all the entities or the names of entities that are within the sentence.",
                    "label": 0
                },
                {
                    "sent": "So here, for instance, we would like to detect that Enrico Palazzo isn't personal.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever we do with sequential learning, we usually use Markov random fields and so in case of sequences we use this chain like structure where the X denote input variables.",
                    "label": 0
                },
                {
                    "sent": "And why are the output variables?",
                    "label": 0
                },
                {
                    "sent": "So this chain extracts are here.",
                    "label": 0
                },
                {
                    "sent": "Is the sequential model for the sentence John is the cat, for instance, and the joint probability of India developer variables.",
                    "label": 1
                },
                {
                    "sent": "Factorizes over their cheeks.",
                    "label": 0
                },
                {
                    "sent": "Here we have two different types of police.",
                    "label": 0
                },
                {
                    "sent": "That's the green clicks between transitions or for transitions, and we have the rupees for available central situations.",
                    "label": 0
                },
                {
                    "sent": "And we can describe this clixby features.",
                    "label": 0
                },
                {
                    "sent": "For instance, is this word a noun?",
                    "label": 0
                },
                {
                    "sent": "Is the following verb, or is this version of it?",
                    "label": 0
                },
                {
                    "sent": "Is the observation job?",
                    "label": 0
                },
                {
                    "sent": "So it's taking up all these features into one large feature vector, which is called often called joint feature representation, because it depends on.",
                    "label": 1
                },
                {
                    "sent": "Not only on the on the input, but also in the output allows us to to rewrite the conditional probability and an output sequence given input sequence as it gives distribution.",
                    "label": 0
                },
                {
                    "sent": "And if we are not really interested in the full posterior, we can take a maximum.",
                    "label": 0
                },
                {
                    "sent": "Approach in and we end up with a with a generalized linear model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most simplest model, I guess that you work with or that tries to adapt this.",
                    "label": 0
                },
                {
                    "sent": "This generalized linear model to data is the structure of perception.",
                    "label": 0
                },
                {
                    "sent": "Is actually very close to the binary perception that you're very familiar with, I guess, so here's a more reminder.",
                    "label": 0
                },
                {
                    "sent": "So the structure perception also starts with an empty model, and then it generates a sequence of models.",
                    "label": 1
                },
                {
                    "sent": "Every time we see a new example of her, we recompute the prediction.",
                    "label": 0
                },
                {
                    "sent": "I will come back to the prediction data and then we compare the outcome of the of this prediction with the true labeling of the sequence and then if they don't do it, they don't go inside.",
                    "label": 0
                },
                {
                    "sent": "If they if they are not identical we need to perform an update.",
                    "label": 0
                },
                {
                    "sent": "So we can show that it converges.",
                    "label": 0
                },
                {
                    "sent": "Is putting on your time with the proof is very similar to the binary perception.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                },
                {
                    "sent": "It has the same guarantee for Morris.",
                    "label": 0
                },
                {
                    "sent": "So they're the only program is problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe the computation of the argument.",
                    "label": 0
                },
                {
                    "sent": "So because now we have sequences, it's a bit more complicated, but.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything turns out to be very simple because.",
                    "label": 0
                },
                {
                    "sent": "And the macro framing feed and the graph structure that I showed you before his rise to the to the Victoria algorithm that is more or less the ticket here.",
                    "label": 0
                },
                {
                    "sent": "I imagine some observations, observation probabilities.",
                    "label": 0
                },
                {
                    "sent": "So if we have the sentence here, look at the record that so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We know that the true output would be the green path in this lettuce.",
                    "label": 0
                },
                {
                    "sent": "That is, we know that hey look, it is not containing entity and then Enrico products.",
                    "label": 1
                },
                {
                    "sent": "So so request the beginning of the person and policy would be inside the person.",
                    "label": 0
                },
                {
                    "sent": "And now the red and has shows an erroneous prediction.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so the prediction says that.",
                    "label": 0
                },
                {
                    "sent": "So is the beginning of your location, for instance, and Enrico is not detected this as an entity that order, so this update that I showed you here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ".",
                    "label": 0
                },
                {
                    "sent": "Is the difference between the feature vector that is generated using the true label sequence and the general feature vector generated with the?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "And very by subjecting them we give rise to retrieve the probability that in the next step when we see the sentence again, for instance, that we will see a path that is more likely to be the green one vendor right now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we want to if we want to find a company, let's say we want to apply or machine learning techniques in the real world, we record data.",
                    "label": 0
                },
                {
                    "sent": "We download everything.",
                    "label": 0
                },
                {
                    "sent": "Most of the times the problem is with the label data, so the the input data is not a problem with.",
                    "label": 0
                },
                {
                    "sent": "The labels are missing.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                },
                {
                    "sent": "Then the the choices you have is like at the label.",
                    "label": 0
                },
                {
                    "sent": "Everything yourself or hire people so it's time consuming, expensive.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you need effort, sometimes you you need maybe even wet lab experiments in biology.",
                    "label": 1
                },
                {
                    "sent": "Things, for instance in English like a bigger team made it even more complex, so the labeling data is is a hard problem.",
                    "label": 0
                },
                {
                    "sent": "If you want to apply.",
                    "label": 0
                },
                {
                    "sent": "Things to do.",
                    "label": 0
                },
                {
                    "sent": "New problems where you don't have that same benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question that we want to address in this talk is.",
                    "label": 0
                },
                {
                    "sent": "Can we leave parts of this annotation to laypeople?",
                    "label": 1
                },
                {
                    "sent": "Can we go to, let's say Mechanical Turk or can we go through pro flow and say?",
                    "label": 0
                },
                {
                    "sent": "David, those parts of our examples, way free, confidently.",
                    "label": 0
                },
                {
                    "sent": "But if you don't know, just leave it unlabeled and.",
                    "label": 0
                },
                {
                    "sent": "We will fill in the gaps later.",
                    "label": 0
                },
                {
                    "sent": "So when is the formalization the problem setting that we try to solve, here's his.",
                    "label": 1
                },
                {
                    "sent": "We want to learn from sequences that are only partially annotated.",
                    "label": 0
                },
                {
                    "sent": "That might have guessed that are unlabeled.",
                    "label": 1
                },
                {
                    "sent": "And where labels might be erroneous.",
                    "label": 0
                },
                {
                    "sent": "And where the missing labels are not uniformly distribute?",
                    "label": 0
                },
                {
                    "sent": "Across the observations.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to.",
                    "label": 0
                },
                {
                    "sent": "To study how difficult it is in principle.",
                    "label": 0
                },
                {
                    "sent": "We we we wanted to show that over that everyone just wants to study the most simplest approach in order to try to see whether or difficult the problem itself is.",
                    "label": 0
                },
                {
                    "sent": "And therefore we came up with a with a simple extension of this perception that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "And so now we again we would have to do the infinite sequence theory of input examples.",
                    "label": 1
                },
                {
                    "sent": "But now the label sequences are only partially David.",
                    "label": 0
                },
                {
                    "sent": "So here for instance, we know a guy who annotated it.",
                    "label": 0
                },
                {
                    "sent": "He knew that the first part of the sentence, hey, look it is the new products who does not contain an entity.",
                    "label": 1
                },
                {
                    "sent": "But then he wasn't sure whether equal Palazzo is like maybe a person or application or whatever, so he didn't know.",
                    "label": 0
                },
                {
                    "sent": "And the idea is.",
                    "label": 0
                },
                {
                    "sent": "We used to use our current model to fill in the gaps in order to use the regular perception idea.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "And to fill in the gaps.",
                    "label": 0
                },
                {
                    "sent": "We could use the constraint between the algorithm, which is pretty much what you what you intuitively would do, so it has a name.",
                    "label": 0
                },
                {
                    "sent": "Is both the constraint.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it works like this.",
                    "label": 0
                },
                {
                    "sent": "So if we already know that the sequence the first part of the sequence does not contain an entity, we can we can follow like we would have like a partial path in the letters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the constraint material algorithm guarantees us that that this path is always followed for the for the optimal solution, and therefore the optimization problem reduces to do only the part that we have.",
                    "label": 0
                },
                {
                    "sent": "We don't have favorite sport.",
                    "label": 0
                },
                {
                    "sent": "Very cute.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we did in the end is we we use the loss of mental perception which had margin is it's a bit more complicated than the one I showed you, but.",
                    "label": 0
                },
                {
                    "sent": "Complicated.",
                    "label": 0
                },
                {
                    "sent": "So the thing that we're doing now is it every time we see a new instance.",
                    "label": 0
                },
                {
                    "sent": "We compute the constraint feature.",
                    "label": 0
                },
                {
                    "sent": "We are ready to fill in the gaps to compute the full labeling sequence of this example, and then we use the regular victory algorithm to compute the loss of mental prediction, and then we compare both and do an update step if necessary.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the most interesting part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "So first of all we want to study whether.",
                    "label": 0
                },
                {
                    "sent": "Control scenario is giving rise to this idea that we can learn from something with this partially sequences.",
                    "label": 0
                },
                {
                    "sent": "So therefore we use the English corner data and we discarded token labels randomly and uniformly.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we wanted to compare the performance to that of the two baselines.",
                    "label": 0
                },
                {
                    "sent": "That is, the discipline of perceptron and and it's only supervised varying.",
                    "label": 1
                },
                {
                    "sent": "But of course these two guys cannot deal with partial nature of the data.",
                    "label": 1
                },
                {
                    "sent": "So what we did is stated.",
                    "label": 1
                },
                {
                    "sent": "We drove for them sentences at random, David them completely and we did that until the ratio of labeled and unlabeled tokens for exactly the same as in in our approach.",
                    "label": 0
                },
                {
                    "sent": "So there is the the data is different, but the number of failures in the information that we put into it is distinct, and we repeated a couple of times to be sure that.",
                    "label": 0
                },
                {
                    "sent": "Randomness doesn't affect.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the result.",
                    "label": 0
                },
                {
                    "sent": "On the horizontal axis, we see that we see the number of the type of percentage of annotated tokens.",
                    "label": 0
                },
                {
                    "sent": "And every time we train all the data, but the data is always the same old training set of coordinates.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In so we see that we are never worse.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And that is, we could say that learning from from from partially annotated data is is if it's uniformly distributed orbit.",
                    "label": 1
                },
                {
                    "sent": "The missing sailor uniform.",
                    "label": 0
                },
                {
                    "sent": "This is almost as good, or maybe a bit better.",
                    "label": 0
                },
                {
                    "sent": "So this was a very promising result.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also show that if we used a certain fraction of data is completely labeled examples and use the remaining parts as partially annotated examples of probability that.",
                    "label": 0
                },
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "But even if we use like 20% of the data is partially annotated data, so there's not so much additional information.",
                    "label": 0
                },
                {
                    "sent": "We can increase the performance by a couple of percentage.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did then is we we want to show that we can also.",
                    "label": 0
                },
                {
                    "sent": "Here, with data that is very noisy, this.",
                    "label": 0
                },
                {
                    "sent": "More realistic anyway, So what we did is we or let's say what we wanted to do is we want to use the code editor.",
                    "label": 0
                },
                {
                    "sent": "And and we want to extract additional data from from Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "Luckily, there's an annotated resource that helped us a lot.",
                    "label": 1
                },
                {
                    "sent": "Text version of the Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "A lot of different energy text annotated.",
                    "label": 0
                },
                {
                    "sent": "And they say they are part of each day or the entities.",
                    "label": 0
                },
                {
                    "sent": "I mean it is not always perfect, right?",
                    "label": 1
                },
                {
                    "sent": "But at least they.",
                    "label": 0
                },
                {
                    "sent": "They did a good job.",
                    "label": 0
                },
                {
                    "sent": "And the most important thing for us is that they preserve the link structure and download it.",
                    "label": 0
                },
                {
                    "sent": "And it's like the whole Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So we we distributed our algorithm because in the end we will have a lot of data and the the distribution scheme wise is recording setup which is pretty much like running the person from on the subset of the data on every computer and then do a join afterwards and then distribute it again.",
                    "label": 0
                },
                {
                    "sent": "Repeat the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the Wikipedia data exchange.",
                    "label": 0
                },
                {
                    "sent": "So here we have like 3 pages.",
                    "label": 0
                },
                {
                    "sent": "It's hope it confuses Spain, Barcelona.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, Barcelona, Madrid the pages for them mentioned Spain because of disease or within Spain and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Office this annotated resource.",
                    "label": 0
                },
                {
                    "sent": "We know that both are Texas locations.",
                    "label": 0
                },
                {
                    "sent": "So this is not always the case, so there there are a lot of errors in this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This annotated resource, but it's that's who we have a perfect world so that we could count the distribution of that so we can count the number of times we have seen Spain.",
                    "label": 0
                },
                {
                    "sent": "David as as a location the number of times you've seen Spain David as a person and we get it, we get this disappearing for distribution.",
                    "label": 0
                },
                {
                    "sent": "So what we do is in order to average more things that we just take them.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eczema?",
                    "label": 0
                },
                {
                    "sent": "So, and that is relatable.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The entity of the entry of of Spain in the Wikipedia said location.",
                    "label": 0
                },
                {
                    "sent": "And then we go back to the text.",
                    "label": 0
                },
                {
                    "sent": "And re label all the occurrences of Spain as a reflection.",
                    "label": 0
                },
                {
                    "sent": "So in using this example, it sounds a bit superfoods but.",
                    "label": 0
                },
                {
                    "sent": "They are set up so that the the tokens are not always there.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you would actually encounter a lot of persons in a lot of organizations, are there for you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we have a sentence that's safer soon as the characters are gone.",
                    "label": 0
                },
                {
                    "sent": "We don't know nothing.",
                    "label": 0
                },
                {
                    "sent": "We only know that Spain is a location.",
                    "label": 0
                },
                {
                    "sent": "And this is the way.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We generate the corpus.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is the the characteristics of the data that we extract.",
                    "label": 0
                },
                {
                    "sent": "So in the end we have like 60,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "Every example contains on average about 0.4 entities which is not developing, so we were missing probably a lot of entities.",
                    "label": 0
                },
                {
                    "sent": "Entities, phrases that do not have a Wikipedia entry.",
                    "label": 0
                },
                {
                    "sent": "Whatever noise could affect us.",
                    "label": 0
                },
                {
                    "sent": "Also, we see that the distribution of entities is different for Francis.",
                    "label": 0
                },
                {
                    "sent": "Persons are not very well captured, so we are we have a completely different distribution in the corner.",
                    "label": 0
                },
                {
                    "sent": "Data is in the in the Wikipedia data and I so.",
                    "label": 0
                },
                {
                    "sent": "To maybe to say already, I think we could if we would take this into account, which we would completely ignore that at that point I think the results could have been much better, but we didn't do that.",
                    "label": 0
                },
                {
                    "sent": "We want to do it as simple.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the response.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the curve that says Cornettos only is trained on the corner elevator on the training set.",
                    "label": 0
                },
                {
                    "sent": "And this other curve is using the partially labeled data that we extracted automatically from Wikipedia as additional data.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We see that we can increase the performance significantly.",
                    "label": 0
                },
                {
                    "sent": "Although the increase is not very impressive, so 0.5 is not very.",
                    "label": 0
                },
                {
                    "sent": "That's a very much so people are not usually at the end of it impressed.",
                    "label": 0
                },
                {
                    "sent": "But you have to take into account that the coordinated training site is already very good for testing on coordinate.",
                    "label": 0
                },
                {
                    "sent": "Which we did.",
                    "label": 0
                },
                {
                    "sent": "The distributions are different and I think people have trained only on say affection of the corner.",
                    "label": 0
                },
                {
                    "sent": "Edge training said it would have been or we would see a lot a lot more here but anyway, so it's a significant.",
                    "label": 0
                },
                {
                    "sent": "I think we can increase the margin.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally we need we repeated this.",
                    "label": 0
                },
                {
                    "sent": "Experimenting in a slightly more complicated way.",
                    "label": 0
                },
                {
                    "sent": "It is we want to do closely with entity recognition.",
                    "label": 0
                },
                {
                    "sent": "That is, we.",
                    "label": 0
                },
                {
                    "sent": "Use the first step that I explain to where we take the Wikipedia entries with.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we take it you using the summary over the distribution.",
                    "label": 0
                },
                {
                    "sent": "And then we propagate the labels.",
                    "label": 0
                },
                {
                    "sent": "And by means of the Wikipedia language, things to the to the corresponding page in another language, which will be Spanish here and then we do the labeling of the sentences in the extraction in Spanish.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that is, once we have the tech for the week.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah page, we propagated to the Spanish version of.",
                    "label": 0
                },
                {
                    "sent": "Of the Wikipedia and now we have the page corresponding.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then related the Spanish sentences, and in this case we know that is funnier is location.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this leaves us with another data state which is much smaller than the English one.",
                    "label": 0
                },
                {
                    "sent": "Because first of all, we expect Wikipedia is a lot smaller than this sort of language is missing, so we lose a lot of data points.",
                    "label": 0
                },
                {
                    "sent": "Again, the distribution is very different, so although now person seems to be represented very well, the locations are are slightly under represented here and But again, we didn't account for for different distributions.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the results show pretty much the same thing.",
                    "label": 0
                },
                {
                    "sent": "We see that we can increase the performance significantly.",
                    "label": 1
                },
                {
                    "sent": "Although again, the the margin in order to say the difference compared to the baseline is is not really high.",
                    "label": 0
                },
                {
                    "sent": "But I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a good start for for the most simplest algorithm that we could think of and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is already.",
                    "label": 0
                },
                {
                    "sent": "Almost the end, so let me conclude.",
                    "label": 0
                },
                {
                    "sent": "So we showed or we studied whether we can learn from data that is partially annotated and perhaps automatically generated.",
                    "label": 1
                },
                {
                    "sent": "Device the simple and straightforward constructive extension of the loss acquainted perceptual.",
                    "label": 0
                },
                {
                    "sent": "We conducted large scale experiments using this data that I showed you, which was extracted from the Wikipedia and our results are always showing and supporting that we can.",
                    "label": 1
                },
                {
                    "sent": "We can indeed use the partially labeled data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, hopefully there will be more impressive results in the future, so thanks for your attention and I'm happy to take it.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Even use a microphone.",
                    "label": 0
                },
                {
                    "sent": "How much are you?",
                    "label": 0
                },
                {
                    "sent": "Depending on checking the prosector approach.",
                    "label": 0
                },
                {
                    "sent": "Work with any other.",
                    "label": 0
                },
                {
                    "sent": "'cause running?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there's yeah of course so we could use everything.",
                    "label": 0
                },
                {
                    "sent": "So the perception is is particularly simple.",
                    "label": 0
                },
                {
                    "sent": "So therefore I think we we picked it because we it's easy to distribute across across the cluster.",
                    "label": 0
                },
                {
                    "sent": "So we used to post for for distributing it, and it's it's relatively straightforward and I think if you have so many data points, like 16,000,000 examples, also is.",
                    "label": 0
                },
                {
                    "sent": "So in my experience is not really necessary to come up with fancy approaches because the data is so.",
                    "label": 0
                },
                {
                    "sent": "There's so much data that it approximates the underlying distribution quite well and then also the perception is very good.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course you could.",
                    "label": 0
                },
                {
                    "sent": "You could use you can project everything and like like in the in the pig's approach, for instance.",
                    "label": 0
                },
                {
                    "sent": "So I mean we have already the first step.",
                    "label": 0
                },
                {
                    "sent": "If we do the projection afterwards, we will have the support vector machine or.",
                    "label": 0
                },
                {
                    "sent": "Play some online version of the condition.",
                    "label": 0
                },
                {
                    "sent": "I was thinking because there is also distributed in that way, yeah?",
                    "label": 0
                },
                {
                    "sent": "Prediction and then.",
                    "label": 0
                },
                {
                    "sent": "Then you could do it, but I thought so in order to convince people that.",
                    "label": 0
                },
                {
                    "sent": "This might be an interesting research to try to learn from things that are only partially annotated, maybe or maybe weekly entertainer.",
                    "label": 0
                },
                {
                    "sent": "I think if you come up with a very sophisticated approach in the beginning then looking OK, sources maybe, or it's it's over.",
                    "label": 0
                },
                {
                    "sent": "So this I think gives gives a lot of ideas and and room for improvement so.",
                    "label": 0
                },
                {
                    "sent": "Go ahead.",
                    "label": 0
                },
                {
                    "sent": "It is really.",
                    "label": 0
                },
                {
                    "sent": "I mean normally we say garbage in, garbage out.",
                    "label": 0
                },
                {
                    "sent": "And what you are saying is, if we put enough garbage in, we get gold out of it.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is really a new claim.",
                    "label": 0
                },
                {
                    "sent": "To convince you, took that easy to understand approach.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it's easier.",
                    "label": 0
                },
                {
                    "sent": "Just wondering with it.",
                    "label": 0
                },
                {
                    "sent": "It could make even more gold out of.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is very old dream of mankind.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think about this.",
                    "label": 0
                },
                {
                    "sent": "I mean this I think is obvious ways to program.",
                    "label": 0
                },
                {
                    "sent": "So if you would print that issue with account for the for the different distributions, or if you perhaps.",
                    "label": 0
                },
                {
                    "sent": "Also if you just using more sophisticated algorithm, I think there's there's much more in it.",
                    "label": 0
                },
                {
                    "sent": "I think I think the most important thing is to come for the different distribution, because you can compare them before and you know the datasets already.",
                    "label": 0
                },
                {
                    "sent": "He yeah, I think if you just for instance, if you incorporate this simple companionship approach into this I, I would expect that it's that it's better.",
                    "label": 0
                },
                {
                    "sent": "Justin.",
                    "label": 0
                },
                {
                    "sent": "You have a message.",
                    "label": 0
                },
                {
                    "sent": "Data and excellent classification.",
                    "label": 0
                }
            ]
        }
    }
}