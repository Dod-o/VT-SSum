{
    "id": "gzwr3jwsut77a4xjwluenqrr4gdb335b",
    "title": "Context-specific Independence Mixture Modelling for Protein Families",
    "info": {
        "author": [
            "Benjamin Georgi, Computational Molecular Biology, Max Planck Institute for Molecular Genetics, Max Planck Institute"
        ],
        "published": "Jan. 29, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecml07_georgi_csi/",
    "segmentation": [
        [
            "So many."
        ],
        [
            "Protein families observe that they fall into subcategories of similar but distinct function, and the specifics of the subfamily is often determined by your small number of residues and one example for that would be the Malays lactate dehydrogenase family were actually a single residue, confers specificity for Ida Malays or lactate, and."
        ],
        [
            "So we're dealing with protein data, which means our input data are multiple sequence alignments and Mrs of protein sequences, and these alignments essentially are just an arrangement of several secret in sequences.",
            "In this first with gaps such that the Houma logis positions are aligned in the same column.",
            "So here we have a small example alignment, and you consider that we have two families, and then, Furthermore, for this example, we assume that we have three functional residues.",
            "We reduced which determine the specific function of these two subfamilies.",
            "And those are shown in the colored columns here, and as you can see on the way these these columns pop up in the alignment is that we have a strong subgroup specific signal of conservation, so that's indicated by the different colors.",
            "So it's all G. Here, it's all are here, and of course in the general case we actually don't know which of these sequences below to which family."
        ],
        [
            "And therefore the problem we are addressing is if you want to do clustering of Perkins sequence clustering of protein sequences to get the subgroup alignment assignments and simultaneously and get predictions for the position of the functional residues.",
            "And there have been prior approaches and dealing with this, but most of those were supervised, that is, they assume that they already know the subgroup memberships of the proteins and also many required additional biological annotations such as knowledge about 3D structures of.",
            "Of protein family members and also many of these methods are based on phylogenetic trees and this might become a problem if you have a data set where it's not possible to infer.",
            "Good phylogenetic tree and so our approach is the first unsupervised method that does not rely on the existence of such a tree.",
            "And so in this method is the context specific independent CSI mixture."
        ],
        [
            "Framework.",
            "So I'm going to introduce this now, and I'm going to start out by giving you notation for the conventional."
        ],
        [
            "Mixture models so mixture models defined for random variable X which is P dimensional and in our case in this application this X represents the row in such a multiple sequence alignment of proteins and then a K component mixture density is just a weighted sum of K distributions over X and these weights are not negative and sum up to one.",
            "So we have paid leave different distributions and they are summed up weighted according to the."
        ],
        [
            "WS.",
            "And so let's consider an example how such a mixture density actually looks like.",
            "So let's consider having MSA of length 4.",
            "So we have semantics four in a three component mixture.",
            "So this is just the definition.",
            "And if you write this out, we get something like this.",
            "So for this application we assume independence between features.",
            "That means that the distribution of the whole vector X is just the product of individual distributions over each feature in each component.",
            "So each row here is a component in the mixture, and if we look a bit closer at this and consider.",
            "Only the parameters of the mixture, we see that it's arranged in this kind of lattice structure.",
            "So for each component each row we have for each feature we have a different set of parameters that we have to estimate from the data we."
        ],
        [
            "Can make this more clear.",
            "By arranging these parameters in this parameter matrix and again the message is that for each component of each row and each feature, we have this distinct.",
            "Set of parameters to estimate and if we take it."
        ],
        [
            "Instead of abstraction, we arrive at, but I prefer to let the model structure matrix and essentially the information is again the same.",
            "So for each cell in this matrix, Excel represent the distinct set of parameters to estimate, and we have fun for the conventional mixtures for each component in each feature, and so now the whole idea of this context specific independence extension."
        ],
        [
            "Is that for many datasets it might be beneficial not to have a distinct set of parameters for all components in all features, so in this kind of CSI structure matrix, But we see here we have a cell spanning multiple rows and this means then that the components you want answer to share a parameterization for the feature X one, and so this modification gives us a number of attractive properties.",
            "So first of all, it obviously reduces the complexity of the resulting model since we have less free parameters that we have to.",
            "Estimate from the data and Secondly consider future explore where we have only a single feature as single set of parameters for all components and so if we have this situation then what this means is that X for the contribution of the feature X4 for the clustering decision the cluster assignment is negated.",
            "So essentially this is seaside structure also allows the negate the contribution of noisy features, so this amounts to a feature selection as part of the structure learning.",
            "And also then, if we have this less complex model, obviously this can help us to avoid overfitting and another aspect of this is that this also gives us a high level description of the relevance of the features for the different clusters.",
            "So for instance we consider feature extreme, it has one set of parameters for the first component and a unique set of parameters in C3, and was this might mean is that actually this feature might be very interesting to characterize feature C3 because this is a unique set of parameters and for all the other clusters we have the same distribution.",
            "And so these are some general."
        ],
        [
            "Parties, but for this specific application, but we want to do is we want to exploit when we use this model structure that we learn to predict functional positions of of these protein cluster."
        ],
        [
            "So would that work?",
            "So let's go back to the example.",
            "So this is our input alignment and now we have considered conventional mixture models of component mixture.",
            "And again, what this means just is that we have for each position in the alignment we have a separate set of parameters in each component."
        ],
        [
            "And then if we learn the structure.",
            "An idealized version of the CSI structure we would like to see and would look like this where we have for all positions which are not functional residues.",
            "We have only a single parameter only where we have the strong signal of subgroup specific conservation.",
            "We actually get the full parameterization.",
            "Now set the idealized cause.",
            "Obviously we wouldn't expect actually to get this picture, but rather than."
        ],
        [
            "The number of positions which are informative to characterize these two clusters and among them are the functional positions.",
            "So more realistic picture might look like this, and then the question is OK, how do we actually get predictions for the functional residues out of those features which have the complete parameterization?"
        ],
        [
            "And for that we were going to rank them, so we're going to rank the features by their importance for the characterization of the specific cluster, and we're doing that by by using the code, but like many but version.",
            "So the relative entropy.",
            "So the rank of feature J for component I is given just by the symmetric relative entropy for one set of parameters Peter Ijeh, which is just the prioritisation, have learned from the data and one set of parameters Peter Zero, at least Peter O is the set of parameters that.",
            "Arises if we assume that this position and Jay is actually uninformative for the clustering, so this.",
            "This allows us to rank the features by their relative importance to the clustering, and then we can take the highest ranking features as the best, most likely predictions for function."
        ],
        [
            "Residues, so of course we all written well, but of course we have to 1st be able to actually learn such a structure from database."
        ],
        [
            "Or we can.",
            "Before we can do that, and the way we approach this is by taking a basic approach, so we score different models, different candidate structures by their posterior."
        ],
        [
            "And so the model looks like this.",
            "So again, so this is the our objective function, the model posterior which we used to score."
        ],
        [
            "And its structures.",
            "And then this is defined by multiplying just like one of the data under our mixture."
        ],
        [
            "By the parameter prior, which is in the typical applications, that's just an uninformative prior and access are similar to adding some accounts to the parameter estimates, and so this means for discrete distributions then the contribute directly."
        ],
        [
            "Bution and the third term is the structure prior, which access in regularizer for the structure.",
            "Learning and introduce the preference for less complex a model into the structure learning and so for this we have a simple factored form."
        ],
        [
            "So how?",
            "How is the structure and actually learn?",
            "So we do this using the structurally and framework by Friedman from 98, and what we can do within this structure and framework is to efficiently store candidate structures by computing the expected sufficient statistics of these structures.",
            "And then this.",
            "This allows us to actually evaluate structures quite quickly, but on the other hand, the space of possible structures.",
            "Of course huge.",
            "So basically what we then do is straightforward research over the structure space.",
            "To arrive at the final at the final."
        ],
        [
            "Suction so so we already started doing that then.",
            "We quickly realized that if we apply the CSI frameworks is a mixture framework for protein data there is."
        ],
        [
            "Conceptual problem and that is that what we're actually interested in is protein function, right?",
            "We want to determine the residues which are most important to determine the function of proteins and that, of course, is a property of the protein fold, but really structure.",
            "But we're working on are the amino acid sequences, and so as he promotes now a few different amino acid substitutions have different structural impacts, and this is the difference in impact is determined by the difference in the chemical properties.",
            "Because if you exchange namino acid with a very different chemical properties, it will have an effect on the structure and if these are more similar than the effect might not be as pronounced and So what we really need is in order to apply these framework and protein data is the notion of amino acid similarity to guide the structure.",
            "Learning to arrive at structures which are actually biological, meaningful and as a remark, this is closely related to the use of substitution matrices in file."
        ],
        [
            "And so let's look at example of this problem.",
            "So if you have now and multiple alignment columns.",
            "Two families, and if you just look at those two columns naively, one might say that OK.",
            "Both of those characterize the clusters that you don't do equally well, because we have no variation basically.",
            "But on the other hand, if we look at the amino acids more closely, we see that E&D.",
            "As for tablet amount are very similar as far as their chemical properties are concerned, whereas easy and aspirin are quite different, and therefore it might be actually better to have a single set of parameters in the second column, cause the substitution of these two amino acids would not have.",
            "In most cases, would not have a strong in."
        ],
        [
            "Check on the structure so so these properties I'm talking about visualized here.",
            "So as you can see, so we have properties like polar, hydrophobic, small, positively charged, negatively charged and so on.",
            "And they form this kind of hierarchy."
        ],
        [
            "And the different way to visualize this would be in such a table set up.",
            "Here we have the 20 amino acids and you have 9 properties we're considering and then X in this table means that this amino acid has this property and that means it has not."
        ],
        [
            "And So what we need to do in order to to get this framework developed on the protein data is to really integrate this.",
            "This amino acid property.",
            "This notion of amino acid property into the model into our objective function with posterior and the idea is to construct a more complex parameter prior Peter which defines the density that is appropriate to do that and the mathematical concept we're going to use our directly mixture priors.",
            "So before I come to."
        ],
        [
            "The mixture priors let me say some words about directly distribution itself.",
            "So the directed sub usually finds the density over distributions, so that is if you have a distribution of some vector theater with 20 components.",
            "So this would be for instance an alphabet of size 20 amino acids.",
            "In our case then this distribution is parameterized by a vector Alpha which has the same number of components and those are non negative and so in essence each of these alphas in a way responsible to the.",
            "For one of the theaters, so A1 gives the preference of the density Formula One and so on, and the way this works is that if we chose Alpha eyes smaller than one then this introduces a preference for expressive firms for a smaller value of feta.",
            "I if it's equal to 1, this is the uninformative case.",
            "Then there is no no bias either way and if Alpha is larger than one then we have a preference for larger pizza.",
            "I so just look at in simple example, let's say before they mentioned.",
            "A distribution with these parameters.",
            "So the 1st two are larger than the second tool.",
            "Then this expresses preference for distributions.",
            "For four dimensional discrete distributions, where theater one and Theater 2 are larger than either three and three to four and essentially the sum of all the parameters of all the alphas gives the strength of the bias of the strength of the."
        ],
        [
            "Prior OK so um.",
            "So as I mentioned, we are going to use several directly distributions as the parameter prior instead of the typical case where we only have a single directly distribution, and then what we gain from this is that we have a lot more flexibility.",
            "It allows for different context and preferences in the induced density over the parameter space of discrete distributions, and one big advantage of course is that it fits seamlessly into the mixture framework, so this decay distribution here is actually retains conjugacy, so the distribution, so all the computations go through.",
            "Quite conveniently, and so the question is, of course, how do we actually model the amino acid prop?"
        ],
        [
            "It is using this framework and so not recall quickly.",
            "This is the property table and what we did simply was before each of these properties he used one component in the digital mixer."
        ],
        [
            "Supplier so we have 9 directly distributions.",
            "And as shown here.",
            "Symbolized here, these distributions are parameterized by parameter vectors, Alpha, which.",
            "Which had basically two values, one act Forex and one dot.",
            "And again this is corresponds to whether the amino acid tested property.",
            "The dedicated solution stands for or not, and then the question is how do we actually chose the specific values of these parameters?",
            "And for now we have used, so we tried different approaches and for now we settled on the rather simple heuristic which gives the values depending on how general the property is.",
            "So basically how many amino acids are there that have?"
        ],
        [
            "Property.",
            "But in any case, So what we gain from this is this yields a probabilistic representation.",
            "The amino acid similarity of this notion of amino acid similarity.",
            "We were looking for, and this.",
            "If we then integrate this into the model, it will drive the parameter estimation and the structure.",
            "Learning to be consistent with this notion of similarity.",
            "And as we have seen, this then yields the improvements of our model."
        ],
        [
            "For protein clustering."
        ],
        [
            "So I come to the results, so we did.",
            "It's first we evaluate this method on three well studied than protein families in order to get the validation of what we have been doing an so we did clustering and prediction of functional residues for this malady lactate dehydrogenase family I mentioned earlier then.",
            "The new cyclist, family and the third data set on kinases, which I won't have time to talk about now."
        ],
        [
            "But so the first data set was the noises and it was a small clean PFM seat alignments, so it was only 29 sequences.",
            "The MSA had length of under 41 and we filtered for highly gap columns because those are.",
            "How to tackle in all these?"
        ],
        [
            "Approaches, and.",
            "So what we get then said I mentioned earlier, single residue determines the specificity forever my lady elected for this family and we did model selection using the normalized entropy criterion would be good.",
            "Got two clusters of being optimal and then actually for the best model we got the very will get a perfect recovery of these two subgroups.",
            "But as I mentioned this is kind of a benign case because it's a very small clean data set and then we did our ranking and consider these top ranks."
        ],
        [
            "Positions.",
            "As predictions of functional residues.",
            "So this is the structure of the E. Coli and a chain one HA, and what is shown here in white.",
            "Hope you can see it.",
            "Other legal interacting sites and these are the top 10 predicted positions according to."
        ],
        [
            "Our model and if we look closely, what we find is that actually the top ranked residue is actually the true, experimentally confirmed residue that determines the substrate specificity and."
        ],
        [
            "Um, if you look at the others, their their biologic plantation was not quite as clear.",
            "So some of those are very quite close to the leading interacting sites, but we're still looking into an additional biological annotation, but actually sort of this sort of personalized result that we get the."
        ],
        [
            "Ranking spot right.",
            "So the second family we are going to talk about was the site places and so there was a big larger than 32 sequences and also large alignment 771.",
            "With the same filter step."
        ],
        [
            "Applied and then for this family has been shown experimentally that the group of five residues will determine the substrate specificity or will at least influence subset specificity when mutated and again model selection gave us two clusters of being optimal and we get the sensitivity of 83% specificity of 87% with respect to the separation into these agencies, and GC and subgroups, and again we considered for the evaluation that."
        ],
        [
            "Rank positions and so this is the structure of the red face."
        ],
        [
            "C2 domain and what we find is that three of the top 10 actually were along those five previously reported experimentally confirmed residues, and actually those three if there are those.",
            "Been found that if those are mutated in a certain way, it will switch the specificity from Guanine to adenine.",
            "So this."
        ],
        [
            "Made sense and we look further.",
            "Also we have two residues which are part of the C1C2 domain at the face of this heterodyne here and it is also known that this heterodimerization is very important for the functional for the function of this."
        ],
        [
            "Cyclists and Lastly, also we found that this residue here was right next to a fossil clean, interacting site and this also gives an indication that this might be relevant for protein function."
        ],
        [
            "So with back I already would like to conclude.",
            "So what I talked about was the clustering of protein families and the simultaneous prediction of functional residues and we are approaches.",
            "Doesn't this integral spit in an unsupervised manner and does not rely on the existence of file kinetic tree?",
            "And I talked about the mixture prior based on amino acid properties we developed and the results on some well studied families are encouraging.",
            "So for future work we are considering obviously.",
            "Whether we can bring machine learning methods to bear to actually learn these parameters of the directly mixture prior from data and also we are going to make a larger analysis of protein families where there are no known initially as a priority known subgroup and assignments and then try to see whether we can make predictions.",
            "Actually for these novel predictions."
        ],
        [
            "So these families."
        ],
        [
            "So as a final remark on the software I developed in order to carry out this analysis is available from our home page."
        ],
        [
            "And that I thank you for your time.",
            "We have a couple of minutes for questions.",
            "Any questions?",
            "One person I have is a.",
            "Your method seems a fixed set of features set, right?",
            "Well, the set of features is given by the length of the multiple sequence alignment.",
            "So basically it's an output of the alignment algorithm, and then we were.",
            "There are two steps where we change number feature.",
            "First.",
            "As I said, we filter for highly get columns in the alignment because they don't care enough information to be useful.",
            "Actually for the analysis and Secondly as part of the structure learning, there's also an implicit feature selection step.",
            "So if you have only single parameter.",
            "For all components then this feature is essentially an negated from the clustering.",
            "It was slightly worse when you bring something into Iraq because you lose some of the distance information between things.",
            "What was the rain generated from the government?",
            "So the ranking was done by?"
        ],
        [
            "Taking the.",
            "The.",
            "By using so this is just the relative entropy between two sets of parameters, and this does the theater ijs are those we get out of the model learning of the parameter estimation, and this is basically a set of parameters that we get if we assume that this position is independent is that it's an informative so not independent but uninformative.",
            "So we took take all the data and if you have model which only a single cluster basically between so you click the clear.",
            "Virgins.",
            "Miracle value, yes, you might get instead of a rank, the top.",
            "The top nine might actually be interesting because there might be a big space between lazy of course.",
            "Wait why you use the rank.",
            "Why not take more account of the actual?",
            "I mean this is something one put the first do, but I mean so.",
            "For this application, when is always in close.",
            "Discussion with the biologist that the experts and they basically want the list of positions to look at, and whether that is interesting and so it's always.",
            "I mean, of course it might be actually be a good idea to get a more complex notion of the ranking, which is expressed by the score, but on the other hand, if it gets too complex to understand then they won't look at it anymore, and so it's simplicity is important in this regard.",
            "But you're right, of course, that probably there's one could do something more involved there.",
            "To follow up on that, but the ranking isn't also look at redundancy in the feature software features, just carbon copy of another and there both Manti.",
            "You're not here.",
            "You're not pulling out now actually, but we don't want to because if you if you consider this example so for this, these two families, all these three features are redundant by your definition, right?",
            "Because they carry the same information for both families.",
            "But actually what we then want is going on.",
            "All three, because they are the functional residues.",
            "So actually this kind of redundancy is not a problem for this approach because it's valuable information we don't want to have the most smallest model feature wise in that regard.",
            "OK, I think we should move on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So many.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Protein families observe that they fall into subcategories of similar but distinct function, and the specifics of the subfamily is often determined by your small number of residues and one example for that would be the Malays lactate dehydrogenase family were actually a single residue, confers specificity for Ida Malays or lactate, and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're dealing with protein data, which means our input data are multiple sequence alignments and Mrs of protein sequences, and these alignments essentially are just an arrangement of several secret in sequences.",
                    "label": 0
                },
                {
                    "sent": "In this first with gaps such that the Houma logis positions are aligned in the same column.",
                    "label": 0
                },
                {
                    "sent": "So here we have a small example alignment, and you consider that we have two families, and then, Furthermore, for this example, we assume that we have three functional residues.",
                    "label": 0
                },
                {
                    "sent": "We reduced which determine the specific function of these two subfamilies.",
                    "label": 0
                },
                {
                    "sent": "And those are shown in the colored columns here, and as you can see on the way these these columns pop up in the alignment is that we have a strong subgroup specific signal of conservation, so that's indicated by the different colors.",
                    "label": 0
                },
                {
                    "sent": "So it's all G. Here, it's all are here, and of course in the general case we actually don't know which of these sequences below to which family.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And therefore the problem we are addressing is if you want to do clustering of Perkins sequence clustering of protein sequences to get the subgroup alignment assignments and simultaneously and get predictions for the position of the functional residues.",
                    "label": 0
                },
                {
                    "sent": "And there have been prior approaches and dealing with this, but most of those were supervised, that is, they assume that they already know the subgroup memberships of the proteins and also many required additional biological annotations such as knowledge about 3D structures of.",
                    "label": 0
                },
                {
                    "sent": "Of protein family members and also many of these methods are based on phylogenetic trees and this might become a problem if you have a data set where it's not possible to infer.",
                    "label": 0
                },
                {
                    "sent": "Good phylogenetic tree and so our approach is the first unsupervised method that does not rely on the existence of such a tree.",
                    "label": 0
                },
                {
                    "sent": "And so in this method is the context specific independent CSI mixture.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to introduce this now, and I'm going to start out by giving you notation for the conventional.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mixture models so mixture models defined for random variable X which is P dimensional and in our case in this application this X represents the row in such a multiple sequence alignment of proteins and then a K component mixture density is just a weighted sum of K distributions over X and these weights are not negative and sum up to one.",
                    "label": 0
                },
                {
                    "sent": "So we have paid leave different distributions and they are summed up weighted according to the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "WS.",
                    "label": 0
                },
                {
                    "sent": "And so let's consider an example how such a mixture density actually looks like.",
                    "label": 1
                },
                {
                    "sent": "So let's consider having MSA of length 4.",
                    "label": 0
                },
                {
                    "sent": "So we have semantics four in a three component mixture.",
                    "label": 1
                },
                {
                    "sent": "So this is just the definition.",
                    "label": 0
                },
                {
                    "sent": "And if you write this out, we get something like this.",
                    "label": 0
                },
                {
                    "sent": "So for this application we assume independence between features.",
                    "label": 0
                },
                {
                    "sent": "That means that the distribution of the whole vector X is just the product of individual distributions over each feature in each component.",
                    "label": 0
                },
                {
                    "sent": "So each row here is a component in the mixture, and if we look a bit closer at this and consider.",
                    "label": 0
                },
                {
                    "sent": "Only the parameters of the mixture, we see that it's arranged in this kind of lattice structure.",
                    "label": 0
                },
                {
                    "sent": "So for each component each row we have for each feature we have a different set of parameters that we have to estimate from the data we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can make this more clear.",
                    "label": 0
                },
                {
                    "sent": "By arranging these parameters in this parameter matrix and again the message is that for each component of each row and each feature, we have this distinct.",
                    "label": 0
                },
                {
                    "sent": "Set of parameters to estimate and if we take it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of abstraction, we arrive at, but I prefer to let the model structure matrix and essentially the information is again the same.",
                    "label": 0
                },
                {
                    "sent": "So for each cell in this matrix, Excel represent the distinct set of parameters to estimate, and we have fun for the conventional mixtures for each component in each feature, and so now the whole idea of this context specific independence extension.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that for many datasets it might be beneficial not to have a distinct set of parameters for all components in all features, so in this kind of CSI structure matrix, But we see here we have a cell spanning multiple rows and this means then that the components you want answer to share a parameterization for the feature X one, and so this modification gives us a number of attractive properties.",
                    "label": 0
                },
                {
                    "sent": "So first of all, it obviously reduces the complexity of the resulting model since we have less free parameters that we have to.",
                    "label": 0
                },
                {
                    "sent": "Estimate from the data and Secondly consider future explore where we have only a single feature as single set of parameters for all components and so if we have this situation then what this means is that X for the contribution of the feature X4 for the clustering decision the cluster assignment is negated.",
                    "label": 0
                },
                {
                    "sent": "So essentially this is seaside structure also allows the negate the contribution of noisy features, so this amounts to a feature selection as part of the structure learning.",
                    "label": 0
                },
                {
                    "sent": "And also then, if we have this less complex model, obviously this can help us to avoid overfitting and another aspect of this is that this also gives us a high level description of the relevance of the features for the different clusters.",
                    "label": 0
                },
                {
                    "sent": "So for instance we consider feature extreme, it has one set of parameters for the first component and a unique set of parameters in C3, and was this might mean is that actually this feature might be very interesting to characterize feature C3 because this is a unique set of parameters and for all the other clusters we have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "And so these are some general.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parties, but for this specific application, but we want to do is we want to exploit when we use this model structure that we learn to predict functional positions of of these protein cluster.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So would that work?",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the example.",
                    "label": 0
                },
                {
                    "sent": "So this is our input alignment and now we have considered conventional mixture models of component mixture.",
                    "label": 1
                },
                {
                    "sent": "And again, what this means just is that we have for each position in the alignment we have a separate set of parameters in each component.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if we learn the structure.",
                    "label": 0
                },
                {
                    "sent": "An idealized version of the CSI structure we would like to see and would look like this where we have for all positions which are not functional residues.",
                    "label": 0
                },
                {
                    "sent": "We have only a single parameter only where we have the strong signal of subgroup specific conservation.",
                    "label": 0
                },
                {
                    "sent": "We actually get the full parameterization.",
                    "label": 0
                },
                {
                    "sent": "Now set the idealized cause.",
                    "label": 0
                },
                {
                    "sent": "Obviously we wouldn't expect actually to get this picture, but rather than.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of positions which are informative to characterize these two clusters and among them are the functional positions.",
                    "label": 0
                },
                {
                    "sent": "So more realistic picture might look like this, and then the question is OK, how do we actually get predictions for the functional residues out of those features which have the complete parameterization?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for that we were going to rank them, so we're going to rank the features by their importance for the characterization of the specific cluster, and we're doing that by by using the code, but like many but version.",
                    "label": 0
                },
                {
                    "sent": "So the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "So the rank of feature J for component I is given just by the symmetric relative entropy for one set of parameters Peter Ijeh, which is just the prioritisation, have learned from the data and one set of parameters Peter Zero, at least Peter O is the set of parameters that.",
                    "label": 0
                },
                {
                    "sent": "Arises if we assume that this position and Jay is actually uninformative for the clustering, so this.",
                    "label": 0
                },
                {
                    "sent": "This allows us to rank the features by their relative importance to the clustering, and then we can take the highest ranking features as the best, most likely predictions for function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Residues, so of course we all written well, but of course we have to 1st be able to actually learn such a structure from database.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or we can.",
                    "label": 0
                },
                {
                    "sent": "Before we can do that, and the way we approach this is by taking a basic approach, so we score different models, different candidate structures by their posterior.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the model looks like this.",
                    "label": 0
                },
                {
                    "sent": "So again, so this is the our objective function, the model posterior which we used to score.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And its structures.",
                    "label": 0
                },
                {
                    "sent": "And then this is defined by multiplying just like one of the data under our mixture.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the parameter prior, which is in the typical applications, that's just an uninformative prior and access are similar to adding some accounts to the parameter estimates, and so this means for discrete distributions then the contribute directly.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bution and the third term is the structure prior, which access in regularizer for the structure.",
                    "label": 0
                },
                {
                    "sent": "Learning and introduce the preference for less complex a model into the structure learning and so for this we have a simple factored form.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "How is the structure and actually learn?",
                    "label": 0
                },
                {
                    "sent": "So we do this using the structurally and framework by Friedman from 98, and what we can do within this structure and framework is to efficiently store candidate structures by computing the expected sufficient statistics of these structures.",
                    "label": 0
                },
                {
                    "sent": "And then this.",
                    "label": 0
                },
                {
                    "sent": "This allows us to actually evaluate structures quite quickly, but on the other hand, the space of possible structures.",
                    "label": 0
                },
                {
                    "sent": "Of course huge.",
                    "label": 0
                },
                {
                    "sent": "So basically what we then do is straightforward research over the structure space.",
                    "label": 0
                },
                {
                    "sent": "To arrive at the final at the final.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suction so so we already started doing that then.",
                    "label": 0
                },
                {
                    "sent": "We quickly realized that if we apply the CSI frameworks is a mixture framework for protein data there is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conceptual problem and that is that what we're actually interested in is protein function, right?",
                    "label": 0
                },
                {
                    "sent": "We want to determine the residues which are most important to determine the function of proteins and that, of course, is a property of the protein fold, but really structure.",
                    "label": 0
                },
                {
                    "sent": "But we're working on are the amino acid sequences, and so as he promotes now a few different amino acid substitutions have different structural impacts, and this is the difference in impact is determined by the difference in the chemical properties.",
                    "label": 0
                },
                {
                    "sent": "Because if you exchange namino acid with a very different chemical properties, it will have an effect on the structure and if these are more similar than the effect might not be as pronounced and So what we really need is in order to apply these framework and protein data is the notion of amino acid similarity to guide the structure.",
                    "label": 0
                },
                {
                    "sent": "Learning to arrive at structures which are actually biological, meaningful and as a remark, this is closely related to the use of substitution matrices in file.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let's look at example of this problem.",
                    "label": 0
                },
                {
                    "sent": "So if you have now and multiple alignment columns.",
                    "label": 0
                },
                {
                    "sent": "Two families, and if you just look at those two columns naively, one might say that OK.",
                    "label": 0
                },
                {
                    "sent": "Both of those characterize the clusters that you don't do equally well, because we have no variation basically.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if we look at the amino acids more closely, we see that E&D.",
                    "label": 0
                },
                {
                    "sent": "As for tablet amount are very similar as far as their chemical properties are concerned, whereas easy and aspirin are quite different, and therefore it might be actually better to have a single set of parameters in the second column, cause the substitution of these two amino acids would not have.",
                    "label": 0
                },
                {
                    "sent": "In most cases, would not have a strong in.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check on the structure so so these properties I'm talking about visualized here.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, so we have properties like polar, hydrophobic, small, positively charged, negatively charged and so on.",
                    "label": 0
                },
                {
                    "sent": "And they form this kind of hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the different way to visualize this would be in such a table set up.",
                    "label": 0
                },
                {
                    "sent": "Here we have the 20 amino acids and you have 9 properties we're considering and then X in this table means that this amino acid has this property and that means it has not.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we need to do in order to to get this framework developed on the protein data is to really integrate this.",
                    "label": 0
                },
                {
                    "sent": "This amino acid property.",
                    "label": 0
                },
                {
                    "sent": "This notion of amino acid property into the model into our objective function with posterior and the idea is to construct a more complex parameter prior Peter which defines the density that is appropriate to do that and the mathematical concept we're going to use our directly mixture priors.",
                    "label": 0
                },
                {
                    "sent": "So before I come to.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The mixture priors let me say some words about directly distribution itself.",
                    "label": 1
                },
                {
                    "sent": "So the directed sub usually finds the density over distributions, so that is if you have a distribution of some vector theater with 20 components.",
                    "label": 0
                },
                {
                    "sent": "So this would be for instance an alphabet of size 20 amino acids.",
                    "label": 0
                },
                {
                    "sent": "In our case then this distribution is parameterized by a vector Alpha which has the same number of components and those are non negative and so in essence each of these alphas in a way responsible to the.",
                    "label": 0
                },
                {
                    "sent": "For one of the theaters, so A1 gives the preference of the density Formula One and so on, and the way this works is that if we chose Alpha eyes smaller than one then this introduces a preference for expressive firms for a smaller value of feta.",
                    "label": 0
                },
                {
                    "sent": "I if it's equal to 1, this is the uninformative case.",
                    "label": 0
                },
                {
                    "sent": "Then there is no no bias either way and if Alpha is larger than one then we have a preference for larger pizza.",
                    "label": 0
                },
                {
                    "sent": "I so just look at in simple example, let's say before they mentioned.",
                    "label": 0
                },
                {
                    "sent": "A distribution with these parameters.",
                    "label": 0
                },
                {
                    "sent": "So the 1st two are larger than the second tool.",
                    "label": 0
                },
                {
                    "sent": "Then this expresses preference for distributions.",
                    "label": 0
                },
                {
                    "sent": "For four dimensional discrete distributions, where theater one and Theater 2 are larger than either three and three to four and essentially the sum of all the parameters of all the alphas gives the strength of the bias of the strength of the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prior OK so um.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, we are going to use several directly distributions as the parameter prior instead of the typical case where we only have a single directly distribution, and then what we gain from this is that we have a lot more flexibility.",
                    "label": 0
                },
                {
                    "sent": "It allows for different context and preferences in the induced density over the parameter space of discrete distributions, and one big advantage of course is that it fits seamlessly into the mixture framework, so this decay distribution here is actually retains conjugacy, so the distribution, so all the computations go through.",
                    "label": 0
                },
                {
                    "sent": "Quite conveniently, and so the question is, of course, how do we actually model the amino acid prop?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is using this framework and so not recall quickly.",
                    "label": 0
                },
                {
                    "sent": "This is the property table and what we did simply was before each of these properties he used one component in the digital mixer.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supplier so we have 9 directly distributions.",
                    "label": 0
                },
                {
                    "sent": "And as shown here.",
                    "label": 0
                },
                {
                    "sent": "Symbolized here, these distributions are parameterized by parameter vectors, Alpha, which.",
                    "label": 0
                },
                {
                    "sent": "Which had basically two values, one act Forex and one dot.",
                    "label": 0
                },
                {
                    "sent": "And again this is corresponds to whether the amino acid tested property.",
                    "label": 0
                },
                {
                    "sent": "The dedicated solution stands for or not, and then the question is how do we actually chose the specific values of these parameters?",
                    "label": 0
                },
                {
                    "sent": "And for now we have used, so we tried different approaches and for now we settled on the rather simple heuristic which gives the values depending on how general the property is.",
                    "label": 0
                },
                {
                    "sent": "So basically how many amino acids are there that have?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Property.",
                    "label": 0
                },
                {
                    "sent": "But in any case, So what we gain from this is this yields a probabilistic representation.",
                    "label": 0
                },
                {
                    "sent": "The amino acid similarity of this notion of amino acid similarity.",
                    "label": 0
                },
                {
                    "sent": "We were looking for, and this.",
                    "label": 0
                },
                {
                    "sent": "If we then integrate this into the model, it will drive the parameter estimation and the structure.",
                    "label": 0
                },
                {
                    "sent": "Learning to be consistent with this notion of similarity.",
                    "label": 0
                },
                {
                    "sent": "And as we have seen, this then yields the improvements of our model.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For protein clustering.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I come to the results, so we did.",
                    "label": 0
                },
                {
                    "sent": "It's first we evaluate this method on three well studied than protein families in order to get the validation of what we have been doing an so we did clustering and prediction of functional residues for this malady lactate dehydrogenase family I mentioned earlier then.",
                    "label": 0
                },
                {
                    "sent": "The new cyclist, family and the third data set on kinases, which I won't have time to talk about now.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But so the first data set was the noises and it was a small clean PFM seat alignments, so it was only 29 sequences.",
                    "label": 0
                },
                {
                    "sent": "The MSA had length of under 41 and we filtered for highly gap columns because those are.",
                    "label": 0
                },
                {
                    "sent": "How to tackle in all these?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approaches, and.",
                    "label": 0
                },
                {
                    "sent": "So what we get then said I mentioned earlier, single residue determines the specificity forever my lady elected for this family and we did model selection using the normalized entropy criterion would be good.",
                    "label": 0
                },
                {
                    "sent": "Got two clusters of being optimal and then actually for the best model we got the very will get a perfect recovery of these two subgroups.",
                    "label": 0
                },
                {
                    "sent": "But as I mentioned this is kind of a benign case because it's a very small clean data set and then we did our ranking and consider these top ranks.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Positions.",
                    "label": 0
                },
                {
                    "sent": "As predictions of functional residues.",
                    "label": 1
                },
                {
                    "sent": "So this is the structure of the E. Coli and a chain one HA, and what is shown here in white.",
                    "label": 0
                },
                {
                    "sent": "Hope you can see it.",
                    "label": 0
                },
                {
                    "sent": "Other legal interacting sites and these are the top 10 predicted positions according to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model and if we look closely, what we find is that actually the top ranked residue is actually the true, experimentally confirmed residue that determines the substrate specificity and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, if you look at the others, their their biologic plantation was not quite as clear.",
                    "label": 0
                },
                {
                    "sent": "So some of those are very quite close to the leading interacting sites, but we're still looking into an additional biological annotation, but actually sort of this sort of personalized result that we get the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ranking spot right.",
                    "label": 0
                },
                {
                    "sent": "So the second family we are going to talk about was the site places and so there was a big larger than 32 sequences and also large alignment 771.",
                    "label": 0
                },
                {
                    "sent": "With the same filter step.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied and then for this family has been shown experimentally that the group of five residues will determine the substrate specificity or will at least influence subset specificity when mutated and again model selection gave us two clusters of being optimal and we get the sensitivity of 83% specificity of 87% with respect to the separation into these agencies, and GC and subgroups, and again we considered for the evaluation that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rank positions and so this is the structure of the red face.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "C2 domain and what we find is that three of the top 10 actually were along those five previously reported experimentally confirmed residues, and actually those three if there are those.",
                    "label": 1
                },
                {
                    "sent": "Been found that if those are mutated in a certain way, it will switch the specificity from Guanine to adenine.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Made sense and we look further.",
                    "label": 0
                },
                {
                    "sent": "Also we have two residues which are part of the C1C2 domain at the face of this heterodyne here and it is also known that this heterodimerization is very important for the functional for the function of this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cyclists and Lastly, also we found that this residue here was right next to a fossil clean, interacting site and this also gives an indication that this might be relevant for protein function.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with back I already would like to conclude.",
                    "label": 0
                },
                {
                    "sent": "So what I talked about was the clustering of protein families and the simultaneous prediction of functional residues and we are approaches.",
                    "label": 0
                },
                {
                    "sent": "Doesn't this integral spit in an unsupervised manner and does not rely on the existence of file kinetic tree?",
                    "label": 0
                },
                {
                    "sent": "And I talked about the mixture prior based on amino acid properties we developed and the results on some well studied families are encouraging.",
                    "label": 0
                },
                {
                    "sent": "So for future work we are considering obviously.",
                    "label": 0
                },
                {
                    "sent": "Whether we can bring machine learning methods to bear to actually learn these parameters of the directly mixture prior from data and also we are going to make a larger analysis of protein families where there are no known initially as a priority known subgroup and assignments and then try to see whether we can make predictions.",
                    "label": 0
                },
                {
                    "sent": "Actually for these novel predictions.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these families.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a final remark on the software I developed in order to carry out this analysis is available from our home page.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that I thank you for your time.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of minutes for questions.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "One person I have is a.",
                    "label": 0
                },
                {
                    "sent": "Your method seems a fixed set of features set, right?",
                    "label": 0
                },
                {
                    "sent": "Well, the set of features is given by the length of the multiple sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "So basically it's an output of the alignment algorithm, and then we were.",
                    "label": 0
                },
                {
                    "sent": "There are two steps where we change number feature.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "As I said, we filter for highly get columns in the alignment because they don't care enough information to be useful.",
                    "label": 0
                },
                {
                    "sent": "Actually for the analysis and Secondly as part of the structure learning, there's also an implicit feature selection step.",
                    "label": 0
                },
                {
                    "sent": "So if you have only single parameter.",
                    "label": 0
                },
                {
                    "sent": "For all components then this feature is essentially an negated from the clustering.",
                    "label": 0
                },
                {
                    "sent": "It was slightly worse when you bring something into Iraq because you lose some of the distance information between things.",
                    "label": 0
                },
                {
                    "sent": "What was the rain generated from the government?",
                    "label": 0
                },
                {
                    "sent": "So the ranking was done by?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "By using so this is just the relative entropy between two sets of parameters, and this does the theater ijs are those we get out of the model learning of the parameter estimation, and this is basically a set of parameters that we get if we assume that this position is independent is that it's an informative so not independent but uninformative.",
                    "label": 0
                },
                {
                    "sent": "So we took take all the data and if you have model which only a single cluster basically between so you click the clear.",
                    "label": 0
                },
                {
                    "sent": "Virgins.",
                    "label": 0
                },
                {
                    "sent": "Miracle value, yes, you might get instead of a rank, the top.",
                    "label": 0
                },
                {
                    "sent": "The top nine might actually be interesting because there might be a big space between lazy of course.",
                    "label": 0
                },
                {
                    "sent": "Wait why you use the rank.",
                    "label": 0
                },
                {
                    "sent": "Why not take more account of the actual?",
                    "label": 0
                },
                {
                    "sent": "I mean this is something one put the first do, but I mean so.",
                    "label": 0
                },
                {
                    "sent": "For this application, when is always in close.",
                    "label": 0
                },
                {
                    "sent": "Discussion with the biologist that the experts and they basically want the list of positions to look at, and whether that is interesting and so it's always.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course it might be actually be a good idea to get a more complex notion of the ranking, which is expressed by the score, but on the other hand, if it gets too complex to understand then they won't look at it anymore, and so it's simplicity is important in this regard.",
                    "label": 0
                },
                {
                    "sent": "But you're right, of course, that probably there's one could do something more involved there.",
                    "label": 0
                },
                {
                    "sent": "To follow up on that, but the ranking isn't also look at redundancy in the feature software features, just carbon copy of another and there both Manti.",
                    "label": 0
                },
                {
                    "sent": "You're not here.",
                    "label": 0
                },
                {
                    "sent": "You're not pulling out now actually, but we don't want to because if you if you consider this example so for this, these two families, all these three features are redundant by your definition, right?",
                    "label": 0
                },
                {
                    "sent": "Because they carry the same information for both families.",
                    "label": 0
                },
                {
                    "sent": "But actually what we then want is going on.",
                    "label": 0
                },
                {
                    "sent": "All three, because they are the functional residues.",
                    "label": 0
                },
                {
                    "sent": "So actually this kind of redundancy is not a problem for this approach because it's valuable information we don't want to have the most smallest model feature wise in that regard.",
                    "label": 0
                },
                {
                    "sent": "OK, I think we should move on.",
                    "label": 0
                }
            ]
        }
    }
}