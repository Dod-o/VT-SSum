{
    "id": "3yg7zx7p6gxqcvllya2fxvr3aiwch236",
    "title": "Sparse Algorithms are Not Stable: A No-free-lunch Theorem",
    "info": {
        "author": [
            "Huan Xu, Department of Mechanical Engineering, National University of Singapore"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_xu_theorem/",
    "segmentation": [
        [
            "So I'm going to talk about the no free lunch theorem.",
            "Basically says that sparse algorithms are not stable.",
            "This is joint work with constant incur meniscus with Utah State and the shame, and now he was my PhD supervisor.",
            "He is now with the technical.",
            "So to set up that we are looking at is the old fashion standard linear?"
        ],
        [
            "Regression, so there's nothing fancy there.",
            "We have points, maybe.",
            "I did, maybe no ID.",
            "Those are the points that you have for each point to have a label.",
            "It's a real value, the label and your Gulf.",
            "Of course, doing a linear regression, your goal is to find some linear function so that given a new, if someone gives you a new point, you can approximately correctly predict the label of this point.",
            "So that's that's the setup that we're looking at.",
            "But we're not going to propose.",
            "And then new regression algorithms.",
            "In contrast, we are looking at all the learning algorithms that are set, so I'm not studying when algorithm said in the whole group of algorithms.",
            "So of course I need to define what is a learning algorithm, so we say that's a learning algorithm is no more no less.",
            "But a mapping from a set of training samples.",
            "Into your solution so you have a solution space.",
            "You have a huge space of possible training sample sets someone gives.",
            "You are training sample set.",
            "You pick you pick a solution and the way how you pick this solution defines your learning algorithm.",
            "So any algorithm is such such kind of mapping.",
            "Yes.",
            "My.",
            "More salt.",
            "And of course, after you find your solution, you have to have to measure whether solutions are good too and on our bed one at this moment.",
            "So we think of an absolute loss later we extend this to more arbitrary loss, so absolute loss, meaning that someone gives you a new testing sample.",
            "You do your prediction and you can pay a prediction with the label, the true label of this sample, and to see how much your prediction deviates from the true label.",
            "Now."
        ],
        [
            "Think of a learning algorithm regression algorithm.",
            "What what will be the properties that you would like if you design the learning algorithms of course."
        ],
        [
            "First thing that we want is the loss should be small.",
            "Since I'm doing regression, I don't want to have a regression which has a huge loss."
        ],
        [
            "But besides that, there are some other properties that people believe at least a lot of machine learning people believes are important."
        ],
        [
            "And the desirable, for example, sparsity sparsity, meaning that when I do linear regression, I want my solution to have as field 90 variables as possible.",
            "So I don't want a solution which is very dense.",
            "I want a solution which has only maybe 390 or 590 out of 100 coefficients, and there's a lot of reason why sparsity is important, But basically can be grouped into two kinds of reasons.",
            "The first thing is it's possible that nature itself prism sparsity pattern that generates the data, and exploiting sparsity.",
            "You can recover this structure.",
            "And the other thing is, in in this high dimensional age we often have the problem of we have less samples but more coefficients and the only way to get a consistent estimation is to exploit this sparsity."
        ],
        [
            "Another property that people also thinks probably is of interest is stability.",
            "Intuitive, intuitively speaking, stability means that if I give you 2 slightly different training sample sets, now you run your learning algorithms.",
            "You will get two different solutions, but these two solutions should be close enough.",
            "Let's the intuitive explanation of stability.",
            "While it's important there's philosophique rhythm and also that's a more technical rhythm.",
            "Because when you have an error which is stable, then it provides a way to show that this algorithm has good statistical properties like consistency or generalization.",
            "So this is a.",
            "This is one more slides are."
        ],
        [
            "A more detailed slides are sparsity for why we want to sparsity, and in the last maybe 5 to 10 years.",
            "There has been thousands of paper showing that what type of algorithms encourages sparsity or encourage what type of sparsity.",
            "Basically it all starts from this.",
            "This framework saying that I want to do regression and I have a cardinality constraint on the solution that I have.",
            "Cheaper sure that this is in fact a very difficult question.",
            "Difficult problem to solve.",
            "So one way is to do convex relaxation, which leads to the now very famous allosso formulation that has been a huge literature on sparsity.",
            "So I would like to."
        ],
        [
            "Spend a little bit more time on stability, since this is a less familiar concept compared to sparsity.",
            "Intuitively speaking, stability is defined if I have training sample set.",
            "I learned a solution with respect to this set.",
            "Now remove 1 sample from my set.",
            "My set may have like 1000 samples and then remove one out of this one.",
            "So the samples I rerun my learning algorithms and I get a new solution.",
            "Given this 999 samples.",
            "Intuitively speaking stability means that these two solutions must be close."
        ],
        [
            "Now, formally, how we define close there's this Seminole Paper by Bruce Catton, at least in 2002 they define notion called the Uniform stability.",
            "So this is like a worst case analysis, it says.",
            "You pick any possible training sample set with N samples.",
            "You pick any training sample, set, you pick any testing sample and you remove any point from your training sample set.",
            "Elinda ceiling solution with respect to the original sets and with respect to sets with one sample removed.",
            "And you test these two solutions with respect to your testing sample that you arbitrarily chosen the gap between the loss should be smaller equals two.",
            "This stability number.",
            "Better so bad.",
            "So this is a worst case analysis because you can pick any training sample set and you can pick any test sample."
        ],
        [
            "So for example, we have both candidates have showed that if you're looking at L2 regularizer regression.",
            "Then the stability bounds.",
            "This is better N scales as fast as of one over."
        ],
        [
            "And the authors show that as long as your stability bounded skills at least as fast as 1 / sqrt N. Then you will have a simple thought symptomatically generalized.",
            "So basically that means the training error you have and the testing error you have will be the same when the number of samples goes to Infinity."
        ],
        [
            "So.",
            "Enough for background.",
            "This is the main meat of today's talk into this talk.",
            "I'm going to say that sparse algorithm spaces are good things.",
            "Stable is a good thing, but you cannot have both.",
            "If you have an algorithm which is sparse, then it cannot be stable.",
            "A little I will make this more formal."
        ],
        [
            "But the plan from here is I will first illustrate this by proving for famous special algorithm.",
            "I show that tell us so we know that illustrates as possible, which I will show that the lawsuit is not stable.",
            "Not stable, meaning that the stability pyramid of Lasso does not goes down.",
            "It is lower bounded by some constant which does not go start when the number of samples increases.",
            "And inspired by this special case, we then talk about how to generalize to a general algorithm that is bus.",
            "So I believe everyone here."
        ],
        [
            "No so Tesla, so this is the formulation."
        ],
        [
            "And.",
            "Now the serum.",
            "In the interview, it says that its stability perimetre of this lawsuit is lower bounded by a constant.",
            "But what constant?"
        ],
        [
            "So here is the definition of this constant."
        ],
        [
            "Called the pseudo maximal error.",
            "It means that if you have the freedom to choose any training sample and any testing sample you run your algorithms on this training sample and the test on your testing sample.",
            "The maximum error that you can get recalled.",
            "This pseudo maximal error.",
            "So assume that so you know the linear algorithms and you want to screw the world who designed this linear algorithm by picking, so that's not ideal adoption, so that's not distribution alarm set, so you can arbitrary picking your training sample in the testing sample tried to.",
            "Make this algorithm break, and this defines a number this week.",
            "Order pseudo maximum error and will show that for Lasso.",
            "The stability bond is lower bounded by this pseudo maximal error.",
            "Of course, the student maximum error will not decrease when you're training.",
            "Samples increase because you can as an adversary can always pick the worst training sample repeatedly.",
            "Pick this, reverse the training sample and then pick a better testing sample.",
            "Right?",
            "So more precisely, this theorem says that we need to think of two regression problem at the same time.",
            "The first problem thinks of a problem of M features."
        ],
        [
            "The second problem is a regression problem with two M features, and the theorem says that the stability bounds of the regression over 2M features is lower bounded by the pseudo maximum error of the problem with M features.",
            "Yes.",
            "It is not infinite because your training sample set maybe a compact set.",
            "Right, but if if you think of like Euclidean space then it can be Infinity, right?",
            "So let's see why this is."
        ],
        [
            "Threat, it's a basically one slide proof, and it's very intuitive, but before goes into a new technique detail.",
            "The idea is very simple.",
            "Uniform stability as I have, however, emphasized is a worst case analysis.",
            "So to show that an algorithm has an uniform stability which is bounded away from zero, I only need to construct away with ample, as long as I can construct the wind training sample set, such as remove one point of this set, you will get a complete different solution and the testing on some testing sample.",
            "The two solutions give dramatically different performance then the proof is complete.",
            "I don't need to talk about like a general sets, only need to construct wampole and the proof is basically constructing this example.",
            "So we first pick BA, Zeda and Zero so that they put together jointly achieve the pseudo maximal error.",
            "And this is for the for the problem with M features.",
            "Now I construct the training sample set for the problems."
        ],
        [
            "Of two M features, and this sexist optimal solution of this training sample set."
        ],
        [
            "So the new sample set for the 2M features is constant like.",
            "This way the first examples is the same as the original sample set for the for the M featured thing, but with M features, which is exactly the same as the previous one.",
            "So I duplicate and features.",
            "So that's that's the first examples.",
            "And then I add a when you sample.",
            "The new samples construct.",
            "This way I have M features which are completed zero and the next N features which is the same as the other stuff and it's labor is given by zero.",
            "So now I have a training sample set with N + 1 sample.",
            "So the next question I ask is."
        ],
        [
            "Which solution is the optimal solution of this one, and my claim is X, star zero is an optimal solution with respect to this training sample set, while that if we only look at the first N samples.",
            "We know that extra is the optimal solution for this part.",
            "And zero doesn't play any role with respect to these features, right?",
            "So X zero is the optimal solution for the first examples.",
            "Also, for this sample, if you test X0 with this sample test, zero loss so X zero, this solution perfectly predicted this sample.",
            "So of course it is also optimal to the last sample.",
            "That's why X zero is an optimal solution with respect to the whole training set.",
            "Right, I hope you will agree with me."
        ],
        [
            "Now remove the last sample.",
            "I only have examples.",
            "I remove the last sample and then retrain my lasso over this N sample set.",
            "By symmetric, it's easy to see that zero X is also optimal solution.",
            "Right?",
            "So I have two complete different solutions with respect to N + 1 samples and the examples so my."
        ],
        [
            "Last step is just to show you that there exists some testing sample such that these two solutions give different performance and the solution is quite the test sample is quite easy as constructors just this one.",
            "If you test X0 with respect to this, it gives you zero loss if you test the 0X with respect to this by the definition of here, you will get to the pseudo maximum error.",
            "Right?",
            "So that's why for less so.",
            "By removing one point you have two solutions which are complete, different whose performance is different with at least the lower bounded by the pseudo maximum error and that proves the.",
            "So as an algorithm is not stable, or at least it does not satisfy the uniform stability concept.",
            "After we."
        ],
        [
            "It's discovered that and the most immediate question that you want to answer is, is this something special for lasso, or can we generalize it to most generalize the algorithms?"
        ],
        [
            "And we do that.",
            "The basic band inverse engineering, meaning that we really look at the proof and see where we can relax the thing.",
            "And also that's one question that that's one point that happened.",
            "I have not addressed yet.",
            "The title says the tradeoff between sparsity and the stability, but within the proof vest, nothing talk about the sparsity, right?"
        ],
        [
            "So basically"
        ],
        [
            "For all space algorithms and more precisely, force positive mean that algorithm that can identify redundant features.",
            "So if you give the algorithm to two columns of each."
        ],
        [
            "Which are completely the same.",
            "Then one of the optimal solution at least should put zero on on this on one of the two columns."
        ],
        [
            "And then this is.",
            "Then we also generalize the notion of algorithms.",
            "Instead of saying that the algorithms is an optimization problem.",
            "Basically, we just say that the algorithm just defines the relationship.",
            "Given training sets, it defines all the relationship."
        ],
        [
            "All the candidates solutions.",
            "Of course, if there will be space optimization, it falls into this cat."
        ],
        [
            "Great, and there's also some assumptions that we need to make.",
            "The first assumption says that.",
            "If one solution is better than the other solution with respect to some training set.",
            "And these two solutions, the Jess coefficient of these two solutions are zero.",
            "Then I don't care what is the value that I put in my training set for this coefficient for this feature.",
            "So if I put a zero on certain coefficient then its respective features, I don't care the value."
        ],
        [
            "The second assumption says that.",
            "If I have a solution which is better than the other one and add the new training sample which is perfectly predicted by the 2nd solution, then to should still be preferrable preferred over the first one.",
            "The certain options."
        ],
        [
            "I said if I added some dummy features and I added, then the preference relationship should not change."
        ],
        [
            "And the last one."
        ],
        [
            "Says that if I do some permutations over the features and over the solutions together, the preference relationship should still preserved.",
            "So these are all I think there's a net."
        ],
        [
            "Options, but given these assumptions, basically."
        ],
        [
            "Using the same, using the same proof technique, you can show that any algorithms that identify redundant features and they satisfy these options.",
            "Is not stable and the proof is the same is just to construct a training sample set which duplicate.",
            "So you defined as the set defined original sets and you duplicate the features.",
            "This give the first training sample set and add another new training sample which is the worst example that you can have.",
            "And using all the assumptions, we can show that."
        ],
        [
            "For this, further the N + 1 set X star zero is an optimal solution, while if you remove 1 sample zero X * is an optimal solution.",
            "Then just test these two solutions with respect to the."
        ],
        [
            "It's the same sample and that gives you the that gives you the pseudo maximum error as a lower bound of your stability."
        ],
        [
            "So before."
        ],
        [
            "Including talk, I want to see some limitation of this work.",
            "So as as as one of the audience points out, uniform stability is a very restrictive concept in terms of stability.",
            "In fact, this has been like a spectrum of different concepts of stability and uniform stability.",
            "Is is the most restrictive one and it is of course very interesting to see.",
            "Beyond the uniform stability, what other relaxed notion is also in conflict with sparsity?",
            "And also what other properties to beyond the stability itself with other properties can be in conflict with sparsity's?",
            "And also we're interested in like extended this framework from regression to other type of learning algorithms."
        ],
        [
            "So thanks for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the no free lunch theorem.",
                    "label": 1
                },
                {
                    "sent": "Basically says that sparse algorithms are not stable.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with constant incur meniscus with Utah State and the shame, and now he was my PhD supervisor.",
                    "label": 0
                },
                {
                    "sent": "He is now with the technical.",
                    "label": 0
                },
                {
                    "sent": "So to set up that we are looking at is the old fashion standard linear?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regression, so there's nothing fancy there.",
                    "label": 0
                },
                {
                    "sent": "We have points, maybe.",
                    "label": 0
                },
                {
                    "sent": "I did, maybe no ID.",
                    "label": 0
                },
                {
                    "sent": "Those are the points that you have for each point to have a label.",
                    "label": 1
                },
                {
                    "sent": "It's a real value, the label and your Gulf.",
                    "label": 1
                },
                {
                    "sent": "Of course, doing a linear regression, your goal is to find some linear function so that given a new, if someone gives you a new point, you can approximately correctly predict the label of this point.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the setup that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to propose.",
                    "label": 0
                },
                {
                    "sent": "And then new regression algorithms.",
                    "label": 0
                },
                {
                    "sent": "In contrast, we are looking at all the learning algorithms that are set, so I'm not studying when algorithm said in the whole group of algorithms.",
                    "label": 0
                },
                {
                    "sent": "So of course I need to define what is a learning algorithm, so we say that's a learning algorithm is no more no less.",
                    "label": 0
                },
                {
                    "sent": "But a mapping from a set of training samples.",
                    "label": 0
                },
                {
                    "sent": "Into your solution so you have a solution space.",
                    "label": 0
                },
                {
                    "sent": "You have a huge space of possible training sample sets someone gives.",
                    "label": 1
                },
                {
                    "sent": "You are training sample set.",
                    "label": 0
                },
                {
                    "sent": "You pick you pick a solution and the way how you pick this solution defines your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So any algorithm is such such kind of mapping.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                },
                {
                    "sent": "More salt.",
                    "label": 0
                },
                {
                    "sent": "And of course, after you find your solution, you have to have to measure whether solutions are good too and on our bed one at this moment.",
                    "label": 0
                },
                {
                    "sent": "So we think of an absolute loss later we extend this to more arbitrary loss, so absolute loss, meaning that someone gives you a new testing sample.",
                    "label": 0
                },
                {
                    "sent": "You do your prediction and you can pay a prediction with the label, the true label of this sample, and to see how much your prediction deviates from the true label.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of a learning algorithm regression algorithm.",
                    "label": 0
                },
                {
                    "sent": "What what will be the properties that you would like if you design the learning algorithms of course.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing that we want is the loss should be small.",
                    "label": 0
                },
                {
                    "sent": "Since I'm doing regression, I don't want to have a regression which has a huge loss.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But besides that, there are some other properties that people believe at least a lot of machine learning people believes are important.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the desirable, for example, sparsity sparsity, meaning that when I do linear regression, I want my solution to have as field 90 variables as possible.",
                    "label": 0
                },
                {
                    "sent": "So I don't want a solution which is very dense.",
                    "label": 0
                },
                {
                    "sent": "I want a solution which has only maybe 390 or 590 out of 100 coefficients, and there's a lot of reason why sparsity is important, But basically can be grouped into two kinds of reasons.",
                    "label": 0
                },
                {
                    "sent": "The first thing is it's possible that nature itself prism sparsity pattern that generates the data, and exploiting sparsity.",
                    "label": 0
                },
                {
                    "sent": "You can recover this structure.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, in in this high dimensional age we often have the problem of we have less samples but more coefficients and the only way to get a consistent estimation is to exploit this sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another property that people also thinks probably is of interest is stability.",
                    "label": 0
                },
                {
                    "sent": "Intuitive, intuitively speaking, stability means that if I give you 2 slightly different training sample sets, now you run your learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "You will get two different solutions, but these two solutions should be close enough.",
                    "label": 0
                },
                {
                    "sent": "Let's the intuitive explanation of stability.",
                    "label": 0
                },
                {
                    "sent": "While it's important there's philosophique rhythm and also that's a more technical rhythm.",
                    "label": 0
                },
                {
                    "sent": "Because when you have an error which is stable, then it provides a way to show that this algorithm has good statistical properties like consistency or generalization.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is one more slides are.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A more detailed slides are sparsity for why we want to sparsity, and in the last maybe 5 to 10 years.",
                    "label": 0
                },
                {
                    "sent": "There has been thousands of paper showing that what type of algorithms encourages sparsity or encourage what type of sparsity.",
                    "label": 0
                },
                {
                    "sent": "Basically it all starts from this.",
                    "label": 0
                },
                {
                    "sent": "This framework saying that I want to do regression and I have a cardinality constraint on the solution that I have.",
                    "label": 0
                },
                {
                    "sent": "Cheaper sure that this is in fact a very difficult question.",
                    "label": 0
                },
                {
                    "sent": "Difficult problem to solve.",
                    "label": 0
                },
                {
                    "sent": "So one way is to do convex relaxation, which leads to the now very famous allosso formulation that has been a huge literature on sparsity.",
                    "label": 0
                },
                {
                    "sent": "So I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spend a little bit more time on stability, since this is a less familiar concept compared to sparsity.",
                    "label": 0
                },
                {
                    "sent": "Intuitively speaking, stability is defined if I have training sample set.",
                    "label": 0
                },
                {
                    "sent": "I learned a solution with respect to this set.",
                    "label": 0
                },
                {
                    "sent": "Now remove 1 sample from my set.",
                    "label": 0
                },
                {
                    "sent": "My set may have like 1000 samples and then remove one out of this one.",
                    "label": 0
                },
                {
                    "sent": "So the samples I rerun my learning algorithms and I get a new solution.",
                    "label": 0
                },
                {
                    "sent": "Given this 999 samples.",
                    "label": 0
                },
                {
                    "sent": "Intuitively speaking stability means that these two solutions must be close.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, formally, how we define close there's this Seminole Paper by Bruce Catton, at least in 2002 they define notion called the Uniform stability.",
                    "label": 1
                },
                {
                    "sent": "So this is like a worst case analysis, it says.",
                    "label": 0
                },
                {
                    "sent": "You pick any possible training sample set with N samples.",
                    "label": 0
                },
                {
                    "sent": "You pick any training sample, set, you pick any testing sample and you remove any point from your training sample set.",
                    "label": 0
                },
                {
                    "sent": "Elinda ceiling solution with respect to the original sets and with respect to sets with one sample removed.",
                    "label": 1
                },
                {
                    "sent": "And you test these two solutions with respect to your testing sample that you arbitrarily chosen the gap between the loss should be smaller equals two.",
                    "label": 0
                },
                {
                    "sent": "This stability number.",
                    "label": 0
                },
                {
                    "sent": "Better so bad.",
                    "label": 0
                },
                {
                    "sent": "So this is a worst case analysis because you can pick any training sample set and you can pick any test sample.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, we have both candidates have showed that if you're looking at L2 regularizer regression.",
                    "label": 0
                },
                {
                    "sent": "Then the stability bounds.",
                    "label": 0
                },
                {
                    "sent": "This is better N scales as fast as of one over.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the authors show that as long as your stability bounded skills at least as fast as 1 / sqrt N. Then you will have a simple thought symptomatically generalized.",
                    "label": 0
                },
                {
                    "sent": "So basically that means the training error you have and the testing error you have will be the same when the number of samples goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Enough for background.",
                    "label": 0
                },
                {
                    "sent": "This is the main meat of today's talk into this talk.",
                    "label": 1
                },
                {
                    "sent": "I'm going to say that sparse algorithm spaces are good things.",
                    "label": 0
                },
                {
                    "sent": "Stable is a good thing, but you cannot have both.",
                    "label": 1
                },
                {
                    "sent": "If you have an algorithm which is sparse, then it cannot be stable.",
                    "label": 1
                },
                {
                    "sent": "A little I will make this more formal.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the plan from here is I will first illustrate this by proving for famous special algorithm.",
                    "label": 1
                },
                {
                    "sent": "I show that tell us so we know that illustrates as possible, which I will show that the lawsuit is not stable.",
                    "label": 1
                },
                {
                    "sent": "Not stable, meaning that the stability pyramid of Lasso does not goes down.",
                    "label": 1
                },
                {
                    "sent": "It is lower bounded by some constant which does not go start when the number of samples increases.",
                    "label": 1
                },
                {
                    "sent": "And inspired by this special case, we then talk about how to generalize to a general algorithm that is bus.",
                    "label": 0
                },
                {
                    "sent": "So I believe everyone here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No so Tesla, so this is the formulation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now the serum.",
                    "label": 0
                },
                {
                    "sent": "In the interview, it says that its stability perimetre of this lawsuit is lower bounded by a constant.",
                    "label": 1
                },
                {
                    "sent": "But what constant?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the definition of this constant.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Called the pseudo maximal error.",
                    "label": 1
                },
                {
                    "sent": "It means that if you have the freedom to choose any training sample and any testing sample you run your algorithms on this training sample and the test on your testing sample.",
                    "label": 0
                },
                {
                    "sent": "The maximum error that you can get recalled.",
                    "label": 0
                },
                {
                    "sent": "This pseudo maximal error.",
                    "label": 0
                },
                {
                    "sent": "So assume that so you know the linear algorithms and you want to screw the world who designed this linear algorithm by picking, so that's not ideal adoption, so that's not distribution alarm set, so you can arbitrary picking your training sample in the testing sample tried to.",
                    "label": 0
                },
                {
                    "sent": "Make this algorithm break, and this defines a number this week.",
                    "label": 0
                },
                {
                    "sent": "Order pseudo maximum error and will show that for Lasso.",
                    "label": 1
                },
                {
                    "sent": "The stability bond is lower bounded by this pseudo maximal error.",
                    "label": 1
                },
                {
                    "sent": "Of course, the student maximum error will not decrease when you're training.",
                    "label": 0
                },
                {
                    "sent": "Samples increase because you can as an adversary can always pick the worst training sample repeatedly.",
                    "label": 0
                },
                {
                    "sent": "Pick this, reverse the training sample and then pick a better testing sample.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So more precisely, this theorem says that we need to think of two regression problem at the same time.",
                    "label": 0
                },
                {
                    "sent": "The first problem thinks of a problem of M features.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second problem is a regression problem with two M features, and the theorem says that the stability bounds of the regression over 2M features is lower bounded by the pseudo maximum error of the problem with M features.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It is not infinite because your training sample set maybe a compact set.",
                    "label": 0
                },
                {
                    "sent": "Right, but if if you think of like Euclidean space then it can be Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "So let's see why this is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Threat, it's a basically one slide proof, and it's very intuitive, but before goes into a new technique detail.",
                    "label": 0
                },
                {
                    "sent": "The idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Uniform stability as I have, however, emphasized is a worst case analysis.",
                    "label": 0
                },
                {
                    "sent": "So to show that an algorithm has an uniform stability which is bounded away from zero, I only need to construct away with ample, as long as I can construct the wind training sample set, such as remove one point of this set, you will get a complete different solution and the testing on some testing sample.",
                    "label": 0
                },
                {
                    "sent": "The two solutions give dramatically different performance then the proof is complete.",
                    "label": 0
                },
                {
                    "sent": "I don't need to talk about like a general sets, only need to construct wampole and the proof is basically constructing this example.",
                    "label": 0
                },
                {
                    "sent": "So we first pick BA, Zeda and Zero so that they put together jointly achieve the pseudo maximal error.",
                    "label": 0
                },
                {
                    "sent": "And this is for the for the problem with M features.",
                    "label": 0
                },
                {
                    "sent": "Now I construct the training sample set for the problems.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of two M features, and this sexist optimal solution of this training sample set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the new sample set for the 2M features is constant like.",
                    "label": 0
                },
                {
                    "sent": "This way the first examples is the same as the original sample set for the for the M featured thing, but with M features, which is exactly the same as the previous one.",
                    "label": 0
                },
                {
                    "sent": "So I duplicate and features.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the first examples.",
                    "label": 0
                },
                {
                    "sent": "And then I add a when you sample.",
                    "label": 0
                },
                {
                    "sent": "The new samples construct.",
                    "label": 0
                },
                {
                    "sent": "This way I have M features which are completed zero and the next N features which is the same as the other stuff and it's labor is given by zero.",
                    "label": 0
                },
                {
                    "sent": "So now I have a training sample set with N + 1 sample.",
                    "label": 0
                },
                {
                    "sent": "So the next question I ask is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which solution is the optimal solution of this one, and my claim is X, star zero is an optimal solution with respect to this training sample set, while that if we only look at the first N samples.",
                    "label": 1
                },
                {
                    "sent": "We know that extra is the optimal solution for this part.",
                    "label": 0
                },
                {
                    "sent": "And zero doesn't play any role with respect to these features, right?",
                    "label": 0
                },
                {
                    "sent": "So X zero is the optimal solution for the first examples.",
                    "label": 0
                },
                {
                    "sent": "Also, for this sample, if you test X0 with this sample test, zero loss so X zero, this solution perfectly predicted this sample.",
                    "label": 0
                },
                {
                    "sent": "So of course it is also optimal to the last sample.",
                    "label": 0
                },
                {
                    "sent": "That's why X zero is an optimal solution with respect to the whole training set.",
                    "label": 0
                },
                {
                    "sent": "Right, I hope you will agree with me.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now remove the last sample.",
                    "label": 0
                },
                {
                    "sent": "I only have examples.",
                    "label": 0
                },
                {
                    "sent": "I remove the last sample and then retrain my lasso over this N sample set.",
                    "label": 1
                },
                {
                    "sent": "By symmetric, it's easy to see that zero X is also optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I have two complete different solutions with respect to N + 1 samples and the examples so my.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last step is just to show you that there exists some testing sample such that these two solutions give different performance and the solution is quite the test sample is quite easy as constructors just this one.",
                    "label": 0
                },
                {
                    "sent": "If you test X0 with respect to this, it gives you zero loss if you test the 0X with respect to this by the definition of here, you will get to the pseudo maximum error.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's why for less so.",
                    "label": 0
                },
                {
                    "sent": "By removing one point you have two solutions which are complete, different whose performance is different with at least the lower bounded by the pseudo maximum error and that proves the.",
                    "label": 0
                },
                {
                    "sent": "So as an algorithm is not stable, or at least it does not satisfy the uniform stability concept.",
                    "label": 0
                },
                {
                    "sent": "After we.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's discovered that and the most immediate question that you want to answer is, is this something special for lasso, or can we generalize it to most generalize the algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do that.",
                    "label": 0
                },
                {
                    "sent": "The basic band inverse engineering, meaning that we really look at the proof and see where we can relax the thing.",
                    "label": 0
                },
                {
                    "sent": "And also that's one question that that's one point that happened.",
                    "label": 0
                },
                {
                    "sent": "I have not addressed yet.",
                    "label": 0
                },
                {
                    "sent": "The title says the tradeoff between sparsity and the stability, but within the proof vest, nothing talk about the sparsity, right?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all space algorithms and more precisely, force positive mean that algorithm that can identify redundant features.",
                    "label": 0
                },
                {
                    "sent": "So if you give the algorithm to two columns of each.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which are completely the same.",
                    "label": 0
                },
                {
                    "sent": "Then one of the optimal solution at least should put zero on on this on one of the two columns.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is.",
                    "label": 0
                },
                {
                    "sent": "Then we also generalize the notion of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Instead of saying that the algorithms is an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, we just say that the algorithm just defines the relationship.",
                    "label": 0
                },
                {
                    "sent": "Given training sets, it defines all the relationship.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the candidates solutions.",
                    "label": 0
                },
                {
                    "sent": "Of course, if there will be space optimization, it falls into this cat.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great, and there's also some assumptions that we need to make.",
                    "label": 0
                },
                {
                    "sent": "The first assumption says that.",
                    "label": 0
                },
                {
                    "sent": "If one solution is better than the other solution with respect to some training set.",
                    "label": 0
                },
                {
                    "sent": "And these two solutions, the Jess coefficient of these two solutions are zero.",
                    "label": 0
                },
                {
                    "sent": "Then I don't care what is the value that I put in my training set for this coefficient for this feature.",
                    "label": 0
                },
                {
                    "sent": "So if I put a zero on certain coefficient then its respective features, I don't care the value.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second assumption says that.",
                    "label": 0
                },
                {
                    "sent": "If I have a solution which is better than the other one and add the new training sample which is perfectly predicted by the 2nd solution, then to should still be preferrable preferred over the first one.",
                    "label": 0
                },
                {
                    "sent": "The certain options.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said if I added some dummy features and I added, then the preference relationship should not change.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last one.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says that if I do some permutations over the features and over the solutions together, the preference relationship should still preserved.",
                    "label": 0
                },
                {
                    "sent": "So these are all I think there's a net.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Options, but given these assumptions, basically.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the same, using the same proof technique, you can show that any algorithms that identify redundant features and they satisfy these options.",
                    "label": 0
                },
                {
                    "sent": "Is not stable and the proof is the same is just to construct a training sample set which duplicate.",
                    "label": 0
                },
                {
                    "sent": "So you defined as the set defined original sets and you duplicate the features.",
                    "label": 0
                },
                {
                    "sent": "This give the first training sample set and add another new training sample which is the worst example that you can have.",
                    "label": 0
                },
                {
                    "sent": "And using all the assumptions, we can show that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this, further the N + 1 set X star zero is an optimal solution, while if you remove 1 sample zero X * is an optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Then just test these two solutions with respect to the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the same sample and that gives you the that gives you the pseudo maximum error as a lower bound of your stability.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Including talk, I want to see some limitation of this work.",
                    "label": 0
                },
                {
                    "sent": "So as as as one of the audience points out, uniform stability is a very restrictive concept in terms of stability.",
                    "label": 0
                },
                {
                    "sent": "In fact, this has been like a spectrum of different concepts of stability and uniform stability.",
                    "label": 0
                },
                {
                    "sent": "Is is the most restrictive one and it is of course very interesting to see.",
                    "label": 0
                },
                {
                    "sent": "Beyond the uniform stability, what other relaxed notion is also in conflict with sparsity?",
                    "label": 0
                },
                {
                    "sent": "And also what other properties to beyond the stability itself with other properties can be in conflict with sparsity's?",
                    "label": 0
                },
                {
                    "sent": "And also we're interested in like extended this framework from regression to other type of learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for attention.",
                    "label": 0
                }
            ]
        }
    }
}