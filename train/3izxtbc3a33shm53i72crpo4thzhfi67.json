{
    "id": "3izxtbc3a33shm53i72crpo4thzhfi67",
    "title": "Sustainable Linked Data generation: the case of DBpedia",
    "info": {
        "author": [
            "Wouter Maroy, Electronics and Information Systems Department, Ghent University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_maroy_DBpedia/",
    "segmentation": [
        [
            "So hi everyone, I'm a master student at Ghent University an today I want to talk about an improvement we have made on the approach of the pedia on extracting link data from Wikipedia."
        ],
        [
            "So DVD's of the most prominent datasets in the world of the semantic Web and link data and the pedia is heavily used for research is also used by companies.",
            "I'm pretty sure that there are people here in this room that have first hand experience with the data set."
        ],
        [
            "If you look at some statistics, we can see that the period describes dirty 8.8 million entities.",
            "If we translate this to triple."
        ],
        [
            "We have more than 3 billion triples, so if it has a very large data set so that it should have the quantity.",
            "But if you look at the."
        ],
        [
            "Quality.",
            "The picture keeps growing and together with Wikipedia, so its Wikipedia is growing every year.",
            "The video grows with it.",
            "We can see that there are still quality issues.",
            "So we can distill."
        ],
        [
            "Which two types we have issues at schema level and issues as data level.",
            "So with schema level I mean that the DP ontology that is applied as applied strongly to the raw data from Wikipedia and some cases.",
            "And with issues at data level, I mean that the data at Wikipedia itself can contain wrong information."
        ],
        [
            "So."
        ],
        [
            "The causes of these issues."
        ],
        [
            "If we look at the approach on how the PD extracts information from Wikipedia.",
            "We see that they use a large extraction framework that contains a lot of functionality to extract.",
            "Information from Wikipedia infoboxes.",
            "The commune"
        ],
        [
            "We decided to create mapping rules to create mapping documents to manually map for each domain.",
            "Parties do link data."
        ],
        [
            "For the issues."
        ],
        [
            "We can see that there are issues in that section framework itself.",
            "But"
        ],
        [
            "Also, in the mapping rules."
        ],
        [
            "Of course, in the Wikipedia data itself.",
            "We cannot directly change the information and Wikipedia automatically to improve the quality."
        ],
        [
            "So you can change the approach on how they generated in the extraction framework and also the accompanying mapping documents."
        ],
        [
            "So as a solution we integrate a generic, modular and sustainable mapping language, and integrating this mapping language by replacing it by replacing the old mapping rules by this new mapping language, we created a sustainable linked data."
        ],
        [
            "Generation framework.",
            "And this didn't stay as a prototype implementation, but the PEDIA also decided to make this."
        ],
        [
            "Which.",
            "Which means that in the near future users will be able to create mapping files, a normal update and upload them.",
            "So that the extraction framework of the pedia can effectively use them to generate their link data.",
            "The data set."
        ],
        [
            "So during this talk I'm going to talk about before the change what the situation was in extraction framework and also after the change how the situation was and how about the progress was an evaluation."
        ],
        [
            "So the limitations that we that we found in extraction framework were.",
            "Hard coded mapping rules.",
            "The fact that there are no machine interpretable mapping rules that we cannot use another ontology and that there is no schema validation on the mapping rules.",
            "So it starts with the first one hard coded."
        ],
        [
            "Rules the mapping rules generate triples so they define how the subject, predicate, and object of its triple is generated.",
            "But with the mapping route will only define."
        ],
        [
            "And how the predicate an object is generated?",
            "This is due to implementation coupling.",
            "And even then we so we have influence on the predicate and the object."
        ],
        [
            "But this is also limited because we are stuck with a fixed ontology.",
            "We can only select.",
            "Predicates out of the DP ontology and we can also not change how the data is transformed from Wikipedia.",
            "So all the values in Infoboxes and Wikipedia are in this specific format that the user decides on how it wants to insert the data in Wikipedia.",
            "But the extraction framework has like as a as a general way of parsing this data.",
            "So it's a general parsing method for all domains over whole Wikipedia.",
            "But the user, it's possible that the user wants to parse the data in a different way for this specific domain, and this cannot be declared in the mapping files."
        ],
        [
            "The next point is that the mapping rules are not machine interpretable."
        ],
        [
            "Mapping rules on wiki text format.",
            "It's the same format that Wikipedia uses to define their Wikipedia articles, so DB pedia reuse the syntax to define mapping rules an to extract data with this."
        ],
        [
            "But this format cannot be interpreted automatically.",
            "So with this I mean we."
        ],
        [
            "Cannot query this data.",
            "We cannot perform schema validation on this and we cannot reuse the mapping rules to affectively generate other mapping rules."
        ],
        [
            "Then I want to go to the dirt point we are restricted to the DBP anthology."
        ],
        [
            "The PEDIA uses one ontology.",
            "It's a very large ontology that is maintained by the DBPR community up until now.",
            "But what if you want to?"
        ],
        [
            "And it's coupled with it."
        ],
        [
            "Implementation as well.",
            "But"
        ],
        [
            "But if you want to change it as this ontology and effectively this is something that the PD's looking into they want to change their ontology.",
            "They want to make another data set with an ontology and limited ontology with of higher quality.",
            "But if you have a new ontology, that's fine.",
            "If we, if we design a new one, that's great.",
            "But what about the mappings?"
        ],
        [
            "The mappings only work for the ultimate aladji, so if you want to make a new.",
            "The pedia, with the new ontology.",
            "We also need a competing map accompanying mappings.",
            "But we have to, if you want to do that, we have to change all the mappings manually.",
            "If you know that there are 700 around 700 mappings for the English Wikipedia, then there are 45 languages that are mapped in the PDL.",
            "They all need to be edited."
        ],
        [
            "Then the last point, no schema validation or mapping rules."
        ],
        [
            "So the previous quality issues and by using a post process they succeed and cleansing the data so they can avoid mistakes in our data set.",
            "But this requires a lot of resources.",
            "This is a large data set more than 3 billion triples, but there are also other options.",
            "By validating rules or other options so we can avoid validating the data set afterwards.",
            "And I want to explain this fact a bit by giving some context.",
            "So here we."
        ],
        [
            "See and infobox template for defying persons on Wikipedia.",
            "So we have you can fill in the name the birth name and birthdate and so on.",
            "So we can define several infoboxes.",
            "So we have your Bill Gates Javarris total Albert Einstein."
        ],
        [
            "Marie Curie so there are more than 250 three 1000 pages who effectively use the personnel box template."
        ],
        [
            "But only one mapping is responsible for this extraction."
        ],
        [
            "So this means if we map the name to Deb you name it wants to change this this will.",
            "This will cause at least 250,000 changes in the data set."
        ],
        [
            "But also for properties that are Maps wrongly.",
            "This will cost you 250,000 errors in the data set.",
            "So a better approach would be to validate the mappings up front so the validate mapping files before the extraction so we can avoid.",
            "Sending this data afterwards."
        ],
        [
            "So validate mappings would be more feasible, more efficient and sustainable.",
            "So.",
            "Let's validate the mapping."
        ],
        [
            "But currently there is no validation for current mapping rules and it's also hard we need to make a custom again a custom framework for this too.",
            "To do this validation."
        ],
        [
            "So these were this was kind of the situation where the extraction framework was in.",
            "Now we did some changes and I'm going to explain."
        ],
        [
            "What the situation is now?",
            "So we propose a sustainable framework.",
            "That provides declarative mapping rules.",
            "Machine interpretable format.",
            "A framework that has schema validation at mapping level.",
            "And use a framework that is ontology independent."
        ],
        [
            "And in our solution we chose for the RDF mapping language.",
            "It isn't a mapping language to map heterogeneous data sources from different kind of format to the RDF model, so we have XML and Jason before and other formats, and we extended it to also support Wikipedia infoboxes."
        ],
        [
            "Our model is also a mapping language that is defined in RDF, so armilar triples so we can exploit this fact and I will explain this later on in the talk."
        ],
        [
            "So I'll start with the declarative mapping rules."
        ],
        [
            "We coupled the implementation coupling with mapping rules and now map."
        ],
        [
            "Losing our milk and define the subject."
        ],
        [
            "We can also."
        ],
        [
            "Declare other ontologies in the mapping files."
        ],
        [
            "We can declare data transformations, so there was.",
            "There has been some work done last year on this topic.",
            "Bibenda Mr."
        ],
        [
            "And the next point machine interpretable format are."
        ],
        [
            "Well, isn't RDF.",
            "RDF is a machine interpretable format."
        ],
        [
            "Which means that we now can query the mapping files if we can query the mapping files.",
            "We can for example search for all mappings in this language that contain a mapping to this property.",
            "And analyze the mappings.",
            "We can perform scheme schema validations on the mapping files as well and also apply rules to mapping files so that we can generate other mapping rules automatically."
        ],
        [
            "Third Point is schema validation."
        ],
        [
            "So now that the mapping files are in our in our mall.",
            "We opted for a tool or DF units too.",
            "Automatically to automatically validate all our Mail mapping files are there if unit is a is a framework that uses constraints to test your RDF.",
            "If your audience, if you RDF is valid, so the purposes of this is that users cannot upload mappings to the repository that are not valid, so we can avoid having around this mapping files at repository in the beginning because at this moment, but you hold mapping files if a mapping file is wrong.",
            "OK, we generate the data with this mapping file, but afterwards we clean up the data, but the mapping file is not adjusted automatically.",
            "So with this approach we enable the automatic changing of the automatic correcting of the mapping files."
        ],
        [
            "The last point is the usage of other ontologies.",
            "So like I mentioned before."
        ],
        [
            "We were stuck with one ontology and if you want to go to another ontology then we need also need the new mapping files."
        ],
        [
            "But now that we know that in all the African create rules, we can create methods for automatically generating the new mapping files for the new ontology."
        ],
        [
            "So I'm going to mention a bit of the progress we have made and evaluation."
        ],
        [
            "So switching to RML required analyzing all the DB pedia mappings that existed in all languages and automatically changing them to their RML version.",
            "And then we."
        ],
        [
            "Had to integrate an RML processor inside the exception framework, so the extraction frame quits, extracts Wikipedia data dumps by using our Mail mapping files and extracting and generating link data in the end."
        ],
        [
            "So as an evaluation we did a complete extraction on the English Wikipedia dump with 90.",
            "98% coverage in comparison with the original datasets.",
            "So it was effectively possible to use to read you sarabel mappings to to generate the PDF data set."
        ],
        [
            "Then on performance, the framework offers more sustainable mapping rules, but it came at a performance cost of 25%.",
            "At this moment it's already a bit less, but we are still busy with optimizing this process so we can get.",
            "Near the original performance of the extraction framework.",
            "As the last."
        ],
        [
            "Nation we want to test the flexibility.",
            "So mapping rules are in RDF, so they can automatically be updated like I mentioned.",
            "And other datasets can also be generated from Wikipedia because of the ontology independency so."
        ],
        [
            "We want to try this out and we created mapping mapping files for.",
            "All persons on Wikipedia.",
            "And we created data set of all processes Wikipedia by using the schema.org vocabulary.",
            "The only thing that you had to change was the mapping file.",
            "We didn't need to change anything in the implementation, we just give the specific mapping file to the Executor framework, let it run over the English Wikipedia data dump and we were done.",
            "It was the only thing that was necessary.",
            "So."
        ],
        [
            "To give review.",
            "The opinion now has a stable framework that has declarative mapping rules and machine and mapping rules that have a machine and portable interpretable format.",
            "Wrapping rules that have schema validation.",
            "A mapping rules that also allow the DPX frame to use other ontologies."
        ],
        [
            "So the future is bright and.",
            "Now I'm really happy to say that.",
            "Sorry, so I'm really happy to see how we could make this possible for DB pedia and not only for the pedia but also for the community.",
            "Also very looking forward to new work that can be done on top of this.",
            "Really looking forward to that and I want to thank."
        ],
        [
            "The Pedia community, as well, for the collaboration that we could do over the last over the past year.",
            "So thank you and thank you for listening.",
            "Think about the number to how many RML I mean is RMS happening between entities and the one the vocabulary is wanted right?",
            "Could you repeat your question?",
            "Please do use RML language to map between entities and don't always right.",
            "Yes, indeed, it's OK. How many?",
            "How many miles you have to produce to cover 98% of Wikipedia?",
            "The mapping files.",
            "This is the same thing files developing files is the same amount of mapping files as the original.",
            "That the picture has originally, did you have an access to the database of Wikipedia or you just scroll the data or descript?",
            "Maybe?",
            "I don't understand very well, but.",
            "The map, what do you mean by mapping between Wikipedia, mapping to the ontology, right?",
            "Yes, so every every infobox Wikipedia so info box that describes an article like a page person for example.",
            "So it has for example name, birthdate and so on and all these properties.",
            "Are mapped to the ontology of the Pedia and a mapping files declare how these properties need to be mapped.",
            "So for every kind of downplayed.",
            "So, for example, for a person, we have one mapping for all persons.",
            "And our mapping is doing the same as the old mapping files, but more in a declarative way.",
            "It's a wiki text format, so it was a custom format that the media used.",
            "It was.",
            "It's a format that is used by Wikipedia to define the articles.",
            "And the pediatri used this format to define their mapping files.",
            "Any other question please yeah.",
            "So yeah, 3838.8 million entities.",
            "No, it's it's.",
            "These are different entities, so we have 38.8 million entities.",
            "Yes, sorry, I didn't understand your question.",
            "OK, thanks for the nice presentation.",
            "Do you have an idea if it's easier or more difficult to write the rules that mapping rules with the new language?",
            "Then with the older write them you mean?",
            "Is it more difficult to write rules in RML so then to rattle manually is more difficult?",
            "Yes, and we try to cope that Cooper that by implementing user interface.",
            "So like.",
            "Last summer it was a Google Summer of Code project.",
            "On this 28 the editing of RML so there is a new mapping Y in development and this UI will help the user to create the mappings so they don't have to do it manually.",
            "It's done by a user interface.",
            "Any other question?",
            "OK, so maybe just once, so in the in the paper you mentioned that OK, you you generate a bit a bit less triples than the than the other one, right?",
            "So there are some unsustainable your rights and things like that.",
            "So what's the status of this triples that are not generated now, so are just they going to disappear if there is any process of cleaning them in some way or making them sustainable in some way?",
            "Is there any crosses their previous thinking about or is just dropped it then at that point it was dropped but there has not been a discussion on whether they should be better.",
            "They should be in a data set or not, 'cause these are.",
            "This this triples were not used as well and they were not that much of an importance to contain them, so there has not been a discussion at either to decide if they should be there or not there OK.",
            "Thank you very much.",
            "If there is not any other question then then we thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hi everyone, I'm a master student at Ghent University an today I want to talk about an improvement we have made on the approach of the pedia on extracting link data from Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So DVD's of the most prominent datasets in the world of the semantic Web and link data and the pedia is heavily used for research is also used by companies.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure that there are people here in this room that have first hand experience with the data set.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at some statistics, we can see that the period describes dirty 8.8 million entities.",
                    "label": 0
                },
                {
                    "sent": "If we translate this to triple.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have more than 3 billion triples, so if it has a very large data set so that it should have the quantity.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quality.",
                    "label": 0
                },
                {
                    "sent": "The picture keeps growing and together with Wikipedia, so its Wikipedia is growing every year.",
                    "label": 0
                },
                {
                    "sent": "The video grows with it.",
                    "label": 0
                },
                {
                    "sent": "We can see that there are still quality issues.",
                    "label": 1
                },
                {
                    "sent": "So we can distill.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which two types we have issues at schema level and issues as data level.",
                    "label": 0
                },
                {
                    "sent": "So with schema level I mean that the DP ontology that is applied as applied strongly to the raw data from Wikipedia and some cases.",
                    "label": 0
                },
                {
                    "sent": "And with issues at data level, I mean that the data at Wikipedia itself can contain wrong information.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The causes of these issues.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we look at the approach on how the PD extracts information from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "We see that they use a large extraction framework that contains a lot of functionality to extract.",
                    "label": 1
                },
                {
                    "sent": "Information from Wikipedia infoboxes.",
                    "label": 0
                },
                {
                    "sent": "The commune",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We decided to create mapping rules to create mapping documents to manually map for each domain.",
                    "label": 0
                },
                {
                    "sent": "Parties do link data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the issues.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can see that there are issues in that section framework itself.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, in the mapping rules.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, in the Wikipedia data itself.",
                    "label": 0
                },
                {
                    "sent": "We cannot directly change the information and Wikipedia automatically to improve the quality.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can change the approach on how they generated in the extraction framework and also the accompanying mapping documents.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a solution we integrate a generic, modular and sustainable mapping language, and integrating this mapping language by replacing it by replacing the old mapping rules by this new mapping language, we created a sustainable linked data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generation framework.",
                    "label": 0
                },
                {
                    "sent": "And this didn't stay as a prototype implementation, but the PEDIA also decided to make this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Which means that in the near future users will be able to create mapping files, a normal update and upload them.",
                    "label": 0
                },
                {
                    "sent": "So that the extraction framework of the pedia can effectively use them to generate their link data.",
                    "label": 0
                },
                {
                    "sent": "The data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So during this talk I'm going to talk about before the change what the situation was in extraction framework and also after the change how the situation was and how about the progress was an evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the limitations that we that we found in extraction framework were.",
                    "label": 0
                },
                {
                    "sent": "Hard coded mapping rules.",
                    "label": 0
                },
                {
                    "sent": "The fact that there are no machine interpretable mapping rules that we cannot use another ontology and that there is no schema validation on the mapping rules.",
                    "label": 1
                },
                {
                    "sent": "So it starts with the first one hard coded.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rules the mapping rules generate triples so they define how the subject, predicate, and object of its triple is generated.",
                    "label": 0
                },
                {
                    "sent": "But with the mapping route will only define.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how the predicate an object is generated?",
                    "label": 0
                },
                {
                    "sent": "This is due to implementation coupling.",
                    "label": 1
                },
                {
                    "sent": "And even then we so we have influence on the predicate and the object.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this is also limited because we are stuck with a fixed ontology.",
                    "label": 1
                },
                {
                    "sent": "We can only select.",
                    "label": 0
                },
                {
                    "sent": "Predicates out of the DP ontology and we can also not change how the data is transformed from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So all the values in Infoboxes and Wikipedia are in this specific format that the user decides on how it wants to insert the data in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But the extraction framework has like as a as a general way of parsing this data.",
                    "label": 0
                },
                {
                    "sent": "So it's a general parsing method for all domains over whole Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But the user, it's possible that the user wants to parse the data in a different way for this specific domain, and this cannot be declared in the mapping files.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next point is that the mapping rules are not machine interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mapping rules on wiki text format.",
                    "label": 0
                },
                {
                    "sent": "It's the same format that Wikipedia uses to define their Wikipedia articles, so DB pedia reuse the syntax to define mapping rules an to extract data with this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this format cannot be interpreted automatically.",
                    "label": 0
                },
                {
                    "sent": "So with this I mean we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cannot query this data.",
                    "label": 0
                },
                {
                    "sent": "We cannot perform schema validation on this and we cannot reuse the mapping rules to affectively generate other mapping rules.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I want to go to the dirt point we are restricted to the DBP anthology.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The PEDIA uses one ontology.",
                    "label": 0
                },
                {
                    "sent": "It's a very large ontology that is maintained by the DBPR community up until now.",
                    "label": 0
                },
                {
                    "sent": "But what if you want to?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's coupled with it.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Implementation as well.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you want to change it as this ontology and effectively this is something that the PD's looking into they want to change their ontology.",
                    "label": 0
                },
                {
                    "sent": "They want to make another data set with an ontology and limited ontology with of higher quality.",
                    "label": 0
                },
                {
                    "sent": "But if you have a new ontology, that's fine.",
                    "label": 0
                },
                {
                    "sent": "If we, if we design a new one, that's great.",
                    "label": 0
                },
                {
                    "sent": "But what about the mappings?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The mappings only work for the ultimate aladji, so if you want to make a new.",
                    "label": 0
                },
                {
                    "sent": "The pedia, with the new ontology.",
                    "label": 0
                },
                {
                    "sent": "We also need a competing map accompanying mappings.",
                    "label": 0
                },
                {
                    "sent": "But we have to, if you want to do that, we have to change all the mappings manually.",
                    "label": 0
                },
                {
                    "sent": "If you know that there are 700 around 700 mappings for the English Wikipedia, then there are 45 languages that are mapped in the PDL.",
                    "label": 0
                },
                {
                    "sent": "They all need to be edited.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the last point, no schema validation or mapping rules.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the previous quality issues and by using a post process they succeed and cleansing the data so they can avoid mistakes in our data set.",
                    "label": 0
                },
                {
                    "sent": "But this requires a lot of resources.",
                    "label": 0
                },
                {
                    "sent": "This is a large data set more than 3 billion triples, but there are also other options.",
                    "label": 1
                },
                {
                    "sent": "By validating rules or other options so we can avoid validating the data set afterwards.",
                    "label": 0
                },
                {
                    "sent": "And I want to explain this fact a bit by giving some context.",
                    "label": 0
                },
                {
                    "sent": "So here we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See and infobox template for defying persons on Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "So we have you can fill in the name the birth name and birthdate and so on.",
                    "label": 0
                },
                {
                    "sent": "So we can define several infoboxes.",
                    "label": 0
                },
                {
                    "sent": "So we have your Bill Gates Javarris total Albert Einstein.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marie Curie so there are more than 250 three 1000 pages who effectively use the personnel box template.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But only one mapping is responsible for this extraction.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this means if we map the name to Deb you name it wants to change this this will.",
                    "label": 0
                },
                {
                    "sent": "This will cause at least 250,000 changes in the data set.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But also for properties that are Maps wrongly.",
                    "label": 0
                },
                {
                    "sent": "This will cost you 250,000 errors in the data set.",
                    "label": 0
                },
                {
                    "sent": "So a better approach would be to validate the mappings up front so the validate mapping files before the extraction so we can avoid.",
                    "label": 0
                },
                {
                    "sent": "Sending this data afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So validate mappings would be more feasible, more efficient and sustainable.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's validate the mapping.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But currently there is no validation for current mapping rules and it's also hard we need to make a custom again a custom framework for this too.",
                    "label": 0
                },
                {
                    "sent": "To do this validation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these were this was kind of the situation where the extraction framework was in.",
                    "label": 0
                },
                {
                    "sent": "Now we did some changes and I'm going to explain.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What the situation is now?",
                    "label": 0
                },
                {
                    "sent": "So we propose a sustainable framework.",
                    "label": 0
                },
                {
                    "sent": "That provides declarative mapping rules.",
                    "label": 1
                },
                {
                    "sent": "Machine interpretable format.",
                    "label": 0
                },
                {
                    "sent": "A framework that has schema validation at mapping level.",
                    "label": 0
                },
                {
                    "sent": "And use a framework that is ontology independent.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in our solution we chose for the RDF mapping language.",
                    "label": 0
                },
                {
                    "sent": "It isn't a mapping language to map heterogeneous data sources from different kind of format to the RDF model, so we have XML and Jason before and other formats, and we extended it to also support Wikipedia infoboxes.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model is also a mapping language that is defined in RDF, so armilar triples so we can exploit this fact and I will explain this later on in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll start with the declarative mapping rules.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We coupled the implementation coupling with mapping rules and now map.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Losing our milk and define the subject.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Declare other ontologies in the mapping files.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can declare data transformations, so there was.",
                    "label": 0
                },
                {
                    "sent": "There has been some work done last year on this topic.",
                    "label": 0
                },
                {
                    "sent": "Bibenda Mr.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the next point machine interpretable format are.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, isn't RDF.",
                    "label": 0
                },
                {
                    "sent": "RDF is a machine interpretable format.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which means that we now can query the mapping files if we can query the mapping files.",
                    "label": 0
                },
                {
                    "sent": "We can for example search for all mappings in this language that contain a mapping to this property.",
                    "label": 0
                },
                {
                    "sent": "And analyze the mappings.",
                    "label": 0
                },
                {
                    "sent": "We can perform scheme schema validations on the mapping files as well and also apply rules to mapping files so that we can generate other mapping rules automatically.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Third Point is schema validation.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now that the mapping files are in our in our mall.",
                    "label": 0
                },
                {
                    "sent": "We opted for a tool or DF units too.",
                    "label": 0
                },
                {
                    "sent": "Automatically to automatically validate all our Mail mapping files are there if unit is a is a framework that uses constraints to test your RDF.",
                    "label": 0
                },
                {
                    "sent": "If your audience, if you RDF is valid, so the purposes of this is that users cannot upload mappings to the repository that are not valid, so we can avoid having around this mapping files at repository in the beginning because at this moment, but you hold mapping files if a mapping file is wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, we generate the data with this mapping file, but afterwards we clean up the data, but the mapping file is not adjusted automatically.",
                    "label": 0
                },
                {
                    "sent": "So with this approach we enable the automatic changing of the automatic correcting of the mapping files.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last point is the usage of other ontologies.",
                    "label": 0
                },
                {
                    "sent": "So like I mentioned before.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were stuck with one ontology and if you want to go to another ontology then we need also need the new mapping files.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now that we know that in all the African create rules, we can create methods for automatically generating the new mapping files for the new ontology.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to mention a bit of the progress we have made and evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So switching to RML required analyzing all the DB pedia mappings that existed in all languages and automatically changing them to their RML version.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Had to integrate an RML processor inside the exception framework, so the extraction frame quits, extracts Wikipedia data dumps by using our Mail mapping files and extracting and generating link data in the end.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as an evaluation we did a complete extraction on the English Wikipedia dump with 90.",
                    "label": 1
                },
                {
                    "sent": "98% coverage in comparison with the original datasets.",
                    "label": 1
                },
                {
                    "sent": "So it was effectively possible to use to read you sarabel mappings to to generate the PDF data set.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then on performance, the framework offers more sustainable mapping rules, but it came at a performance cost of 25%.",
                    "label": 1
                },
                {
                    "sent": "At this moment it's already a bit less, but we are still busy with optimizing this process so we can get.",
                    "label": 0
                },
                {
                    "sent": "Near the original performance of the extraction framework.",
                    "label": 0
                },
                {
                    "sent": "As the last.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation we want to test the flexibility.",
                    "label": 0
                },
                {
                    "sent": "So mapping rules are in RDF, so they can automatically be updated like I mentioned.",
                    "label": 0
                },
                {
                    "sent": "And other datasets can also be generated from Wikipedia because of the ontology independency so.",
                    "label": 1
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to try this out and we created mapping mapping files for.",
                    "label": 0
                },
                {
                    "sent": "All persons on Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And we created data set of all processes Wikipedia by using the schema.org vocabulary.",
                    "label": 1
                },
                {
                    "sent": "The only thing that you had to change was the mapping file.",
                    "label": 0
                },
                {
                    "sent": "We didn't need to change anything in the implementation, we just give the specific mapping file to the Executor framework, let it run over the English Wikipedia data dump and we were done.",
                    "label": 0
                },
                {
                    "sent": "It was the only thing that was necessary.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To give review.",
                    "label": 0
                },
                {
                    "sent": "The opinion now has a stable framework that has declarative mapping rules and machine and mapping rules that have a machine and portable interpretable format.",
                    "label": 1
                },
                {
                    "sent": "Wrapping rules that have schema validation.",
                    "label": 0
                },
                {
                    "sent": "A mapping rules that also allow the DPX frame to use other ontologies.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the future is bright and.",
                    "label": 0
                },
                {
                    "sent": "Now I'm really happy to say that.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so I'm really happy to see how we could make this possible for DB pedia and not only for the pedia but also for the community.",
                    "label": 0
                },
                {
                    "sent": "Also very looking forward to new work that can be done on top of this.",
                    "label": 0
                },
                {
                    "sent": "Really looking forward to that and I want to thank.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Pedia community, as well, for the collaboration that we could do over the last over the past year.",
                    "label": 0
                },
                {
                    "sent": "So thank you and thank you for listening.",
                    "label": 0
                },
                {
                    "sent": "Think about the number to how many RML I mean is RMS happening between entities and the one the vocabulary is wanted right?",
                    "label": 0
                },
                {
                    "sent": "Could you repeat your question?",
                    "label": 0
                },
                {
                    "sent": "Please do use RML language to map between entities and don't always right.",
                    "label": 0
                },
                {
                    "sent": "Yes, indeed, it's OK. How many?",
                    "label": 0
                },
                {
                    "sent": "How many miles you have to produce to cover 98% of Wikipedia?",
                    "label": 0
                },
                {
                    "sent": "The mapping files.",
                    "label": 0
                },
                {
                    "sent": "This is the same thing files developing files is the same amount of mapping files as the original.",
                    "label": 0
                },
                {
                    "sent": "That the picture has originally, did you have an access to the database of Wikipedia or you just scroll the data or descript?",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "I don't understand very well, but.",
                    "label": 0
                },
                {
                    "sent": "The map, what do you mean by mapping between Wikipedia, mapping to the ontology, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so every every infobox Wikipedia so info box that describes an article like a page person for example.",
                    "label": 0
                },
                {
                    "sent": "So it has for example name, birthdate and so on and all these properties.",
                    "label": 0
                },
                {
                    "sent": "Are mapped to the ontology of the Pedia and a mapping files declare how these properties need to be mapped.",
                    "label": 0
                },
                {
                    "sent": "So for every kind of downplayed.",
                    "label": 0
                },
                {
                    "sent": "So, for example, for a person, we have one mapping for all persons.",
                    "label": 0
                },
                {
                    "sent": "And our mapping is doing the same as the old mapping files, but more in a declarative way.",
                    "label": 0
                },
                {
                    "sent": "It's a wiki text format, so it was a custom format that the media used.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                },
                {
                    "sent": "It's a format that is used by Wikipedia to define the articles.",
                    "label": 0
                },
                {
                    "sent": "And the pediatri used this format to define their mapping files.",
                    "label": 0
                },
                {
                    "sent": "Any other question please yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, 3838.8 million entities.",
                    "label": 0
                },
                {
                    "sent": "No, it's it's.",
                    "label": 0
                },
                {
                    "sent": "These are different entities, so we have 38.8 million entities.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry, I didn't understand your question.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for the nice presentation.",
                    "label": 0
                },
                {
                    "sent": "Do you have an idea if it's easier or more difficult to write the rules that mapping rules with the new language?",
                    "label": 0
                },
                {
                    "sent": "Then with the older write them you mean?",
                    "label": 0
                },
                {
                    "sent": "Is it more difficult to write rules in RML so then to rattle manually is more difficult?",
                    "label": 0
                },
                {
                    "sent": "Yes, and we try to cope that Cooper that by implementing user interface.",
                    "label": 0
                },
                {
                    "sent": "So like.",
                    "label": 0
                },
                {
                    "sent": "Last summer it was a Google Summer of Code project.",
                    "label": 0
                },
                {
                    "sent": "On this 28 the editing of RML so there is a new mapping Y in development and this UI will help the user to create the mappings so they don't have to do it manually.",
                    "label": 0
                },
                {
                    "sent": "It's done by a user interface.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe just once, so in the in the paper you mentioned that OK, you you generate a bit a bit less triples than the than the other one, right?",
                    "label": 0
                },
                {
                    "sent": "So there are some unsustainable your rights and things like that.",
                    "label": 0
                },
                {
                    "sent": "So what's the status of this triples that are not generated now, so are just they going to disappear if there is any process of cleaning them in some way or making them sustainable in some way?",
                    "label": 0
                },
                {
                    "sent": "Is there any crosses their previous thinking about or is just dropped it then at that point it was dropped but there has not been a discussion on whether they should be better.",
                    "label": 0
                },
                {
                    "sent": "They should be in a data set or not, 'cause these are.",
                    "label": 0
                },
                {
                    "sent": "This this triples were not used as well and they were not that much of an importance to contain them, so there has not been a discussion at either to decide if they should be there or not there OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "If there is not any other question then then we thank you again.",
                    "label": 0
                }
            ]
        }
    }
}