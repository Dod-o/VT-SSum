{
    "id": "gkhihjntpgz2ovwo7fjvyjlvhctwhmnm",
    "title": "Active Classification based on Value of Classifier",
    "info": {
        "author": [
            "Tianshi Gao, Department of Electrical Engineering, Stanford University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Ensemble Methods"
        ]
    },
    "url": "http://videolectures.net/nips2011_gao_classifier/",
    "segmentation": [
        [
            "So this worker is all about the classification problem, where a good classification model should have both high accuracy and low test time.",
            "So for the accuracy it can often be greatly improved by using multiple types of features combining different kernels or building a sample of classifiers.",
            "While these measures provide good statistical power, they often also occur incur expensive computational cost.",
            "So the motivating question we ask is that can we enjoy the statistical gain of using multiple.",
            "Features kernels or classifiers at a relatively small computational cost."
        ],
        [
            "So let's consider a image classification problem.",
            "We want to predict a semantic label for each test image.",
            "Here we are given a sample of four classifiers, each of which is built on some different features.",
            "For example, we can compute frequency response or extract local descriptors, or compute some region based features on top of each feature.",
            "We can also train different types of classifiers focusing on different subsets of classes or different input distribution over the instances.",
            "In addition, when we apply a classifier, it also associates with two types of computational costs.",
            "The first one is to extract the feature that the classifier is built on.",
            "The second is to evaluate the function value of the classifier after the feature has been computed at that estate, instead of applying all the classifiers to every test instance, we propose an active classification process to intelligently select an instance specific subset of classifiers.",
            "So the basic idea here is that we consider the classification inference task as a sensing problem, where each classifier is viewed as a potential observation that might inform our classification process.",
            "This is a dynamic process where observations are selected sequentially based on previous ones, and we can also make dynamic decision about when to stop the process.",
            "The selection is based on the value of classifier, which is basically a myopic value theoretic computation that balances our estimate of the expected classification gain.",
            "Of the classifier and its computational cost given what has already been observed."
        ],
        [
            "So for example, starting from some uniform belief over the class variable, we want to compute the value for each potential observation for the first observation is classified string to separate groups of classes, so it has the highest information gain.",
            "It's also built on some cheap features, so the computation cost is low, resulting in the highest value.",
            "So we're going to compute it, apply the first observation 1st, and update the posterior over the cost variable.",
            "In the next iteration, based on the 1st observation, we want to update the value for the remaining observations.",
            "In this case, the second observation has the same share, the same feature representation as the first one, meaning that we don't have to compute the feature again resulting in very low computational cost and high value.",
            "So we're going to apply it in this run, and we're going to update the posterior of the class variable again.",
            "We continue this process until we are sufficiently confident.",
            "About our prediction."
        ],
        [
            "No, I'd like to show you some results on the 15 thing image classification task where we consider hundreds of types of features.",
            "Paired to using all the features in a static way or active classification can achieve 26 times faster at the same accuracy.",
            "In addition, we can also achieve the highest accuracy.",
            "If you want to know more about how we achieve this, please stop by our poster W. 33.",
            "I hope to see you there.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this worker is all about the classification problem, where a good classification model should have both high accuracy and low test time.",
                    "label": 0
                },
                {
                    "sent": "So for the accuracy it can often be greatly improved by using multiple types of features combining different kernels or building a sample of classifiers.",
                    "label": 0
                },
                {
                    "sent": "While these measures provide good statistical power, they often also occur incur expensive computational cost.",
                    "label": 1
                },
                {
                    "sent": "So the motivating question we ask is that can we enjoy the statistical gain of using multiple.",
                    "label": 1
                },
                {
                    "sent": "Features kernels or classifiers at a relatively small computational cost.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's consider a image classification problem.",
                    "label": 0
                },
                {
                    "sent": "We want to predict a semantic label for each test image.",
                    "label": 0
                },
                {
                    "sent": "Here we are given a sample of four classifiers, each of which is built on some different features.",
                    "label": 0
                },
                {
                    "sent": "For example, we can compute frequency response or extract local descriptors, or compute some region based features on top of each feature.",
                    "label": 0
                },
                {
                    "sent": "We can also train different types of classifiers focusing on different subsets of classes or different input distribution over the instances.",
                    "label": 0
                },
                {
                    "sent": "In addition, when we apply a classifier, it also associates with two types of computational costs.",
                    "label": 0
                },
                {
                    "sent": "The first one is to extract the feature that the classifier is built on.",
                    "label": 0
                },
                {
                    "sent": "The second is to evaluate the function value of the classifier after the feature has been computed at that estate, instead of applying all the classifiers to every test instance, we propose an active classification process to intelligently select an instance specific subset of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea here is that we consider the classification inference task as a sensing problem, where each classifier is viewed as a potential observation that might inform our classification process.",
                    "label": 1
                },
                {
                    "sent": "This is a dynamic process where observations are selected sequentially based on previous ones, and we can also make dynamic decision about when to stop the process.",
                    "label": 1
                },
                {
                    "sent": "The selection is based on the value of classifier, which is basically a myopic value theoretic computation that balances our estimate of the expected classification gain.",
                    "label": 0
                },
                {
                    "sent": "Of the classifier and its computational cost given what has already been observed.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, starting from some uniform belief over the class variable, we want to compute the value for each potential observation for the first observation is classified string to separate groups of classes, so it has the highest information gain.",
                    "label": 0
                },
                {
                    "sent": "It's also built on some cheap features, so the computation cost is low, resulting in the highest value.",
                    "label": 0
                },
                {
                    "sent": "So we're going to compute it, apply the first observation 1st, and update the posterior over the cost variable.",
                    "label": 0
                },
                {
                    "sent": "In the next iteration, based on the 1st observation, we want to update the value for the remaining observations.",
                    "label": 0
                },
                {
                    "sent": "In this case, the second observation has the same share, the same feature representation as the first one, meaning that we don't have to compute the feature again resulting in very low computational cost and high value.",
                    "label": 0
                },
                {
                    "sent": "So we're going to apply it in this run, and we're going to update the posterior of the class variable again.",
                    "label": 0
                },
                {
                    "sent": "We continue this process until we are sufficiently confident.",
                    "label": 0
                },
                {
                    "sent": "About our prediction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, I'd like to show you some results on the 15 thing image classification task where we consider hundreds of types of features.",
                    "label": 0
                },
                {
                    "sent": "Paired to using all the features in a static way or active classification can achieve 26 times faster at the same accuracy.",
                    "label": 0
                },
                {
                    "sent": "In addition, we can also achieve the highest accuracy.",
                    "label": 1
                },
                {
                    "sent": "If you want to know more about how we achieve this, please stop by our poster W. 33.",
                    "label": 0
                },
                {
                    "sent": "I hope to see you there.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}