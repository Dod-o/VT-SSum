{
    "id": "36ssg4ulxfm423bgcazeji262mcevtro",
    "title": "Non-Redundant Subgroup Discovery Using a Closure System",
    "info": {
        "author": [
            "Mario Boley, Cluster of Excellence Multimodal Computing and Interaction, Saarland University"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_boley_nrsducs/",
    "segmentation": [
        [
            "Hello, welcome to the rest of the session.",
            "Can you hear me in the back but we have to use a mic, thanks.",
            "My name is Mario Bola and I'm presenting joint work with my colleague Kendrick, who gave the talk.",
            "Before this talk.",
            "We're both from for Nova.",
            "Yes, and.",
            "In Germany, the.",
            "Topic is called nonviolent subgroup discovery.",
            "Using your closure system.",
            "And yeah, this title image here for me nicely reflects on intuitive level of that we.",
            "Wants prefer information in the most concise and simple forms and somehow unsatisfied if it's necessary.",
            "Blown up.",
            "And yeah, exactly this idea.",
            "We want to bring to subgroup discovery."
        ],
        [
            "And yeah, the connection will follow in a minute.",
            "First of all, let me briefly recap what's up with Discovery is because we had already talked about that we don't need every detail we again back in the simple scenario of.",
            "Two value attribute attribute and the standard constraint language of attribute value equality constraints.",
            "So we do not use the sophisticated interval constraints.",
            "And yeah, this.",
            "This subgroup descriptions, based on our consisting on conjunctions of attribute value constraints, describe subpopulations of the data and usually in the standard task we are interested in listing all interesting subgroup descriptions and what the exact form of the specific form of interestingness is.",
            "Does not matter to us in this talk, so we just consider the interestingness encapsulated by the quality function Q, and the only assumption we have four Q is.",
            "That it's a function of the subgroup descriptions extension.",
            "So as usually the quality function has a combination of the subgroups, generality with its distributional unusualness we see that for both of these components, that's a very natural requirement that they are functions of the extension.",
            "So function of the extension means that whenever two subgroup descriptions have the same extension on the data, that the quality will be similar.",
            "Identical.",
            "Now what then might happen is that."
        ],
        [
            "We can have many, many different descriptions differently looking descriptions for actually the same.",
            "Extension is the data for the same subgroup.",
            "Now with our assumptions, this means that for every equivalent subgroup we will have the same quality and this leads to unwanted effects.",
            "The first is that there is there can be many unnecessary evaluations of the quality function throughout the search process, cause actually it would have been enough if we know the quality of 1 equivalent subgroup, because the others will be similar.",
            "And yeah, often evaluating the quality function is a time consuming task because you have to scan through the data and in addition it will introduce redundancy in the output.",
            "The idea of this work is now too.",
            "Instead of considering individual subscriptions.",
            "Consider them as grouped by the equivalence relation.",
            "So we want to consider equivalence classes defined by the extension of subscriptions.",
            "And of course, this then introduces a degree of freedom.",
            "So how do we want to represent?",
            "Each of the equivalence classes, so we can choose one representative and this is now closing the cycle to the image at the start slide.",
            "We want to choose the minimum or minimum or shortest length description.",
            "Within an equivalence class.",
            "So first aces intuitive reason for that that you can easily grasp and get an interpretation of short subgroups, and then sometimes subgroups are used as building blocks for global models.",
            "And then it's also common paradigm to choose or prefer short high policies over long ones.",
            "So in addition."
        ],
        [
            "What we want is.",
            "We want to really replace in the search.",
            "The individual descriptions, by the equivalence classes, so we want to have all the nice and good considerations regarding structure search.",
            "An optimistic estimate of pruning.",
            "We want this to be still applicable in order to gain some efficiency by.",
            "But then on the other end, losing it again, so we want to take the best of both worlds."
        ],
        [
            "Or before we go into the algorithmic details.",
            "Let us look at the potential that lies in this new view of equivalence classes.",
            "First in the theoretical or most extreme case it can happen that you have pairs of completely redundant attributes, like.",
            "Here there's a 1A prime one, and so on.",
            "They are all exact copies of one another, at least on this given data portion there.",
            "And then what happens if one wants to describe the subgroup that consisting only of the last?",
            "Datapoint DM here.",
            "Then you have exactly as the degree of freedom for each of the pair to choose exactly 1 to eliminate the corresponding line here until you brought it down to the last line.",
            "And this gives you 2 * 2 * 2 two to the power of choice is to describe this last line, so in most extreme case we can reach an exponential reduction of search space and output size.",
            "So the question is, how is this in practice?",
            "So it's only a theoretical example and the picture there is a bit more diverse, so for some of the UCI datasets you see that so we have here 10 datasets and four 2 thresholds.",
            "Oh are the result 1st result size and then the search space the line following.",
            "And this is pretty high threshold quality special.",
            "Now you see there are some datasets where there is almost no difference, but there are others where the difference is already pretty significant, as in the long data set we go down from 38,000 resulting subgroups to just one.",
            "So in fact the top 31,000 subgroups subgroup descriptions were actually describing one the same extension.",
            "And yeah, for the third space, the effect increases and then it decreases again if you go down down to lower thresholds of course.",
            "So also in practice there is some potential in this."
        ],
        [
            "Now, before going to the algorithmics, one example to actually visualize some of the equivalence classes for small example data set and see some of their properties.",
            "So for this small toy example data set here we have on the right hand side an excerpt from the search space does not contain the complete source space.",
            "Some of the positive or negative literals are missing, so of course a negative literal Nazi hear or see bar means that C is equal to 0.",
            "On this part of this."
        ],
        [
            "Space the equivalence classes look like this sort of marked with a or comprehend with black line and we see.",
            "Yeah, we can make several observations.",
            "First, there is some compression as desired.",
            "There are some.",
            "Description as well in their own equivalence class as well, and what we see is that equivalence class."
        ],
        [
            "We can have several minimal.",
            "So that's many regarding inclusion and several minimum, so shortest descriptions.",
            "So that means we can choose one of them and we take care of this by using lexicographic order.",
            "But for this talk for the slides, we won't go into that detail, but in the papers.",
            "Explained more explicitly, so that's the first observation."
        ],
        [
            "And the second one is that there is a unique maximum representative.",
            "So now I have to mention of course that I forgot this slide before this.",
            "Of course there is a correspondence to closed itemset mining here.",
            "So it's already well known that there is maximal.",
            "There are maximal representatives of each equivalence class in the itemset case and.",
            "In fact, this result directly transfers to our scenario here, but yeah.",
            "Should have mentioned that.",
            "Of course, all the frequent itemset of closed itemset mining algorithms are not applicable to our task because they might closed itemsets that our maximum the maximum representatives, and we are interested in the shortest ones in the minimum.",
            "So sorry for forgetting that.",
            "And but still we have to.",
            "We have the result.",
            "And.",
            "Um, there's not only a unique maximum representative, but also it can be computed efficiently.",
            "So by this mapping Sigma here we get for every subgroup description maximum equivalent subscription.",
            "And this gives rise to the first.",
            "I would make approach.",
            "Makes exactly use of the closed itemset mining algorithm."
        ],
        [
            "Big cause yeah this.",
            "Sigma Mapping is a closed operator.",
            "Or equivalently, the set of maximum representatives slash closed sets form a closure system are closed under intersection.",
            "And there are several algorithms that can directly enumerate these fixed points of this of such operators.",
            "One of them.",
            "Perhaps the fastest is the LCM algorithm.",
            "And this I wasn't really managed to traverse the maximum representatives.",
            "In a short amount of time and using only linear space, so that's a good thing, because naively, if you will traverse the letters of all maximum representatives, you would have to store them in order to avoid double enumeration.",
            "But this algorithm does this in a clever way by doing.",
            "By doing it check whether just traverse edges and minimal edge and some partial order.",
            "So now we have the maximum representatives per conference class, but we want to go down to the minimum.",
            "So how do we do that?"
        ],
        [
            "So the problem is to generate the minimum representative from a given maximum representative of an equivalence class.",
            "And as it turns out, this is in fact a set cover problem.",
            "Becaused when we have a given maximum representative of an equivalence class, so it's defined uniquely by its extension.",
            "In this case it's D3 and D4.",
            "In this data set.",
            "What you have to do is using only literals from this maximum representative, we have to eliminate all the data points of the complement of the of the extension.",
            "So this every literal can eliminate this data.",
            "These data points, and this is then translated into a set cover problem.",
            "In this case, it would look like this year.",
            "So finding a minimum set cover is unfortunately an NP hard problem.",
            "And even worse, it has been shown to let it cannot be even approximated within sub logarithmic ratio in worst case and a reasonable complexity assumption.",
            "But on the other hand there is greedy set cover algorithm who performs which performs pretty well in practice.",
            "So for most of the instances it gives an optimal solution that runs pretty fast and it has a worst case logarithmic approximation measure.",
            "So greedy algorithm is easy, just selecting every step into the literal, eliminating most of the of the data points until everything is covered."
        ],
        [
            "This gives us the first ready to use algorithmic approach for this non violent subgroup discovery idea.",
            "Um and yeah, then we can analyze its performance.",
            "We see the algorithm is pretty good and average, so we have here the average true or exact minimum, and then the average greedy approximation for the data set.",
            "You see, it's for all of the data sets where we were able to compute the.",
            "For minimum, it was pretty close on average, but on the other hand, for some fuel it might be.",
            "Define relatively much.",
            "So, for instance, we hear lung cancer.",
            "Subgroup equivalence class that had a minimum description of links four and we got only one with links 7, which is so somewhat harder already, somewhat harder to understand them.",
            "And really is cheap but not for free of course.",
            "And so we can observe that for the time fraction used by the computation for some of the datasets, we see that they were really dominated by the greedy computations over 90%.",
            "For some of them here.",
            "So, um.",
            "The question is now, can we exploit more structure?",
            "Can we find in a more direct way the minimum representatives without going through this closed set mining algorithms?"
        ],
        [
            "And if there is a way to do that?",
            "One can observe that the minimum representatives equivalence classes are.",
            "Prefixes of one another.",
            "So if I have a minimum representative and remove.",
            "Where am I?",
            "Remove any arbitrary constraint from the minimum representative.",
            "I will reach a new minimum minimum representative of another equivalence class.",
            "So well, this is exactly the properties that we make use of in shortest path problems.",
            "The shortest path is a prefix of one another.",
            "So in fact, what we have now is shortest path problem.",
            "Thanks with unit weights with unit costs and this can be solved by breadth first search traversal of the graph.",
            "Now this gives us an algorithm that runs with the same time complexity.",
            "But now we have no clever way in checking whether we have visited in Vertex already.",
            "So now not only about it, but the same equivalence class.",
            "So now we have to store the already visited equivalence classes, for instance store them by their maximum unique representative or visor extension.",
            "And look that up.",
            "This look up is more expensive with prefix trees or hash tables, but it can require a substantial amount of space.",
            "That's a drawback.",
            "On the other hand, this gives us really exact minimum representatives.",
            "Yeah, this may sound strange because we just started this is NP hard construction, but of course we circumvented the hardness result because we were not asked to compute just one minimum preventative.",
            "Given some equivalence class.",
            "But we compute all of them in an anti monotone service trace and this is not affected by the hardness result this task."
        ],
        [
            "So now we have two algorithmic approaches for the problem and then we can compare the running time with between the two of them and the traditional subgroup discovery paradigm here represented by the DP subgroup algorithm.",
            "And yeah, it's somehow reflecting the data we have already seen.",
            "Considering as a search space reduction.",
            "So we see already for.",
            "Ohio.",
            "The higher threshold.",
            "We have some data sets that really go down some orders of magnitude, like lung cancer was 48 minutes with the subgroup and goes down with both of the algorithmic approaches with the equivalence classes 223 seconds for instance, and similar here 4.3 hours to 18 minutes.",
            "And this of course there are some datasets where is not much compression, so there's not much game.",
            "And the effect is even stronger, of course, for lower special with search spaces are even bigger, as we can see the search spaces.",
            "The difference in the search space squirrels exponentially with depth limit.",
            "We allow for the subgroup descriptions.",
            "And yeah, one last note to that is although.",
            "So this is for some of these datasets.",
            "I don't know exactly what, but as we can see, the search space is from the start on.",
            "It's smaller, but at first the GP subgroup is somewhat faster than the new algorithm, and only there.",
            "Eventually there is a break even point, and this big cause individual steps in the search space also are now somewhat more expensive.",
            "So it takes some time until this amortizes.",
            "Summary so."
        ],
        [
            "What have you done with what have you seen?",
            "We have replaced the search in the space of individual subgroup descriptions by search in the space of subgroup equivalence classes and.",
            "Representing each of the equivalence classes by a minimum length representative.",
            "This can be used to significantly reduce output in search space, thus time.",
            "Running time.",
            "We had two.",
            "I would make approaches for that one, making use of known closed itemset mining algorithms.",
            "But then we had to take into.",
            "In addition, the greedy set cover algorithm, which introduced some approximate problem inaccuracy in the solutions and some additional time.",
            "And then we had this second.",
            "I would make approach based on the shortest path paradigm and here we are getting the exact minimum representatives at the cost of more memory requirements.",
            "Thanks for your attention."
        ],
        [
            "For two questions.",
            "Why do you use exact equivalence listing if you are allowed the states differ in?",
            "Five elements with 1000 or 100,000 for precise donkey metal work doesn't matter for the maybe for the semantics of the outcoming for resulting subgroup sets, but of course you lose the uniqueness in the representation.",
            "For instance, the first approach would be directly in applicable then.",
            "Treasure.",
            "But if you might be more of the maniac strategy, or jump off a pretty cool.",
            "I would be very interested in comparison to that approach.",
            "I don't know exactly that approach, but in the end in itemset mining you are interested if you go to closed itemset mining and getting all minimal generators and all maximal set 'cause what you want to do it in the end is the family of all minimal non redundant Association rules.",
            "So this family is a lot of larger than our family here.",
            "Who is much smaller?",
            "That's exactly his argument that he wanted to get rid of redundancy, and also by July one there is an algorithm using this prefix in order to compress.",
            "Frequent itemset mining.",
            "Way I would like to see the true difference because it might be at this level.",
            "It looks very much the same type.",
            "Of course there will be.",
            "In detail.",
            "Also compare that at the poster in more detail and like right now we don't know.",
            "Probably portable differences.",
            "Session because."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, welcome to the rest of the session.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me in the back but we have to use a mic, thanks.",
                    "label": 0
                },
                {
                    "sent": "My name is Mario Bola and I'm presenting joint work with my colleague Kendrick, who gave the talk.",
                    "label": 0
                },
                {
                    "sent": "Before this talk.",
                    "label": 0
                },
                {
                    "sent": "We're both from for Nova.",
                    "label": 0
                },
                {
                    "sent": "Yes, and.",
                    "label": 0
                },
                {
                    "sent": "In Germany, the.",
                    "label": 0
                },
                {
                    "sent": "Topic is called nonviolent subgroup discovery.",
                    "label": 1
                },
                {
                    "sent": "Using your closure system.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this title image here for me nicely reflects on intuitive level of that we.",
                    "label": 0
                },
                {
                    "sent": "Wants prefer information in the most concise and simple forms and somehow unsatisfied if it's necessary.",
                    "label": 0
                },
                {
                    "sent": "Blown up.",
                    "label": 0
                },
                {
                    "sent": "And yeah, exactly this idea.",
                    "label": 0
                },
                {
                    "sent": "We want to bring to subgroup discovery.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, the connection will follow in a minute.",
                    "label": 0
                },
                {
                    "sent": "First of all, let me briefly recap what's up with Discovery is because we had already talked about that we don't need every detail we again back in the simple scenario of.",
                    "label": 0
                },
                {
                    "sent": "Two value attribute attribute and the standard constraint language of attribute value equality constraints.",
                    "label": 0
                },
                {
                    "sent": "So we do not use the sophisticated interval constraints.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this.",
                    "label": 0
                },
                {
                    "sent": "This subgroup descriptions, based on our consisting on conjunctions of attribute value constraints, describe subpopulations of the data and usually in the standard task we are interested in listing all interesting subgroup descriptions and what the exact form of the specific form of interestingness is.",
                    "label": 0
                },
                {
                    "sent": "Does not matter to us in this talk, so we just consider the interestingness encapsulated by the quality function Q, and the only assumption we have four Q is.",
                    "label": 0
                },
                {
                    "sent": "That it's a function of the subgroup descriptions extension.",
                    "label": 0
                },
                {
                    "sent": "So as usually the quality function has a combination of the subgroups, generality with its distributional unusualness we see that for both of these components, that's a very natural requirement that they are functions of the extension.",
                    "label": 0
                },
                {
                    "sent": "So function of the extension means that whenever two subgroup descriptions have the same extension on the data, that the quality will be similar.",
                    "label": 0
                },
                {
                    "sent": "Identical.",
                    "label": 0
                },
                {
                    "sent": "Now what then might happen is that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can have many, many different descriptions differently looking descriptions for actually the same.",
                    "label": 0
                },
                {
                    "sent": "Extension is the data for the same subgroup.",
                    "label": 0
                },
                {
                    "sent": "Now with our assumptions, this means that for every equivalent subgroup we will have the same quality and this leads to unwanted effects.",
                    "label": 0
                },
                {
                    "sent": "The first is that there is there can be many unnecessary evaluations of the quality function throughout the search process, cause actually it would have been enough if we know the quality of 1 equivalent subgroup, because the others will be similar.",
                    "label": 1
                },
                {
                    "sent": "And yeah, often evaluating the quality function is a time consuming task because you have to scan through the data and in addition it will introduce redundancy in the output.",
                    "label": 0
                },
                {
                    "sent": "The idea of this work is now too.",
                    "label": 1
                },
                {
                    "sent": "Instead of considering individual subscriptions.",
                    "label": 0
                },
                {
                    "sent": "Consider them as grouped by the equivalence relation.",
                    "label": 0
                },
                {
                    "sent": "So we want to consider equivalence classes defined by the extension of subscriptions.",
                    "label": 0
                },
                {
                    "sent": "And of course, this then introduces a degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "So how do we want to represent?",
                    "label": 1
                },
                {
                    "sent": "Each of the equivalence classes, so we can choose one representative and this is now closing the cycle to the image at the start slide.",
                    "label": 0
                },
                {
                    "sent": "We want to choose the minimum or minimum or shortest length description.",
                    "label": 0
                },
                {
                    "sent": "Within an equivalence class.",
                    "label": 0
                },
                {
                    "sent": "So first aces intuitive reason for that that you can easily grasp and get an interpretation of short subgroups, and then sometimes subgroups are used as building blocks for global models.",
                    "label": 0
                },
                {
                    "sent": "And then it's also common paradigm to choose or prefer short high policies over long ones.",
                    "label": 0
                },
                {
                    "sent": "So in addition.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want is.",
                    "label": 0
                },
                {
                    "sent": "We want to really replace in the search.",
                    "label": 0
                },
                {
                    "sent": "The individual descriptions, by the equivalence classes, so we want to have all the nice and good considerations regarding structure search.",
                    "label": 0
                },
                {
                    "sent": "An optimistic estimate of pruning.",
                    "label": 0
                },
                {
                    "sent": "We want this to be still applicable in order to gain some efficiency by.",
                    "label": 0
                },
                {
                    "sent": "But then on the other end, losing it again, so we want to take the best of both worlds.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or before we go into the algorithmic details.",
                    "label": 0
                },
                {
                    "sent": "Let us look at the potential that lies in this new view of equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "First in the theoretical or most extreme case it can happen that you have pairs of completely redundant attributes, like.",
                    "label": 0
                },
                {
                    "sent": "Here there's a 1A prime one, and so on.",
                    "label": 0
                },
                {
                    "sent": "They are all exact copies of one another, at least on this given data portion there.",
                    "label": 0
                },
                {
                    "sent": "And then what happens if one wants to describe the subgroup that consisting only of the last?",
                    "label": 0
                },
                {
                    "sent": "Datapoint DM here.",
                    "label": 0
                },
                {
                    "sent": "Then you have exactly as the degree of freedom for each of the pair to choose exactly 1 to eliminate the corresponding line here until you brought it down to the last line.",
                    "label": 0
                },
                {
                    "sent": "And this gives you 2 * 2 * 2 two to the power of choice is to describe this last line, so in most extreme case we can reach an exponential reduction of search space and output size.",
                    "label": 1
                },
                {
                    "sent": "So the question is, how is this in practice?",
                    "label": 1
                },
                {
                    "sent": "So it's only a theoretical example and the picture there is a bit more diverse, so for some of the UCI datasets you see that so we have here 10 datasets and four 2 thresholds.",
                    "label": 0
                },
                {
                    "sent": "Oh are the result 1st result size and then the search space the line following.",
                    "label": 0
                },
                {
                    "sent": "And this is pretty high threshold quality special.",
                    "label": 0
                },
                {
                    "sent": "Now you see there are some datasets where there is almost no difference, but there are others where the difference is already pretty significant, as in the long data set we go down from 38,000 resulting subgroups to just one.",
                    "label": 0
                },
                {
                    "sent": "So in fact the top 31,000 subgroups subgroup descriptions were actually describing one the same extension.",
                    "label": 0
                },
                {
                    "sent": "And yeah, for the third space, the effect increases and then it decreases again if you go down down to lower thresholds of course.",
                    "label": 0
                },
                {
                    "sent": "So also in practice there is some potential in this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, before going to the algorithmics, one example to actually visualize some of the equivalence classes for small example data set and see some of their properties.",
                    "label": 0
                },
                {
                    "sent": "So for this small toy example data set here we have on the right hand side an excerpt from the search space does not contain the complete source space.",
                    "label": 0
                },
                {
                    "sent": "Some of the positive or negative literals are missing, so of course a negative literal Nazi hear or see bar means that C is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "On this part of this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space the equivalence classes look like this sort of marked with a or comprehend with black line and we see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can make several observations.",
                    "label": 0
                },
                {
                    "sent": "First, there is some compression as desired.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "Description as well in their own equivalence class as well, and what we see is that equivalence class.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can have several minimal.",
                    "label": 0
                },
                {
                    "sent": "So that's many regarding inclusion and several minimum, so shortest descriptions.",
                    "label": 0
                },
                {
                    "sent": "So that means we can choose one of them and we take care of this by using lexicographic order.",
                    "label": 0
                },
                {
                    "sent": "But for this talk for the slides, we won't go into that detail, but in the papers.",
                    "label": 0
                },
                {
                    "sent": "Explained more explicitly, so that's the first observation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second one is that there is a unique maximum representative.",
                    "label": 0
                },
                {
                    "sent": "So now I have to mention of course that I forgot this slide before this.",
                    "label": 0
                },
                {
                    "sent": "Of course there is a correspondence to closed itemset mining here.",
                    "label": 0
                },
                {
                    "sent": "So it's already well known that there is maximal.",
                    "label": 0
                },
                {
                    "sent": "There are maximal representatives of each equivalence class in the itemset case and.",
                    "label": 0
                },
                {
                    "sent": "In fact, this result directly transfers to our scenario here, but yeah.",
                    "label": 1
                },
                {
                    "sent": "Should have mentioned that.",
                    "label": 0
                },
                {
                    "sent": "Of course, all the frequent itemset of closed itemset mining algorithms are not applicable to our task because they might closed itemsets that our maximum the maximum representatives, and we are interested in the shortest ones in the minimum.",
                    "label": 0
                },
                {
                    "sent": "So sorry for forgetting that.",
                    "label": 0
                },
                {
                    "sent": "And but still we have to.",
                    "label": 0
                },
                {
                    "sent": "We have the result.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um, there's not only a unique maximum representative, but also it can be computed efficiently.",
                    "label": 1
                },
                {
                    "sent": "So by this mapping Sigma here we get for every subgroup description maximum equivalent subscription.",
                    "label": 0
                },
                {
                    "sent": "And this gives rise to the first.",
                    "label": 0
                },
                {
                    "sent": "I would make approach.",
                    "label": 0
                },
                {
                    "sent": "Makes exactly use of the closed itemset mining algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Big cause yeah this.",
                    "label": 0
                },
                {
                    "sent": "Sigma Mapping is a closed operator.",
                    "label": 0
                },
                {
                    "sent": "Or equivalently, the set of maximum representatives slash closed sets form a closure system are closed under intersection.",
                    "label": 0
                },
                {
                    "sent": "And there are several algorithms that can directly enumerate these fixed points of this of such operators.",
                    "label": 0
                },
                {
                    "sent": "One of them.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the fastest is the LCM algorithm.",
                    "label": 0
                },
                {
                    "sent": "And this I wasn't really managed to traverse the maximum representatives.",
                    "label": 0
                },
                {
                    "sent": "In a short amount of time and using only linear space, so that's a good thing, because naively, if you will traverse the letters of all maximum representatives, you would have to store them in order to avoid double enumeration.",
                    "label": 0
                },
                {
                    "sent": "But this algorithm does this in a clever way by doing.",
                    "label": 0
                },
                {
                    "sent": "By doing it check whether just traverse edges and minimal edge and some partial order.",
                    "label": 0
                },
                {
                    "sent": "So now we have the maximum representatives per conference class, but we want to go down to the minimum.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is to generate the minimum representative from a given maximum representative of an equivalence class.",
                    "label": 0
                },
                {
                    "sent": "And as it turns out, this is in fact a set cover problem.",
                    "label": 0
                },
                {
                    "sent": "Becaused when we have a given maximum representative of an equivalence class, so it's defined uniquely by its extension.",
                    "label": 0
                },
                {
                    "sent": "In this case it's D3 and D4.",
                    "label": 0
                },
                {
                    "sent": "In this data set.",
                    "label": 0
                },
                {
                    "sent": "What you have to do is using only literals from this maximum representative, we have to eliminate all the data points of the complement of the of the extension.",
                    "label": 1
                },
                {
                    "sent": "So this every literal can eliminate this data.",
                    "label": 0
                },
                {
                    "sent": "These data points, and this is then translated into a set cover problem.",
                    "label": 1
                },
                {
                    "sent": "In this case, it would look like this year.",
                    "label": 0
                },
                {
                    "sent": "So finding a minimum set cover is unfortunately an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "And even worse, it has been shown to let it cannot be even approximated within sub logarithmic ratio in worst case and a reasonable complexity assumption.",
                    "label": 1
                },
                {
                    "sent": "But on the other hand there is greedy set cover algorithm who performs which performs pretty well in practice.",
                    "label": 0
                },
                {
                    "sent": "So for most of the instances it gives an optimal solution that runs pretty fast and it has a worst case logarithmic approximation measure.",
                    "label": 0
                },
                {
                    "sent": "So greedy algorithm is easy, just selecting every step into the literal, eliminating most of the of the data points until everything is covered.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This gives us the first ready to use algorithmic approach for this non violent subgroup discovery idea.",
                    "label": 0
                },
                {
                    "sent": "Um and yeah, then we can analyze its performance.",
                    "label": 0
                },
                {
                    "sent": "We see the algorithm is pretty good and average, so we have here the average true or exact minimum, and then the average greedy approximation for the data set.",
                    "label": 0
                },
                {
                    "sent": "You see, it's for all of the data sets where we were able to compute the.",
                    "label": 0
                },
                {
                    "sent": "For minimum, it was pretty close on average, but on the other hand, for some fuel it might be.",
                    "label": 0
                },
                {
                    "sent": "Define relatively much.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, we hear lung cancer.",
                    "label": 0
                },
                {
                    "sent": "Subgroup equivalence class that had a minimum description of links four and we got only one with links 7, which is so somewhat harder already, somewhat harder to understand them.",
                    "label": 0
                },
                {
                    "sent": "And really is cheap but not for free of course.",
                    "label": 0
                },
                {
                    "sent": "And so we can observe that for the time fraction used by the computation for some of the datasets, we see that they were really dominated by the greedy computations over 90%.",
                    "label": 0
                },
                {
                    "sent": "For some of them here.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "The question is now, can we exploit more structure?",
                    "label": 0
                },
                {
                    "sent": "Can we find in a more direct way the minimum representatives without going through this closed set mining algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if there is a way to do that?",
                    "label": 0
                },
                {
                    "sent": "One can observe that the minimum representatives equivalence classes are.",
                    "label": 0
                },
                {
                    "sent": "Prefixes of one another.",
                    "label": 0
                },
                {
                    "sent": "So if I have a minimum representative and remove.",
                    "label": 0
                },
                {
                    "sent": "Where am I?",
                    "label": 0
                },
                {
                    "sent": "Remove any arbitrary constraint from the minimum representative.",
                    "label": 0
                },
                {
                    "sent": "I will reach a new minimum minimum representative of another equivalence class.",
                    "label": 0
                },
                {
                    "sent": "So well, this is exactly the properties that we make use of in shortest path problems.",
                    "label": 0
                },
                {
                    "sent": "The shortest path is a prefix of one another.",
                    "label": 1
                },
                {
                    "sent": "So in fact, what we have now is shortest path problem.",
                    "label": 1
                },
                {
                    "sent": "Thanks with unit weights with unit costs and this can be solved by breadth first search traversal of the graph.",
                    "label": 0
                },
                {
                    "sent": "Now this gives us an algorithm that runs with the same time complexity.",
                    "label": 0
                },
                {
                    "sent": "But now we have no clever way in checking whether we have visited in Vertex already.",
                    "label": 0
                },
                {
                    "sent": "So now not only about it, but the same equivalence class.",
                    "label": 0
                },
                {
                    "sent": "So now we have to store the already visited equivalence classes, for instance store them by their maximum unique representative or visor extension.",
                    "label": 0
                },
                {
                    "sent": "And look that up.",
                    "label": 0
                },
                {
                    "sent": "This look up is more expensive with prefix trees or hash tables, but it can require a substantial amount of space.",
                    "label": 1
                },
                {
                    "sent": "That's a drawback.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this gives us really exact minimum representatives.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this may sound strange because we just started this is NP hard construction, but of course we circumvented the hardness result because we were not asked to compute just one minimum preventative.",
                    "label": 0
                },
                {
                    "sent": "Given some equivalence class.",
                    "label": 0
                },
                {
                    "sent": "But we compute all of them in an anti monotone service trace and this is not affected by the hardness result this task.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have two algorithmic approaches for the problem and then we can compare the running time with between the two of them and the traditional subgroup discovery paradigm here represented by the DP subgroup algorithm.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it's somehow reflecting the data we have already seen.",
                    "label": 0
                },
                {
                    "sent": "Considering as a search space reduction.",
                    "label": 0
                },
                {
                    "sent": "So we see already for.",
                    "label": 0
                },
                {
                    "sent": "Ohio.",
                    "label": 0
                },
                {
                    "sent": "The higher threshold.",
                    "label": 0
                },
                {
                    "sent": "We have some data sets that really go down some orders of magnitude, like lung cancer was 48 minutes with the subgroup and goes down with both of the algorithmic approaches with the equivalence classes 223 seconds for instance, and similar here 4.3 hours to 18 minutes.",
                    "label": 0
                },
                {
                    "sent": "And this of course there are some datasets where is not much compression, so there's not much game.",
                    "label": 0
                },
                {
                    "sent": "And the effect is even stronger, of course, for lower special with search spaces are even bigger, as we can see the search spaces.",
                    "label": 0
                },
                {
                    "sent": "The difference in the search space squirrels exponentially with depth limit.",
                    "label": 0
                },
                {
                    "sent": "We allow for the subgroup descriptions.",
                    "label": 0
                },
                {
                    "sent": "And yeah, one last note to that is although.",
                    "label": 0
                },
                {
                    "sent": "So this is for some of these datasets.",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly what, but as we can see, the search space is from the start on.",
                    "label": 0
                },
                {
                    "sent": "It's smaller, but at first the GP subgroup is somewhat faster than the new algorithm, and only there.",
                    "label": 0
                },
                {
                    "sent": "Eventually there is a break even point, and this big cause individual steps in the search space also are now somewhat more expensive.",
                    "label": 0
                },
                {
                    "sent": "So it takes some time until this amortizes.",
                    "label": 0
                },
                {
                    "sent": "Summary so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What have you done with what have you seen?",
                    "label": 0
                },
                {
                    "sent": "We have replaced the search in the space of individual subgroup descriptions by search in the space of subgroup equivalence classes and.",
                    "label": 1
                },
                {
                    "sent": "Representing each of the equivalence classes by a minimum length representative.",
                    "label": 0
                },
                {
                    "sent": "This can be used to significantly reduce output in search space, thus time.",
                    "label": 1
                },
                {
                    "sent": "Running time.",
                    "label": 0
                },
                {
                    "sent": "We had two.",
                    "label": 0
                },
                {
                    "sent": "I would make approaches for that one, making use of known closed itemset mining algorithms.",
                    "label": 0
                },
                {
                    "sent": "But then we had to take into.",
                    "label": 0
                },
                {
                    "sent": "In addition, the greedy set cover algorithm, which introduced some approximate problem inaccuracy in the solutions and some additional time.",
                    "label": 0
                },
                {
                    "sent": "And then we had this second.",
                    "label": 0
                },
                {
                    "sent": "I would make approach based on the shortest path paradigm and here we are getting the exact minimum representatives at the cost of more memory requirements.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For two questions.",
                    "label": 0
                },
                {
                    "sent": "Why do you use exact equivalence listing if you are allowed the states differ in?",
                    "label": 0
                },
                {
                    "sent": "Five elements with 1000 or 100,000 for precise donkey metal work doesn't matter for the maybe for the semantics of the outcoming for resulting subgroup sets, but of course you lose the uniqueness in the representation.",
                    "label": 0
                },
                {
                    "sent": "For instance, the first approach would be directly in applicable then.",
                    "label": 0
                },
                {
                    "sent": "Treasure.",
                    "label": 0
                },
                {
                    "sent": "But if you might be more of the maniac strategy, or jump off a pretty cool.",
                    "label": 0
                },
                {
                    "sent": "I would be very interested in comparison to that approach.",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly that approach, but in the end in itemset mining you are interested if you go to closed itemset mining and getting all minimal generators and all maximal set 'cause what you want to do it in the end is the family of all minimal non redundant Association rules.",
                    "label": 0
                },
                {
                    "sent": "So this family is a lot of larger than our family here.",
                    "label": 0
                },
                {
                    "sent": "Who is much smaller?",
                    "label": 0
                },
                {
                    "sent": "That's exactly his argument that he wanted to get rid of redundancy, and also by July one there is an algorithm using this prefix in order to compress.",
                    "label": 0
                },
                {
                    "sent": "Frequent itemset mining.",
                    "label": 0
                },
                {
                    "sent": "Way I would like to see the true difference because it might be at this level.",
                    "label": 0
                },
                {
                    "sent": "It looks very much the same type.",
                    "label": 0
                },
                {
                    "sent": "Of course there will be.",
                    "label": 0
                },
                {
                    "sent": "In detail.",
                    "label": 0
                },
                {
                    "sent": "Also compare that at the poster in more detail and like right now we don't know.",
                    "label": 0
                },
                {
                    "sent": "Probably portable differences.",
                    "label": 0
                },
                {
                    "sent": "Session because.",
                    "label": 0
                }
            ]
        }
    }
}