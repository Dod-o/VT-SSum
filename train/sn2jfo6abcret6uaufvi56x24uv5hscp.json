{
    "id": "sn2jfo6abcret6uaufvi56x24uv5hscp",
    "title": "Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results",
    "info": {
        "author": [
            "Laurent Massouile, Microsoft Research-Inria Joint Centre"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_massoulie_results/",
    "segmentation": [
        [
            "Hello so I will only talk about 1/2 of what's in the paper in the five minutes I have so."
        ],
        [
            "The talk is about stochastic block models, which have been mentioned already this morning which have been introduced in the 80s by hold on data to reflect social networks and the game there was to recover underlying communities based on observing a graph.",
            "So the stochastic block model is constituted of number of blocks of homogeneous individuals and then based on which block individuals belongs two, we draw an edge or not.",
            "And the probability of drawing an edge between two individuals.",
            "The function of their underlying blood, so that would be B sub the blocks they belong to KU KV-4 nodes.",
            "UNP in this case.",
            "Then we can scale that value 1 / N which would let the number of neighbors other nodes scale be kept constant with the system size M and then I have another factor S which is the signal strength.",
            "If you like which you should think of as the.",
            "The average number of neighbors in this in this random graph, and so the the game usually is to recover the blocks from the observation of the adjacency matrix."
        ],
        [
            "So we consider two kinds of twists on the basic model.",
            "The first one is to decorate the edges with labels so we we may.",
            "For instance, if you're trying to model something like the Netflix data set, tag each edge with 125 Star label, and again we may just assume that the distribution of which label is used depends on the underlying blocks, so that's one first generalization."
        ],
        [
            "Second one that we consider that has been also considered in a number of previous papers, notably by A Bola.",
            "Bassett Allen in 2007 is to relax the block assumption, so we may instead assume that each user is not characterized by belonging to a block, but by label node label which may belong to a potentially infinite and even continuous space.",
            "So for instance, this could be the interval 01.",
            "And now it no longer makes sense to retrieve the blocks because there are infinitely many blocks, so the game is now to try and guess what could.",
            "What label would have been on an edge that we don't observe?",
            "So we want to estimate the edge label distributions so."
        ],
        [
            "How do we do that?",
            "So based on this?",
            "Adjacency matrix decorated with labels on the edges.",
            "So the first thing is to.",
            "Move back to a more classical setup so we project the labels that are categorical for the edges onto a numerical value.",
            "So we pick uniform random numbers, say between zero and one, and we attach one such number to each label and we remove the labels and replace them by the these numbers and so we get the metrics and now we are in a framework where we can do spectral processing and so."
        ],
        [
            "So the main tool that we have is that we understand the spectral structure of this matrix, provided we have a strong enough signal and strong enough here means at least logarithmic average degree of nodes.",
            "So this is the assumption we need and given this assumption then we can leverage previous works and understand the spectral structure of this matrix.",
            "So we understand that its eigenvalues are characterized by the eigenvalues of an associated integral operator.",
            "And we can also get an understanding of how the eigen vectors behave, and so one first side result of this analysis is that we can easily craft models which have constant average degree but nevertheless have a power law Spectra, which is something that.",
            "May be interested in in view of empirical analysis of the Netflix datasets say."
        ],
        [
            "But more than that, what was interesting to us is that we can get consistent estimates of the edge labels that we don't observe, so we do a spectral embedding and we can control the accuracy of the embedding with the tail genergy in the tail of the operator.",
            "I've just shown on the previous slide."
        ],
        [
            "Right and so based on that, here's the kind of estimate we can craft.",
            "We do the spectrum back."
        ],
        [
            "Bing and asking what could be the label between I and Jayden?"
        ],
        [
            "We do in the spectral space we do some.",
            "We construct an empirical average in the spectral neighborhood of one of the two nodes, and we get a consistent estimate, the bias being controlled by the tail of this operator, and so I'm."
        ],
        [
            "Done and come and find me.",
            "If you want more details or I'll check the pastor of the paper and there's also the other half of the paper, which is what happens when, instead of having A at least logarithmic signal strength, you have instead order one signal strength, in which case there are impossibility results where no signal is present anymore in the data.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello so I will only talk about 1/2 of what's in the paper in the five minutes I have so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The talk is about stochastic block models, which have been mentioned already this morning which have been introduced in the 80s by hold on data to reflect social networks and the game there was to recover underlying communities based on observing a graph.",
                    "label": 0
                },
                {
                    "sent": "So the stochastic block model is constituted of number of blocks of homogeneous individuals and then based on which block individuals belongs two, we draw an edge or not.",
                    "label": 1
                },
                {
                    "sent": "And the probability of drawing an edge between two individuals.",
                    "label": 0
                },
                {
                    "sent": "The function of their underlying blood, so that would be B sub the blocks they belong to KU KV-4 nodes.",
                    "label": 0
                },
                {
                    "sent": "UNP in this case.",
                    "label": 0
                },
                {
                    "sent": "Then we can scale that value 1 / N which would let the number of neighbors other nodes scale be kept constant with the system size M and then I have another factor S which is the signal strength.",
                    "label": 0
                },
                {
                    "sent": "If you like which you should think of as the.",
                    "label": 0
                },
                {
                    "sent": "The average number of neighbors in this in this random graph, and so the the game usually is to recover the blocks from the observation of the adjacency matrix.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider two kinds of twists on the basic model.",
                    "label": 0
                },
                {
                    "sent": "The first one is to decorate the edges with labels so we we may.",
                    "label": 1
                },
                {
                    "sent": "For instance, if you're trying to model something like the Netflix data set, tag each edge with 125 Star label, and again we may just assume that the distribution of which label is used depends on the underlying blocks, so that's one first generalization.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second one that we consider that has been also considered in a number of previous papers, notably by A Bola.",
                    "label": 0
                },
                {
                    "sent": "Bassett Allen in 2007 is to relax the block assumption, so we may instead assume that each user is not characterized by belonging to a block, but by label node label which may belong to a potentially infinite and even continuous space.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this could be the interval 01.",
                    "label": 0
                },
                {
                    "sent": "And now it no longer makes sense to retrieve the blocks because there are infinitely many blocks, so the game is now to try and guess what could.",
                    "label": 0
                },
                {
                    "sent": "What label would have been on an edge that we don't observe?",
                    "label": 0
                },
                {
                    "sent": "So we want to estimate the edge label distributions so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "So based on this?",
                    "label": 0
                },
                {
                    "sent": "Adjacency matrix decorated with labels on the edges.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is to.",
                    "label": 0
                },
                {
                    "sent": "Move back to a more classical setup so we project the labels that are categorical for the edges onto a numerical value.",
                    "label": 0
                },
                {
                    "sent": "So we pick uniform random numbers, say between zero and one, and we attach one such number to each label and we remove the labels and replace them by the these numbers and so we get the metrics and now we are in a framework where we can do spectral processing and so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main tool that we have is that we understand the spectral structure of this matrix, provided we have a strong enough signal and strong enough here means at least logarithmic average degree of nodes.",
                    "label": 0
                },
                {
                    "sent": "So this is the assumption we need and given this assumption then we can leverage previous works and understand the spectral structure of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So we understand that its eigenvalues are characterized by the eigenvalues of an associated integral operator.",
                    "label": 0
                },
                {
                    "sent": "And we can also get an understanding of how the eigen vectors behave, and so one first side result of this analysis is that we can easily craft models which have constant average degree but nevertheless have a power law Spectra, which is something that.",
                    "label": 0
                },
                {
                    "sent": "May be interested in in view of empirical analysis of the Netflix datasets say.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But more than that, what was interesting to us is that we can get consistent estimates of the edge labels that we don't observe, so we do a spectral embedding and we can control the accuracy of the embedding with the tail genergy in the tail of the operator.",
                    "label": 0
                },
                {
                    "sent": "I've just shown on the previous slide.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right and so based on that, here's the kind of estimate we can craft.",
                    "label": 0
                },
                {
                    "sent": "We do the spectrum back.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing and asking what could be the label between I and Jayden?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do in the spectral space we do some.",
                    "label": 0
                },
                {
                    "sent": "We construct an empirical average in the spectral neighborhood of one of the two nodes, and we get a consistent estimate, the bias being controlled by the tail of this operator, and so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done and come and find me.",
                    "label": 0
                },
                {
                    "sent": "If you want more details or I'll check the pastor of the paper and there's also the other half of the paper, which is what happens when, instead of having A at least logarithmic signal strength, you have instead order one signal strength, in which case there are impossibility results where no signal is present anymore in the data.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}