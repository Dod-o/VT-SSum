{
    "id": "qlqbhrdwqbj4qz6dwl23pzrki2yn76yg",
    "title": "Using Interior Point Methods for Optimization in Training Very Large Scale Support Vector Machines",
    "info": {
        "author": [
            "Jacek Gondzio, School of Mathematics, University of Edinburgh"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/icml09_gondzio_ituipm/",
    "segmentation": [
        [
            "Is it OK?",
            "Good.",
            "Let me let me first say that I had four great days here.",
            "Hearing a lot of talks in machine learning.",
            "It's my first time of when I come to the conference on Machine learning, so it's all fascinating, and I'm amazed that there is so much optimization going on in machine learning, and I'm particularly grateful to the three organizers.",
            "I think everybody here is probably grateful to the three organizers for organizing the workshop, but I'm grateful in particular for giving me this opportunity of seeing what's going on.",
            "And machine learning.",
            "My area of specialization is really in interior point methods and I have heard different things about interior point methods at this conference.",
            "Some of critical remarks about how tough these methods are not necessarily well suited to solving some of the SVM problems.",
            "I would like to defend in this talk interior point methods that they can deliver in SVM context.",
            "It's a joint work with my PhD student Christian Woodson that he's a very.",
            "Very clever guy.",
            "I'm lucky to have such a clever."
        ],
        [
            "Age these children.",
            "Now the outline of my talk.",
            "The talk will be divided into 3 parts.",
            "First, I'd like to convey some ideas of interior point methods to you and show this methodology in a nutshell.",
            "Then I'll focus on support vector machine training and this is something that I'm a little bit embarrassed to talk to you about, because you probably know more than I do about it, so I'll really fly through it and then will join the two things together to see how interior point methods can be specialized for SVM training.",
            "Some challenges still remain.",
            "That's good, because otherwise we will be out of job at the."
        ],
        [
            "Yes it is.",
            "Interior point methods."
        ],
        [
            "Part one what are the elements of interior point method?",
            "It's we combine three mathematical tricks.",
            "The use of logarithmic barriers, the use of duality theory and the Newton method.",
            "And if you want to see how this is done you can see this very nice book on which provides you with the theories of interior point methods.",
            "This is Steve right and you can look at this survey paper which discusses the implementation of tricks necessary to make this methodology working efficiently."
        ],
        [
            "Log Arhythmic barrier has this nice feature that if the argument of the logarithm of the logarithm goes to zero, which I won't be able to show unless I jump here, then minus logarithm shoots to Infinity.",
            "In other words, it creates a barrier that makes it impossible for the inequality to become active to become an equation.",
            "Now look at the transformation.",
            "If we replace V greater than zero with the minus logarithm, and if we minimize this sum.",
            "Which is equivalent to minimizing the exponent.",
            "Then we effectively maximize the product of all V eyes.",
            "This means that we are keeping the point far away from zero with respect to all the components V. And this means we are keeping the point in the interior.",
            "Going slowly if you minimize this, you minimize the sum of logarithms about E to the logarithm is, of course VI.",
            "If you minimize that, you want to maximize E to the plus and E to the plus is the same as that.",
            "Fried.",
            "So longer ethnic barriers are the tools to create interior point method.",
            "That is, are the tools.",
            "Logarithmic barrier is the tool that will throw away the algorithm into the interior far away from the boundary."
        ],
        [
            "Well, let's have a look how it works for quadratic programming.",
            "I won't be able to jump that that that high.",
            "Now we start with a quadratic programming problem in which inequality's are present the greater than or equal.",
            "Than 0 and we replace this inequality where the logarithmic barrier which appears here in the objective.",
            "That way we got rid of certain difficulty in optimization.",
            "The difficulty consists in deciding whether the constraint is active at the optimum or inactive.",
            "That's the big trouble of active set methods.",
            "The whole class of optimization methods.",
            "Now for this problem will formulate the Lagrangian which has the whole objective function and LaGrange times the constraints.",
            "So that's that's the trick known from the end of 18th century LaGrange contribution to mathematics.",
            "Now for this Lagrangian we will now write optimality conditions.",
            "That is, you know that for Lagrangian we will be looking for a saddle point.",
            "That is the point which minimizes the Lagrangian with respect to V and maximizes with respect to LaGrange multiplier Lambda.",
            "To find this point, we will differentiate this function now quick differentiation with respect to V should give me C + Q V. A transpose, Lambda.",
            "What are we going to get from there?",
            "1 / V times.",
            "The barrier parameter mean.",
            "OK, that's what you are going to see now."
        ],
        [
            "Because I computed the derivatives with respect to V and Lambda, the derivative with Lambda is trivial and we now have the.",
            "The system of two equations in the top equation we will make this substitution.",
            "We will define that the vector me Times V inverse E is equal to a small S. By the way, there is a clever notation that I'm using this capital V or capital V inverse means that this is a diagonal matrix on the diagonal of the matrix.",
            "There is a vector sitting.",
            "So if we write Capital V. Like that, it simply means V1V2 up to VN.",
            "If we write it with an inverse, this will be inverses of the of the elements of the vector.",
            "So substitute this S and then you will see that the upper equation becomes this one.",
            "The second equation becomes this one, and Additionally we have this W. Sorry v * S. Times the vector of ones is equal to me.",
            "Kind of the vector of ones and here I need.",
            "I owe you an explanation for this.",
            "Means is the same that VI times SI is equal to me for all I.",
            "From one to two M OK.",
            "So it's another way of writing something that is always a problem in optimization, which is called complementarity condition.",
            "What would you do if I asked you to solve this system of equations?",
            "What would be your favorite method to do it?",
            "Come on, wake up.",
            "It's not that late yet.",
            "Oh no, we are only.",
            "We are deriving interior point method.",
            "You have a system of nonlinear equations.",
            "How do you tackle it in mathematics Newton method we are going back to 16th century, no 17th century, no.",
            "OK, except that probably Newton did not think of that large problems but but otherwise we owe it to him.",
            "So this is the system of equations where the first 2 equations are linear with respect to unknowns, Lambda, SNV.",
            "But the third one is Biliner is mildly nonlinear, and we can solve it using Newton method.",
            "So let us now derive Newton method for the system.",
            "Newton method will consist in taking the whole function here and differentiating it with respect to V, Lambda and S. So for a moment think about the derivative of the first row.",
            "There will be only one non zero element.",
            "This is the derivative with V, there will be zero and zero.",
            "The derivatives with respect to.",
            "Lambda and S right.",
            "Let me show."
        ],
        [
            "Up to you.",
            "That's it, that's what we."
        ],
        [
            "That's what we obtain if we derive the Newton method, we have the derivative.",
            "Of the function F with respect to all the three arguments.",
            "And it's given to you here.",
            "It's a big matrix, but this now means that we can write.",
            "The system of linear equations, the solution of which will give us Newton direction which is supposed to take you closer to the solution of the nonlinear system of equations.",
            "Under the conditions that I'm sloppy about and not telling what conditions are necessary to achieve it, but believe me they are satisfied here.",
            "So we made the following steps.",
            "We took the optimization problem.",
            "We introduced logarithmic barrier to replace inequality's.",
            "Then we wrote Lagrangian function for the problem.",
            "Then we differentiate Lagrangian function to find 1st order optimality conditions and now we apply Newton method to solve these first order optimality conditions.",
            "That's the key concept of interior point method.",
            "Now the question why this should?"
        ],
        [
            "Work better than the well known active set method.",
            "Now the difference is hidden in the fact what do we do with the complementarity condition?",
            "Are we forcing this?",
            "To be zero from the start.",
            "Or are we doing that?",
            "Hey, if we do active set method then we have to guess how complementarity is satisfied and the gas is inevitably come combinatorial in spirit because we have to choose whether.",
            "It's actually not XSV whether V is equal to 0 or S is equal to 0, while if we do it through interior Point methodology, we have a proper mathematical tool.",
            "We have a perturbation and we will drive the perturbation to 0.",
            "So this is the essential difference between what happens in the active set.",
            "What happens in interior point methods?"
        ],
        [
            "It has far reaching consequences this thing because active set methods suffer from non non polynomial convergence non polynomial convergence because you always do this dreadful combinatorial operation, choose subset from a set which is which has exponentially many possibilities to be realized while in interior Point method we force that all the products.",
            "Go to zero at the same rate and only at optimality.",
            "That is only when me really approaches zero, we will reach the optimum.",
            "And this is really a hope that interior point methods should work well whenever the size of the problem gets larger and larger, because when the size of the problem gets larger, the more difficult it is to guess correctly the optimal partition.",
            "My argument here would be the following.",
            "If you use active set methods, you are guessing if you use interior point methods you use proper epsilon mathematics.",
            "You have the proofs that give you convergence.",
            "You have the proper analysis of what's going on with the algorithm."
        ],
        [
            "Right?"
        ],
        [
            "Now back to the Newton method.",
            "You remember I derived this thing or I showed you quickly how it looks like this is.",
            "The equation that gives me Newton direction, but of course it's a big equipped big linear system that can be solved if we eliminate.",
            "Some of the easier elements of the vector from here.",
            "This is diagonal.",
            "Let me remind you, this is diagonal.",
            "This is even better.",
            "This is identity, so we can quickly eliminate Delta S from here and reduce the system and write it only with respect to Delta V and Delta."
        ],
        [
            "Uncle.",
            "That's what happens if we do that.",
            "And now the first trouble appears.",
            "The trouble is that this operation requires creating such a matrix, and the matrix has very bad feature because it is V / S componentwise.",
            "And this means that let me remind you either element of V or element of S has to go to 0.",
            "So we will have either zero in the numerator or a zero in the denominator.",
            "Well, that's dreadful because it means we will have the splitting between the elements.",
            "Some of them will go to Infinity, others will go to zero.",
            "And this is bad for linear algebra people, because this immediately means that we will have condition number of the system that goes to Infinity.",
            "Now if you look at the textbook.",
            "Analysis of the accuracy.",
            "You have to give up and you have to accept that there is no hope to find solutions to the system of linear equations, the condition of which is going to Infinity.",
            "But in practice we are working in such subspaces that it is possible to find the solution of the system and the solution will be actually sufficiently accurate to take us to optimality."
        ],
        [
            "Well, with this augmented system, I'd like to immediately draw your attention to certain thing that we have the matrix Q which corresponds to the quadratic form in the quadratic programming problem.",
            "But if this matrix Q.",
            "Where diagonal.",
            "Then actually we would have diagonal plus diagonal.",
            "We would easily invert this matrix.",
            "We would have a very easy one.",
            "One block in our two by two matrix, right?",
            "We could then eliminate this block.",
            "And if we do that, this would be the normal equation system that we have to solve for Delta Lambda.",
            "Then having computed Delta Lambda we would compute Delta V and we have to go one slide backward to compute Delta S and have the complete Newton direction.",
            "Right?",
            "So bear in mind I would love to have diagonal matrix in my QP.",
            "Because if.",
            "The matrix in the quadratic form in the quadratic optimization is diagonal.",
            "This makes the thing as easy as in linear programming problem.",
            "Such problems have the name in optimization they are called separable and then.",
            "That's that's the reason why separable problems are considered to be easy, and this is the reason why I will try to reformulate all known as VM problems to separable kewpies.",
            "Hoping that if I manage to do that, I will make them easy to solve.",
            "One quick remark about this thing.",
            "If we didn't have the matrix D. If this was a general sparse matrix, the inversion here would be a disaster and you should never invert a sparse matrix.",
            "Presure Dylan told you clearly that mathematicians sometimes write the inverse of the matrix and think this this is inverse of the matrix times the right hand side gives you the solution of the problem on paper.",
            "Yes.",
            "In in practical computations it's never the case we do."
        ],
        [
            "Compute inverses, here's an example of a very simple matrix which has nearly three non zero elements per column.",
            "It's a band matrix, but if you invert it you obtain a completely dense matrix.",
            "The last column of which seems to disappear here.",
            "So you could have the same example for a much larger dimension.",
            "This is the warning.",
            "Don't don't invert sparse matrices because they are inverse, maybe dense.",
            "Instead of inverting them, compute and.",
            "Cholesky or Lup decomposition for such a matrix, and then you can really solve the system, but it will be much, much easier if this was a diagonal matrix, wouldn't it?"
        ],
        [
            "Right, let me wrap up the thing about the interior point methods.",
            "Why these methods offer such a potential?",
            "Well, first of all, there is a well established theory which says that they converge in the number of iterations, which is proportional to the problem dimension.",
            "Or may even be proportional to the square root of the problem dimension.",
            "That's fantastic.",
            "In practice they behave much, much better than this worst case analysis.",
            "In practice, they converge in the number of iterations between 10 and 20, unless you go for a size of 1 billion, then you may have to do 50 iterations.",
            "Let's log N number of iterations.",
            "Now.",
            "This optimistic part of the story, the pessimistic part of the story, is that one iteration may be expensive.",
            "So it does not necessarily mean that we are that fast.",
            "However, the larger the problem, the bigger the chance that into your point will beat the competition.",
            "The numerical kernel of the computations is hidden here for quadratic programming problems we have to build this matrix.",
            "And then we have to compute Cholesky for this matrix.",
            "Now, if we assume that matrix A is a dense matrix with N rows and then and a lot of columns and columns, then the complexity of building this would be NM squared and the decomposition will be M ^3.",
            "This calligraphic oh means complexity.",
            "Well, you know.",
            "So.",
            "To summarize, if we want to solve quadratic programming problem.",
            "With interior Point, we would like the problem to be separable, that is, to have diagonal matrix in the quadratic form and then.",
            "That's the effort we have to expect per iteration of Interior Point method, and we hope to do very few iterations and find the optimal solution."
        ],
        [
            "A comment in conditioning is always present here because this in conditioning is the result of what happens to theater.",
            "Let me remind you some elements of Theatre.",
            "Go to zero, others go to Infinity.",
            "You cannot really do anything about it, but there is a nice paper of Goldfarb and Scheinberg, which tells you that actually, it's it does not hurt that much because you can build Cholesky decomposition and all instability of Theta.",
            "Will only be displayed in Lambda.",
            "The diagonal matrix, while else we will while the matrix will look very nice, will have a very nicely bounded condition number."
        ],
        [
            "Right?",
            "Let me summarize interior point methods.",
            "I like them because and I hope you will like them as well because they offer they are polynomial algorithms.",
            "They have provable first case complexity.",
            "They have excellent practical behavior.",
            "They are competitive for small problems when the size of the problem is below 1 million and they practically have no competition if you go to higher dimensions.",
            "No, this is obviously an opportunity for SVM training because in SVM training we sometimes have very large data sets and we naturally have dense data set, and these computations.",
            "So somehow a marriage of these two things almost imposes itself it's a marriage."
        ],
        [
            "Of convenience.",
            "Support vector machines.",
            "This will be 1 slide, one second, second slide, second another one second OK Huawei because Corey, nor did a great job afew days ago.",
            "She explained all the reminded you all the details about how support vector machines for linear support vector machines are defined.",
            "What is this idea of finding a good mark?"
        ],
        [
            "And separating.",
            "Bad and good points, so let me go very quickly.",
            "We have a classification problem.",
            "We have a set of points and we would like classify them into good and bad guys and we hope the intersection between good and bad guys is empty.",
            "So we are looking for a function that will take positive values for good guys, negative values for bad guys."
        ],
        [
            "Suppose the set is linearly separable.",
            "That is, we can find the hyperplane that separates button good guys.",
            "Then we could have it this decision rule that if F is positive, we give a label plus one to the point saying good or if negative minus one saying it's bad."
        ],
        [
            "Let me jump through the next one, OK?",
            "So we have the separating hyperplane and we have this inequality which is true for all good points and another one which is good for all bad points.",
            "I deliberately normalized this vector because this gives me a proper definition of the distance between good and bad guys and the distance is of course this minus that, which gives Me 2 divided by the normal W. Now you want the distance of separation to be as large as possible, so you want to maximize the distance of separation, which is equivalent to minimizing the denominator here.",
            "So we minimize the norm.",
            "Now if we minimize the quadratic norm, it's the same as minimizing the square of the quadratic norm.",
            "Only it makes our life easier, because let me remind you, we all prefer to differentiate the quadratic function rather than the square root of the quadratic function.",
            "From the secondary school you should remember it was easier to differentiate the quadratic rather than the square root of quadratic."
        ],
        [
            "So QP formulation here looks like that we minimize this norm of square root of norm of W, subject to lots of constraints.",
            "Well, a quick warning is needed here.",
            "Usually problems are not that nice.",
            "For example, we do not have a perfectly separable set of points, some of them are misclassified and we have to add penalty for misclassification.",
            "Sometimes we cannot really separate things with linear hyperplane, we need to do something more complicated."
        ],
        [
            "So 2 pictures to clarify that if we do not have a linearly separable point because the B, which is a bad point, is somewhere among the good guys, then we will penalize for the distance which is measured with C. The distance from the separating hyperplane.",
            "Or a more complicated situation.",
            "You cannot draw a straight line to separate them.",
            "You need to transform.",
            "You need to find the transformation fee that from this separable set will transform it to the new space in which linear separation will be possible.",
            "And this is done by the kernel.",
            "Function well here, but the feature map, but it will."
        ],
        [
            "Into the kernel function.",
            "If we have linearly non separable case then we are these panel disk C. To every misclassified point.",
            "And now our inequality looks like that that see compensates for the case when without see, we would never be able to satisfy the inequality.",
            "Overall, we now want to maximize the margin.",
            "With this minimize the norm.",
            "Of W and at the same time we would like to minimize the the total penalty for misclassification.",
            "We combine the two objectives into a single objective and this gives you the I would say Canonical formulation of the classification problem."
        ],
        [
            "We can derive the dual.",
            "But I."
        ],
        [
            "How?",
            "Go quickly through it.",
            "That's almost identical to the to the talk that Karina gave."
        ],
        [
            "Two days ago, three days ago.",
            "Here's the duel.",
            "I'm really sorry to say it, but it's an impossible for an optimizer to use Alpha as a variable.",
            "I I made an effort, I put it on the slide, but then I just simply couldn't couldn't look at it, so I needed to change to remove.",
            "That offers a step size in every book on optimization, so let's stick to that.",
            "That is the Alpha and the usual notation of SVM community.",
            "Here's the door optimization problem in which we want to maximize this function, and the function is concave.",
            "So this is a good thing.",
            "We minimize the convex function or maximize the concave function that's convex optimization problem."
        ],
        [
            "We can write it in this world in this way.",
            "In matrix notation that we know maximize a linear form minus the quadratic form, subject to just a single constraint and bars."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it OK?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Let me let me first say that I had four great days here.",
                    "label": 0
                },
                {
                    "sent": "Hearing a lot of talks in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's my first time of when I come to the conference on Machine learning, so it's all fascinating, and I'm amazed that there is so much optimization going on in machine learning, and I'm particularly grateful to the three organizers.",
                    "label": 0
                },
                {
                    "sent": "I think everybody here is probably grateful to the three organizers for organizing the workshop, but I'm grateful in particular for giving me this opportunity of seeing what's going on.",
                    "label": 0
                },
                {
                    "sent": "And machine learning.",
                    "label": 0
                },
                {
                    "sent": "My area of specialization is really in interior point methods and I have heard different things about interior point methods at this conference.",
                    "label": 0
                },
                {
                    "sent": "Some of critical remarks about how tough these methods are not necessarily well suited to solving some of the SVM problems.",
                    "label": 0
                },
                {
                    "sent": "I would like to defend in this talk interior point methods that they can deliver in SVM context.",
                    "label": 1
                },
                {
                    "sent": "It's a joint work with my PhD student Christian Woodson that he's a very.",
                    "label": 1
                },
                {
                    "sent": "Very clever guy.",
                    "label": 0
                },
                {
                    "sent": "I'm lucky to have such a clever.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Age these children.",
                    "label": 0
                },
                {
                    "sent": "Now the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "The talk will be divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "First, I'd like to convey some ideas of interior point methods to you and show this methodology in a nutshell.",
                    "label": 0
                },
                {
                    "sent": "Then I'll focus on support vector machine training and this is something that I'm a little bit embarrassed to talk to you about, because you probably know more than I do about it, so I'll really fly through it and then will join the two things together to see how interior point methods can be specialized for SVM training.",
                    "label": 1
                },
                {
                    "sent": "Some challenges still remain.",
                    "label": 0
                },
                {
                    "sent": "That's good, because otherwise we will be out of job at the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes it is.",
                    "label": 0
                },
                {
                    "sent": "Interior point methods.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Part one what are the elements of interior point method?",
                    "label": 1
                },
                {
                    "sent": "It's we combine three mathematical tricks.",
                    "label": 1
                },
                {
                    "sent": "The use of logarithmic barriers, the use of duality theory and the Newton method.",
                    "label": 0
                },
                {
                    "sent": "And if you want to see how this is done you can see this very nice book on which provides you with the theories of interior point methods.",
                    "label": 0
                },
                {
                    "sent": "This is Steve right and you can look at this survey paper which discusses the implementation of tricks necessary to make this methodology working efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Log Arhythmic barrier has this nice feature that if the argument of the logarithm of the logarithm goes to zero, which I won't be able to show unless I jump here, then minus logarithm shoots to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In other words, it creates a barrier that makes it impossible for the inequality to become active to become an equation.",
                    "label": 0
                },
                {
                    "sent": "Now look at the transformation.",
                    "label": 0
                },
                {
                    "sent": "If we replace V greater than zero with the minus logarithm, and if we minimize this sum.",
                    "label": 0
                },
                {
                    "sent": "Which is equivalent to minimizing the exponent.",
                    "label": 1
                },
                {
                    "sent": "Then we effectively maximize the product of all V eyes.",
                    "label": 0
                },
                {
                    "sent": "This means that we are keeping the point far away from zero with respect to all the components V. And this means we are keeping the point in the interior.",
                    "label": 0
                },
                {
                    "sent": "Going slowly if you minimize this, you minimize the sum of logarithms about E to the logarithm is, of course VI.",
                    "label": 0
                },
                {
                    "sent": "If you minimize that, you want to maximize E to the plus and E to the plus is the same as that.",
                    "label": 0
                },
                {
                    "sent": "Fried.",
                    "label": 0
                },
                {
                    "sent": "So longer ethnic barriers are the tools to create interior point method.",
                    "label": 1
                },
                {
                    "sent": "That is, are the tools.",
                    "label": 0
                },
                {
                    "sent": "Logarithmic barrier is the tool that will throw away the algorithm into the interior far away from the boundary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, let's have a look how it works for quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "I won't be able to jump that that that high.",
                    "label": 0
                },
                {
                    "sent": "Now we start with a quadratic programming problem in which inequality's are present the greater than or equal.",
                    "label": 0
                },
                {
                    "sent": "Than 0 and we replace this inequality where the logarithmic barrier which appears here in the objective.",
                    "label": 0
                },
                {
                    "sent": "That way we got rid of certain difficulty in optimization.",
                    "label": 0
                },
                {
                    "sent": "The difficulty consists in deciding whether the constraint is active at the optimum or inactive.",
                    "label": 0
                },
                {
                    "sent": "That's the big trouble of active set methods.",
                    "label": 0
                },
                {
                    "sent": "The whole class of optimization methods.",
                    "label": 0
                },
                {
                    "sent": "Now for this problem will formulate the Lagrangian which has the whole objective function and LaGrange times the constraints.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the trick known from the end of 18th century LaGrange contribution to mathematics.",
                    "label": 0
                },
                {
                    "sent": "Now for this Lagrangian we will now write optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "That is, you know that for Lagrangian we will be looking for a saddle point.",
                    "label": 0
                },
                {
                    "sent": "That is the point which minimizes the Lagrangian with respect to V and maximizes with respect to LaGrange multiplier Lambda.",
                    "label": 0
                },
                {
                    "sent": "To find this point, we will differentiate this function now quick differentiation with respect to V should give me C + Q V. A transpose, Lambda.",
                    "label": 0
                },
                {
                    "sent": "What are we going to get from there?",
                    "label": 0
                },
                {
                    "sent": "1 / V times.",
                    "label": 0
                },
                {
                    "sent": "The barrier parameter mean.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what you are going to see now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because I computed the derivatives with respect to V and Lambda, the derivative with Lambda is trivial and we now have the.",
                    "label": 0
                },
                {
                    "sent": "The system of two equations in the top equation we will make this substitution.",
                    "label": 0
                },
                {
                    "sent": "We will define that the vector me Times V inverse E is equal to a small S. By the way, there is a clever notation that I'm using this capital V or capital V inverse means that this is a diagonal matrix on the diagonal of the matrix.",
                    "label": 0
                },
                {
                    "sent": "There is a vector sitting.",
                    "label": 0
                },
                {
                    "sent": "So if we write Capital V. Like that, it simply means V1V2 up to VN.",
                    "label": 0
                },
                {
                    "sent": "If we write it with an inverse, this will be inverses of the of the elements of the vector.",
                    "label": 0
                },
                {
                    "sent": "So substitute this S and then you will see that the upper equation becomes this one.",
                    "label": 0
                },
                {
                    "sent": "The second equation becomes this one, and Additionally we have this W. Sorry v * S. Times the vector of ones is equal to me.",
                    "label": 1
                },
                {
                    "sent": "Kind of the vector of ones and here I need.",
                    "label": 0
                },
                {
                    "sent": "I owe you an explanation for this.",
                    "label": 0
                },
                {
                    "sent": "Means is the same that VI times SI is equal to me for all I.",
                    "label": 0
                },
                {
                    "sent": "From one to two M OK.",
                    "label": 0
                },
                {
                    "sent": "So it's another way of writing something that is always a problem in optimization, which is called complementarity condition.",
                    "label": 0
                },
                {
                    "sent": "What would you do if I asked you to solve this system of equations?",
                    "label": 0
                },
                {
                    "sent": "What would be your favorite method to do it?",
                    "label": 0
                },
                {
                    "sent": "Come on, wake up.",
                    "label": 0
                },
                {
                    "sent": "It's not that late yet.",
                    "label": 0
                },
                {
                    "sent": "Oh no, we are only.",
                    "label": 0
                },
                {
                    "sent": "We are deriving interior point method.",
                    "label": 0
                },
                {
                    "sent": "You have a system of nonlinear equations.",
                    "label": 0
                },
                {
                    "sent": "How do you tackle it in mathematics Newton method we are going back to 16th century, no 17th century, no.",
                    "label": 0
                },
                {
                    "sent": "OK, except that probably Newton did not think of that large problems but but otherwise we owe it to him.",
                    "label": 0
                },
                {
                    "sent": "So this is the system of equations where the first 2 equations are linear with respect to unknowns, Lambda, SNV.",
                    "label": 0
                },
                {
                    "sent": "But the third one is Biliner is mildly nonlinear, and we can solve it using Newton method.",
                    "label": 0
                },
                {
                    "sent": "So let us now derive Newton method for the system.",
                    "label": 1
                },
                {
                    "sent": "Newton method will consist in taking the whole function here and differentiating it with respect to V, Lambda and S. So for a moment think about the derivative of the first row.",
                    "label": 1
                },
                {
                    "sent": "There will be only one non zero element.",
                    "label": 0
                },
                {
                    "sent": "This is the derivative with V, there will be zero and zero.",
                    "label": 0
                },
                {
                    "sent": "The derivatives with respect to.",
                    "label": 0
                },
                {
                    "sent": "Lambda and S right.",
                    "label": 0
                },
                {
                    "sent": "Let me show.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up to you.",
                    "label": 0
                },
                {
                    "sent": "That's it, that's what we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's what we obtain if we derive the Newton method, we have the derivative.",
                    "label": 0
                },
                {
                    "sent": "Of the function F with respect to all the three arguments.",
                    "label": 0
                },
                {
                    "sent": "And it's given to you here.",
                    "label": 0
                },
                {
                    "sent": "It's a big matrix, but this now means that we can write.",
                    "label": 0
                },
                {
                    "sent": "The system of linear equations, the solution of which will give us Newton direction which is supposed to take you closer to the solution of the nonlinear system of equations.",
                    "label": 1
                },
                {
                    "sent": "Under the conditions that I'm sloppy about and not telling what conditions are necessary to achieve it, but believe me they are satisfied here.",
                    "label": 0
                },
                {
                    "sent": "So we made the following steps.",
                    "label": 0
                },
                {
                    "sent": "We took the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We introduced logarithmic barrier to replace inequality's.",
                    "label": 0
                },
                {
                    "sent": "Then we wrote Lagrangian function for the problem.",
                    "label": 0
                },
                {
                    "sent": "Then we differentiate Lagrangian function to find 1st order optimality conditions and now we apply Newton method to solve these first order optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "That's the key concept of interior point method.",
                    "label": 0
                },
                {
                    "sent": "Now the question why this should?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work better than the well known active set method.",
                    "label": 0
                },
                {
                    "sent": "Now the difference is hidden in the fact what do we do with the complementarity condition?",
                    "label": 0
                },
                {
                    "sent": "Are we forcing this?",
                    "label": 0
                },
                {
                    "sent": "To be zero from the start.",
                    "label": 0
                },
                {
                    "sent": "Or are we doing that?",
                    "label": 0
                },
                {
                    "sent": "Hey, if we do active set method then we have to guess how complementarity is satisfied and the gas is inevitably come combinatorial in spirit because we have to choose whether.",
                    "label": 0
                },
                {
                    "sent": "It's actually not XSV whether V is equal to 0 or S is equal to 0, while if we do it through interior Point methodology, we have a proper mathematical tool.",
                    "label": 0
                },
                {
                    "sent": "We have a perturbation and we will drive the perturbation to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is the essential difference between what happens in the active set.",
                    "label": 0
                },
                {
                    "sent": "What happens in interior point methods?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has far reaching consequences this thing because active set methods suffer from non non polynomial convergence non polynomial convergence because you always do this dreadful combinatorial operation, choose subset from a set which is which has exponentially many possibilities to be realized while in interior Point method we force that all the products.",
                    "label": 0
                },
                {
                    "sent": "Go to zero at the same rate and only at optimality.",
                    "label": 0
                },
                {
                    "sent": "That is only when me really approaches zero, we will reach the optimum.",
                    "label": 0
                },
                {
                    "sent": "And this is really a hope that interior point methods should work well whenever the size of the problem gets larger and larger, because when the size of the problem gets larger, the more difficult it is to guess correctly the optimal partition.",
                    "label": 0
                },
                {
                    "sent": "My argument here would be the following.",
                    "label": 0
                },
                {
                    "sent": "If you use active set methods, you are guessing if you use interior point methods you use proper epsilon mathematics.",
                    "label": 0
                },
                {
                    "sent": "You have the proofs that give you convergence.",
                    "label": 0
                },
                {
                    "sent": "You have the proper analysis of what's going on with the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now back to the Newton method.",
                    "label": 1
                },
                {
                    "sent": "You remember I derived this thing or I showed you quickly how it looks like this is.",
                    "label": 0
                },
                {
                    "sent": "The equation that gives me Newton direction, but of course it's a big equipped big linear system that can be solved if we eliminate.",
                    "label": 0
                },
                {
                    "sent": "Some of the easier elements of the vector from here.",
                    "label": 0
                },
                {
                    "sent": "This is diagonal.",
                    "label": 0
                },
                {
                    "sent": "Let me remind you, this is diagonal.",
                    "label": 0
                },
                {
                    "sent": "This is even better.",
                    "label": 0
                },
                {
                    "sent": "This is identity, so we can quickly eliminate Delta S from here and reduce the system and write it only with respect to Delta V and Delta.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uncle.",
                    "label": 0
                },
                {
                    "sent": "That's what happens if we do that.",
                    "label": 0
                },
                {
                    "sent": "And now the first trouble appears.",
                    "label": 0
                },
                {
                    "sent": "The trouble is that this operation requires creating such a matrix, and the matrix has very bad feature because it is V / S componentwise.",
                    "label": 0
                },
                {
                    "sent": "And this means that let me remind you either element of V or element of S has to go to 0.",
                    "label": 0
                },
                {
                    "sent": "So we will have either zero in the numerator or a zero in the denominator.",
                    "label": 0
                },
                {
                    "sent": "Well, that's dreadful because it means we will have the splitting between the elements.",
                    "label": 0
                },
                {
                    "sent": "Some of them will go to Infinity, others will go to zero.",
                    "label": 0
                },
                {
                    "sent": "And this is bad for linear algebra people, because this immediately means that we will have condition number of the system that goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the textbook.",
                    "label": 0
                },
                {
                    "sent": "Analysis of the accuracy.",
                    "label": 0
                },
                {
                    "sent": "You have to give up and you have to accept that there is no hope to find solutions to the system of linear equations, the condition of which is going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But in practice we are working in such subspaces that it is possible to find the solution of the system and the solution will be actually sufficiently accurate to take us to optimality.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, with this augmented system, I'd like to immediately draw your attention to certain thing that we have the matrix Q which corresponds to the quadratic form in the quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "But if this matrix Q.",
                    "label": 0
                },
                {
                    "sent": "Where diagonal.",
                    "label": 0
                },
                {
                    "sent": "Then actually we would have diagonal plus diagonal.",
                    "label": 0
                },
                {
                    "sent": "We would easily invert this matrix.",
                    "label": 0
                },
                {
                    "sent": "We would have a very easy one.",
                    "label": 0
                },
                {
                    "sent": "One block in our two by two matrix, right?",
                    "label": 0
                },
                {
                    "sent": "We could then eliminate this block.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, this would be the normal equation system that we have to solve for Delta Lambda.",
                    "label": 0
                },
                {
                    "sent": "Then having computed Delta Lambda we would compute Delta V and we have to go one slide backward to compute Delta S and have the complete Newton direction.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So bear in mind I would love to have diagonal matrix in my QP.",
                    "label": 0
                },
                {
                    "sent": "Because if.",
                    "label": 0
                },
                {
                    "sent": "The matrix in the quadratic form in the quadratic optimization is diagonal.",
                    "label": 0
                },
                {
                    "sent": "This makes the thing as easy as in linear programming problem.",
                    "label": 0
                },
                {
                    "sent": "Such problems have the name in optimization they are called separable and then.",
                    "label": 0
                },
                {
                    "sent": "That's that's the reason why separable problems are considered to be easy, and this is the reason why I will try to reformulate all known as VM problems to separable kewpies.",
                    "label": 0
                },
                {
                    "sent": "Hoping that if I manage to do that, I will make them easy to solve.",
                    "label": 0
                },
                {
                    "sent": "One quick remark about this thing.",
                    "label": 0
                },
                {
                    "sent": "If we didn't have the matrix D. If this was a general sparse matrix, the inversion here would be a disaster and you should never invert a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Presure Dylan told you clearly that mathematicians sometimes write the inverse of the matrix and think this this is inverse of the matrix times the right hand side gives you the solution of the problem on paper.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In in practical computations it's never the case we do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute inverses, here's an example of a very simple matrix which has nearly three non zero elements per column.",
                    "label": 0
                },
                {
                    "sent": "It's a band matrix, but if you invert it you obtain a completely dense matrix.",
                    "label": 0
                },
                {
                    "sent": "The last column of which seems to disappear here.",
                    "label": 0
                },
                {
                    "sent": "So you could have the same example for a much larger dimension.",
                    "label": 0
                },
                {
                    "sent": "This is the warning.",
                    "label": 0
                },
                {
                    "sent": "Don't don't invert sparse matrices because they are inverse, maybe dense.",
                    "label": 0
                },
                {
                    "sent": "Instead of inverting them, compute and.",
                    "label": 0
                },
                {
                    "sent": "Cholesky or Lup decomposition for such a matrix, and then you can really solve the system, but it will be much, much easier if this was a diagonal matrix, wouldn't it?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, let me wrap up the thing about the interior point methods.",
                    "label": 1
                },
                {
                    "sent": "Why these methods offer such a potential?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, there is a well established theory which says that they converge in the number of iterations, which is proportional to the problem dimension.",
                    "label": 0
                },
                {
                    "sent": "Or may even be proportional to the square root of the problem dimension.",
                    "label": 0
                },
                {
                    "sent": "That's fantastic.",
                    "label": 0
                },
                {
                    "sent": "In practice they behave much, much better than this worst case analysis.",
                    "label": 0
                },
                {
                    "sent": "In practice, they converge in the number of iterations between 10 and 20, unless you go for a size of 1 billion, then you may have to do 50 iterations.",
                    "label": 0
                },
                {
                    "sent": "Let's log N number of iterations.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This optimistic part of the story, the pessimistic part of the story, is that one iteration may be expensive.",
                    "label": 1
                },
                {
                    "sent": "So it does not necessarily mean that we are that fast.",
                    "label": 0
                },
                {
                    "sent": "However, the larger the problem, the bigger the chance that into your point will beat the competition.",
                    "label": 1
                },
                {
                    "sent": "The numerical kernel of the computations is hidden here for quadratic programming problems we have to build this matrix.",
                    "label": 1
                },
                {
                    "sent": "And then we have to compute Cholesky for this matrix.",
                    "label": 0
                },
                {
                    "sent": "Now, if we assume that matrix A is a dense matrix with N rows and then and a lot of columns and columns, then the complexity of building this would be NM squared and the decomposition will be M ^3.",
                    "label": 0
                },
                {
                    "sent": "This calligraphic oh means complexity.",
                    "label": 0
                },
                {
                    "sent": "Well, you know.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To summarize, if we want to solve quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "With interior Point, we would like the problem to be separable, that is, to have diagonal matrix in the quadratic form and then.",
                    "label": 0
                },
                {
                    "sent": "That's the effort we have to expect per iteration of Interior Point method, and we hope to do very few iterations and find the optimal solution.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A comment in conditioning is always present here because this in conditioning is the result of what happens to theater.",
                    "label": 0
                },
                {
                    "sent": "Let me remind you some elements of Theatre.",
                    "label": 0
                },
                {
                    "sent": "Go to zero, others go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You cannot really do anything about it, but there is a nice paper of Goldfarb and Scheinberg, which tells you that actually, it's it does not hurt that much because you can build Cholesky decomposition and all instability of Theta.",
                    "label": 0
                },
                {
                    "sent": "Will only be displayed in Lambda.",
                    "label": 0
                },
                {
                    "sent": "The diagonal matrix, while else we will while the matrix will look very nice, will have a very nicely bounded condition number.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Let me summarize interior point methods.",
                    "label": 0
                },
                {
                    "sent": "I like them because and I hope you will like them as well because they offer they are polynomial algorithms.",
                    "label": 0
                },
                {
                    "sent": "They have provable first case complexity.",
                    "label": 0
                },
                {
                    "sent": "They have excellent practical behavior.",
                    "label": 0
                },
                {
                    "sent": "They are competitive for small problems when the size of the problem is below 1 million and they practically have no competition if you go to higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "No, this is obviously an opportunity for SVM training because in SVM training we sometimes have very large data sets and we naturally have dense data set, and these computations.",
                    "label": 1
                },
                {
                    "sent": "So somehow a marriage of these two things almost imposes itself it's a marriage.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of convenience.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines.",
                    "label": 0
                },
                {
                    "sent": "This will be 1 slide, one second, second slide, second another one second OK Huawei because Corey, nor did a great job afew days ago.",
                    "label": 0
                },
                {
                    "sent": "She explained all the reminded you all the details about how support vector machines for linear support vector machines are defined.",
                    "label": 1
                },
                {
                    "sent": "What is this idea of finding a good mark?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And separating.",
                    "label": 0
                },
                {
                    "sent": "Bad and good points, so let me go very quickly.",
                    "label": 0
                },
                {
                    "sent": "We have a classification problem.",
                    "label": 0
                },
                {
                    "sent": "We have a set of points and we would like classify them into good and bad guys and we hope the intersection between good and bad guys is empty.",
                    "label": 1
                },
                {
                    "sent": "So we are looking for a function that will take positive values for good guys, negative values for bad guys.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose the set is linearly separable.",
                    "label": 0
                },
                {
                    "sent": "That is, we can find the hyperplane that separates button good guys.",
                    "label": 0
                },
                {
                    "sent": "Then we could have it this decision rule that if F is positive, we give a label plus one to the point saying good or if negative minus one saying it's bad.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me jump through the next one, OK?",
                    "label": 0
                },
                {
                    "sent": "So we have the separating hyperplane and we have this inequality which is true for all good points and another one which is good for all bad points.",
                    "label": 1
                },
                {
                    "sent": "I deliberately normalized this vector because this gives me a proper definition of the distance between good and bad guys and the distance is of course this minus that, which gives Me 2 divided by the normal W. Now you want the distance of separation to be as large as possible, so you want to maximize the distance of separation, which is equivalent to minimizing the denominator here.",
                    "label": 1
                },
                {
                    "sent": "So we minimize the norm.",
                    "label": 0
                },
                {
                    "sent": "Now if we minimize the quadratic norm, it's the same as minimizing the square of the quadratic norm.",
                    "label": 0
                },
                {
                    "sent": "Only it makes our life easier, because let me remind you, we all prefer to differentiate the quadratic function rather than the square root of the quadratic function.",
                    "label": 0
                },
                {
                    "sent": "From the secondary school you should remember it was easier to differentiate the quadratic rather than the square root of quadratic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So QP formulation here looks like that we minimize this norm of square root of norm of W, subject to lots of constraints.",
                    "label": 1
                },
                {
                    "sent": "Well, a quick warning is needed here.",
                    "label": 0
                },
                {
                    "sent": "Usually problems are not that nice.",
                    "label": 0
                },
                {
                    "sent": "For example, we do not have a perfectly separable set of points, some of them are misclassified and we have to add penalty for misclassification.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we cannot really separate things with linear hyperplane, we need to do something more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 2 pictures to clarify that if we do not have a linearly separable point because the B, which is a bad point, is somewhere among the good guys, then we will penalize for the distance which is measured with C. The distance from the separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Or a more complicated situation.",
                    "label": 0
                },
                {
                    "sent": "You cannot draw a straight line to separate them.",
                    "label": 0
                },
                {
                    "sent": "You need to transform.",
                    "label": 0
                },
                {
                    "sent": "You need to find the transformation fee that from this separable set will transform it to the new space in which linear separation will be possible.",
                    "label": 0
                },
                {
                    "sent": "And this is done by the kernel.",
                    "label": 0
                },
                {
                    "sent": "Function well here, but the feature map, but it will.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the kernel function.",
                    "label": 0
                },
                {
                    "sent": "If we have linearly non separable case then we are these panel disk C. To every misclassified point.",
                    "label": 0
                },
                {
                    "sent": "And now our inequality looks like that that see compensates for the case when without see, we would never be able to satisfy the inequality.",
                    "label": 0
                },
                {
                    "sent": "Overall, we now want to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "With this minimize the norm.",
                    "label": 0
                },
                {
                    "sent": "Of W and at the same time we would like to minimize the the total penalty for misclassification.",
                    "label": 0
                },
                {
                    "sent": "We combine the two objectives into a single objective and this gives you the I would say Canonical formulation of the classification problem.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can derive the dual.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Go quickly through it.",
                    "label": 0
                },
                {
                    "sent": "That's almost identical to the to the talk that Karina gave.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two days ago, three days ago.",
                    "label": 0
                },
                {
                    "sent": "Here's the duel.",
                    "label": 0
                },
                {
                    "sent": "I'm really sorry to say it, but it's an impossible for an optimizer to use Alpha as a variable.",
                    "label": 0
                },
                {
                    "sent": "I I made an effort, I put it on the slide, but then I just simply couldn't couldn't look at it, so I needed to change to remove.",
                    "label": 0
                },
                {
                    "sent": "That offers a step size in every book on optimization, so let's stick to that.",
                    "label": 0
                },
                {
                    "sent": "That is the Alpha and the usual notation of SVM community.",
                    "label": 0
                },
                {
                    "sent": "Here's the door optimization problem in which we want to maximize this function, and the function is concave.",
                    "label": 0
                },
                {
                    "sent": "So this is a good thing.",
                    "label": 0
                },
                {
                    "sent": "We minimize the convex function or maximize the concave function that's convex optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can write it in this world in this way.",
                    "label": 0
                },
                {
                    "sent": "In matrix notation that we know maximize a linear form minus the quadratic form, subject to just a single constraint and bars.",
                    "label": 0
                }
            ]
        }
    }
}