{
    "id": "vm6mzexuh6746g52lbeo6l7yu6xf35w7",
    "title": "Message-Passing Algorithms for GMRFs and Non-Linear Optimization",
    "info": {
        "author": [
            "Jason K. Johnson, Center for Future Civic Media, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Feb. 1, 2008",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/abi07_johnson_mpa/",
    "segmentation": [
        [
            "Working with Alan Wilsky and also parts of this joint work with two other of his students, Dimitri Molotov, and then catch on mistaken.",
            "And basically we really focused on working in Gaussian models a lot, so I have some preliminary work here at the end where we just try to apply these ideas to now."
        ],
        [
            "On your problem.",
            "So here's the three main components of the talk.",
            "In the first part, we're going to talk about what we call walk.",
            "Some view of inference in Gaussian models, and we've applied this to look at convergence of several iterative algorithms for inferencing, Gaussian models, and also one new thing.",
            "Basically a modified form of belief propagation that works for non walkable models.",
            "And I'll explain what that means more later.",
            "And the second component is apparently different approach, but it ends up having very similar convergence conditions.",
            "Lagrangian relaxation methods for Gaussian models.",
            "And I'll describe that in more detail later, But basically there's a convergent with what I call maximum diffusion for the Gaussian model, and this is actually all those for Gaussian models is closely related to recent developments in math estimation for discrete models, namely Martin Martin Wainwright, St Related, Max product approach and related LP relaxations of math estimation, and as I said in the last part, will play around with applying these two so."
        ],
        [
            "Nonlinear problems.",
            "OK, so a Gaussian MRF you're familiar with the multivariate Gaussian distribution with the mean X hat and covariance P. But if you're dealing with a graphical model, it's more natural to represent this in terms of this inverse of the covariance matrix information matrix, because that would be sparse to represent the the structure of the model.",
            "The graphical structure of the field and a linear term H here, which represents usually measurements.",
            "So in principle inferencing, Gaussian models is straightforward.",
            "You can do it using Gaussian elimination, but it involves inverting.",
            "Inverting the matrix, which in general has cubic complexity in the size of the field, and even using more efficient versions such as junction tree algorithms, are sparse Cholesky factorization.",
            "The inference would be cubic in the tree with the graph, and so for many practical problems you're forced to use iterative methods if you want to have a fast approach.",
            "Anna."
        ],
        [
            "The focus of our talk.",
            "So this walk some picture starts with well known series, the Newman Series for matrix inverse, so we're going to take the model.",
            "We assume that the variables are scaled so that this information matrix has a unit diagonal.",
            "Then the off diagonal entries are actually partial correlation coefficients defined on the edges of the graph, and this is a natural measure of the interaction between two variables independent of the other variables.",
            "Well, the new in series.",
            "Is is this guy?",
            "So if we want to compute the covariance matrix inverse of the J matrix, the inverse of this I -- R, that's a power series in R. Which converges at the spectral radius of R is less than one.",
            "If you look at the components of this powers of our it as a simple interpretation in terms of sums over walks in the graph where each walk is weighted by a product of these correlation coefficients along those edges."
        ],
        [
            "Um?",
            "OK, so this suggests to walk some view of inference where you describe the covariance between two nodes as a sum over all walks in the graph from I to J of this weight of the walk, which is the product of the edge weights an based on that I also have an interpretation for the estimates, the means, which is just a related walk some, but now you walk from any point in the graph to a node and each walk is weighted by this field value at the start of the walk.",
            "And here's a simple example of this illustrate this if you have a three node graph, these are some of the walks that you would need to compute to get the estimate node one.",
            "They can start from anywhere, wander around the graph and edit node one, and each walk is weighted by the H at the start and also if you look at the variance is a set of self return blocks, so the variance of this node would be all walks.",
            "It should begin their wandering around and come back, and they're allowed to backtrack, go around cycles, and so forth.",
            "But these sums of these walk psalms or psalms over countably infinite sets of walks.",
            "Even in this simple graph, and we want to interpret this in a very loose way.",
            "Basically view these unordered slums and the condition you need for that is absolute convergence of these series.",
            "So we call these."
        ],
        [
            "Models walk symbol.",
            "And we drive the variety of conditions which are equivalent to this walk summability condition.",
            "The simplest is if you take the absolute values of these partial correlation coefficients, they can be between plus and minus one, then that matrix of edge weights have to have spectral spectral radius less than one.",
            "It turns out using pairing for Venus Theory show this is equivalent to the condition that if you negate any negative edges to make all the edges positive that the model is still valid is still a positive definite matrix.",
            "Another condition we called pairwise normalizable is if you can write J as a sum of smaller two by two matrices on each edge of the graph, where each of those is itself positive definite, that's equivalent.",
            "And it also turns out to be related to generalized notion of diagonal dominance if there exists a rescaling so that Jay would become diagonally dominant in the models, walk summable.",
            "And this condition actually is a corollary implies that any non frustrated model is walks onnable.",
            "So that means if you have a valid model and every cycle has an even number of negative edge weights, it's walk summable and this is trivial.",
            "Cases that interactive model with all positive partial correlation coefficients or a tree as well some more.",
            "And these sorts of models."
        ],
        [
            "Arise often in practice.",
            "So.",
            "The first application of this was joint work with Mitchell, Eutaw and we looked at the walk.",
            "Some interpretation of Gaussian belief propagation.",
            "And it turns out that each message in Gaussian BP has a simple interpretation as computing two walk sums.",
            "So basically when you predict from I to J, you first prepare a message summarized by these two numbers, an interpretation of Alpha is itself the sum of all self return walks too.",
            "I but do not visit J.",
            "And beta is so this is relevant to the variance calculations which are self return lock.",
            "Subs in beta is the HP weighted sum of all walks to note I also excluding J. Ann, just thinking of these in terms of walk sums and using purely sort of common tutorial ideas, you can derive recursive set of equations for computing these walks ONS recursively.",
            "Ann, I won't go through this in detail, but it's difficult to what you the Gaussian elimination formulas RVP equations on the tree.",
            "And also from these messages you can compute the variance and the estimate each node of the graph."
        ],
        [
            "Um?",
            "OK, so let's consider what this does when you apply this to a loopy BP on a loopy graph.",
            "So we have some graph here, and we run loopy BP, which basically acts like it's a tree.",
            "So therefore the algorithm is actually equivalent to doing inference on the computation tree.",
            "Applying this walk, some picture is pretty simple.",
            "Interpret what's going on.",
            "Basically the self return walks, which is what you need to compute for variances.",
            "You get some of these walks, but not all of 'em.",
            "So if you consider the walk that goes 1231 in the computation tree, there is such a walk, it goes 1231, but it ends at a different node, not at the root of the tree, and because of that that's not attributed to the variance in loopy belief propagation.",
            "But some some walk such 121 or 1231321, they do get included.",
            "So basically the variances that BP we compute are subset of the correct variances.",
            "So that's the bad news.",
            "The good news though, is that all of these single directional locks basically walks 2 J or you don't care where they started from.",
            "That's still correct.",
            "Even with cycles you have 1231.",
            "Well, there is such a walk here one.",
            "I'll say 13211321 there, so basically it doesn't have to start.",
            "It can start a replica of the root node in the computation tree.",
            "And because of that you get all the locks eventually for the meeting calculations.",
            "And given that this has a walk, some interpretation that walks some ability property implies that Gaussian belief propagation asked to converge in a walk symbol model."
        ],
        [
            "The means are correct.",
            "Um?",
            "So that only works for walk Summable models and a few models a little bit outside of the class of walks on bull, but is simple to modify.",
            "This actually get a conversion algorithm that works more generally, and the idea is so here we have a J matrix which is not walk symbol if you add some diagonal loading to the J matrix, increase the diagonal terms.",
            "You can get something which is walk solvable, and so therefore you could use Gaussian belief propagation to implement this type of iteration.",
            "Basic Gaussian BP will compute this.",
            "This matrix inverse time something, but it would compute the wrong answer, but you can correct for that by adding this feedback loop scaled by gamma, and it's pretty straightforward to basically implement this double loop algorithm, but by a single loop where you basically put this feedback into a damped update of the H vector.",
            "Here in this last line.",
            "For Lambda small enough, this leads to a conversion algorithm for non walks on bull models.",
            "The only catch is is move far away from the walk symbol class.",
            "You would have to do more diagonal loading and its outer loop converge more slowly so the conversion."
        ],
        [
            "Can get slow.",
            "Here's another algorithm we've analyzed from this point of view.",
            "The walks on walks on the embedded trees algorithm developed by Eric Sudderth and Martin Wainwright.",
            "So basically, it's an example of a classical iteration based on into method of this form or user preconditioner.",
            "Their idea was to use embedded trees as preconditioners.",
            "So if you have some graph, you can take some invented tree and basically by sitting setting the cut edges to 0, use that as a precondition in this algorithm.",
            "And you can change preconditioners overtime to try to improve convergence.",
            "Well, we basically just went back and looked at this from this walk.",
            "Some point if you are able to interpret these estimates precisely as having a walk, some interpretation, and because of that, it's showing that these types of methods would converge and walks on will models using any."
        ],
        [
            "Sequence of trees.",
            "But the idea was to try to also use this this analysis to guide the selection of trees.",
            "So we came up with a simple strategy for picking trees, which actually is reduces to Max weight spanning tree algorithm which is efficiently implemented.",
            "An has a walk.",
            "Some interpretation.",
            "Basically this is minimizing a upper bound on the estimation error you step and comparing to just one tree iteration or a two tree iteration alternating between 2 trees.",
            "We found this adaptive approach can accelerate convergence."
        ],
        [
            "OK, so now I'm going to switch gears and talk about another method which is Lagrangian relaxation and is a starting point that the idea is you start with intractable graph and you break the graph up into smaller pieces which are trackable.",
            "The simplest being just individual edges or small cycles of the graph or larger blocks."
        ],
        [
            "Graph.",
            "You also could use things thin subgraphs, basically because inference on these is fast.",
            "Using recursive methods are trees, and so their approach and we describe is actually if you use trees, is essentially equivalent to this TMP approach that Martin Wainwright developed, although that was for."
        ],
        [
            "Screen models.",
            "So here's how it looks for Gaussian models.",
            "So, given such a decomposition in the graph G into a tractable set of graphs G prime.",
            "We also have to split these quadratic form describing the PDF into local pieces defined on each sub graph and subject to these constraints that if you re some those pieces you would recover the same model.",
            "Also, there's a condition here, which is that each of those subgraphs has to be positive definite, so this actually generalizes this pairwise, normalizable idea.",
            "So if you had a splitting based on just edges, this would only be possible for pairwise normalizable models.",
            "But this approach is sound for models which have larger size potentials.",
            "So then the map estimation problem.",
            "You can reformulate it rather than maximizing this.",
            "You can maximize this augmented function, but now subject to a set of linear constraints.",
            "Which is that the replicating nodes should all be consistent.",
            "Well, by relaxing those constraints, you get a tractable dual convex dual problem, and assuming that this splitting is possible is convex, and therefore there's strong duality.",
            "So this leads to an exact map estimate."
        ],
        [
            "Another interpretation which relates more to approximate inference pictures.",
            "You can instead solve this problem, which is where you compute the log partition function on each of these subgraphs and you try to minimize that, but subject to these constraints that the potentials are in this linear subspace and also that the individual subgraphs are positive definite.",
            "And it has a dual problem which is relates to these Gibbs free energies, approximations that people.",
            "Are used to do variational inference.",
            "And I'll."
        ],
        [
            "Skip that though.",
            "So here's the algorithm we can use to solve."
        ],
        [
            "Problems will actually go back for one second, so these kind of problems can be solved using iterative scaling methods, which are, like for instance in proportional fitting is a special case of SARS.",
            "Iterative scaling algorithm is basically a projection algorithm which imposes these marginal consistency constraints in this problem."
        ],
        [
            "And this algorithm I'm describing here can be interpreted in that way.",
            "But it's a very simple algorithm.",
            "Basically you pick some subset S which is contained in the intersection of multiple clusters that you used to break up the graph, and then for each sub graph which contains that cluster in that augmenting graph, G prime, you compute its marginal just using Gaussian elimination within that trackable sub graph.",
            "Then by averaging these marginals and potential domain and doing an update that ensures that basically forces all the marginals degree after the update.",
            "And this update also has the property that maintains a valid parameterisation.",
            "So it stays within that linear subspace of valid representations on G prime.",
            "And by iterating over the different subsets S, you converge to the optimal solution of these prob."
        ],
        [
            "So I described.",
            "So here's an example where I used a 64 by 64 thin plate model which basically is a GM RF which has local potentials defined on subsets of five nodes and it's related to the Laplacian and tries to minimize the curvature of the field and added just basically random measurements and smooth smooth annoys using the basic version of LR, which are basically sort of five cliques of each node is 4 neighbors.",
            "This was the convergence rate to use larger block sizes, you can accelerate the.",
            "Convergence rate.",
            "But one catch here is if you have really strong correlations in the field, any pretty much any iterative method you try to use is going to slow down, but in classical linear algebra literature there's been work on multigrid methods to try to accelerate convergence and such."
        ],
        [
            "Quations and so I try to come up with a version of that for this framework and basically the idea is similar where you, but you first reformulate define scale model at multiple scales.",
            "Subject across scale constraints.",
            "And then you relax those cross scale constraints and also at each scale break it up using this decomposition idea and then relaxing these constraints and running it are valid."
        ],
        [
            "You can solve this.",
            "There's a generalization of this maximum diffusion math."
        ],
        [
            "But I'll skip that slide.",
            "But it can lead to some improvement in the rate of convergence, and I this is sort of a toy problem, but I suspect for much larger problems that you can is more critical to use these types of methods."
        ],
        [
            "OK, so for the last part of the talk I just wanted to apply this to something some non Gaussian problem and so it's not a complicated idea.",
            "Just basically applying these methods to solve for the Newton step in Newton's method or in this levenberg Marquardt method, if the problem is not kind of X1 observation is that if you have a objective function which decomposes into local functions which are convex then this guy is going to satisfy this this.",
            "Normalise ability condition required by Gaussian belief propagation and by Lagrangian relaxation.",
            "So that's a really nice class of models.",
            "For instance, if this is a paralyzed paralyzed graph with pairwise interactions, you can use Gaussian belief propagation to solve systems based on this guy.",
            "But if you try to use non convex potentials, you can run into problems.",
            "But loving Bergmark Court would normally just.",
            "The goal would be to add regularization to guarantee that H is positive definite so that you can invert it.",
            "Well here, similar as in the stabilized version of Gaussian belief propagation, we also make this strong enough to make sure the models walk summable, and then we can use these type."
        ],
        [
            "Some methods to infer the problem, and as an example, we looked at these half half quadratic edge preserving methods for image processing, where basically if this were a square, this would correspond to thin membrane model.",
            "But if you replace that quadratic penalty by some other penalty, it can have better edge preserving properties and but here we are assuming that we have a smooth set of smooth potential functions, so we're going to smooth approximation to this this.",
            "T The P penalty."
        ],
        [
            "And here's what the function will look like for P = 1.",
            "In varying this smoothing parameter beta, you get a sequence of better approximations to the absolute value function.",
            "Anfora P = 1/2.",
            "You get something which is non convex and more sharply peaked here."
        ],
        [
            "And so basically, I basically just applied this idea of using regularization to force the model to be well within the walk symbol class and then it's solving these iteratively.",
            "Basically that walk some condition ensures that pretty much any of the methods I described would converge very quickly at each step, like, say within 3030 iterations.",
            "And this is what the results I got using P = 1 for decreasing values of beta."
        ],
        [
            "And this was for P = 1/2 where you get a much sharper result here."
        ],
        [
            "OK, so in conclusion, there are many approaches that seem to work for walks on little models.",
            "Are this generalized class of of normalizable models based on subsets?",
            "We've looked at Gaussian belief propagation and embedded tree algorithms and this Lagrangian relaxation approach.",
            "Multiscale variations on these ideas can be important for fast convergence in large scale problems with strong long range correlations.",
            "And we also have looked at the idea of using these types of methods.",
            "Basically walk summable models and these corresponding algorithms to precondition harder problems which are either non walk summable or even nonlinear, and that seems to be a sound approach.",
            "Further work would be to sort of relax the distinction between inner and outer loop and these nonlinear methods to try to get a fully distributed message passing algorithm for, say, Newton's method.",
            "Another thing is these diagonal regularization methods.",
            "We I use this very simple choices of this here, but there's a lot of work to decide how much regularization should you use.",
            "Like for instance, once you get inside the walk symbol class, the further in that class you are, the faster these methods would converge.",
            "But then the outer loop will converge slower, so there's a tradeoff.",
            "Something that I didn't discuss here, but we're also interested in trying to get variance estimates within the walk symbol class.",
            "These algorithms compute reasonable variance estimates, but as you approach the boundary that class or move far away from that, especially if you use this diagonal loading, it's not clear if you're going to be able to get very assessments, but maybe there's some way to correct for these regularization methods to try to estimate the variances.",
            "This LR approach actually in our paper on this, which we recently presented at Allerton.",
            "We also discovered we just discussed discrete and Gaussian models separately, but the basic idea should work for hybrid models, although some of the algorithms were using would have to be modified substantially to work in that case.",
            "And one other idea that I'm interested in is maybe even using continuous approaches approaches to discrete problems, similar to how in this how these nonsmooth functions are approximated by a sequence of smooth functions.",
            "You can imagine having a family of functions which initially are unimodal, and then gradually split to focus on 2D functions like plus and minus one, and I'm curious if how well that would work.",
            "I mean, you can solve each of those using continuous methods, but you get a local.",
            "Local Maxima.",
            "That's that's it.",
            "Well, basically it works for any model, but the catch is that I mean so.",
            "Basically using a walk symbol model to precondition something else.",
            "And the more you have to modify the model to make it walk summable, the worse of preconditioner you have.",
            "So basically it will still converge that preconditioner.",
            "Yeah, greater convergence rate of convergence, like basically if you have to, if you have to use a lot of diagonal loading to make a model walk summable, that's probably going to give you a slow preconditioner, but it would be a stable preconditioner.",
            "This approach with the hybrid models I mean are you?",
            "Are you planning to do some projections?",
            "OK, so.",
            "Anne.",
            "I'm not sure.",
            "I mean, that's that's to be determined, but I mean, I think the basic.",
            "The convex optimization formulation would stand.",
            "But this maximum diffusion description?",
            "That's a really nice algorithm 'cause exact closed form solution for basically doing coordinate descent in this hybrid case.",
            "That's not the case, so you have to use a completely different optimization method.",
            "But I think the basic basic idea, the fundamental variational problem, would still be sound.",
            "It may be somewhat similar to EP type ideas, but.",
            "The approach I'm describing, unlike expectation propagation, so the slide on the variational methods with the free energy stuff.",
            "Basically there was some of local entropies there, and there was no negative terms, whereas like beta free energy would also have negative terms.",
            "So that seems to be what distinguishes that approach from other variational methods, but for map estimation that's that's OK, especially in Gaussian models.",
            "Check in.",
            "Switch.",
            "Should I put?",
            "The Castle.",
            "That's forward and backwards in the center, warns laser pointer.",
            "OK. OK, the final.",
            "Bounds on beta free energy production networks by Bolton Jack.",
            "Good evening I'm going to.",
            "I'm going to talk about Beta financial from the Gaussian networks.",
            "And this is joint work with Damascus.",
            "First, we are going to review some things about go simplification.",
            "And then we go for the direct minimization of the base of the energy for Gaussian network and set a lower bound for for that function.",
            "Much of the things about Gaussian belief propagation has already been said earlier.",
            "So I'm going to skip that point.",
            "The problem popped up when we were working with conditional Gaussian models.",
            "And it turns out that we don't really know much of the things are known about discrete models, But it turns out that not much is known about the beta free energy of a Gaussian network.",
            "So we are going to analyze that that thing.",
            "Well inferencing Gaussian models is.",
            "It can be used.",
            "It can be computed so important quantities can be computed.",
            "In a deterministic manner.",
            "However, it's very useful in case of very huge models is useful to use message passing algorithms.",
            "And then for three models it works fine, but it turns out that for graphs with cycles.",
            "The method doesn't always converge.",
            "And this is, these are the equations for the.",
            "Message passing algorithm at most important.",
            "Works on the analysis of this update.",
            "Equation has been done in Washington 2000, ones whether they give a sufficient condition and this one big diagonal dominance of RJ metrics, which is.",
            "The information matrix of the distribution.",
            "And then the state of the art.",
            "Things have been presented by the previous speaker, so I'm not going to go into that.",
            "Well I just.",
            "Brief remarks a sufficient condition of this work, summability or?",
            "Pairwise normalize ability can be expressed by this.",
            "Algebraic condition.",
            "OK, So what we're going to do is approximate the distribution, which can be.",
            "Written in the factor graph notation like this with a distribution.",
            "Like this?",
            "And it turns out that.",
            "This leads to the same minimizing approximate KL divergent.",
            "This is the same.",
            "So finding stationary points of this.",
            "QR versions or the basic financial boils down to finding fixed points of the message passing algorithm.",
            "Well, only work on the direct minimization of debate affinity was done by Welling Maxwell in anyway, thanks.",
            "One day pointed out that.",
            "When we are up for direct minimization that goes in beta finishes, not always bounded from below.",
            "So.",
            "Might happen, that message passing crashes because.",
            "Because it wants to find stable fixed points but or minimums.",
            "But these are somewhere at minus Infinity or they are not existing at all.",
            "And then they also noted that this gives me.",
            "That message message passing and I recommendation converges.",
            "The same solution, or both failed to converge.",
            "This was an experimental.",
            "Result.",
            "And what we're going to do here is instead of using.",
            "The beta Free energy we are going to use the fractional theology.",
            "It turns out that this is going to be useful in the computations, and it gives a much better insight into what's really happening with this beta free energy.",
            "There are some particular cases when, so this is a relaxed version, which can be where the terms these terms are replaced by these ones.",
            "And then it turns out that when when they choose epsilon RJS 0 or in the limiting case, we have the mean field theology.",
            "For Epsilon Jays or set to one we have the beta failure G and for epsilon RJ is bigger than one we have the.",
            "So called 3 related type free energy which was introduced by very right and it's crouthers.",
            "We should also.",
            "We also know that although we don't use any projection.",
            "Power EP List is the same message parsing algorithm as.",
            "The message passing algorithm for finding stable fixed points of a function of the family.",
            "We extend that.",
            "In fact we continue and then extend the work of Maxwell in a UI stack.",
            "And we understand continuation in the sense that they have direct sum formulas, but they didn't really compute.",
            "The didn't really go through all the computations and.",
            "Set the condition for the boundedness of the base of energy and extension.",
            "By extension we mean that we are going to use this fractional free energy.",
            "What we're going to do is express the pseudo marginals and this form.",
            "Therefore, in building all the marginal and normalization constraints into the parametric parameterization.",
            "It turns out that.",
            "We can derive the following properties.",
            "So if we know that this was known earlier, that means fruitful and J is convex in both.",
            "The main parameters and.",
            "And the variance and standard deviations and.",
            "Max and Max Welling antibiotic show that it's convex an the base of images convex in and all diagonal elements.",
            "What we're going to do is compute these two and plug into the plug into the formula in the formula for the fraction of beta theology.",
            "And by setting all these values equal to each other and epsilon RJS, and we're going to use for all of them epsilon.",
            "We can get an increasing sequence of functions in terms of epsilons.",
            "And then as we as I've already said, the limiting case when epsilon tends to 0 is the mean free energy.",
            "And since this is an an increasing sequence of functions, we calculated the limit when epsilon tends to Infinity.",
            "And we have found that there is this lower bound for.",
            "For the fractional beta free energy, one expressed in terms of Sigma, so we know that we minimize with regards to the Sigma J. OK, so the main result of is that.",
            "Using this constraint that all the diagonal or the diagonal elements are one.",
            "We can set the condition for for the boundedness of the beta free energy and it's going is going to be this one.",
            "So this is same condition as works on mobility and or pairwise normalize ability.",
            "Here on the.",
            "Graphs we have.",
            "2 examples This is an example about condition holds and this quantity is 0.9 and this is an example of the condition doesn't hold or I will discount it is 1.1.",
            "What we can see here, what we have done is it turned out that the direction which which this function.",
            "Decreases the most is the direction of the eigenvector corresponding to the highest eigenvalue.",
            "Oh excuse me.",
            "Of this matrix.",
            "So to this eigenvalue.",
            "So the action corresponding to this eigenvalue.",
            "And we are since we cannot really put plot functions in dimensions, we are plot.",
            "This function in this direction.",
            "And you can see that here we have.",
            "The mean field family she.",
            "The beta for lunch with red.",
            "There's not a line and continuous black line is the.",
            "Lower bound.",
            "And we can see that we have used direct minimization with the gradient method and message passing algorithm so.",
            "Both of them converge.",
            "And in case when the condition doesn't hold.",
            "We can see that in this direction at least, beta fairness has a local minimum.",
            "While Mistress has free energy, is a global one and the lower bound is simply unbounded.",
            "And we have plotted on the fractional bit of energies.",
            "And it turned out that it it converge for certain values of epsilon.",
            "And then it didn't converge for.",
            "For values bigger than so.",
            "It's converged for fractional energies up until this one, and then it doesn't converge for these ones.",
            "And the same behavior was shown by the gradient method and the message passing.",
            "Oh OK, so here we plotted the errors.",
            "It turns out that if.",
            "In both cases, if message parsing and direct minimization converged and converged to the same values.",
            "And, well, we know the ground truth.",
            "So in this case we plot the errors.",
            "And we see that we have the same errors and it turns out that.",
            "In the case when we have a paranormal isable model, then increasing.",
            "Epsilon helps a bit, but after awhile ever is going to increase.",
            "But in this case, increasing epsilon leads to decrease in error.",
            "But then after that we have a non convergence and.",
            "OK.",
            "So this state you can conclude that.",
            "Well, there are a couple of things we don't know yet, so we don't really know.",
            "When we have no bars normalizable.",
            "Model we don't really know when the beta village is going to have a local minimum, so this is something to be.",
            "Worked out later.",
            "And we also couldn't show up until now that all the fractional free energies are going to have a global minimum when the condition holds.",
            "Well, based on this we can conclude that.",
            "The weight of the fractional rate of free energies have a lower bound if and only if the model is pairwise normalizable.",
            "An experiment show that that if the condition of Permise dominance ability holds that we are going to have a global minimum.",
            "And when it doesn't.",
            "Then we can always find an epsilon for which.",
            "So by decreasing epsilon we can.",
            "The chances that we are going to converge with the message passing algorithm or we are going to find the local minimum minimum are increasing.",
            "So these results are in line with the results of the previous speaker.",
            "And they.",
            "We should we also want to mention that pairwise normalized doesn't hold.",
            "Then complexification incense over.",
            "Of what Wainwright and his cooperation can harm?",
            "Because in that case we're choosing values for epsilon bigger than one which pushes the fraction of the energies down in that direction.",
            "And then that one instead of helping convergence can harm.",
            "Well, this is a very special continuous model.",
            "But yeah, it can give us some explanation why.",
            "Small epsilon values in power EPI.",
            "Might have convergence.",
            "OK so I can with what would like to conclude with this figure.",
            "So we have seen that that complexification in in.",
            "Discrete models.",
            "Apps.",
            "But in Gaussian models, when we don't have the condition of pairwise normalize ability, then it can harm.",
            "And.",
            "So if we have unboundedness, then it can harm because instead of.",
            "Increasing the chances of getting to a local minimum, it decreases the chances.",
            "OK.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Working with Alan Wilsky and also parts of this joint work with two other of his students, Dimitri Molotov, and then catch on mistaken.",
                    "label": 0
                },
                {
                    "sent": "And basically we really focused on working in Gaussian models a lot, so I have some preliminary work here at the end where we just try to apply these ideas to now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On your problem.",
                    "label": 0
                },
                {
                    "sent": "So here's the three main components of the talk.",
                    "label": 0
                },
                {
                    "sent": "In the first part, we're going to talk about what we call walk.",
                    "label": 0
                },
                {
                    "sent": "Some view of inference in Gaussian models, and we've applied this to look at convergence of several iterative algorithms for inferencing, Gaussian models, and also one new thing.",
                    "label": 1
                },
                {
                    "sent": "Basically a modified form of belief propagation that works for non walkable models.",
                    "label": 0
                },
                {
                    "sent": "And I'll explain what that means more later.",
                    "label": 0
                },
                {
                    "sent": "And the second component is apparently different approach, but it ends up having very similar convergence conditions.",
                    "label": 1
                },
                {
                    "sent": "Lagrangian relaxation methods for Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "And I'll describe that in more detail later, But basically there's a convergent with what I call maximum diffusion for the Gaussian model, and this is actually all those for Gaussian models is closely related to recent developments in math estimation for discrete models, namely Martin Martin Wainwright, St Related, Max product approach and related LP relaxations of math estimation, and as I said in the last part, will play around with applying these two so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nonlinear problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so a Gaussian MRF you're familiar with the multivariate Gaussian distribution with the mean X hat and covariance P. But if you're dealing with a graphical model, it's more natural to represent this in terms of this inverse of the covariance matrix information matrix, because that would be sparse to represent the the structure of the model.",
                    "label": 0
                },
                {
                    "sent": "The graphical structure of the field and a linear term H here, which represents usually measurements.",
                    "label": 0
                },
                {
                    "sent": "So in principle inferencing, Gaussian models is straightforward.",
                    "label": 0
                },
                {
                    "sent": "You can do it using Gaussian elimination, but it involves inverting.",
                    "label": 0
                },
                {
                    "sent": "Inverting the matrix, which in general has cubic complexity in the size of the field, and even using more efficient versions such as junction tree algorithms, are sparse Cholesky factorization.",
                    "label": 0
                },
                {
                    "sent": "The inference would be cubic in the tree with the graph, and so for many practical problems you're forced to use iterative methods if you want to have a fast approach.",
                    "label": 0
                },
                {
                    "sent": "Anna.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The focus of our talk.",
                    "label": 0
                },
                {
                    "sent": "So this walk some picture starts with well known series, the Newman Series for matrix inverse, so we're going to take the model.",
                    "label": 0
                },
                {
                    "sent": "We assume that the variables are scaled so that this information matrix has a unit diagonal.",
                    "label": 1
                },
                {
                    "sent": "Then the off diagonal entries are actually partial correlation coefficients defined on the edges of the graph, and this is a natural measure of the interaction between two variables independent of the other variables.",
                    "label": 0
                },
                {
                    "sent": "Well, the new in series.",
                    "label": 0
                },
                {
                    "sent": "Is is this guy?",
                    "label": 0
                },
                {
                    "sent": "So if we want to compute the covariance matrix inverse of the J matrix, the inverse of this I -- R, that's a power series in R. Which converges at the spectral radius of R is less than one.",
                    "label": 1
                },
                {
                    "sent": "If you look at the components of this powers of our it as a simple interpretation in terms of sums over walks in the graph where each walk is weighted by a product of these correlation coefficients along those edges.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so this suggests to walk some view of inference where you describe the covariance between two nodes as a sum over all walks in the graph from I to J of this weight of the walk, which is the product of the edge weights an based on that I also have an interpretation for the estimates, the means, which is just a related walk some, but now you walk from any point in the graph to a node and each walk is weighted by this field value at the start of the walk.",
                    "label": 0
                },
                {
                    "sent": "And here's a simple example of this illustrate this if you have a three node graph, these are some of the walks that you would need to compute to get the estimate node one.",
                    "label": 0
                },
                {
                    "sent": "They can start from anywhere, wander around the graph and edit node one, and each walk is weighted by the H at the start and also if you look at the variance is a set of self return blocks, so the variance of this node would be all walks.",
                    "label": 0
                },
                {
                    "sent": "It should begin their wandering around and come back, and they're allowed to backtrack, go around cycles, and so forth.",
                    "label": 0
                },
                {
                    "sent": "But these sums of these walk psalms or psalms over countably infinite sets of walks.",
                    "label": 0
                },
                {
                    "sent": "Even in this simple graph, and we want to interpret this in a very loose way.",
                    "label": 0
                },
                {
                    "sent": "Basically view these unordered slums and the condition you need for that is absolute convergence of these series.",
                    "label": 0
                },
                {
                    "sent": "So we call these.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models walk symbol.",
                    "label": 0
                },
                {
                    "sent": "And we drive the variety of conditions which are equivalent to this walk summability condition.",
                    "label": 0
                },
                {
                    "sent": "The simplest is if you take the absolute values of these partial correlation coefficients, they can be between plus and minus one, then that matrix of edge weights have to have spectral spectral radius less than one.",
                    "label": 0
                },
                {
                    "sent": "It turns out using pairing for Venus Theory show this is equivalent to the condition that if you negate any negative edges to make all the edges positive that the model is still valid is still a positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "Another condition we called pairwise normalizable is if you can write J as a sum of smaller two by two matrices on each edge of the graph, where each of those is itself positive definite, that's equivalent.",
                    "label": 0
                },
                {
                    "sent": "And it also turns out to be related to generalized notion of diagonal dominance if there exists a rescaling so that Jay would become diagonally dominant in the models, walk summable.",
                    "label": 0
                },
                {
                    "sent": "And this condition actually is a corollary implies that any non frustrated model is walks onnable.",
                    "label": 0
                },
                {
                    "sent": "So that means if you have a valid model and every cycle has an even number of negative edge weights, it's walk summable and this is trivial.",
                    "label": 1
                },
                {
                    "sent": "Cases that interactive model with all positive partial correlation coefficients or a tree as well some more.",
                    "label": 0
                },
                {
                    "sent": "And these sorts of models.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arise often in practice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first application of this was joint work with Mitchell, Eutaw and we looked at the walk.",
                    "label": 0
                },
                {
                    "sent": "Some interpretation of Gaussian belief propagation.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that each message in Gaussian BP has a simple interpretation as computing two walk sums.",
                    "label": 1
                },
                {
                    "sent": "So basically when you predict from I to J, you first prepare a message summarized by these two numbers, an interpretation of Alpha is itself the sum of all self return walks too.",
                    "label": 0
                },
                {
                    "sent": "I but do not visit J.",
                    "label": 0
                },
                {
                    "sent": "And beta is so this is relevant to the variance calculations which are self return lock.",
                    "label": 0
                },
                {
                    "sent": "Subs in beta is the HP weighted sum of all walks to note I also excluding J. Ann, just thinking of these in terms of walk sums and using purely sort of common tutorial ideas, you can derive recursive set of equations for computing these walks ONS recursively.",
                    "label": 0
                },
                {
                    "sent": "Ann, I won't go through this in detail, but it's difficult to what you the Gaussian elimination formulas RVP equations on the tree.",
                    "label": 0
                },
                {
                    "sent": "And also from these messages you can compute the variance and the estimate each node of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's consider what this does when you apply this to a loopy BP on a loopy graph.",
                    "label": 0
                },
                {
                    "sent": "So we have some graph here, and we run loopy BP, which basically acts like it's a tree.",
                    "label": 0
                },
                {
                    "sent": "So therefore the algorithm is actually equivalent to doing inference on the computation tree.",
                    "label": 1
                },
                {
                    "sent": "Applying this walk, some picture is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Interpret what's going on.",
                    "label": 0
                },
                {
                    "sent": "Basically the self return walks, which is what you need to compute for variances.",
                    "label": 0
                },
                {
                    "sent": "You get some of these walks, but not all of 'em.",
                    "label": 0
                },
                {
                    "sent": "So if you consider the walk that goes 1231 in the computation tree, there is such a walk, it goes 1231, but it ends at a different node, not at the root of the tree, and because of that that's not attributed to the variance in loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "But some some walk such 121 or 1231321, they do get included.",
                    "label": 0
                },
                {
                    "sent": "So basically the variances that BP we compute are subset of the correct variances.",
                    "label": 0
                },
                {
                    "sent": "So that's the bad news.",
                    "label": 0
                },
                {
                    "sent": "The good news though, is that all of these single directional locks basically walks 2 J or you don't care where they started from.",
                    "label": 0
                },
                {
                    "sent": "That's still correct.",
                    "label": 0
                },
                {
                    "sent": "Even with cycles you have 1231.",
                    "label": 0
                },
                {
                    "sent": "Well, there is such a walk here one.",
                    "label": 0
                },
                {
                    "sent": "I'll say 13211321 there, so basically it doesn't have to start.",
                    "label": 1
                },
                {
                    "sent": "It can start a replica of the root node in the computation tree.",
                    "label": 0
                },
                {
                    "sent": "And because of that you get all the locks eventually for the meeting calculations.",
                    "label": 0
                },
                {
                    "sent": "And given that this has a walk, some interpretation that walks some ability property implies that Gaussian belief propagation asked to converge in a walk symbol model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The means are correct.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that only works for walk Summable models and a few models a little bit outside of the class of walks on bull, but is simple to modify.",
                    "label": 0
                },
                {
                    "sent": "This actually get a conversion algorithm that works more generally, and the idea is so here we have a J matrix which is not walk symbol if you add some diagonal loading to the J matrix, increase the diagonal terms.",
                    "label": 0
                },
                {
                    "sent": "You can get something which is walk solvable, and so therefore you could use Gaussian belief propagation to implement this type of iteration.",
                    "label": 0
                },
                {
                    "sent": "Basic Gaussian BP will compute this.",
                    "label": 0
                },
                {
                    "sent": "This matrix inverse time something, but it would compute the wrong answer, but you can correct for that by adding this feedback loop scaled by gamma, and it's pretty straightforward to basically implement this double loop algorithm, but by a single loop where you basically put this feedback into a damped update of the H vector.",
                    "label": 0
                },
                {
                    "sent": "Here in this last line.",
                    "label": 0
                },
                {
                    "sent": "For Lambda small enough, this leads to a conversion algorithm for non walks on bull models.",
                    "label": 0
                },
                {
                    "sent": "The only catch is is move far away from the walk symbol class.",
                    "label": 0
                },
                {
                    "sent": "You would have to do more diagonal loading and its outer loop converge more slowly so the conversion.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can get slow.",
                    "label": 0
                },
                {
                    "sent": "Here's another algorithm we've analyzed from this point of view.",
                    "label": 0
                },
                {
                    "sent": "The walks on walks on the embedded trees algorithm developed by Eric Sudderth and Martin Wainwright.",
                    "label": 0
                },
                {
                    "sent": "So basically, it's an example of a classical iteration based on into method of this form or user preconditioner.",
                    "label": 0
                },
                {
                    "sent": "Their idea was to use embedded trees as preconditioners.",
                    "label": 0
                },
                {
                    "sent": "So if you have some graph, you can take some invented tree and basically by sitting setting the cut edges to 0, use that as a precondition in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you can change preconditioners overtime to try to improve convergence.",
                    "label": 0
                },
                {
                    "sent": "Well, we basically just went back and looked at this from this walk.",
                    "label": 0
                },
                {
                    "sent": "Some point if you are able to interpret these estimates precisely as having a walk, some interpretation, and because of that, it's showing that these types of methods would converge and walks on will models using any.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence of trees.",
                    "label": 0
                },
                {
                    "sent": "But the idea was to try to also use this this analysis to guide the selection of trees.",
                    "label": 0
                },
                {
                    "sent": "So we came up with a simple strategy for picking trees, which actually is reduces to Max weight spanning tree algorithm which is efficiently implemented.",
                    "label": 0
                },
                {
                    "sent": "An has a walk.",
                    "label": 0
                },
                {
                    "sent": "Some interpretation.",
                    "label": 0
                },
                {
                    "sent": "Basically this is minimizing a upper bound on the estimation error you step and comparing to just one tree iteration or a two tree iteration alternating between 2 trees.",
                    "label": 0
                },
                {
                    "sent": "We found this adaptive approach can accelerate convergence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to switch gears and talk about another method which is Lagrangian relaxation and is a starting point that the idea is you start with intractable graph and you break the graph up into smaller pieces which are trackable.",
                    "label": 0
                },
                {
                    "sent": "The simplest being just individual edges or small cycles of the graph or larger blocks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "You also could use things thin subgraphs, basically because inference on these is fast.",
                    "label": 0
                },
                {
                    "sent": "Using recursive methods are trees, and so their approach and we describe is actually if you use trees, is essentially equivalent to this TMP approach that Martin Wainwright developed, although that was for.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Screen models.",
                    "label": 0
                },
                {
                    "sent": "So here's how it looks for Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "So, given such a decomposition in the graph G into a tractable set of graphs G prime.",
                    "label": 0
                },
                {
                    "sent": "We also have to split these quadratic form describing the PDF into local pieces defined on each sub graph and subject to these constraints that if you re some those pieces you would recover the same model.",
                    "label": 0
                },
                {
                    "sent": "Also, there's a condition here, which is that each of those subgraphs has to be positive definite, so this actually generalizes this pairwise, normalizable idea.",
                    "label": 0
                },
                {
                    "sent": "So if you had a splitting based on just edges, this would only be possible for pairwise normalizable models.",
                    "label": 0
                },
                {
                    "sent": "But this approach is sound for models which have larger size potentials.",
                    "label": 0
                },
                {
                    "sent": "So then the map estimation problem.",
                    "label": 0
                },
                {
                    "sent": "You can reformulate it rather than maximizing this.",
                    "label": 0
                },
                {
                    "sent": "You can maximize this augmented function, but now subject to a set of linear constraints.",
                    "label": 0
                },
                {
                    "sent": "Which is that the replicating nodes should all be consistent.",
                    "label": 0
                },
                {
                    "sent": "Well, by relaxing those constraints, you get a tractable dual convex dual problem, and assuming that this splitting is possible is convex, and therefore there's strong duality.",
                    "label": 0
                },
                {
                    "sent": "So this leads to an exact map estimate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another interpretation which relates more to approximate inference pictures.",
                    "label": 0
                },
                {
                    "sent": "You can instead solve this problem, which is where you compute the log partition function on each of these subgraphs and you try to minimize that, but subject to these constraints that the potentials are in this linear subspace and also that the individual subgraphs are positive definite.",
                    "label": 0
                },
                {
                    "sent": "And it has a dual problem which is relates to these Gibbs free energies, approximations that people.",
                    "label": 0
                },
                {
                    "sent": "Are used to do variational inference.",
                    "label": 0
                },
                {
                    "sent": "And I'll.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip that though.",
                    "label": 0
                },
                {
                    "sent": "So here's the algorithm we can use to solve.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems will actually go back for one second, so these kind of problems can be solved using iterative scaling methods, which are, like for instance in proportional fitting is a special case of SARS.",
                    "label": 0
                },
                {
                    "sent": "Iterative scaling algorithm is basically a projection algorithm which imposes these marginal consistency constraints in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this algorithm I'm describing here can be interpreted in that way.",
                    "label": 0
                },
                {
                    "sent": "But it's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically you pick some subset S which is contained in the intersection of multiple clusters that you used to break up the graph, and then for each sub graph which contains that cluster in that augmenting graph, G prime, you compute its marginal just using Gaussian elimination within that trackable sub graph.",
                    "label": 1
                },
                {
                    "sent": "Then by averaging these marginals and potential domain and doing an update that ensures that basically forces all the marginals degree after the update.",
                    "label": 0
                },
                {
                    "sent": "And this update also has the property that maintains a valid parameterisation.",
                    "label": 0
                },
                {
                    "sent": "So it stays within that linear subspace of valid representations on G prime.",
                    "label": 1
                },
                {
                    "sent": "And by iterating over the different subsets S, you converge to the optimal solution of these prob.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I described.",
                    "label": 0
                },
                {
                    "sent": "So here's an example where I used a 64 by 64 thin plate model which basically is a GM RF which has local potentials defined on subsets of five nodes and it's related to the Laplacian and tries to minimize the curvature of the field and added just basically random measurements and smooth smooth annoys using the basic version of LR, which are basically sort of five cliques of each node is 4 neighbors.",
                    "label": 0
                },
                {
                    "sent": "This was the convergence rate to use larger block sizes, you can accelerate the.",
                    "label": 0
                },
                {
                    "sent": "Convergence rate.",
                    "label": 0
                },
                {
                    "sent": "But one catch here is if you have really strong correlations in the field, any pretty much any iterative method you try to use is going to slow down, but in classical linear algebra literature there's been work on multigrid methods to try to accelerate convergence and such.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quations and so I try to come up with a version of that for this framework and basically the idea is similar where you, but you first reformulate define scale model at multiple scales.",
                    "label": 0
                },
                {
                    "sent": "Subject across scale constraints.",
                    "label": 0
                },
                {
                    "sent": "And then you relax those cross scale constraints and also at each scale break it up using this decomposition idea and then relaxing these constraints and running it are valid.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can solve this.",
                    "label": 0
                },
                {
                    "sent": "There's a generalization of this maximum diffusion math.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'll skip that slide.",
                    "label": 0
                },
                {
                    "sent": "But it can lead to some improvement in the rate of convergence, and I this is sort of a toy problem, but I suspect for much larger problems that you can is more critical to use these types of methods.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for the last part of the talk I just wanted to apply this to something some non Gaussian problem and so it's not a complicated idea.",
                    "label": 0
                },
                {
                    "sent": "Just basically applying these methods to solve for the Newton step in Newton's method or in this levenberg Marquardt method, if the problem is not kind of X1 observation is that if you have a objective function which decomposes into local functions which are convex then this guy is going to satisfy this this.",
                    "label": 0
                },
                {
                    "sent": "Normalise ability condition required by Gaussian belief propagation and by Lagrangian relaxation.",
                    "label": 0
                },
                {
                    "sent": "So that's a really nice class of models.",
                    "label": 0
                },
                {
                    "sent": "For instance, if this is a paralyzed paralyzed graph with pairwise interactions, you can use Gaussian belief propagation to solve systems based on this guy.",
                    "label": 0
                },
                {
                    "sent": "But if you try to use non convex potentials, you can run into problems.",
                    "label": 0
                },
                {
                    "sent": "But loving Bergmark Court would normally just.",
                    "label": 0
                },
                {
                    "sent": "The goal would be to add regularization to guarantee that H is positive definite so that you can invert it.",
                    "label": 0
                },
                {
                    "sent": "Well here, similar as in the stabilized version of Gaussian belief propagation, we also make this strong enough to make sure the models walk summable, and then we can use these type.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some methods to infer the problem, and as an example, we looked at these half half quadratic edge preserving methods for image processing, where basically if this were a square, this would correspond to thin membrane model.",
                    "label": 0
                },
                {
                    "sent": "But if you replace that quadratic penalty by some other penalty, it can have better edge preserving properties and but here we are assuming that we have a smooth set of smooth potential functions, so we're going to smooth approximation to this this.",
                    "label": 1
                },
                {
                    "sent": "T The P penalty.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's what the function will look like for P = 1.",
                    "label": 0
                },
                {
                    "sent": "In varying this smoothing parameter beta, you get a sequence of better approximations to the absolute value function.",
                    "label": 0
                },
                {
                    "sent": "Anfora P = 1/2.",
                    "label": 0
                },
                {
                    "sent": "You get something which is non convex and more sharply peaked here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so basically, I basically just applied this idea of using regularization to force the model to be well within the walk symbol class and then it's solving these iteratively.",
                    "label": 0
                },
                {
                    "sent": "Basically that walk some condition ensures that pretty much any of the methods I described would converge very quickly at each step, like, say within 3030 iterations.",
                    "label": 0
                },
                {
                    "sent": "And this is what the results I got using P = 1 for decreasing values of beta.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was for P = 1/2 where you get a much sharper result here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion, there are many approaches that seem to work for walks on little models.",
                    "label": 0
                },
                {
                    "sent": "Are this generalized class of of normalizable models based on subsets?",
                    "label": 1
                },
                {
                    "sent": "We've looked at Gaussian belief propagation and embedded tree algorithms and this Lagrangian relaxation approach.",
                    "label": 0
                },
                {
                    "sent": "Multiscale variations on these ideas can be important for fast convergence in large scale problems with strong long range correlations.",
                    "label": 0
                },
                {
                    "sent": "And we also have looked at the idea of using these types of methods.",
                    "label": 0
                },
                {
                    "sent": "Basically walk summable models and these corresponding algorithms to precondition harder problems which are either non walk summable or even nonlinear, and that seems to be a sound approach.",
                    "label": 0
                },
                {
                    "sent": "Further work would be to sort of relax the distinction between inner and outer loop and these nonlinear methods to try to get a fully distributed message passing algorithm for, say, Newton's method.",
                    "label": 1
                },
                {
                    "sent": "Another thing is these diagonal regularization methods.",
                    "label": 0
                },
                {
                    "sent": "We I use this very simple choices of this here, but there's a lot of work to decide how much regularization should you use.",
                    "label": 0
                },
                {
                    "sent": "Like for instance, once you get inside the walk symbol class, the further in that class you are, the faster these methods would converge.",
                    "label": 0
                },
                {
                    "sent": "But then the outer loop will converge slower, so there's a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Something that I didn't discuss here, but we're also interested in trying to get variance estimates within the walk symbol class.",
                    "label": 0
                },
                {
                    "sent": "These algorithms compute reasonable variance estimates, but as you approach the boundary that class or move far away from that, especially if you use this diagonal loading, it's not clear if you're going to be able to get very assessments, but maybe there's some way to correct for these regularization methods to try to estimate the variances.",
                    "label": 0
                },
                {
                    "sent": "This LR approach actually in our paper on this, which we recently presented at Allerton.",
                    "label": 0
                },
                {
                    "sent": "We also discovered we just discussed discrete and Gaussian models separately, but the basic idea should work for hybrid models, although some of the algorithms were using would have to be modified substantially to work in that case.",
                    "label": 0
                },
                {
                    "sent": "And one other idea that I'm interested in is maybe even using continuous approaches approaches to discrete problems, similar to how in this how these nonsmooth functions are approximated by a sequence of smooth functions.",
                    "label": 1
                },
                {
                    "sent": "You can imagine having a family of functions which initially are unimodal, and then gradually split to focus on 2D functions like plus and minus one, and I'm curious if how well that would work.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can solve each of those using continuous methods, but you get a local.",
                    "label": 0
                },
                {
                    "sent": "Local Maxima.",
                    "label": 0
                },
                {
                    "sent": "That's that's it.",
                    "label": 0
                },
                {
                    "sent": "Well, basically it works for any model, but the catch is that I mean so.",
                    "label": 0
                },
                {
                    "sent": "Basically using a walk symbol model to precondition something else.",
                    "label": 0
                },
                {
                    "sent": "And the more you have to modify the model to make it walk summable, the worse of preconditioner you have.",
                    "label": 1
                },
                {
                    "sent": "So basically it will still converge that preconditioner.",
                    "label": 0
                },
                {
                    "sent": "Yeah, greater convergence rate of convergence, like basically if you have to, if you have to use a lot of diagonal loading to make a model walk summable, that's probably going to give you a slow preconditioner, but it would be a stable preconditioner.",
                    "label": 0
                },
                {
                    "sent": "This approach with the hybrid models I mean are you?",
                    "label": 0
                },
                {
                    "sent": "Are you planning to do some projections?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's that's to be determined, but I mean, I think the basic.",
                    "label": 0
                },
                {
                    "sent": "The convex optimization formulation would stand.",
                    "label": 0
                },
                {
                    "sent": "But this maximum diffusion description?",
                    "label": 0
                },
                {
                    "sent": "That's a really nice algorithm 'cause exact closed form solution for basically doing coordinate descent in this hybrid case.",
                    "label": 0
                },
                {
                    "sent": "That's not the case, so you have to use a completely different optimization method.",
                    "label": 0
                },
                {
                    "sent": "But I think the basic basic idea, the fundamental variational problem, would still be sound.",
                    "label": 0
                },
                {
                    "sent": "It may be somewhat similar to EP type ideas, but.",
                    "label": 0
                },
                {
                    "sent": "The approach I'm describing, unlike expectation propagation, so the slide on the variational methods with the free energy stuff.",
                    "label": 0
                },
                {
                    "sent": "Basically there was some of local entropies there, and there was no negative terms, whereas like beta free energy would also have negative terms.",
                    "label": 0
                },
                {
                    "sent": "So that seems to be what distinguishes that approach from other variational methods, but for map estimation that's that's OK, especially in Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "Check in.",
                    "label": 0
                },
                {
                    "sent": "Switch.",
                    "label": 0
                },
                {
                    "sent": "Should I put?",
                    "label": 0
                },
                {
                    "sent": "The Castle.",
                    "label": 0
                },
                {
                    "sent": "That's forward and backwards in the center, warns laser pointer.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, the final.",
                    "label": 0
                },
                {
                    "sent": "Bounds on beta free energy production networks by Bolton Jack.",
                    "label": 0
                },
                {
                    "sent": "Good evening I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about Beta financial from the Gaussian networks.",
                    "label": 0
                },
                {
                    "sent": "And this is joint work with Damascus.",
                    "label": 0
                },
                {
                    "sent": "First, we are going to review some things about go simplification.",
                    "label": 0
                },
                {
                    "sent": "And then we go for the direct minimization of the base of the energy for Gaussian network and set a lower bound for for that function.",
                    "label": 0
                },
                {
                    "sent": "Much of the things about Gaussian belief propagation has already been said earlier.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to skip that point.",
                    "label": 0
                },
                {
                    "sent": "The problem popped up when we were working with conditional Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that we don't really know much of the things are known about discrete models, But it turns out that not much is known about the beta free energy of a Gaussian network.",
                    "label": 0
                },
                {
                    "sent": "So we are going to analyze that that thing.",
                    "label": 0
                },
                {
                    "sent": "Well inferencing Gaussian models is.",
                    "label": 0
                },
                {
                    "sent": "It can be used.",
                    "label": 0
                },
                {
                    "sent": "It can be computed so important quantities can be computed.",
                    "label": 0
                },
                {
                    "sent": "In a deterministic manner.",
                    "label": 0
                },
                {
                    "sent": "However, it's very useful in case of very huge models is useful to use message passing algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then for three models it works fine, but it turns out that for graphs with cycles.",
                    "label": 0
                },
                {
                    "sent": "The method doesn't always converge.",
                    "label": 0
                },
                {
                    "sent": "And this is, these are the equations for the.",
                    "label": 0
                },
                {
                    "sent": "Message passing algorithm at most important.",
                    "label": 0
                },
                {
                    "sent": "Works on the analysis of this update.",
                    "label": 0
                },
                {
                    "sent": "Equation has been done in Washington 2000, ones whether they give a sufficient condition and this one big diagonal dominance of RJ metrics, which is.",
                    "label": 0
                },
                {
                    "sent": "The information matrix of the distribution.",
                    "label": 0
                },
                {
                    "sent": "And then the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Things have been presented by the previous speaker, so I'm not going to go into that.",
                    "label": 1
                },
                {
                    "sent": "Well I just.",
                    "label": 0
                },
                {
                    "sent": "Brief remarks a sufficient condition of this work, summability or?",
                    "label": 0
                },
                {
                    "sent": "Pairwise normalize ability can be expressed by this.",
                    "label": 0
                },
                {
                    "sent": "Algebraic condition.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're going to do is approximate the distribution, which can be.",
                    "label": 0
                },
                {
                    "sent": "Written in the factor graph notation like this with a distribution.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that.",
                    "label": 0
                },
                {
                    "sent": "This leads to the same minimizing approximate KL divergent.",
                    "label": 0
                },
                {
                    "sent": "This is the same.",
                    "label": 0
                },
                {
                    "sent": "So finding stationary points of this.",
                    "label": 0
                },
                {
                    "sent": "QR versions or the basic financial boils down to finding fixed points of the message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, only work on the direct minimization of debate affinity was done by Welling Maxwell in anyway, thanks.",
                    "label": 0
                },
                {
                    "sent": "One day pointed out that.",
                    "label": 0
                },
                {
                    "sent": "When we are up for direct minimization that goes in beta finishes, not always bounded from below.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Might happen, that message passing crashes because.",
                    "label": 0
                },
                {
                    "sent": "Because it wants to find stable fixed points but or minimums.",
                    "label": 0
                },
                {
                    "sent": "But these are somewhere at minus Infinity or they are not existing at all.",
                    "label": 0
                },
                {
                    "sent": "And then they also noted that this gives me.",
                    "label": 0
                },
                {
                    "sent": "That message message passing and I recommendation converges.",
                    "label": 0
                },
                {
                    "sent": "The same solution, or both failed to converge.",
                    "label": 0
                },
                {
                    "sent": "This was an experimental.",
                    "label": 0
                },
                {
                    "sent": "Result.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do here is instead of using.",
                    "label": 0
                },
                {
                    "sent": "The beta Free energy we are going to use the fractional theology.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this is going to be useful in the computations, and it gives a much better insight into what's really happening with this beta free energy.",
                    "label": 0
                },
                {
                    "sent": "There are some particular cases when, so this is a relaxed version, which can be where the terms these terms are replaced by these ones.",
                    "label": 0
                },
                {
                    "sent": "And then it turns out that when when they choose epsilon RJS 0 or in the limiting case, we have the mean field theology.",
                    "label": 0
                },
                {
                    "sent": "For Epsilon Jays or set to one we have the beta failure G and for epsilon RJ is bigger than one we have the.",
                    "label": 0
                },
                {
                    "sent": "So called 3 related type free energy which was introduced by very right and it's crouthers.",
                    "label": 0
                },
                {
                    "sent": "We should also.",
                    "label": 0
                },
                {
                    "sent": "We also know that although we don't use any projection.",
                    "label": 0
                },
                {
                    "sent": "Power EP List is the same message parsing algorithm as.",
                    "label": 0
                },
                {
                    "sent": "The message passing algorithm for finding stable fixed points of a function of the family.",
                    "label": 0
                },
                {
                    "sent": "We extend that.",
                    "label": 0
                },
                {
                    "sent": "In fact we continue and then extend the work of Maxwell in a UI stack.",
                    "label": 0
                },
                {
                    "sent": "And we understand continuation in the sense that they have direct sum formulas, but they didn't really compute.",
                    "label": 0
                },
                {
                    "sent": "The didn't really go through all the computations and.",
                    "label": 0
                },
                {
                    "sent": "Set the condition for the boundedness of the base of energy and extension.",
                    "label": 0
                },
                {
                    "sent": "By extension we mean that we are going to use this fractional free energy.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is express the pseudo marginals and this form.",
                    "label": 0
                },
                {
                    "sent": "Therefore, in building all the marginal and normalization constraints into the parametric parameterization.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "We can derive the following properties.",
                    "label": 0
                },
                {
                    "sent": "So if we know that this was known earlier, that means fruitful and J is convex in both.",
                    "label": 0
                },
                {
                    "sent": "The main parameters and.",
                    "label": 0
                },
                {
                    "sent": "And the variance and standard deviations and.",
                    "label": 0
                },
                {
                    "sent": "Max and Max Welling antibiotic show that it's convex an the base of images convex in and all diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is compute these two and plug into the plug into the formula in the formula for the fraction of beta theology.",
                    "label": 0
                },
                {
                    "sent": "And by setting all these values equal to each other and epsilon RJS, and we're going to use for all of them epsilon.",
                    "label": 0
                },
                {
                    "sent": "We can get an increasing sequence of functions in terms of epsilons.",
                    "label": 0
                },
                {
                    "sent": "And then as we as I've already said, the limiting case when epsilon tends to 0 is the mean free energy.",
                    "label": 0
                },
                {
                    "sent": "And since this is an an increasing sequence of functions, we calculated the limit when epsilon tends to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And we have found that there is this lower bound for.",
                    "label": 0
                },
                {
                    "sent": "For the fractional beta free energy, one expressed in terms of Sigma, so we know that we minimize with regards to the Sigma J. OK, so the main result of is that.",
                    "label": 0
                },
                {
                    "sent": "Using this constraint that all the diagonal or the diagonal elements are one.",
                    "label": 0
                },
                {
                    "sent": "We can set the condition for for the boundedness of the beta free energy and it's going is going to be this one.",
                    "label": 0
                },
                {
                    "sent": "So this is same condition as works on mobility and or pairwise normalize ability.",
                    "label": 0
                },
                {
                    "sent": "Here on the.",
                    "label": 0
                },
                {
                    "sent": "Graphs we have.",
                    "label": 0
                },
                {
                    "sent": "2 examples This is an example about condition holds and this quantity is 0.9 and this is an example of the condition doesn't hold or I will discount it is 1.1.",
                    "label": 0
                },
                {
                    "sent": "What we can see here, what we have done is it turned out that the direction which which this function.",
                    "label": 0
                },
                {
                    "sent": "Decreases the most is the direction of the eigenvector corresponding to the highest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Oh excuse me.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So to this eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So the action corresponding to this eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And we are since we cannot really put plot functions in dimensions, we are plot.",
                    "label": 0
                },
                {
                    "sent": "This function in this direction.",
                    "label": 0
                },
                {
                    "sent": "And you can see that here we have.",
                    "label": 0
                },
                {
                    "sent": "The mean field family she.",
                    "label": 0
                },
                {
                    "sent": "The beta for lunch with red.",
                    "label": 0
                },
                {
                    "sent": "There's not a line and continuous black line is the.",
                    "label": 0
                },
                {
                    "sent": "Lower bound.",
                    "label": 0
                },
                {
                    "sent": "And we can see that we have used direct minimization with the gradient method and message passing algorithm so.",
                    "label": 0
                },
                {
                    "sent": "Both of them converge.",
                    "label": 0
                },
                {
                    "sent": "And in case when the condition doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "We can see that in this direction at least, beta fairness has a local minimum.",
                    "label": 0
                },
                {
                    "sent": "While Mistress has free energy, is a global one and the lower bound is simply unbounded.",
                    "label": 0
                },
                {
                    "sent": "And we have plotted on the fractional bit of energies.",
                    "label": 0
                },
                {
                    "sent": "And it turned out that it it converge for certain values of epsilon.",
                    "label": 0
                },
                {
                    "sent": "And then it didn't converge for.",
                    "label": 0
                },
                {
                    "sent": "For values bigger than so.",
                    "label": 0
                },
                {
                    "sent": "It's converged for fractional energies up until this one, and then it doesn't converge for these ones.",
                    "label": 0
                },
                {
                    "sent": "And the same behavior was shown by the gradient method and the message passing.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so here we plotted the errors.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if.",
                    "label": 0
                },
                {
                    "sent": "In both cases, if message parsing and direct minimization converged and converged to the same values.",
                    "label": 0
                },
                {
                    "sent": "And, well, we know the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So in this case we plot the errors.",
                    "label": 0
                },
                {
                    "sent": "And we see that we have the same errors and it turns out that.",
                    "label": 0
                },
                {
                    "sent": "In the case when we have a paranormal isable model, then increasing.",
                    "label": 0
                },
                {
                    "sent": "Epsilon helps a bit, but after awhile ever is going to increase.",
                    "label": 0
                },
                {
                    "sent": "But in this case, increasing epsilon leads to decrease in error.",
                    "label": 0
                },
                {
                    "sent": "But then after that we have a non convergence and.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this state you can conclude that.",
                    "label": 0
                },
                {
                    "sent": "Well, there are a couple of things we don't know yet, so we don't really know.",
                    "label": 0
                },
                {
                    "sent": "When we have no bars normalizable.",
                    "label": 0
                },
                {
                    "sent": "Model we don't really know when the beta village is going to have a local minimum, so this is something to be.",
                    "label": 0
                },
                {
                    "sent": "Worked out later.",
                    "label": 0
                },
                {
                    "sent": "And we also couldn't show up until now that all the fractional free energies are going to have a global minimum when the condition holds.",
                    "label": 0
                },
                {
                    "sent": "Well, based on this we can conclude that.",
                    "label": 0
                },
                {
                    "sent": "The weight of the fractional rate of free energies have a lower bound if and only if the model is pairwise normalizable.",
                    "label": 0
                },
                {
                    "sent": "An experiment show that that if the condition of Permise dominance ability holds that we are going to have a global minimum.",
                    "label": 0
                },
                {
                    "sent": "And when it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Then we can always find an epsilon for which.",
                    "label": 0
                },
                {
                    "sent": "So by decreasing epsilon we can.",
                    "label": 0
                },
                {
                    "sent": "The chances that we are going to converge with the message passing algorithm or we are going to find the local minimum minimum are increasing.",
                    "label": 0
                },
                {
                    "sent": "So these results are in line with the results of the previous speaker.",
                    "label": 0
                },
                {
                    "sent": "And they.",
                    "label": 0
                },
                {
                    "sent": "We should we also want to mention that pairwise normalized doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "Then complexification incense over.",
                    "label": 0
                },
                {
                    "sent": "Of what Wainwright and his cooperation can harm?",
                    "label": 0
                },
                {
                    "sent": "Because in that case we're choosing values for epsilon bigger than one which pushes the fraction of the energies down in that direction.",
                    "label": 0
                },
                {
                    "sent": "And then that one instead of helping convergence can harm.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a very special continuous model.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it can give us some explanation why.",
                    "label": 0
                },
                {
                    "sent": "Small epsilon values in power EPI.",
                    "label": 0
                },
                {
                    "sent": "Might have convergence.",
                    "label": 0
                },
                {
                    "sent": "OK so I can with what would like to conclude with this figure.",
                    "label": 0
                },
                {
                    "sent": "So we have seen that that complexification in in.",
                    "label": 0
                },
                {
                    "sent": "Discrete models.",
                    "label": 0
                },
                {
                    "sent": "Apps.",
                    "label": 0
                },
                {
                    "sent": "But in Gaussian models, when we don't have the condition of pairwise normalize ability, then it can harm.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So if we have unboundedness, then it can harm because instead of.",
                    "label": 0
                },
                {
                    "sent": "Increasing the chances of getting to a local minimum, it decreases the chances.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}