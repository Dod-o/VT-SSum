{
    "id": "63mvcw35i34x72vybitqai444kf3oluv",
    "title": "Supervised Deep Learning with Auxiliary Networks",
    "info": {
        "author": [
            "Wei Fan, Huawei Technologies Co., Ltd."
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_fan_auxiliary_networks/",
    "segmentation": [
        [
            "This joint work with my summer student, Jimbo John, from China, Southwest Georgia University and my colleague 120 and they do move from holiness.",
            "Our club in Hong Kong.",
            "So the idea here is we introduce both weak supervision like pairwise constraints.",
            "Also strong supervision like class labels into the autoencoder framework and we show that the framework we propose called sugar.",
            "Actually both the semi supervised and super version have consistently higher accuracy than the autoencoder framework."
        ],
        [
            "OK, so I think people here probably heard a lot about deep learning in this year's conference.",
            "So the main target deep learning is to learn a high level abstraction and in the data by using, you know multiple layers of nonlinear transformations.",
            "So here is 1 example from banjos article before.",
            "So to learn the concept means sitting and we input the visual features after.",
            "Multiple layers of nonlinear transformation.",
            "We learn the concept."
        ],
        [
            "And in deep learning, so we use the multiple layers of a hidden layer transformation.",
            "Usually like 3.",
            "You really are pretty good.",
            "Then going from the shallow models into the deep model."
        ],
        [
            "And deep learning has been shown to be quite successful in many areas, such as a computer vision like in the face recognition and recently, like in speech recognition like one recent application you know in Android have a 25% reduction in error rate and most recently like in natural language processing such as machine translation and matching a short text and.",
            "The the semantic matching.",
            "Actually we have one group in our lab and doing our active research in this area."
        ],
        [
            "OK, here is a quick summary of deep learning, so the manners are supervised, unsupervised or semi supervised."
        ],
        [
            "And that even the models that are popular, including auto encoder.",
            "And IBM and the CNN and recurrent neural networks."
        ],
        [
            "So here are two examples of the deep learning architecture and one on the top is DBM to DBN, and so we do a multiple layers of pre training which are typically unsupervised.",
            "Then the final step will do a fine tuning and which is a supervised and also in stacked autoencoder.",
            "The pre training are also typically unsupervised.",
            "And we also then we also do a final fine tuning which is supervised.",
            "We actually there are some shown that this supervised pretraining actually typically have a less accuracy than unsupervised."
        ],
        [
            "So here is, uh, OK, what we want to solve in our formulation, so we actually have a lot of you know, supervised labels, supervised supervision in the data, so the so it would be great.",
            "We really can show that using supervision in deep learning can really have a consistently higher accuracy than unsupervised pre training.",
            "Also in the supervision there are actually two types you can use.",
            "When is the?",
            "Strong type, which is the class label per example or the weak type, which is the similarity among the different examples.",
            "So in our framework we actually using the sugar formulation.",
            "We actually can incorporate both the weak and strong supervision."
        ],
        [
            "So here is the sugar framework and sugar stands for super Supervision, Guided auto encoder and there are three major components in the framework.",
            "The main framework and the auxiliary framework and also the bridge."
        ],
        [
            "So the main framework is used to construct the the input, such as using the unsupervised autoencoder.",
            "Anne."
        ],
        [
            "The auxiliary framework is to regularize the lend framework by either pairwise similarity or these narrative constraints or strong supervision, and in our formulation, we use supervised hashing."
        ],
        [
            "And the bridge is used to connect the main network and the Exhilarate network to enforce by enforcing the correlation of their their weights."
        ],
        [
            "So here is the formulation on the main network."
        ],
        [
            "So the main difference from the."
        ],
        [
            "Traditional."
        ],
        [
            "Encoder in now."
        ],
        [
            "Or a formulation of the."
        ],
        [
            "In network is the addition of the.",
            "Penalty on sparsity.",
            "So we have the Lambda, the one norm on the on the weight matrix, so this one will prevent overfitting."
        ],
        [
            "Here is your formulation on the auxiliary framework, and we use the supervised hashing representation and I see here is a sine function and P is a weight matrix and T is the bias."
        ],
        [
            "Here.",
            "So then we put into the following objective function.",
            "So this objective tries to minimize after the going into the hashing space, minimize the distance between examples of the same class or the similar examples, and want to maximize the difference of examples belong to different classes or the dissimilar examples."
        ],
        [
            "Here we have a matrix."
        ],
        [
            "Dentation, so we use the indicator matrix here to simplify the note notation.",
            "We also to simplify the mass.",
            "We assume the examples are zero centers, so that will can remove the bias T. And we also, in order to have a better numerical credibility, instead of using the sine function, we use assigned magnitude of the PX.",
            "Then we have the following relaxed objective.",
            "Then we have another constraint, which is to guarantee the the author the orthogonal and after the hashing conversion had the P&PT."
        ],
        [
            "Down here and see the bridge.",
            "So the bridge here incorporates the.",
            "Autoencoder, the main network on the left and we have here we introduce the guiding parameter Alpha and the first term is the main network from the auto encoder.",
            "And the second term, why men's Alpha and then is the from the supervised hashing and then we have a third term which is your correlation between the way to matrix and the supervised hashing matrix.",
            "That is the penalty on the on the sparsity, and this one is obviously it's convex.",
            "So."
        ],
        [
            "So to solve this optimization, we use the alternate stochastic gradient descent."
        ],
        [
            "So here is a sugar framework without.",
            "Different extensions, so instead of using the traditional autoencoder we can also use the denoising autoencoder and also the controller contractive.",
            "Autoencoder Andy formulations are almost identical to the previous formulation, I show just.",
            "The only difference is the log function."
        ],
        [
            "And."
        ],
        [
            "To have a deep sugar, so we just.",
            "Put layer by layer and so after we learned first layer we just abandoned the theory network.",
            "Only maintain the.",
            "The F the encoding function through a feedforward pass through the checking of the modules."
        ],
        [
            "And these are experiments and we have a down using the sugar formulation.",
            "So we use 9.",
            "Benchmark datasets when you see MNIST datasets and the other are variations on the datasets, including the different shapes, rectangle and convex.",
            "Also different variations on the MNIST datasets, including the basic one with random noise with image."
        ],
        [
            "And these are the baseline methods and we have compared including a SVM using the RBF kernel and the polynomial kernel and feedforward neural network and gated softmax classifier and non GSM.",
            "And I say also RBM."
        ],
        [
            "So here is a performance evaluation."
        ],
        [
            "On the shallow architecture.",
            "So the pre training we use the package and on the auto encoder and they are so this three additional parameters in the framework including the guiding coefficient.",
            "The Alpha and which combines the supervised hashing with the autoencoder, the auxiliary network with the main network, also the amount of supervision, the labeled data also on the penalty on the sparsity constraints.",
            "And the chart on the left and they are the we have the error rate on the traditional autoencoder, which is the red line on the top and the blue green bar.",
            "The error rate of the sugar framework.",
            "But I see that the they are consistently have lower error rate than the traditional autoencoder.",
            "Then the chart in the middle shows the amount of labeled data by comparing the sugar framework with the auto encoder.",
            "And we also see the consistent lower rate of the three sugar framework as compared to auto encoder.",
            "Then the last chart and there are two lines and the red line on the top.",
            "Is he autoencoder without the sparsity penalty and the blue line below that show the when you have a different parameter?",
            "How the error rate changes as the Lambda changes in value?"
        ],
        [
            "And the figure here shows the.",
            "When this positive changes and how the futures look like when the spicy change goes from 10% to 70%."
        ],
        [
            "And these are the results on the.",
            "Sugar framework when combined with the denoising autoencoder, also the contractive autoencoder, and by using a different amount of labeled data can see they are very similar to the previous chart.",
            "There the sugar framework have a consistently lower error rate than the traditional denoising autoencoder also contractive autoencoder."
        ],
        [
            "OK, and these are the results.",
            "On the deep network on the deep architecture and the chart on the top, we have 3 sugar results.",
            "So with one hidden layer up to three hidden layer and then the purple one is arrays out of the stacked autoencoder with Level 3.",
            "So using the eight benchmark datasets, the only one that have a lower accuracy.",
            "The sugar have a lower accuracy than the stacked autoencoder.",
            "Is the first one, the rectangle and all the other ones or the other seven?",
            "We actually have a higher accuracy than the.",
            "Traditional auto encoder."
        ],
        [
            "So here is a quick conclusion.",
            "So the sugar framework proposed in this paper, we incorporate both weak supervision or pairwise constraints or strong supervision labeled into the autoencoder framework, and we show that both semi supervised or the fully supervised sugar is have a consistently higher accuracy than the.",
            "Supervisor code encoder.",
            "And we have shown using the nine benchmark datasets.",
            "And also in the lab we because we always a telecommunication carrier provider.",
            "So where we using the sugar framework to solve some problems in for the carrier and the code for sugar will be available for download from the hyper link below."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This joint work with my summer student, Jimbo John, from China, Southwest Georgia University and my colleague 120 and they do move from holiness.",
                    "label": 0
                },
                {
                    "sent": "Our club in Hong Kong.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is we introduce both weak supervision like pairwise constraints.",
                    "label": 0
                },
                {
                    "sent": "Also strong supervision like class labels into the autoencoder framework and we show that the framework we propose called sugar.",
                    "label": 0
                },
                {
                    "sent": "Actually both the semi supervised and super version have consistently higher accuracy than the autoencoder framework.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I think people here probably heard a lot about deep learning in this year's conference.",
                    "label": 1
                },
                {
                    "sent": "So the main target deep learning is to learn a high level abstraction and in the data by using, you know multiple layers of nonlinear transformations.",
                    "label": 1
                },
                {
                    "sent": "So here is 1 example from banjos article before.",
                    "label": 0
                },
                {
                    "sent": "So to learn the concept means sitting and we input the visual features after.",
                    "label": 0
                },
                {
                    "sent": "Multiple layers of nonlinear transformation.",
                    "label": 0
                },
                {
                    "sent": "We learn the concept.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in deep learning, so we use the multiple layers of a hidden layer transformation.",
                    "label": 1
                },
                {
                    "sent": "Usually like 3.",
                    "label": 0
                },
                {
                    "sent": "You really are pretty good.",
                    "label": 1
                },
                {
                    "sent": "Then going from the shallow models into the deep model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And deep learning has been shown to be quite successful in many areas, such as a computer vision like in the face recognition and recently, like in speech recognition like one recent application you know in Android have a 25% reduction in error rate and most recently like in natural language processing such as machine translation and matching a short text and.",
                    "label": 1
                },
                {
                    "sent": "The the semantic matching.",
                    "label": 0
                },
                {
                    "sent": "Actually we have one group in our lab and doing our active research in this area.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here is a quick summary of deep learning, so the manners are supervised, unsupervised or semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that even the models that are popular, including auto encoder.",
                    "label": 0
                },
                {
                    "sent": "And IBM and the CNN and recurrent neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are two examples of the deep learning architecture and one on the top is DBM to DBN, and so we do a multiple layers of pre training which are typically unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Then the final step will do a fine tuning and which is a supervised and also in stacked autoencoder.",
                    "label": 0
                },
                {
                    "sent": "The pre training are also typically unsupervised.",
                    "label": 0
                },
                {
                    "sent": "And we also then we also do a final fine tuning which is supervised.",
                    "label": 0
                },
                {
                    "sent": "We actually there are some shown that this supervised pretraining actually typically have a less accuracy than unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is, uh, OK, what we want to solve in our formulation, so we actually have a lot of you know, supervised labels, supervised supervision in the data, so the so it would be great.",
                    "label": 0
                },
                {
                    "sent": "We really can show that using supervision in deep learning can really have a consistently higher accuracy than unsupervised pre training.",
                    "label": 0
                },
                {
                    "sent": "Also in the supervision there are actually two types you can use.",
                    "label": 0
                },
                {
                    "sent": "When is the?",
                    "label": 0
                },
                {
                    "sent": "Strong type, which is the class label per example or the weak type, which is the similarity among the different examples.",
                    "label": 0
                },
                {
                    "sent": "So in our framework we actually using the sugar formulation.",
                    "label": 0
                },
                {
                    "sent": "We actually can incorporate both the weak and strong supervision.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the sugar framework and sugar stands for super Supervision, Guided auto encoder and there are three major components in the framework.",
                    "label": 0
                },
                {
                    "sent": "The main framework and the auxiliary framework and also the bridge.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main framework is used to construct the the input, such as using the unsupervised autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The auxiliary framework is to regularize the lend framework by either pairwise similarity or these narrative constraints or strong supervision, and in our formulation, we use supervised hashing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the bridge is used to connect the main network and the Exhilarate network to enforce by enforcing the correlation of their their weights.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the formulation on the main network.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main difference from the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Traditional.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Encoder in now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or a formulation of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In network is the addition of the.",
                    "label": 0
                },
                {
                    "sent": "Penalty on sparsity.",
                    "label": 0
                },
                {
                    "sent": "So we have the Lambda, the one norm on the on the weight matrix, so this one will prevent overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is your formulation on the auxiliary framework, and we use the supervised hashing representation and I see here is a sine function and P is a weight matrix and T is the bias.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So then we put into the following objective function.",
                    "label": 0
                },
                {
                    "sent": "So this objective tries to minimize after the going into the hashing space, minimize the distance between examples of the same class or the similar examples, and want to maximize the difference of examples belong to different classes or the dissimilar examples.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have a matrix.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dentation, so we use the indicator matrix here to simplify the note notation.",
                    "label": 0
                },
                {
                    "sent": "We also to simplify the mass.",
                    "label": 0
                },
                {
                    "sent": "We assume the examples are zero centers, so that will can remove the bias T. And we also, in order to have a better numerical credibility, instead of using the sine function, we use assigned magnitude of the PX.",
                    "label": 0
                },
                {
                    "sent": "Then we have the following relaxed objective.",
                    "label": 0
                },
                {
                    "sent": "Then we have another constraint, which is to guarantee the the author the orthogonal and after the hashing conversion had the P&PT.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down here and see the bridge.",
                    "label": 0
                },
                {
                    "sent": "So the bridge here incorporates the.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder, the main network on the left and we have here we introduce the guiding parameter Alpha and the first term is the main network from the auto encoder.",
                    "label": 0
                },
                {
                    "sent": "And the second term, why men's Alpha and then is the from the supervised hashing and then we have a third term which is your correlation between the way to matrix and the supervised hashing matrix.",
                    "label": 0
                },
                {
                    "sent": "That is the penalty on the on the sparsity, and this one is obviously it's convex.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to solve this optimization, we use the alternate stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a sugar framework without.",
                    "label": 0
                },
                {
                    "sent": "Different extensions, so instead of using the traditional autoencoder we can also use the denoising autoencoder and also the controller contractive.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder Andy formulations are almost identical to the previous formulation, I show just.",
                    "label": 0
                },
                {
                    "sent": "The only difference is the log function.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To have a deep sugar, so we just.",
                    "label": 0
                },
                {
                    "sent": "Put layer by layer and so after we learned first layer we just abandoned the theory network.",
                    "label": 0
                },
                {
                    "sent": "Only maintain the.",
                    "label": 0
                },
                {
                    "sent": "The F the encoding function through a feedforward pass through the checking of the modules.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are experiments and we have a down using the sugar formulation.",
                    "label": 0
                },
                {
                    "sent": "So we use 9.",
                    "label": 0
                },
                {
                    "sent": "Benchmark datasets when you see MNIST datasets and the other are variations on the datasets, including the different shapes, rectangle and convex.",
                    "label": 0
                },
                {
                    "sent": "Also different variations on the MNIST datasets, including the basic one with random noise with image.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the baseline methods and we have compared including a SVM using the RBF kernel and the polynomial kernel and feedforward neural network and gated softmax classifier and non GSM.",
                    "label": 0
                },
                {
                    "sent": "And I say also RBM.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a performance evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the shallow architecture.",
                    "label": 0
                },
                {
                    "sent": "So the pre training we use the package and on the auto encoder and they are so this three additional parameters in the framework including the guiding coefficient.",
                    "label": 0
                },
                {
                    "sent": "The Alpha and which combines the supervised hashing with the autoencoder, the auxiliary network with the main network, also the amount of supervision, the labeled data also on the penalty on the sparsity constraints.",
                    "label": 0
                },
                {
                    "sent": "And the chart on the left and they are the we have the error rate on the traditional autoencoder, which is the red line on the top and the blue green bar.",
                    "label": 0
                },
                {
                    "sent": "The error rate of the sugar framework.",
                    "label": 0
                },
                {
                    "sent": "But I see that the they are consistently have lower error rate than the traditional autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Then the chart in the middle shows the amount of labeled data by comparing the sugar framework with the auto encoder.",
                    "label": 0
                },
                {
                    "sent": "And we also see the consistent lower rate of the three sugar framework as compared to auto encoder.",
                    "label": 0
                },
                {
                    "sent": "Then the last chart and there are two lines and the red line on the top.",
                    "label": 0
                },
                {
                    "sent": "Is he autoencoder without the sparsity penalty and the blue line below that show the when you have a different parameter?",
                    "label": 0
                },
                {
                    "sent": "How the error rate changes as the Lambda changes in value?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the figure here shows the.",
                    "label": 0
                },
                {
                    "sent": "When this positive changes and how the futures look like when the spicy change goes from 10% to 70%.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the results on the.",
                    "label": 0
                },
                {
                    "sent": "Sugar framework when combined with the denoising autoencoder, also the contractive autoencoder, and by using a different amount of labeled data can see they are very similar to the previous chart.",
                    "label": 0
                },
                {
                    "sent": "There the sugar framework have a consistently lower error rate than the traditional denoising autoencoder also contractive autoencoder.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and these are the results.",
                    "label": 0
                },
                {
                    "sent": "On the deep network on the deep architecture and the chart on the top, we have 3 sugar results.",
                    "label": 0
                },
                {
                    "sent": "So with one hidden layer up to three hidden layer and then the purple one is arrays out of the stacked autoencoder with Level 3.",
                    "label": 0
                },
                {
                    "sent": "So using the eight benchmark datasets, the only one that have a lower accuracy.",
                    "label": 0
                },
                {
                    "sent": "The sugar have a lower accuracy than the stacked autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Is the first one, the rectangle and all the other ones or the other seven?",
                    "label": 0
                },
                {
                    "sent": "We actually have a higher accuracy than the.",
                    "label": 0
                },
                {
                    "sent": "Traditional auto encoder.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a quick conclusion.",
                    "label": 0
                },
                {
                    "sent": "So the sugar framework proposed in this paper, we incorporate both weak supervision or pairwise constraints or strong supervision labeled into the autoencoder framework, and we show that both semi supervised or the fully supervised sugar is have a consistently higher accuracy than the.",
                    "label": 1
                },
                {
                    "sent": "Supervisor code encoder.",
                    "label": 0
                },
                {
                    "sent": "And we have shown using the nine benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "And also in the lab we because we always a telecommunication carrier provider.",
                    "label": 0
                },
                {
                    "sent": "So where we using the sugar framework to solve some problems in for the carrier and the code for sugar will be available for download from the hyper link below.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}