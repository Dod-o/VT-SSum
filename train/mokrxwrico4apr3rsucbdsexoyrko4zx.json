{
    "id": "mokrxwrico4apr3rsucbdsexoyrko4zx",
    "title": "Instanced-based mapping between thesauri and folksonomies",
    "info": {
        "author": [
            "Christian Wartena, Telematica Institut"
        ],
        "published": "Nov. 24, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/iswc08_wartena_ibm/",
    "segmentation": [
        [
            "Good morning everybody.",
            "Thanks for attending this presentation.",
            "My name is Christian."
        ],
        [
            "After them.",
            "So I will talk about more or less the same topic, but with other data and other methods.",
            "It is about interoperability of keywords.",
            "We did an experiment with categories from Wikipedia, which we will interpret is keywords and text from delicious.",
            "Then I will tell something about our methods to compute keyword similarity.",
            "I will report on the experiment and find the conclusion of course."
        ],
        [
            "So what we find in many cases it is that we have collections of documents.",
            "I always talk about documents, but it can also be pictures, movies and everything and these things are annotated with keywords and these are used for organization and retrieval.",
            "But if we have different collections, there might be different ways in which keywords or tags or something like that is used.",
            "If you have a formal system.",
            "Used by libraries.",
            "For example, we have in many cases thesaurus of keywords and the keyboards have to be restricted to words from the thesaurus.",
            "On the other hand side, if we have systems that are developed by end users.",
            "We talk about folksonomies.",
            "So we will try to find mappings between these keywords and we only will look at the usage so it's an instance based method.",
            "And we did as I said with."
        ],
        [
            "Wikipedia and delicious.",
            "So I think delicious is well known in this community.",
            "It's a social bookmarking site.",
            "And in most cases we can interpret bookmarks as tax for the URLs that are bookmarked.",
            "Entin interesting thing is that many delicious users also bookmark and take Wikipedia articles.",
            "Wikipedia has labels, labels, articles with one or more categories, and this is done by mainly by the authors of the articles.",
            "The categories are organized, organized hierarchically.",
            "And if you look at the way categories come up, there's a discussion and.",
            "And there they are very carefully and introducing you categories.",
            "So it has many characteristics of a tesouras.",
            "More than like a folksonomy.",
            "So."
        ],
        [
            "Is the problem of alignment some a bit more formal?",
            "Given a key voting system A, what is the most similar keywords in System B?",
            "In our case, given attack from Delicious, what is the most similar Wikipedia category or vice versa?",
            "And as I said, our approach is to interpret similarity as similarity of usage.",
            "And we compute it using a common subcollection that we have as I said.",
            "And evaluation finally is due."
        ],
        [
            "And by human judgment.",
            "So what is a measure to say?",
            "Keyboards have a similar usage.",
            "So one thing is we could.",
            "Investigate the distribution of of keywords over documents and compute diversions of the distributions.",
            "Or we can interpret the distribution as a factor in a document space and computer cosine.",
            "And another type of measure that's often uses the yakar coefficient."
        ],
        [
            "We will follow a slightly different approach.",
            "We will say keyboards have a similar usage.",
            "If the coworker was similar frequency with all other keyboards.",
            "So.",
            "Comparing to your car coefficient, for example, we use the frequency information that is interesting if we look at folksonomies.",
            "So in a more formal system we have only the keyword is present or not.",
            "But in a folksonomy we can always.",
            "We can also see how often a keyword is assigned or attack is assigned.",
            "And the other kind of information we use is the Co occurrence with all other words or terms.",
            "So this helps to cope with past data.",
            "In other words, terms are similar if they have similar Co occurrence patterns.",
            "And this approach is very similar.",
            "I found a few weeks ago to the tech context similarity defined by siroka to that he will present in a presentation tomorrow."
        ],
        [
            "So.",
            "So give an artificial example that makes things maybe a little bit more clear.",
            "We have distribution of we have keywords and Co occurrences of them in this table and we see that for example the Co occurrence of mission and Security Council is very low.",
            "Nevertheless, if we look at the patterns of Co occurrence, we see that the Co occurrence with all other words is very similar.",
            "Excuse me so this red things indicate the similarity with all other keyboards that we could conclude that mission and Security Council have very similar though their direct."
        ],
        [
            "Couple occurrences very low.",
            "So to make this formal, we define a distribution of Co occurring terms for each term this.",
            "In this case for a term.",
            "That is the way we define a distribution piece at bar.",
            "And that is mainly the.",
            "Average.",
            "Distribution of keywords in document in documents that contain the keyword Z, weighted by the relevance.",
            "Of the documents for Z."
        ],
        [
            "Once we have this distributions, we can start computing the distance between keywords.",
            "And that can be.",
            "We can simply do this using for example, the color clipart divergance.",
            "I think it's well known in this community.",
            "But we don't use that because it's an essay metric.",
            "Measure and we have problems by dividing by zero.",
            "So we use this."
        ],
        [
            "Symmetrized version Jensen Shannon Divergent which computes the mutual.",
            "Distance to a common mean to a mean distribution."
        ],
        [
            "Now we can start doing alignments.",
            "And we simply take the word or the keyboard from the other collection that has the smallest distance."
        ],
        [
            "We did two experiments.",
            "The first experiment was very small experiment to see whether the methods works at all.",
            "We had keywords from broadcasting programs and user text that were assigned in an small experiment by high school students.",
            "And this was only really small set, but it seemed to work very good and we did the experiment."
        ],
        [
            "The larger set.",
            "So we have a text from Delicious.",
            "And categories from Wikipedia and in the collection of delicious we got from Matias looks.",
            "We found that there were nearly 60,000 Wikipedia articles tagged, so this gives us a total of about half a million of text and category annotations.",
            "We computed the mappings for all taxand.",
            "And categories that occur and at least 10 documents.",
            "So this limits the mappings we have to compute to a few thousands, otherwise it would take too much time.",
            "And of course for categories or text for which we have only one instance, we can cannot expect good results.",
            "But we did not restrict our computation to only this Texan categories.",
            "We used the Co occurrence with all 50,000 tax to do the computations."
        ],
        [
            "We did a manual evaluation.",
            "And we classified the results in a few classes, namely broader terms, narrower terms, related terms.",
            "Unrelated I forgot to put in, it's identical.",
            "We also have some cases in which the source term is is not a keyword.",
            "For example, to read, you cannot expect Wikipedia category that matches with two reads and we try to filter out all these kind of text, but some survived.",
            "And there are also some cases in which we simply didn't know whether the mapping was correct."
        ],
        [
            "Or not.",
            "So these are the basic results.",
            "At the left bars are there is the mapping for categories to tax and the right the other way around.",
            "We see that in the left side you see the good ones.",
            "About the first half to up to this part is the good mappings to the right are the best mappings.",
            "Anne.",
            "They are fairly few identical or synonyms, but a lot of broader and narrower categories found.",
            "And Interestingly, mapping from categories to text, we mainly find broader categories and from text to categories narrow, never once.",
            "This might be due to the fact that Wikipedia users tend to use the most specific category becausw the other ones follow from the hierarchy and we only consider this most specific specific ones."
        ],
        [
            "So next we wanted to know whether the distance between the mapped elements is a good indication for the credit quality of the mapping.",
            "And it turns out it is that it is so the picture.",
            "Shows in in the left and the blue bars.",
            "Other is the evaluation for the 100 mappings with the closest distance.",
            "And the purple one is the mapping for the 100 mappings with the largest distance, and we see that the.",
            "Mappings with the closest distance are clearly better evaluated as the bad ones.",
            "The first picture is the categories to text and we see a similar pattern for the text to categories.",
            "So we can conclude that the distance we find is really a good indication of the."
        ],
        [
            "Of the quality of the mapping.",
            "Next, we wanted to know.",
            "What are the frequency of the keywords or that acts as a big influence on the quality we find?",
            "So we plotted the frequency of the text.",
            "From the source to the.",
            "Distance we found.",
            "And in the best mapping and we see there is no correlation."
        ],
        [
            "Finally, we wanted to compare with other methods.",
            "Enter computed also the mapping using a car coefficient.",
            "Anne.",
            "The first picture again is the mapping for categories to tags and the second one from text to categories.",
            "We also consider two variants of their car coefficient that was proposed.",
            "Bye.",
            "My people from University of Amsterdam.",
            "But we didn't find a great difference between those, but we found clearly that in at least in this case, our methods gives clearly better results.",
            "So we see a lot of more of related mappings and less unrelated.",
            "The number of synonyms are identical.",
            "This almost."
        ],
        [
            "Same.",
            "So if we discuss the results, we see that the mapping seems to work very well in this test.",
            "In first place we get very good mapping results.",
            "Further, we see that the distance of the message that the measure gives is a good indication of the quality.",
            "And we are rather insensitive to the frequency.",
            "That is, of course because we used the cover comes with all other tax and it's a very nice property, but be cause for many tax.",
            "We have only very few data.",
            "So Furthermore, we saw that the measure is better than your car coefficient.",
            "Yeah.",
            "Main reasons are as I said, because we consider the Co occurrence with auto attacks and the frequency with which attack is assigned to document.",
            "But this is of course we could apply this because we consider Texan delicious for collections like KB, we only have attack is present or not keyword president.",
            "You can not use that kind of information.",
            "So in such cases we think our advantage advantage of our methods is is not that big.",
            "But we have to try it out.",
            "We also did some experiments, but that's another paper with clustering of text.",
            "And it also works very well if for clustering this distance measure."
        ],
        [
            "So finally I can come to future work.",
            "So one of the weak points.",
            "Many of the reviewers noted notices that the evaluation is manual, so we put it on the website so everyone can inspect it, but it still has some kind of subjectivity.",
            "So one of the things we want to do is semantic grounding using.",
            "An external source like for example word net.",
            "Another thing is we compared now to a car measure, but of course there are a lot of other distance measures and we also would like to compare that to them.",
            "The more interesting things for future work is we use now documents that were completely annotated by the Wikipedia or authors and by the delicious community.",
            "But we can also consider collections with very small overlap or maybe even without overlap becausw our distance measure is based on a distribution of keywords over other keywords and not over documents.",
            "This if you can identify.",
            "Some keyboards that are the same in both systems.",
            "You have at that level a common base, so in principle if you can identify some keywords, you can compute the.",
            "The similarity between keyboards without a common subcollection.",
            "But that's the theory we have to check in practice whether that works.",
            "And another thing is.",
            "We find many mappings that are not too synonyms but to broader or narrower terms.",
            "An interesting thing would be, can we predict whether the mapping gives a synonym abroad or an error term in the way to do it would be to use the asymmetry that is present in the crew backlight plot divergance.",
            "So now we use the symmetrized version and we and this information was lost.",
            "But maybe it might be a very good idea to use that information."
        ],
        [
            "So to come to a conclusion, a very general conclusion using Co occurrence patterns seems to be fruitful approach.",
            "And on the other level we have seen.",
            "That frequent terms from Folksonomies do behave very similar to carefully assigned keywords.",
            "And this of course becausw.",
            "We used a method based on usage.",
            "The method works good, so the usage has to be similar.",
            "So you could also draw the conclusion that Folksonomies work very good in assigning keywords.",
            "Thank you for your attention.",
            "You have time for questions, but I'll start with one.",
            "How did the?",
            "Did the users judge relatedness versus unrelated NIS?",
            "'cause everything could be in some ways related to something else.",
            "Yes, I don't know.",
            "I have some examples here.",
            "So this these are things that are really closely related, but you couldn't call a synonym.",
            "Let me give an example like a book and novel things like that.",
            "Maybe someone is is a narrow term, but you're not sure whether so you classify it is is related.",
            "In most cases it's rather clear that you say OK, there's the same domain, it's it's the same object with another aspect, but it's not very formal, I agree.",
            "So I mean, did you how many users were involved?",
            "How many users were involved in, well, how many?",
            "And was there some sort of a criteria that they had to follow to make that judgment?",
            "No, there was no formal criterion, and we only did it ourselves.",
            "So two persons were involved.",
            "I have a question.",
            "How is, in your opinion, at tagging fundamentally different from assigning keywords except for the fact that tags are repeatedly added by multiple individuals?",
            "I mean for example, adding a category in Wikipedia is also a base in the social process, because if people disagree they remove it, and if they agree they leave it in.",
            "But basically adding a category in Wikipedia or adding a keyword in a document is in my opinion not so fundamentally different from adding tags.",
            "So you're finding that.",
            "The behavior is pretty simple.",
            "It doesn't surprise me.",
            "Yes, I agree, it's it's very similar.",
            "Even if you look at documentaries working in archives, they also have discussions about it and collaborate in doing this.",
            "But a major difference is that in most systems, even Wikipedia, you start choosing from existing categories.",
            "You have a system of categories and you choose one of this.",
            "Of course you can add a new one, but.",
            "You start with looking.",
            "Is there an existing one and in this is you are not restricted to something well in that respect.",
            "That would disagree because we have experiments going on with companies who have been adding keywords in using uncontrolled keywords for a long time and basically their procedure is not very different from tagging except for the fact that confirming of a keyword by users is not done explicitly by repeatedly adding the same tag or the same string to the object.",
            "But believing it in so the only difference would then be that the social graph associated to the behavior is explicit and you can do better mining.",
            "But the basic relationship between the resource and that string shouldn't be fundamentally different.",
            "I mean you have open, you have keyboard based annotation based on controlled vocabularies and you have been having it for decades on uncontrolled vocabularies.",
            "Yes, if you look at uncontrolled vocabularies the difference are marginalized.",
            "Thanks for the talk.",
            "I've got a question regarding the evaluation as well.",
            "If I understand correctly, you just check for the correctness of the answers that were given so you only check for precision and not for recall.",
            "Do you have any estimation of how a how you could do this to estimate recall as well?",
            "And whether given your method, you might just have the found the simple cases.",
            "To be mapped.",
            "Yes, it's very difficult to compute recall in this situation, 'cause you need the gold standard.",
            "And this is, of course, why we also want to do the method on different collections, so you can also compute the recall.",
            "Did you?",
            "Did you suffer from?",
            "Ambiguity of terms mean one.",
            "Anyone experiment we were doing when people were judging whether the result was a correct match or not.",
            "They would look at the at the tag regardless of anything else or how it was used.",
            "And so you might actually correlate it correctly, but then it's for completely different meaning according to the user.",
            "But was that a concern at all or?",
            "Yes, you find that types of ambiguities, but there are very few.",
            "If you evaluate 100 and that made it maybe 1.",
            "Most most texts are rather unambiguous, or the it's correlates to another category attack that has the same ambiguity.",
            "That's another kind, so there's no problem.",
            "I think I mean the the interesting thing to see is that when you when you search for these things in Wikipedia, almost anything can be disambiguated.",
            "According to Wikipedia, is that it will fit with quite a few things that you know one would have never heard of.",
            "But we can talk about that offline.",
            "If no more questions.",
            "We moved to the third talk.",
            "Well, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "Thanks for attending this presentation.",
                    "label": 0
                },
                {
                    "sent": "My name is Christian.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After them.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about more or less the same topic, but with other data and other methods.",
                    "label": 0
                },
                {
                    "sent": "It is about interoperability of keywords.",
                    "label": 1
                },
                {
                    "sent": "We did an experiment with categories from Wikipedia, which we will interpret is keywords and text from delicious.",
                    "label": 0
                },
                {
                    "sent": "Then I will tell something about our methods to compute keyword similarity.",
                    "label": 0
                },
                {
                    "sent": "I will report on the experiment and find the conclusion of course.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we find in many cases it is that we have collections of documents.",
                    "label": 0
                },
                {
                    "sent": "I always talk about documents, but it can also be pictures, movies and everything and these things are annotated with keywords and these are used for organization and retrieval.",
                    "label": 1
                },
                {
                    "sent": "But if we have different collections, there might be different ways in which keywords or tags or something like that is used.",
                    "label": 0
                },
                {
                    "sent": "If you have a formal system.",
                    "label": 0
                },
                {
                    "sent": "Used by libraries.",
                    "label": 0
                },
                {
                    "sent": "For example, we have in many cases thesaurus of keywords and the keyboards have to be restricted to words from the thesaurus.",
                    "label": 0
                },
                {
                    "sent": "On the other hand side, if we have systems that are developed by end users.",
                    "label": 0
                },
                {
                    "sent": "We talk about folksonomies.",
                    "label": 0
                },
                {
                    "sent": "So we will try to find mappings between these keywords and we only will look at the usage so it's an instance based method.",
                    "label": 0
                },
                {
                    "sent": "And we did as I said with.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wikipedia and delicious.",
                    "label": 0
                },
                {
                    "sent": "So I think delicious is well known in this community.",
                    "label": 0
                },
                {
                    "sent": "It's a social bookmarking site.",
                    "label": 0
                },
                {
                    "sent": "And in most cases we can interpret bookmarks as tax for the URLs that are bookmarked.",
                    "label": 1
                },
                {
                    "sent": "Entin interesting thing is that many delicious users also bookmark and take Wikipedia articles.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia has labels, labels, articles with one or more categories, and this is done by mainly by the authors of the articles.",
                    "label": 1
                },
                {
                    "sent": "The categories are organized, organized hierarchically.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the way categories come up, there's a discussion and.",
                    "label": 0
                },
                {
                    "sent": "And there they are very carefully and introducing you categories.",
                    "label": 0
                },
                {
                    "sent": "So it has many characteristics of a tesouras.",
                    "label": 0
                },
                {
                    "sent": "More than like a folksonomy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the problem of alignment some a bit more formal?",
                    "label": 0
                },
                {
                    "sent": "Given a key voting system A, what is the most similar keywords in System B?",
                    "label": 1
                },
                {
                    "sent": "In our case, given attack from Delicious, what is the most similar Wikipedia category or vice versa?",
                    "label": 1
                },
                {
                    "sent": "And as I said, our approach is to interpret similarity as similarity of usage.",
                    "label": 0
                },
                {
                    "sent": "And we compute it using a common subcollection that we have as I said.",
                    "label": 0
                },
                {
                    "sent": "And evaluation finally is due.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And by human judgment.",
                    "label": 0
                },
                {
                    "sent": "So what is a measure to say?",
                    "label": 0
                },
                {
                    "sent": "Keyboards have a similar usage.",
                    "label": 1
                },
                {
                    "sent": "So one thing is we could.",
                    "label": 1
                },
                {
                    "sent": "Investigate the distribution of of keywords over documents and compute diversions of the distributions.",
                    "label": 0
                },
                {
                    "sent": "Or we can interpret the distribution as a factor in a document space and computer cosine.",
                    "label": 0
                },
                {
                    "sent": "And another type of measure that's often uses the yakar coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will follow a slightly different approach.",
                    "label": 0
                },
                {
                    "sent": "We will say keyboards have a similar usage.",
                    "label": 0
                },
                {
                    "sent": "If the coworker was similar frequency with all other keyboards.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Comparing to your car coefficient, for example, we use the frequency information that is interesting if we look at folksonomies.",
                    "label": 0
                },
                {
                    "sent": "So in a more formal system we have only the keyword is present or not.",
                    "label": 0
                },
                {
                    "sent": "But in a folksonomy we can always.",
                    "label": 0
                },
                {
                    "sent": "We can also see how often a keyword is assigned or attack is assigned.",
                    "label": 0
                },
                {
                    "sent": "And the other kind of information we use is the Co occurrence with all other words or terms.",
                    "label": 1
                },
                {
                    "sent": "So this helps to cope with past data.",
                    "label": 0
                },
                {
                    "sent": "In other words, terms are similar if they have similar Co occurrence patterns.",
                    "label": 1
                },
                {
                    "sent": "And this approach is very similar.",
                    "label": 0
                },
                {
                    "sent": "I found a few weeks ago to the tech context similarity defined by siroka to that he will present in a presentation tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So give an artificial example that makes things maybe a little bit more clear.",
                    "label": 0
                },
                {
                    "sent": "We have distribution of we have keywords and Co occurrences of them in this table and we see that for example the Co occurrence of mission and Security Council is very low.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, if we look at the patterns of Co occurrence, we see that the Co occurrence with all other words is very similar.",
                    "label": 0
                },
                {
                    "sent": "Excuse me so this red things indicate the similarity with all other keyboards that we could conclude that mission and Security Council have very similar though their direct.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple occurrences very low.",
                    "label": 0
                },
                {
                    "sent": "So to make this formal, we define a distribution of Co occurring terms for each term this.",
                    "label": 0
                },
                {
                    "sent": "In this case for a term.",
                    "label": 0
                },
                {
                    "sent": "That is the way we define a distribution piece at bar.",
                    "label": 0
                },
                {
                    "sent": "And that is mainly the.",
                    "label": 0
                },
                {
                    "sent": "Average.",
                    "label": 0
                },
                {
                    "sent": "Distribution of keywords in document in documents that contain the keyword Z, weighted by the relevance.",
                    "label": 1
                },
                {
                    "sent": "Of the documents for Z.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we have this distributions, we can start computing the distance between keywords.",
                    "label": 0
                },
                {
                    "sent": "And that can be.",
                    "label": 0
                },
                {
                    "sent": "We can simply do this using for example, the color clipart divergance.",
                    "label": 0
                },
                {
                    "sent": "I think it's well known in this community.",
                    "label": 0
                },
                {
                    "sent": "But we don't use that because it's an essay metric.",
                    "label": 0
                },
                {
                    "sent": "Measure and we have problems by dividing by zero.",
                    "label": 0
                },
                {
                    "sent": "So we use this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Symmetrized version Jensen Shannon Divergent which computes the mutual.",
                    "label": 0
                },
                {
                    "sent": "Distance to a common mean to a mean distribution.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can start doing alignments.",
                    "label": 0
                },
                {
                    "sent": "And we simply take the word or the keyboard from the other collection that has the smallest distance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did two experiments.",
                    "label": 0
                },
                {
                    "sent": "The first experiment was very small experiment to see whether the methods works at all.",
                    "label": 0
                },
                {
                    "sent": "We had keywords from broadcasting programs and user text that were assigned in an small experiment by high school students.",
                    "label": 1
                },
                {
                    "sent": "And this was only really small set, but it seemed to work very good and we did the experiment.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The larger set.",
                    "label": 0
                },
                {
                    "sent": "So we have a text from Delicious.",
                    "label": 0
                },
                {
                    "sent": "And categories from Wikipedia and in the collection of delicious we got from Matias looks.",
                    "label": 0
                },
                {
                    "sent": "We found that there were nearly 60,000 Wikipedia articles tagged, so this gives us a total of about half a million of text and category annotations.",
                    "label": 1
                },
                {
                    "sent": "We computed the mappings for all taxand.",
                    "label": 1
                },
                {
                    "sent": "And categories that occur and at least 10 documents.",
                    "label": 0
                },
                {
                    "sent": "So this limits the mappings we have to compute to a few thousands, otherwise it would take too much time.",
                    "label": 1
                },
                {
                    "sent": "And of course for categories or text for which we have only one instance, we can cannot expect good results.",
                    "label": 0
                },
                {
                    "sent": "But we did not restrict our computation to only this Texan categories.",
                    "label": 0
                },
                {
                    "sent": "We used the Co occurrence with all 50,000 tax to do the computations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did a manual evaluation.",
                    "label": 1
                },
                {
                    "sent": "And we classified the results in a few classes, namely broader terms, narrower terms, related terms.",
                    "label": 0
                },
                {
                    "sent": "Unrelated I forgot to put in, it's identical.",
                    "label": 0
                },
                {
                    "sent": "We also have some cases in which the source term is is not a keyword.",
                    "label": 1
                },
                {
                    "sent": "For example, to read, you cannot expect Wikipedia category that matches with two reads and we try to filter out all these kind of text, but some survived.",
                    "label": 0
                },
                {
                    "sent": "And there are also some cases in which we simply didn't know whether the mapping was correct.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or not.",
                    "label": 0
                },
                {
                    "sent": "So these are the basic results.",
                    "label": 0
                },
                {
                    "sent": "At the left bars are there is the mapping for categories to tax and the right the other way around.",
                    "label": 0
                },
                {
                    "sent": "We see that in the left side you see the good ones.",
                    "label": 0
                },
                {
                    "sent": "About the first half to up to this part is the good mappings to the right are the best mappings.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "They are fairly few identical or synonyms, but a lot of broader and narrower categories found.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly, mapping from categories to text, we mainly find broader categories and from text to categories narrow, never once.",
                    "label": 0
                },
                {
                    "sent": "This might be due to the fact that Wikipedia users tend to use the most specific category becausw the other ones follow from the hierarchy and we only consider this most specific specific ones.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next we wanted to know whether the distance between the mapped elements is a good indication for the credit quality of the mapping.",
                    "label": 0
                },
                {
                    "sent": "And it turns out it is that it is so the picture.",
                    "label": 0
                },
                {
                    "sent": "Shows in in the left and the blue bars.",
                    "label": 0
                },
                {
                    "sent": "Other is the evaluation for the 100 mappings with the closest distance.",
                    "label": 0
                },
                {
                    "sent": "And the purple one is the mapping for the 100 mappings with the largest distance, and we see that the.",
                    "label": 1
                },
                {
                    "sent": "Mappings with the closest distance are clearly better evaluated as the bad ones.",
                    "label": 1
                },
                {
                    "sent": "The first picture is the categories to text and we see a similar pattern for the text to categories.",
                    "label": 0
                },
                {
                    "sent": "So we can conclude that the distance we find is really a good indication of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the quality of the mapping.",
                    "label": 0
                },
                {
                    "sent": "Next, we wanted to know.",
                    "label": 0
                },
                {
                    "sent": "What are the frequency of the keywords or that acts as a big influence on the quality we find?",
                    "label": 0
                },
                {
                    "sent": "So we plotted the frequency of the text.",
                    "label": 0
                },
                {
                    "sent": "From the source to the.",
                    "label": 0
                },
                {
                    "sent": "Distance we found.",
                    "label": 0
                },
                {
                    "sent": "And in the best mapping and we see there is no correlation.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we wanted to compare with other methods.",
                    "label": 0
                },
                {
                    "sent": "Enter computed also the mapping using a car coefficient.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The first picture again is the mapping for categories to tags and the second one from text to categories.",
                    "label": 1
                },
                {
                    "sent": "We also consider two variants of their car coefficient that was proposed.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "My people from University of Amsterdam.",
                    "label": 0
                },
                {
                    "sent": "But we didn't find a great difference between those, but we found clearly that in at least in this case, our methods gives clearly better results.",
                    "label": 0
                },
                {
                    "sent": "So we see a lot of more of related mappings and less unrelated.",
                    "label": 0
                },
                {
                    "sent": "The number of synonyms are identical.",
                    "label": 0
                },
                {
                    "sent": "This almost.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same.",
                    "label": 0
                },
                {
                    "sent": "So if we discuss the results, we see that the mapping seems to work very well in this test.",
                    "label": 0
                },
                {
                    "sent": "In first place we get very good mapping results.",
                    "label": 1
                },
                {
                    "sent": "Further, we see that the distance of the message that the measure gives is a good indication of the quality.",
                    "label": 1
                },
                {
                    "sent": "And we are rather insensitive to the frequency.",
                    "label": 0
                },
                {
                    "sent": "That is, of course because we used the cover comes with all other tax and it's a very nice property, but be cause for many tax.",
                    "label": 0
                },
                {
                    "sent": "We have only very few data.",
                    "label": 0
                },
                {
                    "sent": "So Furthermore, we saw that the measure is better than your car coefficient.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Main reasons are as I said, because we consider the Co occurrence with auto attacks and the frequency with which attack is assigned to document.",
                    "label": 1
                },
                {
                    "sent": "But this is of course we could apply this because we consider Texan delicious for collections like KB, we only have attack is present or not keyword president.",
                    "label": 0
                },
                {
                    "sent": "You can not use that kind of information.",
                    "label": 0
                },
                {
                    "sent": "So in such cases we think our advantage advantage of our methods is is not that big.",
                    "label": 0
                },
                {
                    "sent": "But we have to try it out.",
                    "label": 0
                },
                {
                    "sent": "We also did some experiments, but that's another paper with clustering of text.",
                    "label": 0
                },
                {
                    "sent": "And it also works very well if for clustering this distance measure.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally I can come to future work.",
                    "label": 1
                },
                {
                    "sent": "So one of the weak points.",
                    "label": 1
                },
                {
                    "sent": "Many of the reviewers noted notices that the evaluation is manual, so we put it on the website so everyone can inspect it, but it still has some kind of subjectivity.",
                    "label": 0
                },
                {
                    "sent": "So one of the things we want to do is semantic grounding using.",
                    "label": 0
                },
                {
                    "sent": "An external source like for example word net.",
                    "label": 0
                },
                {
                    "sent": "Another thing is we compared now to a car measure, but of course there are a lot of other distance measures and we also would like to compare that to them.",
                    "label": 1
                },
                {
                    "sent": "The more interesting things for future work is we use now documents that were completely annotated by the Wikipedia or authors and by the delicious community.",
                    "label": 0
                },
                {
                    "sent": "But we can also consider collections with very small overlap or maybe even without overlap becausw our distance measure is based on a distribution of keywords over other keywords and not over documents.",
                    "label": 0
                },
                {
                    "sent": "This if you can identify.",
                    "label": 0
                },
                {
                    "sent": "Some keyboards that are the same in both systems.",
                    "label": 0
                },
                {
                    "sent": "You have at that level a common base, so in principle if you can identify some keywords, you can compute the.",
                    "label": 0
                },
                {
                    "sent": "The similarity between keyboards without a common subcollection.",
                    "label": 0
                },
                {
                    "sent": "But that's the theory we have to check in practice whether that works.",
                    "label": 0
                },
                {
                    "sent": "And another thing is.",
                    "label": 0
                },
                {
                    "sent": "We find many mappings that are not too synonyms but to broader or narrower terms.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing would be, can we predict whether the mapping gives a synonym abroad or an error term in the way to do it would be to use the asymmetry that is present in the crew backlight plot divergance.",
                    "label": 0
                },
                {
                    "sent": "So now we use the symmetrized version and we and this information was lost.",
                    "label": 0
                },
                {
                    "sent": "But maybe it might be a very good idea to use that information.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to come to a conclusion, a very general conclusion using Co occurrence patterns seems to be fruitful approach.",
                    "label": 0
                },
                {
                    "sent": "And on the other level we have seen.",
                    "label": 0
                },
                {
                    "sent": "That frequent terms from Folksonomies do behave very similar to carefully assigned keywords.",
                    "label": 1
                },
                {
                    "sent": "And this of course becausw.",
                    "label": 0
                },
                {
                    "sent": "We used a method based on usage.",
                    "label": 0
                },
                {
                    "sent": "The method works good, so the usage has to be similar.",
                    "label": 0
                },
                {
                    "sent": "So you could also draw the conclusion that Folksonomies work very good in assigning keywords.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "You have time for questions, but I'll start with one.",
                    "label": 0
                },
                {
                    "sent": "How did the?",
                    "label": 0
                },
                {
                    "sent": "Did the users judge relatedness versus unrelated NIS?",
                    "label": 0
                },
                {
                    "sent": "'cause everything could be in some ways related to something else.",
                    "label": 0
                },
                {
                    "sent": "Yes, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I have some examples here.",
                    "label": 0
                },
                {
                    "sent": "So this these are things that are really closely related, but you couldn't call a synonym.",
                    "label": 0
                },
                {
                    "sent": "Let me give an example like a book and novel things like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone is is a narrow term, but you're not sure whether so you classify it is is related.",
                    "label": 0
                },
                {
                    "sent": "In most cases it's rather clear that you say OK, there's the same domain, it's it's the same object with another aspect, but it's not very formal, I agree.",
                    "label": 0
                },
                {
                    "sent": "So I mean, did you how many users were involved?",
                    "label": 0
                },
                {
                    "sent": "How many users were involved in, well, how many?",
                    "label": 0
                },
                {
                    "sent": "And was there some sort of a criteria that they had to follow to make that judgment?",
                    "label": 0
                },
                {
                    "sent": "No, there was no formal criterion, and we only did it ourselves.",
                    "label": 0
                },
                {
                    "sent": "So two persons were involved.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "How is, in your opinion, at tagging fundamentally different from assigning keywords except for the fact that tags are repeatedly added by multiple individuals?",
                    "label": 0
                },
                {
                    "sent": "I mean for example, adding a category in Wikipedia is also a base in the social process, because if people disagree they remove it, and if they agree they leave it in.",
                    "label": 0
                },
                {
                    "sent": "But basically adding a category in Wikipedia or adding a keyword in a document is in my opinion not so fundamentally different from adding tags.",
                    "label": 0
                },
                {
                    "sent": "So you're finding that.",
                    "label": 0
                },
                {
                    "sent": "The behavior is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It doesn't surprise me.",
                    "label": 0
                },
                {
                    "sent": "Yes, I agree, it's it's very similar.",
                    "label": 0
                },
                {
                    "sent": "Even if you look at documentaries working in archives, they also have discussions about it and collaborate in doing this.",
                    "label": 0
                },
                {
                    "sent": "But a major difference is that in most systems, even Wikipedia, you start choosing from existing categories.",
                    "label": 0
                },
                {
                    "sent": "You have a system of categories and you choose one of this.",
                    "label": 0
                },
                {
                    "sent": "Of course you can add a new one, but.",
                    "label": 0
                },
                {
                    "sent": "You start with looking.",
                    "label": 0
                },
                {
                    "sent": "Is there an existing one and in this is you are not restricted to something well in that respect.",
                    "label": 0
                },
                {
                    "sent": "That would disagree because we have experiments going on with companies who have been adding keywords in using uncontrolled keywords for a long time and basically their procedure is not very different from tagging except for the fact that confirming of a keyword by users is not done explicitly by repeatedly adding the same tag or the same string to the object.",
                    "label": 0
                },
                {
                    "sent": "But believing it in so the only difference would then be that the social graph associated to the behavior is explicit and you can do better mining.",
                    "label": 0
                },
                {
                    "sent": "But the basic relationship between the resource and that string shouldn't be fundamentally different.",
                    "label": 0
                },
                {
                    "sent": "I mean you have open, you have keyboard based annotation based on controlled vocabularies and you have been having it for decades on uncontrolled vocabularies.",
                    "label": 0
                },
                {
                    "sent": "Yes, if you look at uncontrolled vocabularies the difference are marginalized.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I've got a question regarding the evaluation as well.",
                    "label": 0
                },
                {
                    "sent": "If I understand correctly, you just check for the correctness of the answers that were given so you only check for precision and not for recall.",
                    "label": 0
                },
                {
                    "sent": "Do you have any estimation of how a how you could do this to estimate recall as well?",
                    "label": 0
                },
                {
                    "sent": "And whether given your method, you might just have the found the simple cases.",
                    "label": 0
                },
                {
                    "sent": "To be mapped.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's very difficult to compute recall in this situation, 'cause you need the gold standard.",
                    "label": 0
                },
                {
                    "sent": "And this is, of course, why we also want to do the method on different collections, so you can also compute the recall.",
                    "label": 0
                },
                {
                    "sent": "Did you?",
                    "label": 0
                },
                {
                    "sent": "Did you suffer from?",
                    "label": 0
                },
                {
                    "sent": "Ambiguity of terms mean one.",
                    "label": 0
                },
                {
                    "sent": "Anyone experiment we were doing when people were judging whether the result was a correct match or not.",
                    "label": 0
                },
                {
                    "sent": "They would look at the at the tag regardless of anything else or how it was used.",
                    "label": 0
                },
                {
                    "sent": "And so you might actually correlate it correctly, but then it's for completely different meaning according to the user.",
                    "label": 0
                },
                {
                    "sent": "But was that a concern at all or?",
                    "label": 0
                },
                {
                    "sent": "Yes, you find that types of ambiguities, but there are very few.",
                    "label": 0
                },
                {
                    "sent": "If you evaluate 100 and that made it maybe 1.",
                    "label": 0
                },
                {
                    "sent": "Most most texts are rather unambiguous, or the it's correlates to another category attack that has the same ambiguity.",
                    "label": 0
                },
                {
                    "sent": "That's another kind, so there's no problem.",
                    "label": 0
                },
                {
                    "sent": "I think I mean the the interesting thing to see is that when you when you search for these things in Wikipedia, almost anything can be disambiguated.",
                    "label": 0
                },
                {
                    "sent": "According to Wikipedia, is that it will fit with quite a few things that you know one would have never heard of.",
                    "label": 0
                },
                {
                    "sent": "But we can talk about that offline.",
                    "label": 0
                },
                {
                    "sent": "If no more questions.",
                    "label": 0
                },
                {
                    "sent": "We moved to the third talk.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}