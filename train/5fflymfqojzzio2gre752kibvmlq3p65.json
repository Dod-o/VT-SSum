{
    "id": "5fflymfqojzzio2gre752kibvmlq3p65",
    "title": "Output Kernel Learning Methods",
    "info": {
        "author": [
            "Francesco Dinuzzo, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_dinuzzo_output/",
    "segmentation": [
        [
            "This is joint work with chain so long and can you forgive me too and it's about multi task learning so we want to address the situation where we want to learn simultaneously multiple tasks and on the same time you would like to estimate the relationships between these tasks.",
            "So."
        ],
        [
            "We already had a talk before about multitask learning with a lot of motivations, why learning simultaneously multiple task can be beneficial, but I'm going to give some further examples and the simplest examples that I know too that illustrates very well why multitask learning can be useful is multiple regression.",
            "So let's consider this very simple application in pharmacokinetics and what you can see here.",
            "Estimates these curves are all estimates of.",
            "The concentration profile overtime of some drug in the blood of 27 human subjects, and.",
            "You can see all these curves start from zero, which is the time where the drug we have this Drug Administration.",
            "Then you have a region where the concentration is increasing and then you have an elimination phase where the drug is expelled is eliminated from the Organism.",
            "So all these curves have the same shape.",
            "But as you can see there is microscopic Inter individual variability.",
            "So the lowest of these curves has a maximum at around 40, whereas the highest of this curve says the maximum weight around 100.",
            "So now the problem is you may want to estimate these curves from the."
        ],
        [
            "The sets that look like this, so for each of the subjects you can imagine, you know that possibly you have very few samples like two or three points per subject.",
            "So of course if you try to solve the regression problem with two or three data points.",
            "In dependently this would.",
            "Not give a good solution.",
            "But the only way we can solve the problem is to put together the datasets from the different subjects and exploiting the knowledge that the shape is the same.",
            "We can try to estimate all the curves simultaneously."
        ],
        [
            "OK, another example."
        ],
        [
            "All of multi task learning which is very successful is collaborative filtering, recommender systems or more generally user preference estimation.",
            "So in this type of problem which probably became popular after the Netflix competition, we have a collection.",
            "The datasets are collection of ratings assigned by several users to a set of items.",
            "The items maybe movies.",
            "They may be books or whatever.",
            "And the goal is of course to estimate the preferences of every user for all the items in the database.",
            "Now here see this is a popular website for movie recommendations called Movie Lens and you can register on this website.",
            "You can browse a huge database of movies and you can assign some ratings from one to five stars to each of these movies so that the system can give you then recommendations of the world.",
            "Now the problem here is that of course you cannot give a rating for all the movies in the database.",
            "You will be only be able to give you writing for a very small subset.",
            "There are thousands and thousands of movies, so preference profiles are different from every user.",
            "However, we again want to exploit the fact that similar users have similar preferences.",
            "And what is interesting about these problems of collaborative filtering, in my opinion, is that you have also."
        ],
        [
            "Only information in addition to the ratings, you may have additional data about the items.",
            "So for example if we talk about movie, you can take one movie and you go in a website like this.",
            "the Internet movie database and you can retrieve a lot of information such as the ear when it was released, the name of the director, the name of the actors in other, a lot of information you may have additional data about, the users like gender, age and occupation and so on.",
            "And you may have even additional metadata about the rating themselves.",
            "So for example, you know that.",
            "Timestamp attached when the rating was given or you have tags attached to the ratings, or you may also have a small pieces of text associated with the rating.",
            "So the question is can we combine all these data from different users to better estimate individual preferences?",
            "So what is?"
        ],
        [
            "In common between these two problems, is the data set structure and the point is that we have many tasks, so many subjects or many users, and sampling can be very sparse in the sense that for every task we have very few samples, so."
        ],
        [
            "The case of pharmacokinetics.",
            "We have very few measurements in time for each subject, and in the case of recommender systems, we are able to measure a rating only for a very tiny subset of the whole set of movies, however."
        ],
        [
            "Each sample is shared by many tasks and this is what allows us to somehow do something.",
            "Another problem that this seemingly unrelated with this is odd."
        ],
        [
            "Recognition with, let's say multi classification classification with structured discovery.",
            "So this is a multi class classification problem.",
            "Very popular in computer and computer vision.",
            "It's a contact 156.",
            "We have a data set of images of different categories and the goal is the classical goal is to build a classifier multiclass classifier with good generalization.",
            "Performance is so given a new image we want to predict the label.",
            "But another thing that you may want to do is for example to discover relationships between the different classes.",
            "And about relationship I mean something."
        ],
        [
            "Like this, so you may want to automatically solve the classification problem on the same time produce graph, for example, that expresses the relationship between the classes.",
            "So for example, that tells us that images of motor bikes are very similar to images of mountain bikes, images of chopsticks, baseball bat, sword have something in common, and so on.",
            "And you want to obtain this without you know, hard coding this information in the in the in the in the model.",
            "OK, so now I'm going to introduce."
        ],
        [
            "Use some methods to solve multiple multi task learning problems and on the same time try to estimate relationship between the tasks in this in."
        ],
        [
            "Context of this talk.",
            "When I talk about multitask learning, I essentially talk about this multitask, supervised learning, where the goal is to synthesize multiple functions.",
            "FJ mapping some input set X can be any set set of images set of text documents, and so on into some output set, which can be a set of scholars simply or something else.",
            "From multiple data sets of input output pairs.",
            "And here for simplicity, I assume that the inputs it is common for all the for all the tasks.",
            "But of course one can think about more general situations where the input sets are different.",
            "So if I want to use a kernel methods to solve this problem, whatever method they use at the end of the day, I have to specify a kernel multitask kernel of this form, so I have to specify a similarity measure that for every pair of inputs X1 and X2, and for every pair of task in this is inj.",
            "Specifies the joint similarity between inputs and tasks.",
            "Now equivalently, I can express the same information by using a matrix valued function H. Of course, that looks formally more similar to the usual kernel because it takes as an argument only the two inputs X one X2, but in Maps into matrices M by M&IJ entries is exactly this similarity here.",
            "So there are two equivalent ways to represent this similarity, But the problem is, given specific multitasking problem, how do we choose the kernel?",
            "How do we specify?",
            "Some meaningful joint similarity measure is not is not trivial becausw we have a lot of flexibility here, So what many people do."
        ],
        [
            "It has been done a lot in the literature is to make this assumption here that simply says OK, I'm going to assume that this similarity measure can be decomposed as the product of a kernel on the inputs only, and a kernel on the task indices that expresses the similarities within the task.",
            "I in the task J.",
            "Or equivalently, if you think in terms of metrics valued kernel, you are going to have a scalar kernel K which multiplies a constant matrix L. And of course, this contest custom matrix has to be symmetric and positive semidefinite for all this to be a valid kernel.",
            "So we're going to call KX the input kernel.",
            "And key Y the output kernel.",
            "Or equivalently, this matrix L the output kernel.",
            "So we.",
            "Want to solve the multitasking problem by using a regularization?"
        ],
        [
            "Approach.",
            "So we build a reproducing kernel Hilbert space of functions of vector valued functions associated with this decomposable kernel here.",
            "And we optimize over this space by minimizing the sum of some loss term over the tasks and some regularization which is simply in this case the squared norm in the kitchen.",
            "So the usual framework of regularization.",
            "And we know by the representative theorem that the solution looks like this.",
            "So the J component of the optimal function FFJ.",
            "Can be written in this form where we have kernels, expansions on the individual datasets.",
            "And then we have a linear combination of these kernel expansions that involve this LJK.",
            "So the entries of this matrix L. Now, not only the solution depends on L via this combination here, but also the coefficient CIJ are going to depend implicitly on L. Because of the optimization problem here, so the norm here depends on L and also the space itself.",
            "So the question is, in this context, how do we choose the output kernel?",
            "Of course there is a knave baseline, which is just choose L to the identity and this would correspond to independent single task learning.",
            "So if we choose L to the identity, essentially what we're doing is we're learning all the components of this vector valued functions in dependently.",
            "The other extreme is pulling single task learning, so I choose L equals to the matrix of all ones or a constant matrix.",
            "In this case, what we are doing is essentially learn a vector valued functions with components which are all equal.",
            "So essentially we are learning only one function for all the tasks.",
            "So we can do better than these baselines.",
            "Of course, in many situations, sometimes actually these are optimal already, but in many cases they are not.",
            "So if we have some prior knowledge about the relationship between the task, we should use it to design this L in advance, but in practice I do see many problems where we have quantitative information about the similarity between task in advance.",
            "So the most meaningful way to proceed seems to be to learn this L from the data simultaneously with our function in the opposing cannabis space.",
            "So one possibility that immediately comes to mind is to use multiple kernel."
        ],
        [
            "Learning so one can design some Dictionary of basis kernels KK.",
            "Take a chronic combination of these.",
            "And then learn the weights of this combination simultaneously with the function in the architectures this is 1 possible approach is very general and of course I can use this approach also for multi task learning because I think a combination of multitasking kernels.",
            "In particular, I can take combination of decomposable multitask kernels and I can do the use the same machinery of multi task learning.",
            "So even more specific I can choose this KXK.",
            "To be in the same kernel and what I'm doing effectively is to learn a combination of these matrices kyk.",
            "By solving an optimization problem.",
            "But there are two issues with this approach.",
            "OK, the first issue is that the maximum number of kernels that I can use is in fact limited by memory constraints.",
            "So although in principle I can use an arbitrarily high number of kernel, even an infinite number of kernels I have to store the kernel matrices and I cannot store too large number of them.",
            "And the other issue is that I don't want my user to specify this set of this huge set of kernels.",
            "So I want something more automatic.",
            "I want something that doesn't require domain knowledge.",
            "So what we propose here is a more direct approach that applies under the assumption that the kernel is decomposable, and essentially what we do is simp."
        ],
        [
            "Modifying the regulation problem by adding an additional regularization.",
            "Omega fell on the output kernel.",
            "And then so this Omega Val is going to be typically a convex function, or in some cases maybe also nonconvex.",
            "And we're going to minimize not only over this space HL, but we're also going to minimize over the set of symmetric and positive semidefinite matrices for L. Now this problem is non convex and possible examples of choices of course are all possible norms that you can imagine on L, But for some of them we have actually developed some algorithms that solve the optimization problem.",
            "For others we haven't yet.",
            "So for this one for example, we have some algorithms, the squared for business norm, but something more interesting that one may want to do is to use for example some sparsity inducing regularizer.",
            "So in the case where we know that.",
            "Some some of the tasks are related.",
            "Some of the tasks are not related at all, but we don't know which one are related, which one are not.",
            "We may want to use something like this to discover automatically which does related which tasks are not related or another structure that we may want to impose this low rank so we can use the trace of L which will encourage low rank output kernel.",
            "And we can also sometimes use a hard rank constraint, so this is the indicator function of the set where the rank of value is less or equal than P. Of course, when P is equal to M, this thing it doesn't influence the solution, but when P strictly less than M then we have a hard rank constraint and I just want to talk a little bit about this last regularizer here."
        ],
        [
            "Because for this I actually I've developed some code that works.",
            "In the case of least squared loss.",
            "One can show that after applying the representative theorem, the problem boils down to this finite dimensional optimization problem where why a bold?",
            "Why is the matrix containing the output data, which can be an incomplete matrix in the case of in general, in the case of multi task learning.",
            "So we use a binary mask.",
            "This is the element wise product to make this loss depending only on the entries that we actually observe.",
            "K is the input kernel matrix, the classical.",
            "Matrix containing all pairwise evaluations of the input kernel L is the output current matrix and C is the matrix of coefficients CIJ that we want to optimize.",
            "The in the squared norm in their FHS can be written in this form, and you can see that there is a quadratic function of C and is linear in K&L.",
            "So now we have also trace organization and we minimize over the set of symmetric and.",
            "Most of them are matrices with rank less or equal than P. So this one can show that this problem is actually a nonlinear kernelized generalization of reduced rank regression.",
            "This is a popular technique in technique in statistics, so this is a kernelized version of that, where in addition we have some regularization here.",
            "Now one can reformulate this problem in different ways.",
            "This problem is of course, nonconvex.",
            "And one of the way to reformulate the problem only involves low rank matrices, and this is the reformulation that we use in this moment to solve the optimization problem.",
            "So essentially the problem is reformulated as an unconstrained problem, where we optimize respect to A&BB is essentially a low rank factor of L. And all these two matrices are have a dimension that can be controlled.",
            "It's B.",
            "This is can be controlled by the by the user, and therefore we have a control on the on the memory occupation and currently what I do is simply to do block or dissent for this, where I alternate between optimizing A&B and at each step I have to solve quadratic problem, linear matrix equation effect.",
            "Which I solve iteratively by using approximate preconditioned conjugate gradient.",
            "OK, more details about this formulation and the way we we solve the problem are containing these two papers.",
            "So I'm just going to show you some."
        ],
        [
            "My experiments with this method, let's have a look at the move."
        ],
        [
            "Less datasets these are three datasets for collaborative filtering.",
            "We have different sizes tend to the five to six in 6:50 ratings, different number of users, different number of movies.",
            "We also have some metadata about the movies.",
            "Um and.",
            "Now I want to use this method here and if you look at it you will see that essentially what we need to specify here in addition to the data is just the input kernel.",
            "And the regulation parameter, because all the rest is optimized from the data.",
            "So for the input kernel we design some kernel, which in this case measure the similarity between 2 movies by using some meta data that we have in this data sets about some general categorization of the movies.",
            "So we generate some binary vectors, we compute the humming distance of these.",
            "We take the exponential of minus the Hamming distance, and this is a condition to be a valid positive symmetric kernel and then the problem with this if I only use this.",
            "The problem is that I cannot distinguish two different movies with identical metadata and therefore I add a little bit of Delta kernel based on the ID of the movie, so that now I can distinguish between two different movies with identical general categorization, and this is quite important for performance.",
            "Is the methods that I compare our independent single task learning pool edge single task learning?",
            "Likewise, matrix factorization, which is very popular in this context of collaborative filtering and low rank output, cannot learning.",
            "So the."
        ],
        [
            "Cup is the following for each of the user.",
            "I take 50% of the rating for training and the remaining four test.",
            "I do some validation to choose the regulation parameter by extracting a subset of the training set.",
            "And these are the performances on the small data set 100K ratings.",
            "You can see that the pool, it baseline works pretty well actually.",
            "So for this actually we are estimating one reference profile for all the users.",
            "The same preference profile, exactly the same for all the users, and it's already pretty good.",
            "Actually, it's better than regress matrix factorization, but this difference can be explained by the fact that this method uses the meta data about the movies, whereas this method only uses the rating.",
            "The independent work, solid work worse and you can see that this method outperforms the baselines as well as rigorous metrics.",
            "Factorization and the same can be observed also on the bigger data set, although the difference in performance is becomes less significant.",
            "OK, now let me just tell you how did we obtain this graph.",
            "So essentially we."
        ],
        [
            "Have a multi class classification problem and we formulate this as a multi task learning problem where each of the components of our vector value function is a score for a given given class and then we classify to the argmax.",
            "Again, we have to design the input kernel, and in this case the input kernel has to compare 2 images.",
            "So for this we just used a kernel designed in this paper in SV by Gaylor and Nobles in and output kernel is learned using output kernel learning."
        ],
        [
            "Essentially, what this graph represents is the entries of the matrix L after some thresholding, where we take only the values with the with high absolute value.",
            "OK, to conclude so much."
        ],
        [
            "Machine learning promises are structured and multi task.",
            "It is clear that indeed solving multiple problems simultaneously can improve performance is, especially if you have many tasks and a few data points per task.",
            "These methods output going to learning centers are a good alternative to MCL.",
            "They can solve multitasking problems and on the same time reveal Inter task relationship relationships we have to solve some non convex optimization problems but they have some special structure and it seems like the simple alternate optimization.",
            "It's already already gives good performances at least in terms of learning.",
            "There is some code available online for some of these methods.",
            "Thanks for you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with chain so long and can you forgive me too and it's about multi task learning so we want to address the situation where we want to learn simultaneously multiple tasks and on the same time you would like to estimate the relationships between these tasks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We already had a talk before about multitask learning with a lot of motivations, why learning simultaneously multiple task can be beneficial, but I'm going to give some further examples and the simplest examples that I know too that illustrates very well why multitask learning can be useful is multiple regression.",
                    "label": 0
                },
                {
                    "sent": "So let's consider this very simple application in pharmacokinetics and what you can see here.",
                    "label": 0
                },
                {
                    "sent": "Estimates these curves are all estimates of.",
                    "label": 0
                },
                {
                    "sent": "The concentration profile overtime of some drug in the blood of 27 human subjects, and.",
                    "label": 1
                },
                {
                    "sent": "You can see all these curves start from zero, which is the time where the drug we have this Drug Administration.",
                    "label": 0
                },
                {
                    "sent": "Then you have a region where the concentration is increasing and then you have an elimination phase where the drug is expelled is eliminated from the Organism.",
                    "label": 1
                },
                {
                    "sent": "So all these curves have the same shape.",
                    "label": 0
                },
                {
                    "sent": "But as you can see there is microscopic Inter individual variability.",
                    "label": 0
                },
                {
                    "sent": "So the lowest of these curves has a maximum at around 40, whereas the highest of this curve says the maximum weight around 100.",
                    "label": 0
                },
                {
                    "sent": "So now the problem is you may want to estimate these curves from the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sets that look like this, so for each of the subjects you can imagine, you know that possibly you have very few samples like two or three points per subject.",
                    "label": 0
                },
                {
                    "sent": "So of course if you try to solve the regression problem with two or three data points.",
                    "label": 0
                },
                {
                    "sent": "In dependently this would.",
                    "label": 0
                },
                {
                    "sent": "Not give a good solution.",
                    "label": 0
                },
                {
                    "sent": "But the only way we can solve the problem is to put together the datasets from the different subjects and exploiting the knowledge that the shape is the same.",
                    "label": 0
                },
                {
                    "sent": "We can try to estimate all the curves simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of multi task learning which is very successful is collaborative filtering, recommender systems or more generally user preference estimation.",
                    "label": 0
                },
                {
                    "sent": "So in this type of problem which probably became popular after the Netflix competition, we have a collection.",
                    "label": 0
                },
                {
                    "sent": "The datasets are collection of ratings assigned by several users to a set of items.",
                    "label": 1
                },
                {
                    "sent": "The items maybe movies.",
                    "label": 0
                },
                {
                    "sent": "They may be books or whatever.",
                    "label": 1
                },
                {
                    "sent": "And the goal is of course to estimate the preferences of every user for all the items in the database.",
                    "label": 0
                },
                {
                    "sent": "Now here see this is a popular website for movie recommendations called Movie Lens and you can register on this website.",
                    "label": 0
                },
                {
                    "sent": "You can browse a huge database of movies and you can assign some ratings from one to five stars to each of these movies so that the system can give you then recommendations of the world.",
                    "label": 0
                },
                {
                    "sent": "Now the problem here is that of course you cannot give a rating for all the movies in the database.",
                    "label": 1
                },
                {
                    "sent": "You will be only be able to give you writing for a very small subset.",
                    "label": 1
                },
                {
                    "sent": "There are thousands and thousands of movies, so preference profiles are different from every user.",
                    "label": 0
                },
                {
                    "sent": "However, we again want to exploit the fact that similar users have similar preferences.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting about these problems of collaborative filtering, in my opinion, is that you have also.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only information in addition to the ratings, you may have additional data about the items.",
                    "label": 1
                },
                {
                    "sent": "So for example if we talk about movie, you can take one movie and you go in a website like this.",
                    "label": 0
                },
                {
                    "sent": "the Internet movie database and you can retrieve a lot of information such as the ear when it was released, the name of the director, the name of the actors in other, a lot of information you may have additional data about, the users like gender, age and occupation and so on.",
                    "label": 0
                },
                {
                    "sent": "And you may have even additional metadata about the rating themselves.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know that.",
                    "label": 0
                },
                {
                    "sent": "Timestamp attached when the rating was given or you have tags attached to the ratings, or you may also have a small pieces of text associated with the rating.",
                    "label": 0
                },
                {
                    "sent": "So the question is can we combine all these data from different users to better estimate individual preferences?",
                    "label": 1
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In common between these two problems, is the data set structure and the point is that we have many tasks, so many subjects or many users, and sampling can be very sparse in the sense that for every task we have very few samples, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The case of pharmacokinetics.",
                    "label": 0
                },
                {
                    "sent": "We have very few measurements in time for each subject, and in the case of recommender systems, we are able to measure a rating only for a very tiny subset of the whole set of movies, however.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each sample is shared by many tasks and this is what allows us to somehow do something.",
                    "label": 0
                },
                {
                    "sent": "Another problem that this seemingly unrelated with this is odd.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Recognition with, let's say multi classification classification with structured discovery.",
                    "label": 0
                },
                {
                    "sent": "So this is a multi class classification problem.",
                    "label": 0
                },
                {
                    "sent": "Very popular in computer and computer vision.",
                    "label": 0
                },
                {
                    "sent": "It's a contact 156.",
                    "label": 0
                },
                {
                    "sent": "We have a data set of images of different categories and the goal is the classical goal is to build a classifier multiclass classifier with good generalization.",
                    "label": 0
                },
                {
                    "sent": "Performance is so given a new image we want to predict the label.",
                    "label": 0
                },
                {
                    "sent": "But another thing that you may want to do is for example to discover relationships between the different classes.",
                    "label": 1
                },
                {
                    "sent": "And about relationship I mean something.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, so you may want to automatically solve the classification problem on the same time produce graph, for example, that expresses the relationship between the classes.",
                    "label": 0
                },
                {
                    "sent": "So for example, that tells us that images of motor bikes are very similar to images of mountain bikes, images of chopsticks, baseball bat, sword have something in common, and so on.",
                    "label": 0
                },
                {
                    "sent": "And you want to obtain this without you know, hard coding this information in the in the in the in the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to introduce.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use some methods to solve multiple multi task learning problems and on the same time try to estimate relationship between the tasks in this in.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context of this talk.",
                    "label": 0
                },
                {
                    "sent": "When I talk about multitask learning, I essentially talk about this multitask, supervised learning, where the goal is to synthesize multiple functions.",
                    "label": 0
                },
                {
                    "sent": "FJ mapping some input set X can be any set set of images set of text documents, and so on into some output set, which can be a set of scholars simply or something else.",
                    "label": 0
                },
                {
                    "sent": "From multiple data sets of input output pairs.",
                    "label": 0
                },
                {
                    "sent": "And here for simplicity, I assume that the inputs it is common for all the for all the tasks.",
                    "label": 0
                },
                {
                    "sent": "But of course one can think about more general situations where the input sets are different.",
                    "label": 0
                },
                {
                    "sent": "So if I want to use a kernel methods to solve this problem, whatever method they use at the end of the day, I have to specify a kernel multitask kernel of this form, so I have to specify a similarity measure that for every pair of inputs X1 and X2, and for every pair of task in this is inj.",
                    "label": 1
                },
                {
                    "sent": "Specifies the joint similarity between inputs and tasks.",
                    "label": 0
                },
                {
                    "sent": "Now equivalently, I can express the same information by using a matrix valued function H. Of course, that looks formally more similar to the usual kernel because it takes as an argument only the two inputs X one X2, but in Maps into matrices M by M&IJ entries is exactly this similarity here.",
                    "label": 0
                },
                {
                    "sent": "So there are two equivalent ways to represent this similarity, But the problem is, given specific multitasking problem, how do we choose the kernel?",
                    "label": 0
                },
                {
                    "sent": "How do we specify?",
                    "label": 0
                },
                {
                    "sent": "Some meaningful joint similarity measure is not is not trivial becausw we have a lot of flexibility here, So what many people do.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has been done a lot in the literature is to make this assumption here that simply says OK, I'm going to assume that this similarity measure can be decomposed as the product of a kernel on the inputs only, and a kernel on the task indices that expresses the similarities within the task.",
                    "label": 0
                },
                {
                    "sent": "I in the task J.",
                    "label": 0
                },
                {
                    "sent": "Or equivalently, if you think in terms of metrics valued kernel, you are going to have a scalar kernel K which multiplies a constant matrix L. And of course, this contest custom matrix has to be symmetric and positive semidefinite for all this to be a valid kernel.",
                    "label": 0
                },
                {
                    "sent": "So we're going to call KX the input kernel.",
                    "label": 1
                },
                {
                    "sent": "And key Y the output kernel.",
                    "label": 1
                },
                {
                    "sent": "Or equivalently, this matrix L the output kernel.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Want to solve the multitasking problem by using a regularization?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach.",
                    "label": 0
                },
                {
                    "sent": "So we build a reproducing kernel Hilbert space of functions of vector valued functions associated with this decomposable kernel here.",
                    "label": 0
                },
                {
                    "sent": "And we optimize over this space by minimizing the sum of some loss term over the tasks and some regularization which is simply in this case the squared norm in the kitchen.",
                    "label": 0
                },
                {
                    "sent": "So the usual framework of regularization.",
                    "label": 0
                },
                {
                    "sent": "And we know by the representative theorem that the solution looks like this.",
                    "label": 0
                },
                {
                    "sent": "So the J component of the optimal function FFJ.",
                    "label": 0
                },
                {
                    "sent": "Can be written in this form where we have kernels, expansions on the individual datasets.",
                    "label": 0
                },
                {
                    "sent": "And then we have a linear combination of these kernel expansions that involve this LJK.",
                    "label": 0
                },
                {
                    "sent": "So the entries of this matrix L. Now, not only the solution depends on L via this combination here, but also the coefficient CIJ are going to depend implicitly on L. Because of the optimization problem here, so the norm here depends on L and also the space itself.",
                    "label": 0
                },
                {
                    "sent": "So the question is, in this context, how do we choose the output kernel?",
                    "label": 1
                },
                {
                    "sent": "Of course there is a knave baseline, which is just choose L to the identity and this would correspond to independent single task learning.",
                    "label": 0
                },
                {
                    "sent": "So if we choose L to the identity, essentially what we're doing is we're learning all the components of this vector valued functions in dependently.",
                    "label": 0
                },
                {
                    "sent": "The other extreme is pulling single task learning, so I choose L equals to the matrix of all ones or a constant matrix.",
                    "label": 0
                },
                {
                    "sent": "In this case, what we are doing is essentially learn a vector valued functions with components which are all equal.",
                    "label": 0
                },
                {
                    "sent": "So essentially we are learning only one function for all the tasks.",
                    "label": 0
                },
                {
                    "sent": "So we can do better than these baselines.",
                    "label": 0
                },
                {
                    "sent": "Of course, in many situations, sometimes actually these are optimal already, but in many cases they are not.",
                    "label": 0
                },
                {
                    "sent": "So if we have some prior knowledge about the relationship between the task, we should use it to design this L in advance, but in practice I do see many problems where we have quantitative information about the similarity between task in advance.",
                    "label": 1
                },
                {
                    "sent": "So the most meaningful way to proceed seems to be to learn this L from the data simultaneously with our function in the opposing cannabis space.",
                    "label": 0
                },
                {
                    "sent": "So one possibility that immediately comes to mind is to use multiple kernel.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning so one can design some Dictionary of basis kernels KK.",
                    "label": 0
                },
                {
                    "sent": "Take a chronic combination of these.",
                    "label": 0
                },
                {
                    "sent": "And then learn the weights of this combination simultaneously with the function in the architectures this is 1 possible approach is very general and of course I can use this approach also for multi task learning because I think a combination of multitasking kernels.",
                    "label": 0
                },
                {
                    "sent": "In particular, I can take combination of decomposable multitask kernels and I can do the use the same machinery of multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So even more specific I can choose this KXK.",
                    "label": 0
                },
                {
                    "sent": "To be in the same kernel and what I'm doing effectively is to learn a combination of these matrices kyk.",
                    "label": 0
                },
                {
                    "sent": "By solving an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "But there are two issues with this approach.",
                    "label": 0
                },
                {
                    "sent": "OK, the first issue is that the maximum number of kernels that I can use is in fact limited by memory constraints.",
                    "label": 0
                },
                {
                    "sent": "So although in principle I can use an arbitrarily high number of kernel, even an infinite number of kernels I have to store the kernel matrices and I cannot store too large number of them.",
                    "label": 0
                },
                {
                    "sent": "And the other issue is that I don't want my user to specify this set of this huge set of kernels.",
                    "label": 0
                },
                {
                    "sent": "So I want something more automatic.",
                    "label": 0
                },
                {
                    "sent": "I want something that doesn't require domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "So what we propose here is a more direct approach that applies under the assumption that the kernel is decomposable, and essentially what we do is simp.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modifying the regulation problem by adding an additional regularization.",
                    "label": 0
                },
                {
                    "sent": "Omega fell on the output kernel.",
                    "label": 1
                },
                {
                    "sent": "And then so this Omega Val is going to be typically a convex function, or in some cases maybe also nonconvex.",
                    "label": 0
                },
                {
                    "sent": "And we're going to minimize not only over this space HL, but we're also going to minimize over the set of symmetric and positive semidefinite matrices for L. Now this problem is non convex and possible examples of choices of course are all possible norms that you can imagine on L, But for some of them we have actually developed some algorithms that solve the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "For others we haven't yet.",
                    "label": 1
                },
                {
                    "sent": "So for this one for example, we have some algorithms, the squared for business norm, but something more interesting that one may want to do is to use for example some sparsity inducing regularizer.",
                    "label": 0
                },
                {
                    "sent": "So in the case where we know that.",
                    "label": 0
                },
                {
                    "sent": "Some some of the tasks are related.",
                    "label": 0
                },
                {
                    "sent": "Some of the tasks are not related at all, but we don't know which one are related, which one are not.",
                    "label": 0
                },
                {
                    "sent": "We may want to use something like this to discover automatically which does related which tasks are not related or another structure that we may want to impose this low rank so we can use the trace of L which will encourage low rank output kernel.",
                    "label": 0
                },
                {
                    "sent": "And we can also sometimes use a hard rank constraint, so this is the indicator function of the set where the rank of value is less or equal than P. Of course, when P is equal to M, this thing it doesn't influence the solution, but when P strictly less than M then we have a hard rank constraint and I just want to talk a little bit about this last regularizer here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because for this I actually I've developed some code that works.",
                    "label": 0
                },
                {
                    "sent": "In the case of least squared loss.",
                    "label": 0
                },
                {
                    "sent": "One can show that after applying the representative theorem, the problem boils down to this finite dimensional optimization problem where why a bold?",
                    "label": 0
                },
                {
                    "sent": "Why is the matrix containing the output data, which can be an incomplete matrix in the case of in general, in the case of multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So we use a binary mask.",
                    "label": 0
                },
                {
                    "sent": "This is the element wise product to make this loss depending only on the entries that we actually observe.",
                    "label": 0
                },
                {
                    "sent": "K is the input kernel matrix, the classical.",
                    "label": 0
                },
                {
                    "sent": "Matrix containing all pairwise evaluations of the input kernel L is the output current matrix and C is the matrix of coefficients CIJ that we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "The in the squared norm in their FHS can be written in this form, and you can see that there is a quadratic function of C and is linear in K&L.",
                    "label": 0
                },
                {
                    "sent": "So now we have also trace organization and we minimize over the set of symmetric and.",
                    "label": 0
                },
                {
                    "sent": "Most of them are matrices with rank less or equal than P. So this one can show that this problem is actually a nonlinear kernelized generalization of reduced rank regression.",
                    "label": 1
                },
                {
                    "sent": "This is a popular technique in technique in statistics, so this is a kernelized version of that, where in addition we have some regularization here.",
                    "label": 0
                },
                {
                    "sent": "Now one can reformulate this problem in different ways.",
                    "label": 0
                },
                {
                    "sent": "This problem is of course, nonconvex.",
                    "label": 1
                },
                {
                    "sent": "And one of the way to reformulate the problem only involves low rank matrices, and this is the reformulation that we use in this moment to solve the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So essentially the problem is reformulated as an unconstrained problem, where we optimize respect to A&BB is essentially a low rank factor of L. And all these two matrices are have a dimension that can be controlled.",
                    "label": 0
                },
                {
                    "sent": "It's B.",
                    "label": 0
                },
                {
                    "sent": "This is can be controlled by the by the user, and therefore we have a control on the on the memory occupation and currently what I do is simply to do block or dissent for this, where I alternate between optimizing A&B and at each step I have to solve quadratic problem, linear matrix equation effect.",
                    "label": 0
                },
                {
                    "sent": "Which I solve iteratively by using approximate preconditioned conjugate gradient.",
                    "label": 1
                },
                {
                    "sent": "OK, more details about this formulation and the way we we solve the problem are containing these two papers.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to show you some.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My experiments with this method, let's have a look at the move.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Less datasets these are three datasets for collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "We have different sizes tend to the five to six in 6:50 ratings, different number of users, different number of movies.",
                    "label": 1
                },
                {
                    "sent": "We also have some metadata about the movies.",
                    "label": 0
                },
                {
                    "sent": "Um and.",
                    "label": 0
                },
                {
                    "sent": "Now I want to use this method here and if you look at it you will see that essentially what we need to specify here in addition to the data is just the input kernel.",
                    "label": 1
                },
                {
                    "sent": "And the regulation parameter, because all the rest is optimized from the data.",
                    "label": 1
                },
                {
                    "sent": "So for the input kernel we design some kernel, which in this case measure the similarity between 2 movies by using some meta data that we have in this data sets about some general categorization of the movies.",
                    "label": 0
                },
                {
                    "sent": "So we generate some binary vectors, we compute the humming distance of these.",
                    "label": 0
                },
                {
                    "sent": "We take the exponential of minus the Hamming distance, and this is a condition to be a valid positive symmetric kernel and then the problem with this if I only use this.",
                    "label": 0
                },
                {
                    "sent": "The problem is that I cannot distinguish two different movies with identical metadata and therefore I add a little bit of Delta kernel based on the ID of the movie, so that now I can distinguish between two different movies with identical general categorization, and this is quite important for performance.",
                    "label": 0
                },
                {
                    "sent": "Is the methods that I compare our independent single task learning pool edge single task learning?",
                    "label": 0
                },
                {
                    "sent": "Likewise, matrix factorization, which is very popular in this context of collaborative filtering and low rank output, cannot learning.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cup is the following for each of the user.",
                    "label": 1
                },
                {
                    "sent": "I take 50% of the rating for training and the remaining four test.",
                    "label": 1
                },
                {
                    "sent": "I do some validation to choose the regulation parameter by extracting a subset of the training set.",
                    "label": 0
                },
                {
                    "sent": "And these are the performances on the small data set 100K ratings.",
                    "label": 0
                },
                {
                    "sent": "You can see that the pool, it baseline works pretty well actually.",
                    "label": 0
                },
                {
                    "sent": "So for this actually we are estimating one reference profile for all the users.",
                    "label": 0
                },
                {
                    "sent": "The same preference profile, exactly the same for all the users, and it's already pretty good.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's better than regress matrix factorization, but this difference can be explained by the fact that this method uses the meta data about the movies, whereas this method only uses the rating.",
                    "label": 0
                },
                {
                    "sent": "The independent work, solid work worse and you can see that this method outperforms the baselines as well as rigorous metrics.",
                    "label": 0
                },
                {
                    "sent": "Factorization and the same can be observed also on the bigger data set, although the difference in performance is becomes less significant.",
                    "label": 0
                },
                {
                    "sent": "OK, now let me just tell you how did we obtain this graph.",
                    "label": 0
                },
                {
                    "sent": "So essentially we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a multi class classification problem and we formulate this as a multi task learning problem where each of the components of our vector value function is a score for a given given class and then we classify to the argmax.",
                    "label": 0
                },
                {
                    "sent": "Again, we have to design the input kernel, and in this case the input kernel has to compare 2 images.",
                    "label": 0
                },
                {
                    "sent": "So for this we just used a kernel designed in this paper in SV by Gaylor and Nobles in and output kernel is learned using output kernel learning.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, what this graph represents is the entries of the matrix L after some thresholding, where we take only the values with the with high absolute value.",
                    "label": 0
                },
                {
                    "sent": "OK, to conclude so much.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning promises are structured and multi task.",
                    "label": 1
                },
                {
                    "sent": "It is clear that indeed solving multiple problems simultaneously can improve performance is, especially if you have many tasks and a few data points per task.",
                    "label": 1
                },
                {
                    "sent": "These methods output going to learning centers are a good alternative to MCL.",
                    "label": 0
                },
                {
                    "sent": "They can solve multitasking problems and on the same time reveal Inter task relationship relationships we have to solve some non convex optimization problems but they have some special structure and it seems like the simple alternate optimization.",
                    "label": 0
                },
                {
                    "sent": "It's already already gives good performances at least in terms of learning.",
                    "label": 0
                },
                {
                    "sent": "There is some code available online for some of these methods.",
                    "label": 0
                },
                {
                    "sent": "Thanks for you.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}