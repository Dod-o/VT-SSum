{
    "id": "ut6njbg6aogio25d2jmfskcjdjpfdzst",
    "title": "Online Reinforcement Learning and Sequential Forecasting and Partial Feedback",
    "info": {
        "author": [
            "Peter Auer, Chair for Information Technology, Montanuniversit\u00e4t Leoben"
        ],
        "published": "Feb. 7, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/psm08_auer_orl/",
    "segmentation": [
        [
            "I am I'm presenting to pump priming projects which I was involved in.",
            "This is online performance of reinforcement learning and sequential forecasting and partial feedback.",
            "So."
        ],
        [
            "So the first one."
        ],
        [
            "Had several parts."
        ],
        [
            "There is."
        ],
        [
            "On"
        ],
        [
            "Benadryl, cough and myself."
        ],
        [
            "And the goal was to get an online analysis of reinforcement learning.",
            "And I will give you some details during the end of my talk.",
            "It was also about carrying this analysis to continuous state spaces and maybe design internal reward functions.",
            "And I will also say a little bit about it.",
            "At the end of my talk."
        ],
        [
            "The other project was on sequential forecast and partial feedback applications to machine learning had quite a number of partners.",
            "So I'm giving this part of the talk for hundreds gergi who cannot couldn't make it.",
            "And I'm listing here just some of the goals for this project.",
            "One was to use machine learning techniques for parameter tuning in the sense that for complicated optimization problems large optimization problems, we want to more quickly find the right parameters for the optimum.",
            "Another one was inverse reinforcement learning for apprenticeship learning, and I give you some details on that.",
            "And another one more general one was how we can deal with changes when we do sequential forecasting.",
            "For example, if we want to present interesting content to use and the interest of this user is changing overtime."
        ],
        [
            "OK, this is a partial list of activities, so we hired some researchers.",
            "We had some workshops, we had a challenge.",
            "I want to point you to the last two activities which have an impact beyond the project.",
            "One was a reinforcement learning workshop in Tubingen which reignited series of workshops which were held on reinforcement learning in Europe and the next one will be little this summer.",
            "And another project which will draw expertise from these projects I'm presenting here is strep in fuel, which was already mentioned in the talk before the lunch break.",
            "OK.",
            "There is a."
        ],
        [
            "List of.",
            "Papers Workshop and Journal papers which came out of the project is also partial list, just to show you that there's published work coming from this project and.",
            "For the rest of the talk I'm giving you."
        ],
        [
            "Some more details about the things we actually did.",
            "The first thing is work which used UCT.",
            "Which is short for upper confidence for trees.",
            "Which is a machine learning algorithm for parameter optimization.",
            "The UCT algorithm is can be viewed as a method to explore trees, and it is based on the UCB.",
            "So upper confidence bounds algorithm for the bandit problem and essentially it is about when you want to explore the tree.",
            "The questions which branch of a tree should you follow and use it be can be used to give you the answer.",
            "Which branch of the tree you want to explore.",
            "Use it is also very successfully used in mogul which is world champion in computer go.",
            "One very important person this is Sylvain Gelly.",
            "He who did this in his PhD theses.",
            "And to apply this for parameter optimization, we build a tree which divides the search interval for parameter and we think it's easiest to show."
        ],
        [
            "You want an example.",
            "So soon we have this interval for which in which we are searching for the right parameter and."
        ],
        [
            "Start with one note representing the whole interval of between zero and one and the first thing we do is we just pick maybe a random value from this interval.",
            "So point .4 is the random.",
            "Well we pick and point .9 is the response we get.",
            "So after getting this response, we divide the interval maybe?"
        ],
        [
            "But taking it into two halves, and we forward the query we already have got or the result of the query we already got to the responding leave.",
            "So B represents the lower half of the interval.",
            "So .4 is in this interval and the response was .9 and the other leaf is unexplored.",
            "Yet also on the branch we put some information about what we got in this subtree here.",
            "So we got response .9 in one trial.",
            "And we got zero response in Syria trials.",
            "So the next leave we will explore is still."
        ],
        [
            "See so we pick .7 and get respond.",
            "I response points one and put this information up here and now.",
            "The question is which of the branches should we explore more?",
            "And now it seems quite obvious that this branch is more promising and UCP can be used to give a more founded answer to this question.",
            "Which branch to explore more?",
            "So the next thing we will do is we split up."
        ],
        [
            "This interval into two parts and forward .4 to the respective leaf and query the other leaf which is and we query at .2 and get a response .7 and forward the information we got here up to the branch here.",
            "So now the total response we got is point 1.6 and we after two trials where we go on for this and can be shown that."
        ],
        [
            "And the some assumptions on the optimization problem this is.",
            "Advantageous procedure to do the optimization.",
            "It was tried in."
        ],
        [
            "In application it was churn prediction for telecommunication company using in our prop as an optimization as using the approach algorithm which has seven parameters and UCT converged five times faster than an alternative method for optimization.",
            "We"
        ],
        [
            "Which is significant if you know that one run of our probe takes 12 hours and it takes quite a long time to do the whole optimization.",
            "So a savings of effective 5 is quite significant in real time.",
            "So the other.",
            "Work I want to."
        ],
        [
            "Report on.",
            "Is apprenticeship learning using inverse reinforcement learning?",
            "Um, in inverse reinforcement learning, the question is how can we imitate the pause?",
            "Probably optimal behavior of an expert and one.",
            "Night."
        ],
        [
            "Eve approach or immediate approach is to just imitate the behavior in the states we have actually observed from the expert.",
            "The drawback."
        ],
        [
            "Of this approach is that it does not generalize wealth.",
            "Two states which we have not observed for which in which the expert has not been, so that we don't have no the actions an expert would take in this state.",
            "Send alternative approaches."
        ],
        [
            "To line and reward our value function which explains the behavior of the expert.",
            "So if you have a value function, we can apply it for the whole state space and can derive actions for the whole state space."
        ],
        [
            "To do this more concretely, we assume that we can parameterized the rewards.",
            "So in this example a very simple parameterisation.",
            "The rewards are just a linear combination of some.",
            "Functions file here.",
            "And the problem is to find a good setting of these parameters later, which explain the experts behavior so.",
            "Or to phrase it in another way, if we pick the right parameters and we calculate an optimal policy according to this parameters, then the policy which we get should be similar or as simple as similar as possible to the policy of the expert.",
            "So this is.",
            "Captain this notion here, so we sum over all the States and want that's the action chosen by his optimal policy given a certain data is close to the action chosen by the expert, and we have a waiting here for over the stationary distribution of the states by them for the experts policy doing this we can use natural gradient techniques to actually calculate good values for these parameters data."
        ],
        [
            "This is an example how this works, so in the top picture you see the trajectory of an expert and on the left lower left picture you see an marked red regions where the.",
            "Policy which does.",
            "Direct policy mutation, which directly mimics the.",
            "Policy of the expert.",
            "So so the weather.",
            "The regions where this weather policy learned by direct policy imitation is close to the experts policy.",
            "What the experts would actually do and what you see is that in.",
            "In states we have observed this correspondence is good, but in other in other states this correspondence is quite bad.",
            "In contrast, on the right hand side you see what we get by inverse reinforcement learning.",
            "When we actually get in.",
            "Reward function which explains the expert expert's behavior, and this general is generalizes well over the whole state space.",
            "But this is quite nice result here.",
            "OK.",
            "So.",
            "Last thing I want to talk when it takes take me with more time that will be a bit give you a bit more details on that."
        ],
        [
            "Is about online reinforcement learning, so we.",
            "Look at the generic reinforcement learning problem, so we have.",
            "Now on a set of states, instead of actions we have rewards for taking a certain action is in a state S, so are other rewards, and we have a transition probabilities.",
            "So if as an agent takes a certain action in the state as, then it is transferred to a new state as prime given by governed by some probability distribution.",
            "To the learner this transition transition probabilities are unknown and needs essentially to be learned.",
            "Um, what we want is an optimal policy, and policy is just a mapping from states to actions.",
            "I'm a little bit sloppy here because sometimes.",
            "444 learning algorithm.",
            "This is not a fixed mapping, but will also depend on the past on the observations the agent has made so far.",
            "But what we want to achieve is a policy which optimizes the rewards in the sense that the sum of all rewards during a certain number of trials is maximal where STR the states visited by the agent.",
            "Given his policy and 80 other actions chosen by this but his agent.",
            "I'm.",
            "OK."
        ],
        [
            "And what we are interested in in an online analysis is the difference or is what we lose against an optimal policy?",
            "When we need to learn the MDP so the optimal policy can be chosen when we know the transition probabilities of the MDP, but the line it doesn't know this transition probabilities needs to learn them and the question is how much worse is the behavior of the learning agent against what an optimal policy could achieve.",
            "So Delta is the regret."
        ],
        [
            "And that's what we want to get a good bound on.",
            "And an algorithm which achieves smaller regret.",
            "Well, if we go back, you see that we I don't have a discount factor here, so I'm talking about undiscounted reinforcement learning.",
            "This has a simple reason if we use a discount factor then any.",
            "Some over the discounted some of the rewards or the differences between two sums of discounted rewards is just a constant for any discount factor smaller than one.",
            "It doesn't give us any interesting regret bounds, so that's why we're talking here.",
            "I'm talking here only about undiscounted reinforcement learning."
        ],
        [
            "So yeah, this is the result, so we have an algorithm which we call you SRL.",
            "Again, upper confidence bounds reinforcement learning an and if you have Estates and a actions then the bound is on their regret.",
            "This D * S Times Square root 80.",
            "Which is just the dependency on the square root of the number of trials and the number of actions and the linear dependency on the number of States and Steve, what I.",
            "But I called the diameter of the MDP.",
            "This is actually I think that's the main contribution of one of the.",
            "Main contributions here that we have this diameter in the bounds.",
            "The diameter is.",
            "Is such that for any pair of states as one as two?",
            "There's a strategy which moves you from S1 to SS2 within these steps.",
            "So for each pair, we can choose a strategy such that it moves from S1 to S2 within these steps."
        ],
        [
            "This seems a very natural quantity for an MDP, because when you want to explore the MDP, you need to be able to move from one state to the other, and if it takes very long then you're learning behavior will be worse than if it takes just a small number of steps.",
            "So this seems a very natural quantity for Markov decision processes.",
            "And we also have a lower bound which shows that you not very far off, only the experts.",
            "Only the exponents of on D&S are in our upper bound little words.",
            "I do believe that the lower bound is the correct one.",
            "We might be able to tighten the analysis for the upper bound, but that's just a guess.",
            "OK, so."
        ],
        [
            "What is the relation to other work?",
            "So there has been quite some work on the park like bonds for reinforcement learning.",
            "So the first one, not the first one.",
            "The first one is maybe the second one, then the well known one is the cubed algorithm by current sensing which show that if the polynomial number of steps the you'll converge to a policy which is at most epsilon worse than the optimal policy.",
            "Anne and.",
            "The analysis of card on our marks GIFs.",
            "More precise and the better bound on this, so he bounced a number of actions which are not epsilon optimal.",
            "Antique gives Gap gets this bound on this number of steps where you're not optimal, so are bound can be quite easily converted into such a bound on the number of non optimal."
        ],
        [
            "Steps and looks actually quite similar to Chotis bound.",
            "They only did well.",
            "We're a little better independency on the dependence on epsilon and but the main difference is that.",
            "They also can sing and Carter have this.",
            "Mixing time epsilon mixing time in the bound where we have the diameter."
        ],
        [
            "So the epsilon mixing time is the number of steps such that any policy converge basically to its or get close to its.",
            "Stationary distribution more or less, it's a little more involved, so after that many steps you get the expected reward is epsilon close.",
            "To the, uh, the actual reward is epsilon close to the expected reward, but this is quite similar to getting close to the stationary distribution."
        ],
        [
            "So now the question is, what's the relation between the diameter and steps of mixing time and for small epsilon?",
            "The relationship is that the smaller the epsilon, the larger the mixing time, so you get an additional factors on epsilon in the bound and.",
            "This is just an lower bound on the mixing time.",
            "The mixing time can actually be arbitrarily bad.",
            "Because it might, you might have an observing state, and it's very hard to get there with your current policy.",
            "Then the mixing time will be very large.",
            "And there's other work on reinforcement learning where right now we had.",
            "Bounce, which essentially translates to bounds like.",
            "Which square root T there is all the way."
        ],
        [
            "Rick, where you get locked bounds on the regret for this kind of bounds, you need to assume that there's a gap between the best policy and the second best policy.",
            "If you assume this gap, which I called JM."
        ],
        [
            "Then Jay, so maybe not Jay.",
            "You get this time.",
            "This type of bounds from our algorithm with a little bit more complicated analysis and."
        ],
        [
            "His previous work by Bernadette this, Kathy Hawkins and the very bad let recently which gets.",
            "Who get very similar bounds, even the better dependency on the number of states.",
            "But what I consider the main difference is that they have a different quantity which measures the difficulty of the MDP here.",
            "And if."
        ],
        [
            "You look at the definition of the Max, it's.",
            "The quantity such that for any policy.",
            "You will move from S1 to S2 with Indy Mac steps.",
            "So we have them in here.",
            "So for each pair we can choose a different policy and they have the Max here, which says for any policy you move from S1 to SS2 within this dymax steps.",
            "This is a big difference because considering consider very simple MDP like amazing world, a great world where you move up, down, right, left or right then obviously their policies which never will get back to the lower left corner.",
            "So that the Max will be infinite.",
            "Whereas in our different or where is our diameter is just depends just on the size of the of the maze of the grids and gives you good bounds still.",
            "So I really believe that this step from the D Max to the diameter is essential contribution of one of the essential contributions here."
        ],
        [
            "So this is the algorithm.",
            "The algorithm runs, runs in rounds and each round starts at some point, TK, sometime TK and we start a new round.",
            "Whenever for one state action pair, the number of visits to this state action pair doubles, so this NSATK is the number of state action pairs.",
            "When this round is started.",
            "And and you round this started when for some of these state action pairs, the count for this state action pair has doubled.",
            "So it's quite simple, it's just it's needed to guarantee certain properties of the algorithm which you need in the analysis, which I'm not going into.",
            "Within each round we fix the policy, so which I call Pi~ K here and this policy is chosen such that it maximizes the expected reward.",
            "For the best Plausable MVP, best means giving the maximal reward and plausible means that the oops that's the the transition probabilities for the plausible MDP.",
            "I'm not too far from the empirical estimates, so this is just.",
            "Essentially this is a confidence region which holds with high probability and has this.",
            "Yeah, this is the width of the confidence interval."
        ],
        [
            "Um?",
            "So it's a very simple algorithm.",
            "The only question obviously is can we calculate such a policy easily or relatively easily, and this is possible.",
            "We can do this by value iteration.",
            "If you recall value iteration Bellman."
        ],
        [
            "Updates for discounted reinforcement learning.",
            "They look like this.",
            "It's a little bit more complicated for undiscounted reinforcement learning, because this discounting guarantees that this update converges."
        ],
        [
            "If we look at the similar update, which we call bias update for undiscounted reinforcement learning, we don't have the discounting here and which means that this update would converge to Infinity for all the lambdas here.",
            "So we need to risk."
        ],
        [
            "Till normalize this lambdas just by pulling the smallest one to series, which is done by this update here.",
            "So we take the minimum of the lambdas and subtracted, which guarantees that the lambdas remain finite and it can be shown that."
        ],
        [
            "Using this update, the update converges to lambdas with which are between zero and the diameter, which is then important for the analysis."
        ],
        [
            "Fortunately, don't have time to show you."
        ],
        [
            "Analysis.",
            "Um?"
        ],
        [
            "So yeah, you need to use this this Lambda."
        ],
        [
            "US and relate this.",
            "This lambdas despises two day."
        ],
        [
            "Regret then you plug it in and do some calculation and then you get the final result here."
        ],
        [
            "OK, so I want to finish talking about future directions."
        ],
        [
            "And one interesting question question is can we?",
            "Have an algorithm which adapts to changes in the MDP so if we change for example transition probabilities, can we have a learning algorithm which picks this changes up with the current algorithm?",
            "This is not possible because it relies on statistical estimates and doesn't look back if they should be."
        ],
        [
            "Changed now the question is continuous direction spaces.",
            "We haven't got as far as we would have liked to.",
            "And."
        ],
        [
            "Another question which is maybe my hobby autonomous rewards.",
            "So the question is, can we design reward functions which which do not rely on an external signal but still drive?",
            "Both the consolidation of the land knowledge and extensions, or exploration to unknown.",
            "States situations OK thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am I'm presenting to pump priming projects which I was involved in.",
                    "label": 0
                },
                {
                    "sent": "This is online performance of reinforcement learning and sequential forecasting and partial feedback.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first one.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Had several parts.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Benadryl, cough and myself.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the goal was to get an online analysis of reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "And I will give you some details during the end of my talk.",
                    "label": 1
                },
                {
                    "sent": "It was also about carrying this analysis to continuous state spaces and maybe design internal reward functions.",
                    "label": 0
                },
                {
                    "sent": "And I will also say a little bit about it.",
                    "label": 0
                },
                {
                    "sent": "At the end of my talk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other project was on sequential forecast and partial feedback applications to machine learning had quite a number of partners.",
                    "label": 1
                },
                {
                    "sent": "So I'm giving this part of the talk for hundreds gergi who cannot couldn't make it.",
                    "label": 0
                },
                {
                    "sent": "And I'm listing here just some of the goals for this project.",
                    "label": 1
                },
                {
                    "sent": "One was to use machine learning techniques for parameter tuning in the sense that for complicated optimization problems large optimization problems, we want to more quickly find the right parameters for the optimum.",
                    "label": 1
                },
                {
                    "sent": "Another one was inverse reinforcement learning for apprenticeship learning, and I give you some details on that.",
                    "label": 0
                },
                {
                    "sent": "And another one more general one was how we can deal with changes when we do sequential forecasting.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to present interesting content to use and the interest of this user is changing overtime.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is a partial list of activities, so we hired some researchers.",
                    "label": 1
                },
                {
                    "sent": "We had some workshops, we had a challenge.",
                    "label": 0
                },
                {
                    "sent": "I want to point you to the last two activities which have an impact beyond the project.",
                    "label": 0
                },
                {
                    "sent": "One was a reinforcement learning workshop in Tubingen which reignited series of workshops which were held on reinforcement learning in Europe and the next one will be little this summer.",
                    "label": 1
                },
                {
                    "sent": "And another project which will draw expertise from these projects I'm presenting here is strep in fuel, which was already mentioned in the talk before the lunch break.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "List of.",
                    "label": 0
                },
                {
                    "sent": "Papers Workshop and Journal papers which came out of the project is also partial list, just to show you that there's published work coming from this project and.",
                    "label": 0
                },
                {
                    "sent": "For the rest of the talk I'm giving you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more details about the things we actually did.",
                    "label": 0
                },
                {
                    "sent": "The first thing is work which used UCT.",
                    "label": 0
                },
                {
                    "sent": "Which is short for upper confidence for trees.",
                    "label": 1
                },
                {
                    "sent": "Which is a machine learning algorithm for parameter optimization.",
                    "label": 1
                },
                {
                    "sent": "The UCT algorithm is can be viewed as a method to explore trees, and it is based on the UCB.",
                    "label": 1
                },
                {
                    "sent": "So upper confidence bounds algorithm for the bandit problem and essentially it is about when you want to explore the tree.",
                    "label": 0
                },
                {
                    "sent": "The questions which branch of a tree should you follow and use it be can be used to give you the answer.",
                    "label": 1
                },
                {
                    "sent": "Which branch of the tree you want to explore.",
                    "label": 0
                },
                {
                    "sent": "Use it is also very successfully used in mogul which is world champion in computer go.",
                    "label": 0
                },
                {
                    "sent": "One very important person this is Sylvain Gelly.",
                    "label": 0
                },
                {
                    "sent": "He who did this in his PhD theses.",
                    "label": 0
                },
                {
                    "sent": "And to apply this for parameter optimization, we build a tree which divides the search interval for parameter and we think it's easiest to show.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want an example.",
                    "label": 0
                },
                {
                    "sent": "So soon we have this interval for which in which we are searching for the right parameter and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with one note representing the whole interval of between zero and one and the first thing we do is we just pick maybe a random value from this interval.",
                    "label": 0
                },
                {
                    "sent": "So point .4 is the random.",
                    "label": 0
                },
                {
                    "sent": "Well we pick and point .9 is the response we get.",
                    "label": 0
                },
                {
                    "sent": "So after getting this response, we divide the interval maybe?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But taking it into two halves, and we forward the query we already have got or the result of the query we already got to the responding leave.",
                    "label": 0
                },
                {
                    "sent": "So B represents the lower half of the interval.",
                    "label": 0
                },
                {
                    "sent": "So .4 is in this interval and the response was .9 and the other leaf is unexplored.",
                    "label": 0
                },
                {
                    "sent": "Yet also on the branch we put some information about what we got in this subtree here.",
                    "label": 0
                },
                {
                    "sent": "So we got response .9 in one trial.",
                    "label": 0
                },
                {
                    "sent": "And we got zero response in Syria trials.",
                    "label": 0
                },
                {
                    "sent": "So the next leave we will explore is still.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See so we pick .7 and get respond.",
                    "label": 0
                },
                {
                    "sent": "I response points one and put this information up here and now.",
                    "label": 0
                },
                {
                    "sent": "The question is which of the branches should we explore more?",
                    "label": 0
                },
                {
                    "sent": "And now it seems quite obvious that this branch is more promising and UCP can be used to give a more founded answer to this question.",
                    "label": 0
                },
                {
                    "sent": "Which branch to explore more?",
                    "label": 0
                },
                {
                    "sent": "So the next thing we will do is we split up.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This interval into two parts and forward .4 to the respective leaf and query the other leaf which is and we query at .2 and get a response .7 and forward the information we got here up to the branch here.",
                    "label": 0
                },
                {
                    "sent": "So now the total response we got is point 1.6 and we after two trials where we go on for this and can be shown that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the some assumptions on the optimization problem this is.",
                    "label": 0
                },
                {
                    "sent": "Advantageous procedure to do the optimization.",
                    "label": 0
                },
                {
                    "sent": "It was tried in.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In application it was churn prediction for telecommunication company using in our prop as an optimization as using the approach algorithm which has seven parameters and UCT converged five times faster than an alternative method for optimization.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is significant if you know that one run of our probe takes 12 hours and it takes quite a long time to do the whole optimization.",
                    "label": 0
                },
                {
                    "sent": "So a savings of effective 5 is quite significant in real time.",
                    "label": 0
                },
                {
                    "sent": "So the other.",
                    "label": 0
                },
                {
                    "sent": "Work I want to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Report on.",
                    "label": 0
                },
                {
                    "sent": "Is apprenticeship learning using inverse reinforcement learning?",
                    "label": 1
                },
                {
                    "sent": "Um, in inverse reinforcement learning, the question is how can we imitate the pause?",
                    "label": 0
                },
                {
                    "sent": "Probably optimal behavior of an expert and one.",
                    "label": 0
                },
                {
                    "sent": "Night.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eve approach or immediate approach is to just imitate the behavior in the states we have actually observed from the expert.",
                    "label": 0
                },
                {
                    "sent": "The drawback.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this approach is that it does not generalize wealth.",
                    "label": 1
                },
                {
                    "sent": "Two states which we have not observed for which in which the expert has not been, so that we don't have no the actions an expert would take in this state.",
                    "label": 0
                },
                {
                    "sent": "Send alternative approaches.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To line and reward our value function which explains the behavior of the expert.",
                    "label": 0
                },
                {
                    "sent": "So if you have a value function, we can apply it for the whole state space and can derive actions for the whole state space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do this more concretely, we assume that we can parameterized the rewards.",
                    "label": 0
                },
                {
                    "sent": "So in this example a very simple parameterisation.",
                    "label": 0
                },
                {
                    "sent": "The rewards are just a linear combination of some.",
                    "label": 0
                },
                {
                    "sent": "Functions file here.",
                    "label": 0
                },
                {
                    "sent": "And the problem is to find a good setting of these parameters later, which explain the experts behavior so.",
                    "label": 0
                },
                {
                    "sent": "Or to phrase it in another way, if we pick the right parameters and we calculate an optimal policy according to this parameters, then the policy which we get should be similar or as simple as similar as possible to the policy of the expert.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Captain this notion here, so we sum over all the States and want that's the action chosen by his optimal policy given a certain data is close to the action chosen by the expert, and we have a waiting here for over the stationary distribution of the states by them for the experts policy doing this we can use natural gradient techniques to actually calculate good values for these parameters data.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an example how this works, so in the top picture you see the trajectory of an expert and on the left lower left picture you see an marked red regions where the.",
                    "label": 0
                },
                {
                    "sent": "Policy which does.",
                    "label": 0
                },
                {
                    "sent": "Direct policy mutation, which directly mimics the.",
                    "label": 0
                },
                {
                    "sent": "Policy of the expert.",
                    "label": 0
                },
                {
                    "sent": "So so the weather.",
                    "label": 0
                },
                {
                    "sent": "The regions where this weather policy learned by direct policy imitation is close to the experts policy.",
                    "label": 1
                },
                {
                    "sent": "What the experts would actually do and what you see is that in.",
                    "label": 0
                },
                {
                    "sent": "In states we have observed this correspondence is good, but in other in other states this correspondence is quite bad.",
                    "label": 0
                },
                {
                    "sent": "In contrast, on the right hand side you see what we get by inverse reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "When we actually get in.",
                    "label": 0
                },
                {
                    "sent": "Reward function which explains the expert expert's behavior, and this general is generalizes well over the whole state space.",
                    "label": 0
                },
                {
                    "sent": "But this is quite nice result here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Last thing I want to talk when it takes take me with more time that will be a bit give you a bit more details on that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is about online reinforcement learning, so we.",
                    "label": 1
                },
                {
                    "sent": "Look at the generic reinforcement learning problem, so we have.",
                    "label": 0
                },
                {
                    "sent": "Now on a set of states, instead of actions we have rewards for taking a certain action is in a state S, so are other rewards, and we have a transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "So if as an agent takes a certain action in the state as, then it is transferred to a new state as prime given by governed by some probability distribution.",
                    "label": 1
                },
                {
                    "sent": "To the learner this transition transition probabilities are unknown and needs essentially to be learned.",
                    "label": 0
                },
                {
                    "sent": "Um, what we want is an optimal policy, and policy is just a mapping from states to actions.",
                    "label": 0
                },
                {
                    "sent": "I'm a little bit sloppy here because sometimes.",
                    "label": 0
                },
                {
                    "sent": "444 learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is not a fixed mapping, but will also depend on the past on the observations the agent has made so far.",
                    "label": 1
                },
                {
                    "sent": "But what we want to achieve is a policy which optimizes the rewards in the sense that the sum of all rewards during a certain number of trials is maximal where STR the states visited by the agent.",
                    "label": 0
                },
                {
                    "sent": "Given his policy and 80 other actions chosen by this but his agent.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we are interested in in an online analysis is the difference or is what we lose against an optimal policy?",
                    "label": 0
                },
                {
                    "sent": "When we need to learn the MDP so the optimal policy can be chosen when we know the transition probabilities of the MDP, but the line it doesn't know this transition probabilities needs to learn them and the question is how much worse is the behavior of the learning agent against what an optimal policy could achieve.",
                    "label": 0
                },
                {
                    "sent": "So Delta is the regret.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what we want to get a good bound on.",
                    "label": 0
                },
                {
                    "sent": "And an algorithm which achieves smaller regret.",
                    "label": 0
                },
                {
                    "sent": "Well, if we go back, you see that we I don't have a discount factor here, so I'm talking about undiscounted reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "This has a simple reason if we use a discount factor then any.",
                    "label": 0
                },
                {
                    "sent": "Some over the discounted some of the rewards or the differences between two sums of discounted rewards is just a constant for any discount factor smaller than one.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give us any interesting regret bounds, so that's why we're talking here.",
                    "label": 0
                },
                {
                    "sent": "I'm talking here only about undiscounted reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, this is the result, so we have an algorithm which we call you SRL.",
                    "label": 0
                },
                {
                    "sent": "Again, upper confidence bounds reinforcement learning an and if you have Estates and a actions then the bound is on their regret.",
                    "label": 0
                },
                {
                    "sent": "This D * S Times Square root 80.",
                    "label": 0
                },
                {
                    "sent": "Which is just the dependency on the square root of the number of trials and the number of actions and the linear dependency on the number of States and Steve, what I.",
                    "label": 0
                },
                {
                    "sent": "But I called the diameter of the MDP.",
                    "label": 0
                },
                {
                    "sent": "This is actually I think that's the main contribution of one of the.",
                    "label": 0
                },
                {
                    "sent": "Main contributions here that we have this diameter in the bounds.",
                    "label": 0
                },
                {
                    "sent": "The diameter is.",
                    "label": 0
                },
                {
                    "sent": "Is such that for any pair of states as one as two?",
                    "label": 0
                },
                {
                    "sent": "There's a strategy which moves you from S1 to SS2 within these steps.",
                    "label": 0
                },
                {
                    "sent": "So for each pair, we can choose a strategy such that it moves from S1 to S2 within these steps.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This seems a very natural quantity for an MDP, because when you want to explore the MDP, you need to be able to move from one state to the other, and if it takes very long then you're learning behavior will be worse than if it takes just a small number of steps.",
                    "label": 0
                },
                {
                    "sent": "So this seems a very natural quantity for Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "And we also have a lower bound which shows that you not very far off, only the experts.",
                    "label": 0
                },
                {
                    "sent": "Only the exponents of on D&S are in our upper bound little words.",
                    "label": 0
                },
                {
                    "sent": "I do believe that the lower bound is the correct one.",
                    "label": 0
                },
                {
                    "sent": "We might be able to tighten the analysis for the upper bound, but that's just a guess.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the relation to other work?",
                    "label": 1
                },
                {
                    "sent": "So there has been quite some work on the park like bonds for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So the first one, not the first one.",
                    "label": 0
                },
                {
                    "sent": "The first one is maybe the second one, then the well known one is the cubed algorithm by current sensing which show that if the polynomial number of steps the you'll converge to a policy which is at most epsilon worse than the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 1
                },
                {
                    "sent": "The analysis of card on our marks GIFs.",
                    "label": 0
                },
                {
                    "sent": "More precise and the better bound on this, so he bounced a number of actions which are not epsilon optimal.",
                    "label": 1
                },
                {
                    "sent": "Antique gives Gap gets this bound on this number of steps where you're not optimal, so are bound can be quite easily converted into such a bound on the number of non optimal.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steps and looks actually quite similar to Chotis bound.",
                    "label": 0
                },
                {
                    "sent": "They only did well.",
                    "label": 0
                },
                {
                    "sent": "We're a little better independency on the dependence on epsilon and but the main difference is that.",
                    "label": 0
                },
                {
                    "sent": "They also can sing and Carter have this.",
                    "label": 0
                },
                {
                    "sent": "Mixing time epsilon mixing time in the bound where we have the diameter.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the epsilon mixing time is the number of steps such that any policy converge basically to its or get close to its.",
                    "label": 1
                },
                {
                    "sent": "Stationary distribution more or less, it's a little more involved, so after that many steps you get the expected reward is epsilon close.",
                    "label": 1
                },
                {
                    "sent": "To the, uh, the actual reward is epsilon close to the expected reward, but this is quite similar to getting close to the stationary distribution.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the question is, what's the relation between the diameter and steps of mixing time and for small epsilon?",
                    "label": 0
                },
                {
                    "sent": "The relationship is that the smaller the epsilon, the larger the mixing time, so you get an additional factors on epsilon in the bound and.",
                    "label": 0
                },
                {
                    "sent": "This is just an lower bound on the mixing time.",
                    "label": 1
                },
                {
                    "sent": "The mixing time can actually be arbitrarily bad.",
                    "label": 0
                },
                {
                    "sent": "Because it might, you might have an observing state, and it's very hard to get there with your current policy.",
                    "label": 0
                },
                {
                    "sent": "Then the mixing time will be very large.",
                    "label": 1
                },
                {
                    "sent": "And there's other work on reinforcement learning where right now we had.",
                    "label": 0
                },
                {
                    "sent": "Bounce, which essentially translates to bounds like.",
                    "label": 0
                },
                {
                    "sent": "Which square root T there is all the way.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rick, where you get locked bounds on the regret for this kind of bounds, you need to assume that there's a gap between the best policy and the second best policy.",
                    "label": 0
                },
                {
                    "sent": "If you assume this gap, which I called JM.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then Jay, so maybe not Jay.",
                    "label": 0
                },
                {
                    "sent": "You get this time.",
                    "label": 0
                },
                {
                    "sent": "This type of bounds from our algorithm with a little bit more complicated analysis and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His previous work by Bernadette this, Kathy Hawkins and the very bad let recently which gets.",
                    "label": 0
                },
                {
                    "sent": "Who get very similar bounds, even the better dependency on the number of states.",
                    "label": 0
                },
                {
                    "sent": "But what I consider the main difference is that they have a different quantity which measures the difficulty of the MDP here.",
                    "label": 0
                },
                {
                    "sent": "And if.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You look at the definition of the Max, it's.",
                    "label": 0
                },
                {
                    "sent": "The quantity such that for any policy.",
                    "label": 0
                },
                {
                    "sent": "You will move from S1 to S2 with Indy Mac steps.",
                    "label": 0
                },
                {
                    "sent": "So we have them in here.",
                    "label": 0
                },
                {
                    "sent": "So for each pair we can choose a different policy and they have the Max here, which says for any policy you move from S1 to SS2 within this dymax steps.",
                    "label": 0
                },
                {
                    "sent": "This is a big difference because considering consider very simple MDP like amazing world, a great world where you move up, down, right, left or right then obviously their policies which never will get back to the lower left corner.",
                    "label": 0
                },
                {
                    "sent": "So that the Max will be infinite.",
                    "label": 0
                },
                {
                    "sent": "Whereas in our different or where is our diameter is just depends just on the size of the of the maze of the grids and gives you good bounds still.",
                    "label": 0
                },
                {
                    "sent": "So I really believe that this step from the D Max to the diameter is essential contribution of one of the essential contributions here.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm runs, runs in rounds and each round starts at some point, TK, sometime TK and we start a new round.",
                    "label": 1
                },
                {
                    "sent": "Whenever for one state action pair, the number of visits to this state action pair doubles, so this NSATK is the number of state action pairs.",
                    "label": 0
                },
                {
                    "sent": "When this round is started.",
                    "label": 0
                },
                {
                    "sent": "And and you round this started when for some of these state action pairs, the count for this state action pair has doubled.",
                    "label": 0
                },
                {
                    "sent": "So it's quite simple, it's just it's needed to guarantee certain properties of the algorithm which you need in the analysis, which I'm not going into.",
                    "label": 1
                },
                {
                    "sent": "Within each round we fix the policy, so which I call Pi~ K here and this policy is chosen such that it maximizes the expected reward.",
                    "label": 1
                },
                {
                    "sent": "For the best Plausable MVP, best means giving the maximal reward and plausible means that the oops that's the the transition probabilities for the plausible MDP.",
                    "label": 0
                },
                {
                    "sent": "I'm not too far from the empirical estimates, so this is just.",
                    "label": 0
                },
                {
                    "sent": "Essentially this is a confidence region which holds with high probability and has this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the width of the confidence interval.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "The only question obviously is can we calculate such a policy easily or relatively easily, and this is possible.",
                    "label": 0
                },
                {
                    "sent": "We can do this by value iteration.",
                    "label": 1
                },
                {
                    "sent": "If you recall value iteration Bellman.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Updates for discounted reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "They look like this.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more complicated for undiscounted reinforcement learning, because this discounting guarantees that this update converges.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the similar update, which we call bias update for undiscounted reinforcement learning, we don't have the discounting here and which means that this update would converge to Infinity for all the lambdas here.",
                    "label": 0
                },
                {
                    "sent": "So we need to risk.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Till normalize this lambdas just by pulling the smallest one to series, which is done by this update here.",
                    "label": 0
                },
                {
                    "sent": "So we take the minimum of the lambdas and subtracted, which guarantees that the lambdas remain finite and it can be shown that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this update, the update converges to lambdas with which are between zero and the diameter, which is then important for the analysis.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately, don't have time to show you.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analysis.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, you need to use this this Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "US and relate this.",
                    "label": 0
                },
                {
                    "sent": "This lambdas despises two day.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regret then you plug it in and do some calculation and then you get the final result here.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I want to finish talking about future directions.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one interesting question question is can we?",
                    "label": 0
                },
                {
                    "sent": "Have an algorithm which adapts to changes in the MDP so if we change for example transition probabilities, can we have a learning algorithm which picks this changes up with the current algorithm?",
                    "label": 1
                },
                {
                    "sent": "This is not possible because it relies on statistical estimates and doesn't look back if they should be.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Changed now the question is continuous direction spaces.",
                    "label": 0
                },
                {
                    "sent": "We haven't got as far as we would have liked to.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another question which is maybe my hobby autonomous rewards.",
                    "label": 1
                },
                {
                    "sent": "So the question is, can we design reward functions which which do not rely on an external signal but still drive?",
                    "label": 1
                },
                {
                    "sent": "Both the consolidation of the land knowledge and extensions, or exploration to unknown.",
                    "label": 1
                },
                {
                    "sent": "States situations OK thank you.",
                    "label": 0
                }
            ]
        }
    }
}