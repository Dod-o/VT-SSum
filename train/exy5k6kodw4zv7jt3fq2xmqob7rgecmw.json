{
    "id": "exy5k6kodw4zv7jt3fq2xmqob7rgecmw",
    "title": "Consensus Group Stable Feature Selection",
    "info": {
        "author": [
            "Steven Loscalzo, Binghamton University, State University of New York"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Data Mining->Statistical & Consensus Methods"
        ]
    },
    "url": "http://videolectures.net/kdd09_loscalzo_cgsfs/",
    "segmentation": [
        [
            "Hello I'm Steven look Scalzo from Binghamton University where my PhD student there.",
            "And today I'd like to present some work called consensus groups table feature selection stand in conjunction with layhew.",
            "Also from Binghamton University, Ann, Kristin from the University of Texas at Arlington.",
            "And as you can see from the title, our focus is feature selection, but only feature selection.",
            "We're interested in stable feature selection and we use consensus methods to achieve this, so I'll detail what these terms mean in just a second."
        ],
        [
            "Some overview first will go into background, So what is stable feature selection?",
            "Why is it interesting?",
            "Before moving on to our proposed consensus feature, Group Framework consists of two main parts.",
            "One is finding what a consensus feature group is.",
            "And the other is how to use these consensus groups for feature selection.",
            "And then I'll move on to the experimental study, where we show that this approach can give us stable an accurate feature selection results.",
            "And then move on to some conclusions."
        ],
        [
            "So feature selection stability.",
            "Let's look at this generic learning process here where we start out with some just training data.",
            "And then the end goal is to build some model like SVM.",
            "Or this isn't true whatever the model might be and.",
            "Get some accuracy on some unseen test data.",
            "So a lot of times to simulate the unseen test data which you might not have initially.",
            "We sampled the initial training data a number of times, and on each of these samples.",
            "Will build some model, get some accuracy and in the middle here is a feature selection step.",
            "So if the data has high number of features it's too hard to learn directly.",
            "Perform some feature selection and then build the model.",
            "And you can see here which is typical in a lot of scenarios then and no matter what the sample is, you get similar accuracy results from your model.",
            "So you might think you have a good model.",
            "A good approach when you look further at the data.",
            "Let me go back.",
            "Look for the data and look at the features that were selected from each sample and you'll see that on this toy example where I only select two features for example.",
            "You get very different sets of selected features each time, so your feature selection mechanism is unstable.",
            "So why is this interesting?",
            "Why do we care that we select different features each time from different samples?"
        ],
        [
            "Well, in some domains you're really trying to learn more about why you're getting these good accuracy results on your model.",
            "So for instance, in biology, if you're using genetic microarray data.",
            "You're interested in what genes are causing some disease.",
            "So in order to figure out what genes are important, you have many thousands, so you might use some feature selection technique to reduce this number.",
            "If you get different genes every time from slightly different subsamples of your data, which teams are really important?",
            "You don't know.",
            "You don't have the confidence to perform laboratory tests on the selected features.",
            "They selected genes here, so we really want to see what is truly important out of the data.",
            "What features are truly relevant to all the different subsamples that you might have?",
            "And Additionally, when we're using consensus groups, consensus feature groups, feature groups are useful because they help model the interaction between features.",
            "So a lot of times in your data sets, your features are correlated to some degree, or maybe completely redundant.",
            "So you might select one feature for your model.",
            "But you don't know much about it.",
            "Much more information about it from this biology point of view.",
            "But if you know what feature group it is from, you might have already done a lot of research into a related feature, so we could save time save energy in that respect.",
            "So that's why we are looking for these feature groups."
        ],
        [
            "Previously had previous work we came up with some proposed solution.",
            "Proposed approach for getting stable and accurate feature selection using dense feature groups.",
            "And So what the framework here?",
            "Looked at.",
            "As we transpose the usual data matrix are given.",
            "So given thousands of features, they now become found thousands of points in a space that's defined by.",
            "6200 samples so it's.",
            "Transpose what we usually look at when we talk about clustering.",
            "And then we use the kernel density estimation technique and this new space to locate dense regions of features.",
            "So once we found these dense features, we selected the top most dense ones because we believe them to be stable and we go into reasons why and related work.",
            "I don't have time to talk about here.",
            "And we select the top relevant.",
            "Groups from these dense groups to use for feature selection.",
            "But there were some some important limitations to this framework.",
            "One is the density estimation on these high dimension spaces 60 to 100 dimensions.",
            "The game is difficult to get, harder to really.",
            "Look at density in a real way.",
            "There's a lot of space to travel in a lot of directions.",
            "And possibly more importantly, we're looking for accuracy of our results.",
            "We only selected features from the most dense feature groups.",
            "So if there was a relevant feature, but it wasn't in a dense feature group, it was by itself somewhere we didn't use it to build our models, so.",
            "We want to incorporate those features also."
        ],
        [
            "Away.",
            "So that means to get to that, and we propose the consensus feature group framework.",
            "And consensus feature groups.",
            "Are an ensemble of other feature grouping results.",
            "So.",
            "For our approach here we use the dense feature group results that we proposed last year, we.",
            "We do that in multiple subsamples of are given training data to get a number of diverse grouping results as the samples changed a little bit.",
            "The group change also slightly the group membership.",
            "And we can combine these whole groups together using consensus clustering techniques.",
            "To get a set of consensus groups from then we could select relevant groups and they should be more stable because we have a better idea of the whole data.",
            "And so we could address the unreliable aspect of the fence group by itself, and also the relevant.",
            "The relevancy issue by selecting from the entire set of.",
            "Features not just the most dense areas.",
            "So there's two challenges here.",
            "The first challenge is how do we get the initial?",
            "Feature group results.",
            "So as we said, we just use the dense group Finder algorithm from our previous work.",
            "And the second, the second difficulty is how do we aggregate all these feature groups?",
            "Feature grouping results, but we might have different numbers of feature groups returned for different subsamples.",
            "Certainly the groups themselves will be slightly different as to their membership."
        ],
        [
            "So we had three ideas on this, and another small example here.",
            "Given only three data subsamples, so there's only three results in our ensemble.",
            "We have these different feature groups that were found in each of these subsamples.",
            "So how to combine them?",
            "One idea is to use some heuristic reference set, for instance so we can look at the overall.",
            "Training sample where these three subsamples were generated from find what feature groups exist there and merge similar groups.",
            "To the original groups found on the.",
            "Source training set, but this is very limited because.",
            "If your if your reference set isn't very good, then you can't really fix it much because you're only comparing with it, so not too good.",
            "Then we looked at two other approaches and in this case we treated.",
            "You could either with the group itself is 1 entity and you try to re cluster all these different groups together to find your consensus groups.",
            "Or you could look at each instance, each feature, sorry.",
            "As its own entity and look how how common.",
            "These features occur together in different groups from your different data subsamples.",
            "And the cluster based result is hard to use for our problems because we don't get 3 feature group results.",
            "We get maybe hundreds.",
            "So this doesn't scale well as you have more and more data subsamples to increase your ensemble, it's really becomes unfeasible to do.",
            "But the instance based approach stays constant in the number of features you have in your problem, no matter how many data subsamples you use.",
            "So we went with the instance based approach.",
            "And for an idea on how we use this, let's.",
            "Treat this is a graph of five features from where these problems will draw a line.",
            "Between any 2 features that were found together in a particular subsample result, so features F1 and F2 have aligned together because they were in a group together, and so on.",
            "We could do this for each of our.",
            "Feature group results.",
            "Until we finally draw lines from all of our subsamples.",
            "And then we could use some glamour agglomerative hierarchical clustering algorithm, some other similar algorithm to look at how similar different features are and the most similar features.",
            "The more times they occur together in the same groups over the different runs.",
            "They get merged together.",
            "Sooner so eventually.",
            "You have some threshold and cutoff at some point which we could talk about the Post recession.",
            "It's also in our paper you come up with your consensus feature groups.",
            "Now from your initial.",
            "Sets of feature groups that you had from all your different data samples."
        ],
        [
            "So this methodology is encapsulated in our CGS algorithm, so the consensus groups table feature selection algorithm.",
            "It's a lot of writing, so let's go through the example given some training.",
            "Sample some initial training sample.",
            "We partition it or bootstrap or do some other sampling technique to create T. Subsamples from this initial training sample.",
            "And on each of these subsamples we perform our base algorithm or based feature clustering feature grouping algorithm.",
            "To get a number of feature grouping results so those are the sets of groups from the last slide, we only had three sets here.",
            "We have T sets.",
            "We measured the instance cooccurrence as in the last time, so.",
            "Have our graph of our lines or however it's represented.",
            "Perform hierarchical clustering algorithm.",
            "To come up with our consensus feature groups.",
            "So we still have the problem that some of these groups might not be relevant, so we might have better form groups from the point that they won't shift as much in their membership as we look at different samples of the data.",
            "But they still could just be comprised of irrelevant features to the target concept.",
            "So we have to measure the relevance of the groups, and there's a number of ways we could do this.",
            "We chose just to look at a central feature from the group.",
            "Look at the relevance of that feature to the entire.",
            "To the class label.",
            "And rank the groups based on that relevance measure.",
            "So from there we could take the top relevant groups however many groups we want and use these representative features to give to our learning algorithm.",
            "SVM or decision tree or whatever we choose to use to represent the groups.",
            "So as a."
        ],
        [
            "Some validation to these claims that we could find stable and accurate results.",
            "We performed the following experiments.",
            "And these six datasets.",
            "So they have several thousands of genes, so these are the features here and very few.",
            "Samples are very few dimensions.",
            "So once again, each of these genes would be a point in our.",
            "Groups what we did here was for every sample we had.",
            "We partitioned into sent ten sets of groups.",
            "There we have nine of them to the CGS algorithm and compared.",
            "Models built on the information gained from the selected features on those nine groups to the one holdout test set and we tried that for all 10 different variations and we shuffled the data 10 different times to create a 100 different.",
            "Test sets.",
            "And then we compared our results so the accuracy and stability of our results with the CGS algorithm to our previous drags algorithm which just used the top density groups and also a SVM RFE algorithm which is known to find accurate.",
            "Features from these types of datasets these high dimensional.",
            "Low numbers of sample datasets.",
            "So."
        ],
        [
            "First we take a look at the stability of our selected groups and we can only do this for.",
            "We can only fairly do this with drags and CGS because they find groups of features, whereas SVM, RFE finds individual features and we can see that.",
            "CGS is the red line, and it's on top of our existing approach, so it's improved the stability of our groups, so the consensus idea does find more stable groups.",
            "The SIM ID is the measurement of the stability, so it's basically.",
            "Looks at how similar the membership of each group is across the different results from your different.",
            "Folds and shuffles.",
            "Averages them together.",
            "And.",
            "To compare the selected features, the features used in building the classification models.",
            "We can look at CGS drags and RFE together.",
            "Now we're down to individual features we can once again see that the CGS algorithm finds more stable features.",
            "With respect to the stability of the ID.",
            "Of the selected feature, so it's quite an improvement over the previous.",
            "Approaches.",
            "And again, stability isn't the only thing.",
            "If you can't build an accurate model, then why bother?",
            "So we have to."
        ],
        [
            "The accuracy.",
            "Of our results, and again, we've shown an improvement over the other approaches.",
            "And especially you could see.",
            "Over the drags approach.",
            "So now that we select from the full spectrum of all the groups that we find, not just the most dense ones, the inclusion of these smaller groups can really help the relevance of our selected features."
        ],
        [
            "So in conclusion.",
            "The proposed consensus framework that we have here does find both stable an accurate features.",
            "There's a number of future directions we could look into different techniques, different based algorithms we could use to find feature groups.",
            "It's a lot of open.",
            "Open directions here so this time.",
            "I'll take any questions you might have.",
            "I."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello I'm Steven look Scalzo from Binghamton University where my PhD student there.",
                    "label": 0
                },
                {
                    "sent": "And today I'd like to present some work called consensus groups table feature selection stand in conjunction with layhew.",
                    "label": 0
                },
                {
                    "sent": "Also from Binghamton University, Ann, Kristin from the University of Texas at Arlington.",
                    "label": 1
                },
                {
                    "sent": "And as you can see from the title, our focus is feature selection, but only feature selection.",
                    "label": 0
                },
                {
                    "sent": "We're interested in stable feature selection and we use consensus methods to achieve this, so I'll detail what these terms mean in just a second.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some overview first will go into background, So what is stable feature selection?",
                    "label": 0
                },
                {
                    "sent": "Why is it interesting?",
                    "label": 0
                },
                {
                    "sent": "Before moving on to our proposed consensus feature, Group Framework consists of two main parts.",
                    "label": 1
                },
                {
                    "sent": "One is finding what a consensus feature group is.",
                    "label": 0
                },
                {
                    "sent": "And the other is how to use these consensus groups for feature selection.",
                    "label": 0
                },
                {
                    "sent": "And then I'll move on to the experimental study, where we show that this approach can give us stable an accurate feature selection results.",
                    "label": 0
                },
                {
                    "sent": "And then move on to some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So feature selection stability.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this generic learning process here where we start out with some just training data.",
                    "label": 0
                },
                {
                    "sent": "And then the end goal is to build some model like SVM.",
                    "label": 0
                },
                {
                    "sent": "Or this isn't true whatever the model might be and.",
                    "label": 0
                },
                {
                    "sent": "Get some accuracy on some unseen test data.",
                    "label": 0
                },
                {
                    "sent": "So a lot of times to simulate the unseen test data which you might not have initially.",
                    "label": 0
                },
                {
                    "sent": "We sampled the initial training data a number of times, and on each of these samples.",
                    "label": 1
                },
                {
                    "sent": "Will build some model, get some accuracy and in the middle here is a feature selection step.",
                    "label": 0
                },
                {
                    "sent": "So if the data has high number of features it's too hard to learn directly.",
                    "label": 0
                },
                {
                    "sent": "Perform some feature selection and then build the model.",
                    "label": 1
                },
                {
                    "sent": "And you can see here which is typical in a lot of scenarios then and no matter what the sample is, you get similar accuracy results from your model.",
                    "label": 0
                },
                {
                    "sent": "So you might think you have a good model.",
                    "label": 0
                },
                {
                    "sent": "A good approach when you look further at the data.",
                    "label": 1
                },
                {
                    "sent": "Let me go back.",
                    "label": 0
                },
                {
                    "sent": "Look for the data and look at the features that were selected from each sample and you'll see that on this toy example where I only select two features for example.",
                    "label": 0
                },
                {
                    "sent": "You get very different sets of selected features each time, so your feature selection mechanism is unstable.",
                    "label": 0
                },
                {
                    "sent": "So why is this interesting?",
                    "label": 0
                },
                {
                    "sent": "Why do we care that we select different features each time from different samples?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, in some domains you're really trying to learn more about why you're getting these good accuracy results on your model.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in biology, if you're using genetic microarray data.",
                    "label": 0
                },
                {
                    "sent": "You're interested in what genes are causing some disease.",
                    "label": 0
                },
                {
                    "sent": "So in order to figure out what genes are important, you have many thousands, so you might use some feature selection technique to reduce this number.",
                    "label": 0
                },
                {
                    "sent": "If you get different genes every time from slightly different subsamples of your data, which teams are really important?",
                    "label": 0
                },
                {
                    "sent": "You don't know.",
                    "label": 0
                },
                {
                    "sent": "You don't have the confidence to perform laboratory tests on the selected features.",
                    "label": 1
                },
                {
                    "sent": "They selected genes here, so we really want to see what is truly important out of the data.",
                    "label": 1
                },
                {
                    "sent": "What features are truly relevant to all the different subsamples that you might have?",
                    "label": 0
                },
                {
                    "sent": "And Additionally, when we're using consensus groups, consensus feature groups, feature groups are useful because they help model the interaction between features.",
                    "label": 0
                },
                {
                    "sent": "So a lot of times in your data sets, your features are correlated to some degree, or maybe completely redundant.",
                    "label": 0
                },
                {
                    "sent": "So you might select one feature for your model.",
                    "label": 0
                },
                {
                    "sent": "But you don't know much about it.",
                    "label": 0
                },
                {
                    "sent": "Much more information about it from this biology point of view.",
                    "label": 1
                },
                {
                    "sent": "But if you know what feature group it is from, you might have already done a lot of research into a related feature, so we could save time save energy in that respect.",
                    "label": 0
                },
                {
                    "sent": "So that's why we are looking for these feature groups.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previously had previous work we came up with some proposed solution.",
                    "label": 0
                },
                {
                    "sent": "Proposed approach for getting stable and accurate feature selection using dense feature groups.",
                    "label": 1
                },
                {
                    "sent": "And So what the framework here?",
                    "label": 0
                },
                {
                    "sent": "Looked at.",
                    "label": 0
                },
                {
                    "sent": "As we transpose the usual data matrix are given.",
                    "label": 0
                },
                {
                    "sent": "So given thousands of features, they now become found thousands of points in a space that's defined by.",
                    "label": 0
                },
                {
                    "sent": "6200 samples so it's.",
                    "label": 0
                },
                {
                    "sent": "Transpose what we usually look at when we talk about clustering.",
                    "label": 1
                },
                {
                    "sent": "And then we use the kernel density estimation technique and this new space to locate dense regions of features.",
                    "label": 0
                },
                {
                    "sent": "So once we found these dense features, we selected the top most dense ones because we believe them to be stable and we go into reasons why and related work.",
                    "label": 1
                },
                {
                    "sent": "I don't have time to talk about here.",
                    "label": 0
                },
                {
                    "sent": "And we select the top relevant.",
                    "label": 0
                },
                {
                    "sent": "Groups from these dense groups to use for feature selection.",
                    "label": 1
                },
                {
                    "sent": "But there were some some important limitations to this framework.",
                    "label": 0
                },
                {
                    "sent": "One is the density estimation on these high dimension spaces 60 to 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "The game is difficult to get, harder to really.",
                    "label": 0
                },
                {
                    "sent": "Look at density in a real way.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of space to travel in a lot of directions.",
                    "label": 0
                },
                {
                    "sent": "And possibly more importantly, we're looking for accuracy of our results.",
                    "label": 0
                },
                {
                    "sent": "We only selected features from the most dense feature groups.",
                    "label": 0
                },
                {
                    "sent": "So if there was a relevant feature, but it wasn't in a dense feature group, it was by itself somewhere we didn't use it to build our models, so.",
                    "label": 0
                },
                {
                    "sent": "We want to incorporate those features also.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Away.",
                    "label": 0
                },
                {
                    "sent": "So that means to get to that, and we propose the consensus feature group framework.",
                    "label": 1
                },
                {
                    "sent": "And consensus feature groups.",
                    "label": 0
                },
                {
                    "sent": "Are an ensemble of other feature grouping results.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For our approach here we use the dense feature group results that we proposed last year, we.",
                    "label": 0
                },
                {
                    "sent": "We do that in multiple subsamples of are given training data to get a number of diverse grouping results as the samples changed a little bit.",
                    "label": 0
                },
                {
                    "sent": "The group change also slightly the group membership.",
                    "label": 0
                },
                {
                    "sent": "And we can combine these whole groups together using consensus clustering techniques.",
                    "label": 0
                },
                {
                    "sent": "To get a set of consensus groups from then we could select relevant groups and they should be more stable because we have a better idea of the whole data.",
                    "label": 1
                },
                {
                    "sent": "And so we could address the unreliable aspect of the fence group by itself, and also the relevant.",
                    "label": 0
                },
                {
                    "sent": "The relevancy issue by selecting from the entire set of.",
                    "label": 0
                },
                {
                    "sent": "Features not just the most dense areas.",
                    "label": 0
                },
                {
                    "sent": "So there's two challenges here.",
                    "label": 1
                },
                {
                    "sent": "The first challenge is how do we get the initial?",
                    "label": 0
                },
                {
                    "sent": "Feature group results.",
                    "label": 0
                },
                {
                    "sent": "So as we said, we just use the dense group Finder algorithm from our previous work.",
                    "label": 0
                },
                {
                    "sent": "And the second, the second difficulty is how do we aggregate all these feature groups?",
                    "label": 1
                },
                {
                    "sent": "Feature grouping results, but we might have different numbers of feature groups returned for different subsamples.",
                    "label": 0
                },
                {
                    "sent": "Certainly the groups themselves will be slightly different as to their membership.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we had three ideas on this, and another small example here.",
                    "label": 0
                },
                {
                    "sent": "Given only three data subsamples, so there's only three results in our ensemble.",
                    "label": 0
                },
                {
                    "sent": "We have these different feature groups that were found in each of these subsamples.",
                    "label": 0
                },
                {
                    "sent": "So how to combine them?",
                    "label": 0
                },
                {
                    "sent": "One idea is to use some heuristic reference set, for instance so we can look at the overall.",
                    "label": 0
                },
                {
                    "sent": "Training sample where these three subsamples were generated from find what feature groups exist there and merge similar groups.",
                    "label": 0
                },
                {
                    "sent": "To the original groups found on the.",
                    "label": 0
                },
                {
                    "sent": "Source training set, but this is very limited because.",
                    "label": 0
                },
                {
                    "sent": "If your if your reference set isn't very good, then you can't really fix it much because you're only comparing with it, so not too good.",
                    "label": 0
                },
                {
                    "sent": "Then we looked at two other approaches and in this case we treated.",
                    "label": 0
                },
                {
                    "sent": "You could either with the group itself is 1 entity and you try to re cluster all these different groups together to find your consensus groups.",
                    "label": 0
                },
                {
                    "sent": "Or you could look at each instance, each feature, sorry.",
                    "label": 0
                },
                {
                    "sent": "As its own entity and look how how common.",
                    "label": 0
                },
                {
                    "sent": "These features occur together in different groups from your different data subsamples.",
                    "label": 0
                },
                {
                    "sent": "And the cluster based result is hard to use for our problems because we don't get 3 feature group results.",
                    "label": 1
                },
                {
                    "sent": "We get maybe hundreds.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't scale well as you have more and more data subsamples to increase your ensemble, it's really becomes unfeasible to do.",
                    "label": 0
                },
                {
                    "sent": "But the instance based approach stays constant in the number of features you have in your problem, no matter how many data subsamples you use.",
                    "label": 1
                },
                {
                    "sent": "So we went with the instance based approach.",
                    "label": 0
                },
                {
                    "sent": "And for an idea on how we use this, let's.",
                    "label": 0
                },
                {
                    "sent": "Treat this is a graph of five features from where these problems will draw a line.",
                    "label": 0
                },
                {
                    "sent": "Between any 2 features that were found together in a particular subsample result, so features F1 and F2 have aligned together because they were in a group together, and so on.",
                    "label": 0
                },
                {
                    "sent": "We could do this for each of our.",
                    "label": 0
                },
                {
                    "sent": "Feature group results.",
                    "label": 0
                },
                {
                    "sent": "Until we finally draw lines from all of our subsamples.",
                    "label": 0
                },
                {
                    "sent": "And then we could use some glamour agglomerative hierarchical clustering algorithm, some other similar algorithm to look at how similar different features are and the most similar features.",
                    "label": 0
                },
                {
                    "sent": "The more times they occur together in the same groups over the different runs.",
                    "label": 0
                },
                {
                    "sent": "They get merged together.",
                    "label": 0
                },
                {
                    "sent": "Sooner so eventually.",
                    "label": 0
                },
                {
                    "sent": "You have some threshold and cutoff at some point which we could talk about the Post recession.",
                    "label": 1
                },
                {
                    "sent": "It's also in our paper you come up with your consensus feature groups.",
                    "label": 0
                },
                {
                    "sent": "Now from your initial.",
                    "label": 0
                },
                {
                    "sent": "Sets of feature groups that you had from all your different data samples.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this methodology is encapsulated in our CGS algorithm, so the consensus groups table feature selection algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's a lot of writing, so let's go through the example given some training.",
                    "label": 0
                },
                {
                    "sent": "Sample some initial training sample.",
                    "label": 0
                },
                {
                    "sent": "We partition it or bootstrap or do some other sampling technique to create T. Subsamples from this initial training sample.",
                    "label": 0
                },
                {
                    "sent": "And on each of these subsamples we perform our base algorithm or based feature clustering feature grouping algorithm.",
                    "label": 0
                },
                {
                    "sent": "To get a number of feature grouping results so those are the sets of groups from the last slide, we only had three sets here.",
                    "label": 0
                },
                {
                    "sent": "We have T sets.",
                    "label": 1
                },
                {
                    "sent": "We measured the instance cooccurrence as in the last time, so.",
                    "label": 0
                },
                {
                    "sent": "Have our graph of our lines or however it's represented.",
                    "label": 0
                },
                {
                    "sent": "Perform hierarchical clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "To come up with our consensus feature groups.",
                    "label": 0
                },
                {
                    "sent": "So we still have the problem that some of these groups might not be relevant, so we might have better form groups from the point that they won't shift as much in their membership as we look at different samples of the data.",
                    "label": 0
                },
                {
                    "sent": "But they still could just be comprised of irrelevant features to the target concept.",
                    "label": 0
                },
                {
                    "sent": "So we have to measure the relevance of the groups, and there's a number of ways we could do this.",
                    "label": 1
                },
                {
                    "sent": "We chose just to look at a central feature from the group.",
                    "label": 0
                },
                {
                    "sent": "Look at the relevance of that feature to the entire.",
                    "label": 1
                },
                {
                    "sent": "To the class label.",
                    "label": 0
                },
                {
                    "sent": "And rank the groups based on that relevance measure.",
                    "label": 0
                },
                {
                    "sent": "So from there we could take the top relevant groups however many groups we want and use these representative features to give to our learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "SVM or decision tree or whatever we choose to use to represent the groups.",
                    "label": 0
                },
                {
                    "sent": "So as a.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some validation to these claims that we could find stable and accurate results.",
                    "label": 0
                },
                {
                    "sent": "We performed the following experiments.",
                    "label": 0
                },
                {
                    "sent": "And these six datasets.",
                    "label": 0
                },
                {
                    "sent": "So they have several thousands of genes, so these are the features here and very few.",
                    "label": 0
                },
                {
                    "sent": "Samples are very few dimensions.",
                    "label": 0
                },
                {
                    "sent": "So once again, each of these genes would be a point in our.",
                    "label": 0
                },
                {
                    "sent": "Groups what we did here was for every sample we had.",
                    "label": 0
                },
                {
                    "sent": "We partitioned into sent ten sets of groups.",
                    "label": 0
                },
                {
                    "sent": "There we have nine of them to the CGS algorithm and compared.",
                    "label": 0
                },
                {
                    "sent": "Models built on the information gained from the selected features on those nine groups to the one holdout test set and we tried that for all 10 different variations and we shuffled the data 10 different times to create a 100 different.",
                    "label": 0
                },
                {
                    "sent": "Test sets.",
                    "label": 0
                },
                {
                    "sent": "And then we compared our results so the accuracy and stability of our results with the CGS algorithm to our previous drags algorithm which just used the top density groups and also a SVM RFE algorithm which is known to find accurate.",
                    "label": 0
                },
                {
                    "sent": "Features from these types of datasets these high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Low numbers of sample datasets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First we take a look at the stability of our selected groups and we can only do this for.",
                    "label": 1
                },
                {
                    "sent": "We can only fairly do this with drags and CGS because they find groups of features, whereas SVM, RFE finds individual features and we can see that.",
                    "label": 0
                },
                {
                    "sent": "CGS is the red line, and it's on top of our existing approach, so it's improved the stability of our groups, so the consensus idea does find more stable groups.",
                    "label": 0
                },
                {
                    "sent": "The SIM ID is the measurement of the stability, so it's basically.",
                    "label": 0
                },
                {
                    "sent": "Looks at how similar the membership of each group is across the different results from your different.",
                    "label": 0
                },
                {
                    "sent": "Folds and shuffles.",
                    "label": 0
                },
                {
                    "sent": "Averages them together.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To compare the selected features, the features used in building the classification models.",
                    "label": 1
                },
                {
                    "sent": "We can look at CGS drags and RFE together.",
                    "label": 0
                },
                {
                    "sent": "Now we're down to individual features we can once again see that the CGS algorithm finds more stable features.",
                    "label": 0
                },
                {
                    "sent": "With respect to the stability of the ID.",
                    "label": 0
                },
                {
                    "sent": "Of the selected feature, so it's quite an improvement over the previous.",
                    "label": 0
                },
                {
                    "sent": "Approaches.",
                    "label": 0
                },
                {
                    "sent": "And again, stability isn't the only thing.",
                    "label": 0
                },
                {
                    "sent": "If you can't build an accurate model, then why bother?",
                    "label": 0
                },
                {
                    "sent": "So we have to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The accuracy.",
                    "label": 0
                },
                {
                    "sent": "Of our results, and again, we've shown an improvement over the other approaches.",
                    "label": 0
                },
                {
                    "sent": "And especially you could see.",
                    "label": 0
                },
                {
                    "sent": "Over the drags approach.",
                    "label": 0
                },
                {
                    "sent": "So now that we select from the full spectrum of all the groups that we find, not just the most dense ones, the inclusion of these smaller groups can really help the relevance of our selected features.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "The proposed consensus framework that we have here does find both stable an accurate features.",
                    "label": 1
                },
                {
                    "sent": "There's a number of future directions we could look into different techniques, different based algorithms we could use to find feature groups.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of open.",
                    "label": 0
                },
                {
                    "sent": "Open directions here so this time.",
                    "label": 0
                },
                {
                    "sent": "I'll take any questions you might have.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        }
    }
}