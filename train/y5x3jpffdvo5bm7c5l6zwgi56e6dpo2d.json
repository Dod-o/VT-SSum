{
    "id": "y5x3jpffdvo5bm7c5l6zwgi56e6dpo2d",
    "title": "On manifolds and autoencoders",
    "info": {
        "author": [
            "Pascal Vincent, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_vincent_autoencoders/",
    "segmentation": [
        [
            "Now you know everything about the.",
            "Multilayer perceptron, neural networks, gradient descent, Bozeman machines.",
            "Deep belief networks you know that so we can move.",
            "We can move on.",
            "I'm."
        ],
        [
            "My plan will have two parts of things that seemingly totally unrelated, but we'll see there.",
            "The first is I'll talk about manifolds briefly and how to what we call the manifold hypothesis.",
            "Or rather, assumption and how?",
            "Why this is important?",
            "How we can leverage it?",
            "And then the main part of my talk will be about autoencoders, which are specific type of of multilayer perceptron neural networks.",
            "And how to regularize them so all this country into my first lecture where I focused mostly on prediction that is supervised learning, predicting one variable random variable from a set of others.",
            "Here my talk will be mostly about unsupervised learning and unsupervised learning will focus on mostly two things.",
            "One is basically what is a lot of the of the point of deep learning which is.",
            "Learning representations an A little bit also about density estimation, which is kind of the mother of all.",
            "Say learning algorithms."
        ],
        [
            "Alright, so to begin with, I start with a traditional unsupervised learning task, which is simply a dimensionality reduction.",
            "So dimensionality reduction.",
            "The principle is very simple.",
            "You start from some X that's in some high dimension, D. At some H. It's in some you want to map it to some agents, some lower dimension.",
            "MY here called M. Later I called DH.",
            "That's smaller than the OK, and you want to learn a function F Theta that does that, and ideally want to do that.",
            "In such a way that you preserve all the whatever is useful, relevant, important information in your initial input, but it's in in fewer number of dimensions.",
            "It's a classical problem, So what can this be useful for?",
            "Tell me ideas?",
            "For visualization, yeah, that's 11 aspect.",
            "Feature extraction if you want to extract the most relevant features, maybe to help a subsequent classifier to focus on those.",
            "Yeah, I guess what would you just said for future instruction?",
            "It may help for generalization in some settings.",
            "Compressi"
        ],
        [
            "Then yeah, well, that's another point right here.",
            "We are presenting something with that used a lot of numbers with a small nephew number.",
            "So these are basically all mentioned data compression data set visualization.",
            "So visualization is we can't really visualize our data set in hundred dimension.",
            "OK, 100 dimensional data set the point cloud, but a 2 dimensional or three dimensional point cloud.",
            "We can more easily visualize.",
            "And discarding most important features, that's the feature extraction part.",
            "OK, can you tell me what's the most classical or first algorithm that comes to your mind for dimensionality reduction?",
            "PCA OK?"
        ],
        [
            "Yeah, so that's a classic.",
            "Not to say old algorithm.",
            "It is at least two back to 1901.",
            "And the principle is you have your point cloud, so it's a.",
            "It's an unsupervised learning algorithm.",
            "So you disregard any any class of labels.",
            "Further for those points and your point cloud.",
            "The PCA algorithm is going to find.",
            "Learn really the K, where K you specify K the K directions in that space so that BK set of K vectors that we call principle directions.",
            "In which the data has highest variance.",
            "So if you project your data points on those vectors, that's where they will have the highest variance.",
            "So in effect it learns AK dimensional subspace of your initial space.",
            "So initially you were in SpaceX.",
            "Now you're in the space of the WS here OK?",
            "OK, so it learns the W's.",
            "Which are they?",
            "Principle directions or eigenvectors.",
            "Put them in a W matrix OK and that will be the parameters of our function and then with.",
            "Thanks to these parameters we can project the inputs on these vectors and yield are reduced dimension representation with the benefit that our features that we get here will be decorrelated.",
            "OK, that's one of the interesting aspect of it.",
            "Not independent unless your data was Gaussian initially, but decorrelated.",
            "And this representation is what we call the principle components.",
            "So so to link with our terminology.",
            "Sorry, but without notation we have some X as input, some F Theta here, which is a function that's going to map it to some H. OK Ann, the way it's done is just linear.",
            "It's W X + B or actually in PSU for WX minus mu, where mu is the is the the the average of the data points.",
            "OK, so our parameters here are WMU.",
            "Alright, so that's very classical.",
            "We probably all know about this, so why?"
        ],
        [
            "Did I mention PCA?",
            "Oh OK, why Dempsey?",
            "Right, yeah?",
            "I remember the goal of this talk right at 2.7.",
            "Sorry.",
            "OK, thank you so I have one manifold learning.",
            "So for me, PCA is a prototypical manifold modeling algorithm.",
            "OK, I'll get back to that.",
            "It's also the prototypical unsupervised representation learning algorithm and it's related to auto encoders.",
            "Will get back to that.",
            "OK."
        ],
        [
            "Now I'm going to manifold modeling, so PCA is really a case where you're modeling a a linear manifold, right?",
            "So linear algorithm.",
            "So you kind of suppose that your data will be well represented if you pass a a hyperplane through them.",
            "All sorts and this is really the prototype of a linear manifold.",
            "A subspace is a linear manifold.",
            "The principle components, then that you obtained by projecting on them.",
            "They can be seen as a location of your points of your data points.",
            "The corners of your point on the manifold in a coordinate system.",
            "All the manifold OK.",
            "So that's the linear manifold part.",
            "Now of course we are our data or complex high dimensional data is not does not live in such a simple manifold that we do simple.",
            "So now what will be really interested is much more complex nonlinear manifold.",
            "So here's an example of a non linear 2D manifold in 3D input space.",
            "So, but these are two examples of lower dimensional manifolds embedded in a high dimensional space OK?",
            "That's what we're interested in here."
        ],
        [
            "Alright.",
            "So why?",
            "Why do we talk about Manifolding in machine learning?",
            "Was there this interest?",
            "Well, one of the reasons is, well, manifolds are cool, but.",
            "Another reason is is this so called manifold hypothesis or that some call manifold assumptions.",
            "It's really kind of something of both.",
            "It's more an assumption.",
            "And it could be summarized in this way phrasing this way that natural data, the kind of data that we collect that we obtained, that we work with in high dimensional spaces, is likely to concentrate close to lower dimensional manifolds in that space.",
            "OK.",
            "So it doesn't mean that all the data points will strictly be on your manifold, it just means that you have high density on the manifold and then they probably density will is expected to decrease rapidly when moving away from that manifold while staying relatively high while staying on the manifold.",
            "And the point is so lower dimensional manifold in your input space."
        ],
        [
            "So it's a bit of a hand WAVY kind of hypothesis or assumption.",
            "I'm going to try to give you some sense of where it comes from, where what we could, where people fall over making these assumptions.",
            "One is the problem of the curse of dimensionality.",
            "So the curse of dimensionality, if you were to ask to ask.",
            "Classical statisticians, what you could possibly hope to do in a.",
            "10,000 a 100,000 dimensional space with a couple of examples.",
            "They would say it's hopeless because of the curse of dimensionality.",
            "OK, but our techniques for deep learning and all they do, we are computer vision.",
            "We're able to deal with that kind of data, so that's because there's a lot of structure in that data.",
            "But just to to give you an idea of how big that space is, if you think take the space of 200 by 200 RGB images.",
            "OK, 8 bit RGB images.",
            "When you look at the number of possible vectors representing those, there's actually 10 to the 96,329 possible such images.",
            "OK, it's enormous.",
            "Is gigantic, so if you want to sample uniformly in that space, what you think you'd get?",
            "No, just simple.",
            "Where you get your face."
        ],
        [
            "Now I get something like that.",
            "Overwhelmingly so, so that's that's that's the Occupy this kind of thing is what occupies the majority of 200 by 200 pixel space images."
        ],
        [
            "Now, but what we're interested in is or what we we face are what we deal with that kind of data that we expose are going to that.",
            "Do we want to to model is more natural images or hear an example of faces?",
            "So natural images reasonably, if you look at these two things you could, we could expect them to occupy a tiny fraction of that huge space A tiny fraction.",
            "So that suggests a very peak density.",
            "The other point is that if you look at faces here, OK, you've all seen these are morphing face morphing algorithms, right?",
            "So even if you take yourself aging etc.",
            "Actually you can think of a path that's moving from one face to the other while all the time maintaining a realistic looking face.",
            "Right, it's possible, so there's a realistic smooth transformation from one image to another that suggested continuous path along a high manifold of high density along the manifold where all the images will be highly highly likely.",
            "So the manifold hypothesis according to which data density concentrates near lower dimensional manifold which suggested by these observation, can this big hope there because it can shift the curse of dimensionality basically from the high dimensional input space.",
            "This large D to the dimension of the manifold, all the low dimensional manifold.",
            "So there's hope.",
            "OK, classic listed decisions.",
            "Don't despair, there is hope even with their curse of dimensionality.",
            "Without making necessary to strong hypothesis."
        ],
        [
            "OK.",
            "So another argument for the for this manifold business.",
            "Is that the manifold for follow?",
            "Naturally from this notion of continuous underlying factors?",
            "There's this idea that whatever we get as input may be high dimensional, but it may have a smaller number of explanatory factors or underlying factors that are responsible for this.",
            "So let's take a restricted setting against against faces.",
            "Yeah I bought this picture from the they didn't vision that website.",
            "If you think of a face or even that any any rigid object, let's take rigid object would be even even simpler than you think at all the different poses 3D poses of that object.",
            "And a couple of variations of illuminations well.",
            "That the image of that object may be pretty high dimensional, you know.",
            "1000 dimensions or whatever, but the underlying factors mean you may have 1000 dimensions in what you observe, but if all you have is variations of poser over thing, well, there's a maybe a couple of 10s of underlying real true factors, right?",
            "So this these ten factors?",
            "Actually completely did will be completely determine this very restricted setting setup will completely determine the appearance.",
            "What you what you get with your thousand thousand observation variables.",
            "So these notion of continuous underlying factors basically.",
            "Mean that will have a kind of manifold structure and then of course part of meaningful representation.",
            "For example, if you have a base or an object, one of the characteristics that you may want to to find from it.",
            "Ideally in an unsupervised way is exposed parameters an illumination where the light comes from.",
            "Things like that OK?",
            "Here's another example that's that's taken from digits.",
            "If you think again about bitmap images of digits.",
            "And say this point here the X is actually represents a is a vector for presenting the digit four and then you do some tiny tiny tiny changes to that digit like tiny rotations or tiny scaling or that digit.",
            "OK, well if it's very very tiny.",
            "Even if you have a I know 50 by 50 image, well there is going to be continuous path going from one to the other as you rotate in that high dimensional space.",
            "So there's a continuous path and if you.",
            "If you take for example the rotations and scalings etc, you have a couple of such underlying continuous factors which will basically give you the dimension idea of the manifold of force.",
            "At least close to that point, OK?",
            "Hum.",
            "OK, any questions so far don't hesitate to interrupt yes.",
            "Right, yeah, yeah.",
            "So so.",
            "So this is this falls naturally from from from continuous factor, so the binary cases more switching from 1 foot to the other item.",
            "But it it it.",
            "It may depend on how you, how you, how you can see them right?",
            "Yes?",
            "No, no, no.",
            "It's an open question.",
            "Spaces.",
            "Small change yeah.",
            "Set.",
            "It's.",
            "Set.",
            "There will be continuous variable.",
            "The other way.",
            "I like to think of extending that to the discrete case is if you think in terms of transformations.",
            "OK if you're at this point X here and you apply a tiny rotations.",
            "That's what we just said to stay on the manifold.",
            "This is because you're in a continuous space.",
            "You can you can apply as tiny rotations as you may possibly want to think, so it's it can be infinitesimal in a in a discrete space.",
            "Suppose we exposing wearing a discrete space.",
            "You could still have this notion.",
            "You can still have this notion that there's an operator that.",
            "Will move you not too far from where you start with an that still gives you is put.",
            "Your state has you stay in a high density region, gives you a high probability thing so you can think of the discrete rotation operator.",
            "OK, it's just I mean, let's think about that.",
            "Could be discrete.",
            "Imagine space.",
            "We just just be 90 degrees.",
            "OK, you could.",
            "You could have it all discrete, but it will still be a valid operator that generalizes this notion of OK.",
            "I can apply that operator an I I'm staying.",
            "I'm still having high probability observations.",
            "Yes.",
            "Alright, that's so yes, yes yes it does so so so the.",
            "The notion of a of a.",
            "Of a manifold here shouldn't be taken literally as as meaning we need to have a super formal, super formal, mathematically defined manifold and align this.",
            "Then maybe this continue it is, it may change.",
            "You may have critical points or things like that.",
            "It may change dimension, you may have one dimensional manifold that crosses 150 dimension manifold and things like that it's expected to be a very complicated space, it's just.",
            "It's just this notion of, as Joshua pointed out, of density, concentration, concentration along something that can be characterized with fewer dimensions, and in this example that are continuous.",
            "But I can't.",
            "Also, there's a notion of continuity.",
            "You can have low entropy distributions that are that are super picky, but then where you have a, you have couple of peaks OK and no path between them.",
            "So that's not the idea of a manifold.",
            "The idea is that you have many points where you have you have a multi dimensional path among points that follows this high density areas.",
            "But these multiple dimensions are lower than the input input dimension.",
            "Yeah.",
            "So I think should move on when you keep a manifold question to the end.",
            "Otherwise this is like 1, one 110th of my presentation so far."
        ],
        [
            "So yeah, just just a little remark that manifolds can be represented by patchwork of tangent spaces.",
            "This notion that every point if you look infinitesimally, you can look at what directions you can move basically, and that gives you a tangent space."
        ],
        [
            "So so to anchor these ideas a little more I dig.",
            "Dig back some older work of mine in annual.",
            "Sure, in the realm of nonparametric density estimations.",
            "Remember I told you, don't know.",
            "We have the highest number of farmers so.",
            "This is classical person windows OK so.",
            "You've had the lecture ready on the director graphical models.",
            "Now is that to some extent, so you know, mixture of Gaussian, right?",
            "You know it makes we've gotten so classical person.",
            "Windows is just a giant, gantic mixture of Gaussians where you put one Gaussian on top of each of your training points.",
            "OK, you don't learn the centers, you just put it on top of them.",
            "So that's a personal window with a with a Gaussian kernel.",
            "But you could use other kinds of kernels and the.",
            "Parameters of the kernel itself, where their high performance that you may tune that kind of smoothing your empirical distribution so the point is you have your ex is here your XI here, which are your training examples and you're putting a Gaussian on centered on each other's eyes.",
            "Am I get back to the covariance matrix in a minute and just averaging them and that gives you your density?",
            "So this is a 1D example where you had a training example.",
            "You put the gas on top of them, you add them or average them altogether, and that gives you your estimated density.",
            "Here's the 2D equivalent.",
            "So here I drew points notice in a kind of a manifold fashion, right?",
            "This is there like along a 1D manifold embedded in a 2 dimensional space.",
            "That's a toy example of it.",
            "So if we're to use classical parts in Windows, we put a little Gaussian bump on each of these points.",
            "So the problem with this is if the manifold hypothesis is true.",
            "That that means here you're not using in any sense this notion that that you have a low dimensional manifold, because each of these Gaussian and Gaussian mixture will actually allocate probability mass away from the manifold as much as it does in the direction of the manifold.",
            "So there's this little idea of a fix, which is which we call manifold parts in Windows.",
            "And that's simply to put a kind of oriented Gaussian on top of each point that's oriented.",
            "It's like an oriented Gaussian pancake that's almost flat.",
            "Orthogonal to the manifold and elongated along the manifold.",
            "So how can we find those directions?",
            "I mean it's simple you Ristic.",
            "We can do a little local PCA, so you take a point.",
            "This point.",
            "You look at its closest neighbors.",
            "OK, you do a little PCA on it and you know in which direction the closest neighbors are mostly aligned.",
            "That's how you get these covariance matrices here.",
            "So actually we use a low rank position.",
            "All those covariance matrices that use only the nearest local PCA directions obtained from the nearest neighbors.",
            "This parametric cousins to this, like the mixture of Gaussian pancakes.",
            "This is nonparametric use.",
            "It's really putting one on top of each."
        ],
        [
            "OK, so we have these isotopic pars and which is the classical one.",
            "We have the manifold parzen where we learn occurrence matrix.",
            "That's an oriented pancake derived from the PCA on the K nearest neighbors local directions.",
            "And a cool extension that worked with it with Joshua and you go.",
            "That is actually not now learning using a neural network to predict these covariance directions.",
            "So we now have predictive function with this."
        ],
        [
            "OK so just to show a quick little example of what with, this may look like we have we had here sign a sinusoidal wave and hear the an in famous Swiss roll.",
            "This is what you get with a mixture of a couple of Gaussians.",
            "OK, this is the optimal thing that the argument was able to find in terms of density.",
            "That's where the parzen window, or here is the optimal you look very fat, very fat Gaussians.",
            "Here's a manifold parts, and so it's much more sharper distribution, and here's a nonlocal manifold which is both sharper and smoother.",
            "Um?",
            "OK, and it does improve on.",
            "Small data set.",
            "USPS at the time, so it shows that this manifold hypothesis actually helps, because if we use those things it did help improve the classification performance even in those high dimensional space."
        ],
        [
            "OK.",
            "So manifold learning is a rich subfield.",
            "There's basically purely nonparametric approaches, the manifold parzen and various methods used for mostly for visualization, like local linear embedding, ISOMAP T sne that you probably heard of.",
            "And there are some learned parameterized function that are often extensions of these, like there's a parametric version of T Sne, where again, you learn a parameterized function to predict the locations in the on the manifold.",
            "So what do all these approaches have in common here in India?",
            "Yes."
        ],
        [
            "Their neighborhood based exactly so they explicitly use distance based neighborhoods, their training with a K nearest neighbors, or pairs of points or neighborhood graphs or whatever, and these are typically based on Euclidean neighbors.",
            "But in high dimension, if you're in high dimension, your nearest Euclidean neighbor."
        ],
        [
            "Can be very different from you.",
            "Right?"
        ],
        [
            "Serve.",
            "That's why K nearest neighbor doesn't work all that well on our complex kind of data, right?",
            "So these neighborhoods are kind of reliable."
        ],
        [
            "Now I'll leave the manifolds for awhile and get back to it later.",
            "You see from a strange angle, an move to auto encoders and their organization.",
            "So auto encoders they were going to build on more of what we what you have seen in the past.",
            "Today's."
        ],
        [
            "Here's my actually one slide on multilayer perception.",
            "Well I have another.",
            "I have two slides but it's just this one.",
            "The multilayer perceptions here I'm showing a single layer perceptron, single hidden layer perceptron.",
            "You have your input layer, your hidden layer H, and an A single output neuron.",
            "OK, and we're using sigmoid words.",
            "Here's 10 H, but I'm actually going to use sigmoid, SIG model hidden hidden units so the output F theater of X is actually the sigmoid.",
            "Yet to see model output.",
            "Off the dot product between the weight vector of that output unit W small W&H, which is the vector of the representation that you get here.",
            "Plus some scalar bias OK, and this vector H is itself a sigmoid off, so non linearity of linear transformation of X that we can write as a deep rhymes times D matrix an D primes times 1 hidden bias vector.",
            "OK.",
            "So these are far parmiters here and we are training this by gradient descent.",
            "To optimize the.",
            "Here is the empirical risk, it's a regularised empirical risk.",
            "More the empirical risk minimization.",
            "Principle, so that's basically just taking the loss.",
            "The sum of the losses could be.",
            "The averages didn't change anything.",
            "All of the examples of the training set here T is a target.",
            "Of the true thing you want to predict an your what your function is predicting.",
            "Plus some organization term that's inducing a preference over your parameters that may or may not be there.",
            "So I'm writing this objective that we're optimizing here J MLP.",
            "I'm going to write our objectives J something KJMOP that's the multilayer perception objective filthy to so let's a supervise case you see because we have a target.",
            "We have an explicit target T here that's and we have an input X. OK."
        ],
        [
            "It's clear.",
            "Alright, so now autoencoders.",
            "Well, autoencoders are simply multilayer perceptrons, just like those simple we've seen.",
            "But now we're going to use them for unsupervised learning instead of supervised learning.",
            "So that means we don't have a target.",
            "We don't have an explicit target, So what are we going to use as a target?",
            "The input OK is simple.",
            "We make the output layer the same size as the input layer, so we take as many output prediction neurons as our input prediction input neurons.",
            "And we put the target equal to the input.",
            "OK, so basically that's what the network looks like.",
            "It has some input.",
            "It still has some hidden and it has some output layer that will now call the reconstruction OK, 'cause that output layer is actually going to try and reconstruct that input.",
            "And the loss encourages the output reconstruction to be close to the input.",
            "OK, we're minimizing the discrepancy between X and whatever we got here.",
            "Is reconstruction by going through the network.",
            "Right?",
            "So these were also called dyeable networks.",
            "Traditionally, are sandglass shaped Nets because they have this bottleneck here in the middle C it's lower dimensional representation in the middle.",
            "That's the part that's actually interesting, right?",
            "That's what you get something interesting?",
            "OK, thank there.",
            "So I'm just going to move things around a little bit.",
            "I'm going to show you the autoencoder, but this time is exactly the same network.",
            "It's just that I'm going to move the reconstruction here and I'm going to put it besides the input, just to emphasize that there in the same space, actually the same dimensional space.",
            "And is there really a reconstruction of the input?"
        ],
        [
            "So that's what it looks like, but it's otherwise it's really the same.",
            "So you start with some input X, you map it.",
            "That's the typical format, map it with some encoding function, H2, hidden representation H and the encoding him representation H here that you get.",
            "Actually the encoder is typically sigmoid or some nonlinearity off a linear transformation of X.",
            "So linear transform X put some nonlinearity.",
            "You get your hidden representation.",
            "And then from that in representation you want to be able to decode it to kind of put it back from the information that you got in there to reconstruct your something that's as close as possible to your input.",
            "And that's going to use a decoder G with the typical form that's similarly going to be a linear mapping of H plus some bias, optionally followed by a non linearity here that may may be the same or different from that one.",
            "And then the loss is going to simply compare how.",
            "Poorly, you're doing reconstruction basically how how the discrepancy, how far your Reconstruction R is from your age, original X OK, and the training is going to minimize that, so it's going to minimize the autoencoder.",
            "For examples, X belong to the training set.",
            "The discrepancy between your X and your reconstruction of X, which is the composition of the encoder and decoder.",
            "OK. And for reconstruction error, well, we can.",
            "You can use squared error or Bernoulli cross entropy for binary variable that's more appropriate.",
            "Yes.",
            "No.",
            "There's many reasons so traditional automakers don't didn't have transposed ways.",
            "There's no prior reason in this setting, at least four to have transposed weights now transpose weights in juice.",
            "Two things.",
            "First, they come, they induce a kind of regularization by themselves because by having transposed weights, there's some games the ultimate coder can't play, which is like making one the guy very small.",
            "The other very big and things like that and transpose weights also appear and we get back to that they appear in.",
            "Restricted Boltzmann machines restrictive transpose wait.",
            "They fall naturally from the energy function.",
            "And this connection between, but fundamentally, there's no reason.",
            "Actually the encoder and decoder in this general autoencoder framework very flexible.",
            "You can use whichever function you want, they can be parameterized in completely different ways, so there's no reason that their parameters should be the same, right?",
            "Yes.",
            "Well, the kind of inverse in a statistical sense, in the sense that you want.",
            "Basically, this guy here should do something close to finding the map of P of H given X.",
            "If you were thinking of random variables and this will be the reverse P of X given H, so that's why I'm saying that kind of.",
            "Inverse is interested in this sense or in a functional sense you could say really AGS, trying to invert whatever aged it OK, but its inverse is a function.",
            "Inverse is a function, doesn't mean that you are going to transpose your matrices.",
            "Actually it means that in a very specific small restricted set of case yes.",
            "Setting there's nothing that really delineates where the encoder is and where the default.",
            "No, there's no.",
            "There's nothing that's entirely true, so there's there's still one thing in this picture.",
            "OK, is that here?",
            "The representation we have here is the bottleneck.",
            "It's the lowest dimensional representation.",
            "That's probably the one that's most interesting that we want to use.",
            "That's the one that captures all the information.",
            "That's retained by that thing.",
            "Yeah sorry yeah.",
            "So, so it's, uh, it's uh, for the.",
            "For here.",
            "It's a it's a continuous space because it's like a multilayer perceptrons.",
            "We want to learn that by backpropagation.",
            "So we need to get it greatness to get through and etc.",
            "It's.",
            "OK. And you can have binary inputs.",
            "It works with with binary inputs, but the representation here OK is not binarized.",
            "It's like it's like if we were to binarize it, is like using using.",
            "Hard hard threshold units.",
            "Binary units in a neural network.",
            "You know if you've seen gradient decent, you've seen that you can't get any gradient through that, and you can't learn it.",
            "I mean not not in the straight gradient descent way."
        ],
        [
            "Alright, so linear autoencoders relationship between linear autoencoders and PCA.",
            "So linear autoencoders.",
            "Here we talking about the traditional one where we had the dimension of the other hidden layer that's smaller than the input.",
            "Actually with linear neurons and squared loss.",
            "The outline coder learned exactly the same subspace as principal component analysis.",
            "So it happens that this is also true if you use a single sigmoidal hidden layer with a sigmoid.",
            "And still a linear output neuron with squared loss provided you have untied weights.",
            "If you tie the weights then you're introducing something more complicated so it can't learn basically what it does here is, it puts itself, it can put itself in the linear regime, so it's still back to linear.",
            "So it's equal to PC, so not not not not super interesting, so it won't learn the exact same basis, SPCA the WS will not necessarily be orthogonal or orthonormal country to PC and not ordered, but they will spend the exact same subspace."
        ],
        [
            "OK, so so it's doing something interesting, something reasonable.",
            "OK, now similarity between auto encoders on restricted Boltzmann machines.",
            "Well, now considering autoencoder with a single hidden layer, again with a sigmoid nonlinearity.",
            "Add a Sigma Sigma output nonlinearity.",
            "This time we have sigmoid output nonlinearity.",
            "No squared error.",
            "Ann and you tie the encoder decoder weights.",
            "If we look at what the auto encoder looks like for its encoder.",
            "Looks like taking a linear transformation or an affine transformation of X. Primitives by WNB here and passing it through a sigmoid.",
            "And the decoder does the reverse.",
            "So it starts from.",
            "It takes H2 through through Nefyn mapping and passes it through a sigmoid.",
            "You get the reconstruction.",
            "And that's the kind of thing that you would use for binary units when you have binary units because the sigmoid will map something between zero and one.",
            "And if you look at the RBM.",
            "So the IBM can be derived from the energy function, but if you look if you if you do the computation from that energy function at look at the conditional probabilities.",
            "So in our BMS we have stochastic variables.",
            "Your visa is a binary input variable and your H is also binary, so it can take values zero and one.",
            "But if you look at what is the probability that some given H is 1?",
            "Given the input, what it's the exact same computation as what we had here for HK?",
            "So basically what we did in what do we do in GBM to compute the probability of a hidden unit being on is exactly what we have in the auto encoder encoding here.",
            "OK, so H will be the probably link is far beyond probably and here is a deterministic function so and similarly for the decoder, so that'll be the probability that the visual visible unit sonar beams.",
            "RV is like RX is here OK?",
            "An we have these transposed weights here, so if we tide autoencoder also have transpose face.",
            "Now the big difference is that in autoencoder is really deterministic mapping.",
            "We were talking just about functions H as a function of X. OK, are they?",
            "Reconstruction is also a function of H, so their continuous differentiable functions.",
            "Here we're talking about stochastic mapping.",
            "HH is considered a random variable that you're.",
            "Ideally you'd be sampling to get those things which always always be zero or one.",
            "This thing is was always be between zero and one.",
            "This thing was always be zero and one, but with the probability that's the same at this thing.",
            "OK, just to show so they look.",
            "Kind of related, right, right?",
            "Not going."
        ],
        [
            "Anyway, so that was the basic.",
            "The basic thing that was no test is if you so due to their relatedness.",
            "Now let's get back to the greedy layer wise pretraining that's used for restricted Boltzmann machine, Ann Hinton.",
            "And coworkers proposed in 2006 which kind of re launched this field of deep learning by showing that if you did this.",
            "Layer by layer unsupervised pre training with RBM's.",
            "OK so do your first layer second layer or the third layer etc.",
            "At this is a very good way, very efficient way to initialize a deep network.",
            "Pre train a deep network.",
            "Initialize a deep network that gave us the deep belief networks.",
            "Which worked pretty well, so we thought, well, you know.",
            "I say yeah, sure, yeah sure thought IBM's an all time quarters are.",
            "Kind of cousins, so why not try that?",
            "Let's try that with with the ultimate coders, so it's exact."
        ],
        [
            "Same principle, just instead of using an RBM.",
            "BMT pre training Slayer user shallow sigmoid autoencoder.",
            "K to pretend it retraining, Slayer.",
            "So you train your first autoencoder to reconstruct its input.",
            "Once you've done that, you get you get ready.",
            "You can forget about the reconstruction part, you take the new representation.",
            "At this level, you start over again and over again.",
            "Etc yes.",
            "Because that's what.",
            "Anne.",
            "OK so I shouldn't say that that's not what works best without encoders, and that's what worked best.",
            "Weather without BMS.",
            "The idea is to.",
            "I'm sorry within a lot of deep learning is that the have an overcomplete representation is better than have a compressed representation.",
            "That's more that was more of an empirical.",
            "Finding, I think that we found it.",
            "It did work best, but for a long time and this may have heard the progress in the field.",
            "We had this vision about.",
            "The information is better to have to be compressed if we presented fewer fewer few units.",
            "But given the fact that the encoder that you have here to go from one to the other is a very very simple function, it's really kind of just linear with some thresholding.",
            "Anything it can do something very complicated, so it can't.",
            "And you can't in one go here.",
            "Hope to extract compact continuous coordinates of something.",
            "Usually.",
            "So because of that, it's it's proved better to map the input to a larger dimensional space.",
            "And eventually get back to your smaller target or whatever.",
            "App.",
            "Yes, so I'm going to get to that.",
            "That's that's that's the whole point that I'm getting at.",
            "But it yeah, it's a very good track, so I had."
        ],
        [
            "The same, but before I moved to that I just that I know I didn't probably didn't perfectly answer your question.",
            "I hope it will become clear.",
            "There is no more bottleneck here.",
            "In this example.",
            "That's true an in the way we use them now.",
            "There is no bottleneck, at least not at at the shallow level layers.",
            "There may be one later on at some point, but not not in the first layers.",
            "Anne.",
            "OK.",
            "I could elaborate on why I think this needs to be for for, like for vision tasks, but I'm with it to the end.",
            "Anne.",
            "OK, so now supervised fine tuning so you have seen this with the Baltimore machines.",
            "Sorry with the deep belief networks.",
            "Well, it's the same principle here with once we've trained, sorry once we've trained our stack, which is all unsupervised, right?",
            "To this point, we haven't used any prediction target at all.",
            "It's all unsupervised.",
            "Weather in our BMR or the autoencoder case, but if we want to use it for to have a deep network for doing something like classification supervised classification, then we just use this first step as initialization and then we do the ordinary thing.",
            "It's just that instead of having initialize our network purely randomly, all the layers we have we have pre trained it with this layer by layer.",
            "This layer by layer fashion and then we just add the output layer, supervised costs with the target and do the normal back propagation.",
            "Or fine tuning this network?",
            "OK, yes.",
            "Yes.",
            "If instead of.",
            "Yes.",
            "Yeah.",
            "OK. From the yeah.",
            "Right, so so that's a very good idea, and now it's not exactly the same, because here in the end we're only using the supervised gradient objective, so it's going to tune only for that supervised objective.",
            "And actually, we did some experiments doing what you suggested and in most cases it actually hurt the performance.",
            "So the important point here is that the unsupervised pretraining is only used to initialize the network in some reasonable space.",
            "That's more reasonable, more useful, more interesting.",
            "Then if we were doing random random initialization.",
            "But apparently it's closer than if you start with random initialization.",
            "That's why it works better.",
            "But I have some experiments that go in direction of asking or answering that question further, yes.",
            "In terms of performance, I'm getting to that, yes.",
            "Well, at the time we did, all these experiments were still using a sigmoid unit, so I still.",
            "I'm sure it still was a problem.",
            "It was probably less of a problem then then with random initialization.",
            "Yeah.",
            "Yeah.",
            "Yes.",
            "Yes.",
            "Well, OK, so so so I'm going historically here is a development of deep learning at the time that we did, that the state of the art was pre training with our BMS.",
            "OK IBM, they have this Sigma is everywhere if you if you use them.",
            "So we use the closest thing which was auto encoders with Sigma.",
            "It's OK for this experience and at the time everybody was still using Sigmoids, and I mean the more mostly rally.",
            "We just it wasn't clear that.",
            "Yet that that it would make bring such an advantage, but there is no reason or probably reason to not to train a a an autoencoder with values.",
            "You can train autoencoder with values to for your pre training if you want to do pretraining.",
            "The idea is that of course what you have in your autoencoder should be the same function that you eventually have in your deep network, right?",
            "And that's kind of.",
            "Similar kind of more natural even for for auto encoders and for our for our BMS because our BMS.",
            "If you want proper PBM it you need to bit needs to correspond to a nice energy function that that gives you all the equation that you hope you get more.",
            "Billy Ray, I don't know if there is an equivalent PBM torrell use.",
            "Probably you can come up with one with some approximation, but for autoencoders kind of natural you can choose whatever you want as the encoder and it's kind of more.",
            "They're both neural networks that you're tuning right?",
            "Just one that is deeper than the other.",
            "But for the whatever each layer is, you can use the same exact thing that it was that you're going to use for your final deep network.",
            "Inside yes.",
            "So so so this trick for our BMS come comes from the actual generative view about what's going on with our BMS.",
            "In practice.",
            "That's not what we did do with with.",
            "Without encoders, there's this.",
            "No, there's not.",
            "This unrolling the actual.",
            "Inference that this is an approximation to.",
            "Yeah."
        ],
        [
            "OK, so super size tuning happens to be quite important.",
            "Here's an experiment where this was an infinite variation.",
            "Type of EM nystagmus with some more variations and all these curves have been actually pre trained with.",
            "Autoencoders it's the same if we pre training with with our BMS on amnesty.",
            "And here we just throw showing what's going on with the supervised phase.",
            "At this point here, so this this.",
            "This is the point.",
            "Where we do the supervised fine tuning.",
            "So we see it first.",
            "It's very important.",
            "So all the actually we start with the same network here.",
            "But here we compare different ways of doing this supervised fine tuning and this is answering part of your question.",
            "So we tried doing purely the supervised fine tuning with only the supervised fine tuning criterion.",
            "So we see this, it gives us this curve.",
            "But if we do it with no.",
            "With.",
            "No supervised fine tuning, you get that.",
            "And these other curves it is dotted.",
            "This dashed is if we use the fine tuning gradient in addition to keeping a under the unsupervised gradient from the reconstruction at the same time as we do this with fine tuning and you see it hurts performance here.",
            "OK, so put training basic outline quarters stack are better than no pre training.",
            "And basic autoencoders stack.",
            "In our experiments, almost matched stacks of RBM's OK, but the key point is here is."
        ],
        [
            "Almost.",
            "So basic outline coders turned out to be not as good feature learners as restricted Boltzmann machines.",
            "So what's the problem?",
            "So, as you rightly pointed out.",
            "This is this is related to the fact that traditional time codes were for dimensionality reduction, but many of the deep learning success seems to depend on the ability to learn what's called overcomplete representation, weather size.",
            "Our presentation is larger than your input.",
            "And basic outgoing quarters.",
            "If you do that, it feels kind of a trivial useless solution.",
            "The identity mapping it can learn simply, it can reproduce or put all the information that it had in X, Co. Pay it back into H and Ann have a perfect reconstruction, yes.",
            "Sorry.",
            "Well OK, the OK how to say the initial RBM?",
            "The binary binary RBM has Bernoulli variables in is visible and hidden.",
            "And sorry an visible and hidden there Bernoulli variables so.",
            "Strictly speaking, you should be using binary data, but when they trained, when they use it on images or anything, they don't use a binary values, you just use a real valued OK, so they're kind of little hacks like that.",
            "So it's true that the binary binary RPMS should be binary data, but they were applied and at the time that was also the case, there were applied on non binary data.",
            "Anne, sorry.",
            "Yeah, so, so that's that's the binary binary RBM, but you can extend our BMS to have any distribution of for your hidden and visible variables basically so.",
            "That include continuous distributions.",
            "OK, yes.",
            "Because the training criterion is very different.",
            "OK, even though the functional form looks very similar, the training criterion are.",
            "BMS looks very different.",
            "The main reason is it falls out from this energy function that you're that you're optimizing.",
            "And if you do that, there's a term that's the partition function.",
            "Basically the these two terms.",
            "There's a term that's going to look somewhat like the reconstruction.",
            "That's going to say.",
            "OK, I want to do an encoding that that gives me a good reconstruction, but there's another term that's going to try to make sure that you can't reconstruct that you can reconstruct well only the training data.",
            "That's the way to interpret it.",
            "What's going on?",
            "If you were to take our BMS and interpret them at all time quarters, there's a term that says OK, reconstruct well.",
            "But this is a term that says reconstruct well, but only on the training data.",
            "Don't reconstruct well anything else, and you don't have that in autoencoders.",
            "Not yet, at least.",
            "Yes.",
            "OK, if you allow I'll carry out."
        ],
        [
            "Really super late on my schedule.",
            "So this is, uh, this got me the idea to introduce the denoising autoencoder so it's a very simple twist simple modification to the autoencoder.",
            "The simple idea is that we're going to destroy some information of the input by randomly selecting some input features.",
            "Putting them 20.",
            "That's what and train the auto encoder to restore it.",
            "OK, so that's why I used initially destroy the feature information by putting them to zero.",
            "That's why I called 0 masking noise.",
            "Now everybody calls it drop off noise.",
            "And.",
            "The thing is, this task of mere reconstruction now becomes the task of denoising 'cause you're corrupting the input and asking that thing to clean it for you, right?",
            "And denoising is vastly more challenging task than reconstruction.",
            "Because even in the widely overcomplete case, so suppose you have many many many hidden variables.",
            "OK, it must learn intelligent encoding decoding for it to help to be able to do this.",
            "Denoising.",
            "So Moreover, it will encourage our presentation that is robust to small perturbations of the input."
        ],
        [
            "OK, 'cause you're perturbing your input all right, so it's a very, very minor twist to this autoencoder architecture.",
            "Is simply you add this little step here.",
            "Your input X you first corrupt it.",
            "OK, so pick some units, put them to 0 or use some other corruption corrupting noise, and then you encode that corrupted version.",
            "Get the hidden representation, you decode it and you get your reconstruction and you hope that your reconstruction is as close as possible to the clean input, very simple."
        ],
        [
            "OK, and this minimizes a slightly modified criterion compared to the original autoencoder, where now we do again a sum of all training examples.",
            "But here is an expectation over.",
            "So instead of taking the discrepancy between clean between the original X and the reconstruction of the original X, who will take the discrepancy?",
            "The loss between the original X, the clean X?",
            "An the kind of reconstructed denoise version of the corrupted X OK, and we do that by an expectation by taking corrupted versions of our original X OK."
        ],
        [
            "So this will learn robust and useful features.",
            "The easier to train than DBM feature.",
            "Then DBM feature extractors.",
            "And I will see that they will similar or better classifications."
        ],
        [
            "So the element code training minimizes this.",
            "As I said, the denoising autoencoder minimizes this.",
            "We're just adding here a kind of expectation over this is the corruption process OK?",
            "Now this except expectation user, because this is kind of highlighted.",
            "This is nonlinear, so we can't compute that exactly OK, we can compute the expectation exactly.",
            "So what we do instead, we used to cast a gradient descent, but adding some levels to casted City by sampling corrupted inputs.",
            "So real it's really going to stochastically corrupt our input X an use a corrupted version, because we can't compute the expectation exactly."
        ],
        [
            "OK and possible corruptions.",
            "There's a zero in pixel at random, just called drop dropout, noise additive, Gaussian noise, audible noise.",
            "This others you can come up with.",
            "OK, so here we using stochastic perturbations, yes.",
            "The influence are there.",
            "Oh I.",
            "Right, so we haven't looked at the effect on iterations per see we will look at the effect on generalization performance, so I can't answer in terms of iterations.",
            "What you're saying is that probably if you corrupt more it will take more time to kind of converge.",
            "To get it there, yeah, that should be expected.",
            "I think that's that's that's an empirical observation that already heard also for dropout in supervised networks and so that it will take longer to converge, but it will converge to something better, yes?",
            "So, so I'm going to show in an variant that that does the corruption at the hidden layer.",
            "I think they're they're playing very similar role.",
            "I don't think it makes much much of a difference.",
            "So from the manifold perspective, it kind of makes more sense to add it as in the input an I'll show the manifold perspective in awhile.",
            "The short answer is that you can view the encoder as projecting on the manifold, so you don't.",
            "You don't want to use to add isotropic noise on your manifold, you want to add isotropic noise on your inputs.",
            "But I'll show an image in in a minute."
        ],
        [
            "OK, just so just to see."
        ],
        [
            "Difference of what this thing is learns OK if you train a basic autoencoder, OK, that's that's complete or over complete, so it has more units than there.",
            "On the natural image patches, so these are extracted from natural images.",
            "That's basically the filters you know where the filters are there.",
            "Just just looking at the weights.",
            "That's our incoming to your hidden Hurons, right?",
            "So each of these little image corresponds to the weights of an incoming urine, and that's what's going to multiply your input.",
            "That's when you train an auto encoder.",
            "That's when you train is regularised autoencoder OK?",
            "It's not a standard way of recognizing it with simply wait to case L2 penalty on on your weights.",
            "Because there are some classical results that show that for linear things, if you actually corrupt your input with a Gaussian noise, it's equivalent to add to add it to penalizing your weights so it was.",
            "You might think.",
            "Well yeah, that's going to be equivalent to doing away today, but it's not.",
            "If you do a denoising autoencoder, that's the kind of filters you get.",
            "OK, so there are clearly agitators here."
        ],
        [
            "OK, and here's another example that is meant to illustrate the role of that.",
            "Now we have a nice new hyperparameter, which is the level of noise, the corruption.",
            "So this is a network that was trained from the same initial random position and we train it 1 version with zero Noise, 1 version with 10% noise, one oil version with 20% one version with 50%.",
            "OK, and what we see is we trained with zero percent which corresponds to the regular autoencoder.",
            "It doesn't learn much interesting.",
            "That's basically all the filters it it gets.",
            "It gets some of them.",
            "But as we increase, the noise level is going to learn these more.",
            "Localized specific filters.",
            "And the reason for that intuitively it's kind of clear, right?",
            "You're corrupting your thing more.",
            "So what it needs to do to predict one pixel, it will need to pull more information about the pixels around it, because it can't rely on that pixels value anymore, right?",
            "Here it can rely on it either just there, here, it can't.",
            "It can't rely on it, so it has to pull more information around it to kind of infer what even knowing that it's it's very noisy what the pixel value will be.",
            "So yes.",
            "If your goal is.",
            "Dependencies.",
            "Corruption.",
            "Yes, you certainly can have random structured corruptions and that that that that can help in specific setting, but this where did this work?",
            "We're really thinking about it was the time we're trying to build a competitor to PBM.",
            "So the the task was.",
            "Say building learners and learning algorithms without any prior knowledge without that could be generically used, used, used for any kind of data without incorporating specific prior knowledge.",
            "But if you do have specific prior knowledge of the structure of the corruption of the data, that makes sense, etc.",
            "It surely can help.",
            "If you integrate that prior knowledge in there."
        ],
        [
            "OK, now the manifold view of this.",
            "There's a manifold interpretation of what's going on with the denoising auto encoder.",
            "Basically, if we think our if we make the hypothesis assumption that our data the ex is yet Alexis is close to some lower dimensional manifold, here 1 dimensional manifold in 2D space, that's a cartoon view of things and we corrupt we take some original input that's close to the manifold and we corrupt it.",
            "OK, so this corruption is kind of isotropic, it doesn't have any preference of direction, it doesn't know about manifolds.",
            "It doesn't care about manifolds, it's just.",
            "Prior noise that we put, so it's going to likely put it in any direction and most directions will be kind of away from the manifold to some extent, so it's corrupting the input and then the.",
            "The encoder is actually going to try and put that point back in to manage something that corresponds to manifold coordinates and then from there is going to be reconstructed.",
            "OK. Again, you can think of it.",
            "Think of the parallel with PCA, PCA and the linear autoencoder.",
            "That's what they're doing, they're projecting.",
            "On the, I mean the encoding or the principle components is really projecting onto coordinates and coordinate system.",
            "All the manifold and then from that coordinate system on the manifold.",
            "The point can be reconstructed.",
            "Yep.",
            "Yes.",
            "This was this was a what we call 0 masking noise that's now called Dropout Dropout Noise.",
            "You might also hear blank out noise, so it's just that we took.",
            "We flipped a sample Bernoulli variable so flip to coins with 20% of it being 20% chance of it being one OK. And if it was one, we set the unit to 0.",
            "So with 20% chance each unit independently was set to 0.",
            "But you can try that.",
            "We tried, we tried with other types of noise, even got additive Gaussian noise etc.",
            "You get, you get nice filters.",
            "Yeah.",
            "So we didn't try with multiplicative Gaussian noise at the time we tried with additive Gaussian noise and there were.",
            "Similar, I'd say they would get slightly better results with the with performance in classification afterwards.",
            "On this there's always a question.",
            "How do you evaluate your your unsupervised representation?",
            "Usually it's done with building classify on top of them, and so regardless of the kind of noise that we use, if we use the right level it it did help.",
            "It did boost in performance compared to not using any and some noises were better than others.",
            "For some cases the kind of drop out.",
            "Blackout was seemed to be better in most cases, but.",
            "It's like just a slightly."
        ],
        [
            "OK.",
            "So here's some experiments, stacked denoising autoencoders.",
            "They were dead.",
            "That's on the OK.",
            "This is the infinite amnist again, and that's the online classification error.",
            "So Infiniteness is very big.",
            "So it's like as if you were measuring actually generalization error at every step.",
            "And that you can see what we get with the 1 hidden layer net with no pretraining.",
            "That's this black line.",
            "This dashed black nine with three neural network with three hidden layers without pre training.",
            "So that was a sigmoid network with three hidden layers without any pre training OK and no noise injection, no nothing.",
            "You see it's worse is doing worse actually than the 1 one layer network OK.",
            "If we did one layer with RBM pretraining.",
            "You get this blue dashed line slam.",
            "Better three layers with RBM training.",
            "We're doing a little better.",
            "His experiment, one layer with our denoising autoencoder, pretraining these following pretty closely.",
            "The one layer PBM maybe.",
            "Well.",
            "Similar to little bit noisy and find the three layer with denoising autoencoder.",
            "In this experiment using.",
            "Really good.",
            "So it works, yes.",
            "Yeah, so people have played with similar with schemes like that changing your corruption rate as you train, etc.",
            "Quite recently added it can help.",
            "So I have a theory for that.",
            "OK, this is hyperparameters.",
            "I talked about my in my in my first first lecture hyperparameters so everybody hates hyperparameters.",
            "Then you need to tune them, right?",
            "That's very annoying.",
            "It's super annoying you don't know what to do, but had this theory that you take an algorithm, you take some smaller modifications where you add some hyperparameters to tune.",
            "OK, well, the more happy with this you have to tune, the better your performance will be.",
            "So so if you do that, in effect, you're talking about the schedule of noise.",
            "OK, that's kind of complicated high parameter to tune because you can choose which we have put more in the beginning, more later, etc.",
            "There will be some optimal setting for this schedule, and this optimal setting for this schedule will allow you to get a little gain in the end.",
            "Yes.",
            "2.",
            "With two hidden layers, you mean Nope, no reason for that.",
            "Which.",
            "Over the prior in this get this guy.",
            "Oh, actually well we had.",
            "We did other experiments where we increase the number of layers and it's just that it wasn't shown here, but it's two layers will be in between the two.",
            "Yeah, it it.",
            "Usually it it it.",
            "It does improve up to some point where maybe you you get into optimization difficulties.",
            "Would I don't remember these experiments if we went beyond three?",
            "But basically I think we wanted to compare also with.",
            "Yeah no, I think not.",
            "I just want to mention some some advantages of denoising autoencoders over our BMS for pretraining stacking.",
            "Except in addition to the fact that they work just as well or sometimes better.",
            "No partition function to deal with in autoencoders.",
            "So you can actually measure the actual training criterion that you're optimizing.",
            "The true kind of training criterion.",
            "You can measure it quite quite accurately in our brains is a little bit of a problem.",
            "You don't know how well you doing on the training criterion necessarily.",
            "The It's a very general framework in the sense that the encoder decoder in and out all time quarter.",
            "You can use any permit parameterized function, so some persons mentioned the idea of maybe using values.",
            "You know it's very natural to use, but you couldn't also imagine using autoencoders with more than one hidden layer.",
            "OK, you could imagine lots of things.",
            "So it's very flexible, maybe two flexible.",
            "And I want person."
        ],
        [
            "OK. OK, so.",
            "Right, so somebody asked, somebody asked why we do that at the level of the input, the corruption rather than at the level of the representation.",
            "Well, you could do that.",
            "Another presentation that's reasonable, right?",
            "The denoising autoencoders basically encourages the reconstruction to be insensitive to input corruptions, but alternative an alternative would be to encourage the representation to be insensitive to input corruptions.",
            "That's another way.",
            "You could also do encourage the that's where you were hinting at the reconstruction to be insensitive to hidden layer presentation corruptions, but here I'm going to talk about encouraging your presentation to be insensitive still, so our presentation that we want to be insensitive that falls from that manifold hypothesis.",
            "Again, if you view the encoder as projecting on the manifold, what you want is really for a lot of variations of your input that are.",
            "Outside that, along with the manifold you want them all to map to the same point, so you want your representation to be rather insensitive to changes.",
            "Two small changes that are in your input."
        ],
        [
            "Anne.",
            "So if we do that, encourage representation to be insensitive, that gives us the.",
            "The formula would be like this, we would.",
            "Optimize reconstruction error and we could add a stochastic realization.",
            "We take the representation of computer presentation for X and we compute their presentation were corrupted X.",
            "And we look at how different they are and we make sure that they're not too different by penalising the squared difference between them.",
            "OK makes sense.",
            "So and if we have tideways, so that's important.",
            "In this setup, it wasn't for pure denoising auto encoder, but for this it is.",
            "It will prevent W from collapsing the encoder to 0.",
            "OK, so that's the stochastic regularization.",
            "Again, we're sampling corrupted examples.",
            "I'm telling you I'm moving to that.",
            "I'm going to move away from stochastic regulation starting from that thing.",
            "Now that's."
        ],
        [
            "The S for stochastic CAE.",
            "I'll get to it.",
            "See a means later.",
            "So you have this regularization OK, corrupting X look at representation.",
            "Take Kleenex and make sure the two representations not too far from one another.",
            "That's our regularizer.",
            "If you consider small additive noise like Gaussian noise, and you take a Taylor series expansion of H of X plus epsilon, which will be there XX~ which is X plus epsilon here.",
            "You can get your Taylor series expansion well, it's kind of.",
            "It's it's simple to show that the the.",
            "This term, here the regularization is a proxy.",
            "If you cut the terms in the in the Taylor series expansion, it will be close to Sigma Square, which is your noise here times.",
            "This thing here, which is a derivative.",
            "Let's see concert Taylor exterior.",
            "See expansion.",
            "The derivative of your hidden representation with respect to X, so that's what's called the Jacobian.",
            "OK directive.",
            "Actually, that's that's a matrix.",
            "OK, so derivative of each hidden layer hidden unit with respect to each input unit, so it's a it's a matrix, and we take the Frobenius norm of that matrix.",
            "Alright, so that's really saying here we're saying OK, I want the representation for stochastically corrupted X to be close to the presentation for X.",
            "Here, what I'm saying is really if I move X infinitesimally well, I want age not to move.",
            "OK, this is what they would do.",
            "Is computing right moving your thing a little bit your input?",
            "How much is the H moving?",
            "Well, we're penalizing that, so we're saying we want, how far, how much H moves if we move X infinitesimally wanted to be super super small.",
            "So that's going from here that there was a stochastic penalty to analytic penalty.",
            "So there's no more sampling, normal random sampling, normal."
        ],
        [
            "That's called a contractive autoencoder.",
            "Really, we minimize tradeoff between the reconstruction error and this analytic term which is penalising the Frobenius norm of the Jacobian.",
            "How much H is sensitive to X infinitesimally, and that's a contractive term.",
            "You say it's going to try to make the mapping from X to H contractive?",
            "That's called, that's why I call it CAE for contractive autoencoder and the previous thing I had was Whoops was a stochastic contractive autoencoder.",
            "This is an analytic.",
            "Poop.",
            "This is an analytic stochastic control projective autoencoder.",
            "So for training examples, encounters both small reconstruction error and representation to be insensitive to small variations."
        ],
        [
            "Around the examples."
        ],
        [
            "Alright.",
            "OK so here is my small like computational considerations.",
            "If we define our hidden representation as being simply are defined linear mapping, followed by an elementwise nonlinearity S and suppose S prime is its first derivative, then actually this Jacobian, you can write each.",
            "Row of the Jacobian here is represents a Joe Aro as being simply this formula, so you can see that the ACA penalty is simply.",
            "Taking the squared norm of each row of your.",
            "Weight vector of your weight matrix.",
            "And multiplying by this derivative of your non linearity.",
            "So that's kind of interesting.",
            "'cause when you compare this to weight decay, so the standard traditional weight decay is really this, right?",
            "It's it's penalising the sum of the square of the weights.",
            "And here you penalizing you also looking at the sum of the square norm of the weights for each row of your weight matrix.",
            "But you're also taking into account the derivative of your non linearity squared.",
            "OK, so that means what there's two ways.",
            "So in squared in weight decay there's just one way to satisfy the penalty is to keep W small.",
            "OK, here there's two ways to satisfy the penalty.",
            "Can you can someone tell me what the two ways are?",
            "You can.",
            "Decide.",
            "OK, so you can keep the weight small or you can make the divertive small and the digital small means saturating your sigmoid.",
            "If it's a single.",
            "OK, so it's happy it can it get its happy to have large wait as long as a set Sigma is saturated.",
            "So it's going to tend to move towards sparse activations.",
            "And the other thing that's interesting is that these two things are really the same complexity, so it's no more.",
            "It looks.",
            "I mean, if you look at it.",
            "If you look at this you say wow, that's going to be complicated.",
            "That's going to be costly to compute, right?",
            "It's it's?",
            "It's for being his normal the Jacobian matrix, but it's not.",
            "It's not, it's really the same complexity as a normal weight decay.",
            "Alright well well yeah.",
            "Well you ready I mean this these two cases for value there's one where it's one and there's one way zero.",
            "OK but if it's zero it will be happy as well so.",
            "So it will be happy if it has more than being 0.",
            "Yeah, so when they get there, how it gets a great into that.",
            "That's a good question though, but I mean how it gets, how it gets to get raised to 0 otherwise.",
            "So good point, there contracted autoencoders with values.",
            "I don't know if we should try that.",
            "I said this research was with before values were became so popular.",
            "And before dropout became so popular that it had a role to play for dropout becoming popular, 'cause there was a precursor to drop out.",
            "Alright."
        ],
        [
            "I'm OK, just to mention there was an extension I won't spend much time on this.",
            "It's a higher order contractive outline quarter.",
            "Here we penalize Jacobian, but we could say we could try to penalize the higher order derivatives right?",
            "This came just from it from a Taylor series expansion so we could we could penalize also.",
            "The the the High order derivative.",
            "So the second derivative the problem is the first area to hear the Jacobian matrix.",
            "OK, the second derivative is that three tensor that is getting a little bit too crazy and unlikely to deal with.",
            "So what we did instead was we still use this penalty here analytically for the Jacobian, for the for the.",
            "First derivative and for the second derivative, well, we use a stochastic approximation and the stochastic saying if you want the second derivative to be to be small, well, you can do is stochastically by saying I want the first derivative at X and I want the first rative at X plus some noise to be close.",
            "OK. That's that's alright."
        ],
        [
            "So this.",
            "Uses both stochastic and analytic regularization simultaneously."
        ],
        [
            "Here is some filters that we we obtain on this work, where this contractive autoencoder C + H. The orders he and some filters we get for that, so they tend to be cleaner here, although it's a little more involved computationally."
        ],
        [
            "OK, now the manifold.",
            "Let's get back to our manifold.",
            "I give a very brief manifold interpretation of the denoising autoencoder, so now I'm going to try to give a manifold interpretation of the contract if autoencoders.",
            "Again, we're trying to think of what's happened, what might happen if our true data distribution is follows as a low dimensional manifold, right so?",
            "Let's look at the two terms of the contract.",
            "If outside corner, you have a reconstruction term and that's being traded off here.",
            "With this hyperparameter Lambda for a contraction term, OK, so.",
            "The reconstruction."
        ],
        [
            "OK, so let's take some point some some of our training point from our training data.",
            "And look at what's happening if."
        ],
        [
            "If we look at the pressures on that representation being being changed, sensitive to directions when when we move if we do infinitesimal moves at this point in this direction, OK?",
            "Well."
        ],
        [
            "And in this direction, so we're looking for considering the direction that's parallel to the manifold and one that's orthogonal to my phone.",
            "So what's happening is?",
            "If.",
            "OK, get back here.",
            "What sites you have to do.",
            "Thank you, and often I think for this animation he did that slide, it worked.",
            "If we look at this contract if penalty, this contractive penalty is going to try to contract the sensitivity in all directions.",
            "It's isotropic, right?",
            "It's isotropic, so it's going to contract the sensitivity in all directions, so it's going to try to make that guy very small and it will succeed.",
            "That's why we're taking it away."
        ],
        [
            "But it will also try to do it for that guy.",
            "OK so it will try to make it to contract contract contract contract.",
            "But the."
        ],
        [
            "Problem is it will.",
            "If it does, it means that the system won't be able to distinguish between those two points, right?",
            "So it will go against the reconstruction costs, the contracted pressure, which is isotropic, gets countered by the reconstruction pressure, and it gets counted only in the directions that are along the manifold.",
            "Alright."
        ],
        [
            "So we can go a little further with this manifold analysis.",
            "We can look at our Jacobian that's measures how sensitive our representation is to all input direction changes.",
            "Once it's learned yes.",
            "Yeah.",
            "Alright, right so OK.",
            "Yes, so that's very important and there's something that I didn't mention here.",
            "There was a warning slide.",
            "It was it in that one.",
            "Oh yeah, yeah, I had it in there.",
            "I remember I saw this recently.",
            "It's in the it's a little area alright, OK, in red.",
            "Thing in red warning may require tide waits.",
            "That's to address what you're what you're which is posing as a problem.",
            "So what is what is it's actually?",
            "It's something that I never quite liked about the contractive autoencoder to be honest, is that it requires tide waits to work contrary to the denoising autoencoder which you can do sideways on tideways encoder decoder.",
            "Completely different?",
            "No problem.",
            "OK, but there for the contractive autoencoder to make sense it requires tide waits because otherwise if the weights are not tide, what it can do?",
            "It can contract that 202.",
            "Just by taking a very, very, very small encoder, weights coefficient, global coefficient, and it can blow the thing up again upon reconstruction in the decoder, so it can satisfy both criteria perfectly.",
            "So did this.",
            "There needs to be a way to kind of constrain the weights, and this is done kind of indirectly if you have tide waits because you can't play the game of having encoder ways to be super small and decoder way to be super large.",
            "Yeah, right.",
            "You could use weight constraints, yeah?",
            "But maybe just if you if you.",
            "If you do, we weight decay on the reconstruction weights?",
            "Yeah, probably because if you prevent the reconstruction weight from growing too much, that will address that.",
            "Yeah.",
            "Oh my time is almost up.",
            "My time is up.",
            "OK, I know, is I?",
            "Yes, you have to go through this jumping of arrows for another half hour.",
            "OK, so just this very quick side because I think it can you give me like 2 minutes.",
            "Alright.",
            "If we look at the Jacobian which encompasses the sensitivity of our encoding of an encoder of encoding with respect to variations in input directions, we can try to analyze it and no good way to analyze."
        ],
        [
            "This is to do a a singular value decomposition of it and then our singular value spectrum will tell us.",
            "Which directions it is most sensitive to on which it is not right?",
            "So that's nice analysis we can make, so we didn't make that analysis an.",
            "Actually if we look at between autoencoder, regular autoencoders, contractive autoencoder, these are the singular value spectrums.",
            "There averaged all in an average over over the data set.",
            "But you see that differ.",
            "The regular autoencoder singular values are kind of as many that are high, right?",
            "But other contracted autoencoder.",
            "It decreases quite rapidly, So what you'd expect if you would really had a a manifold thing if it was really projection on the money flow.",
            "If you think of this of doing this with the PCA, what you get for PCA, you get K values that are non zero and all the others that are zero because you have a K dimensional manifold in effect that you're projecting onto.",
            "So here we see this is this is local at every point, so the direction is different at every point.",
            "It's like a local PC but all parameterized right and not based on neighborhood.",
            "So we see that the.",
            "This is consistent with the manifold hypothesis in the sense that.",
            "It learned to have few very few directions to which which it is sensitive, and these directions will vary from point to point, but there still be an average, always a few small number of directions."
        ],
        [
            "Just to compare local PCA where we take the neighbors, remember."
        ],
        [
            "Member neighbors right.",
            "OK, so we take the neighbors an do look at the principle directions, so do the same trick there.",
            "Do the PCA so we find the principle directions around that image.",
            "That's what the principle directions we get, so it's kinda blurry noisy if we do it on a trained contractive auto encoders.",
            "So that's what the contract, if that's the direction that the contractive autoencoder learn to be sensitive to.",
            "Well, it's much more sensible, right?",
            "And the main reason, I believe is it's we're not basing it on explicit neighbors or pairs of points, not basing it on these neighborhoods that are very.",
            "Very noisy.",
            "OK."
        ],
        [
            "How to exploit that?",
            "So I think I'll stop there.",
            "How to exploit the learn tangents.",
            "There's several ways you can exploit them.",
            "One there's 22 old algorithms that were proposed by simar that can exploit the tangents to define a distance measure.",
            "If we use a basic Euclidean distance measure as we said, it's your neighbor may be very not like you.",
            "But if we have first captured this notion of well, you're on the manifold, then instead of taking the Euclidean.",
            "Distance between two points OK.",
            "So here's the example.",
            "If you took the Euclidian distance between E&P, OK, well, it will be.",
            "Anne.",
            "Alright, that's actually the intention distance.",
            "So instead of taking the Euclidean distance, we kind of take the distance along the manifold tangents.",
            "So.",
            "So that's tangent distance and then you can use.",
            "You have to find a new distance.",
            "You can use it like in key nearest neighbors.",
            "We did some experiments, it did improve pretty much unconscious neighbors.",
            "You can use the same kind of intuition and something that's called tangent propagation, which is A twist on."
        ],
        [
            "The standard back propagation to take into account these notions of some directions that you want to be sensitive or insensitive to when fine tuning a deep network an at the time in 2011 that this gave us the state of the art result on permutation invariant M missed by.",
            "So we trained a contractive autoencoders, extracted the tangents by using this Jacobian SDDS.",
            "And use that with tangent propagation for the fine tuning.",
            "Another thing that we can do is move along tangents to allow nice nice sampling, and I guess Joshua's talk will talk a lot about that, so I think that's a good point.",
            "Good place to end.",
            "Even though I had another 10 slide after that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you know everything about the.",
                    "label": 0
                },
                {
                    "sent": "Multilayer perceptron, neural networks, gradient descent, Bozeman machines.",
                    "label": 0
                },
                {
                    "sent": "Deep belief networks you know that so we can move.",
                    "label": 0
                },
                {
                    "sent": "We can move on.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My plan will have two parts of things that seemingly totally unrelated, but we'll see there.",
                    "label": 0
                },
                {
                    "sent": "The first is I'll talk about manifolds briefly and how to what we call the manifold hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Or rather, assumption and how?",
                    "label": 0
                },
                {
                    "sent": "Why this is important?",
                    "label": 0
                },
                {
                    "sent": "How we can leverage it?",
                    "label": 0
                },
                {
                    "sent": "And then the main part of my talk will be about autoencoders, which are specific type of of multilayer perceptron neural networks.",
                    "label": 0
                },
                {
                    "sent": "And how to regularize them so all this country into my first lecture where I focused mostly on prediction that is supervised learning, predicting one variable random variable from a set of others.",
                    "label": 0
                },
                {
                    "sent": "Here my talk will be mostly about unsupervised learning and unsupervised learning will focus on mostly two things.",
                    "label": 1
                },
                {
                    "sent": "One is basically what is a lot of the of the point of deep learning which is.",
                    "label": 0
                },
                {
                    "sent": "Learning representations an A little bit also about density estimation, which is kind of the mother of all.",
                    "label": 0
                },
                {
                    "sent": "Say learning algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so to begin with, I start with a traditional unsupervised learning task, which is simply a dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "The principle is very simple.",
                    "label": 0
                },
                {
                    "sent": "You start from some X that's in some high dimension, D. At some H. It's in some you want to map it to some agents, some lower dimension.",
                    "label": 0
                },
                {
                    "sent": "MY here called M. Later I called DH.",
                    "label": 0
                },
                {
                    "sent": "That's smaller than the OK, and you want to learn a function F Theta that does that, and ideally want to do that.",
                    "label": 0
                },
                {
                    "sent": "In such a way that you preserve all the whatever is useful, relevant, important information in your initial input, but it's in in fewer number of dimensions.",
                    "label": 1
                },
                {
                    "sent": "It's a classical problem, So what can this be useful for?",
                    "label": 0
                },
                {
                    "sent": "Tell me ideas?",
                    "label": 0
                },
                {
                    "sent": "For visualization, yeah, that's 11 aspect.",
                    "label": 0
                },
                {
                    "sent": "Feature extraction if you want to extract the most relevant features, maybe to help a subsequent classifier to focus on those.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess what would you just said for future instruction?",
                    "label": 0
                },
                {
                    "sent": "It may help for generalization in some settings.",
                    "label": 0
                },
                {
                    "sent": "Compressi",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then yeah, well, that's another point right here.",
                    "label": 0
                },
                {
                    "sent": "We are presenting something with that used a lot of numbers with a small nephew number.",
                    "label": 0
                },
                {
                    "sent": "So these are basically all mentioned data compression data set visualization.",
                    "label": 1
                },
                {
                    "sent": "So visualization is we can't really visualize our data set in hundred dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, 100 dimensional data set the point cloud, but a 2 dimensional or three dimensional point cloud.",
                    "label": 0
                },
                {
                    "sent": "We can more easily visualize.",
                    "label": 1
                },
                {
                    "sent": "And discarding most important features, that's the feature extraction part.",
                    "label": 0
                },
                {
                    "sent": "OK, can you tell me what's the most classical or first algorithm that comes to your mind for dimensionality reduction?",
                    "label": 1
                },
                {
                    "sent": "PCA OK?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so that's a classic.",
                    "label": 1
                },
                {
                    "sent": "Not to say old algorithm.",
                    "label": 0
                },
                {
                    "sent": "It is at least two back to 1901.",
                    "label": 0
                },
                {
                    "sent": "And the principle is you have your point cloud, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's an unsupervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you disregard any any class of labels.",
                    "label": 0
                },
                {
                    "sent": "Further for those points and your point cloud.",
                    "label": 0
                },
                {
                    "sent": "The PCA algorithm is going to find.",
                    "label": 0
                },
                {
                    "sent": "Learn really the K, where K you specify K the K directions in that space so that BK set of K vectors that we call principle directions.",
                    "label": 0
                },
                {
                    "sent": "In which the data has highest variance.",
                    "label": 1
                },
                {
                    "sent": "So if you project your data points on those vectors, that's where they will have the highest variance.",
                    "label": 0
                },
                {
                    "sent": "So in effect it learns AK dimensional subspace of your initial space.",
                    "label": 0
                },
                {
                    "sent": "So initially you were in SpaceX.",
                    "label": 0
                },
                {
                    "sent": "Now you're in the space of the WS here OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so it learns the W's.",
                    "label": 0
                },
                {
                    "sent": "Which are they?",
                    "label": 0
                },
                {
                    "sent": "Principle directions or eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Put them in a W matrix OK and that will be the parameters of our function and then with.",
                    "label": 0
                },
                {
                    "sent": "Thanks to these parameters we can project the inputs on these vectors and yield are reduced dimension representation with the benefit that our features that we get here will be decorrelated.",
                    "label": 1
                },
                {
                    "sent": "OK, that's one of the interesting aspect of it.",
                    "label": 0
                },
                {
                    "sent": "Not independent unless your data was Gaussian initially, but decorrelated.",
                    "label": 0
                },
                {
                    "sent": "And this representation is what we call the principle components.",
                    "label": 0
                },
                {
                    "sent": "So so to link with our terminology.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but without notation we have some X as input, some F Theta here, which is a function that's going to map it to some H. OK Ann, the way it's done is just linear.",
                    "label": 0
                },
                {
                    "sent": "It's W X + B or actually in PSU for WX minus mu, where mu is the is the the the average of the data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so our parameters here are WMU.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's very classical.",
                    "label": 0
                },
                {
                    "sent": "We probably all know about this, so why?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did I mention PCA?",
                    "label": 0
                },
                {
                    "sent": "Oh OK, why Dempsey?",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "I remember the goal of this talk right at 2.7.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you so I have one manifold learning.",
                    "label": 0
                },
                {
                    "sent": "So for me, PCA is a prototypical manifold modeling algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK, I'll get back to that.",
                    "label": 0
                },
                {
                    "sent": "It's also the prototypical unsupervised representation learning algorithm and it's related to auto encoders.",
                    "label": 1
                },
                {
                    "sent": "Will get back to that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to manifold modeling, so PCA is really a case where you're modeling a a linear manifold, right?",
                    "label": 0
                },
                {
                    "sent": "So linear algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you kind of suppose that your data will be well represented if you pass a a hyperplane through them.",
                    "label": 0
                },
                {
                    "sent": "All sorts and this is really the prototype of a linear manifold.",
                    "label": 0
                },
                {
                    "sent": "A subspace is a linear manifold.",
                    "label": 1
                },
                {
                    "sent": "The principle components, then that you obtained by projecting on them.",
                    "label": 0
                },
                {
                    "sent": "They can be seen as a location of your points of your data points.",
                    "label": 0
                },
                {
                    "sent": "The corners of your point on the manifold in a coordinate system.",
                    "label": 1
                },
                {
                    "sent": "All the manifold OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the linear manifold part.",
                    "label": 0
                },
                {
                    "sent": "Now of course we are our data or complex high dimensional data is not does not live in such a simple manifold that we do simple.",
                    "label": 0
                },
                {
                    "sent": "So now what will be really interested is much more complex nonlinear manifold.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of a non linear 2D manifold in 3D input space.",
                    "label": 1
                },
                {
                    "sent": "So, but these are two examples of lower dimensional manifolds embedded in a high dimensional space OK?",
                    "label": 1
                },
                {
                    "sent": "That's what we're interested in here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Why do we talk about Manifolding in machine learning?",
                    "label": 0
                },
                {
                    "sent": "Was there this interest?",
                    "label": 0
                },
                {
                    "sent": "Well, one of the reasons is, well, manifolds are cool, but.",
                    "label": 0
                },
                {
                    "sent": "Another reason is is this so called manifold hypothesis or that some call manifold assumptions.",
                    "label": 0
                },
                {
                    "sent": "It's really kind of something of both.",
                    "label": 0
                },
                {
                    "sent": "It's more an assumption.",
                    "label": 0
                },
                {
                    "sent": "And it could be summarized in this way phrasing this way that natural data, the kind of data that we collect that we obtained, that we work with in high dimensional spaces, is likely to concentrate close to lower dimensional manifolds in that space.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't mean that all the data points will strictly be on your manifold, it just means that you have high density on the manifold and then they probably density will is expected to decrease rapidly when moving away from that manifold while staying relatively high while staying on the manifold.",
                    "label": 0
                },
                {
                    "sent": "And the point is so lower dimensional manifold in your input space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's a bit of a hand WAVY kind of hypothesis or assumption.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to give you some sense of where it comes from, where what we could, where people fall over making these assumptions.",
                    "label": 0
                },
                {
                    "sent": "One is the problem of the curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So the curse of dimensionality, if you were to ask to ask.",
                    "label": 0
                },
                {
                    "sent": "Classical statisticians, what you could possibly hope to do in a.",
                    "label": 0
                },
                {
                    "sent": "10,000 a 100,000 dimensional space with a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "They would say it's hopeless because of the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "OK, but our techniques for deep learning and all they do, we are computer vision.",
                    "label": 0
                },
                {
                    "sent": "We're able to deal with that kind of data, so that's because there's a lot of structure in that data.",
                    "label": 0
                },
                {
                    "sent": "But just to to give you an idea of how big that space is, if you think take the space of 200 by 200 RGB images.",
                    "label": 1
                },
                {
                    "sent": "OK, 8 bit RGB images.",
                    "label": 0
                },
                {
                    "sent": "When you look at the number of possible vectors representing those, there's actually 10 to the 96,329 possible such images.",
                    "label": 0
                },
                {
                    "sent": "OK, it's enormous.",
                    "label": 0
                },
                {
                    "sent": "Is gigantic, so if you want to sample uniformly in that space, what you think you'd get?",
                    "label": 0
                },
                {
                    "sent": "No, just simple.",
                    "label": 0
                },
                {
                    "sent": "Where you get your face.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I get something like that.",
                    "label": 0
                },
                {
                    "sent": "Overwhelmingly so, so that's that's that's the Occupy this kind of thing is what occupies the majority of 200 by 200 pixel space images.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, but what we're interested in is or what we we face are what we deal with that kind of data that we expose are going to that.",
                    "label": 0
                },
                {
                    "sent": "Do we want to to model is more natural images or hear an example of faces?",
                    "label": 0
                },
                {
                    "sent": "So natural images reasonably, if you look at these two things you could, we could expect them to occupy a tiny fraction of that huge space A tiny fraction.",
                    "label": 1
                },
                {
                    "sent": "So that suggests a very peak density.",
                    "label": 0
                },
                {
                    "sent": "The other point is that if you look at faces here, OK, you've all seen these are morphing face morphing algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "So even if you take yourself aging etc.",
                    "label": 0
                },
                {
                    "sent": "Actually you can think of a path that's moving from one face to the other while all the time maintaining a realistic looking face.",
                    "label": 0
                },
                {
                    "sent": "Right, it's possible, so there's a realistic smooth transformation from one image to another that suggested continuous path along a high manifold of high density along the manifold where all the images will be highly highly likely.",
                    "label": 1
                },
                {
                    "sent": "So the manifold hypothesis according to which data density concentrates near lower dimensional manifold which suggested by these observation, can this big hope there because it can shift the curse of dimensionality basically from the high dimensional input space.",
                    "label": 1
                },
                {
                    "sent": "This large D to the dimension of the manifold, all the low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "So there's hope.",
                    "label": 0
                },
                {
                    "sent": "OK, classic listed decisions.",
                    "label": 0
                },
                {
                    "sent": "Don't despair, there is hope even with their curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Without making necessary to strong hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So another argument for the for this manifold business.",
                    "label": 0
                },
                {
                    "sent": "Is that the manifold for follow?",
                    "label": 0
                },
                {
                    "sent": "Naturally from this notion of continuous underlying factors?",
                    "label": 1
                },
                {
                    "sent": "There's this idea that whatever we get as input may be high dimensional, but it may have a smaller number of explanatory factors or underlying factors that are responsible for this.",
                    "label": 0
                },
                {
                    "sent": "So let's take a restricted setting against against faces.",
                    "label": 0
                },
                {
                    "sent": "Yeah I bought this picture from the they didn't vision that website.",
                    "label": 0
                },
                {
                    "sent": "If you think of a face or even that any any rigid object, let's take rigid object would be even even simpler than you think at all the different poses 3D poses of that object.",
                    "label": 0
                },
                {
                    "sent": "And a couple of variations of illuminations well.",
                    "label": 0
                },
                {
                    "sent": "That the image of that object may be pretty high dimensional, you know.",
                    "label": 0
                },
                {
                    "sent": "1000 dimensions or whatever, but the underlying factors mean you may have 1000 dimensions in what you observe, but if all you have is variations of poser over thing, well, there's a maybe a couple of 10s of underlying real true factors, right?",
                    "label": 0
                },
                {
                    "sent": "So this these ten factors?",
                    "label": 0
                },
                {
                    "sent": "Actually completely did will be completely determine this very restricted setting setup will completely determine the appearance.",
                    "label": 0
                },
                {
                    "sent": "What you what you get with your thousand thousand observation variables.",
                    "label": 0
                },
                {
                    "sent": "So these notion of continuous underlying factors basically.",
                    "label": 1
                },
                {
                    "sent": "Mean that will have a kind of manifold structure and then of course part of meaningful representation.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a base or an object, one of the characteristics that you may want to to find from it.",
                    "label": 0
                },
                {
                    "sent": "Ideally in an unsupervised way is exposed parameters an illumination where the light comes from.",
                    "label": 0
                },
                {
                    "sent": "Things like that OK?",
                    "label": 0
                },
                {
                    "sent": "Here's another example that's that's taken from digits.",
                    "label": 1
                },
                {
                    "sent": "If you think again about bitmap images of digits.",
                    "label": 0
                },
                {
                    "sent": "And say this point here the X is actually represents a is a vector for presenting the digit four and then you do some tiny tiny tiny changes to that digit like tiny rotations or tiny scaling or that digit.",
                    "label": 0
                },
                {
                    "sent": "OK, well if it's very very tiny.",
                    "label": 0
                },
                {
                    "sent": "Even if you have a I know 50 by 50 image, well there is going to be continuous path going from one to the other as you rotate in that high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So there's a continuous path and if you.",
                    "label": 0
                },
                {
                    "sent": "If you take for example the rotations and scalings etc, you have a couple of such underlying continuous factors which will basically give you the dimension idea of the manifold of force.",
                    "label": 0
                },
                {
                    "sent": "At least close to that point, OK?",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions so far don't hesitate to interrupt yes.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So this is this falls naturally from from from continuous factor, so the binary cases more switching from 1 foot to the other item.",
                    "label": 0
                },
                {
                    "sent": "But it it it.",
                    "label": 0
                },
                {
                    "sent": "It may depend on how you, how you, how you can see them right?",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It's an open question.",
                    "label": 0
                },
                {
                    "sent": "Spaces.",
                    "label": 0
                },
                {
                    "sent": "Small change yeah.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "There will be continuous variable.",
                    "label": 0
                },
                {
                    "sent": "The other way.",
                    "label": 0
                },
                {
                    "sent": "I like to think of extending that to the discrete case is if you think in terms of transformations.",
                    "label": 0
                },
                {
                    "sent": "OK if you're at this point X here and you apply a tiny rotations.",
                    "label": 0
                },
                {
                    "sent": "That's what we just said to stay on the manifold.",
                    "label": 0
                },
                {
                    "sent": "This is because you're in a continuous space.",
                    "label": 0
                },
                {
                    "sent": "You can you can apply as tiny rotations as you may possibly want to think, so it's it can be infinitesimal in a in a discrete space.",
                    "label": 0
                },
                {
                    "sent": "Suppose we exposing wearing a discrete space.",
                    "label": 0
                },
                {
                    "sent": "You could still have this notion.",
                    "label": 0
                },
                {
                    "sent": "You can still have this notion that there's an operator that.",
                    "label": 0
                },
                {
                    "sent": "Will move you not too far from where you start with an that still gives you is put.",
                    "label": 0
                },
                {
                    "sent": "Your state has you stay in a high density region, gives you a high probability thing so you can think of the discrete rotation operator.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just I mean, let's think about that.",
                    "label": 0
                },
                {
                    "sent": "Could be discrete.",
                    "label": 0
                },
                {
                    "sent": "Imagine space.",
                    "label": 0
                },
                {
                    "sent": "We just just be 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "OK, you could.",
                    "label": 0
                },
                {
                    "sent": "You could have it all discrete, but it will still be a valid operator that generalizes this notion of OK.",
                    "label": 0
                },
                {
                    "sent": "I can apply that operator an I I'm staying.",
                    "label": 0
                },
                {
                    "sent": "I'm still having high probability observations.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's so yes, yes yes it does so so so the.",
                    "label": 1
                },
                {
                    "sent": "The notion of a of a.",
                    "label": 0
                },
                {
                    "sent": "Of a manifold here shouldn't be taken literally as as meaning we need to have a super formal, super formal, mathematically defined manifold and align this.",
                    "label": 0
                },
                {
                    "sent": "Then maybe this continue it is, it may change.",
                    "label": 0
                },
                {
                    "sent": "You may have critical points or things like that.",
                    "label": 0
                },
                {
                    "sent": "It may change dimension, you may have one dimensional manifold that crosses 150 dimension manifold and things like that it's expected to be a very complicated space, it's just.",
                    "label": 0
                },
                {
                    "sent": "It's just this notion of, as Joshua pointed out, of density, concentration, concentration along something that can be characterized with fewer dimensions, and in this example that are continuous.",
                    "label": 0
                },
                {
                    "sent": "But I can't.",
                    "label": 0
                },
                {
                    "sent": "Also, there's a notion of continuity.",
                    "label": 0
                },
                {
                    "sent": "You can have low entropy distributions that are that are super picky, but then where you have a, you have couple of peaks OK and no path between them.",
                    "label": 0
                },
                {
                    "sent": "So that's not the idea of a manifold.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you have many points where you have you have a multi dimensional path among points that follows this high density areas.",
                    "label": 0
                },
                {
                    "sent": "But these multiple dimensions are lower than the input input dimension.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I think should move on when you keep a manifold question to the end.",
                    "label": 0
                },
                {
                    "sent": "Otherwise this is like 1, one 110th of my presentation so far.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, just just a little remark that manifolds can be represented by patchwork of tangent spaces.",
                    "label": 0
                },
                {
                    "sent": "This notion that every point if you look infinitesimally, you can look at what directions you can move basically, and that gives you a tangent space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so to anchor these ideas a little more I dig.",
                    "label": 0
                },
                {
                    "sent": "Dig back some older work of mine in annual.",
                    "label": 0
                },
                {
                    "sent": "Sure, in the realm of nonparametric density estimations.",
                    "label": 1
                },
                {
                    "sent": "Remember I told you, don't know.",
                    "label": 0
                },
                {
                    "sent": "We have the highest number of farmers so.",
                    "label": 0
                },
                {
                    "sent": "This is classical person windows OK so.",
                    "label": 0
                },
                {
                    "sent": "You've had the lecture ready on the director graphical models.",
                    "label": 0
                },
                {
                    "sent": "Now is that to some extent, so you know, mixture of Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "You know it makes we've gotten so classical person.",
                    "label": 0
                },
                {
                    "sent": "Windows is just a giant, gantic mixture of Gaussians where you put one Gaussian on top of each of your training points.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't learn the centers, you just put it on top of them.",
                    "label": 0
                },
                {
                    "sent": "So that's a personal window with a with a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "But you could use other kinds of kernels and the.",
                    "label": 0
                },
                {
                    "sent": "Parameters of the kernel itself, where their high performance that you may tune that kind of smoothing your empirical distribution so the point is you have your ex is here your XI here, which are your training examples and you're putting a Gaussian on centered on each other's eyes.",
                    "label": 0
                },
                {
                    "sent": "Am I get back to the covariance matrix in a minute and just averaging them and that gives you your density?",
                    "label": 0
                },
                {
                    "sent": "So this is a 1D example where you had a training example.",
                    "label": 0
                },
                {
                    "sent": "You put the gas on top of them, you add them or average them altogether, and that gives you your estimated density.",
                    "label": 0
                },
                {
                    "sent": "Here's the 2D equivalent.",
                    "label": 0
                },
                {
                    "sent": "So here I drew points notice in a kind of a manifold fashion, right?",
                    "label": 0
                },
                {
                    "sent": "This is there like along a 1D manifold embedded in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "That's a toy example of it.",
                    "label": 0
                },
                {
                    "sent": "So if we're to use classical parts in Windows, we put a little Gaussian bump on each of these points.",
                    "label": 0
                },
                {
                    "sent": "So the problem with this is if the manifold hypothesis is true.",
                    "label": 0
                },
                {
                    "sent": "That that means here you're not using in any sense this notion that that you have a low dimensional manifold, because each of these Gaussian and Gaussian mixture will actually allocate probability mass away from the manifold as much as it does in the direction of the manifold.",
                    "label": 1
                },
                {
                    "sent": "So there's this little idea of a fix, which is which we call manifold parts in Windows.",
                    "label": 1
                },
                {
                    "sent": "And that's simply to put a kind of oriented Gaussian on top of each point that's oriented.",
                    "label": 0
                },
                {
                    "sent": "It's like an oriented Gaussian pancake that's almost flat.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal to the manifold and elongated along the manifold.",
                    "label": 1
                },
                {
                    "sent": "So how can we find those directions?",
                    "label": 0
                },
                {
                    "sent": "I mean it's simple you Ristic.",
                    "label": 0
                },
                {
                    "sent": "We can do a little local PCA, so you take a point.",
                    "label": 0
                },
                {
                    "sent": "This point.",
                    "label": 0
                },
                {
                    "sent": "You look at its closest neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, you do a little PCA on it and you know in which direction the closest neighbors are mostly aligned.",
                    "label": 0
                },
                {
                    "sent": "That's how you get these covariance matrices here.",
                    "label": 0
                },
                {
                    "sent": "So actually we use a low rank position.",
                    "label": 1
                },
                {
                    "sent": "All those covariance matrices that use only the nearest local PCA directions obtained from the nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "This parametric cousins to this, like the mixture of Gaussian pancakes.",
                    "label": 1
                },
                {
                    "sent": "This is nonparametric use.",
                    "label": 0
                },
                {
                    "sent": "It's really putting one on top of each.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have these isotopic pars and which is the classical one.",
                    "label": 0
                },
                {
                    "sent": "We have the manifold parzen where we learn occurrence matrix.",
                    "label": 1
                },
                {
                    "sent": "That's an oriented pancake derived from the PCA on the K nearest neighbors local directions.",
                    "label": 0
                },
                {
                    "sent": "And a cool extension that worked with it with Joshua and you go.",
                    "label": 0
                },
                {
                    "sent": "That is actually not now learning using a neural network to predict these covariance directions.",
                    "label": 0
                },
                {
                    "sent": "So we now have predictive function with this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so just to show a quick little example of what with, this may look like we have we had here sign a sinusoidal wave and hear the an in famous Swiss roll.",
                    "label": 0
                },
                {
                    "sent": "This is what you get with a mixture of a couple of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the optimal thing that the argument was able to find in terms of density.",
                    "label": 0
                },
                {
                    "sent": "That's where the parzen window, or here is the optimal you look very fat, very fat Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Here's a manifold parts, and so it's much more sharper distribution, and here's a nonlocal manifold which is both sharper and smoother.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and it does improve on.",
                    "label": 0
                },
                {
                    "sent": "Small data set.",
                    "label": 0
                },
                {
                    "sent": "USPS at the time, so it shows that this manifold hypothesis actually helps, because if we use those things it did help improve the classification performance even in those high dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So manifold learning is a rich subfield.",
                    "label": 1
                },
                {
                    "sent": "There's basically purely nonparametric approaches, the manifold parzen and various methods used for mostly for visualization, like local linear embedding, ISOMAP T sne that you probably heard of.",
                    "label": 1
                },
                {
                    "sent": "And there are some learned parameterized function that are often extensions of these, like there's a parametric version of T Sne, where again, you learn a parameterized function to predict the locations in the on the manifold.",
                    "label": 1
                },
                {
                    "sent": "So what do all these approaches have in common here in India?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their neighborhood based exactly so they explicitly use distance based neighborhoods, their training with a K nearest neighbors, or pairs of points or neighborhood graphs or whatever, and these are typically based on Euclidean neighbors.",
                    "label": 0
                },
                {
                    "sent": "But in high dimension, if you're in high dimension, your nearest Euclidean neighbor.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be very different from you.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Serve.",
                    "label": 0
                },
                {
                    "sent": "That's why K nearest neighbor doesn't work all that well on our complex kind of data, right?",
                    "label": 0
                },
                {
                    "sent": "So these neighborhoods are kind of reliable.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'll leave the manifolds for awhile and get back to it later.",
                    "label": 0
                },
                {
                    "sent": "You see from a strange angle, an move to auto encoders and their organization.",
                    "label": 1
                },
                {
                    "sent": "So auto encoders they were going to build on more of what we what you have seen in the past.",
                    "label": 0
                },
                {
                    "sent": "Today's.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's my actually one slide on multilayer perception.",
                    "label": 0
                },
                {
                    "sent": "Well I have another.",
                    "label": 0
                },
                {
                    "sent": "I have two slides but it's just this one.",
                    "label": 0
                },
                {
                    "sent": "The multilayer perceptions here I'm showing a single layer perceptron, single hidden layer perceptron.",
                    "label": 1
                },
                {
                    "sent": "You have your input layer, your hidden layer H, and an A single output neuron.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're using sigmoid words.",
                    "label": 1
                },
                {
                    "sent": "Here's 10 H, but I'm actually going to use sigmoid, SIG model hidden hidden units so the output F theater of X is actually the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Yet to see model output.",
                    "label": 0
                },
                {
                    "sent": "Off the dot product between the weight vector of that output unit W small W&H, which is the vector of the representation that you get here.",
                    "label": 0
                },
                {
                    "sent": "Plus some scalar bias OK, and this vector H is itself a sigmoid off, so non linearity of linear transformation of X that we can write as a deep rhymes times D matrix an D primes times 1 hidden bias vector.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are far parmiters here and we are training this by gradient descent.",
                    "label": 0
                },
                {
                    "sent": "To optimize the.",
                    "label": 1
                },
                {
                    "sent": "Here is the empirical risk, it's a regularised empirical risk.",
                    "label": 0
                },
                {
                    "sent": "More the empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Principle, so that's basically just taking the loss.",
                    "label": 1
                },
                {
                    "sent": "The sum of the losses could be.",
                    "label": 0
                },
                {
                    "sent": "The averages didn't change anything.",
                    "label": 0
                },
                {
                    "sent": "All of the examples of the training set here T is a target.",
                    "label": 0
                },
                {
                    "sent": "Of the true thing you want to predict an your what your function is predicting.",
                    "label": 1
                },
                {
                    "sent": "Plus some organization term that's inducing a preference over your parameters that may or may not be there.",
                    "label": 0
                },
                {
                    "sent": "So I'm writing this objective that we're optimizing here J MLP.",
                    "label": 0
                },
                {
                    "sent": "I'm going to write our objectives J something KJMOP that's the multilayer perception objective filthy to so let's a supervise case you see because we have a target.",
                    "label": 0
                },
                {
                    "sent": "We have an explicit target T here that's and we have an input X. OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's clear.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Well, autoencoders are simply multilayer perceptrons, just like those simple we've seen.",
                    "label": 1
                },
                {
                    "sent": "But now we're going to use them for unsupervised learning instead of supervised learning.",
                    "label": 1
                },
                {
                    "sent": "So that means we don't have a target.",
                    "label": 0
                },
                {
                    "sent": "We don't have an explicit target, So what are we going to use as a target?",
                    "label": 1
                },
                {
                    "sent": "The input OK is simple.",
                    "label": 0
                },
                {
                    "sent": "We make the output layer the same size as the input layer, so we take as many output prediction neurons as our input prediction input neurons.",
                    "label": 0
                },
                {
                    "sent": "And we put the target equal to the input.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically that's what the network looks like.",
                    "label": 0
                },
                {
                    "sent": "It has some input.",
                    "label": 0
                },
                {
                    "sent": "It still has some hidden and it has some output layer that will now call the reconstruction OK, 'cause that output layer is actually going to try and reconstruct that input.",
                    "label": 0
                },
                {
                    "sent": "And the loss encourages the output reconstruction to be close to the input.",
                    "label": 1
                },
                {
                    "sent": "OK, we're minimizing the discrepancy between X and whatever we got here.",
                    "label": 0
                },
                {
                    "sent": "Is reconstruction by going through the network.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So these were also called dyeable networks.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, are sandglass shaped Nets because they have this bottleneck here in the middle C it's lower dimensional representation in the middle.",
                    "label": 0
                },
                {
                    "sent": "That's the part that's actually interesting, right?",
                    "label": 1
                },
                {
                    "sent": "That's what you get something interesting?",
                    "label": 0
                },
                {
                    "sent": "OK, thank there.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to move things around a little bit.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you the autoencoder, but this time is exactly the same network.",
                    "label": 0
                },
                {
                    "sent": "It's just that I'm going to move the reconstruction here and I'm going to put it besides the input, just to emphasize that there in the same space, actually the same dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And is there really a reconstruction of the input?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what it looks like, but it's otherwise it's really the same.",
                    "label": 0
                },
                {
                    "sent": "So you start with some input X, you map it.",
                    "label": 1
                },
                {
                    "sent": "That's the typical format, map it with some encoding function, H2, hidden representation H and the encoding him representation H here that you get.",
                    "label": 0
                },
                {
                    "sent": "Actually the encoder is typically sigmoid or some nonlinearity off a linear transformation of X.",
                    "label": 0
                },
                {
                    "sent": "So linear transform X put some nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "You get your hidden representation.",
                    "label": 1
                },
                {
                    "sent": "And then from that in representation you want to be able to decode it to kind of put it back from the information that you got in there to reconstruct your something that's as close as possible to your input.",
                    "label": 0
                },
                {
                    "sent": "And that's going to use a decoder G with the typical form that's similarly going to be a linear mapping of H plus some bias, optionally followed by a non linearity here that may may be the same or different from that one.",
                    "label": 0
                },
                {
                    "sent": "And then the loss is going to simply compare how.",
                    "label": 0
                },
                {
                    "sent": "Poorly, you're doing reconstruction basically how how the discrepancy, how far your Reconstruction R is from your age, original X OK, and the training is going to minimize that, so it's going to minimize the autoencoder.",
                    "label": 0
                },
                {
                    "sent": "For examples, X belong to the training set.",
                    "label": 1
                },
                {
                    "sent": "The discrepancy between your X and your reconstruction of X, which is the composition of the encoder and decoder.",
                    "label": 1
                },
                {
                    "sent": "OK. And for reconstruction error, well, we can.",
                    "label": 0
                },
                {
                    "sent": "You can use squared error or Bernoulli cross entropy for binary variable that's more appropriate.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "There's many reasons so traditional automakers don't didn't have transposed ways.",
                    "label": 0
                },
                {
                    "sent": "There's no prior reason in this setting, at least four to have transposed weights now transpose weights in juice.",
                    "label": 0
                },
                {
                    "sent": "Two things.",
                    "label": 0
                },
                {
                    "sent": "First, they come, they induce a kind of regularization by themselves because by having transposed weights, there's some games the ultimate coder can't play, which is like making one the guy very small.",
                    "label": 0
                },
                {
                    "sent": "The other very big and things like that and transpose weights also appear and we get back to that they appear in.",
                    "label": 1
                },
                {
                    "sent": "Restricted Boltzmann machines restrictive transpose wait.",
                    "label": 0
                },
                {
                    "sent": "They fall naturally from the energy function.",
                    "label": 0
                },
                {
                    "sent": "And this connection between, but fundamentally, there's no reason.",
                    "label": 0
                },
                {
                    "sent": "Actually the encoder and decoder in this general autoencoder framework very flexible.",
                    "label": 0
                },
                {
                    "sent": "You can use whichever function you want, they can be parameterized in completely different ways, so there's no reason that their parameters should be the same, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, the kind of inverse in a statistical sense, in the sense that you want.",
                    "label": 0
                },
                {
                    "sent": "Basically, this guy here should do something close to finding the map of P of H given X.",
                    "label": 0
                },
                {
                    "sent": "If you were thinking of random variables and this will be the reverse P of X given H, so that's why I'm saying that kind of.",
                    "label": 0
                },
                {
                    "sent": "Inverse is interested in this sense or in a functional sense you could say really AGS, trying to invert whatever aged it OK, but its inverse is a function.",
                    "label": 0
                },
                {
                    "sent": "Inverse is a function, doesn't mean that you are going to transpose your matrices.",
                    "label": 0
                },
                {
                    "sent": "Actually it means that in a very specific small restricted set of case yes.",
                    "label": 0
                },
                {
                    "sent": "Setting there's nothing that really delineates where the encoder is and where the default.",
                    "label": 0
                },
                {
                    "sent": "No, there's no.",
                    "label": 0
                },
                {
                    "sent": "There's nothing that's entirely true, so there's there's still one thing in this picture.",
                    "label": 0
                },
                {
                    "sent": "OK, is that here?",
                    "label": 0
                },
                {
                    "sent": "The representation we have here is the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "It's the lowest dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "That's probably the one that's most interesting that we want to use.",
                    "label": 0
                },
                {
                    "sent": "That's the one that captures all the information.",
                    "label": 0
                },
                {
                    "sent": "That's retained by that thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah sorry yeah.",
                    "label": 0
                },
                {
                    "sent": "So, so it's, uh, it's uh, for the.",
                    "label": 0
                },
                {
                    "sent": "For here.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a continuous space because it's like a multilayer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "We want to learn that by backpropagation.",
                    "label": 0
                },
                {
                    "sent": "So we need to get it greatness to get through and etc.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "OK. And you can have binary inputs.",
                    "label": 0
                },
                {
                    "sent": "It works with with binary inputs, but the representation here OK is not binarized.",
                    "label": 0
                },
                {
                    "sent": "It's like it's like if we were to binarize it, is like using using.",
                    "label": 0
                },
                {
                    "sent": "Hard hard threshold units.",
                    "label": 0
                },
                {
                    "sent": "Binary units in a neural network.",
                    "label": 0
                },
                {
                    "sent": "You know if you've seen gradient decent, you've seen that you can't get any gradient through that, and you can't learn it.",
                    "label": 0
                },
                {
                    "sent": "I mean not not in the straight gradient descent way.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so linear autoencoders relationship between linear autoencoders and PCA.",
                    "label": 1
                },
                {
                    "sent": "So linear autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Here we talking about the traditional one where we had the dimension of the other hidden layer that's smaller than the input.",
                    "label": 1
                },
                {
                    "sent": "Actually with linear neurons and squared loss.",
                    "label": 1
                },
                {
                    "sent": "The outline coder learned exactly the same subspace as principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "So it happens that this is also true if you use a single sigmoidal hidden layer with a sigmoid.",
                    "label": 1
                },
                {
                    "sent": "And still a linear output neuron with squared loss provided you have untied weights.",
                    "label": 0
                },
                {
                    "sent": "If you tie the weights then you're introducing something more complicated so it can't learn basically what it does here is, it puts itself, it can put itself in the linear regime, so it's still back to linear.",
                    "label": 0
                },
                {
                    "sent": "So it's equal to PC, so not not not not super interesting, so it won't learn the exact same basis, SPCA the WS will not necessarily be orthogonal or orthonormal country to PC and not ordered, but they will spend the exact same subspace.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so it's doing something interesting, something reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, now similarity between auto encoders on restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "Well, now considering autoencoder with a single hidden layer, again with a sigmoid nonlinearity.",
                    "label": 1
                },
                {
                    "sent": "Add a Sigma Sigma output nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "This time we have sigmoid output nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "No squared error.",
                    "label": 1
                },
                {
                    "sent": "Ann and you tie the encoder decoder weights.",
                    "label": 0
                },
                {
                    "sent": "If we look at what the auto encoder looks like for its encoder.",
                    "label": 0
                },
                {
                    "sent": "Looks like taking a linear transformation or an affine transformation of X. Primitives by WNB here and passing it through a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "And the decoder does the reverse.",
                    "label": 0
                },
                {
                    "sent": "So it starts from.",
                    "label": 0
                },
                {
                    "sent": "It takes H2 through through Nefyn mapping and passes it through a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "You get the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "And that's the kind of thing that you would use for binary units when you have binary units because the sigmoid will map something between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the RBM.",
                    "label": 0
                },
                {
                    "sent": "So the IBM can be derived from the energy function, but if you look if you if you do the computation from that energy function at look at the conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "So in our BMS we have stochastic variables.",
                    "label": 0
                },
                {
                    "sent": "Your visa is a binary input variable and your H is also binary, so it can take values zero and one.",
                    "label": 0
                },
                {
                    "sent": "But if you look at what is the probability that some given H is 1?",
                    "label": 0
                },
                {
                    "sent": "Given the input, what it's the exact same computation as what we had here for HK?",
                    "label": 0
                },
                {
                    "sent": "So basically what we did in what do we do in GBM to compute the probability of a hidden unit being on is exactly what we have in the auto encoder encoding here.",
                    "label": 1
                },
                {
                    "sent": "OK, so H will be the probably link is far beyond probably and here is a deterministic function so and similarly for the decoder, so that'll be the probability that the visual visible unit sonar beams.",
                    "label": 1
                },
                {
                    "sent": "RV is like RX is here OK?",
                    "label": 1
                },
                {
                    "sent": "An we have these transposed weights here, so if we tide autoencoder also have transpose face.",
                    "label": 0
                },
                {
                    "sent": "Now the big difference is that in autoencoder is really deterministic mapping.",
                    "label": 0
                },
                {
                    "sent": "We were talking just about functions H as a function of X. OK, are they?",
                    "label": 0
                },
                {
                    "sent": "Reconstruction is also a function of H, so their continuous differentiable functions.",
                    "label": 0
                },
                {
                    "sent": "Here we're talking about stochastic mapping.",
                    "label": 0
                },
                {
                    "sent": "HH is considered a random variable that you're.",
                    "label": 0
                },
                {
                    "sent": "Ideally you'd be sampling to get those things which always always be zero or one.",
                    "label": 0
                },
                {
                    "sent": "This thing is was always be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This thing was always be zero and one, but with the probability that's the same at this thing.",
                    "label": 0
                },
                {
                    "sent": "OK, just to show so they look.",
                    "label": 0
                },
                {
                    "sent": "Kind of related, right, right?",
                    "label": 0
                },
                {
                    "sent": "Not going.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, so that was the basic.",
                    "label": 0
                },
                {
                    "sent": "The basic thing that was no test is if you so due to their relatedness.",
                    "label": 0
                },
                {
                    "sent": "Now let's get back to the greedy layer wise pretraining that's used for restricted Boltzmann machine, Ann Hinton.",
                    "label": 1
                },
                {
                    "sent": "And coworkers proposed in 2006 which kind of re launched this field of deep learning by showing that if you did this.",
                    "label": 1
                },
                {
                    "sent": "Layer by layer unsupervised pre training with RBM's.",
                    "label": 0
                },
                {
                    "sent": "OK so do your first layer second layer or the third layer etc.",
                    "label": 0
                },
                {
                    "sent": "At this is a very good way, very efficient way to initialize a deep network.",
                    "label": 0
                },
                {
                    "sent": "Pre train a deep network.",
                    "label": 0
                },
                {
                    "sent": "Initialize a deep network that gave us the deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "Which worked pretty well, so we thought, well, you know.",
                    "label": 0
                },
                {
                    "sent": "I say yeah, sure, yeah sure thought IBM's an all time quarters are.",
                    "label": 0
                },
                {
                    "sent": "Kind of cousins, so why not try that?",
                    "label": 0
                },
                {
                    "sent": "Let's try that with with the ultimate coders, so it's exact.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same principle, just instead of using an RBM.",
                    "label": 0
                },
                {
                    "sent": "BMT pre training Slayer user shallow sigmoid autoencoder.",
                    "label": 0
                },
                {
                    "sent": "K to pretend it retraining, Slayer.",
                    "label": 0
                },
                {
                    "sent": "So you train your first autoencoder to reconstruct its input.",
                    "label": 0
                },
                {
                    "sent": "Once you've done that, you get you get ready.",
                    "label": 0
                },
                {
                    "sent": "You can forget about the reconstruction part, you take the new representation.",
                    "label": 0
                },
                {
                    "sent": "At this level, you start over again and over again.",
                    "label": 0
                },
                {
                    "sent": "Etc yes.",
                    "label": 0
                },
                {
                    "sent": "Because that's what.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK so I shouldn't say that that's not what works best without encoders, and that's what worked best.",
                    "label": 1
                },
                {
                    "sent": "Weather without BMS.",
                    "label": 0
                },
                {
                    "sent": "The idea is to.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry within a lot of deep learning is that the have an overcomplete representation is better than have a compressed representation.",
                    "label": 0
                },
                {
                    "sent": "That's more that was more of an empirical.",
                    "label": 0
                },
                {
                    "sent": "Finding, I think that we found it.",
                    "label": 0
                },
                {
                    "sent": "It did work best, but for a long time and this may have heard the progress in the field.",
                    "label": 0
                },
                {
                    "sent": "We had this vision about.",
                    "label": 0
                },
                {
                    "sent": "The information is better to have to be compressed if we presented fewer fewer few units.",
                    "label": 0
                },
                {
                    "sent": "But given the fact that the encoder that you have here to go from one to the other is a very very simple function, it's really kind of just linear with some thresholding.",
                    "label": 0
                },
                {
                    "sent": "Anything it can do something very complicated, so it can't.",
                    "label": 0
                },
                {
                    "sent": "And you can't in one go here.",
                    "label": 0
                },
                {
                    "sent": "Hope to extract compact continuous coordinates of something.",
                    "label": 0
                },
                {
                    "sent": "Usually.",
                    "label": 0
                },
                {
                    "sent": "So because of that, it's it's proved better to map the input to a larger dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And eventually get back to your smaller target or whatever.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I'm going to get to that.",
                    "label": 0
                },
                {
                    "sent": "That's that's that's the whole point that I'm getting at.",
                    "label": 0
                },
                {
                    "sent": "But it yeah, it's a very good track, so I had.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same, but before I moved to that I just that I know I didn't probably didn't perfectly answer your question.",
                    "label": 0
                },
                {
                    "sent": "I hope it will become clear.",
                    "label": 0
                },
                {
                    "sent": "There is no more bottleneck here.",
                    "label": 0
                },
                {
                    "sent": "In this example.",
                    "label": 0
                },
                {
                    "sent": "That's true an in the way we use them now.",
                    "label": 0
                },
                {
                    "sent": "There is no bottleneck, at least not at at the shallow level layers.",
                    "label": 0
                },
                {
                    "sent": "There may be one later on at some point, but not not in the first layers.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I could elaborate on why I think this needs to be for for, like for vision tasks, but I'm with it to the end.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, so now supervised fine tuning so you have seen this with the Baltimore machines.",
                    "label": 1
                },
                {
                    "sent": "Sorry with the deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "Well, it's the same principle here with once we've trained, sorry once we've trained our stack, which is all unsupervised, right?",
                    "label": 0
                },
                {
                    "sent": "To this point, we haven't used any prediction target at all.",
                    "label": 0
                },
                {
                    "sent": "It's all unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Weather in our BMR or the autoencoder case, but if we want to use it for to have a deep network for doing something like classification supervised classification, then we just use this first step as initialization and then we do the ordinary thing.",
                    "label": 0
                },
                {
                    "sent": "It's just that instead of having initialize our network purely randomly, all the layers we have we have pre trained it with this layer by layer.",
                    "label": 0
                },
                {
                    "sent": "This layer by layer fashion and then we just add the output layer, supervised costs with the target and do the normal back propagation.",
                    "label": 0
                },
                {
                    "sent": "Or fine tuning this network?",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "If instead of.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. From the yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so so that's a very good idea, and now it's not exactly the same, because here in the end we're only using the supervised gradient objective, so it's going to tune only for that supervised objective.",
                    "label": 0
                },
                {
                    "sent": "And actually, we did some experiments doing what you suggested and in most cases it actually hurt the performance.",
                    "label": 0
                },
                {
                    "sent": "So the important point here is that the unsupervised pretraining is only used to initialize the network in some reasonable space.",
                    "label": 1
                },
                {
                    "sent": "That's more reasonable, more useful, more interesting.",
                    "label": 0
                },
                {
                    "sent": "Then if we were doing random random initialization.",
                    "label": 0
                },
                {
                    "sent": "But apparently it's closer than if you start with random initialization.",
                    "label": 0
                },
                {
                    "sent": "That's why it works better.",
                    "label": 0
                },
                {
                    "sent": "But I have some experiments that go in direction of asking or answering that question further, yes.",
                    "label": 0
                },
                {
                    "sent": "In terms of performance, I'm getting to that, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, at the time we did, all these experiments were still using a sigmoid unit, so I still.",
                    "label": 0
                },
                {
                    "sent": "I'm sure it still was a problem.",
                    "label": 0
                },
                {
                    "sent": "It was probably less of a problem then then with random initialization.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so so so I'm going historically here is a development of deep learning at the time that we did, that the state of the art was pre training with our BMS.",
                    "label": 0
                },
                {
                    "sent": "OK IBM, they have this Sigma is everywhere if you if you use them.",
                    "label": 0
                },
                {
                    "sent": "So we use the closest thing which was auto encoders with Sigma.",
                    "label": 0
                },
                {
                    "sent": "It's OK for this experience and at the time everybody was still using Sigmoids, and I mean the more mostly rally.",
                    "label": 0
                },
                {
                    "sent": "We just it wasn't clear that.",
                    "label": 0
                },
                {
                    "sent": "Yet that that it would make bring such an advantage, but there is no reason or probably reason to not to train a a an autoencoder with values.",
                    "label": 0
                },
                {
                    "sent": "You can train autoencoder with values to for your pre training if you want to do pretraining.",
                    "label": 0
                },
                {
                    "sent": "The idea is that of course what you have in your autoencoder should be the same function that you eventually have in your deep network, right?",
                    "label": 0
                },
                {
                    "sent": "And that's kind of.",
                    "label": 0
                },
                {
                    "sent": "Similar kind of more natural even for for auto encoders and for our for our BMS because our BMS.",
                    "label": 0
                },
                {
                    "sent": "If you want proper PBM it you need to bit needs to correspond to a nice energy function that that gives you all the equation that you hope you get more.",
                    "label": 1
                },
                {
                    "sent": "Billy Ray, I don't know if there is an equivalent PBM torrell use.",
                    "label": 0
                },
                {
                    "sent": "Probably you can come up with one with some approximation, but for autoencoders kind of natural you can choose whatever you want as the encoder and it's kind of more.",
                    "label": 0
                },
                {
                    "sent": "They're both neural networks that you're tuning right?",
                    "label": 0
                },
                {
                    "sent": "Just one that is deeper than the other.",
                    "label": 0
                },
                {
                    "sent": "But for the whatever each layer is, you can use the same exact thing that it was that you're going to use for your final deep network.",
                    "label": 0
                },
                {
                    "sent": "Inside yes.",
                    "label": 0
                },
                {
                    "sent": "So so so this trick for our BMS come comes from the actual generative view about what's going on with our BMS.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "That's not what we did do with with.",
                    "label": 0
                },
                {
                    "sent": "Without encoders, there's this.",
                    "label": 0
                },
                {
                    "sent": "No, there's not.",
                    "label": 0
                },
                {
                    "sent": "This unrolling the actual.",
                    "label": 0
                },
                {
                    "sent": "Inference that this is an approximation to.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so super size tuning happens to be quite important.",
                    "label": 0
                },
                {
                    "sent": "Here's an experiment where this was an infinite variation.",
                    "label": 0
                },
                {
                    "sent": "Type of EM nystagmus with some more variations and all these curves have been actually pre trained with.",
                    "label": 0
                },
                {
                    "sent": "Autoencoders it's the same if we pre training with with our BMS on amnesty.",
                    "label": 0
                },
                {
                    "sent": "And here we just throw showing what's going on with the supervised phase.",
                    "label": 1
                },
                {
                    "sent": "At this point here, so this this.",
                    "label": 0
                },
                {
                    "sent": "This is the point.",
                    "label": 0
                },
                {
                    "sent": "Where we do the supervised fine tuning.",
                    "label": 1
                },
                {
                    "sent": "So we see it first.",
                    "label": 0
                },
                {
                    "sent": "It's very important.",
                    "label": 0
                },
                {
                    "sent": "So all the actually we start with the same network here.",
                    "label": 0
                },
                {
                    "sent": "But here we compare different ways of doing this supervised fine tuning and this is answering part of your question.",
                    "label": 0
                },
                {
                    "sent": "So we tried doing purely the supervised fine tuning with only the supervised fine tuning criterion.",
                    "label": 0
                },
                {
                    "sent": "So we see this, it gives us this curve.",
                    "label": 0
                },
                {
                    "sent": "But if we do it with no.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "No supervised fine tuning, you get that.",
                    "label": 0
                },
                {
                    "sent": "And these other curves it is dotted.",
                    "label": 0
                },
                {
                    "sent": "This dashed is if we use the fine tuning gradient in addition to keeping a under the unsupervised gradient from the reconstruction at the same time as we do this with fine tuning and you see it hurts performance here.",
                    "label": 1
                },
                {
                    "sent": "OK, so put training basic outline quarters stack are better than no pre training.",
                    "label": 1
                },
                {
                    "sent": "And basic autoencoders stack.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, almost matched stacks of RBM's OK, but the key point is here is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Almost.",
                    "label": 0
                },
                {
                    "sent": "So basic outline coders turned out to be not as good feature learners as restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "So what's the problem?",
                    "label": 0
                },
                {
                    "sent": "So, as you rightly pointed out.",
                    "label": 0
                },
                {
                    "sent": "This is this is related to the fact that traditional time codes were for dimensionality reduction, but many of the deep learning success seems to depend on the ability to learn what's called overcomplete representation, weather size.",
                    "label": 1
                },
                {
                    "sent": "Our presentation is larger than your input.",
                    "label": 0
                },
                {
                    "sent": "And basic outgoing quarters.",
                    "label": 0
                },
                {
                    "sent": "If you do that, it feels kind of a trivial useless solution.",
                    "label": 0
                },
                {
                    "sent": "The identity mapping it can learn simply, it can reproduce or put all the information that it had in X, Co. Pay it back into H and Ann have a perfect reconstruction, yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Well OK, the OK how to say the initial RBM?",
                    "label": 0
                },
                {
                    "sent": "The binary binary RBM has Bernoulli variables in is visible and hidden.",
                    "label": 0
                },
                {
                    "sent": "And sorry an visible and hidden there Bernoulli variables so.",
                    "label": 0
                },
                {
                    "sent": "Strictly speaking, you should be using binary data, but when they trained, when they use it on images or anything, they don't use a binary values, you just use a real valued OK, so they're kind of little hacks like that.",
                    "label": 0
                },
                {
                    "sent": "So it's true that the binary binary RPMS should be binary data, but they were applied and at the time that was also the case, there were applied on non binary data.",
                    "label": 0
                },
                {
                    "sent": "Anne, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so, so that's that's the binary binary RBM, but you can extend our BMS to have any distribution of for your hidden and visible variables basically so.",
                    "label": 0
                },
                {
                    "sent": "That include continuous distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Because the training criterion is very different.",
                    "label": 0
                },
                {
                    "sent": "OK, even though the functional form looks very similar, the training criterion are.",
                    "label": 0
                },
                {
                    "sent": "BMS looks very different.",
                    "label": 0
                },
                {
                    "sent": "The main reason is it falls out from this energy function that you're that you're optimizing.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, there's a term that's the partition function.",
                    "label": 0
                },
                {
                    "sent": "Basically the these two terms.",
                    "label": 0
                },
                {
                    "sent": "There's a term that's going to look somewhat like the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "That's going to say.",
                    "label": 1
                },
                {
                    "sent": "OK, I want to do an encoding that that gives me a good reconstruction, but there's another term that's going to try to make sure that you can't reconstruct that you can reconstruct well only the training data.",
                    "label": 0
                },
                {
                    "sent": "That's the way to interpret it.",
                    "label": 0
                },
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "If you were to take our BMS and interpret them at all time quarters, there's a term that says OK, reconstruct well.",
                    "label": 1
                },
                {
                    "sent": "But this is a term that says reconstruct well, but only on the training data.",
                    "label": 0
                },
                {
                    "sent": "Don't reconstruct well anything else, and you don't have that in autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Not yet, at least.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, if you allow I'll carry out.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really super late on my schedule.",
                    "label": 0
                },
                {
                    "sent": "So this is, uh, this got me the idea to introduce the denoising autoencoder so it's a very simple twist simple modification to the autoencoder.",
                    "label": 0
                },
                {
                    "sent": "The simple idea is that we're going to destroy some information of the input by randomly selecting some input features.",
                    "label": 0
                },
                {
                    "sent": "Putting them 20.",
                    "label": 0
                },
                {
                    "sent": "That's what and train the auto encoder to restore it.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's why I used initially destroy the feature information by putting them to zero.",
                    "label": 1
                },
                {
                    "sent": "That's why I called 0 masking noise.",
                    "label": 0
                },
                {
                    "sent": "Now everybody calls it drop off noise.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The thing is, this task of mere reconstruction now becomes the task of denoising 'cause you're corrupting the input and asking that thing to clean it for you, right?",
                    "label": 0
                },
                {
                    "sent": "And denoising is vastly more challenging task than reconstruction.",
                    "label": 1
                },
                {
                    "sent": "Because even in the widely overcomplete case, so suppose you have many many many hidden variables.",
                    "label": 1
                },
                {
                    "sent": "OK, it must learn intelligent encoding decoding for it to help to be able to do this.",
                    "label": 0
                },
                {
                    "sent": "Denoising.",
                    "label": 0
                },
                {
                    "sent": "So Moreover, it will encourage our presentation that is robust to small perturbations of the input.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, 'cause you're perturbing your input all right, so it's a very, very minor twist to this autoencoder architecture.",
                    "label": 0
                },
                {
                    "sent": "Is simply you add this little step here.",
                    "label": 0
                },
                {
                    "sent": "Your input X you first corrupt it.",
                    "label": 0
                },
                {
                    "sent": "OK, so pick some units, put them to 0 or use some other corruption corrupting noise, and then you encode that corrupted version.",
                    "label": 0
                },
                {
                    "sent": "Get the hidden representation, you decode it and you get your reconstruction and you hope that your reconstruction is as close as possible to the clean input, very simple.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this minimizes a slightly modified criterion compared to the original autoencoder, where now we do again a sum of all training examples.",
                    "label": 0
                },
                {
                    "sent": "But here is an expectation over.",
                    "label": 0
                },
                {
                    "sent": "So instead of taking the discrepancy between clean between the original X and the reconstruction of the original X, who will take the discrepancy?",
                    "label": 0
                },
                {
                    "sent": "The loss between the original X, the clean X?",
                    "label": 0
                },
                {
                    "sent": "An the kind of reconstructed denoise version of the corrupted X OK, and we do that by an expectation by taking corrupted versions of our original X OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this will learn robust and useful features.",
                    "label": 1
                },
                {
                    "sent": "The easier to train than DBM feature.",
                    "label": 1
                },
                {
                    "sent": "Then DBM feature extractors.",
                    "label": 1
                },
                {
                    "sent": "And I will see that they will similar or better classifications.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the element code training minimizes this.",
                    "label": 1
                },
                {
                    "sent": "As I said, the denoising autoencoder minimizes this.",
                    "label": 1
                },
                {
                    "sent": "We're just adding here a kind of expectation over this is the corruption process OK?",
                    "label": 0
                },
                {
                    "sent": "Now this except expectation user, because this is kind of highlighted.",
                    "label": 1
                },
                {
                    "sent": "This is nonlinear, so we can't compute that exactly OK, we can compute the expectation exactly.",
                    "label": 1
                },
                {
                    "sent": "So what we do instead, we used to cast a gradient descent, but adding some levels to casted City by sampling corrupted inputs.",
                    "label": 0
                },
                {
                    "sent": "So real it's really going to stochastically corrupt our input X an use a corrupted version, because we can't compute the expectation exactly.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK and possible corruptions.",
                    "label": 0
                },
                {
                    "sent": "There's a zero in pixel at random, just called drop dropout, noise additive, Gaussian noise, audible noise.",
                    "label": 1
                },
                {
                    "sent": "This others you can come up with.",
                    "label": 1
                },
                {
                    "sent": "OK, so here we using stochastic perturbations, yes.",
                    "label": 0
                },
                {
                    "sent": "The influence are there.",
                    "label": 0
                },
                {
                    "sent": "Oh I.",
                    "label": 0
                },
                {
                    "sent": "Right, so we haven't looked at the effect on iterations per see we will look at the effect on generalization performance, so I can't answer in terms of iterations.",
                    "label": 0
                },
                {
                    "sent": "What you're saying is that probably if you corrupt more it will take more time to kind of converge.",
                    "label": 0
                },
                {
                    "sent": "To get it there, yeah, that should be expected.",
                    "label": 0
                },
                {
                    "sent": "I think that's that's that's an empirical observation that already heard also for dropout in supervised networks and so that it will take longer to converge, but it will converge to something better, yes?",
                    "label": 0
                },
                {
                    "sent": "So, so I'm going to show in an variant that that does the corruption at the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "I think they're they're playing very similar role.",
                    "label": 0
                },
                {
                    "sent": "I don't think it makes much much of a difference.",
                    "label": 1
                },
                {
                    "sent": "So from the manifold perspective, it kind of makes more sense to add it as in the input an I'll show the manifold perspective in awhile.",
                    "label": 0
                },
                {
                    "sent": "The short answer is that you can view the encoder as projecting on the manifold, so you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't want to use to add isotropic noise on your manifold, you want to add isotropic noise on your inputs.",
                    "label": 0
                },
                {
                    "sent": "But I'll show an image in in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just so just to see.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difference of what this thing is learns OK if you train a basic autoencoder, OK, that's that's complete or over complete, so it has more units than there.",
                    "label": 1
                },
                {
                    "sent": "On the natural image patches, so these are extracted from natural images.",
                    "label": 0
                },
                {
                    "sent": "That's basically the filters you know where the filters are there.",
                    "label": 0
                },
                {
                    "sent": "Just just looking at the weights.",
                    "label": 0
                },
                {
                    "sent": "That's our incoming to your hidden Hurons, right?",
                    "label": 1
                },
                {
                    "sent": "So each of these little image corresponds to the weights of an incoming urine, and that's what's going to multiply your input.",
                    "label": 0
                },
                {
                    "sent": "That's when you train an auto encoder.",
                    "label": 0
                },
                {
                    "sent": "That's when you train is regularised autoencoder OK?",
                    "label": 1
                },
                {
                    "sent": "It's not a standard way of recognizing it with simply wait to case L2 penalty on on your weights.",
                    "label": 0
                },
                {
                    "sent": "Because there are some classical results that show that for linear things, if you actually corrupt your input with a Gaussian noise, it's equivalent to add to add it to penalizing your weights so it was.",
                    "label": 0
                },
                {
                    "sent": "You might think.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, that's going to be equivalent to doing away today, but it's not.",
                    "label": 1
                },
                {
                    "sent": "If you do a denoising autoencoder, that's the kind of filters you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are clearly agitators here.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and here's another example that is meant to illustrate the role of that.",
                    "label": 0
                },
                {
                    "sent": "Now we have a nice new hyperparameter, which is the level of noise, the corruption.",
                    "label": 0
                },
                {
                    "sent": "So this is a network that was trained from the same initial random position and we train it 1 version with zero Noise, 1 version with 10% noise, one oil version with 20% one version with 50%.",
                    "label": 0
                },
                {
                    "sent": "OK, and what we see is we trained with zero percent which corresponds to the regular autoencoder.",
                    "label": 0
                },
                {
                    "sent": "It doesn't learn much interesting.",
                    "label": 0
                },
                {
                    "sent": "That's basically all the filters it it gets.",
                    "label": 0
                },
                {
                    "sent": "It gets some of them.",
                    "label": 0
                },
                {
                    "sent": "But as we increase, the noise level is going to learn these more.",
                    "label": 0
                },
                {
                    "sent": "Localized specific filters.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that intuitively it's kind of clear, right?",
                    "label": 0
                },
                {
                    "sent": "You're corrupting your thing more.",
                    "label": 0
                },
                {
                    "sent": "So what it needs to do to predict one pixel, it will need to pull more information about the pixels around it, because it can't rely on that pixels value anymore, right?",
                    "label": 0
                },
                {
                    "sent": "Here it can rely on it either just there, here, it can't.",
                    "label": 0
                },
                {
                    "sent": "It can't rely on it, so it has to pull more information around it to kind of infer what even knowing that it's it's very noisy what the pixel value will be.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                },
                {
                    "sent": "If your goal is.",
                    "label": 0
                },
                {
                    "sent": "Dependencies.",
                    "label": 0
                },
                {
                    "sent": "Corruption.",
                    "label": 0
                },
                {
                    "sent": "Yes, you certainly can have random structured corruptions and that that that that can help in specific setting, but this where did this work?",
                    "label": 0
                },
                {
                    "sent": "We're really thinking about it was the time we're trying to build a competitor to PBM.",
                    "label": 0
                },
                {
                    "sent": "So the the task was.",
                    "label": 0
                },
                {
                    "sent": "Say building learners and learning algorithms without any prior knowledge without that could be generically used, used, used for any kind of data without incorporating specific prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "But if you do have specific prior knowledge of the structure of the corruption of the data, that makes sense, etc.",
                    "label": 0
                },
                {
                    "sent": "It surely can help.",
                    "label": 0
                },
                {
                    "sent": "If you integrate that prior knowledge in there.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the manifold view of this.",
                    "label": 0
                },
                {
                    "sent": "There's a manifold interpretation of what's going on with the denoising auto encoder.",
                    "label": 1
                },
                {
                    "sent": "Basically, if we think our if we make the hypothesis assumption that our data the ex is yet Alexis is close to some lower dimensional manifold, here 1 dimensional manifold in 2D space, that's a cartoon view of things and we corrupt we take some original input that's close to the manifold and we corrupt it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this corruption is kind of isotropic, it doesn't have any preference of direction, it doesn't know about manifolds.",
                    "label": 0
                },
                {
                    "sent": "It doesn't care about manifolds, it's just.",
                    "label": 0
                },
                {
                    "sent": "Prior noise that we put, so it's going to likely put it in any direction and most directions will be kind of away from the manifold to some extent, so it's corrupting the input and then the.",
                    "label": 0
                },
                {
                    "sent": "The encoder is actually going to try and put that point back in to manage something that corresponds to manifold coordinates and then from there is going to be reconstructed.",
                    "label": 0
                },
                {
                    "sent": "OK. Again, you can think of it.",
                    "label": 0
                },
                {
                    "sent": "Think of the parallel with PCA, PCA and the linear autoencoder.",
                    "label": 0
                },
                {
                    "sent": "That's what they're doing, they're projecting.",
                    "label": 0
                },
                {
                    "sent": "On the, I mean the encoding or the principle components is really projecting onto coordinates and coordinate system.",
                    "label": 1
                },
                {
                    "sent": "All the manifold and then from that coordinate system on the manifold.",
                    "label": 0
                },
                {
                    "sent": "The point can be reconstructed.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This was this was a what we call 0 masking noise that's now called Dropout Dropout Noise.",
                    "label": 0
                },
                {
                    "sent": "You might also hear blank out noise, so it's just that we took.",
                    "label": 0
                },
                {
                    "sent": "We flipped a sample Bernoulli variable so flip to coins with 20% of it being 20% chance of it being one OK. And if it was one, we set the unit to 0.",
                    "label": 0
                },
                {
                    "sent": "So with 20% chance each unit independently was set to 0.",
                    "label": 0
                },
                {
                    "sent": "But you can try that.",
                    "label": 0
                },
                {
                    "sent": "We tried, we tried with other types of noise, even got additive Gaussian noise etc.",
                    "label": 0
                },
                {
                    "sent": "You get, you get nice filters.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we didn't try with multiplicative Gaussian noise at the time we tried with additive Gaussian noise and there were.",
                    "label": 0
                },
                {
                    "sent": "Similar, I'd say they would get slightly better results with the with performance in classification afterwards.",
                    "label": 0
                },
                {
                    "sent": "On this there's always a question.",
                    "label": 0
                },
                {
                    "sent": "How do you evaluate your your unsupervised representation?",
                    "label": 0
                },
                {
                    "sent": "Usually it's done with building classify on top of them, and so regardless of the kind of noise that we use, if we use the right level it it did help.",
                    "label": 0
                },
                {
                    "sent": "It did boost in performance compared to not using any and some noises were better than others.",
                    "label": 0
                },
                {
                    "sent": "For some cases the kind of drop out.",
                    "label": 0
                },
                {
                    "sent": "Blackout was seemed to be better in most cases, but.",
                    "label": 0
                },
                {
                    "sent": "It's like just a slightly.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's some experiments, stacked denoising autoencoders.",
                    "label": 0
                },
                {
                    "sent": "They were dead.",
                    "label": 0
                },
                {
                    "sent": "That's on the OK.",
                    "label": 0
                },
                {
                    "sent": "This is the infinite amnist again, and that's the online classification error.",
                    "label": 0
                },
                {
                    "sent": "So Infiniteness is very big.",
                    "label": 0
                },
                {
                    "sent": "So it's like as if you were measuring actually generalization error at every step.",
                    "label": 0
                },
                {
                    "sent": "And that you can see what we get with the 1 hidden layer net with no pretraining.",
                    "label": 0
                },
                {
                    "sent": "That's this black line.",
                    "label": 0
                },
                {
                    "sent": "This dashed black nine with three neural network with three hidden layers without pre training.",
                    "label": 0
                },
                {
                    "sent": "So that was a sigmoid network with three hidden layers without any pre training OK and no noise injection, no nothing.",
                    "label": 0
                },
                {
                    "sent": "You see it's worse is doing worse actually than the 1 one layer network OK.",
                    "label": 0
                },
                {
                    "sent": "If we did one layer with RBM pretraining.",
                    "label": 0
                },
                {
                    "sent": "You get this blue dashed line slam.",
                    "label": 0
                },
                {
                    "sent": "Better three layers with RBM training.",
                    "label": 0
                },
                {
                    "sent": "We're doing a little better.",
                    "label": 0
                },
                {
                    "sent": "His experiment, one layer with our denoising autoencoder, pretraining these following pretty closely.",
                    "label": 0
                },
                {
                    "sent": "The one layer PBM maybe.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Similar to little bit noisy and find the three layer with denoising autoencoder.",
                    "label": 0
                },
                {
                    "sent": "In this experiment using.",
                    "label": 0
                },
                {
                    "sent": "Really good.",
                    "label": 0
                },
                {
                    "sent": "So it works, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so people have played with similar with schemes like that changing your corruption rate as you train, etc.",
                    "label": 0
                },
                {
                    "sent": "Quite recently added it can help.",
                    "label": 0
                },
                {
                    "sent": "So I have a theory for that.",
                    "label": 0
                },
                {
                    "sent": "OK, this is hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "I talked about my in my in my first first lecture hyperparameters so everybody hates hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Then you need to tune them, right?",
                    "label": 0
                },
                {
                    "sent": "That's very annoying.",
                    "label": 0
                },
                {
                    "sent": "It's super annoying you don't know what to do, but had this theory that you take an algorithm, you take some smaller modifications where you add some hyperparameters to tune.",
                    "label": 0
                },
                {
                    "sent": "OK, well, the more happy with this you have to tune, the better your performance will be.",
                    "label": 0
                },
                {
                    "sent": "So so if you do that, in effect, you're talking about the schedule of noise.",
                    "label": 0
                },
                {
                    "sent": "OK, that's kind of complicated high parameter to tune because you can choose which we have put more in the beginning, more later, etc.",
                    "label": 0
                },
                {
                    "sent": "There will be some optimal setting for this schedule, and this optimal setting for this schedule will allow you to get a little gain in the end.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "With two hidden layers, you mean Nope, no reason for that.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Over the prior in this get this guy.",
                    "label": 0
                },
                {
                    "sent": "Oh, actually well we had.",
                    "label": 0
                },
                {
                    "sent": "We did other experiments where we increase the number of layers and it's just that it wasn't shown here, but it's two layers will be in between the two.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it.",
                    "label": 0
                },
                {
                    "sent": "Usually it it it.",
                    "label": 0
                },
                {
                    "sent": "It does improve up to some point where maybe you you get into optimization difficulties.",
                    "label": 0
                },
                {
                    "sent": "Would I don't remember these experiments if we went beyond three?",
                    "label": 0
                },
                {
                    "sent": "But basically I think we wanted to compare also with.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, I think not.",
                    "label": 0
                },
                {
                    "sent": "I just want to mention some some advantages of denoising autoencoders over our BMS for pretraining stacking.",
                    "label": 1
                },
                {
                    "sent": "Except in addition to the fact that they work just as well or sometimes better.",
                    "label": 1
                },
                {
                    "sent": "No partition function to deal with in autoencoders.",
                    "label": 0
                },
                {
                    "sent": "So you can actually measure the actual training criterion that you're optimizing.",
                    "label": 0
                },
                {
                    "sent": "The true kind of training criterion.",
                    "label": 0
                },
                {
                    "sent": "You can measure it quite quite accurately in our brains is a little bit of a problem.",
                    "label": 1
                },
                {
                    "sent": "You don't know how well you doing on the training criterion necessarily.",
                    "label": 1
                },
                {
                    "sent": "The It's a very general framework in the sense that the encoder decoder in and out all time quarter.",
                    "label": 1
                },
                {
                    "sent": "You can use any permit parameterized function, so some persons mentioned the idea of maybe using values.",
                    "label": 0
                },
                {
                    "sent": "You know it's very natural to use, but you couldn't also imagine using autoencoders with more than one hidden layer.",
                    "label": 0
                },
                {
                    "sent": "OK, you could imagine lots of things.",
                    "label": 0
                },
                {
                    "sent": "So it's very flexible, maybe two flexible.",
                    "label": 0
                },
                {
                    "sent": "And I want person.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "Right, so somebody asked, somebody asked why we do that at the level of the input, the corruption rather than at the level of the representation.",
                    "label": 0
                },
                {
                    "sent": "Well, you could do that.",
                    "label": 0
                },
                {
                    "sent": "Another presentation that's reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "The denoising autoencoders basically encourages the reconstruction to be insensitive to input corruptions, but alternative an alternative would be to encourage the representation to be insensitive to input corruptions.",
                    "label": 1
                },
                {
                    "sent": "That's another way.",
                    "label": 0
                },
                {
                    "sent": "You could also do encourage the that's where you were hinting at the reconstruction to be insensitive to hidden layer presentation corruptions, but here I'm going to talk about encouraging your presentation to be insensitive still, so our presentation that we want to be insensitive that falls from that manifold hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Again, if you view the encoder as projecting on the manifold, what you want is really for a lot of variations of your input that are.",
                    "label": 1
                },
                {
                    "sent": "Outside that, along with the manifold you want them all to map to the same point, so you want your representation to be rather insensitive to changes.",
                    "label": 0
                },
                {
                    "sent": "Two small changes that are in your input.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So if we do that, encourage representation to be insensitive, that gives us the.",
                    "label": 1
                },
                {
                    "sent": "The formula would be like this, we would.",
                    "label": 1
                },
                {
                    "sent": "Optimize reconstruction error and we could add a stochastic realization.",
                    "label": 0
                },
                {
                    "sent": "We take the representation of computer presentation for X and we compute their presentation were corrupted X.",
                    "label": 0
                },
                {
                    "sent": "And we look at how different they are and we make sure that they're not too different by penalising the squared difference between them.",
                    "label": 0
                },
                {
                    "sent": "OK makes sense.",
                    "label": 0
                },
                {
                    "sent": "So and if we have tideways, so that's important.",
                    "label": 0
                },
                {
                    "sent": "In this setup, it wasn't for pure denoising auto encoder, but for this it is.",
                    "label": 1
                },
                {
                    "sent": "It will prevent W from collapsing the encoder to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the stochastic regularization.",
                    "label": 0
                },
                {
                    "sent": "Again, we're sampling corrupted examples.",
                    "label": 0
                },
                {
                    "sent": "I'm telling you I'm moving to that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to move away from stochastic regulation starting from that thing.",
                    "label": 0
                },
                {
                    "sent": "Now that's.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The S for stochastic CAE.",
                    "label": 1
                },
                {
                    "sent": "I'll get to it.",
                    "label": 0
                },
                {
                    "sent": "See a means later.",
                    "label": 0
                },
                {
                    "sent": "So you have this regularization OK, corrupting X look at representation.",
                    "label": 0
                },
                {
                    "sent": "Take Kleenex and make sure the two representations not too far from one another.",
                    "label": 0
                },
                {
                    "sent": "That's our regularizer.",
                    "label": 1
                },
                {
                    "sent": "If you consider small additive noise like Gaussian noise, and you take a Taylor series expansion of H of X plus epsilon, which will be there XX~ which is X plus epsilon here.",
                    "label": 0
                },
                {
                    "sent": "You can get your Taylor series expansion well, it's kind of.",
                    "label": 1
                },
                {
                    "sent": "It's it's simple to show that the the.",
                    "label": 0
                },
                {
                    "sent": "This term, here the regularization is a proxy.",
                    "label": 0
                },
                {
                    "sent": "If you cut the terms in the in the Taylor series expansion, it will be close to Sigma Square, which is your noise here times.",
                    "label": 0
                },
                {
                    "sent": "This thing here, which is a derivative.",
                    "label": 0
                },
                {
                    "sent": "Let's see concert Taylor exterior.",
                    "label": 0
                },
                {
                    "sent": "See expansion.",
                    "label": 0
                },
                {
                    "sent": "The derivative of your hidden representation with respect to X, so that's what's called the Jacobian.",
                    "label": 0
                },
                {
                    "sent": "OK directive.",
                    "label": 1
                },
                {
                    "sent": "Actually, that's that's a matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so derivative of each hidden layer hidden unit with respect to each input unit, so it's a it's a matrix, and we take the Frobenius norm of that matrix.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's really saying here we're saying OK, I want the representation for stochastically corrupted X to be close to the presentation for X.",
                    "label": 0
                },
                {
                    "sent": "Here, what I'm saying is really if I move X infinitesimally well, I want age not to move.",
                    "label": 0
                },
                {
                    "sent": "OK, this is what they would do.",
                    "label": 0
                },
                {
                    "sent": "Is computing right moving your thing a little bit your input?",
                    "label": 0
                },
                {
                    "sent": "How much is the H moving?",
                    "label": 0
                },
                {
                    "sent": "Well, we're penalizing that, so we're saying we want, how far, how much H moves if we move X infinitesimally wanted to be super super small.",
                    "label": 0
                },
                {
                    "sent": "So that's going from here that there was a stochastic penalty to analytic penalty.",
                    "label": 1
                },
                {
                    "sent": "So there's no more sampling, normal random sampling, normal.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's called a contractive autoencoder.",
                    "label": 1
                },
                {
                    "sent": "Really, we minimize tradeoff between the reconstruction error and this analytic term which is penalising the Frobenius norm of the Jacobian.",
                    "label": 1
                },
                {
                    "sent": "How much H is sensitive to X infinitesimally, and that's a contractive term.",
                    "label": 0
                },
                {
                    "sent": "You say it's going to try to make the mapping from X to H contractive?",
                    "label": 0
                },
                {
                    "sent": "That's called, that's why I call it CAE for contractive autoencoder and the previous thing I had was Whoops was a stochastic contractive autoencoder.",
                    "label": 0
                },
                {
                    "sent": "This is an analytic.",
                    "label": 0
                },
                {
                    "sent": "Poop.",
                    "label": 0
                },
                {
                    "sent": "This is an analytic stochastic control projective autoencoder.",
                    "label": 0
                },
                {
                    "sent": "So for training examples, encounters both small reconstruction error and representation to be insensitive to small variations.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around the examples.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK so here is my small like computational considerations.",
                    "label": 1
                },
                {
                    "sent": "If we define our hidden representation as being simply are defined linear mapping, followed by an elementwise nonlinearity S and suppose S prime is its first derivative, then actually this Jacobian, you can write each.",
                    "label": 1
                },
                {
                    "sent": "Row of the Jacobian here is represents a Joe Aro as being simply this formula, so you can see that the ACA penalty is simply.",
                    "label": 0
                },
                {
                    "sent": "Taking the squared norm of each row of your.",
                    "label": 1
                },
                {
                    "sent": "Weight vector of your weight matrix.",
                    "label": 0
                },
                {
                    "sent": "And multiplying by this derivative of your non linearity.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "'cause when you compare this to weight decay, so the standard traditional weight decay is really this, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's penalising the sum of the square of the weights.",
                    "label": 0
                },
                {
                    "sent": "And here you penalizing you also looking at the sum of the square norm of the weights for each row of your weight matrix.",
                    "label": 1
                },
                {
                    "sent": "But you're also taking into account the derivative of your non linearity squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means what there's two ways.",
                    "label": 0
                },
                {
                    "sent": "So in squared in weight decay there's just one way to satisfy the penalty is to keep W small.",
                    "label": 0
                },
                {
                    "sent": "OK, here there's two ways to satisfy the penalty.",
                    "label": 0
                },
                {
                    "sent": "Can you can someone tell me what the two ways are?",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Decide.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can keep the weight small or you can make the divertive small and the digital small means saturating your sigmoid.",
                    "label": 0
                },
                {
                    "sent": "If it's a single.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's happy it can it get its happy to have large wait as long as a set Sigma is saturated.",
                    "label": 0
                },
                {
                    "sent": "So it's going to tend to move towards sparse activations.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that's interesting is that these two things are really the same complexity, so it's no more.",
                    "label": 0
                },
                {
                    "sent": "It looks.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at it.",
                    "label": 0
                },
                {
                    "sent": "If you look at this you say wow, that's going to be complicated.",
                    "label": 0
                },
                {
                    "sent": "That's going to be costly to compute, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's?",
                    "label": 0
                },
                {
                    "sent": "It's for being his normal the Jacobian matrix, but it's not.",
                    "label": 1
                },
                {
                    "sent": "It's not, it's really the same complexity as a normal weight decay.",
                    "label": 0
                },
                {
                    "sent": "Alright well well yeah.",
                    "label": 0
                },
                {
                    "sent": "Well you ready I mean this these two cases for value there's one where it's one and there's one way zero.",
                    "label": 0
                },
                {
                    "sent": "OK but if it's zero it will be happy as well so.",
                    "label": 0
                },
                {
                    "sent": "So it will be happy if it has more than being 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so when they get there, how it gets a great into that.",
                    "label": 0
                },
                {
                    "sent": "That's a good question though, but I mean how it gets, how it gets to get raised to 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So good point, there contracted autoencoders with values.",
                    "label": 0
                },
                {
                    "sent": "I don't know if we should try that.",
                    "label": 0
                },
                {
                    "sent": "I said this research was with before values were became so popular.",
                    "label": 0
                },
                {
                    "sent": "And before dropout became so popular that it had a role to play for dropout becoming popular, 'cause there was a precursor to drop out.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm OK, just to mention there was an extension I won't spend much time on this.",
                    "label": 0
                },
                {
                    "sent": "It's a higher order contractive outline quarter.",
                    "label": 1
                },
                {
                    "sent": "Here we penalize Jacobian, but we could say we could try to penalize the higher order derivatives right?",
                    "label": 1
                },
                {
                    "sent": "This came just from it from a Taylor series expansion so we could we could penalize also.",
                    "label": 1
                },
                {
                    "sent": "The the the High order derivative.",
                    "label": 0
                },
                {
                    "sent": "So the second derivative the problem is the first area to hear the Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, the second derivative is that three tensor that is getting a little bit too crazy and unlikely to deal with.",
                    "label": 0
                },
                {
                    "sent": "So what we did instead was we still use this penalty here analytically for the Jacobian, for the for the.",
                    "label": 0
                },
                {
                    "sent": "First derivative and for the second derivative, well, we use a stochastic approximation and the stochastic saying if you want the second derivative to be to be small, well, you can do is stochastically by saying I want the first derivative at X and I want the first rative at X plus some noise to be close.",
                    "label": 1
                },
                {
                    "sent": "OK. That's that's alright.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Uses both stochastic and analytic regularization simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is some filters that we we obtain on this work, where this contractive autoencoder C + H. The orders he and some filters we get for that, so they tend to be cleaner here, although it's a little more involved computationally.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now the manifold.",
                    "label": 0
                },
                {
                    "sent": "Let's get back to our manifold.",
                    "label": 0
                },
                {
                    "sent": "I give a very brief manifold interpretation of the denoising autoencoder, so now I'm going to try to give a manifold interpretation of the contract if autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Again, we're trying to think of what's happened, what might happen if our true data distribution is follows as a low dimensional manifold, right so?",
                    "label": 0
                },
                {
                    "sent": "Let's look at the two terms of the contract.",
                    "label": 0
                },
                {
                    "sent": "If outside corner, you have a reconstruction term and that's being traded off here.",
                    "label": 0
                },
                {
                    "sent": "With this hyperparameter Lambda for a contraction term, OK, so.",
                    "label": 0
                },
                {
                    "sent": "The reconstruction.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's take some point some some of our training point from our training data.",
                    "label": 0
                },
                {
                    "sent": "And look at what's happening if.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the pressures on that representation being being changed, sensitive to directions when when we move if we do infinitesimal moves at this point in this direction, OK?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this direction, so we're looking for considering the direction that's parallel to the manifold and one that's orthogonal to my phone.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is?",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "OK, get back here.",
                    "label": 0
                },
                {
                    "sent": "What sites you have to do.",
                    "label": 0
                },
                {
                    "sent": "Thank you, and often I think for this animation he did that slide, it worked.",
                    "label": 0
                },
                {
                    "sent": "If we look at this contract if penalty, this contractive penalty is going to try to contract the sensitivity in all directions.",
                    "label": 0
                },
                {
                    "sent": "It's isotropic, right?",
                    "label": 0
                },
                {
                    "sent": "It's isotropic, so it's going to contract the sensitivity in all directions, so it's going to try to make that guy very small and it will succeed.",
                    "label": 0
                },
                {
                    "sent": "That's why we're taking it away.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it will also try to do it for that guy.",
                    "label": 0
                },
                {
                    "sent": "OK so it will try to make it to contract contract contract contract.",
                    "label": 0
                },
                {
                    "sent": "But the.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is it will.",
                    "label": 0
                },
                {
                    "sent": "If it does, it means that the system won't be able to distinguish between those two points, right?",
                    "label": 0
                },
                {
                    "sent": "So it will go against the reconstruction costs, the contracted pressure, which is isotropic, gets countered by the reconstruction pressure, and it gets counted only in the directions that are along the manifold.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can go a little further with this manifold analysis.",
                    "label": 0
                },
                {
                    "sent": "We can look at our Jacobian that's measures how sensitive our representation is to all input direction changes.",
                    "label": 0
                },
                {
                    "sent": "Once it's learned yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, right so OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's very important and there's something that I didn't mention here.",
                    "label": 0
                },
                {
                    "sent": "There was a warning slide.",
                    "label": 0
                },
                {
                    "sent": "It was it in that one.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah, I had it in there.",
                    "label": 0
                },
                {
                    "sent": "I remember I saw this recently.",
                    "label": 0
                },
                {
                    "sent": "It's in the it's a little area alright, OK, in red.",
                    "label": 0
                },
                {
                    "sent": "Thing in red warning may require tide waits.",
                    "label": 0
                },
                {
                    "sent": "That's to address what you're what you're which is posing as a problem.",
                    "label": 0
                },
                {
                    "sent": "So what is what is it's actually?",
                    "label": 0
                },
                {
                    "sent": "It's something that I never quite liked about the contractive autoencoder to be honest, is that it requires tide waits to work contrary to the denoising autoencoder which you can do sideways on tideways encoder decoder.",
                    "label": 0
                },
                {
                    "sent": "Completely different?",
                    "label": 0
                },
                {
                    "sent": "No problem.",
                    "label": 0
                },
                {
                    "sent": "OK, but there for the contractive autoencoder to make sense it requires tide waits because otherwise if the weights are not tide, what it can do?",
                    "label": 0
                },
                {
                    "sent": "It can contract that 202.",
                    "label": 0
                },
                {
                    "sent": "Just by taking a very, very, very small encoder, weights coefficient, global coefficient, and it can blow the thing up again upon reconstruction in the decoder, so it can satisfy both criteria perfectly.",
                    "label": 0
                },
                {
                    "sent": "So did this.",
                    "label": 0
                },
                {
                    "sent": "There needs to be a way to kind of constrain the weights, and this is done kind of indirectly if you have tide waits because you can't play the game of having encoder ways to be super small and decoder way to be super large.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right.",
                    "label": 0
                },
                {
                    "sent": "You could use weight constraints, yeah?",
                    "label": 0
                },
                {
                    "sent": "But maybe just if you if you.",
                    "label": 0
                },
                {
                    "sent": "If you do, we weight decay on the reconstruction weights?",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably because if you prevent the reconstruction weight from growing too much, that will address that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh my time is almost up.",
                    "label": 0
                },
                {
                    "sent": "My time is up.",
                    "label": 0
                },
                {
                    "sent": "OK, I know, is I?",
                    "label": 0
                },
                {
                    "sent": "Yes, you have to go through this jumping of arrows for another half hour.",
                    "label": 0
                },
                {
                    "sent": "OK, so just this very quick side because I think it can you give me like 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "If we look at the Jacobian which encompasses the sensitivity of our encoding of an encoder of encoding with respect to variations in input directions, we can try to analyze it and no good way to analyze.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is to do a a singular value decomposition of it and then our singular value spectrum will tell us.",
                    "label": 0
                },
                {
                    "sent": "Which directions it is most sensitive to on which it is not right?",
                    "label": 0
                },
                {
                    "sent": "So that's nice analysis we can make, so we didn't make that analysis an.",
                    "label": 0
                },
                {
                    "sent": "Actually if we look at between autoencoder, regular autoencoders, contractive autoencoder, these are the singular value spectrums.",
                    "label": 0
                },
                {
                    "sent": "There averaged all in an average over over the data set.",
                    "label": 0
                },
                {
                    "sent": "But you see that differ.",
                    "label": 0
                },
                {
                    "sent": "The regular autoencoder singular values are kind of as many that are high, right?",
                    "label": 0
                },
                {
                    "sent": "But other contracted autoencoder.",
                    "label": 0
                },
                {
                    "sent": "It decreases quite rapidly, So what you'd expect if you would really had a a manifold thing if it was really projection on the money flow.",
                    "label": 0
                },
                {
                    "sent": "If you think of this of doing this with the PCA, what you get for PCA, you get K values that are non zero and all the others that are zero because you have a K dimensional manifold in effect that you're projecting onto.",
                    "label": 0
                },
                {
                    "sent": "So here we see this is this is local at every point, so the direction is different at every point.",
                    "label": 0
                },
                {
                    "sent": "It's like a local PC but all parameterized right and not based on neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So we see that the.",
                    "label": 0
                },
                {
                    "sent": "This is consistent with the manifold hypothesis in the sense that.",
                    "label": 0
                },
                {
                    "sent": "It learned to have few very few directions to which which it is sensitive, and these directions will vary from point to point, but there still be an average, always a few small number of directions.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to compare local PCA where we take the neighbors, remember.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Member neighbors right.",
                    "label": 0
                },
                {
                    "sent": "OK, so we take the neighbors an do look at the principle directions, so do the same trick there.",
                    "label": 0
                },
                {
                    "sent": "Do the PCA so we find the principle directions around that image.",
                    "label": 1
                },
                {
                    "sent": "That's what the principle directions we get, so it's kinda blurry noisy if we do it on a trained contractive auto encoders.",
                    "label": 0
                },
                {
                    "sent": "So that's what the contract, if that's the direction that the contractive autoencoder learn to be sensitive to.",
                    "label": 1
                },
                {
                    "sent": "Well, it's much more sensible, right?",
                    "label": 0
                },
                {
                    "sent": "And the main reason, I believe is it's we're not basing it on explicit neighbors or pairs of points, not basing it on these neighborhoods that are very.",
                    "label": 1
                },
                {
                    "sent": "Very noisy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to exploit that?",
                    "label": 0
                },
                {
                    "sent": "So I think I'll stop there.",
                    "label": 0
                },
                {
                    "sent": "How to exploit the learn tangents.",
                    "label": 1
                },
                {
                    "sent": "There's several ways you can exploit them.",
                    "label": 1
                },
                {
                    "sent": "One there's 22 old algorithms that were proposed by simar that can exploit the tangents to define a distance measure.",
                    "label": 0
                },
                {
                    "sent": "If we use a basic Euclidean distance measure as we said, it's your neighbor may be very not like you.",
                    "label": 0
                },
                {
                    "sent": "But if we have first captured this notion of well, you're on the manifold, then instead of taking the Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Distance between two points OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the example.",
                    "label": 0
                },
                {
                    "sent": "If you took the Euclidian distance between E&P, OK, well, it will be.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's actually the intention distance.",
                    "label": 1
                },
                {
                    "sent": "So instead of taking the Euclidean distance, we kind of take the distance along the manifold tangents.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "So that's tangent distance and then you can use.",
                    "label": 1
                },
                {
                    "sent": "You have to find a new distance.",
                    "label": 0
                },
                {
                    "sent": "You can use it like in key nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "We did some experiments, it did improve pretty much unconscious neighbors.",
                    "label": 0
                },
                {
                    "sent": "You can use the same kind of intuition and something that's called tangent propagation, which is A twist on.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The standard back propagation to take into account these notions of some directions that you want to be sensitive or insensitive to when fine tuning a deep network an at the time in 2011 that this gave us the state of the art result on permutation invariant M missed by.",
                    "label": 1
                },
                {
                    "sent": "So we trained a contractive autoencoders, extracted the tangents by using this Jacobian SDDS.",
                    "label": 1
                },
                {
                    "sent": "And use that with tangent propagation for the fine tuning.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we can do is move along tangents to allow nice nice sampling, and I guess Joshua's talk will talk a lot about that, so I think that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Good place to end.",
                    "label": 0
                },
                {
                    "sent": "Even though I had another 10 slide after that.",
                    "label": 0
                }
            ]
        }
    }
}