{
    "id": "cnjkhme4hmbrbtahegft7tc7u66vryb4",
    "title": "On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference",
    "info": {
        "author": [
            "Guy Van den Broeck, Computer Science Department, University of California, Los Angeles, UCLA"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/nips2011_broeck_inference/",
    "segmentation": [
        [
            "So I'll be talking about the completeness of lifted inference."
        ],
        [
            "Um?",
            "Oh so this is in the field of probabilistic logic, so I'll first try to introduce to you the field of probabilistic logic and what it means to do lifted inference in probabilistic logic.",
            "Then I'll talk about the two contributions of this paper, which is a compilation algorithm to do first order knowledge compilation.",
            "And a theoretical analysis of this algorithm about its completeness.",
            "So."
        ],
        [
            "First, what is probabilistic logic?"
        ],
        [
            "No promises.",
            "Logic is an extension of 1st order logic.",
            "So what is first order logic?",
            "This is a sentence in first order logic friends XY and smokes X entails smokes.",
            "Why what it means is that if two people X&Y or friends and person X smokes, then person Y will also smoke.",
            "So this exists of atoms such as friends XY, which are combined with other atoms through a logical operators such as and or and not.",
            "The X and the Y here are logical variables and they have a domain of constants associated with them.",
            "So for instance, here X&Y range over the domain of people, for instance the constants Alice and Bob.",
            "We call a formula ground if it does not contain any logical variables.",
            "So for instance, if we substitute X&Y by Alice and Bob, we get the sentence here and it's a ground formula."
        ],
        [
            "So if we make this probabilistic, we get probabilistic logic and there are many representation languages.",
            "One of them is mark of logic, so this here is a sentence in Markov logic.",
            "It's again just the normal formula in first order logic.",
            "But now there is a weight associated to the formula and you can think of the weights as kind of a probability to hire the waits.",
            "The more likely it isn't a possible world that this sentence will hold.",
            "The atoms that are represented by this logical theory, the ground atoms they represent random variables in a probabilistic sense that so they can take on the values true or false.",
            "For instance, smokes Alice.",
            "Here is a random variable.",
            "It can be true or false.",
            "What is Mark Logic Network means is that if you ground the formula so if you fill in constants for the logical variables you get ground formula which represents a factor in a propositional factor graph.",
            "So the factor graph you see here has six random variables which are the ground atoms and it has four factors which are the ground formulas and the factor tables are defined by the weights of this formula.",
            "So this is very shortly what it what probabilistic logic is."
        ],
        [
            "Now the problem in these representations is that the factor graph representation in which you could do inference really explodes very quickly, and propositional inference becomes intractable very quickly, because if you have large domain sizes, you get very large propositional factor graphs, such as the one shown here.",
            "This is for a domain size of 50 people only.",
            "You already get 2500 notes in your factor graph, and of course inference here is not possible exactly.",
            "So the solution that has been proposed to this problem is called lifted inference, and people have described it as.",
            "Exploiting symmetries during inference reasoning at the 1st order level.",
            "Reasoning about groups of objects as a whole or groups of random variables as a whole.",
            "Trying to avoid repeated computations during inference or in a more logical way, trying to mimic resolution into improving.",
            "So there is a common understanding of what lifted inference means.",
            "It's supposed to be more efficient and propositional inference, but there's really no formal definition of lifted inference."
        ],
        [
            "So the questions we try to answer in this paper or these two questions first one, what is commonly understood as lifted inference.",
            "And to answer the answer to this question, is the first contribution of this paper, which is a formal framework for lifted inference.",
            "So a definition of lifted inference in terms of the complexity of doing inference an if you're into machine learning, you can think this is similar to what popular inability is for learning.",
            "This is a formal framework to define what is lifted inference in terms of the complexity of doing inference.",
            "And then the second question we try to answer is when can a model be lifted?",
            "And to answer this question, we first extend the 1st Order Logic compilation approach to lifted inference and then after that analyze the theoretical properties of this algorithm to come up with the completeness result and in the end the takeaway message of this talk should be that probabilistic models which two logical variables per formula are liftable and this is what I'll talk about for the next part of my talk."
        ],
        [
            "So I'll explain a bit more the details of how lifted inference works in practice.",
            "More specifically, I'll talk about that."
        ],
        [
            "1st Order Knowledge Compilation approach to lifted inference.",
            "So for the past 10 years people have taken propositional inference algorithms such as variable elimination, belief propagation and lifted them to the 1st order case.",
            "And what we did this year in our each type paper is extend the knowledge compilation approach by darwich to the 1st order case and this knowledge compilation approaches is arguably the state of the art for exact inference invasion networks.",
            "And it's not also the state of the art.",
            "I would argue for lifted inference, so this approach consists of four steps.",
            "First, we take a model in first order probabilistic logic.",
            "It can be a Markov logic network apart, factor graph, or anything, and we transform it into a weighted 1st order model counting problem in first order logic and I'll explain what this is in the next slide.",
            "Then we compile this logical theory into a circuit where we can compute probability sufficiently.",
            "I will talk about these four steps now in more detail.",
            "So."
        ],
        [
            "First, what is weighted 1st order model counting?",
            "So also in logic and in model counting.",
            "Like I said before, you have these ground atoms and they represent random variables.",
            "So you could make a list of all the possible assignments to these random variables and you would call them possible worlds or in logic layer called interpretations.",
            "So here you see three of many interpretations of the logical theory had before, where 0 means that the random variable gets values false and otherwise true."
        ],
        [
            "Now, given a logic theory, we can check whether the theory is satisfied by the interpretation.",
            "So whether it holds this formula in truth value assignments to these variables.",
            "So for instance, the 1st and the last interpretation they satisfy the theory.",
            "The middle one doesn't, and these interpretations that satisfy the theory or called models in logic.",
            "Now we."
        ],
        [
            "The 1st order model counting we also associate await function to all the predicates.",
            "So here you assign a weight to the variables given their assignments.",
            "You multiply them and this defines a weight for every model and then finally if we sum up."
        ],
        [
            "All these weights we get the weighted 1st order model count of the theory.",
            "And yeah, you might notice that this is actually very similar to computing the partition function of a graphical model."
        ],
        [
            "So this is first order model counting and like I said we take."
        ],
        [
            "A first order problematic logic model we transform."
        ],
        [
            "It into a weighted 1st order model counting problem in first order logic where the weighted 1st order model count is the same as the partition function of the probabilistic model, and how this is done in detail is also explained in the paper."
        ],
        [
            "Now what people for already 10 years have been doing is given such a first order model grounded to propositional logic by filling in each of the constants and then doing inference in the propositional model.",
            "So for instance, if you ground this theory you could compile it into a logical DNF circuit.",
            "This is the knowledge compilation approach which induces an arithmetic circuits which you can evaluate to compute probabilities.",
            "The problem is though, that for if you have domains that change for instance.",
            "That increase in size.",
            "You will have to recompile the circuit every time and Furthermore the size of the circuit will grow exponentially with the size of the domains.",
            "So this is a problem which makes propositional inference intractable."
        ],
        [
            "And the approach we take is first order knowledge compilation.",
            "So we compile this first order model to a first order circuit which is independent of the domain size of the logical variables.",
            "And then once we have this circuit, we."
        ],
        [
            "Evaluated for a given domain size and evaluation is polynomial in this domain size so we can compute these probabilities and this weighted 1st order model counts very efficiently in these circuits, and because because we kind of generalize to the 1st order case, this is a lifted inference approach and we avoid the problems we get when we ground and have to do propositional inference."
        ],
        [
            "So this was a brief overview of the lifted inference approach that I I will talk about.",
            "Next I will give you a little more detail about how this compilation happens from first order logic to these circuits."
        ],
        [
            "So in our each paper this year we propose 6 compilation rules that take as an inputs such a first order logic theory and that has an output, a first order circuit and in this circuit computing weighted model accounts and therefore computing probabilities or partition functions is very efficient.",
            "Each compilation rule recursively compiles a simpler theory, but I'll just give you an example to explain.",
            "It's a very simple rule called independence, so here you have a theory, not friends Bob X, which means that Bob has no friends and then smokes.",
            "X entails not friends Alice X, which means that if you're a smoker, Ellis will not be friends with you.",
            "So if you would ground this through the factor graph representation, you would see that it actually consists of two independent parts that are not connected and also in a logical sense these two formulas or independent.",
            "So what we?"
        ],
        [
            "Can do is can join them.",
            "Computer weighted model counts of the two theories in dependently and then just multiply them.",
            "So this is a very simple rule which allows you to efficiently compute these counts."
        ],
        [
            "And what we did in this paper is add to the six inference rules, another rule, domain recursion, which in itself is rather simple, but it will have powerful consequences.",
            "And I again will not explain it in detail, but just give you an example.",
            "So this formula here, friends XY entails friends yx it it makes sure that the friends relation is a symmetric relation.",
            "So that's an important concept in mathematics.",
            "But such a formula could not be lifted by existing existing lifted inference approaches.",
            "They would have to ground this out to do inference.",
            "The approach we take is to split up the domain of people into two disjoint domains, one containing only a constant C. It can be any arbitrary person, and then all the other constants.",
            "And if you do this, you get."
        ],
        [
            "Three independent subseries.",
            "One where both of the logical variables are equal to the constant."
        ],
        [
            "One word are both not equal to the constants."
        ],
        [
            "And then one which covers all other cases and well in shorts.",
            "The story here is that computing the weighted model count of the 1st and the last theory is very easy because there are really easy problems from an old compilation POV and the second theory is actually the same as the original one.",
            "Only the domain sizes have decreased by one.",
            "So this is one rule that we introduced."
        ],
        [
            "We also did some experiments with this new inference algorithm comparing C 2D, which is propositional knowledge compilation to the existing knowledge compilation algorithm and then finally at the bottom our knowledge completion algorithm that we proposed in this paper.",
            "And as you can see, propositional inference is a lot less scalable than then lifted inference.",
            "But also our new approach scales a lot better than the existing one, and this is on the X axis.",
            "It's showing the domain size on the Y axis.",
            "It's showing the runtime of the inference algorithm."
        ],
        [
            "OK, now that you have a rough idea of what knowledge compilation and lifted inference is, we can do some theoretical analysis of the algorithm with respect to its completeness.",
            "And you might have noticed that."
        ],
        [
            "I still haven't formally defined what is lifted inference yet, and actually this is also the first contribution of this paper.",
            "This definition of domain lifted inference.",
            "It's it's one choice for defining lifted inference, and it says that an algorithm is a domain lifted promise inference algorithm if computing the probability of a query given evidence in a model is polynomial in the domain sizes of the logical variables in the query, the evidence and the model.",
            "So this only talks about complexity of inference with respect to the domain sizes.",
            "It is possible inference is exponential in the size of the query.",
            "The evidence and model.",
            "For instance, in the number of predicates, the number of atoms and number of arguments, the number of formulas, or even the number of constants that are explicitly mentioned in the model.",
            "So the motivation for focusing on the domain size for this definition is that actually it is the large domain sizes that leads to intractable propositional representations.",
            "So this is really the bottleneck in applying in doing inference in first order models it's these domain sizes.",
            "No."
        ],
        [
            "Given this definition of lifted inference, we can talk about the completeness of an algorithm, and we call well a procedure that is domain lifted for all models in a certain class of models is called complete for this class of models.",
            "And intuitively this means that an algorithm is complete for M if all models in Mr. Liftable according to the definition of liftable.",
            "And then well.",
            "The problem so far was that there were no completeness results at all for existing algorithms.",
            "So until now, if you gave me a model and you asked me can you apply lifted inference, or will you have to ground to propositional inference, then I could not tell you the answer.",
            "I would just have to run the algorithm, see if it got stuck somewhere, and then tell you yeah it got stuck.",
            "So yeah, we cannot handle this kind of model."
        ],
        [
            "But now with this paper, the most important contribution is this completeness result.",
            "First we define a class of models which is called kW FOMC, which is all the weighted model counting problems with logical theories that have up to K logical variables per formula.",
            "And while the 1st result is about the existing algorithms, so the existing algorithms are complete domain lifted for one W FMC.",
            "So if you have one logical variable per formula.",
            "But this is of course a very limited class of models.",
            "So for instance, these formulas cannot be lifted by existing approaches.",
            "The first one is a symmetric relation, the second one that if X is a parent of Y, white cannot be the parent of X is an antisymmetric relation.",
            "And then the last one is a total relation.",
            "So these are very important mathematical concepts and they could not be handled by lifted inference algorithms.",
            "And then the most important theorem in the paper is that we proved that the CR 2.",
            "So the existing rules combined with the new rule we we proposed in this paper is a complete domain list algorithm for 2 W FOMC.",
            "So for theories with two logical variables per formula.",
            "So."
        ],
        [
            "You might wonder why is this important?",
            "Well, these are sufficient conditions for domain lifted inference, so they are sufficient conditions for saying when something is liftable and it's really the first completeness result so far for probabilistic lifted inference, and I would also argue that this class of models is really nontrivial.",
            "It contains these very useful concepts, such as symmetric total relations.",
            "If you compare two existing algorithms before knowledge compilation, knowledge completion could already lift many more problems than previous approaches.",
            "And now with this new algorithm we proposed in this paper we can solve even more now the entire class of 2 W FOMC.",
            "Of course there are still many open questions such as are there other classes that can be lifted or their negative results of classes that could never be lifted?"
        ],
        [
            "So now to conclude my talk."
        ],
        [
            "We have three contributions.",
            "The first one is a formal framework for lifted inference, which defines lifted inference in terms of the complexity of doing inference, which you might think is similar to a popular inability for learning.",
            "We propose a new compilation rule for 1st order Knowledge compilation, which both empirically is proving its use and also theoretically in the third contribution allows us to prove the completeness of our algorithm for the first time.",
            "And in short, to take away messages that this class of models is liftable.",
            "And this is really the first nontrivial class of problems where we can say we can apply lifted inference to it.",
            "And then."
        ],
        [
            "Let me also point you towards my poster today, so tonight's and mentioned that we have a website and an implementation of the algorithm.",
            "Here's the URL, but you can also find it by Googling W FOMC so that concludes my talk, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be talking about the completeness of lifted inference.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Oh so this is in the field of probabilistic logic, so I'll first try to introduce to you the field of probabilistic logic and what it means to do lifted inference in probabilistic logic.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk about the two contributions of this paper, which is a compilation algorithm to do first order knowledge compilation.",
                    "label": 0
                },
                {
                    "sent": "And a theoretical analysis of this algorithm about its completeness.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, what is probabilistic logic?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No promises.",
                    "label": 0
                },
                {
                    "sent": "Logic is an extension of 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "So what is first order logic?",
                    "label": 0
                },
                {
                    "sent": "This is a sentence in first order logic friends XY and smokes X entails smokes.",
                    "label": 0
                },
                {
                    "sent": "Why what it means is that if two people X&Y or friends and person X smokes, then person Y will also smoke.",
                    "label": 0
                },
                {
                    "sent": "So this exists of atoms such as friends XY, which are combined with other atoms through a logical operators such as and or and not.",
                    "label": 0
                },
                {
                    "sent": "The X and the Y here are logical variables and they have a domain of constants associated with them.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here X&Y range over the domain of people, for instance the constants Alice and Bob.",
                    "label": 1
                },
                {
                    "sent": "We call a formula ground if it does not contain any logical variables.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we substitute X&Y by Alice and Bob, we get the sentence here and it's a ground formula.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we make this probabilistic, we get probabilistic logic and there are many representation languages.",
                    "label": 0
                },
                {
                    "sent": "One of them is mark of logic, so this here is a sentence in Markov logic.",
                    "label": 0
                },
                {
                    "sent": "It's again just the normal formula in first order logic.",
                    "label": 0
                },
                {
                    "sent": "But now there is a weight associated to the formula and you can think of the weights as kind of a probability to hire the waits.",
                    "label": 0
                },
                {
                    "sent": "The more likely it isn't a possible world that this sentence will hold.",
                    "label": 0
                },
                {
                    "sent": "The atoms that are represented by this logical theory, the ground atoms they represent random variables in a probabilistic sense that so they can take on the values true or false.",
                    "label": 0
                },
                {
                    "sent": "For instance, smokes Alice.",
                    "label": 0
                },
                {
                    "sent": "Here is a random variable.",
                    "label": 0
                },
                {
                    "sent": "It can be true or false.",
                    "label": 0
                },
                {
                    "sent": "What is Mark Logic Network means is that if you ground the formula so if you fill in constants for the logical variables you get ground formula which represents a factor in a propositional factor graph.",
                    "label": 1
                },
                {
                    "sent": "So the factor graph you see here has six random variables which are the ground atoms and it has four factors which are the ground formulas and the factor tables are defined by the weights of this formula.",
                    "label": 0
                },
                {
                    "sent": "So this is very shortly what it what probabilistic logic is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem in these representations is that the factor graph representation in which you could do inference really explodes very quickly, and propositional inference becomes intractable very quickly, because if you have large domain sizes, you get very large propositional factor graphs, such as the one shown here.",
                    "label": 0
                },
                {
                    "sent": "This is for a domain size of 50 people only.",
                    "label": 0
                },
                {
                    "sent": "You already get 2500 notes in your factor graph, and of course inference here is not possible exactly.",
                    "label": 0
                },
                {
                    "sent": "So the solution that has been proposed to this problem is called lifted inference, and people have described it as.",
                    "label": 0
                },
                {
                    "sent": "Exploiting symmetries during inference reasoning at the 1st order level.",
                    "label": 0
                },
                {
                    "sent": "Reasoning about groups of objects as a whole or groups of random variables as a whole.",
                    "label": 1
                },
                {
                    "sent": "Trying to avoid repeated computations during inference or in a more logical way, trying to mimic resolution into improving.",
                    "label": 1
                },
                {
                    "sent": "So there is a common understanding of what lifted inference means.",
                    "label": 0
                },
                {
                    "sent": "It's supposed to be more efficient and propositional inference, but there's really no formal definition of lifted inference.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the questions we try to answer in this paper or these two questions first one, what is commonly understood as lifted inference.",
                    "label": 1
                },
                {
                    "sent": "And to answer the answer to this question, is the first contribution of this paper, which is a formal framework for lifted inference.",
                    "label": 0
                },
                {
                    "sent": "So a definition of lifted inference in terms of the complexity of doing inference an if you're into machine learning, you can think this is similar to what popular inability is for learning.",
                    "label": 0
                },
                {
                    "sent": "This is a formal framework to define what is lifted inference in terms of the complexity of doing inference.",
                    "label": 1
                },
                {
                    "sent": "And then the second question we try to answer is when can a model be lifted?",
                    "label": 0
                },
                {
                    "sent": "And to answer this question, we first extend the 1st Order Logic compilation approach to lifted inference and then after that analyze the theoretical properties of this algorithm to come up with the completeness result and in the end the takeaway message of this talk should be that probabilistic models which two logical variables per formula are liftable and this is what I'll talk about for the next part of my talk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll explain a bit more the details of how lifted inference works in practice.",
                    "label": 0
                },
                {
                    "sent": "More specifically, I'll talk about that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st Order Knowledge Compilation approach to lifted inference.",
                    "label": 1
                },
                {
                    "sent": "So for the past 10 years people have taken propositional inference algorithms such as variable elimination, belief propagation and lifted them to the 1st order case.",
                    "label": 1
                },
                {
                    "sent": "And what we did this year in our each type paper is extend the knowledge compilation approach by darwich to the 1st order case and this knowledge compilation approaches is arguably the state of the art for exact inference invasion networks.",
                    "label": 0
                },
                {
                    "sent": "And it's not also the state of the art.",
                    "label": 0
                },
                {
                    "sent": "I would argue for lifted inference, so this approach consists of four steps.",
                    "label": 0
                },
                {
                    "sent": "First, we take a model in first order probabilistic logic.",
                    "label": 0
                },
                {
                    "sent": "It can be a Markov logic network apart, factor graph, or anything, and we transform it into a weighted 1st order model counting problem in first order logic and I'll explain what this is in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Then we compile this logical theory into a circuit where we can compute probability sufficiently.",
                    "label": 0
                },
                {
                    "sent": "I will talk about these four steps now in more detail.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, what is weighted 1st order model counting?",
                    "label": 1
                },
                {
                    "sent": "So also in logic and in model counting.",
                    "label": 0
                },
                {
                    "sent": "Like I said before, you have these ground atoms and they represent random variables.",
                    "label": 0
                },
                {
                    "sent": "So you could make a list of all the possible assignments to these random variables and you would call them possible worlds or in logic layer called interpretations.",
                    "label": 0
                },
                {
                    "sent": "So here you see three of many interpretations of the logical theory had before, where 0 means that the random variable gets values false and otherwise true.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, given a logic theory, we can check whether the theory is satisfied by the interpretation.",
                    "label": 0
                },
                {
                    "sent": "So whether it holds this formula in truth value assignments to these variables.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the 1st and the last interpretation they satisfy the theory.",
                    "label": 0
                },
                {
                    "sent": "The middle one doesn't, and these interpretations that satisfy the theory or called models in logic.",
                    "label": 1
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The 1st order model counting we also associate await function to all the predicates.",
                    "label": 1
                },
                {
                    "sent": "So here you assign a weight to the variables given their assignments.",
                    "label": 1
                },
                {
                    "sent": "You multiply them and this defines a weight for every model and then finally if we sum up.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these weights we get the weighted 1st order model count of the theory.",
                    "label": 0
                },
                {
                    "sent": "And yeah, you might notice that this is actually very similar to computing the partition function of a graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is first order model counting and like I said we take.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A first order problematic logic model we transform.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It into a weighted 1st order model counting problem in first order logic where the weighted 1st order model count is the same as the partition function of the probabilistic model, and how this is done in detail is also explained in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what people for already 10 years have been doing is given such a first order model grounded to propositional logic by filling in each of the constants and then doing inference in the propositional model.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if you ground this theory you could compile it into a logical DNF circuit.",
                    "label": 0
                },
                {
                    "sent": "This is the knowledge compilation approach which induces an arithmetic circuits which you can evaluate to compute probabilities.",
                    "label": 1
                },
                {
                    "sent": "The problem is though, that for if you have domains that change for instance.",
                    "label": 0
                },
                {
                    "sent": "That increase in size.",
                    "label": 0
                },
                {
                    "sent": "You will have to recompile the circuit every time and Furthermore the size of the circuit will grow exponentially with the size of the domains.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem which makes propositional inference intractable.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the approach we take is first order knowledge compilation.",
                    "label": 1
                },
                {
                    "sent": "So we compile this first order model to a first order circuit which is independent of the domain size of the logical variables.",
                    "label": 1
                },
                {
                    "sent": "And then once we have this circuit, we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluated for a given domain size and evaluation is polynomial in this domain size so we can compute these probabilities and this weighted 1st order model counts very efficiently in these circuits, and because because we kind of generalize to the 1st order case, this is a lifted inference approach and we avoid the problems we get when we ground and have to do propositional inference.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was a brief overview of the lifted inference approach that I I will talk about.",
                    "label": 0
                },
                {
                    "sent": "Next I will give you a little more detail about how this compilation happens from first order logic to these circuits.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our each paper this year we propose 6 compilation rules that take as an inputs such a first order logic theory and that has an output, a first order circuit and in this circuit computing weighted model accounts and therefore computing probabilities or partition functions is very efficient.",
                    "label": 0
                },
                {
                    "sent": "Each compilation rule recursively compiles a simpler theory, but I'll just give you an example to explain.",
                    "label": 1
                },
                {
                    "sent": "It's a very simple rule called independence, so here you have a theory, not friends Bob X, which means that Bob has no friends and then smokes.",
                    "label": 0
                },
                {
                    "sent": "X entails not friends Alice X, which means that if you're a smoker, Ellis will not be friends with you.",
                    "label": 0
                },
                {
                    "sent": "So if you would ground this through the factor graph representation, you would see that it actually consists of two independent parts that are not connected and also in a logical sense these two formulas or independent.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can do is can join them.",
                    "label": 0
                },
                {
                    "sent": "Computer weighted model counts of the two theories in dependently and then just multiply them.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple rule which allows you to efficiently compute these counts.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we did in this paper is add to the six inference rules, another rule, domain recursion, which in itself is rather simple, but it will have powerful consequences.",
                    "label": 1
                },
                {
                    "sent": "And I again will not explain it in detail, but just give you an example.",
                    "label": 0
                },
                {
                    "sent": "So this formula here, friends XY entails friends yx it it makes sure that the friends relation is a symmetric relation.",
                    "label": 0
                },
                {
                    "sent": "So that's an important concept in mathematics.",
                    "label": 0
                },
                {
                    "sent": "But such a formula could not be lifted by existing existing lifted inference approaches.",
                    "label": 0
                },
                {
                    "sent": "They would have to ground this out to do inference.",
                    "label": 1
                },
                {
                    "sent": "The approach we take is to split up the domain of people into two disjoint domains, one containing only a constant C. It can be any arbitrary person, and then all the other constants.",
                    "label": 0
                },
                {
                    "sent": "And if you do this, you get.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three independent subseries.",
                    "label": 0
                },
                {
                    "sent": "One where both of the logical variables are equal to the constant.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One word are both not equal to the constants.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then one which covers all other cases and well in shorts.",
                    "label": 0
                },
                {
                    "sent": "The story here is that computing the weighted model count of the 1st and the last theory is very easy because there are really easy problems from an old compilation POV and the second theory is actually the same as the original one.",
                    "label": 0
                },
                {
                    "sent": "Only the domain sizes have decreased by one.",
                    "label": 0
                },
                {
                    "sent": "So this is one rule that we introduced.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also did some experiments with this new inference algorithm comparing C 2D, which is propositional knowledge compilation to the existing knowledge compilation algorithm and then finally at the bottom our knowledge completion algorithm that we proposed in this paper.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, propositional inference is a lot less scalable than then lifted inference.",
                    "label": 0
                },
                {
                    "sent": "But also our new approach scales a lot better than the existing one, and this is on the X axis.",
                    "label": 0
                },
                {
                    "sent": "It's showing the domain size on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "It's showing the runtime of the inference algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now that you have a rough idea of what knowledge compilation and lifted inference is, we can do some theoretical analysis of the algorithm with respect to its completeness.",
                    "label": 0
                },
                {
                    "sent": "And you might have noticed that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I still haven't formally defined what is lifted inference yet, and actually this is also the first contribution of this paper.",
                    "label": 0
                },
                {
                    "sent": "This definition of domain lifted inference.",
                    "label": 0
                },
                {
                    "sent": "It's it's one choice for defining lifted inference, and it says that an algorithm is a domain lifted promise inference algorithm if computing the probability of a query given evidence in a model is polynomial in the domain sizes of the logical variables in the query, the evidence and the model.",
                    "label": 1
                },
                {
                    "sent": "So this only talks about complexity of inference with respect to the domain sizes.",
                    "label": 1
                },
                {
                    "sent": "It is possible inference is exponential in the size of the query.",
                    "label": 0
                },
                {
                    "sent": "The evidence and model.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the number of predicates, the number of atoms and number of arguments, the number of formulas, or even the number of constants that are explicitly mentioned in the model.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for focusing on the domain size for this definition is that actually it is the large domain sizes that leads to intractable propositional representations.",
                    "label": 0
                },
                {
                    "sent": "So this is really the bottleneck in applying in doing inference in first order models it's these domain sizes.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given this definition of lifted inference, we can talk about the completeness of an algorithm, and we call well a procedure that is domain lifted for all models in a certain class of models is called complete for this class of models.",
                    "label": 1
                },
                {
                    "sent": "And intuitively this means that an algorithm is complete for M if all models in Mr. Liftable according to the definition of liftable.",
                    "label": 0
                },
                {
                    "sent": "And then well.",
                    "label": 1
                },
                {
                    "sent": "The problem so far was that there were no completeness results at all for existing algorithms.",
                    "label": 0
                },
                {
                    "sent": "So until now, if you gave me a model and you asked me can you apply lifted inference, or will you have to ground to propositional inference, then I could not tell you the answer.",
                    "label": 0
                },
                {
                    "sent": "I would just have to run the algorithm, see if it got stuck somewhere, and then tell you yeah it got stuck.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we cannot handle this kind of model.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now with this paper, the most important contribution is this completeness result.",
                    "label": 0
                },
                {
                    "sent": "First we define a class of models which is called kW FOMC, which is all the weighted model counting problems with logical theories that have up to K logical variables per formula.",
                    "label": 1
                },
                {
                    "sent": "And while the 1st result is about the existing algorithms, so the existing algorithms are complete domain lifted for one W FMC.",
                    "label": 0
                },
                {
                    "sent": "So if you have one logical variable per formula.",
                    "label": 0
                },
                {
                    "sent": "But this is of course a very limited class of models.",
                    "label": 0
                },
                {
                    "sent": "So for instance, these formulas cannot be lifted by existing approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one is a symmetric relation, the second one that if X is a parent of Y, white cannot be the parent of X is an antisymmetric relation.",
                    "label": 0
                },
                {
                    "sent": "And then the last one is a total relation.",
                    "label": 0
                },
                {
                    "sent": "So these are very important mathematical concepts and they could not be handled by lifted inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then the most important theorem in the paper is that we proved that the CR 2.",
                    "label": 0
                },
                {
                    "sent": "So the existing rules combined with the new rule we we proposed in this paper is a complete domain list algorithm for 2 W FOMC.",
                    "label": 0
                },
                {
                    "sent": "So for theories with two logical variables per formula.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You might wonder why is this important?",
                    "label": 0
                },
                {
                    "sent": "Well, these are sufficient conditions for domain lifted inference, so they are sufficient conditions for saying when something is liftable and it's really the first completeness result so far for probabilistic lifted inference, and I would also argue that this class of models is really nontrivial.",
                    "label": 1
                },
                {
                    "sent": "It contains these very useful concepts, such as symmetric total relations.",
                    "label": 0
                },
                {
                    "sent": "If you compare two existing algorithms before knowledge compilation, knowledge completion could already lift many more problems than previous approaches.",
                    "label": 0
                },
                {
                    "sent": "And now with this new algorithm we proposed in this paper we can solve even more now the entire class of 2 W FOMC.",
                    "label": 0
                },
                {
                    "sent": "Of course there are still many open questions such as are there other classes that can be lifted or their negative results of classes that could never be lifted?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now to conclude my talk.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have three contributions.",
                    "label": 0
                },
                {
                    "sent": "The first one is a formal framework for lifted inference, which defines lifted inference in terms of the complexity of doing inference, which you might think is similar to a popular inability for learning.",
                    "label": 1
                },
                {
                    "sent": "We propose a new compilation rule for 1st order Knowledge compilation, which both empirically is proving its use and also theoretically in the third contribution allows us to prove the completeness of our algorithm for the first time.",
                    "label": 0
                },
                {
                    "sent": "And in short, to take away messages that this class of models is liftable.",
                    "label": 0
                },
                {
                    "sent": "And this is really the first nontrivial class of problems where we can say we can apply lifted inference to it.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me also point you towards my poster today, so tonight's and mentioned that we have a website and an implementation of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here's the URL, but you can also find it by Googling W FOMC so that concludes my talk, thanks.",
                    "label": 0
                }
            ]
        }
    }
}