{
    "id": "g2vjo24uwewsehr2c3vob7rkaskef7b3",
    "title": "Sharp analysis of low-rank kernel matrix approximations",
    "info": {
        "author": [
            "Francis R. Bach, INRIA - SIERRA project-team"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_bach_analysis/",
    "segmentation": [
        [
            "Today I'm going to talk about kernel methods, so before I start maybe a small comments or those of you have kids will be know that when they're young they're real sponges.",
            "They believe what they really believe.",
            "What you say.",
            "Then it changes, but at least for the first 10 years of life, what you say is supposed to be true.",
            "My academic life bit similar so if my first name was in 2000 at the peak of the kernel years, so in a sense I will be like my kids.",
            "I believe what I was I was told when I was a kid and academic kid.",
            "So today my main message.",
            "So I moved move down a bit.",
            "Things those days the messages really."
        ],
        [
            "Don't forget to methods.",
            "OK, so the goal of today's could show you that there is life life beyond just plain linear models and the second message of my."
        ],
        [
            "Talk is don't forget ascetic statistics analysis, so this supposed to be super old school, but typically just replacing the central limit theorem by having inequality or Bernstein doesn't buy you much.",
            "OK, it makes for Mr. Matt OK, so you have very long papers, so I've been involved in that so that criticism so applies to me, but the goal of today is to remind you of the old old results from the statistics and see how they can be combined with machine learning."
        ],
        [
            "OK, so I'm going to talk mainly about supervised machine learning related to complex optimization.",
            "And when I joined like a snips community, it was a pig.",
            "So the peak of the kernel kernel, yours, an essentially, every thought that everything could be solved using kernels OK. And it was people were doing like nonlinear methods and it was classically seen as nonparametric statistics where the number of implicit parameters were growing with the size of your problem.",
            "And typically people have convergence rates in terms of prediction error.",
            "Which of the turmoil?",
            "Enter the problem far and Alpha being often between 1/2 and one.",
            "This being said, it was good for statistics, but typically people are using using it in small scale problems, mainly because the complexity.",
            "Of algorithms for at least N square."
        ],
        [
            "OK then people moved on and since the late 2000 people are using mostly linear models and with or without spasticity, but at the end they still use a linear models.",
            "OK so we go back to classical parametric statistics where typical typically the convergence rates are between one or one over root N OK. And of course this was.",
            "Pushed by the large scale of the problem that people want you to solve, and here people have managed to be able to attain algorithms with complexity which is the size of your data and typically Orth OK. Well message about to convey in this talk is in fact that we went from naive optimization OK when we were doing kernels in the sense we were pretty naval community in terms of optimization, but now we are more refined in terms of optimization.",
            "We went back to naive statistical models and in particular somewhat, I think, naive analysis.",
            "So the goal of today is really to try to bridge that gap between kana methods and."
        ],
        [
            "An optimization, so of course I will only provide a partial answer."
        ],
        [
            "And this will be the outline of my talk.",
            "So we first by review what I mean by supervised machine learning.",
            "Then I will review existing results.",
            "I will try to give a critical view of those and then I will go over classical techniques to have efficient methods with kernels.",
            "And now then I will review also classical result from the 70s or 80s from the statistics community.",
            "And finally I will go into the topic of the talk.",
            "The actual new material which is trained to analyze low rank approximations.",
            "Well being to be fast without losing any performance.",
            "And finally if time permits, I would also consider an analysis of the choice that regulation parameter 'cause I think this is largely overlooked and this remains already the practical problem.",
            "So it's a workshop talk, so don't tend to interrupt me if you disagree.",
            "Also, if you agree of course, and ask any questions if things are not super clean."
        ],
        [
            "OK, So what I mean by supervised machine learning.",
            "So you have some elevations.",
            "Xiy Island, Zi, back in color.",
            "SpaceX I can be anything can be an image, can be a graph.",
            "You have an output, why I but you want to predict in this talk I will assume the data ID and I will predict.",
            "OK, so we're function F. That function will be belong to a Hilbert space and traditionally you can see that function as a dot product, which is your element of your Hilbert space.",
            "An feature functions cervix.",
            "OK so this field.",
            "This can be either explicit if you build it directly or implicit.",
            "So I'm going to consider the classic colorized empirical recommendation for work where I want to find the solution F had of that convex optimization problem, which is composed of a convex.",
            "That are fitting term over there, plus some regularizer.",
            "An throughout this talk.",
            "That regularizer will always be as a square norm of Hilbert space."
        ],
        [
            "So given that problem OK, you have several quantities of interest.",
            "Have so-called training costs, which is a sum of a loss or losses in the training set.",
            "This is what you have access to, and you have the testing cost.",
            "This is what you really want to minimize.",
            "We don't.",
            "We don't have access to that, and you want to minimize that.",
            "So of course there are two big questions related to that problem.",
            "First, how do you compute that?",
            "I've had OK. How do you optimize that convex optimization problem?",
            "So why convex?",
            "Because I will assume aware that the losses convex.",
            "So imagine like least squares or logistic or the ECM and the second question is are you?",
            "How can you analyze F but OK, so this was done traditionally separately, and since the last few years people have started to consider those two problems simultaneously.",
            "You both want to have a good effort in an efficient way of computing effort."
        ],
        [
            "So let's look at some nice results from a skeletal, so which are related to other results by Bushman massage.",
            "So with very very small number of assumptions on the problem.",
            "So we still could R the expected risk.",
            "And of course R is overloaded.",
            "No, no, it's not.",
            "It is expected risks are at is the empirical risk.",
            "And as I told you, our predictor will be based on the attacker regularised in particular estimation when you have.",
            "When you have our heart plus your square norm, you shouldn't.",
            "Data are bounded.",
            "You assume the Lipschitz loss like the VM and boom, what you get, you get that the excess between the regularizer risk of your predictor and the smallest possible regularised risk is bounded by something in high probability with constant OK with depend on your problem on the register, your data divided by something times N OK.",
            "So the good thing is that this corresponds to a fast rate because it depends on North.",
            "OK, but my main criticism all of that sequence of results is that at the end there is not only a."
        ],
        [
            "This number OK and Lambda OK, so we should tend to zero as we have more data or typically Lambda is smaller when you have more data because we want to adapt to your problem.",
            "So at the end if that tends to 0.",
            "Then this isn't really.",
            "Might not go to zero as fast as we can.",
            "OK, so this was of course noticed by many people and typically."
        ],
        [
            "What you do is you consider alarm data decays with North, so here this is the same result without the the important constants.",
            "So if you take the regularizer particular risk, it converges as rates Wonderland again.",
            "But because we use empirical because we use it regularize risk, you have some bias due to that Lambda term and if we want to get a good tradeoff between the bias and this over over one Lambda and then you have to take Lambda at the other one over root North.",
            "And in the worst case situation, if you don't know anything about your problem beyond the fact that it is bounded, what you get is something which is the risk of your product or minus the best possible risk of your class is bounded from above by Underoath.",
            "OK, so this is essentially a summary of the worst case results that people consider a lot in this community.",
            "Either one of one of the root N. If you don't have any assumptions and one other alarm, then if you put some Taco, mix it into the into the picture either.",
            "By adding the square norm, or maybe because the data are low dimensional."
        ],
        [
            "OK, so the amazing aspect of that framework is that this is done for the the global optimum F hat, and it turns out that you don't need that global optimum and you can find algorithms which will achieve the exact same race OK, but in a single path through the data OK and this is possible only.",
            "When are the you have an explicit description description of your features?",
            "So essentially when F is RP, then with a single path for data with complexity of PM, you can achieve the exact same right.",
            "OK, so if you make no assumptions to get one of those N, if you add the squared regularizer you get one."
        ],
        [
            "So here, but I think those results are good things.",
            "They provide a good intuition on what is hard and what is difficult, but essentially they go from either like super super bad Rate 2.",
            "What seems to be a good right but with Wanda OK in the sense there is a gap between one over root N in one of our North.",
            "And this means that it's worse.",
            "I think it's playing that gap and these correspond to what these correspond exactly to looking at what is inside the containment tricks or inside your current metrics.",
            "So essentially all the eigenvalues of the kernel matrix and of the occurrence matrix has to be have to be taken into account, and that's simply the largest one and just move the small but all of them.",
            "And the goal of today, ready to look into this in a more."
        ],
        [
            "GPL.",
            "So why kernels?",
            "OK, So what what people typically do, but I don't have kernels when you have like you have an explicit description of your features that you do have efficient optimization.",
            "As I mentioned that essentially stochastic resonance and his variance and for fixed Lambda Westie remain unclear is what should not be OK.",
            "So typically this is done by crystallization and what I really want a message thing which is very important to me is the fact that if from the start before you see a single data point.",
            "You affix the size of your representation that you fix P. Then as N grows at one point, you will under fit OK. You could achieve the.",
            "You will achieve the good companies are good.",
            "You will achieve the performance at one point and then you will have under 50 because you don't adapt to your to your data.",
            "Whereas if you do, if you scan methods were infinite."
        ],
        [
            "So linear models, then, first of course there are no few algorithms to do to do this.",
            "When N is a dentist, 12 is maybe difficult.",
            "The choice of Lambda still remains unclear, but here in terms of statistics, we can adapt to the incoming flow of data.",
            "OK, so the classical primary problem, but of course here you have higher risk of overfitting, and this is of course problem.",
            "Another another thing which I think is quite important.",
            "Like that even though you have like finite number of features.",
            "OK, just seeing them as infinite I think is good because typically in the regimes where we work on we are not exactly attaining the aseptic regime will bit in between.",
            "So seeing your features that influence dimensional or those are finite I think provide you so good good set of tools."
        ],
        [
            "So like you know also that reduce they, at least to me that provide a good attraction of high dimensional feature space is of course they allow nonlinear estimation.",
            "And this is like useful in many many fields and I'm sure there are many other wants.",
            "Is it true in computer vision, bioinformatics, or new imaging?",
            "I don't believe that you can get away only with linear models.",
            "At one point you want to add something which is a bit more flexible, and I think those fields are full of those of those problems.",
            "And of course the goal being you want to improve the predictions as an Andrews.",
            "So of course the big problem is the fact that if you tell a bit, naive if you just compute the kernel matrix boom, you get 10 square.",
            "OK, so when N is very large.",
            "This is not so possible."
        ],
        [
            "OK, so the main goal of this talk is trying to see if you can go below this quadratic complexity.",
            "I try to derive like upper bound or lower bounds.",
            "To get the fat algorithm but still preserve this nonparametric nature, if you want to be fast, I knew of Agnes algorithm that does nothing is super fast, but it doesn't do much.",
            "But you really want is to be able to achieve this nonparametric nature of consumer thirds while remaining subglottic.",
            "And this this might seem might seem quite theoretical theoretical problem, but still think it is a big particular problem.",
            "OK, so there are only few methods that people use a lot that can do to achieve this increasing number.",
            "The increasing complexity of your predictor while remaining some quadratic."
        ],
        [
            "OK, let's do let's go into more details.",
            "So first we consider the regularised minimization OK, and using the theorem then it is known that because you have an Euclidean norm over damn, you know that you know you don't.",
            "You don't need to search about your function in the full Hilbert space, you can parameterise your function as a linear combination of of your input data points.",
            "OK, so you go from a problem which can be of large size, maybe infinite to something.",
            "In our end, OK, and typically once you have this, if you define the kernel K of XX prime as being the dot product between few of X and Felix Prime, then you can express your prediction F of X as a sum of kernel function.",
            "This is very classical and now if you take that.",
            "That thing and put it back in to put it back into the problem.",
            "OK then these guys becomes K Alpha.",
            "The ice component of Kalfa and these guys become Alpha transpose Alpha.",
            "So essentially when you saw the kernel methods that is 1 possibility is to use that represented silver and you get that problem.",
            "OK, so why is the problem complicated?",
            "Because as soon as you start the problem if you write it down, the parameters are ready or when squared.",
            "So it's called the audit log."
        ],
        [
            "So essentially, all techniques that people have considered, they have to consider, like not computing the econometrics.",
            "So let's look 1st at the simplest approach to try to achieve fast fast convergence for for an algorithm, let's text Cassidy Senter.",
            "Let's try to do it directly in the Hilbert space.",
            "OK, so he sees your iteration in the Hilbert space.",
            "You go from the function at time T + 1 to the function at time T. By simply scaling the function and subtracting the gradient of the loss.",
            "Associated to your this data point, taken at your prediction at time T -- 1.",
            "OK, so here this has been noticed, of course, before is the fact that this at the end can also be penalized at iteration kernelized.",
            "Essentially, you can, as you can see at every time step you go from FT minus one 2FT by scaling FT minus one and adding that guy.",
            "OK, so this means at the end that FT can be represented as a sum of feature vectors.",
            "OK, only the 1st that you have seen so far.",
            "So now you can go take this and this and now you have this nice and simple and simple recursion for stochastic gradient in the in the alphas where the new Alpha is every time you have a new Alpha and you scare the other alphas.",
            "So what is the problem that approach the problem with that is that like action you still have that computation.",
            "This is a sum over the element, sodium and elements, so this at the end it makes oh FT at iteration T. So if you make any iterations, it is a sum of 1 + 2 + 3 + 4.",
            "Up to N. So at the end you get oh.",
            "Or N squared?",
            "OK so this has been a."
        ],
        [
            "Which is before and people have considered fancy ways or to reduce the amount of computation essentially.",
            "Either because you have the support vectors.",
            "So here maybe I should mention this.",
            "Some of the alphas will be 0 at the end if you use a hinge loss, in particular for the hinge loss, that gradient will still will often be 0 if you are in the flat part of the hinge loss.",
            "But still you will still have a linear term of support vectors in general, so you still need even if you use a hinge loss, you still need to reduce that number of support vectors and this as this is done by several people and gave like.",
            "Nice name for those clubs, those algorithms.",
            "So forget home projector olympi GSD essentially share the same type of technique.",
            "Once in awhile, OK, you remove one of the alphas.",
            "OK, you said one of the elders to zero once in awhile, or hoping controlling the fact that by remote putting that Alpha to zero you don't perturb to match your prediction and what they have derived their job.",
            "Very nice guarantees for that.",
            "OK, so where they show where they show essentially is that we have a technique where you get you get the same worst case guarantees as the original problem and one big message of this talk is that.",
            "So essentially the gaze.",
            "Overall 10 guarantee OK.",
            "So for the worst possible problem they get the same type of performance before any particular problem.",
            "They might do.",
            "Worse, OK, in fact they don't control the loss in performance for the vote of particular instance and not maximize overall possible instances.",
            "So in this store will try to go beyond those worst case guarantees and trying to provide for any given problem.",
            "I don't want to lose anything for that particular problem then the other other other strategies, in particular the one by Autobahn is.",
            "Unburden his colleagues called Ella.",
            "Ella is VM Ware.",
            "They do an online selection of examples so this works very well in practice, but this comes with no no guarantees in terms of number, the size of your budget that you have to use to get a good optimum.",
            "OK, so now let's look at the second set of possibilities to have a fast fast."
        ],
        [
            "Fast algorithms and it is closer to what degree to present later you have a nice so the random features of a hobby and wrecked OK and often the cord that kitchen sinks for reason.",
            "I don't really understand, but this is the name of the method.",
            "So essentially these words for kernels which can be expressed as an expectation.",
            "OK, so this is true for all kernels based on like on 40 features like all at Constellation, inviting kernels you can, you can see them at the expectation of an available Omega or the next set of features.",
            "So what they have done, I think would be nice is you joy play.",
            "This is expectation by metrical leverage.",
            "OK and you take P samples and if P is large enough then this the empirical.",
            "So the approximate kernel will be close to that and they also derive worst case guarantees which are of similar nature at what was done in the previous slide.",
            "OK, they show that you don't lose in the worst case performance an, so I see you know it's not true.",
            "So they use a cover number.",
            "Elephant.",
            "The right argument is it's just a random projection.",
            "Those results are real that you randomly project.",
            "I mean forget about her mother.",
            "Do any Project X.",
            "You really get a wonderful weekend should you just.",
            "Sure, yeah, you still get well.",
            "You still get the same one of our 10 performance and as we see later you can go the worst case.",
            "You cannot do better, but for any particular case you can do better and this is a goal of the next\n Then let's look at what could be the topic of."
        ],
        [
            "Dog is column sampling, so here the ID is very simple.",
            "So we have a big kernel matrix OK, which is huge.",
            "Let's try to approximate it only by a small number of columns and this has many names.",
            "OK, so I'm sure I'm missing some of those.",
            "And if you're not in that slide, I'm sorry and I will add it for the next time.",
            "So it was first.",
            "I guess in machine learning push by William Seager.",
            "So Nystrom method also by Alex smaller.",
            "There is incomplete Cholesky decomposition, Gram Schmidt, auto normalization and more recently this was also picked up by a different community and it's called Cor decomposition by Melanie and engineers.",
            "So how those method works?",
            "Those methods work.",
            "So isn't she?"
        ],
        [
            "You're given the positive definite matrix KN by North, and we called V being the set 112 N. OK, so goal here is to approximate the full condom metrics formally, a subset of its columns.",
            "So of course, since K is symmetric, having a set of columns mean so you have you have a set of rows.",
            "OK, so here is actually given some subset I OK, then you want to proceed with the full matrix given only the blue, the blue part.",
            "So the way it works.",
            "Essentially if you go back, if you see the econometrics as adult.",
            "Between like feature vectors, essentially once you select, so this is a point like black and red.",
            "Are your points in feature space, and you choose the red points and you want to project all the black points on the subspace spanned by the by the by the red point you obtain the blue points at the end.",
            "Essentially you replace all Black Point blue points and these correspond to replacing that guy OK by simply that matrix.",
            "That matrix is only using that guy care of GI.",
            "Where JS accomplishment of I and care of you have to inverse care of.",
            "I can get kij.",
            "OK, so this is really classical classical analysis of the problem.",
            "OK, so here from now on.",
            "But we do.",
            "I will approximate.",
            "The metrics came by the metrics LOL."
        ],
        [
            "So what does the properties of that of that thing?",
            "The first one, which is very important for the proof, but I won't need it too much for this talk, is the fact that you always overestimate and metrics in the metric sense.",
            "OK, so K is always.",
            "Greater than L also important at this time.",
            "OK, if you want to apply this to a new data point then you have an explicit feature map which is obtained like this.",
            "If you have a new point, OK, you compute the kernel between X and order points which you have selected an at the end.",
            "You do that small inversion, so this is for test time.",
            "So the key aspect is that this can be computed in times P ^2 N FP is a casualty of I and you have many ways of doing it.",
            "Essentially is a good classical ways.",
            "Complete kaliski decomposition.",
            "So what are the main questions associated with that problem?",
            "See first how do you choose your columns?",
            "OK, so this is a key.",
            "A key problem.",
            "So you have two main strategies.",
            "The first one is to do pivoting.",
            "We try to select the best one's ground so I won't talk too much about it.",
            "Today I will show some simulations showing that can't relate to what I long thought doesn't change a lot compared to random sampling.",
            "So this is random sampling.",
            "I will use Undersampling and the main question I'm trying to ask, how big, how big or small should be be?"
        ],
        [
            "So here again, I'm not the first one to consider that problem and previous work.",
            "I've considered approximations of the kernel matrix.",
            "OK, so since we use air to approximate K, it would be nice to have P. So P will always be the identity of.",
            "I is nice to FP, which is big enough so that K is close to L in any norm.",
            "Number for this number.",
            "So this was done in particular by Mahoney and Renasant Kumar.",
            "And colleagues, I guess socially they use like nice tools from that joint drop described in this tutorial, and I'm going to use the same type of tools.",
            "So here.",
            "Of course for machine learning this is not super interesting, but we really want it's rare to see if we can use L to do good prediction, so again this was done also by several people and typically what the user user two step approach in the sense that the first show that you have a bound on the matrix approximation and then use that bound to.",
            "To get like performance guarantees for the problem.",
            "So the main two I think works which are related to mine at the world by wrong Jeannette.",
            "Our recent work where they do the two step approach and try to derive like performance guarantees.",
            "But still they still rely on the worst case performance in the whole class.",
            "My goal is to get.",
            "No lots of performance before a particular instance so well by Karina and colleagues where they do the similar similar approach where you try to bound cameras L it wants to know that L is close to K, then you can't receive your predictions.",
            "ASMR are close, but here in what they do, typically since they do they do a two step approach.",
            "They have to have a term of the form one over Lambda or wonderful undersquare hiding somewhere, and as I told you Lambda will be small and can be very small as I will show you at the end of the talk so at the end this does not provide a guarantee that LLP can be can be soon enough, and to destroy those results.",
            "I took like 2 simple problems.",
            "Well, I compares on the left.",
            "It will be under sampling on the rise is incomplete.",
            "Click position with partial pivoting OK and I consider the bound the relative error between either in terms of metrics approximation.",
            "OK, this is essentially what those guys are showing this season.",
            "The black and blue.",
            "So in blue.",
            "This is for the operator norm and Black for Trace Norm an in red.",
            "This is the prediction error.",
            "When you use L as a prediction, as you can see, this one goes much faster to go than those ones.",
            "OK, so if you take here, stop the curve, not because I'm lazy, but cause this is a point where you predict as well as the full condom metrics so you can see that you get very good performance even though you error is not super good in terms of approximating econometrics.",
            "This is for random sampling and this is for incomplete critical position and it goes a bit faster.",
            "OK, but not immensely faster.",
            "It's good performance in terms of prediction a bit faster, so it's Judy.",
            "The main goal of this talk is trying to avoid to do this these two step."
        ],
        [
            "Airport.",
            "OK, to do this I've talked a lot about like I don't want worse case analysis, so let's do the classical analysis and just to be clear what I'm going to give you right now is a very partial answer to that problem.",
            "OK, I'm going to use.",
            "Only squares and only a fixed design analysis.",
            "OK, so this is really really partial OK. OK, so let's look at."
        ],
        [
            "Under his regression, socially you have the square loss.",
            "So this is my problem which I which I obtain once I have the representative theorem like this and they can put everything into a square number like that.",
            "And if you set the guidance to zero you get a solution Alpha of that form, and if you if you look at the prediction on your training data then you have K times Alpha is of that form.",
            "OK, so here since I'm using fixed design setting we only care on predictions on the training data.",
            "OK, as you can see this is linear and why?",
            "OK, and this is very common in statistics where predictions we do not problematic statistiques.",
            "I obtained as a linear function of your observations, and edge typically is called the smoothing matrix, so the hat matrix, because it puts hat on why?",
            "And essentially what I'm going to do right now is consider the theory of like smoothing in the fixed design, searching for a particular matrix edge which is obtained by."
        ],
        [
            "Analogous.",
            "So let's look at exactly the analysis.",
            "We assume that X the points are deterministic, while in a fixed design setting.",
            "So we called why I buy observations.",
            "If I could see the expected value, which is what I want to predict an epsilon, I is going to be some noise.",
            "So here we only assume, and this is important that the noises as finite variance.",
            "OK, we not assume the noises ID, but we could see the covariance matrix of the of the data so that they had.",
            "Is your predictor?",
            "It's called is equal to HY, and so essentially what I want now is to compute the expected value OK of exact the noise variables of the error that I make in reconstructing my signal by synergy.",
            "OK, so goal is to do this and this has been done many times before.",
            "OK, 'cause it's back to a bar and I think even even earlier.",
            "And this is an instance of generalized additive models looked at by highstein tips running.",
            "OK, so now you have an expectation of the square norm, so this is equal to the square norm between the expectation in your in your constant term plus the variance that we can use a file at the Hut is of that form.",
            "So this is so expectation expectation of the heart is simply edge times E, so you get that term and the the variance of the heart since she had is a linear is linear in the epsilon, we get that simple terms, it get edge time C types edge which is a trace of Ch squared.",
            "So typically this is."
        ],
        [
            "Is divided into 2 terms bias term OK to the first term and you have a variance term which is the other one?",
            "OK, so why the variance term 'cause this is if you have no noise, this term is zero.",
            "OK, so this is the variance term.",
            "So the key limit.",
            "OK, you have to notice that.",
            "OK, so as Lambda as Lambda increases that this guy this guy decreases OK have less variance if you regularize a bit more, but is Lambda is increasing, you get more bias cases classical classical as well.",
            "OK, so now."
        ],
        [
            "Let's give a name a bit to those quantities, so it's still a bias.",
            "Terms of science term.",
            "So if see if the noises ID again, you can replace these trace of C * 8 square by simple square over N times, trace trace of edge square so that quantity OK is often referred to as the degrees of freedom of your exposing metrics.",
            "OK, so instead they are clear if it is trace of edge of price of Edge Square.",
            "But since I'm going to use another one that's not so that's a big problem.",
            "OK so wait, it causes.",
            "This is hidden why?",
            "Because essentially the way to see that is as an implicit number of parameters associated with your.",
            "With your exposing problem.",
            "Why is it so?",
            "Because if you if the rank of the econometrics is POK and normalize equal to 0, then P the decades of freedom using that definition of definitions to equal 2 POK.",
            "So of course, as when you eigenvalues not equal to 0 after a certain point, this is not.",
            "This is not the case.",
            "OK, you have something which is.",
            "Only an implicit and Arthur metals."
        ],
        [
            "So here for my results I will need to design another another version OK, which will be adapted to my proof technique essentially.",
            "So here we replace.",
            "So the the decades of freedom which appear here is a trace of edge square.",
            "So it is some of these are some of.",
            "So some of the elements, the sum of the squared elements of the metrics edge and I'm going to approach this by the largest element of the digital edge.",
            "OK times N. OK so this you have this inequality that trace of edge square.",
            "OK since edge in my case as all values less than one is less than tracer Edge and trace of edge, the sum of the Edmund norm of the diagonal.",
            "OK and I replace the element known by end Times Infinity norm.",
            "So here I will depart from the classical definition.",
            "Because my bound requires requires to go through that.",
            "So as I will show this, essentially it's not a big deal if your diagonal elements are not too not too.",
            "Don't worry too much.",
            "So if you do have one outlier, this will blow up, but typically, at least in my simulations.",
            "That term is closed to those times, OK?"
        ],
        [
            "Going down to the no not yet.",
            "OK, let's look at so.",
            "Now we have two.",
            "We have two quantities which are interest to us.",
            "If you can.",
            "If you do column sampling, then you select the columns.",
            "OK, you get a P explicit features an discribes degrees of hidden giving you D implicit features.",
            "OK, so the question is, is there any link between PND?",
            "OK, so the goal of course is to have P as far as small as possible, but large enough so that you don't get any loss in performance.",
            "So first.",
            "In a sense, we must have P because energy.",
            "So here I have no formal proof and I think it's not always true.",
            "OK that P has to be larger than the.",
            "There are cases where if you do a low rank column approximation you did better than the original problem.",
            "OK, and this happens sometimes.",
            "But if your problem is set up so that.",
            "Do the column sampling from ahead out data to avoid that your approximation is related to your predictor, and if the band that you obtained with Academy fix optimal, then there's no way you can beat.",
            "You can beat D here, but this is not really what I'm going to talk about today.",
            "I will try to do the opposite.",
            "How big P needs to be so that you do as well as a full cometrics."
        ],
        [
            "Can you have P being constant times D and you see the main result?",
            "I'm going to show now."
        ],
        [
            "OK, so let's make some assumptions.",
            "So here in fact there are no assumptions.",
            "No, like no.",
            "We're assumptions OK. Or how many?",
            "But there are natural OK, at least for me.",
            "So Z is your signal 'cause it is important we want to estimate case or matrix.",
            "So I define my maximal marginal degrees of freedom, DA different, the radius of the data are square.",
            "Then I take any random vector for the noise.",
            "OK, so this will simply have finite variance of 0 mean and it does not need to be ID.",
            "I think a uniform random subset of the indices in One North.",
            "I consider my low rank matrix approximation OK and I consider.",
            "Zehut K which is what I would obtain.",
            "What I would obtain with the full kernel matrix, an LOLOL which is what I would obtain if I take the approximate, metrics, because the goal is to have the performance of the add her to be the one as the same or close to the same as the performance of the ADK."
        ],
        [
            "OK, So what we obtain is that for any Delta in 01, if P is large enough, then I will comment.",
            "Is a formula then the expectation?",
            "Over both your random sample, your random selection of columns and your noise OK is less and the constant times the expectation of your performance 4K with the metrics.",
            "OK, so if this guy OK if you remove the 1 + 4 Delta, this is the performance of the full mirror metrics.",
            "OK Times 1 + 4 Delta what we show is that what I show that on average when you average over over I.",
            "Which is a you are dumb self columns.",
            "Then you only lose a constant factor.",
            "OK and the way that constant sector will of course impact the minimum value of P and the minimum value P yes, as some like terms over there an is proportional to the OK.",
            "So here is a P has to be larger than a constant times D. OK so here this is what we want.",
            "I want you to show namely that if P is a linear function of D then you don't lose any.",
            "You don't lose any.",
            "The performance when you applied your technique."
        ],
        [
            "Alright, so let's look, let's discuss.",
            "Let's discuss the results, but at least.",
            "But if you do PCA then you will get the squared.",
            "You won't be able to do this another time.",
            "So the proof technique follows like classical result from top and his students.",
            "So here I think a key element compared to produce work on that similar techniques that we don't need.",
            "Again gaps.",
            "OK so the activities, candy cake indicate to zero gracefully.",
            "And there's no need for gap.",
            "There's no assumptions on the noise.",
            "So we get a relative approximation guarantee.",
            "OK, so here one key limitation, at least of the weight is return is the fact that I take only expectations.",
            "OK, it's not.",
            "With high probability, so in fact is a proof we prove a result in high probability with respect to.",
            "I OK with respect to the sampling of the columns.",
            "OK, if you want to do the same thing with respect to the sampling of the noise, then you need extra assumptions and the goal was to go beyond those extra assumptions.",
            "We also also get some longer term OK and here the only appears in the log factor.",
            "OK, remember that Lambda will decay to zero quite quite fast and this we avoid having to be killed by longer because it's only in the long term.",
            "OK, so."
        ],
        [
            "Of course, one of the criticisms that you might want to go beyond the squares, and you have ways to do it.",
            "I haven't done it, but I thought I would take about 3 to talk about more of your work, which nobody knows about.",
            "Since I have time now, I can talk about it a bit.",
            "I think it's cute and I'm going to use lack results from optimization.",
            "Essentially, the theory of several gardens.",
            "OK, so This is why don't.",
            "The real valued function G is set set coordinate essentially.",
            "If the 3rd order directive OK is controlled by the power of the second derivative, OK, so just by this simple assumption you can derive like convergence bounds for Newton in a very very elegant way, and very sharp and short way.",
            "But unfortunately for the logistic is not true.",
            "OK, you have to define a new notion of several gardens and essentially this is simply a bound of.",
            "The third debate is bounded by a constant times.",
            "The second derivative, but with no with no powers.",
            "OK, essentially using that same tool and all the proof techniques of linear of skin is still off.",
            "You can derive no static version of the results that you have the classical aesthetic results which you obtained from estimation and the key element is that you get the exact 1st order term.",
            "OK, so you don't do a worst case analysis, so the third term is the exact aseptic term and then you bound the remainders using those techniques.",
            "So what I've started to do, but.",
            "Gotta get lost for the moment is try to apply the same thing to that to that problem as well, because this would show that also for a bit harder problems and maybe closer to what we want to do machine learning you can have similar similar behavior."
        ],
        [
            "OK, let's go to the very don't be afraid by the large large large table.",
            "OK so here one also.",
            "One problem is about Alpha.",
            "Not comfortable.",
            "I talk to people that how should you choose Lambda.",
            "OK so it's a key problem.",
            "So choice of Lambda is really a problem.",
            "So I try to do is try to see what would be the optimal Lambda parameter.",
            "And for similar situations.",
            "OK, so here what is important is in that context, and this was of course notice before if the decay of the eigenvalues of the kernel matrix OK and also the decay of the coordinates of your of your function on those eigenvectors.",
            "OK, I will call you I the decay of the counter metrics OK, and I will assume they are summable.",
            "OK, so that's a trace grows with North, which is typically the case, and I will also assume that the sum of neways OK is also summer.",
            "Also that the overall norm of your prediction is also bounded from below.",
            "OK, so we have taken several cases.",
            "Either you have you have a polynomial decay of your problem OK, or an exponential decay.",
            "OK, so for all of those cases I took the bias and the variance which of the formulas which are shown before and found that equivalent for the BIOS for the variance for the BIOS from which I'm computing the optimal Lambda OK then I get.",
            "We put it back into those terms and some I get the best patient performance and then I guess the degrees of freedom for the optimal Lambda.",
            "OK, so here this takes the same the same way."
        ],
        [
            "Let's look at what's good and not good about the table.",
            "OK, first, what are the good?",
            "The good, right?",
            "So it's known by for awhile and can see paper by John Stoner by John Watt and his colleagues that.",
            "OK, so here in UI is a characteristic of your signal.",
            "OK, so given the given the signal, you have an optimal estimation.",
            "It is known that that optimal right for those types of decay is that type of rate.",
            "OK, but worth.",
            "And this is achieved by essentially by that line and that lens of the red points are the situation, so kernels which achieves the minimax estimation rate and for exponential decay you get Logan over North OK, which is over there.",
            "So here's the intuition is the following.",
            "So if your new I saw you again values of your metrics, oh sorry, this should be.",
            "There should be a zed distribute the function.",
            "If those decay a fast way, this corresponds to.",
            "Smooth functions OK, think about new I and UI and UI is being free coefficients OK if they're free coefficients, then if you have a fast decay, have smooth function.",
            "So we have a simple problem to estimate, whereas if you use the decay of values of K is fast then it means that you have a smaller feature space.",
            "OK, so fast as a DK this module space.",
            "So here we get.",
            "We see two situation.",
            "OK first, if the space is too large, space is too large.",
            "Mean it means that the DK.",
            "Of the eigenvalues of the cosmetics is.",
            "Slow, OK, and this is the case here where beta beta is quite small and as you can see that you do underfitting OK, you don't achieve the mini Max rate.",
            "OK, this is OK, that's life.",
            "If you take two big official space you won't be able to regularize correct way and you lose performance.",
            "So this is classical, but to me what I want you to get too.",
            "In fact this was related to work, don't with Mark Smith Nickel Arrow."
        ],
        [
            "Paper, but which we presented as a main conference.",
            "We said, oh we need the parameter Lambda to be bigger than North.",
            "OK, for our method to do the work.",
            "So I wanted to know is it really true that Lambda is always bigger than North to get good performance here you duties that in some cases if if if the feature space is too small, OK, then sometimes even though you get the optimal convergence right at the end.",
            "Lambda would be very small and can be exponentially smaller.",
            "This means that if you want to find the optimal value machine, you won't be able to do it because essentially you will have to go below matching precision to get the optimal right.",
            "OK, so this is something in fact, which had been, which had been noticed by other people and nuclear rule that.",
            "Colonel aggregate too fast and sometimes you got is very hard to have a gas.",
            "Regular fit OK if you put under Cosimo with gas and kernel, often it doesn't overfit stabilized and not do terrible.",
            "OK, so this is one way of seeing it.",
            "Is that the optimal Lambda can be pretty small if your kernel kernel is 2 is too small or if your space is too small.",
            "Yeah, so this is 101."
        ],
        [
            "OK, let's forget about that.",
            "So let's look at small at some at some some simulations.",
            "OK, so here I took really basic simulations, for which I couldn't.",
            "I could like.",
            "I knew all the decades of all things.",
            "So this essentially, but you're disposing clients and with the full coefficients, and I've tried several several things.",
            "OK, so on the left here.",
            "I show OK, so I've taken several values for the kernel parameters and several one value for the kernel parameter for the signal parameter and several values for the.",
            "Odometers OK and you compare the.",
            "What is a log of the Lambda?",
            "OK, the optimal Lambda.",
            "As you can see it stabilizes.",
            "That goes to zero smoothly when betrayals Wanan four, which corresponds to feature spaces, which are a big enough.",
            "But here when beta is 8, so this means the feature space is too small.",
            "Then whatever you want to do, it wants to have as small as possible parameter an here in such a rights at machine precision and for that.",
            "Below 'cause if I go below I can't do the estimation OK and in terms of performance this is a generalization performance.",
            "You can see that so the optimal one is a blue one and this corresponds to the my previous slide for the red Red columns.",
            "When the Colonel is.",
            "When does this is optimal convergence rate the black correspond to better course.",
            "One answer better equals 1 means that the GK here is is a slow decay.",
            "So this means that I will tend to overfit and this is what I obtain.",
            "I don't get the same convergence rate and for the this curve I'm supposed to get the best possible performance, but to get the best possible performance and I need a very small number and it says I can't numerically do Rice bowl Lambda.",
            "Typically Matlab impose a land of 10s.",
            "16 I can't achieve the optimal constraint because of medical problems, and in fact I was quite surprised about that.",
            "And final simulation.",
            "Is I tried to compare, try to see if my results which are obtained like the previous slides is correct or not, at least empirically satisfied or not.",
            "I'm comparing the performance of the full matrix over the performance of the approximate approximation of the matrix and I think I compute the rank.",
            "The rank at which attained like 10% I make less than 10% of increase loss with respect the full matrix.",
            "So here I compute several things.",
            "I compared to the true degrees of freedom.",
            "OK in blue to my know the true degrees of freedom and read the new version version of this freedom is in black in the is in blue OK. As you can see the rank the optimal rank at the end to achieve dollars of performances between like one and two which is close to what I was expecting.",
            "The sense it is linear.",
            "The rank that we need to get the same performance is linear in terms of freedom whether you use the.",
            "Classical version in red or my version.",
            "So here in the dotted is the same thing with incomplete crisci decomposition with pivoting.",
            "So instead of doing random sampling, I try to do a greedy selection of columns so it does a bit better, but you get at least of those simulations to get the factor of two OK.",
            "In fact I had.",
            "I thought that by doing like really selection you should do much better.",
            "OK you do a bit better, but at least in those simulations, don't do immensely better."
        ],
        [
            "Castle."
        ],
        [
            "Prudent.",
            "So I've tried to provide analysis of column sampling for kernel, least square regression and here the key take home message that the degrees of freedom, which is a classical quantity that people consider for statistical reason.",
            "Is also useful.",
            "You saw a good guidance for as a computational computational tool that you see how big they may need to be.",
            "So of course I'll submit several problems with the current approach in practice, since you don't know the decades of freedom when you start doesn't give you huge improvement to select P, although you can have ways to approximate the econometrics with a P with P decently large.",
            "Get a small simple approximation of D and try to iterate like this, so it's not.",
            "That's the problem for the moment, so there's no doesn't provide actual an actual algorithm.",
            "They also extensions which are worth considering.",
            "First is way to go beyond uniform sampling.",
            "OK, so this is a people have considered different ways of something.",
            "The columns not uniformly but with a better better way here clearly going to random design is important, so here to have the most simple result.",
            "OK, very simple.",
            "It's no not so many assumptions.",
            "I will assume the fix design, analysis and clearly doing going on design by by using results of super Sham and talk.",
            "Then you could do the same thing, but here I think we need to add extra assumptions.",
            "Also, currently if you have, if you have a rank of P for the approximation, you achieve P ^2 * N complexity.",
            "OK, so here P the main message of my talk is P is close to the degrees of freedom.",
            "OK, So what you gain as a complete as you get as a capacity at the moment is the square times N OK. Is it possible to achieve the time then?",
            "OK, that's a question that would be nice to answer.",
            "Finally, trying to go beyond these squares.",
            "OK, so hinted at possibilities for logistic regression.",
            "You could do a similar thing for the SVM because like nice paper by Blanchar colleagues, we showed that the quantity the degrees of freedom, the other similar quantity, the SVM case and then the last two.",
            "Of course.",
            "In fact this was the motivation behind at work trying to see if you could do an online technique to do control machines with the complexity that goes sublinearly in or not quadratically.",
            "And as a first step had to see whether you can do column sampling in an efficient way.",
            "So clearly what we're going to focus on is trying to do the true online setting with the true decay of Lambda OK, as opposed to the current classical techniques.",
            "And finally, as the last advertisement for a septic statistics activities.",
            "I think if you do a very stochastic gradient OK and this is well known from 1992, but similar seemingly forgotten often at NIPS is that you get the optimal governance, right?",
            "OK, you get the.",
            "Without any metric conversions you get the the you achieve the camera lower bound and I think this is essentially what I think should be needed to get this the same performance at the full as a full problem."
        ],
        [
            "Good for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about kernel methods, so before I start maybe a small comments or those of you have kids will be know that when they're young they're real sponges.",
                    "label": 0
                },
                {
                    "sent": "They believe what they really believe.",
                    "label": 0
                },
                {
                    "sent": "What you say.",
                    "label": 0
                },
                {
                    "sent": "Then it changes, but at least for the first 10 years of life, what you say is supposed to be true.",
                    "label": 0
                },
                {
                    "sent": "My academic life bit similar so if my first name was in 2000 at the peak of the kernel years, so in a sense I will be like my kids.",
                    "label": 0
                },
                {
                    "sent": "I believe what I was I was told when I was a kid and academic kid.",
                    "label": 0
                },
                {
                    "sent": "So today my main message.",
                    "label": 0
                },
                {
                    "sent": "So I moved move down a bit.",
                    "label": 0
                },
                {
                    "sent": "Things those days the messages really.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't forget to methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so the goal of today's could show you that there is life life beyond just plain linear models and the second message of my.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk is don't forget ascetic statistics analysis, so this supposed to be super old school, but typically just replacing the central limit theorem by having inequality or Bernstein doesn't buy you much.",
                    "label": 0
                },
                {
                    "sent": "OK, it makes for Mr. Matt OK, so you have very long papers, so I've been involved in that so that criticism so applies to me, but the goal of today is to remind you of the old old results from the statistics and see how they can be combined with machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk mainly about supervised machine learning related to complex optimization.",
                    "label": 1
                },
                {
                    "sent": "And when I joined like a snips community, it was a pig.",
                    "label": 0
                },
                {
                    "sent": "So the peak of the kernel kernel, yours, an essentially, every thought that everything could be solved using kernels OK. And it was people were doing like nonlinear methods and it was classically seen as nonparametric statistics where the number of implicit parameters were growing with the size of your problem.",
                    "label": 0
                },
                {
                    "sent": "And typically people have convergence rates in terms of prediction error.",
                    "label": 1
                },
                {
                    "sent": "Which of the turmoil?",
                    "label": 0
                },
                {
                    "sent": "Enter the problem far and Alpha being often between 1/2 and one.",
                    "label": 0
                },
                {
                    "sent": "This being said, it was good for statistics, but typically people are using using it in small scale problems, mainly because the complexity.",
                    "label": 0
                },
                {
                    "sent": "Of algorithms for at least N square.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK then people moved on and since the late 2000 people are using mostly linear models and with or without spasticity, but at the end they still use a linear models.",
                    "label": 0
                },
                {
                    "sent": "OK so we go back to classical parametric statistics where typical typically the convergence rates are between one or one over root N OK. And of course this was.",
                    "label": 0
                },
                {
                    "sent": "Pushed by the large scale of the problem that people want you to solve, and here people have managed to be able to attain algorithms with complexity which is the size of your data and typically Orth OK. Well message about to convey in this talk is in fact that we went from naive optimization OK when we were doing kernels in the sense we were pretty naval community in terms of optimization, but now we are more refined in terms of optimization.",
                    "label": 0
                },
                {
                    "sent": "We went back to naive statistical models and in particular somewhat, I think, naive analysis.",
                    "label": 0
                },
                {
                    "sent": "So the goal of today is really to try to bridge that gap between kana methods and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An optimization, so of course I will only provide a partial answer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this will be the outline of my talk.",
                    "label": 1
                },
                {
                    "sent": "So we first by review what I mean by supervised machine learning.",
                    "label": 1
                },
                {
                    "sent": "Then I will review existing results.",
                    "label": 0
                },
                {
                    "sent": "I will try to give a critical view of those and then I will go over classical techniques to have efficient methods with kernels.",
                    "label": 0
                },
                {
                    "sent": "And now then I will review also classical result from the 70s or 80s from the statistics community.",
                    "label": 0
                },
                {
                    "sent": "And finally I will go into the topic of the talk.",
                    "label": 0
                },
                {
                    "sent": "The actual new material which is trained to analyze low rank approximations.",
                    "label": 0
                },
                {
                    "sent": "Well being to be fast without losing any performance.",
                    "label": 0
                },
                {
                    "sent": "And finally if time permits, I would also consider an analysis of the choice that regulation parameter 'cause I think this is largely overlooked and this remains already the practical problem.",
                    "label": 0
                },
                {
                    "sent": "So it's a workshop talk, so don't tend to interrupt me if you disagree.",
                    "label": 0
                },
                {
                    "sent": "Also, if you agree of course, and ask any questions if things are not super clean.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I mean by supervised machine learning.",
                    "label": 1
                },
                {
                    "sent": "So you have some elevations.",
                    "label": 0
                },
                {
                    "sent": "Xiy Island, Zi, back in color.",
                    "label": 0
                },
                {
                    "sent": "SpaceX I can be anything can be an image, can be a graph.",
                    "label": 0
                },
                {
                    "sent": "You have an output, why I but you want to predict in this talk I will assume the data ID and I will predict.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're function F. That function will be belong to a Hilbert space and traditionally you can see that function as a dot product, which is your element of your Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "An feature functions cervix.",
                    "label": 0
                },
                {
                    "sent": "OK so this field.",
                    "label": 0
                },
                {
                    "sent": "This can be either explicit if you build it directly or implicit.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to consider the classic colorized empirical recommendation for work where I want to find the solution F had of that convex optimization problem, which is composed of a convex.",
                    "label": 1
                },
                {
                    "sent": "That are fitting term over there, plus some regularizer.",
                    "label": 1
                },
                {
                    "sent": "An throughout this talk.",
                    "label": 0
                },
                {
                    "sent": "That regularizer will always be as a square norm of Hilbert space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given that problem OK, you have several quantities of interest.",
                    "label": 0
                },
                {
                    "sent": "Have so-called training costs, which is a sum of a loss or losses in the training set.",
                    "label": 0
                },
                {
                    "sent": "This is what you have access to, and you have the testing cost.",
                    "label": 0
                },
                {
                    "sent": "This is what you really want to minimize.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have access to that, and you want to minimize that.",
                    "label": 0
                },
                {
                    "sent": "So of course there are two big questions related to that problem.",
                    "label": 0
                },
                {
                    "sent": "First, how do you compute that?",
                    "label": 0
                },
                {
                    "sent": "I've had OK. How do you optimize that convex optimization problem?",
                    "label": 0
                },
                {
                    "sent": "So why convex?",
                    "label": 0
                },
                {
                    "sent": "Because I will assume aware that the losses convex.",
                    "label": 0
                },
                {
                    "sent": "So imagine like least squares or logistic or the ECM and the second question is are you?",
                    "label": 0
                },
                {
                    "sent": "How can you analyze F but OK, so this was done traditionally separately, and since the last few years people have started to consider those two problems simultaneously.",
                    "label": 0
                },
                {
                    "sent": "You both want to have a good effort in an efficient way of computing effort.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at some nice results from a skeletal, so which are related to other results by Bushman massage.",
                    "label": 1
                },
                {
                    "sent": "So with very very small number of assumptions on the problem.",
                    "label": 0
                },
                {
                    "sent": "So we still could R the expected risk.",
                    "label": 1
                },
                {
                    "sent": "And of course R is overloaded.",
                    "label": 0
                },
                {
                    "sent": "No, no, it's not.",
                    "label": 1
                },
                {
                    "sent": "It is expected risks are at is the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And as I told you, our predictor will be based on the attacker regularised in particular estimation when you have.",
                    "label": 0
                },
                {
                    "sent": "When you have our heart plus your square norm, you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "Data are bounded.",
                    "label": 0
                },
                {
                    "sent": "You assume the Lipschitz loss like the VM and boom, what you get, you get that the excess between the regularizer risk of your predictor and the smallest possible regularised risk is bounded by something in high probability with constant OK with depend on your problem on the register, your data divided by something times N OK.",
                    "label": 0
                },
                {
                    "sent": "So the good thing is that this corresponds to a fast rate because it depends on North.",
                    "label": 0
                },
                {
                    "sent": "OK, but my main criticism all of that sequence of results is that at the end there is not only a.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This number OK and Lambda OK, so we should tend to zero as we have more data or typically Lambda is smaller when you have more data because we want to adapt to your problem.",
                    "label": 1
                },
                {
                    "sent": "So at the end if that tends to 0.",
                    "label": 0
                },
                {
                    "sent": "Then this isn't really.",
                    "label": 0
                },
                {
                    "sent": "Might not go to zero as fast as we can.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was of course noticed by many people and typically.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you do is you consider alarm data decays with North, so here this is the same result without the the important constants.",
                    "label": 0
                },
                {
                    "sent": "So if you take the regularizer particular risk, it converges as rates Wonderland again.",
                    "label": 0
                },
                {
                    "sent": "But because we use empirical because we use it regularize risk, you have some bias due to that Lambda term and if we want to get a good tradeoff between the bias and this over over one Lambda and then you have to take Lambda at the other one over root North.",
                    "label": 0
                },
                {
                    "sent": "And in the worst case situation, if you don't know anything about your problem beyond the fact that it is bounded, what you get is something which is the risk of your product or minus the best possible risk of your class is bounded from above by Underoath.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is essentially a summary of the worst case results that people consider a lot in this community.",
                    "label": 0
                },
                {
                    "sent": "Either one of one of the root N. If you don't have any assumptions and one other alarm, then if you put some Taco, mix it into the into the picture either.",
                    "label": 0
                },
                {
                    "sent": "By adding the square norm, or maybe because the data are low dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the amazing aspect of that framework is that this is done for the the global optimum F hat, and it turns out that you don't need that global optimum and you can find algorithms which will achieve the exact same race OK, but in a single path through the data OK and this is possible only.",
                    "label": 0
                },
                {
                    "sent": "When are the you have an explicit description description of your features?",
                    "label": 0
                },
                {
                    "sent": "So essentially when F is RP, then with a single path for data with complexity of PM, you can achieve the exact same right.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you make no assumptions to get one of those N, if you add the squared regularizer you get one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, but I think those results are good things.",
                    "label": 0
                },
                {
                    "sent": "They provide a good intuition on what is hard and what is difficult, but essentially they go from either like super super bad Rate 2.",
                    "label": 0
                },
                {
                    "sent": "What seems to be a good right but with Wanda OK in the sense there is a gap between one over root N in one of our North.",
                    "label": 0
                },
                {
                    "sent": "And this means that it's worse.",
                    "label": 0
                },
                {
                    "sent": "I think it's playing that gap and these correspond to what these correspond exactly to looking at what is inside the containment tricks or inside your current metrics.",
                    "label": 0
                },
                {
                    "sent": "So essentially all the eigenvalues of the kernel matrix and of the occurrence matrix has to be have to be taken into account, and that's simply the largest one and just move the small but all of them.",
                    "label": 1
                },
                {
                    "sent": "And the goal of today, ready to look into this in a more.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "GPL.",
                    "label": 0
                },
                {
                    "sent": "So why kernels?",
                    "label": 0
                },
                {
                    "sent": "OK, So what what people typically do, but I don't have kernels when you have like you have an explicit description of your features that you do have efficient optimization.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned that essentially stochastic resonance and his variance and for fixed Lambda Westie remain unclear is what should not be OK.",
                    "label": 0
                },
                {
                    "sent": "So typically this is done by crystallization and what I really want a message thing which is very important to me is the fact that if from the start before you see a single data point.",
                    "label": 0
                },
                {
                    "sent": "You affix the size of your representation that you fix P. Then as N grows at one point, you will under fit OK. You could achieve the.",
                    "label": 0
                },
                {
                    "sent": "You will achieve the good companies are good.",
                    "label": 0
                },
                {
                    "sent": "You will achieve the performance at one point and then you will have under 50 because you don't adapt to your to your data.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you do, if you scan methods were infinite.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So linear models, then, first of course there are no few algorithms to do to do this.",
                    "label": 1
                },
                {
                    "sent": "When N is a dentist, 12 is maybe difficult.",
                    "label": 0
                },
                {
                    "sent": "The choice of Lambda still remains unclear, but here in terms of statistics, we can adapt to the incoming flow of data.",
                    "label": 1
                },
                {
                    "sent": "OK, so the classical primary problem, but of course here you have higher risk of overfitting, and this is of course problem.",
                    "label": 1
                },
                {
                    "sent": "Another another thing which I think is quite important.",
                    "label": 0
                },
                {
                    "sent": "Like that even though you have like finite number of features.",
                    "label": 0
                },
                {
                    "sent": "OK, just seeing them as infinite I think is good because typically in the regimes where we work on we are not exactly attaining the aseptic regime will bit in between.",
                    "label": 0
                },
                {
                    "sent": "So seeing your features that influence dimensional or those are finite I think provide you so good good set of tools.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So like you know also that reduce they, at least to me that provide a good attraction of high dimensional feature space is of course they allow nonlinear estimation.",
                    "label": 0
                },
                {
                    "sent": "And this is like useful in many many fields and I'm sure there are many other wants.",
                    "label": 0
                },
                {
                    "sent": "Is it true in computer vision, bioinformatics, or new imaging?",
                    "label": 1
                },
                {
                    "sent": "I don't believe that you can get away only with linear models.",
                    "label": 0
                },
                {
                    "sent": "At one point you want to add something which is a bit more flexible, and I think those fields are full of those of those problems.",
                    "label": 0
                },
                {
                    "sent": "And of course the goal being you want to improve the predictions as an Andrews.",
                    "label": 0
                },
                {
                    "sent": "So of course the big problem is the fact that if you tell a bit, naive if you just compute the kernel matrix boom, you get 10 square.",
                    "label": 0
                },
                {
                    "sent": "OK, so when N is very large.",
                    "label": 0
                },
                {
                    "sent": "This is not so possible.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the main goal of this talk is trying to see if you can go below this quadratic complexity.",
                    "label": 0
                },
                {
                    "sent": "I try to derive like upper bound or lower bounds.",
                    "label": 0
                },
                {
                    "sent": "To get the fat algorithm but still preserve this nonparametric nature, if you want to be fast, I knew of Agnes algorithm that does nothing is super fast, but it doesn't do much.",
                    "label": 0
                },
                {
                    "sent": "But you really want is to be able to achieve this nonparametric nature of consumer thirds while remaining subglottic.",
                    "label": 0
                },
                {
                    "sent": "And this this might seem might seem quite theoretical theoretical problem, but still think it is a big particular problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are only few methods that people use a lot that can do to achieve this increasing number.",
                    "label": 0
                },
                {
                    "sent": "The increasing complexity of your predictor while remaining some quadratic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's do let's go into more details.",
                    "label": 0
                },
                {
                    "sent": "So first we consider the regularised minimization OK, and using the theorem then it is known that because you have an Euclidean norm over damn, you know that you know you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't need to search about your function in the full Hilbert space, you can parameterise your function as a linear combination of of your input data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so you go from a problem which can be of large size, maybe infinite to something.",
                    "label": 0
                },
                {
                    "sent": "In our end, OK, and typically once you have this, if you define the kernel K of XX prime as being the dot product between few of X and Felix Prime, then you can express your prediction F of X as a sum of kernel function.",
                    "label": 0
                },
                {
                    "sent": "This is very classical and now if you take that.",
                    "label": 0
                },
                {
                    "sent": "That thing and put it back in to put it back into the problem.",
                    "label": 0
                },
                {
                    "sent": "OK then these guys becomes K Alpha.",
                    "label": 0
                },
                {
                    "sent": "The ice component of Kalfa and these guys become Alpha transpose Alpha.",
                    "label": 0
                },
                {
                    "sent": "So essentially when you saw the kernel methods that is 1 possibility is to use that represented silver and you get that problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is the problem complicated?",
                    "label": 0
                },
                {
                    "sent": "Because as soon as you start the problem if you write it down, the parameters are ready or when squared.",
                    "label": 0
                },
                {
                    "sent": "So it's called the audit log.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially, all techniques that people have considered, they have to consider, like not computing the econometrics.",
                    "label": 0
                },
                {
                    "sent": "So let's look 1st at the simplest approach to try to achieve fast fast convergence for for an algorithm, let's text Cassidy Senter.",
                    "label": 0
                },
                {
                    "sent": "Let's try to do it directly in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, so he sees your iteration in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "You go from the function at time T + 1 to the function at time T. By simply scaling the function and subtracting the gradient of the loss.",
                    "label": 0
                },
                {
                    "sent": "Associated to your this data point, taken at your prediction at time T -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so here this has been noticed, of course, before is the fact that this at the end can also be penalized at iteration kernelized.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you can, as you can see at every time step you go from FT minus one 2FT by scaling FT minus one and adding that guy.",
                    "label": 0
                },
                {
                    "sent": "OK, so this means at the end that FT can be represented as a sum of feature vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, only the 1st that you have seen so far.",
                    "label": 0
                },
                {
                    "sent": "So now you can go take this and this and now you have this nice and simple and simple recursion for stochastic gradient in the in the alphas where the new Alpha is every time you have a new Alpha and you scare the other alphas.",
                    "label": 0
                },
                {
                    "sent": "So what is the problem that approach the problem with that is that like action you still have that computation.",
                    "label": 0
                },
                {
                    "sent": "This is a sum over the element, sodium and elements, so this at the end it makes oh FT at iteration T. So if you make any iterations, it is a sum of 1 + 2 + 3 + 4.",
                    "label": 0
                },
                {
                    "sent": "Up to N. So at the end you get oh.",
                    "label": 0
                },
                {
                    "sent": "Or N squared?",
                    "label": 0
                },
                {
                    "sent": "OK so this has been a.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is before and people have considered fancy ways or to reduce the amount of computation essentially.",
                    "label": 0
                },
                {
                    "sent": "Either because you have the support vectors.",
                    "label": 1
                },
                {
                    "sent": "So here maybe I should mention this.",
                    "label": 0
                },
                {
                    "sent": "Some of the alphas will be 0 at the end if you use a hinge loss, in particular for the hinge loss, that gradient will still will often be 0 if you are in the flat part of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But still you will still have a linear term of support vectors in general, so you still need even if you use a hinge loss, you still need to reduce that number of support vectors and this as this is done by several people and gave like.",
                    "label": 0
                },
                {
                    "sent": "Nice name for those clubs, those algorithms.",
                    "label": 0
                },
                {
                    "sent": "So forget home projector olympi GSD essentially share the same type of technique.",
                    "label": 0
                },
                {
                    "sent": "Once in awhile, OK, you remove one of the alphas.",
                    "label": 0
                },
                {
                    "sent": "OK, you said one of the elders to zero once in awhile, or hoping controlling the fact that by remote putting that Alpha to zero you don't perturb to match your prediction and what they have derived their job.",
                    "label": 0
                },
                {
                    "sent": "Very nice guarantees for that.",
                    "label": 0
                },
                {
                    "sent": "OK, so where they show where they show essentially is that we have a technique where you get you get the same worst case guarantees as the original problem and one big message of this talk is that.",
                    "label": 0
                },
                {
                    "sent": "So essentially the gaze.",
                    "label": 0
                },
                {
                    "sent": "Overall 10 guarantee OK.",
                    "label": 0
                },
                {
                    "sent": "So for the worst possible problem they get the same type of performance before any particular problem.",
                    "label": 0
                },
                {
                    "sent": "They might do.",
                    "label": 0
                },
                {
                    "sent": "Worse, OK, in fact they don't control the loss in performance for the vote of particular instance and not maximize overall possible instances.",
                    "label": 0
                },
                {
                    "sent": "So in this store will try to go beyond those worst case guarantees and trying to provide for any given problem.",
                    "label": 0
                },
                {
                    "sent": "I don't want to lose anything for that particular problem then the other other other strategies, in particular the one by Autobahn is.",
                    "label": 0
                },
                {
                    "sent": "Unburden his colleagues called Ella.",
                    "label": 0
                },
                {
                    "sent": "Ella is VM Ware.",
                    "label": 0
                },
                {
                    "sent": "They do an online selection of examples so this works very well in practice, but this comes with no no guarantees in terms of number, the size of your budget that you have to use to get a good optimum.",
                    "label": 1
                },
                {
                    "sent": "OK, so now let's look at the second set of possibilities to have a fast fast.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fast algorithms and it is closer to what degree to present later you have a nice so the random features of a hobby and wrecked OK and often the cord that kitchen sinks for reason.",
                    "label": 0
                },
                {
                    "sent": "I don't really understand, but this is the name of the method.",
                    "label": 0
                },
                {
                    "sent": "So essentially these words for kernels which can be expressed as an expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is true for all kernels based on like on 40 features like all at Constellation, inviting kernels you can, you can see them at the expectation of an available Omega or the next set of features.",
                    "label": 0
                },
                {
                    "sent": "So what they have done, I think would be nice is you joy play.",
                    "label": 0
                },
                {
                    "sent": "This is expectation by metrical leverage.",
                    "label": 0
                },
                {
                    "sent": "OK and you take P samples and if P is large enough then this the empirical.",
                    "label": 0
                },
                {
                    "sent": "So the approximate kernel will be close to that and they also derive worst case guarantees which are of similar nature at what was done in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "OK, they show that you don't lose in the worst case performance an, so I see you know it's not true.",
                    "label": 0
                },
                {
                    "sent": "So they use a cover number.",
                    "label": 0
                },
                {
                    "sent": "Elephant.",
                    "label": 0
                },
                {
                    "sent": "The right argument is it's just a random projection.",
                    "label": 0
                },
                {
                    "sent": "Those results are real that you randomly project.",
                    "label": 0
                },
                {
                    "sent": "I mean forget about her mother.",
                    "label": 0
                },
                {
                    "sent": "Do any Project X.",
                    "label": 0
                },
                {
                    "sent": "You really get a wonderful weekend should you just.",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah, you still get well.",
                    "label": 0
                },
                {
                    "sent": "You still get the same one of our 10 performance and as we see later you can go the worst case.",
                    "label": 0
                },
                {
                    "sent": "You cannot do better, but for any particular case you can do better and this is a goal of the next\n Then let's look at what could be the topic of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dog is column sampling, so here the ID is very simple.",
                    "label": 0
                },
                {
                    "sent": "So we have a big kernel matrix OK, which is huge.",
                    "label": 0
                },
                {
                    "sent": "Let's try to approximate it only by a small number of columns and this has many names.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm sure I'm missing some of those.",
                    "label": 0
                },
                {
                    "sent": "And if you're not in that slide, I'm sorry and I will add it for the next time.",
                    "label": 0
                },
                {
                    "sent": "So it was first.",
                    "label": 0
                },
                {
                    "sent": "I guess in machine learning push by William Seager.",
                    "label": 0
                },
                {
                    "sent": "So Nystrom method also by Alex smaller.",
                    "label": 0
                },
                {
                    "sent": "There is incomplete Cholesky decomposition, Gram Schmidt, auto normalization and more recently this was also picked up by a different community and it's called Cor decomposition by Melanie and engineers.",
                    "label": 0
                },
                {
                    "sent": "So how those method works?",
                    "label": 0
                },
                {
                    "sent": "Those methods work.",
                    "label": 0
                },
                {
                    "sent": "So isn't she?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're given the positive definite matrix KN by North, and we called V being the set 112 N. OK, so goal here is to approximate the full condom metrics formally, a subset of its columns.",
                    "label": 0
                },
                {
                    "sent": "So of course, since K is symmetric, having a set of columns mean so you have you have a set of rows.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is actually given some subset I OK, then you want to proceed with the full matrix given only the blue, the blue part.",
                    "label": 0
                },
                {
                    "sent": "So the way it works.",
                    "label": 0
                },
                {
                    "sent": "Essentially if you go back, if you see the econometrics as adult.",
                    "label": 0
                },
                {
                    "sent": "Between like feature vectors, essentially once you select, so this is a point like black and red.",
                    "label": 0
                },
                {
                    "sent": "Are your points in feature space, and you choose the red points and you want to project all the black points on the subspace spanned by the by the by the red point you obtain the blue points at the end.",
                    "label": 0
                },
                {
                    "sent": "Essentially you replace all Black Point blue points and these correspond to replacing that guy OK by simply that matrix.",
                    "label": 0
                },
                {
                    "sent": "That matrix is only using that guy care of GI.",
                    "label": 0
                },
                {
                    "sent": "Where JS accomplishment of I and care of you have to inverse care of.",
                    "label": 0
                },
                {
                    "sent": "I can get kij.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really classical classical analysis of the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so here from now on.",
                    "label": 0
                },
                {
                    "sent": "But we do.",
                    "label": 0
                },
                {
                    "sent": "I will approximate.",
                    "label": 0
                },
                {
                    "sent": "The metrics came by the metrics LOL.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does the properties of that of that thing?",
                    "label": 0
                },
                {
                    "sent": "The first one, which is very important for the proof, but I won't need it too much for this talk, is the fact that you always overestimate and metrics in the metric sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so K is always.",
                    "label": 0
                },
                {
                    "sent": "Greater than L also important at this time.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to apply this to a new data point then you have an explicit feature map which is obtained like this.",
                    "label": 1
                },
                {
                    "sent": "If you have a new point, OK, you compute the kernel between X and order points which you have selected an at the end.",
                    "label": 0
                },
                {
                    "sent": "You do that small inversion, so this is for test time.",
                    "label": 1
                },
                {
                    "sent": "So the key aspect is that this can be computed in times P ^2 N FP is a casualty of I and you have many ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "Essentially is a good classical ways.",
                    "label": 0
                },
                {
                    "sent": "Complete kaliski decomposition.",
                    "label": 0
                },
                {
                    "sent": "So what are the main questions associated with that problem?",
                    "label": 1
                },
                {
                    "sent": "See first how do you choose your columns?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a key.",
                    "label": 0
                },
                {
                    "sent": "A key problem.",
                    "label": 0
                },
                {
                    "sent": "So you have two main strategies.",
                    "label": 0
                },
                {
                    "sent": "The first one is to do pivoting.",
                    "label": 0
                },
                {
                    "sent": "We try to select the best one's ground so I won't talk too much about it.",
                    "label": 0
                },
                {
                    "sent": "Today I will show some simulations showing that can't relate to what I long thought doesn't change a lot compared to random sampling.",
                    "label": 0
                },
                {
                    "sent": "So this is random sampling.",
                    "label": 1
                },
                {
                    "sent": "I will use Undersampling and the main question I'm trying to ask, how big, how big or small should be be?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here again, I'm not the first one to consider that problem and previous work.",
                    "label": 0
                },
                {
                    "sent": "I've considered approximations of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so since we use air to approximate K, it would be nice to have P. So P will always be the identity of.",
                    "label": 0
                },
                {
                    "sent": "I is nice to FP, which is big enough so that K is close to L in any norm.",
                    "label": 0
                },
                {
                    "sent": "Number for this number.",
                    "label": 0
                },
                {
                    "sent": "So this was done in particular by Mahoney and Renasant Kumar.",
                    "label": 0
                },
                {
                    "sent": "And colleagues, I guess socially they use like nice tools from that joint drop described in this tutorial, and I'm going to use the same type of tools.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Of course for machine learning this is not super interesting, but we really want it's rare to see if we can use L to do good prediction, so again this was done also by several people and typically what the user user two step approach in the sense that the first show that you have a bound on the matrix approximation and then use that bound to.",
                    "label": 0
                },
                {
                    "sent": "To get like performance guarantees for the problem.",
                    "label": 0
                },
                {
                    "sent": "So the main two I think works which are related to mine at the world by wrong Jeannette.",
                    "label": 0
                },
                {
                    "sent": "Our recent work where they do the two step approach and try to derive like performance guarantees.",
                    "label": 0
                },
                {
                    "sent": "But still they still rely on the worst case performance in the whole class.",
                    "label": 0
                },
                {
                    "sent": "My goal is to get.",
                    "label": 0
                },
                {
                    "sent": "No lots of performance before a particular instance so well by Karina and colleagues where they do the similar similar approach where you try to bound cameras L it wants to know that L is close to K, then you can't receive your predictions.",
                    "label": 0
                },
                {
                    "sent": "ASMR are close, but here in what they do, typically since they do they do a two step approach.",
                    "label": 0
                },
                {
                    "sent": "They have to have a term of the form one over Lambda or wonderful undersquare hiding somewhere, and as I told you Lambda will be small and can be very small as I will show you at the end of the talk so at the end this does not provide a guarantee that LLP can be can be soon enough, and to destroy those results.",
                    "label": 0
                },
                {
                    "sent": "I took like 2 simple problems.",
                    "label": 0
                },
                {
                    "sent": "Well, I compares on the left.",
                    "label": 0
                },
                {
                    "sent": "It will be under sampling on the rise is incomplete.",
                    "label": 0
                },
                {
                    "sent": "Click position with partial pivoting OK and I consider the bound the relative error between either in terms of metrics approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, this is essentially what those guys are showing this season.",
                    "label": 0
                },
                {
                    "sent": "The black and blue.",
                    "label": 0
                },
                {
                    "sent": "So in blue.",
                    "label": 0
                },
                {
                    "sent": "This is for the operator norm and Black for Trace Norm an in red.",
                    "label": 0
                },
                {
                    "sent": "This is the prediction error.",
                    "label": 0
                },
                {
                    "sent": "When you use L as a prediction, as you can see, this one goes much faster to go than those ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you take here, stop the curve, not because I'm lazy, but cause this is a point where you predict as well as the full condom metrics so you can see that you get very good performance even though you error is not super good in terms of approximating econometrics.",
                    "label": 0
                },
                {
                    "sent": "This is for random sampling and this is for incomplete critical position and it goes a bit faster.",
                    "label": 0
                },
                {
                    "sent": "OK, but not immensely faster.",
                    "label": 0
                },
                {
                    "sent": "It's good performance in terms of prediction a bit faster, so it's Judy.",
                    "label": 0
                },
                {
                    "sent": "The main goal of this talk is trying to avoid to do this these two step.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Airport.",
                    "label": 0
                },
                {
                    "sent": "OK, to do this I've talked a lot about like I don't want worse case analysis, so let's do the classical analysis and just to be clear what I'm going to give you right now is a very partial answer to that problem.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to use.",
                    "label": 0
                },
                {
                    "sent": "Only squares and only a fixed design analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really really partial OK. OK, so let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under his regression, socially you have the square loss.",
                    "label": 0
                },
                {
                    "sent": "So this is my problem which I which I obtain once I have the representative theorem like this and they can put everything into a square number like that.",
                    "label": 0
                },
                {
                    "sent": "And if you set the guidance to zero you get a solution Alpha of that form, and if you if you look at the prediction on your training data then you have K times Alpha is of that form.",
                    "label": 1
                },
                {
                    "sent": "OK, so here since I'm using fixed design setting we only care on predictions on the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, as you can see this is linear and why?",
                    "label": 0
                },
                {
                    "sent": "OK, and this is very common in statistics where predictions we do not problematic statistiques.",
                    "label": 1
                },
                {
                    "sent": "I obtained as a linear function of your observations, and edge typically is called the smoothing matrix, so the hat matrix, because it puts hat on why?",
                    "label": 0
                },
                {
                    "sent": "And essentially what I'm going to do right now is consider the theory of like smoothing in the fixed design, searching for a particular matrix edge which is obtained by.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Analogous.",
                    "label": 0
                },
                {
                    "sent": "So let's look at exactly the analysis.",
                    "label": 0
                },
                {
                    "sent": "We assume that X the points are deterministic, while in a fixed design setting.",
                    "label": 1
                },
                {
                    "sent": "So we called why I buy observations.",
                    "label": 0
                },
                {
                    "sent": "If I could see the expected value, which is what I want to predict an epsilon, I is going to be some noise.",
                    "label": 0
                },
                {
                    "sent": "So here we only assume, and this is important that the noises as finite variance.",
                    "label": 0
                },
                {
                    "sent": "OK, we not assume the noises ID, but we could see the covariance matrix of the of the data so that they had.",
                    "label": 1
                },
                {
                    "sent": "Is your predictor?",
                    "label": 0
                },
                {
                    "sent": "It's called is equal to HY, and so essentially what I want now is to compute the expected value OK of exact the noise variables of the error that I make in reconstructing my signal by synergy.",
                    "label": 0
                },
                {
                    "sent": "OK, so goal is to do this and this has been done many times before.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause it's back to a bar and I think even even earlier.",
                    "label": 0
                },
                {
                    "sent": "And this is an instance of generalized additive models looked at by highstein tips running.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you have an expectation of the square norm, so this is equal to the square norm between the expectation in your in your constant term plus the variance that we can use a file at the Hut is of that form.",
                    "label": 0
                },
                {
                    "sent": "So this is so expectation expectation of the heart is simply edge times E, so you get that term and the the variance of the heart since she had is a linear is linear in the epsilon, we get that simple terms, it get edge time C types edge which is a trace of Ch squared.",
                    "label": 0
                },
                {
                    "sent": "So typically this is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is divided into 2 terms bias term OK to the first term and you have a variance term which is the other one?",
                    "label": 0
                },
                {
                    "sent": "OK, so why the variance term 'cause this is if you have no noise, this term is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the variance term.",
                    "label": 0
                },
                {
                    "sent": "So the key limit.",
                    "label": 0
                },
                {
                    "sent": "OK, you have to notice that.",
                    "label": 0
                },
                {
                    "sent": "OK, so as Lambda as Lambda increases that this guy this guy decreases OK have less variance if you regularize a bit more, but is Lambda is increasing, you get more bias cases classical classical as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's give a name a bit to those quantities, so it's still a bias.",
                    "label": 0
                },
                {
                    "sent": "Terms of science term.",
                    "label": 0
                },
                {
                    "sent": "So if see if the noises ID again, you can replace these trace of C * 8 square by simple square over N times, trace trace of edge square so that quantity OK is often referred to as the degrees of freedom of your exposing metrics.",
                    "label": 1
                },
                {
                    "sent": "OK, so instead they are clear if it is trace of edge of price of Edge Square.",
                    "label": 0
                },
                {
                    "sent": "But since I'm going to use another one that's not so that's a big problem.",
                    "label": 0
                },
                {
                    "sent": "OK so wait, it causes.",
                    "label": 0
                },
                {
                    "sent": "This is hidden why?",
                    "label": 0
                },
                {
                    "sent": "Because essentially the way to see that is as an implicit number of parameters associated with your.",
                    "label": 1
                },
                {
                    "sent": "With your exposing problem.",
                    "label": 0
                },
                {
                    "sent": "Why is it so?",
                    "label": 1
                },
                {
                    "sent": "Because if you if the rank of the econometrics is POK and normalize equal to 0, then P the decades of freedom using that definition of definitions to equal 2 POK.",
                    "label": 0
                },
                {
                    "sent": "So of course, as when you eigenvalues not equal to 0 after a certain point, this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not the case.",
                    "label": 0
                },
                {
                    "sent": "OK, you have something which is.",
                    "label": 0
                },
                {
                    "sent": "Only an implicit and Arthur metals.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here for my results I will need to design another another version OK, which will be adapted to my proof technique essentially.",
                    "label": 0
                },
                {
                    "sent": "So here we replace.",
                    "label": 0
                },
                {
                    "sent": "So the the decades of freedom which appear here is a trace of edge square.",
                    "label": 0
                },
                {
                    "sent": "So it is some of these are some of.",
                    "label": 0
                },
                {
                    "sent": "So some of the elements, the sum of the squared elements of the metrics edge and I'm going to approach this by the largest element of the digital edge.",
                    "label": 0
                },
                {
                    "sent": "OK times N. OK so this you have this inequality that trace of edge square.",
                    "label": 0
                },
                {
                    "sent": "OK since edge in my case as all values less than one is less than tracer Edge and trace of edge, the sum of the Edmund norm of the diagonal.",
                    "label": 0
                },
                {
                    "sent": "OK and I replace the element known by end Times Infinity norm.",
                    "label": 0
                },
                {
                    "sent": "So here I will depart from the classical definition.",
                    "label": 0
                },
                {
                    "sent": "Because my bound requires requires to go through that.",
                    "label": 0
                },
                {
                    "sent": "So as I will show this, essentially it's not a big deal if your diagonal elements are not too not too.",
                    "label": 0
                },
                {
                    "sent": "Don't worry too much.",
                    "label": 0
                },
                {
                    "sent": "So if you do have one outlier, this will blow up, but typically, at least in my simulations.",
                    "label": 0
                },
                {
                    "sent": "That term is closed to those times, OK?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going down to the no not yet.",
                    "label": 0
                },
                {
                    "sent": "OK, let's look at so.",
                    "label": 0
                },
                {
                    "sent": "Now we have two.",
                    "label": 0
                },
                {
                    "sent": "We have two quantities which are interest to us.",
                    "label": 0
                },
                {
                    "sent": "If you can.",
                    "label": 0
                },
                {
                    "sent": "If you do column sampling, then you select the columns.",
                    "label": 1
                },
                {
                    "sent": "OK, you get a P explicit features an discribes degrees of hidden giving you D implicit features.",
                    "label": 1
                },
                {
                    "sent": "OK, so the question is, is there any link between PND?",
                    "label": 0
                },
                {
                    "sent": "OK, so the goal of course is to have P as far as small as possible, but large enough so that you don't get any loss in performance.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "In a sense, we must have P because energy.",
                    "label": 0
                },
                {
                    "sent": "So here I have no formal proof and I think it's not always true.",
                    "label": 0
                },
                {
                    "sent": "OK that P has to be larger than the.",
                    "label": 0
                },
                {
                    "sent": "There are cases where if you do a low rank column approximation you did better than the original problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and this happens sometimes.",
                    "label": 0
                },
                {
                    "sent": "But if your problem is set up so that.",
                    "label": 0
                },
                {
                    "sent": "Do the column sampling from ahead out data to avoid that your approximation is related to your predictor, and if the band that you obtained with Academy fix optimal, then there's no way you can beat.",
                    "label": 0
                },
                {
                    "sent": "You can beat D here, but this is not really what I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "I will try to do the opposite.",
                    "label": 0
                },
                {
                    "sent": "How big P needs to be so that you do as well as a full cometrics.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you have P being constant times D and you see the main result?",
                    "label": 0
                },
                {
                    "sent": "I'm going to show now.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "So here in fact there are no assumptions.",
                    "label": 0
                },
                {
                    "sent": "No, like no.",
                    "label": 0
                },
                {
                    "sent": "We're assumptions OK. Or how many?",
                    "label": 0
                },
                {
                    "sent": "But there are natural OK, at least for me.",
                    "label": 0
                },
                {
                    "sent": "So Z is your signal 'cause it is important we want to estimate case or matrix.",
                    "label": 0
                },
                {
                    "sent": "So I define my maximal marginal degrees of freedom, DA different, the radius of the data are square.",
                    "label": 0
                },
                {
                    "sent": "Then I take any random vector for the noise.",
                    "label": 1
                },
                {
                    "sent": "OK, so this will simply have finite variance of 0 mean and it does not need to be ID.",
                    "label": 0
                },
                {
                    "sent": "I think a uniform random subset of the indices in One North.",
                    "label": 1
                },
                {
                    "sent": "I consider my low rank matrix approximation OK and I consider.",
                    "label": 0
                },
                {
                    "sent": "Zehut K which is what I would obtain.",
                    "label": 0
                },
                {
                    "sent": "What I would obtain with the full kernel matrix, an LOLOL which is what I would obtain if I take the approximate, metrics, because the goal is to have the performance of the add her to be the one as the same or close to the same as the performance of the ADK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we obtain is that for any Delta in 01, if P is large enough, then I will comment.",
                    "label": 0
                },
                {
                    "sent": "Is a formula then the expectation?",
                    "label": 0
                },
                {
                    "sent": "Over both your random sample, your random selection of columns and your noise OK is less and the constant times the expectation of your performance 4K with the metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, so if this guy OK if you remove the 1 + 4 Delta, this is the performance of the full mirror metrics.",
                    "label": 0
                },
                {
                    "sent": "OK Times 1 + 4 Delta what we show is that what I show that on average when you average over over I.",
                    "label": 0
                },
                {
                    "sent": "Which is a you are dumb self columns.",
                    "label": 0
                },
                {
                    "sent": "Then you only lose a constant factor.",
                    "label": 0
                },
                {
                    "sent": "OK and the way that constant sector will of course impact the minimum value of P and the minimum value P yes, as some like terms over there an is proportional to the OK.",
                    "label": 0
                },
                {
                    "sent": "So here is a P has to be larger than a constant times D. OK so here this is what we want.",
                    "label": 0
                },
                {
                    "sent": "I want you to show namely that if P is a linear function of D then you don't lose any.",
                    "label": 0
                },
                {
                    "sent": "You don't lose any.",
                    "label": 0
                },
                {
                    "sent": "The performance when you applied your technique.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's look, let's discuss.",
                    "label": 0
                },
                {
                    "sent": "Let's discuss the results, but at least.",
                    "label": 0
                },
                {
                    "sent": "But if you do PCA then you will get the squared.",
                    "label": 0
                },
                {
                    "sent": "You won't be able to do this another time.",
                    "label": 0
                },
                {
                    "sent": "So the proof technique follows like classical result from top and his students.",
                    "label": 0
                },
                {
                    "sent": "So here I think a key element compared to produce work on that similar techniques that we don't need.",
                    "label": 0
                },
                {
                    "sent": "Again gaps.",
                    "label": 0
                },
                {
                    "sent": "OK so the activities, candy cake indicate to zero gracefully.",
                    "label": 0
                },
                {
                    "sent": "And there's no need for gap.",
                    "label": 0
                },
                {
                    "sent": "There's no assumptions on the noise.",
                    "label": 1
                },
                {
                    "sent": "So we get a relative approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "OK, so here one key limitation, at least of the weight is return is the fact that I take only expectations.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not.",
                    "label": 0
                },
                {
                    "sent": "With high probability, so in fact is a proof we prove a result in high probability with respect to.",
                    "label": 0
                },
                {
                    "sent": "I OK with respect to the sampling of the columns.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to do the same thing with respect to the sampling of the noise, then you need extra assumptions and the goal was to go beyond those extra assumptions.",
                    "label": 1
                },
                {
                    "sent": "We also also get some longer term OK and here the only appears in the log factor.",
                    "label": 0
                },
                {
                    "sent": "OK, remember that Lambda will decay to zero quite quite fast and this we avoid having to be killed by longer because it's only in the long term.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, one of the criticisms that you might want to go beyond the squares, and you have ways to do it.",
                    "label": 0
                },
                {
                    "sent": "I haven't done it, but I thought I would take about 3 to talk about more of your work, which nobody knows about.",
                    "label": 0
                },
                {
                    "sent": "Since I have time now, I can talk about it a bit.",
                    "label": 0
                },
                {
                    "sent": "I think it's cute and I'm going to use lack results from optimization.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the theory of several gardens.",
                    "label": 0
                },
                {
                    "sent": "OK, so This is why don't.",
                    "label": 0
                },
                {
                    "sent": "The real valued function G is set set coordinate essentially.",
                    "label": 0
                },
                {
                    "sent": "If the 3rd order directive OK is controlled by the power of the second derivative, OK, so just by this simple assumption you can derive like convergence bounds for Newton in a very very elegant way, and very sharp and short way.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately for the logistic is not true.",
                    "label": 0
                },
                {
                    "sent": "OK, you have to define a new notion of several gardens and essentially this is simply a bound of.",
                    "label": 0
                },
                {
                    "sent": "The third debate is bounded by a constant times.",
                    "label": 0
                },
                {
                    "sent": "The second derivative, but with no with no powers.",
                    "label": 0
                },
                {
                    "sent": "OK, essentially using that same tool and all the proof techniques of linear of skin is still off.",
                    "label": 0
                },
                {
                    "sent": "You can derive no static version of the results that you have the classical aesthetic results which you obtained from estimation and the key element is that you get the exact 1st order term.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't do a worst case analysis, so the third term is the exact aseptic term and then you bound the remainders using those techniques.",
                    "label": 0
                },
                {
                    "sent": "So what I've started to do, but.",
                    "label": 0
                },
                {
                    "sent": "Gotta get lost for the moment is try to apply the same thing to that to that problem as well, because this would show that also for a bit harder problems and maybe closer to what we want to do machine learning you can have similar similar behavior.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's go to the very don't be afraid by the large large large table.",
                    "label": 0
                },
                {
                    "sent": "OK so here one also.",
                    "label": 0
                },
                {
                    "sent": "One problem is about Alpha.",
                    "label": 0
                },
                {
                    "sent": "Not comfortable.",
                    "label": 0
                },
                {
                    "sent": "I talk to people that how should you choose Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK so it's a key problem.",
                    "label": 0
                },
                {
                    "sent": "So choice of Lambda is really a problem.",
                    "label": 0
                },
                {
                    "sent": "So I try to do is try to see what would be the optimal Lambda parameter.",
                    "label": 0
                },
                {
                    "sent": "And for similar situations.",
                    "label": 0
                },
                {
                    "sent": "OK, so here what is important is in that context, and this was of course notice before if the decay of the eigenvalues of the kernel matrix OK and also the decay of the coordinates of your of your function on those eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "OK, I will call you I the decay of the counter metrics OK, and I will assume they are summable.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a trace grows with North, which is typically the case, and I will also assume that the sum of neways OK is also summer.",
                    "label": 0
                },
                {
                    "sent": "Also that the overall norm of your prediction is also bounded from below.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have taken several cases.",
                    "label": 0
                },
                {
                    "sent": "Either you have you have a polynomial decay of your problem OK, or an exponential decay.",
                    "label": 0
                },
                {
                    "sent": "OK, so for all of those cases I took the bias and the variance which of the formulas which are shown before and found that equivalent for the BIOS for the variance for the BIOS from which I'm computing the optimal Lambda OK then I get.",
                    "label": 0
                },
                {
                    "sent": "We put it back into those terms and some I get the best patient performance and then I guess the degrees of freedom for the optimal Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, so here this takes the same the same way.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at what's good and not good about the table.",
                    "label": 0
                },
                {
                    "sent": "OK, first, what are the good?",
                    "label": 0
                },
                {
                    "sent": "The good, right?",
                    "label": 0
                },
                {
                    "sent": "So it's known by for awhile and can see paper by John Stoner by John Watt and his colleagues that.",
                    "label": 0
                },
                {
                    "sent": "OK, so here in UI is a characteristic of your signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so given the given the signal, you have an optimal estimation.",
                    "label": 0
                },
                {
                    "sent": "It is known that that optimal right for those types of decay is that type of rate.",
                    "label": 0
                },
                {
                    "sent": "OK, but worth.",
                    "label": 0
                },
                {
                    "sent": "And this is achieved by essentially by that line and that lens of the red points are the situation, so kernels which achieves the minimax estimation rate and for exponential decay you get Logan over North OK, which is over there.",
                    "label": 0
                },
                {
                    "sent": "So here's the intuition is the following.",
                    "label": 0
                },
                {
                    "sent": "So if your new I saw you again values of your metrics, oh sorry, this should be.",
                    "label": 0
                },
                {
                    "sent": "There should be a zed distribute the function.",
                    "label": 0
                },
                {
                    "sent": "If those decay a fast way, this corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Smooth functions OK, think about new I and UI and UI is being free coefficients OK if they're free coefficients, then if you have a fast decay, have smooth function.",
                    "label": 0
                },
                {
                    "sent": "So we have a simple problem to estimate, whereas if you use the decay of values of K is fast then it means that you have a smaller feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, so fast as a DK this module space.",
                    "label": 0
                },
                {
                    "sent": "So here we get.",
                    "label": 0
                },
                {
                    "sent": "We see two situation.",
                    "label": 0
                },
                {
                    "sent": "OK first, if the space is too large, space is too large.",
                    "label": 0
                },
                {
                    "sent": "Mean it means that the DK.",
                    "label": 0
                },
                {
                    "sent": "Of the eigenvalues of the cosmetics is.",
                    "label": 0
                },
                {
                    "sent": "Slow, OK, and this is the case here where beta beta is quite small and as you can see that you do underfitting OK, you don't achieve the mini Max rate.",
                    "label": 0
                },
                {
                    "sent": "OK, this is OK, that's life.",
                    "label": 0
                },
                {
                    "sent": "If you take two big official space you won't be able to regularize correct way and you lose performance.",
                    "label": 0
                },
                {
                    "sent": "So this is classical, but to me what I want you to get too.",
                    "label": 0
                },
                {
                    "sent": "In fact this was related to work, don't with Mark Smith Nickel Arrow.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper, but which we presented as a main conference.",
                    "label": 0
                },
                {
                    "sent": "We said, oh we need the parameter Lambda to be bigger than North.",
                    "label": 0
                },
                {
                    "sent": "OK, for our method to do the work.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to know is it really true that Lambda is always bigger than North to get good performance here you duties that in some cases if if if the feature space is too small, OK, then sometimes even though you get the optimal convergence right at the end.",
                    "label": 0
                },
                {
                    "sent": "Lambda would be very small and can be exponentially smaller.",
                    "label": 0
                },
                {
                    "sent": "This means that if you want to find the optimal value machine, you won't be able to do it because essentially you will have to go below matching precision to get the optimal right.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is something in fact, which had been, which had been noticed by other people and nuclear rule that.",
                    "label": 0
                },
                {
                    "sent": "Colonel aggregate too fast and sometimes you got is very hard to have a gas.",
                    "label": 0
                },
                {
                    "sent": "Regular fit OK if you put under Cosimo with gas and kernel, often it doesn't overfit stabilized and not do terrible.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one way of seeing it.",
                    "label": 0
                },
                {
                    "sent": "Is that the optimal Lambda can be pretty small if your kernel kernel is 2 is too small or if your space is too small.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is 101.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's forget about that.",
                    "label": 0
                },
                {
                    "sent": "So let's look at small at some at some some simulations.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I took really basic simulations, for which I couldn't.",
                    "label": 0
                },
                {
                    "sent": "I could like.",
                    "label": 0
                },
                {
                    "sent": "I knew all the decades of all things.",
                    "label": 0
                },
                {
                    "sent": "So this essentially, but you're disposing clients and with the full coefficients, and I've tried several several things.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the left here.",
                    "label": 0
                },
                {
                    "sent": "I show OK, so I've taken several values for the kernel parameters and several one value for the kernel parameter for the signal parameter and several values for the.",
                    "label": 0
                },
                {
                    "sent": "Odometers OK and you compare the.",
                    "label": 0
                },
                {
                    "sent": "What is a log of the Lambda?",
                    "label": 0
                },
                {
                    "sent": "OK, the optimal Lambda.",
                    "label": 0
                },
                {
                    "sent": "As you can see it stabilizes.",
                    "label": 0
                },
                {
                    "sent": "That goes to zero smoothly when betrayals Wanan four, which corresponds to feature spaces, which are a big enough.",
                    "label": 0
                },
                {
                    "sent": "But here when beta is 8, so this means the feature space is too small.",
                    "label": 0
                },
                {
                    "sent": "Then whatever you want to do, it wants to have as small as possible parameter an here in such a rights at machine precision and for that.",
                    "label": 0
                },
                {
                    "sent": "Below 'cause if I go below I can't do the estimation OK and in terms of performance this is a generalization performance.",
                    "label": 0
                },
                {
                    "sent": "You can see that so the optimal one is a blue one and this corresponds to the my previous slide for the red Red columns.",
                    "label": 0
                },
                {
                    "sent": "When the Colonel is.",
                    "label": 0
                },
                {
                    "sent": "When does this is optimal convergence rate the black correspond to better course.",
                    "label": 0
                },
                {
                    "sent": "One answer better equals 1 means that the GK here is is a slow decay.",
                    "label": 0
                },
                {
                    "sent": "So this means that I will tend to overfit and this is what I obtain.",
                    "label": 0
                },
                {
                    "sent": "I don't get the same convergence rate and for the this curve I'm supposed to get the best possible performance, but to get the best possible performance and I need a very small number and it says I can't numerically do Rice bowl Lambda.",
                    "label": 0
                },
                {
                    "sent": "Typically Matlab impose a land of 10s.",
                    "label": 0
                },
                {
                    "sent": "16 I can't achieve the optimal constraint because of medical problems, and in fact I was quite surprised about that.",
                    "label": 0
                },
                {
                    "sent": "And final simulation.",
                    "label": 0
                },
                {
                    "sent": "Is I tried to compare, try to see if my results which are obtained like the previous slides is correct or not, at least empirically satisfied or not.",
                    "label": 0
                },
                {
                    "sent": "I'm comparing the performance of the full matrix over the performance of the approximate approximation of the matrix and I think I compute the rank.",
                    "label": 0
                },
                {
                    "sent": "The rank at which attained like 10% I make less than 10% of increase loss with respect the full matrix.",
                    "label": 0
                },
                {
                    "sent": "So here I compute several things.",
                    "label": 0
                },
                {
                    "sent": "I compared to the true degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "OK in blue to my know the true degrees of freedom and read the new version version of this freedom is in black in the is in blue OK. As you can see the rank the optimal rank at the end to achieve dollars of performances between like one and two which is close to what I was expecting.",
                    "label": 0
                },
                {
                    "sent": "The sense it is linear.",
                    "label": 0
                },
                {
                    "sent": "The rank that we need to get the same performance is linear in terms of freedom whether you use the.",
                    "label": 0
                },
                {
                    "sent": "Classical version in red or my version.",
                    "label": 0
                },
                {
                    "sent": "So here in the dotted is the same thing with incomplete crisci decomposition with pivoting.",
                    "label": 0
                },
                {
                    "sent": "So instead of doing random sampling, I try to do a greedy selection of columns so it does a bit better, but you get at least of those simulations to get the factor of two OK.",
                    "label": 0
                },
                {
                    "sent": "In fact I had.",
                    "label": 0
                },
                {
                    "sent": "I thought that by doing like really selection you should do much better.",
                    "label": 0
                },
                {
                    "sent": "OK you do a bit better, but at least in those simulations, don't do immensely better.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Castle.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prudent.",
                    "label": 0
                },
                {
                    "sent": "So I've tried to provide analysis of column sampling for kernel, least square regression and here the key take home message that the degrees of freedom, which is a classical quantity that people consider for statistical reason.",
                    "label": 1
                },
                {
                    "sent": "Is also useful.",
                    "label": 0
                },
                {
                    "sent": "You saw a good guidance for as a computational computational tool that you see how big they may need to be.",
                    "label": 0
                },
                {
                    "sent": "So of course I'll submit several problems with the current approach in practice, since you don't know the decades of freedom when you start doesn't give you huge improvement to select P, although you can have ways to approximate the econometrics with a P with P decently large.",
                    "label": 0
                },
                {
                    "sent": "Get a small simple approximation of D and try to iterate like this, so it's not.",
                    "label": 0
                },
                {
                    "sent": "That's the problem for the moment, so there's no doesn't provide actual an actual algorithm.",
                    "label": 0
                },
                {
                    "sent": "They also extensions which are worth considering.",
                    "label": 1
                },
                {
                    "sent": "First is way to go beyond uniform sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a people have considered different ways of something.",
                    "label": 0
                },
                {
                    "sent": "The columns not uniformly but with a better better way here clearly going to random design is important, so here to have the most simple result.",
                    "label": 0
                },
                {
                    "sent": "OK, very simple.",
                    "label": 0
                },
                {
                    "sent": "It's no not so many assumptions.",
                    "label": 0
                },
                {
                    "sent": "I will assume the fix design, analysis and clearly doing going on design by by using results of super Sham and talk.",
                    "label": 0
                },
                {
                    "sent": "Then you could do the same thing, but here I think we need to add extra assumptions.",
                    "label": 0
                },
                {
                    "sent": "Also, currently if you have, if you have a rank of P for the approximation, you achieve P ^2 * N complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so here P the main message of my talk is P is close to the degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you gain as a complete as you get as a capacity at the moment is the square times N OK. Is it possible to achieve the time then?",
                    "label": 0
                },
                {
                    "sent": "OK, that's a question that would be nice to answer.",
                    "label": 1
                },
                {
                    "sent": "Finally, trying to go beyond these squares.",
                    "label": 0
                },
                {
                    "sent": "OK, so hinted at possibilities for logistic regression.",
                    "label": 0
                },
                {
                    "sent": "You could do a similar thing for the SVM because like nice paper by Blanchar colleagues, we showed that the quantity the degrees of freedom, the other similar quantity, the SVM case and then the last two.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "In fact this was the motivation behind at work trying to see if you could do an online technique to do control machines with the complexity that goes sublinearly in or not quadratically.",
                    "label": 0
                },
                {
                    "sent": "And as a first step had to see whether you can do column sampling in an efficient way.",
                    "label": 0
                },
                {
                    "sent": "So clearly what we're going to focus on is trying to do the true online setting with the true decay of Lambda OK, as opposed to the current classical techniques.",
                    "label": 0
                },
                {
                    "sent": "And finally, as the last advertisement for a septic statistics activities.",
                    "label": 0
                },
                {
                    "sent": "I think if you do a very stochastic gradient OK and this is well known from 1992, but similar seemingly forgotten often at NIPS is that you get the optimal governance, right?",
                    "label": 0
                },
                {
                    "sent": "OK, you get the.",
                    "label": 0
                },
                {
                    "sent": "Without any metric conversions you get the the you achieve the camera lower bound and I think this is essentially what I think should be needed to get this the same performance at the full as a full problem.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}