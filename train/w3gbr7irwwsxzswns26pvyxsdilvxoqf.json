{
    "id": "w3gbr7irwwsxzswns26pvyxsdilvxoqf",
    "title": "Accurate Max-margin Training for Structured Output Spaces",
    "info": {
        "author": [
            "Sunita Sarawagi, Indian Institute of Technology Madras"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_sarawagi_acmmt/",
    "segmentation": [
        [
            "Then it will be presented by Sunita Saraogi.",
            "Thanks Pedro.",
            "So this talk is about designing loss functions for making structured predictions.",
            "We want to design loss functions which of course have to be accurate and also which have to be computably tractable.",
            "Given that in structured output space is the space of possible outputs for any given input is expn."
        ],
        [
            "Initially large, so again just a brief overview of structured models, so in structured models you have an input X for which you have to predict AY, which is structured, structured in the sense that it could be anything or most it could be a vector.",
            "It could be like suppose if your input X is a sentence, output could be a vector of labels assigned to each word in the sentence.",
            "It could be a segmentation of the sentence.",
            "It could be a parse tree and alignment.",
            "Whatever.",
            "The only thing that is required from the user in.",
            "Using a structured model is to define a feature vector, which is a fixed dimensionality which takes us input an XY pair, the input output pair and gives a fixed dimensional real representation of this XY pair.",
            "The model in turn, assigns a weight vector to this feature vector so that now for any given XY pair, you are able to define a score.",
            "So the score of predicting Y for an input X.",
            "Is this dot product of the weight vector with the feature vector?",
            "So now your structured model just has to find the Y for which the score is maximum.",
            "So the predicted by star for any given input X is the Y amongst the exponentially many possible ways for which the score is maximum.",
            "And because of this exponential space, a voice within which up to search for the best possible.",
            "But why you have to be careful about how you design your feature vector.",
            "So typically the feature vectors are designed in such a way that they decompose over smaller parts of why.",
            "So?",
            "For example, if I consider the case where my input X is a sentence.",
            "And my output is a vector of labels assigned to each word of the sentence.",
            "Then the space of possible wise is even when I'm considering binary possibilities for each of these positions is 2 to the power of the length of the sentence, and I would typically decompose the feature vector into the into smaller components of wine.",
            "So for typical information extraction task, the components might be like, say, adjacent voice.",
            "So now the exploiting this decomposability of why?",
            "You can design a number of dynamic programming based algorithms to solve this efficiently.",
            "So this talk is not about solving this inference task, although it's an challenging problem on which lot has."
        ],
        [
            "I'm done, but about finding the W's so we are interested in the training task where we are given as input several XY pairs which are known to be correct.",
            "And now, unlike for scalar prediction, not all wise are equally bad.",
            "Because why is like this big extensive thing?",
            "There are some wise which are more wrong than others and therefore the user in addition can define an error function which for an example I will output a positive real value indicating how rank you would be if you were to predict Y.",
            "For this example I for the XYZ example.",
            "And again for computational reasons, and also because it makes sense from an application point of view, the error typically decomposes over smaller parts, so we will use this notation so the error of this entire structured output.",
            "Why decomposes over smaller parts C of so that you can say that the error is defined over this model parts?",
            "Why see so very popular example is Hamming error, where you claim that the error.",
            "Of predicting Y for the instance is essentially just the number of positions in which the predicted Y differs from the correct way.",
            "So this is our setup and we want to find a W which gives us low training error and of course it has to generalize over unseen instances.",
            "Now it's Williams have won our confidence as good models which are accurate and also computationally tractable in the scalar Bolt.",
            "So a bunch of work has been done on giving us the Max margin advantage of SVM to making structured predictions."
        ],
        [
            "So this set to some of the two of the most prominent works in this direction.",
            "So Ben Taskar with his advisor and a few others, has proposed a formulation Max margin formulation which actually uses a loss function which we're going to call margin scaling.",
            "And then there is this sort of almost independently developed work on again on structured Max margin structure prediction, which proposes the margin scaling loss function, but also a few others.",
            "And of course there are many, many other papers which use these formulations or develop on this formulation, so I'll provide efficient algorithms for solving simultaneously this inference and the parameter estimation tasks.",
            "But in this talk we are essentially concerned about designing loss functions."
        ],
        [
            "So let's first look at the most popular of this last loss functions, which is the margin scaling loss function.",
            "So here we are at the at the very start we are interested in making sure that the score of the correct output Yi.",
            "For an input XI is separated from the score of all other incorrect wise by a margin which is scaled by the error of that incorrect way.",
            "So this basically.",
            "It makes sense that the things which are really wrong have to be really, really far away from the correct ones, and so on.",
            "And then."
        ],
        [
            "You can kind of add around it the slack variables, IE to make it handle the non separable case and this gives you again a QP formulation of your training task which looks similar to the SVM QB.",
            "But let's look at this formulation for a moment.",
            "The first thing which is very dissatisfying is that you know we are asking the user to provide us a feature vector, and this error function.",
            "In general.",
            "This could be some arbitrarily scaled quantities which have nothing to do with each other and in the training objective we have combined them additively.",
            "So, so this is the second formulation which was proposed in the secondary.",
            "This paper is this lack scaling formulation which actually is mixed.",
            "There seems to be more sensible, So what that formulation claims is that make sure that the correct why is separated from other incorrect wise by still a margin of 1.",
            "But then the fact that you have different wise which are wrong in different ways.",
            "You capture that only a mix of eyes which actually violate your margin of 1.",
            "And then you scale them.",
            "By this quantity, which essentially makes sure that the ones which are rank by a large amount are multiplicatively penalized, and like for the margins case, where they are additively penalized.",
            "So once kind of small nice property of this formulation which is pointed also in the securities paper, is that you could arbitrarily scale the error function, and for that you can find a corresponding, see which will give rise to the same solution, and such a property does not exist for the margin.",
            "Last formulation, but the reason margin scaling as has survived and almost you know people don't even talk about slacks killing at all.",
            "You know it's not like OK, you know we're using margin scaling because it's computationally efficient, but here we are apologetic about it.",
            "Now it has just vanished from the literature.",
            "So So what we have in this talk will try to say that, OK, it's not so bad you know Slack scaling.",
            "The reason it says it has vanished from the literature is because it imposes kind of difficult inference task.",
            "So let me elaborate on that.",
            "So if you compare both of these formulations, the main challenge to solving them efficiently is that the number of constraints is exponential in the number of possible in this in the size of the input, because the set of possible wise is exponential in the size of the input.",
            "So it's the same for both margin and Slack scaling, and so you have to use some cutting plane kind of method to solve it efficiently.",
            "And now if we look at."
        ],
        [
            "The one step of the cutting plane algorithm for each of these loss formulation.",
            "You will find that for matching scaling you have to find for so you have you have a W and now you have to find the constraint which is most violated by the cutting plane algorithm and this is going to be the most violated constraint for any given instance I.",
            "When you're using the margin loss Kelly.",
            "So that is the one for which the sum of the school and the error function is maximized.",
            "Now when the error function in the score function.",
            "Decompose the same way, then the kind of inference that you would have to eliminate anyway.",
            "Implement for doing structured prediction would work well for finding the most politic constraint.",
            "In contrast, for Slack scaling, the most violating constraint is the one for which the score minus the XI I divided by the error function is maximum, and even if the error function decomposes over smaller parts, it does not help you at all, because it appears in this funny way in the denominator.",
            "But MFC convenient to argue that actually Slack scaling provides better generalizability.",
            "And qualitatively, you know, I can just appeal to you.",
            "So when you do Slack scaling, you concentrate on the wise, which are relevant for prediction because you're really concentrating on the wise, which are kind of close to the correct why.",
            "Whereas in margin scaling you will be worrying about wise even if they are not likely to appear in the prediction.",
            "Ultimately, when you are doing structured prediction.",
            "You will be on your test data will be finding that Y which has the highest score.",
            "So you really don't care about the wise which come in ranked twenty $200 whereas in slacks in margin scaling you will continue to worry about them.",
            "So so where I said."
        ],
        [
            "So because of that, even empirically we did this experiment on two structured learning tasks where we found that slack scaling gives better test error then margin scaling.",
            "So these are two tasks, one is sequence labeling and the segmentation both arise in information extraction where we have inputs are sentences and we have to assign either.",
            "We can pose it as a sequential labeling problem for each word in the sentence we have to assign a label or as a segmentation problem.",
            "We're given a sentence we have to break it up such that each segment corresponds to an entity.",
            "And we are considering three different kinds of information extraction tasks.",
            "One is address information extracting elements of addresses from the text strings.",
            "The second is this Cora citation tasks where we have to find things like author names and titles from citation strings, and this is the famous kernel task where we have to find named entities like people name and organization name from sentences and the Y axis.",
            "Here you have spanned F1 error, the blue line, the blue bars are for margins, killing the red bars are first class LAX killing.",
            "We find that you know Slack scaling can sometimes give very impressive gains over margin scaling.",
            "So now the second part of this talk is about what we can do to solve this lack inference problem so that we can apply it on arbitrary structure learning tasks."
        ],
        [
            "So the key thing that we employ is that so if you look at the slack inference problem, we have to find the Y for which the school and the this particular funny ratio of the error is maximized.",
            "And the first observation we make is that although this is kind of otherwise, as is hard to get a handle of, but this happens to be concave in error in the in the quantity in the denominator.",
            "And therefore you can apply variation methods to rewrite this as a linear function.",
            "As of the error as follows.",
            "So actually this quantity is equal to this expression over here, where Lambda is your variational variable, it's a scalar of positive scalar and now the error appears scaled by this positive scalar quantity.",
            "And then there is this term which does not depend on why this is actually very simple.",
            "Generation will not go through the derivation in the talk, it's there in the paper.",
            "You just take this formula as is.",
            "So now so with this as a rewrite of this difficult quantity, Now we can solve the slack inference problem as follows.",
            "So our goal is to find the Y for which the score minus this quantity is maximized.",
            "So this is equal to this quantity based on the derivation that we just did.",
            "And this is the first place where we are using an approximation.",
            "We interchange the Max and min.",
            "And now this when you interesting formats in the mean, you get an upper bound, so we want to find the minimum over all possible scalar choice of lambdas and for each given Lambda will find the Y for which this quantity is maximized.",
            "And then there is this term which comes out of the variational approximation."
        ],
        [
            "And this particular inference task can be solved efficiently for two reasons.",
            "First, you know, this quantity actually is very similar to the quantity for which you have to optimize in margin scaling."
        ],
        [
            "And then the second nice thing is that this entire quantity over here is actually convex in Lambda.",
            "So you can search for the best possible Lambda using any method like Saline search and also because it so happens that we are only interested in wise which will violate the constraints of the cutting of the QP.",
            "We can come up with a bounded interval.",
            "Again, I'm not go through the derivation of this interval in the talk which will enable us to actually claim also finite termination of the search for the lambdas.",
            "So this gives us.",
            "A way of finding the most violating approximate slack constraint by trying out different values of lambdas, scaling the error function by that Lambda quantity."
        ],
        [
            "And finding the best possible way.",
            "And this actually happens to approximate the objective.",
            "The slack objective quite well, as you see in this diagrams over here."
        ],
        [
            "So this is the this is the part about how we can adapt an existing nice formulation to doing structured training.",
            "But the second part is what more?"
        ],
        [
            "Excited about and I want to make a case that just because of their Genesis as arising out of scalar methods, the structured prediction problem has not been thought of afresh in the context of the error functions that arise in standard structure learning.",
            "So one of the big source of problem is the use of a shared slack variable in both margin and Slack scaling for all possible wise coming out of an instance.",
            "I we have a single shared Slack variable and that can sometimes give sort of an under coverage of the loss function."
        ],
        [
            "As I re listed in this example, so here I have say a structured prediction problem where I have to predict for a sentence with three tokens are labeled vector of size 3, and let's assume that 000 is the correct prediction and this prediction has a score of 0 for the correct for the current W vector.",
            "And let's assume that there are two other labelings Y1 and Y2 which differ from the correct YO in positions two and three respectively and therefore.",
            "They have a Hamming loss of one each, and they also happen to have the same score of 0.",
            "And also, let's assume that all the other labelings are comfort ubly separated from these three problems from these two other problematic labelings so we don't have to worry about any of the other labelings now in this case both margin and the slack loss will be one.",
            "But now it's considered the case there.",
            "Why to happens to involve feature vectors which makes Y2 non separable from this the third position of the correct way, but because of the shared slack variable, now the GOP will be perfectly happy to terminate and it will not do nothing about trying to separate why one, even though why one in general in structured learning would involve a different set of features."
        ],
        [
            "So this motivates for us at different loss function, which we called loss pose learned.",
            "So imposing on we try to claim that look if your error is additive over different positions, then for generalizability you better in introduce a matching for each possible error position.",
            "It's not just sufficient to have one single margin constraint on the entire labeling when your error is adding more positions than the margin also has to be imposed at each and every position, so this is more to this formulation achieves this mixture.",
            "So here now we have a slack variable for each position of our structured labeling, and this constraint basically tells us that the correct why has to be separated from all the incorrect wise which differ from the correct why at the CIA.",
            "Addition and you will have one such constraint for each of these positions.",
            "See of any given I now you can just compare it with Slack scaling to see the differ."
        ],
        [
            "And now this particular loss in the example that I just mentioned.",
            "Now we will have a different Slack variable for each of these three positions, and then the post landmasses two as against the margin, or the slack loss, which was one, and now the optimizer will want to continue to optimize for by one, and it will reduce the puzzle and last one."
        ],
        [
            "Final, so in particular in particular, also we see a big improvement in accuracy.",
            "It generalizes better.",
            "We for example, earlier in segmentation tasks, both margins lack and approximate Slackware, giving same kind of accuracy.",
            "Whereas with coastline we get a dusting."
        ],
        [
            "Improvement in accuracy.",
            "The other advantage of the first land QP is that the inference task that you have to solve for is also simplified significantly, so we have to find for any given I, See that why which differs from the correct labeling only in the seat position.",
            "So chili you can solve the map inference problem first you have to find for any given position the why which differs only at the seat position and you can just search over all possible wise because the outer set.",
            "It's more so this inference problem is also very simple.",
            "For example when we are using Markov models, then you can simultaneously find all possible violating wise by just finding the maximum."
        ],
        [
            "Yes.",
            "So this is my summary in somebody.",
            "So basically there are two contributions.",
            "One it gave an approximation of the slack inference problem that arises when you try to use the slack loss function and that works well empirically.",
            "And then we try to motivate the need for alternative loss functions, and I believe that this is just the way it is.",
            "I am not saying that this is the last word in designing loss functions for structured learning.",
            "I believe that more can be done.",
            "And if you talk to me, I will tell you what I find wrong with the pose loss function.",
            "And you know definitely more work is needed in this area.",
            "Yes.",
            "Yes.",
            "Although you know when you tried to implement the Pose lawn equivalent of lagless, there were some numerical instability issues, which I mean I have not investigated this enough.",
            "But yes, all of these have a parallel in the log world, so I have a question.",
            "So one of the nice things about this, like scaling formulation, is that you know there's a theorem that says by adding a polynomial of constraints you get to satisfy all the exponential number.",
            "Right now you give you get there's this theorem.",
            "Yes yes, yes.",
            "So that my question is like this theorem still applies here.",
            "What's the general section has trees to pass trees?",
            "So you have it up there on the slide, yes?",
            "How do you come up with this individual margins, infection?",
            "So in the case of Paris trees, you try to claim for each internal node of the tree whether it has the correct nonterminal label assigned to it, and whether it's two children are correct or not.",
            "So that is how you decompose the power.",
            "The loss function in the case of prostrates for segmentation.",
            "Also, the decomposition is somewhat nontrivial if there in the paper.",
            "More questions.",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it will be presented by Sunita Saraogi.",
                    "label": 0
                },
                {
                    "sent": "Thanks Pedro.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about designing loss functions for making structured predictions.",
                    "label": 0
                },
                {
                    "sent": "We want to design loss functions which of course have to be accurate and also which have to be computably tractable.",
                    "label": 0
                },
                {
                    "sent": "Given that in structured output space is the space of possible outputs for any given input is expn.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Initially large, so again just a brief overview of structured models, so in structured models you have an input X for which you have to predict AY, which is structured, structured in the sense that it could be anything or most it could be a vector.",
                    "label": 0
                },
                {
                    "sent": "It could be like suppose if your input X is a sentence, output could be a vector of labels assigned to each word in the sentence.",
                    "label": 0
                },
                {
                    "sent": "It could be a segmentation of the sentence.",
                    "label": 0
                },
                {
                    "sent": "It could be a parse tree and alignment.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 0
                },
                {
                    "sent": "The only thing that is required from the user in.",
                    "label": 0
                },
                {
                    "sent": "Using a structured model is to define a feature vector, which is a fixed dimensionality which takes us input an XY pair, the input output pair and gives a fixed dimensional real representation of this XY pair.",
                    "label": 0
                },
                {
                    "sent": "The model in turn, assigns a weight vector to this feature vector so that now for any given XY pair, you are able to define a score.",
                    "label": 0
                },
                {
                    "sent": "So the score of predicting Y for an input X.",
                    "label": 1
                },
                {
                    "sent": "Is this dot product of the weight vector with the feature vector?",
                    "label": 0
                },
                {
                    "sent": "So now your structured model just has to find the Y for which the score is maximum.",
                    "label": 0
                },
                {
                    "sent": "So the predicted by star for any given input X is the Y amongst the exponentially many possible ways for which the score is maximum.",
                    "label": 0
                },
                {
                    "sent": "And because of this exponential space, a voice within which up to search for the best possible.",
                    "label": 0
                },
                {
                    "sent": "But why you have to be careful about how you design your feature vector.",
                    "label": 0
                },
                {
                    "sent": "So typically the feature vectors are designed in such a way that they decompose over smaller parts of why.",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "For example, if I consider the case where my input X is a sentence.",
                    "label": 0
                },
                {
                    "sent": "And my output is a vector of labels assigned to each word of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Then the space of possible wise is even when I'm considering binary possibilities for each of these positions is 2 to the power of the length of the sentence, and I would typically decompose the feature vector into the into smaller components of wine.",
                    "label": 0
                },
                {
                    "sent": "So for typical information extraction task, the components might be like, say, adjacent voice.",
                    "label": 0
                },
                {
                    "sent": "So now the exploiting this decomposability of why?",
                    "label": 0
                },
                {
                    "sent": "You can design a number of dynamic programming based algorithms to solve this efficiently.",
                    "label": 0
                },
                {
                    "sent": "So this talk is not about solving this inference task, although it's an challenging problem on which lot has.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm done, but about finding the W's so we are interested in the training task where we are given as input several XY pairs which are known to be correct.",
                    "label": 0
                },
                {
                    "sent": "And now, unlike for scalar prediction, not all wise are equally bad.",
                    "label": 0
                },
                {
                    "sent": "Because why is like this big extensive thing?",
                    "label": 0
                },
                {
                    "sent": "There are some wise which are more wrong than others and therefore the user in addition can define an error function which for an example I will output a positive real value indicating how rank you would be if you were to predict Y.",
                    "label": 0
                },
                {
                    "sent": "For this example I for the XYZ example.",
                    "label": 0
                },
                {
                    "sent": "And again for computational reasons, and also because it makes sense from an application point of view, the error typically decomposes over smaller parts, so we will use this notation so the error of this entire structured output.",
                    "label": 1
                },
                {
                    "sent": "Why decomposes over smaller parts C of so that you can say that the error is defined over this model parts?",
                    "label": 0
                },
                {
                    "sent": "Why see so very popular example is Hamming error, where you claim that the error.",
                    "label": 0
                },
                {
                    "sent": "Of predicting Y for the instance is essentially just the number of positions in which the predicted Y differs from the correct way.",
                    "label": 1
                },
                {
                    "sent": "So this is our setup and we want to find a W which gives us low training error and of course it has to generalize over unseen instances.",
                    "label": 0
                },
                {
                    "sent": "Now it's Williams have won our confidence as good models which are accurate and also computationally tractable in the scalar Bolt.",
                    "label": 0
                },
                {
                    "sent": "So a bunch of work has been done on giving us the Max margin advantage of SVM to making structured predictions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this set to some of the two of the most prominent works in this direction.",
                    "label": 0
                },
                {
                    "sent": "So Ben Taskar with his advisor and a few others, has proposed a formulation Max margin formulation which actually uses a loss function which we're going to call margin scaling.",
                    "label": 0
                },
                {
                    "sent": "And then there is this sort of almost independently developed work on again on structured Max margin structure prediction, which proposes the margin scaling loss function, but also a few others.",
                    "label": 0
                },
                {
                    "sent": "And of course there are many, many other papers which use these formulations or develop on this formulation, so I'll provide efficient algorithms for solving simultaneously this inference and the parameter estimation tasks.",
                    "label": 0
                },
                {
                    "sent": "But in this talk we are essentially concerned about designing loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first look at the most popular of this last loss functions, which is the margin scaling loss function.",
                    "label": 0
                },
                {
                    "sent": "So here we are at the at the very start we are interested in making sure that the score of the correct output Yi.",
                    "label": 0
                },
                {
                    "sent": "For an input XI is separated from the score of all other incorrect wise by a margin which is scaled by the error of that incorrect way.",
                    "label": 0
                },
                {
                    "sent": "So this basically.",
                    "label": 0
                },
                {
                    "sent": "It makes sense that the things which are really wrong have to be really, really far away from the correct ones, and so on.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can kind of add around it the slack variables, IE to make it handle the non separable case and this gives you again a QP formulation of your training task which looks similar to the SVM QB.",
                    "label": 0
                },
                {
                    "sent": "But let's look at this formulation for a moment.",
                    "label": 0
                },
                {
                    "sent": "The first thing which is very dissatisfying is that you know we are asking the user to provide us a feature vector, and this error function.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "This could be some arbitrarily scaled quantities which have nothing to do with each other and in the training objective we have combined them additively.",
                    "label": 0
                },
                {
                    "sent": "So, so this is the second formulation which was proposed in the secondary.",
                    "label": 0
                },
                {
                    "sent": "This paper is this lack scaling formulation which actually is mixed.",
                    "label": 0
                },
                {
                    "sent": "There seems to be more sensible, So what that formulation claims is that make sure that the correct why is separated from other incorrect wise by still a margin of 1.",
                    "label": 0
                },
                {
                    "sent": "But then the fact that you have different wise which are wrong in different ways.",
                    "label": 0
                },
                {
                    "sent": "You capture that only a mix of eyes which actually violate your margin of 1.",
                    "label": 0
                },
                {
                    "sent": "And then you scale them.",
                    "label": 0
                },
                {
                    "sent": "By this quantity, which essentially makes sure that the ones which are rank by a large amount are multiplicatively penalized, and like for the margins case, where they are additively penalized.",
                    "label": 0
                },
                {
                    "sent": "So once kind of small nice property of this formulation which is pointed also in the securities paper, is that you could arbitrarily scale the error function, and for that you can find a corresponding, see which will give rise to the same solution, and such a property does not exist for the margin.",
                    "label": 0
                },
                {
                    "sent": "Last formulation, but the reason margin scaling as has survived and almost you know people don't even talk about slacks killing at all.",
                    "label": 0
                },
                {
                    "sent": "You know it's not like OK, you know we're using margin scaling because it's computationally efficient, but here we are apologetic about it.",
                    "label": 0
                },
                {
                    "sent": "Now it has just vanished from the literature.",
                    "label": 0
                },
                {
                    "sent": "So So what we have in this talk will try to say that, OK, it's not so bad you know Slack scaling.",
                    "label": 0
                },
                {
                    "sent": "The reason it says it has vanished from the literature is because it imposes kind of difficult inference task.",
                    "label": 0
                },
                {
                    "sent": "So let me elaborate on that.",
                    "label": 0
                },
                {
                    "sent": "So if you compare both of these formulations, the main challenge to solving them efficiently is that the number of constraints is exponential in the number of possible in this in the size of the input, because the set of possible wise is exponential in the size of the input.",
                    "label": 0
                },
                {
                    "sent": "So it's the same for both margin and Slack scaling, and so you have to use some cutting plane kind of method to solve it efficiently.",
                    "label": 0
                },
                {
                    "sent": "And now if we look at.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one step of the cutting plane algorithm for each of these loss formulation.",
                    "label": 0
                },
                {
                    "sent": "You will find that for matching scaling you have to find for so you have you have a W and now you have to find the constraint which is most violated by the cutting plane algorithm and this is going to be the most violated constraint for any given instance I.",
                    "label": 1
                },
                {
                    "sent": "When you're using the margin loss Kelly.",
                    "label": 0
                },
                {
                    "sent": "So that is the one for which the sum of the school and the error function is maximized.",
                    "label": 0
                },
                {
                    "sent": "Now when the error function in the score function.",
                    "label": 0
                },
                {
                    "sent": "Decompose the same way, then the kind of inference that you would have to eliminate anyway.",
                    "label": 0
                },
                {
                    "sent": "Implement for doing structured prediction would work well for finding the most politic constraint.",
                    "label": 0
                },
                {
                    "sent": "In contrast, for Slack scaling, the most violating constraint is the one for which the score minus the XI I divided by the error function is maximum, and even if the error function decomposes over smaller parts, it does not help you at all, because it appears in this funny way in the denominator.",
                    "label": 1
                },
                {
                    "sent": "But MFC convenient to argue that actually Slack scaling provides better generalizability.",
                    "label": 0
                },
                {
                    "sent": "And qualitatively, you know, I can just appeal to you.",
                    "label": 0
                },
                {
                    "sent": "So when you do Slack scaling, you concentrate on the wise, which are relevant for prediction because you're really concentrating on the wise, which are kind of close to the correct why.",
                    "label": 0
                },
                {
                    "sent": "Whereas in margin scaling you will be worrying about wise even if they are not likely to appear in the prediction.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, when you are doing structured prediction.",
                    "label": 0
                },
                {
                    "sent": "You will be on your test data will be finding that Y which has the highest score.",
                    "label": 0
                },
                {
                    "sent": "So you really don't care about the wise which come in ranked twenty $200 whereas in slacks in margin scaling you will continue to worry about them.",
                    "label": 0
                },
                {
                    "sent": "So so where I said.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because of that, even empirically we did this experiment on two structured learning tasks where we found that slack scaling gives better test error then margin scaling.",
                    "label": 0
                },
                {
                    "sent": "So these are two tasks, one is sequence labeling and the segmentation both arise in information extraction where we have inputs are sentences and we have to assign either.",
                    "label": 0
                },
                {
                    "sent": "We can pose it as a sequential labeling problem for each word in the sentence we have to assign a label or as a segmentation problem.",
                    "label": 0
                },
                {
                    "sent": "We're given a sentence we have to break it up such that each segment corresponds to an entity.",
                    "label": 0
                },
                {
                    "sent": "And we are considering three different kinds of information extraction tasks.",
                    "label": 0
                },
                {
                    "sent": "One is address information extracting elements of addresses from the text strings.",
                    "label": 0
                },
                {
                    "sent": "The second is this Cora citation tasks where we have to find things like author names and titles from citation strings, and this is the famous kernel task where we have to find named entities like people name and organization name from sentences and the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here you have spanned F1 error, the blue line, the blue bars are for margins, killing the red bars are first class LAX killing.",
                    "label": 0
                },
                {
                    "sent": "We find that you know Slack scaling can sometimes give very impressive gains over margin scaling.",
                    "label": 1
                },
                {
                    "sent": "So now the second part of this talk is about what we can do to solve this lack inference problem so that we can apply it on arbitrary structure learning tasks.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key thing that we employ is that so if you look at the slack inference problem, we have to find the Y for which the school and the this particular funny ratio of the error is maximized.",
                    "label": 0
                },
                {
                    "sent": "And the first observation we make is that although this is kind of otherwise, as is hard to get a handle of, but this happens to be concave in error in the in the quantity in the denominator.",
                    "label": 0
                },
                {
                    "sent": "And therefore you can apply variation methods to rewrite this as a linear function.",
                    "label": 1
                },
                {
                    "sent": "As of the error as follows.",
                    "label": 0
                },
                {
                    "sent": "So actually this quantity is equal to this expression over here, where Lambda is your variational variable, it's a scalar of positive scalar and now the error appears scaled by this positive scalar quantity.",
                    "label": 0
                },
                {
                    "sent": "And then there is this term which does not depend on why this is actually very simple.",
                    "label": 0
                },
                {
                    "sent": "Generation will not go through the derivation in the talk, it's there in the paper.",
                    "label": 0
                },
                {
                    "sent": "You just take this formula as is.",
                    "label": 1
                },
                {
                    "sent": "So now so with this as a rewrite of this difficult quantity, Now we can solve the slack inference problem as follows.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to find the Y for which the score minus this quantity is maximized.",
                    "label": 0
                },
                {
                    "sent": "So this is equal to this quantity based on the derivation that we just did.",
                    "label": 0
                },
                {
                    "sent": "And this is the first place where we are using an approximation.",
                    "label": 0
                },
                {
                    "sent": "We interchange the Max and min.",
                    "label": 0
                },
                {
                    "sent": "And now this when you interesting formats in the mean, you get an upper bound, so we want to find the minimum over all possible scalar choice of lambdas and for each given Lambda will find the Y for which this quantity is maximized.",
                    "label": 0
                },
                {
                    "sent": "And then there is this term which comes out of the variational approximation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this particular inference task can be solved efficiently for two reasons.",
                    "label": 0
                },
                {
                    "sent": "First, you know, this quantity actually is very similar to the quantity for which you have to optimize in margin scaling.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the second nice thing is that this entire quantity over here is actually convex in Lambda.",
                    "label": 1
                },
                {
                    "sent": "So you can search for the best possible Lambda using any method like Saline search and also because it so happens that we are only interested in wise which will violate the constraints of the cutting of the QP.",
                    "label": 0
                },
                {
                    "sent": "We can come up with a bounded interval.",
                    "label": 1
                },
                {
                    "sent": "Again, I'm not go through the derivation of this interval in the talk which will enable us to actually claim also finite termination of the search for the lambdas.",
                    "label": 0
                },
                {
                    "sent": "So this gives us.",
                    "label": 0
                },
                {
                    "sent": "A way of finding the most violating approximate slack constraint by trying out different values of lambdas, scaling the error function by that Lambda quantity.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finding the best possible way.",
                    "label": 0
                },
                {
                    "sent": "And this actually happens to approximate the objective.",
                    "label": 0
                },
                {
                    "sent": "The slack objective quite well, as you see in this diagrams over here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the this is the part about how we can adapt an existing nice formulation to doing structured training.",
                    "label": 0
                },
                {
                    "sent": "But the second part is what more?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Excited about and I want to make a case that just because of their Genesis as arising out of scalar methods, the structured prediction problem has not been thought of afresh in the context of the error functions that arise in standard structure learning.",
                    "label": 0
                },
                {
                    "sent": "So one of the big source of problem is the use of a shared slack variable in both margin and Slack scaling for all possible wise coming out of an instance.",
                    "label": 0
                },
                {
                    "sent": "I we have a single shared Slack variable and that can sometimes give sort of an under coverage of the loss function.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I re listed in this example, so here I have say a structured prediction problem where I have to predict for a sentence with three tokens are labeled vector of size 3, and let's assume that 000 is the correct prediction and this prediction has a score of 0 for the correct for the current W vector.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that there are two other labelings Y1 and Y2 which differ from the correct YO in positions two and three respectively and therefore.",
                    "label": 0
                },
                {
                    "sent": "They have a Hamming loss of one each, and they also happen to have the same score of 0.",
                    "label": 0
                },
                {
                    "sent": "And also, let's assume that all the other labelings are comfort ubly separated from these three problems from these two other problematic labelings so we don't have to worry about any of the other labelings now in this case both margin and the slack loss will be one.",
                    "label": 0
                },
                {
                    "sent": "But now it's considered the case there.",
                    "label": 0
                },
                {
                    "sent": "Why to happens to involve feature vectors which makes Y2 non separable from this the third position of the correct way, but because of the shared slack variable, now the GOP will be perfectly happy to terminate and it will not do nothing about trying to separate why one, even though why one in general in structured learning would involve a different set of features.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this motivates for us at different loss function, which we called loss pose learned.",
                    "label": 0
                },
                {
                    "sent": "So imposing on we try to claim that look if your error is additive over different positions, then for generalizability you better in introduce a matching for each possible error position.",
                    "label": 0
                },
                {
                    "sent": "It's not just sufficient to have one single margin constraint on the entire labeling when your error is adding more positions than the margin also has to be imposed at each and every position, so this is more to this formulation achieves this mixture.",
                    "label": 0
                },
                {
                    "sent": "So here now we have a slack variable for each position of our structured labeling, and this constraint basically tells us that the correct why has to be separated from all the incorrect wise which differ from the correct why at the CIA.",
                    "label": 0
                },
                {
                    "sent": "Addition and you will have one such constraint for each of these positions.",
                    "label": 0
                },
                {
                    "sent": "See of any given I now you can just compare it with Slack scaling to see the differ.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now this particular loss in the example that I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "Now we will have a different Slack variable for each of these three positions, and then the post landmasses two as against the margin, or the slack loss, which was one, and now the optimizer will want to continue to optimize for by one, and it will reduce the puzzle and last one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Final, so in particular in particular, also we see a big improvement in accuracy.",
                    "label": 0
                },
                {
                    "sent": "It generalizes better.",
                    "label": 0
                },
                {
                    "sent": "We for example, earlier in segmentation tasks, both margins lack and approximate Slackware, giving same kind of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Whereas with coastline we get a dusting.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improvement in accuracy.",
                    "label": 0
                },
                {
                    "sent": "The other advantage of the first land QP is that the inference task that you have to solve for is also simplified significantly, so we have to find for any given I, See that why which differs from the correct labeling only in the seat position.",
                    "label": 0
                },
                {
                    "sent": "So chili you can solve the map inference problem first you have to find for any given position the why which differs only at the seat position and you can just search over all possible wise because the outer set.",
                    "label": 0
                },
                {
                    "sent": "It's more so this inference problem is also very simple.",
                    "label": 0
                },
                {
                    "sent": "For example when we are using Markov models, then you can simultaneously find all possible violating wise by just finding the maximum.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So this is my summary in somebody.",
                    "label": 0
                },
                {
                    "sent": "So basically there are two contributions.",
                    "label": 0
                },
                {
                    "sent": "One it gave an approximation of the slack inference problem that arises when you try to use the slack loss function and that works well empirically.",
                    "label": 1
                },
                {
                    "sent": "And then we try to motivate the need for alternative loss functions, and I believe that this is just the way it is.",
                    "label": 0
                },
                {
                    "sent": "I am not saying that this is the last word in designing loss functions for structured learning.",
                    "label": 1
                },
                {
                    "sent": "I believe that more can be done.",
                    "label": 0
                },
                {
                    "sent": "And if you talk to me, I will tell you what I find wrong with the pose loss function.",
                    "label": 0
                },
                {
                    "sent": "And you know definitely more work is needed in this area.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Although you know when you tried to implement the Pose lawn equivalent of lagless, there were some numerical instability issues, which I mean I have not investigated this enough.",
                    "label": 0
                },
                {
                    "sent": "But yes, all of these have a parallel in the log world, so I have a question.",
                    "label": 0
                },
                {
                    "sent": "So one of the nice things about this, like scaling formulation, is that you know there's a theorem that says by adding a polynomial of constraints you get to satisfy all the exponential number.",
                    "label": 0
                },
                {
                    "sent": "Right now you give you get there's this theorem.",
                    "label": 0
                },
                {
                    "sent": "Yes yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So that my question is like this theorem still applies here.",
                    "label": 0
                },
                {
                    "sent": "What's the general section has trees to pass trees?",
                    "label": 0
                },
                {
                    "sent": "So you have it up there on the slide, yes?",
                    "label": 0
                },
                {
                    "sent": "How do you come up with this individual margins, infection?",
                    "label": 0
                },
                {
                    "sent": "So in the case of Paris trees, you try to claim for each internal node of the tree whether it has the correct nonterminal label assigned to it, and whether it's two children are correct or not.",
                    "label": 0
                },
                {
                    "sent": "So that is how you decompose the power.",
                    "label": 0
                },
                {
                    "sent": "The loss function in the case of prostrates for segmentation.",
                    "label": 0
                },
                {
                    "sent": "Also, the decomposition is somewhat nontrivial if there in the paper.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}