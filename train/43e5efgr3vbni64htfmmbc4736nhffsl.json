{
    "id": "43e5efgr3vbni64htfmmbc4736nhffsl",
    "title": "Pairwise Probabilistic Clustering Using Evidence Accumulation",
    "info": {
        "author": [
            "Samuel Rota Bulo, University Ca' Foscari"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_rota_bulo_ppc/",
    "segmentation": [
        [
            "And Marcelo Padilla from the University of Venice and Andre, Orange and unafraid from the University of Lisbon, Portugal.",
            "And this is a work within the same project."
        ],
        [
            "So I would give an introduction and the motivation of this work.",
            "Then I will introduce our work and show some experiments and then draw conclusions.",
            "So the goal is we want to develop a principled clustering, actually."
        ],
        [
            "Top clustering approach which is built upon the evidence accumulation framework.",
            "And so how does escalation works?",
            "So we start from an assemble of partitions which may be built from different algorithms with different parametrization and even from subsampled version of the data set.",
            "From this we can build a metrics which is called the causation metrics, where each entry of the concentration metrics it's it's an estimation of the probability that two object inj occur in the same class cluster.",
            "I will work given discuss decision Matrix, tries to find a good consistent position.",
            "In principle, in a principled way, taking into account that this metrics is based on is composed by probabilities, so we take into account what is the nature of the conversation metrics.",
            "In order to do clustering.",
            "If you just run any algorithm clustering."
        ],
        [
            "Written on these metrics in order to get the consensus clustering, then you are not taking into account which is the nature of that matrix.",
            "So just some notation.",
            "I consider all the set of data objects.",
            "We have an object.",
            "K is the number of desired clusters in the last step.",
            "Then this epsilon is in sample of N clusterings.",
            "So capital N is the number of clusterings we have.",
            "See how well of I is the right partition, which is a function from a subset of the set of objects, because because we also assume that you can have subsampled version of the original data set.",
            "2 available set where each clustering can have different number of labels.",
            "Which may differ from the K we use in the last step.",
            "Since we allow for subsampling, so we have that not in all partition inj occur.",
            "So with the Omega IJ we have the indices of the partition where IJ have been classified.",
            "An NIJ is the number of partitions in which I have been classified.",
            "The goal is to learn from the sample of clustering, so in particular, from the coefficient matrix, how to cluster the object into K classes?"
        ],
        [
            "So we make some assumptions.",
            "We start from the assumption that objects can be softly or probabilistically assigned to the labels."
        ],
        [
            "So for each object I.",
            "We estimate an unknown assignment.",
            "Why I so why?",
            "I is a common vector which is a probability distribution over the set of labels so it can be considered a point of the standard simplex.",
            "And we also assume an under under the assumption of independent cluster assignments.",
            "The probability of object INJ to occur in the same class can be can be derived as why I transposed YJ.",
            "Now with this one."
        ],
        [
            "Why so with the unknown assignments, we can build a matrix why?",
            "Where we stack in a row all the assignments for the cluster assignments for each object and with the white transposed?",
            "Why we can build an end by end matrix where each entry gives us the probability of object INJ to be clustered together?",
            "Now assume that for each pair of objects inj we have a Bernoulli random variable indicating whether object I&J occur in the same cluster.",
            "Then the the parameter of this Bernoulli variables are exactly why I transposed YJ, which are our unknown.",
            "From the clustering samples, we can derive observations of this Bernoulli random variables, in particular, for each partition we have an observation.",
            "We give this one if I&J have been clustered together and 0 otherwise."
        ],
        [
            "So from this observation, we can build the causation metrics that Joel has already introduced.",
            "Here we have just a slightly more general definition because we allow for subsampling.",
            "But it's the same definition.",
            "So if you take the mean of the observation, we have an empirical estimate of the parameter of each Bernoulli variable.",
            "So if you write this CIJ is a metric.",
            "See, we have that C is the empirical coefficient matrix Y, transpose Y is the true question metrics which is unknown.",
            "So we want to estimate this from C. So the way we do it, this is just by minimizing the divergences between C&Y transpose Y where."
        ],
        [
            "Why is strict on the multi simplex?",
            "Because each row each column of Y is a probability distribution.",
            "So we can rewrite this optimization in terms of maximizing homogeneous polynomial.",
            "With no negative coefficients, we prove that this has no negative negative coefficient over probability domain, and we do this because.",
            "We want to use this result in probability domain, which is the bombing on inequality.",
            "So whenever you have a polynomial with no negative coefficients with variables in probability domain, then you can."
        ],
        [
            "Create this dynamical system and you will know that each step of dynamical system strictly increase the value of the polynomial until we reach a fixed point.",
            "So you can use this dynamic to locally optimize.",
            "This polynomial, since we have rewritten this formulation in terms of.",
            "The purposes of this theorem we can use it in order to optimize our our objective function.",
            "Indeed, we end up with this dynamical system.",
            "So you start from some point in the multi simplex.",
            "You run the dynamics and you end up with the solution.",
            "Why this is not a convex optimization problem is a local optimization problem.",
            "So if you have some appear information you can use it as starting point.",
            "But in the experiment we conducted we saw that we run it from also from random points in the multi simplex and we saw that there are quite large."
        ],
        [
            "Abstraction, so the solution is almost the same.",
            "So we conducted experiments on some real data set from the."
        ],
        [
            "Lucy, I machine learning repository.",
            "So on those and also on a synthetic data set image complex we created then samples using single link complete link.",
            "Average Lincoln K means as based algorithms.",
            "And we created.",
            "Different goal question metrics.",
            "One for each algorithm in the sense that we kept just in samples from each algorithm algorithms in.",
            "You need to have at the concession metric only of an samples from the single link.",
            "Everything completely K means and also the samples where we put everything together just to see the behavior in those cases.",
            "And we compared against the performance obtained by the same algorithms on when they when we apply them directly on the coefficient matrix in order to have the consensus partition.",
            "Sedition is"
        ],
        [
            "Example, this is breast cancer.",
            "It has two classes.",
            "This is the cost decision.",
            "Matrix Actually is all the confusion matrix when we put everything together and this is the estimated true cost decision matrix.",
            "What we have soft assignments that we can see in this part.",
            "We have more probability on the first class and hear more probability on the second class and the advantage of having soft assignments is that we can also estimate the uncertainty.",
            "In our solution, so we are about sure quite sure about the first class.",
            "We have less sure about the second class and one can use this information posteriori maybe to go back to the example in order to find out which are, let's say noisy partition remove them an apply again or the whole process."
        ],
        [
            "These are the results when we apply our method is the first one singling, averaging and completely on the different data sets and we can see that we performance.",
            "On average, always better.",
            "You can see that in some cases some algorithms originally performs well here, but worse in other data sets.",
            "In particular, we can see when when we go on the single question metrics where we related the single algorithms.",
            "In some cases we have, for instance here."
        ],
        [
            "That's complete link in this case works quite quite comparable in this case, when we have the acquisition metrics built with K means.",
            "But for instance we when we have the acquisition method built with singling, then here it performs poorly by our approach.",
            "On average performance, always good in All in all cases, and we put up with and when we put everything together, in some cases we can even go better than.",
            "In cases, so it's not true that we are always good as the best algorithm we can.",
            "Even when we put everything together, go better than the best algorithm.",
            "So conclusion we by taking advantage of the published interpretation of the computer."
        ],
        [
            "Similarities in the question metrics with."
        ],
        [
            "Phillips principle soft clustering method particular we consider the fact that the position metrics has probabilities and our matter reduces the custom problem to polynomial optimization probability domain and we attack this by the booming on inequality and the new method produces soft partition of the data.",
            "Although for these experiments we induced hard partition in order to compare against the other approaches but even.",
            "When we do this, we were able to obtain better results.",
            "Just two."
        ],
        [
            "To link to the other papers.",
            "So.",
            "When we do in summer clustering, actually we have a nice in the regional data, but we can introduce noise on also when we create this ensembles.",
            "And the information about the centre or uncertainty that we have when we estimate this Y could be helpful in order to go back to the ensemble and remove or try to identify which are noisy partitions, remove them in order to filter the noise in December and run the process again in order to be more robust to noise.",
            "Also in the ensembles and the second question is that.",
            "It's linked with the first talk.",
            "Actually, we can use evidence accumulation as a means of learning parallel similarities from multiple source.",
            "So when we want to combine multiple similarities or when we have a different data representation for the same objects.",
            "So.",
            "Does a reference."
        ],
        [
            "And this concludes my talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Marcelo Padilla from the University of Venice and Andre, Orange and unafraid from the University of Lisbon, Portugal.",
                    "label": 0
                },
                {
                    "sent": "And this is a work within the same project.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I would give an introduction and the motivation of this work.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce our work and show some experiments and then draw conclusions.",
                    "label": 0
                },
                {
                    "sent": "So the goal is we want to develop a principled clustering, actually.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Top clustering approach which is built upon the evidence accumulation framework.",
                    "label": 1
                },
                {
                    "sent": "And so how does escalation works?",
                    "label": 0
                },
                {
                    "sent": "So we start from an assemble of partitions which may be built from different algorithms with different parametrization and even from subsampled version of the data set.",
                    "label": 0
                },
                {
                    "sent": "From this we can build a metrics which is called the causation metrics, where each entry of the concentration metrics it's it's an estimation of the probability that two object inj occur in the same class cluster.",
                    "label": 0
                },
                {
                    "sent": "I will work given discuss decision Matrix, tries to find a good consistent position.",
                    "label": 0
                },
                {
                    "sent": "In principle, in a principled way, taking into account that this metrics is based on is composed by probabilities, so we take into account what is the nature of the conversation metrics.",
                    "label": 0
                },
                {
                    "sent": "In order to do clustering.",
                    "label": 0
                },
                {
                    "sent": "If you just run any algorithm clustering.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Written on these metrics in order to get the consensus clustering, then you are not taking into account which is the nature of that matrix.",
                    "label": 0
                },
                {
                    "sent": "So just some notation.",
                    "label": 0
                },
                {
                    "sent": "I consider all the set of data objects.",
                    "label": 1
                },
                {
                    "sent": "We have an object.",
                    "label": 1
                },
                {
                    "sent": "K is the number of desired clusters in the last step.",
                    "label": 0
                },
                {
                    "sent": "Then this epsilon is in sample of N clusterings.",
                    "label": 0
                },
                {
                    "sent": "So capital N is the number of clusterings we have.",
                    "label": 1
                },
                {
                    "sent": "See how well of I is the right partition, which is a function from a subset of the set of objects, because because we also assume that you can have subsampled version of the original data set.",
                    "label": 0
                },
                {
                    "sent": "2 available set where each clustering can have different number of labels.",
                    "label": 1
                },
                {
                    "sent": "Which may differ from the K we use in the last step.",
                    "label": 0
                },
                {
                    "sent": "Since we allow for subsampling, so we have that not in all partition inj occur.",
                    "label": 0
                },
                {
                    "sent": "So with the Omega IJ we have the indices of the partition where IJ have been classified.",
                    "label": 0
                },
                {
                    "sent": "An NIJ is the number of partitions in which I have been classified.",
                    "label": 0
                },
                {
                    "sent": "The goal is to learn from the sample of clustering, so in particular, from the coefficient matrix, how to cluster the object into K classes?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we make some assumptions.",
                    "label": 0
                },
                {
                    "sent": "We start from the assumption that objects can be softly or probabilistically assigned to the labels.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for each object I.",
                    "label": 1
                },
                {
                    "sent": "We estimate an unknown assignment.",
                    "label": 1
                },
                {
                    "sent": "Why I so why?",
                    "label": 0
                },
                {
                    "sent": "I is a common vector which is a probability distribution over the set of labels so it can be considered a point of the standard simplex.",
                    "label": 1
                },
                {
                    "sent": "And we also assume an under under the assumption of independent cluster assignments.",
                    "label": 1
                },
                {
                    "sent": "The probability of object INJ to occur in the same class can be can be derived as why I transposed YJ.",
                    "label": 0
                },
                {
                    "sent": "Now with this one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why so with the unknown assignments, we can build a matrix why?",
                    "label": 0
                },
                {
                    "sent": "Where we stack in a row all the assignments for the cluster assignments for each object and with the white transposed?",
                    "label": 0
                },
                {
                    "sent": "Why we can build an end by end matrix where each entry gives us the probability of object INJ to be clustered together?",
                    "label": 0
                },
                {
                    "sent": "Now assume that for each pair of objects inj we have a Bernoulli random variable indicating whether object I&J occur in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "Then the the parameter of this Bernoulli variables are exactly why I transposed YJ, which are our unknown.",
                    "label": 0
                },
                {
                    "sent": "From the clustering samples, we can derive observations of this Bernoulli random variables, in particular, for each partition we have an observation.",
                    "label": 0
                },
                {
                    "sent": "We give this one if I&J have been clustered together and 0 otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from this observation, we can build the causation metrics that Joel has already introduced.",
                    "label": 0
                },
                {
                    "sent": "Here we have just a slightly more general definition because we allow for subsampling.",
                    "label": 0
                },
                {
                    "sent": "But it's the same definition.",
                    "label": 0
                },
                {
                    "sent": "So if you take the mean of the observation, we have an empirical estimate of the parameter of each Bernoulli variable.",
                    "label": 1
                },
                {
                    "sent": "So if you write this CIJ is a metric.",
                    "label": 1
                },
                {
                    "sent": "See, we have that C is the empirical coefficient matrix Y, transpose Y is the true question metrics which is unknown.",
                    "label": 0
                },
                {
                    "sent": "So we want to estimate this from C. So the way we do it, this is just by minimizing the divergences between C&Y transpose Y where.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why is strict on the multi simplex?",
                    "label": 0
                },
                {
                    "sent": "Because each row each column of Y is a probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So we can rewrite this optimization in terms of maximizing homogeneous polynomial.",
                    "label": 1
                },
                {
                    "sent": "With no negative coefficients, we prove that this has no negative negative coefficient over probability domain, and we do this because.",
                    "label": 0
                },
                {
                    "sent": "We want to use this result in probability domain, which is the bombing on inequality.",
                    "label": 1
                },
                {
                    "sent": "So whenever you have a polynomial with no negative coefficients with variables in probability domain, then you can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Create this dynamical system and you will know that each step of dynamical system strictly increase the value of the polynomial until we reach a fixed point.",
                    "label": 0
                },
                {
                    "sent": "So you can use this dynamic to locally optimize.",
                    "label": 0
                },
                {
                    "sent": "This polynomial, since we have rewritten this formulation in terms of.",
                    "label": 0
                },
                {
                    "sent": "The purposes of this theorem we can use it in order to optimize our our objective function.",
                    "label": 0
                },
                {
                    "sent": "Indeed, we end up with this dynamical system.",
                    "label": 0
                },
                {
                    "sent": "So you start from some point in the multi simplex.",
                    "label": 0
                },
                {
                    "sent": "You run the dynamics and you end up with the solution.",
                    "label": 0
                },
                {
                    "sent": "Why this is not a convex optimization problem is a local optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So if you have some appear information you can use it as starting point.",
                    "label": 0
                },
                {
                    "sent": "But in the experiment we conducted we saw that we run it from also from random points in the multi simplex and we saw that there are quite large.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abstraction, so the solution is almost the same.",
                    "label": 0
                },
                {
                    "sent": "So we conducted experiments on some real data set from the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lucy, I machine learning repository.",
                    "label": 0
                },
                {
                    "sent": "So on those and also on a synthetic data set image complex we created then samples using single link complete link.",
                    "label": 1
                },
                {
                    "sent": "Average Lincoln K means as based algorithms.",
                    "label": 0
                },
                {
                    "sent": "And we created.",
                    "label": 0
                },
                {
                    "sent": "Different goal question metrics.",
                    "label": 1
                },
                {
                    "sent": "One for each algorithm in the sense that we kept just in samples from each algorithm algorithms in.",
                    "label": 1
                },
                {
                    "sent": "You need to have at the concession metric only of an samples from the single link.",
                    "label": 0
                },
                {
                    "sent": "Everything completely K means and also the samples where we put everything together just to see the behavior in those cases.",
                    "label": 1
                },
                {
                    "sent": "And we compared against the performance obtained by the same algorithms on when they when we apply them directly on the coefficient matrix in order to have the consensus partition.",
                    "label": 0
                },
                {
                    "sent": "Sedition is",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, this is breast cancer.",
                    "label": 0
                },
                {
                    "sent": "It has two classes.",
                    "label": 0
                },
                {
                    "sent": "This is the cost decision.",
                    "label": 0
                },
                {
                    "sent": "Matrix Actually is all the confusion matrix when we put everything together and this is the estimated true cost decision matrix.",
                    "label": 0
                },
                {
                    "sent": "What we have soft assignments that we can see in this part.",
                    "label": 0
                },
                {
                    "sent": "We have more probability on the first class and hear more probability on the second class and the advantage of having soft assignments is that we can also estimate the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In our solution, so we are about sure quite sure about the first class.",
                    "label": 0
                },
                {
                    "sent": "We have less sure about the second class and one can use this information posteriori maybe to go back to the example in order to find out which are, let's say noisy partition remove them an apply again or the whole process.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the results when we apply our method is the first one singling, averaging and completely on the different data sets and we can see that we performance.",
                    "label": 0
                },
                {
                    "sent": "On average, always better.",
                    "label": 0
                },
                {
                    "sent": "You can see that in some cases some algorithms originally performs well here, but worse in other data sets.",
                    "label": 0
                },
                {
                    "sent": "In particular, we can see when when we go on the single question metrics where we related the single algorithms.",
                    "label": 0
                },
                {
                    "sent": "In some cases we have, for instance here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's complete link in this case works quite quite comparable in this case, when we have the acquisition metrics built with K means.",
                    "label": 0
                },
                {
                    "sent": "But for instance we when we have the acquisition method built with singling, then here it performs poorly by our approach.",
                    "label": 0
                },
                {
                    "sent": "On average performance, always good in All in all cases, and we put up with and when we put everything together, in some cases we can even go better than.",
                    "label": 0
                },
                {
                    "sent": "In cases, so it's not true that we are always good as the best algorithm we can.",
                    "label": 0
                },
                {
                    "sent": "Even when we put everything together, go better than the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "So conclusion we by taking advantage of the published interpretation of the computer.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarities in the question metrics with.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Phillips principle soft clustering method particular we consider the fact that the position metrics has probabilities and our matter reduces the custom problem to polynomial optimization probability domain and we attack this by the booming on inequality and the new method produces soft partition of the data.",
                    "label": 1
                },
                {
                    "sent": "Although for these experiments we induced hard partition in order to compare against the other approaches but even.",
                    "label": 1
                },
                {
                    "sent": "When we do this, we were able to obtain better results.",
                    "label": 0
                },
                {
                    "sent": "Just two.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To link to the other papers.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "When we do in summer clustering, actually we have a nice in the regional data, but we can introduce noise on also when we create this ensembles.",
                    "label": 0
                },
                {
                    "sent": "And the information about the centre or uncertainty that we have when we estimate this Y could be helpful in order to go back to the ensemble and remove or try to identify which are noisy partitions, remove them in order to filter the noise in December and run the process again in order to be more robust to noise.",
                    "label": 0
                },
                {
                    "sent": "Also in the ensembles and the second question is that.",
                    "label": 0
                },
                {
                    "sent": "It's linked with the first talk.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can use evidence accumulation as a means of learning parallel similarities from multiple source.",
                    "label": 1
                },
                {
                    "sent": "So when we want to combine multiple similarities or when we have a different data representation for the same objects.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Does a reference.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this concludes my talk.",
                    "label": 0
                }
            ]
        }
    }
}