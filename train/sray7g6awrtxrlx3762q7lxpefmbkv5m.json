{
    "id": "sray7g6awrtxrlx3762q7lxpefmbkv5m",
    "title": "A Preliminary Evaluation of Word Representations for Named-Entity Recognition",
    "info": {
        "author": [
            "Joseph Turian, University of Montreal"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_turian_pewrner/",
    "segmentation": [
        [
            "Yeah, so my name is Joseph Terry and I'm going to be talking about word representations that I evaluated on.",
            "Named entity recognition.",
            "This is joint work with Michael Arrows, Yoshua Bengio, Levron off and then."
        ],
        [
            "So I'm going to give this running example in natural language processing where we're doing sequence labeling.",
            "We have 3 words in this example, and we're going to try and predict using a linear model, which is very common in natural plus a linear model.",
            "We're going to predict a probability distribution over possible labels.",
            "So."
        ],
        [
            "What are we working with?",
            "Natural language processing, no matter what you're doing, you really can't get around the fact that we're working with words.",
            "Words, and yet more words.",
            "And that is the object of this study.",
            "So how do we?"
        ],
        [
            "Typically handle words when we're doing natural language processing.",
            "I would argue that we don't actually handle them very well.",
            "And the reason is.",
            "We are typically."
        ],
        [
            "Using A1 hot representation, if we have a say 50,000 word vocabulary size V here, then what we do is that each time we see a word, we just activate one bit in a 50,000 dimensional representation or size V representation, and then we use this one hot representation of this concatenate one hot representation to predict the probability distribution over the labels."
        ],
        [
            "The problem is that most of the words in the vocabulary actually occur quite rarely, which means that we are purely supervised.",
            "Training regime will have poor estimates for the probability distribution of labels given these rare words.",
            "So."
        ],
        [
            "One approach to solving the problem of rare words that we should do is manual feature engineering, right?"
        ],
        [
            "No, I don't advocate manual feature engineering.",
            "I'm going to add."
        ],
        [
            "Kate a semi supervised approach where we use a large corpus and deuce word representations in an unsupervised manner.",
            "And then we use these word representations as features for our supervised task.",
            "The idea being that if we were able to come up with better word representations of our large corpus, then we will be able to better estimate statistics in our supervised task."
        ],
        [
            "So how do we get less sparse word representations?",
            "There are two schools of thought on this, essentially class base or clustering word representations and distributed word representations.",
            "So in the literature people have tried both of them, but no one has ever done a controlled, controlled comparison between the two.",
            "So that's really the essence of this particular work that we're going to be comparing.",
            "These two schools of thought.",
            "So first in the class base."
        ],
        [
            "Where representations just to give you an example here, if we have C classes in a hard clustering, we had imagined that we now have still A1 hot representation, but over the class representation, not the original vocabulary, and this C is much much smaller than the size of the original vocabulary.",
            "We"
        ],
        [
            "Consider different styles of clustering.",
            "There can be hard clustering, soft clustering.",
            "You can have a hierarchical clustering or a flat clustering."
        ],
        [
            "And I'm going to be talking about two clustering based approaches, Brown clustering and Hmm's."
        ],
        [
            "Inbound clustering.",
            "We have a hard hierarchical class based language model.",
            "It's essentially estimating or maximizing the mutual information between the class bigrams.",
            "It's a greedy technique and it essentially merges words into a hierarchy using contextual similarity comes from similar work by Brown at all in 92."
        ],
        [
            "So here's an example.",
            "Brown clustering will see on the right we have these words.",
            "These verbs these verbs down here are faster, so they're clustered together.",
            "Over here we have different positions.",
            "In the middle we have months, and the representation for the word chairman would be its path from the root.",
            "But we can also consider prefixes of the cluster as another type of feature.",
            "For example, the two level prefix for the word Chairman we.",
            "Oh oh, which are basically all words that are job titles or positions in a company.",
            "So."
        ],
        [
            "In our Brown clustering, we have, as I said, a hard hard hierarchical class based LM.",
            "We have 1000 classes.",
            "This is just standard number everyone uses in the literature an we consider features based on prefix length of four, 610 and 20.",
            "So so kind of standard in the literature."
        ],
        [
            "For hmm's"
        ],
        [
            "We have a multinomial distribution over the hidden states.",
            "What we admit are the actual words in the vocabulary and the multinomial distribution in the states is what we would treat as our word representation.",
            "Playing in the 80s, about 80 states.",
            "Unfortunately we don't have any results yet with the HMM based approach.",
            "In"
        ],
        [
            "Distributed word representations.",
            "Instead of having this high dimensional one hot representation will have a K dimensional dense representation.",
            "So K is around 50 and the idea here is that instead of having 1000 classes, one of which is active, we can maybe represent two to the K concepts by having kaydence dimensions.",
            "So what we're going to learn is this word embedding matrix, which is Maps the vocabulary of size V to the low dimensional K. So to show you a little bit more how this work in a supervised setting."
        ],
        [
            "We have this word matrix embedding matrix E which contributes these tide waits and it's how we map the one hot vector to a dense representation and then we take that and then we have this dense computation where we try to predict the probability distribution over the labels M. So."
        ],
        [
            "We explored to recent approaches in these distributed representations, the approach of color bear in Weston as well as mean hintons HLB L approach."
        ],
        [
            "So what color bear?"
        ],
        [
            "Western are doing.",
            "They have a model whereby they are estimating scores over 5 grams in the corpus and what they say is let's take this.",
            "5 gram will pass it through a 100 dimensional hidden layer and then predict a score.",
            "And our digital Radha is that this 5 gram that its score if we were take take the five gram take the last word and corrupt it, just pick a random word in the vocabulary that this score of the correct 5 gram that actually appears in the corpus is at least margin mu higher than the noise Ngram score.",
            "They learn over an entire corpus the back propagate the error and they tune the embedding simultaneously.",
            "And magically?"
        ],
        [
            "They get pretty good embeddings.",
            "This is a T SNE visualization and two dimensional vision.",
            "Visualization of their words.",
            "Down here you can see the entire word space.",
            "Things are kind of grouped together.",
            "Here you can see the words a bit more clearly.",
            "You'll see that there are kind of syntactically the words are syntactically and semantically related.",
            "Just tend to be closer together in this space, which in that some pronouns up here.",
            "How weather why?",
            "And."
        ],
        [
            "Alternate approach to learning these embeddings is the."
        ],
        [
            "Log by linear language model of me in Hinton.",
            "What they do is they say we have the first 4 words it's another 5 gram model, at least in the experiments that have been conducted.",
            "They take the first 4 words.",
            "They have a linear model to predict the representation of the fifth word.",
            "They pass it through an exponential distribution to compute the probability normalized with respect to all the other words in the vocabulary.",
            "The probability that this prediction this linear prediction of the final word matches the target prediction and they back propagate their error to tune the embeddings as well as the model.",
            "Features are they use?",
            "There are no features, they are tuning on the embedding matrix.",
            "This matrix E over here.",
            "That's that's what they're tuning.",
            "They have the weights that that map word to its embedding.",
            "They have the weights that map the four gram to the 5th, the fifth word, and they basically want to learn the model as well as the embeddings simultaneously so that the first 4 words are good at predicting the last word, and that's it.",
            "The words themselves.",
            "And then that's passed into the embedding matrix.",
            "Yes, the one heart is the is the original representation.",
            "It's multiplied by the embedding matrix and the embedding matrix, as well as the linear model is tuned, and that's it.",
            "It's a similar to Colbert, and Weston is just a different form, but everything, everything is just automatically tuned here.",
            "So the."
        ],
        [
            "Is that there's that partition partition function that has to be computed.",
            "Pesky partition function mean Hinton in 2009.",
            "Come up with a hierarchical approach for computing the partition function, which is much faster."
        ],
        [
            "So, just to summarize the approach again, we're going to use a large corpus and in an unsupervised manner induced word representation.",
            "Brown it took three days, the LBL approach in one week we have about 100 epochs of training Colbert and Weston.",
            "About four weeks we can do about 20 epochs of training.",
            "The downside with these these distributed representations, you're never really sure when to stop training.",
            "According to Ronan, he just kind of lets the computer keep going.",
            "And then we're going to use this word features in the supervised task."
        ],
        [
            "Our unsupervised corpus.",
            "We used the Reuters corpus.",
            "It's about 40,000,000 words and about 270 word types in the."
        ],
        [
            "Our supervised task we're looking at named entity recognition.",
            "Our model is an average perceptron, which is a linear classifier, and we're basing our work on the state of the art model of right and often wrath."
        ],
        [
            "So so one thing we have to do is when we when we insert the Brown features Brown embedding this features they just kind of work.",
            "Whereas with the distributor representations it's not really clear if the scale of these features matches those of the other features which are discrete and class based.",
            "So one hyperparameter we have to tune.",
            "I just show the tuning curve.",
            "Here is a normalization constant that.",
            "Adjust the weight of the distributed representation features with respect to the other features in the model."
        ],
        [
            "Now another thing, Percy Liang does this and later, I think Terry Koo did this.",
            "They apply a cleaning technique whereby they throw out about 50% of the sentences in the Reuters corpus that don't really look like real sentence.",
            "Is things like sports scores and stuff like that.",
            "We tried that out, and it turns out that just in general, both with the distributor representations and with the Brown embeddings that cleaning the corpus or pre processing it.",
            "Leads to better word representations.",
            "This just suggests that we need better training strategies.",
            "In fact, we found that like ad hoc pruning methods also sometimes increase the accuracy.",
            "It suggests that.",
            "Some sort of some sort of cleaning process needs to be applied, or perhaps we want a curriculum strategy that learns the core of the language 1st and then expands outwards to what is less clear about if we were to do that, maybe we could do the cleaning automatically, and when it needs some sort of preprocessing step that we've thought through."
        ],
        [
            "So let me show you some results.",
            "Not all of these are in the proceedings, unfortunately.",
            "Some of them were late breaking, but generally what we see is tuning, set evaluation set and mock seven data.",
            "We also we also try to a few more combinations of tuning set in valuations that generally you find that the Brown embeddings have the best F1 score.",
            "But it's not indicating the valuation set, but in every other condition we saw, we saw that the 100 dimensional ECHL is better than the 50 dimensional ECHL.",
            "We also found that Colbert in Weston embeddings performed on about on par with the LBL embeddings and Lastly, the baseline is pretty far behind, so there were two things that were surprising about these results to us.",
            "The first was that we were expecting the distributed representations.",
            "To be much better because they were estimated over 5 grams and we felt that because it was able to capture statistics over this entire window, that the representations would just be inherently more rich than the Brown clusters which were over bigram statistics.",
            "But this did not turn out to be true.",
            "In our experiment.",
            "The other hypothesis we had was we speculated that perhaps because we had a linear supervised classifier that using.",
            "A linear unsupervised model would be better.",
            "That would suggest that the color bear in Weston embeddings would not necessarily be as good as the HLB L embeddings, but that didn't actually turn out to be true when they when they were controlled for the same number of dimensions, they turn out to be about the same."
        ],
        [
            "So here's our working hypothesis right now.",
            "If you have a linear supervised model like the Perceptron, and this is generally the case in natural language processing.",
            "In general, you'll probably want to prefer representations that are high dimensional and sparse, because if you have a dent in low dimensional representation, it is too difficult for a linear classifier to correctly estimate the probability distribution of the labels this.",
            "Hypothesis, we believe, will be especially true when the supervised task allowed the decisions are based on rare words, which is the case with named entity recognition.",
            "Perhaps if we were doing parsing, then the distributor representations would be stronger than the Brown embeddings, but we don't know.",
            "We don't know, but the main thing here is that.",
            "With a linear model, we do feel like the high dimensional sparse representation is superior."
        ],
        [
            "So there are a few other difficulties when working with word embeddings that I want to talk about.",
            "The first is that there's no clear stopping criterion during training.",
            "The second is that training is actually the supervised training phase is actually slower because there are more active features.",
            "50 active features worth versus maybe five in the Brown clusters.",
            "And there are also more hyperparameters you need to learn.",
            "There's the learning rate for the model.",
            "Optionally, you can also have a separate learning rate for the embeddings, which we found works better.",
            "And Lastly, there's the normalization constant.",
            "They adjust the importance of these features with respect to the other features.",
            "And with the Brown clusters, there aren't actually any hyperparameters that we tuned.",
            "There's maybe the number of classes, but we just said it at 1000 like everyone else in larger does."
        ],
        [
            "Just to summarize.",
            "When we're talking about in our experiments, specifically in our experiments, when there is a linear model and we're evaluating named entity recognition, we found the Brown works better than the distributor representations that the 100 dimensional distributor representations work better than the 50 dimensional ones.",
            "And the LBL is about on par with the Colbert in Weston.",
            "We also found that the H LBL approach was about 20 times faster train than Colbert in West, and the reason is first of all there's no hidden layer and technical under D uses a diagonal matrix, just faster.",
            "Another advantage that we found the Brown House is that it just it just says 0 hyperparameters.",
            "That it's training is also faster because there are fewer active features in future work.",
            "Were interested to explore whether a sparse in high dimensional approach.",
            "To inducing the word embeddings would be superior to the sparse high dimensional Brown approach, and if it would be better than just the dense low dimensional approach.",
            "That's my talk.",
            "Two questions.",
            "So what about the adapting representations?",
            "The problem?",
            "So that's pretty straightforward, right?",
            "Credit on them, whereas if you Brown that stuff.",
            "I agree.",
            "I mean, that's definitely an advantage when you're using a gradient based method.",
            "The reason we didn't do that, there were several restrictions that we placed ourselves in this work that at least you know for us we consider fairly unnatural, like a linear classifier or not tuning the embeddings.",
            "The goal is basically to.",
            "Create word features offline and then give them to people in the NLP community and expect that they will use the techniques that they typically use and then take the features and run with it.",
            "But as you said, if you want to be more sophisticated you could tune the embeddings.",
            "When you re training?",
            "It's true, that's actually true.",
            "You're right, you're right about that.",
            "I'm just wondering.",
            "How did the did you look at how the various algorithms do?",
            "Did you look at how the various algorithms do as a function of the frequency of the words?",
            "Like?",
            "Is it that maybe they distributed once I'm not doing as well the rare words, but we met the frequency and maybe get some sort of overfitting.",
            "Yeah, and that's a really good idea.",
            "We intend to do that, but we haven't looked at yet."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so my name is Joseph Terry and I'm going to be talking about word representations that I evaluated on.",
                    "label": 0
                },
                {
                    "sent": "Named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Michael Arrows, Yoshua Bengio, Levron off and then.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to give this running example in natural language processing where we're doing sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "We have 3 words in this example, and we're going to try and predict using a linear model, which is very common in natural plus a linear model.",
                    "label": 0
                },
                {
                    "sent": "We're going to predict a probability distribution over possible labels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are we working with?",
                    "label": 0
                },
                {
                    "sent": "Natural language processing, no matter what you're doing, you really can't get around the fact that we're working with words.",
                    "label": 1
                },
                {
                    "sent": "Words, and yet more words.",
                    "label": 0
                },
                {
                    "sent": "And that is the object of this study.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Typically handle words when we're doing natural language processing.",
                    "label": 1
                },
                {
                    "sent": "I would argue that we don't actually handle them very well.",
                    "label": 0
                },
                {
                    "sent": "And the reason is.",
                    "label": 0
                },
                {
                    "sent": "We are typically.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using A1 hot representation, if we have a say 50,000 word vocabulary size V here, then what we do is that each time we see a word, we just activate one bit in a 50,000 dimensional representation or size V representation, and then we use this one hot representation of this concatenate one hot representation to predict the probability distribution over the labels.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that most of the words in the vocabulary actually occur quite rarely, which means that we are purely supervised.",
                    "label": 0
                },
                {
                    "sent": "Training regime will have poor estimates for the probability distribution of labels given these rare words.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One approach to solving the problem of rare words that we should do is manual feature engineering, right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, I don't advocate manual feature engineering.",
                    "label": 0
                },
                {
                    "sent": "I'm going to add.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kate a semi supervised approach where we use a large corpus and deuce word representations in an unsupervised manner.",
                    "label": 0
                },
                {
                    "sent": "And then we use these word representations as features for our supervised task.",
                    "label": 1
                },
                {
                    "sent": "The idea being that if we were able to come up with better word representations of our large corpus, then we will be able to better estimate statistics in our supervised task.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we get less sparse word representations?",
                    "label": 1
                },
                {
                    "sent": "There are two schools of thought on this, essentially class base or clustering word representations and distributed word representations.",
                    "label": 0
                },
                {
                    "sent": "So in the literature people have tried both of them, but no one has ever done a controlled, controlled comparison between the two.",
                    "label": 0
                },
                {
                    "sent": "So that's really the essence of this particular work that we're going to be comparing.",
                    "label": 0
                },
                {
                    "sent": "These two schools of thought.",
                    "label": 0
                },
                {
                    "sent": "So first in the class base.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where representations just to give you an example here, if we have C classes in a hard clustering, we had imagined that we now have still A1 hot representation, but over the class representation, not the original vocabulary, and this C is much much smaller than the size of the original vocabulary.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider different styles of clustering.",
                    "label": 0
                },
                {
                    "sent": "There can be hard clustering, soft clustering.",
                    "label": 1
                },
                {
                    "sent": "You can have a hierarchical clustering or a flat clustering.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to be talking about two clustering based approaches, Brown clustering and Hmm's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inbound clustering.",
                    "label": 0
                },
                {
                    "sent": "We have a hard hierarchical class based language model.",
                    "label": 1
                },
                {
                    "sent": "It's essentially estimating or maximizing the mutual information between the class bigrams.",
                    "label": 0
                },
                {
                    "sent": "It's a greedy technique and it essentially merges words into a hierarchy using contextual similarity comes from similar work by Brown at all in 92.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Brown clustering will see on the right we have these words.",
                    "label": 1
                },
                {
                    "sent": "These verbs these verbs down here are faster, so they're clustered together.",
                    "label": 0
                },
                {
                    "sent": "Over here we have different positions.",
                    "label": 0
                },
                {
                    "sent": "In the middle we have months, and the representation for the word chairman would be its path from the root.",
                    "label": 0
                },
                {
                    "sent": "But we can also consider prefixes of the cluster as another type of feature.",
                    "label": 0
                },
                {
                    "sent": "For example, the two level prefix for the word Chairman we.",
                    "label": 0
                },
                {
                    "sent": "Oh oh, which are basically all words that are job titles or positions in a company.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our Brown clustering, we have, as I said, a hard hard hierarchical class based LM.",
                    "label": 1
                },
                {
                    "sent": "We have 1000 classes.",
                    "label": 0
                },
                {
                    "sent": "This is just standard number everyone uses in the literature an we consider features based on prefix length of four, 610 and 20.",
                    "label": 0
                },
                {
                    "sent": "So so kind of standard in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For hmm's",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a multinomial distribution over the hidden states.",
                    "label": 1
                },
                {
                    "sent": "What we admit are the actual words in the vocabulary and the multinomial distribution in the states is what we would treat as our word representation.",
                    "label": 0
                },
                {
                    "sent": "Playing in the 80s, about 80 states.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately we don't have any results yet with the HMM based approach.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distributed word representations.",
                    "label": 0
                },
                {
                    "sent": "Instead of having this high dimensional one hot representation will have a K dimensional dense representation.",
                    "label": 0
                },
                {
                    "sent": "So K is around 50 and the idea here is that instead of having 1000 classes, one of which is active, we can maybe represent two to the K concepts by having kaydence dimensions.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to learn is this word embedding matrix, which is Maps the vocabulary of size V to the low dimensional K. So to show you a little bit more how this work in a supervised setting.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this word matrix embedding matrix E which contributes these tide waits and it's how we map the one hot vector to a dense representation and then we take that and then we have this dense computation where we try to predict the probability distribution over the labels M. So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We explored to recent approaches in these distributed representations, the approach of color bear in Weston as well as mean hintons HLB L approach.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what color bear?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Western are doing.",
                    "label": 0
                },
                {
                    "sent": "They have a model whereby they are estimating scores over 5 grams in the corpus and what they say is let's take this.",
                    "label": 0
                },
                {
                    "sent": "5 gram will pass it through a 100 dimensional hidden layer and then predict a score.",
                    "label": 0
                },
                {
                    "sent": "And our digital Radha is that this 5 gram that its score if we were take take the five gram take the last word and corrupt it, just pick a random word in the vocabulary that this score of the correct 5 gram that actually appears in the corpus is at least margin mu higher than the noise Ngram score.",
                    "label": 0
                },
                {
                    "sent": "They learn over an entire corpus the back propagate the error and they tune the embedding simultaneously.",
                    "label": 0
                },
                {
                    "sent": "And magically?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They get pretty good embeddings.",
                    "label": 0
                },
                {
                    "sent": "This is a T SNE visualization and two dimensional vision.",
                    "label": 0
                },
                {
                    "sent": "Visualization of their words.",
                    "label": 0
                },
                {
                    "sent": "Down here you can see the entire word space.",
                    "label": 0
                },
                {
                    "sent": "Things are kind of grouped together.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the words a bit more clearly.",
                    "label": 0
                },
                {
                    "sent": "You'll see that there are kind of syntactically the words are syntactically and semantically related.",
                    "label": 0
                },
                {
                    "sent": "Just tend to be closer together in this space, which in that some pronouns up here.",
                    "label": 0
                },
                {
                    "sent": "How weather why?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alternate approach to learning these embeddings is the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Log by linear language model of me in Hinton.",
                    "label": 1
                },
                {
                    "sent": "What they do is they say we have the first 4 words it's another 5 gram model, at least in the experiments that have been conducted.",
                    "label": 0
                },
                {
                    "sent": "They take the first 4 words.",
                    "label": 0
                },
                {
                    "sent": "They have a linear model to predict the representation of the fifth word.",
                    "label": 0
                },
                {
                    "sent": "They pass it through an exponential distribution to compute the probability normalized with respect to all the other words in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "The probability that this prediction this linear prediction of the final word matches the target prediction and they back propagate their error to tune the embeddings as well as the model.",
                    "label": 1
                },
                {
                    "sent": "Features are they use?",
                    "label": 0
                },
                {
                    "sent": "There are no features, they are tuning on the embedding matrix.",
                    "label": 0
                },
                {
                    "sent": "This matrix E over here.",
                    "label": 0
                },
                {
                    "sent": "That's that's what they're tuning.",
                    "label": 0
                },
                {
                    "sent": "They have the weights that that map word to its embedding.",
                    "label": 0
                },
                {
                    "sent": "They have the weights that map the four gram to the 5th, the fifth word, and they basically want to learn the model as well as the embeddings simultaneously so that the first 4 words are good at predicting the last word, and that's it.",
                    "label": 0
                },
                {
                    "sent": "The words themselves.",
                    "label": 0
                },
                {
                    "sent": "And then that's passed into the embedding matrix.",
                    "label": 0
                },
                {
                    "sent": "Yes, the one heart is the is the original representation.",
                    "label": 0
                },
                {
                    "sent": "It's multiplied by the embedding matrix and the embedding matrix, as well as the linear model is tuned, and that's it.",
                    "label": 0
                },
                {
                    "sent": "It's a similar to Colbert, and Weston is just a different form, but everything, everything is just automatically tuned here.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that there's that partition partition function that has to be computed.",
                    "label": 0
                },
                {
                    "sent": "Pesky partition function mean Hinton in 2009.",
                    "label": 0
                },
                {
                    "sent": "Come up with a hierarchical approach for computing the partition function, which is much faster.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize the approach again, we're going to use a large corpus and in an unsupervised manner induced word representation.",
                    "label": 0
                },
                {
                    "sent": "Brown it took three days, the LBL approach in one week we have about 100 epochs of training Colbert and Weston.",
                    "label": 0
                },
                {
                    "sent": "About four weeks we can do about 20 epochs of training.",
                    "label": 1
                },
                {
                    "sent": "The downside with these these distributed representations, you're never really sure when to stop training.",
                    "label": 0
                },
                {
                    "sent": "According to Ronan, he just kind of lets the computer keep going.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to use this word features in the supervised task.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our unsupervised corpus.",
                    "label": 0
                },
                {
                    "sent": "We used the Reuters corpus.",
                    "label": 0
                },
                {
                    "sent": "It's about 40,000,000 words and about 270 word types in the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our supervised task we're looking at named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Our model is an average perceptron, which is a linear classifier, and we're basing our work on the state of the art model of right and often wrath.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so one thing we have to do is when we when we insert the Brown features Brown embedding this features they just kind of work.",
                    "label": 0
                },
                {
                    "sent": "Whereas with the distributor representations it's not really clear if the scale of these features matches those of the other features which are discrete and class based.",
                    "label": 0
                },
                {
                    "sent": "So one hyperparameter we have to tune.",
                    "label": 0
                },
                {
                    "sent": "I just show the tuning curve.",
                    "label": 0
                },
                {
                    "sent": "Here is a normalization constant that.",
                    "label": 0
                },
                {
                    "sent": "Adjust the weight of the distributed representation features with respect to the other features in the model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another thing, Percy Liang does this and later, I think Terry Koo did this.",
                    "label": 0
                },
                {
                    "sent": "They apply a cleaning technique whereby they throw out about 50% of the sentences in the Reuters corpus that don't really look like real sentence.",
                    "label": 0
                },
                {
                    "sent": "Is things like sports scores and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "We tried that out, and it turns out that just in general, both with the distributor representations and with the Brown embeddings that cleaning the corpus or pre processing it.",
                    "label": 0
                },
                {
                    "sent": "Leads to better word representations.",
                    "label": 0
                },
                {
                    "sent": "This just suggests that we need better training strategies.",
                    "label": 0
                },
                {
                    "sent": "In fact, we found that like ad hoc pruning methods also sometimes increase the accuracy.",
                    "label": 0
                },
                {
                    "sent": "It suggests that.",
                    "label": 0
                },
                {
                    "sent": "Some sort of some sort of cleaning process needs to be applied, or perhaps we want a curriculum strategy that learns the core of the language 1st and then expands outwards to what is less clear about if we were to do that, maybe we could do the cleaning automatically, and when it needs some sort of preprocessing step that we've thought through.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you some results.",
                    "label": 0
                },
                {
                    "sent": "Not all of these are in the proceedings, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Some of them were late breaking, but generally what we see is tuning, set evaluation set and mock seven data.",
                    "label": 0
                },
                {
                    "sent": "We also we also try to a few more combinations of tuning set in valuations that generally you find that the Brown embeddings have the best F1 score.",
                    "label": 0
                },
                {
                    "sent": "But it's not indicating the valuation set, but in every other condition we saw, we saw that the 100 dimensional ECHL is better than the 50 dimensional ECHL.",
                    "label": 0
                },
                {
                    "sent": "We also found that Colbert in Weston embeddings performed on about on par with the LBL embeddings and Lastly, the baseline is pretty far behind, so there were two things that were surprising about these results to us.",
                    "label": 0
                },
                {
                    "sent": "The first was that we were expecting the distributed representations.",
                    "label": 0
                },
                {
                    "sent": "To be much better because they were estimated over 5 grams and we felt that because it was able to capture statistics over this entire window, that the representations would just be inherently more rich than the Brown clusters which were over bigram statistics.",
                    "label": 0
                },
                {
                    "sent": "But this did not turn out to be true.",
                    "label": 0
                },
                {
                    "sent": "In our experiment.",
                    "label": 0
                },
                {
                    "sent": "The other hypothesis we had was we speculated that perhaps because we had a linear supervised classifier that using.",
                    "label": 0
                },
                {
                    "sent": "A linear unsupervised model would be better.",
                    "label": 0
                },
                {
                    "sent": "That would suggest that the color bear in Weston embeddings would not necessarily be as good as the HLB L embeddings, but that didn't actually turn out to be true when they when they were controlled for the same number of dimensions, they turn out to be about the same.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our working hypothesis right now.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear supervised model like the Perceptron, and this is generally the case in natural language processing.",
                    "label": 1
                },
                {
                    "sent": "In general, you'll probably want to prefer representations that are high dimensional and sparse, because if you have a dent in low dimensional representation, it is too difficult for a linear classifier to correctly estimate the probability distribution of the labels this.",
                    "label": 1
                },
                {
                    "sent": "Hypothesis, we believe, will be especially true when the supervised task allowed the decisions are based on rare words, which is the case with named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Perhaps if we were doing parsing, then the distributor representations would be stronger than the Brown embeddings, but we don't know.",
                    "label": 0
                },
                {
                    "sent": "We don't know, but the main thing here is that.",
                    "label": 0
                },
                {
                    "sent": "With a linear model, we do feel like the high dimensional sparse representation is superior.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a few other difficulties when working with word embeddings that I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "The first is that there's no clear stopping criterion during training.",
                    "label": 1
                },
                {
                    "sent": "The second is that training is actually the supervised training phase is actually slower because there are more active features.",
                    "label": 1
                },
                {
                    "sent": "50 active features worth versus maybe five in the Brown clusters.",
                    "label": 0
                },
                {
                    "sent": "And there are also more hyperparameters you need to learn.",
                    "label": 0
                },
                {
                    "sent": "There's the learning rate for the model.",
                    "label": 1
                },
                {
                    "sent": "Optionally, you can also have a separate learning rate for the embeddings, which we found works better.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, there's the normalization constant.",
                    "label": 0
                },
                {
                    "sent": "They adjust the importance of these features with respect to the other features.",
                    "label": 0
                },
                {
                    "sent": "And with the Brown clusters, there aren't actually any hyperparameters that we tuned.",
                    "label": 0
                },
                {
                    "sent": "There's maybe the number of classes, but we just said it at 1000 like everyone else in larger does.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to summarize.",
                    "label": 0
                },
                {
                    "sent": "When we're talking about in our experiments, specifically in our experiments, when there is a linear model and we're evaluating named entity recognition, we found the Brown works better than the distributor representations that the 100 dimensional distributor representations work better than the 50 dimensional ones.",
                    "label": 0
                },
                {
                    "sent": "And the LBL is about on par with the Colbert in Weston.",
                    "label": 0
                },
                {
                    "sent": "We also found that the H LBL approach was about 20 times faster train than Colbert in West, and the reason is first of all there's no hidden layer and technical under D uses a diagonal matrix, just faster.",
                    "label": 0
                },
                {
                    "sent": "Another advantage that we found the Brown House is that it just it just says 0 hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "That it's training is also faster because there are fewer active features in future work.",
                    "label": 0
                },
                {
                    "sent": "Were interested to explore whether a sparse in high dimensional approach.",
                    "label": 0
                },
                {
                    "sent": "To inducing the word embeddings would be superior to the sparse high dimensional Brown approach, and if it would be better than just the dense low dimensional approach.",
                    "label": 0
                },
                {
                    "sent": "That's my talk.",
                    "label": 0
                },
                {
                    "sent": "Two questions.",
                    "label": 0
                },
                {
                    "sent": "So what about the adapting representations?",
                    "label": 0
                },
                {
                    "sent": "The problem?",
                    "label": 0
                },
                {
                    "sent": "So that's pretty straightforward, right?",
                    "label": 0
                },
                {
                    "sent": "Credit on them, whereas if you Brown that stuff.",
                    "label": 0
                },
                {
                    "sent": "I agree.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's definitely an advantage when you're using a gradient based method.",
                    "label": 0
                },
                {
                    "sent": "The reason we didn't do that, there were several restrictions that we placed ourselves in this work that at least you know for us we consider fairly unnatural, like a linear classifier or not tuning the embeddings.",
                    "label": 0
                },
                {
                    "sent": "The goal is basically to.",
                    "label": 0
                },
                {
                    "sent": "Create word features offline and then give them to people in the NLP community and expect that they will use the techniques that they typically use and then take the features and run with it.",
                    "label": 0
                },
                {
                    "sent": "But as you said, if you want to be more sophisticated you could tune the embeddings.",
                    "label": 0
                },
                {
                    "sent": "When you re training?",
                    "label": 0
                },
                {
                    "sent": "It's true, that's actually true.",
                    "label": 0
                },
                {
                    "sent": "You're right, you're right about that.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering.",
                    "label": 0
                },
                {
                    "sent": "How did the did you look at how the various algorithms do?",
                    "label": 0
                },
                {
                    "sent": "Did you look at how the various algorithms do as a function of the frequency of the words?",
                    "label": 0
                },
                {
                    "sent": "Like?",
                    "label": 0
                },
                {
                    "sent": "Is it that maybe they distributed once I'm not doing as well the rare words, but we met the frequency and maybe get some sort of overfitting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and that's a really good idea.",
                    "label": 0
                },
                {
                    "sent": "We intend to do that, but we haven't looked at yet.",
                    "label": 0
                }
            ]
        }
    }
}