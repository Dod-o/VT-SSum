{
    "id": "ocfvyaf6mcgybze52vpiqyawutxtzn5x",
    "title": "Preparing multi-modal data for natural language processing",
    "info": {
        "author": [
            "Erik Novak, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Oct. 23, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/sikdd2018_novak_natural_language_processing/",
    "segmentation": [
        [
            "Hello, my name is Eric Novak and I'm going to talk about our work on preparing multimodal data from natural language processing, and this work was done in collaboration with also, yes, no bunch and mechanical, so let's first talk about the motivation five we went or deal with this topic."
        ],
        [
            "So we all know that there is that there are students and teachers.",
            "While they're like studying, they're normally also searching for learning materials from which they can learn from, and like students learn for their degree or to go through their learning process while the teachers.",
            "Search for learning materials so that it would then help for explaining some of the topics that he's an expert in and there are millions of educational materials found everywhere and they are in different modalities.",
            "Different formats like text, video, audio and and others.",
            "There are also written in different languages, so there's also this language barrier that they need to overcome.",
            "And there are also different learning preferences preferences, so in this case like some of the students rather watch.",
            "Longer lectures and watch longer videos or just watch videos while the others like to read papers and articles and so on.",
            "So in here what we wanted to do is help.",
            "Help to find this educational materials that would then be personalized for the users.",
            "For the students and for the teachers.",
            "But before we go that path we also need to preprocess this data and collect this data and for that we we did the preprocessing pipeline that handles multi model in cross lingual data and for here I'm going to focus on educational material but it can be also used on other domains and another.",
            "Data sources.",
            "And I'm going to talk about this preprocessing pipe."
        ],
        [
            "So first I'm going to go through the how the pipeline looks like.",
            "What are the components, and then I'm going to explain how what kind of data we acquired using this pipeline and what we find out from the data and at the end we're going to show how we use the data in the end, which is in the form of a recommender engine.",
            "And here the focus of the data is open educational resources and these are learning materials that are openly accessed and can be used for mostly for educational purposes, but also for research purposes."
        ],
        [
            "So this is how the preprocessing pipeline looks like.",
            "So the first 2 steps are basically like.",
            "It's not really something special you.",
            "First we need to get the data and then we format it so that we know what kind of topics or what kind of fields we are looking for or what we expect to get and then out of these materials we get we extract text and then we go through the verification process and in the end we store the data into a database.",
            "And now I'm going to go through each of these steps."
        ],
        [
            "So for this open educational resources, we targeted four repositories from which we use dedicated APIs and custom crawlers so that we could gather as much data as we could.",
            "In each of material of this material, we collected the meta data which is the title, the description, the URL.",
            "So where the material is located, the type.",
            "So the type of material.",
            "So if it's a text with the audio and so on, and then also the language in which it was written or spoken for video and audio files, and who is the provider?",
            "So here the provider is one of these two.",
            "One of these four.",
            "Repository's.",
            "And because there are different repositories and because of for each positive repository, we get different types of data, different metadata."
        ],
        [
            "We had to format the data in such a way that so here we designated.",
            "What are the material attributes that are required.",
            "So what are the fields that we must have so that we can do then further analysis and to set up when setting up this?",
            "Setting up this schema So what are the fields that are required?",
            "We also have some setting for checking missing material attributes so at the end of the pipeline we check if all of these materials are price or all of these attributes of the material are present and then we store in the data.",
            "Otherwise we have it somewhere else.",
            "Located so that we can then in the future we would then go through and check what went wrong.",
            "And once we have this format."
        ],
        [
            "We extract the content from the material in a textual form white actual form because we have other, because text is quite easy to analyze, its we have still.",
            "I mean we still have like different methods.",
            "How to analyze the text?",
            "And here based on the type of field type of file we handle, each type of file type separately.",
            "For text we used extract which is library for extracting the content of the actual file out in a raw text it omits figures, but it also, but currently as far as we know it doesn't know how to represent mathematical equations and other symbols.",
            "So we need to also handle this.",
            "Problems or this this problem?",
            "While for the video and audio files we use translators, which is a service which for a given file it returns a transcripts or translation translation of the file.",
            "In RDF XP format, which is the standard for for captions, an for subtitles and this service is for now at least as far as we have access it.",
            "It supports 4 languages, English, Spanish, German and Slovene.",
            "But because of this limited language coverage we will also need to find out what are the other possibilities for other languages.",
            "But for now for this prototype it's.",
            "Sufficient enough?",
            "And from the this captions and subtitles we then extract the whole subtitles in raw text, which is then same format As for when we handle the actual files."
        ],
        [
            "Next we go through the week ification process.",
            "So here we take the raw text and put it into this process and this.",
            "This actually does the following.",
            "It links the material texture components to the corresponding Wikipedia page.",
            "So within the textual file or within the raw text, we extract out water.",
            "The Wikipedia concepts that are related to this material and also this Wikipedia concepts can be used as an abstract or abstract of the file.",
            "And for this we use Wiki, wiki fire Services Service, which finds Wikipedia concepts that are related to textual input.",
            "And it also supports crossing Multilinguality, so it automatically detects in which language the text is written in.",
            "The only limitation that we found is that wiki fire has a limit of 20,000 characters, for which this is the limit for us, because some of the files are or some of some of the files are quite long and we have a lot of long textual files that.",
            "Go over 20,000 characters, so for that we need to split this whole.",
            "Content into chunks, and then preprocess sent each chunk to the weekly fire process and then aggregate this concept so that we have some represent Wikipedia concept representation for the whole material.",
            "So in the end we validate the attributes and then we store it into the database.",
            "So for the four repositories that was that were mentioned in the previous slides."
        ],
        [
            "We acquired in preprocess approximately 90,000 items and in which we found that this repository is cover about 103 languages and this graph shows the languages that have at least 100 materials, so the most dominant language this graph is in log scale.",
            "And here we see that the most dominant language is English followed by Italian and Slovenian, and for the unknown languages.",
            "We need to handle some some other way because it seems that some of these files or some yeah some of these files had a lot of noise for which the services that we used for language detection couldn't identify what what, what is the actual language."
        ],
        [
            "And then.",
            "We also have, like we did the statistics for the file type, so we saw that there are in different formats that the most dominant one is the text which is we can see in PDF, PowerPoint and word documents, but followed.",
            "Then it's followed by videos.",
            "And if we generalize this graph, we can do all of the forearm material.",
            "We can then say the text is the dominant.",
            "Textual files are the dominant learning materials out there.",
            "So for the preprocessing pipeline we would then need to handle or more specialized for this kind of materials.",
            "And now that we have like we saw what this pipeline can do, and now I'm going to present."
        ],
        [
            "Application of the recommend, which is the recommended engine.",
            "Here we developed a simple content based recommender engine using K nearest neighbor algorithm and 1st first of all we tried to the bag of words approach where we found out that this approach doesn't support or doesn't provide any.",
            "Cross lingual materials.",
            "So if you provide an text in English, it will provide back materials that are written in English while here.",
            "When we use Wikipedia concepts so we make bag of concepts.",
            "And use this to compare materials.",
            "We found out that it will provide cross lingual recommendations.",
            "We can see here from the graph where this one is in English.",
            "This is in German and this is also in Italian.",
            "And the other thing is, because the Wikipedia concepts were extracted from the material content, it doesn't like it doesn't.",
            "Matter if the file is in text or video or audio and so on.",
            "So using Wikipedia concepts is also provide multimodel results.",
            "So here.",
            "We found out that by using the preprocessor pipe preprocessing pipeline, we can then handle both multimodel materials and also cross lingual materials.",
            "So."
        ],
        [
            "To conclude.",
            "We presented to be developed a methodology for preprocessing, multimodel and cross lingual items and in the future what we want to do is improve the textual extraction methods and also handle missing material attributes so automatically so that it would automatically handle this kind of stuff and also add new features which would then help into the textual analysis for the learning materials, in this case to determine the quality and also the topic of the material.",
            "So this is it.",
            "Thank you for your.",
            "Thank you and yeah I can take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, my name is Eric Novak and I'm going to talk about our work on preparing multimodal data from natural language processing, and this work was done in collaboration with also, yes, no bunch and mechanical, so let's first talk about the motivation five we went or deal with this topic.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we all know that there is that there are students and teachers.",
                    "label": 1
                },
                {
                    "sent": "While they're like studying, they're normally also searching for learning materials from which they can learn from, and like students learn for their degree or to go through their learning process while the teachers.",
                    "label": 0
                },
                {
                    "sent": "Search for learning materials so that it would then help for explaining some of the topics that he's an expert in and there are millions of educational materials found everywhere and they are in different modalities.",
                    "label": 0
                },
                {
                    "sent": "Different formats like text, video, audio and and others.",
                    "label": 1
                },
                {
                    "sent": "There are also written in different languages, so there's also this language barrier that they need to overcome.",
                    "label": 0
                },
                {
                    "sent": "And there are also different learning preferences preferences, so in this case like some of the students rather watch.",
                    "label": 0
                },
                {
                    "sent": "Longer lectures and watch longer videos or just watch videos while the others like to read papers and articles and so on.",
                    "label": 0
                },
                {
                    "sent": "So in here what we wanted to do is help.",
                    "label": 0
                },
                {
                    "sent": "Help to find this educational materials that would then be personalized for the users.",
                    "label": 0
                },
                {
                    "sent": "For the students and for the teachers.",
                    "label": 0
                },
                {
                    "sent": "But before we go that path we also need to preprocess this data and collect this data and for that we we did the preprocessing pipeline that handles multi model in cross lingual data and for here I'm going to focus on educational material but it can be also used on other domains and another.",
                    "label": 1
                },
                {
                    "sent": "Data sources.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about this preprocessing pipe.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'm going to go through the how the pipeline looks like.",
                    "label": 0
                },
                {
                    "sent": "What are the components, and then I'm going to explain how what kind of data we acquired using this pipeline and what we find out from the data and at the end we're going to show how we use the data in the end, which is in the form of a recommender engine.",
                    "label": 0
                },
                {
                    "sent": "And here the focus of the data is open educational resources and these are learning materials that are openly accessed and can be used for mostly for educational purposes, but also for research purposes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is how the preprocessing pipeline looks like.",
                    "label": 1
                },
                {
                    "sent": "So the first 2 steps are basically like.",
                    "label": 0
                },
                {
                    "sent": "It's not really something special you.",
                    "label": 0
                },
                {
                    "sent": "First we need to get the data and then we format it so that we know what kind of topics or what kind of fields we are looking for or what we expect to get and then out of these materials we get we extract text and then we go through the verification process and in the end we store the data into a database.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to go through each of these steps.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this open educational resources, we targeted four repositories from which we use dedicated APIs and custom crawlers so that we could gather as much data as we could.",
                    "label": 1
                },
                {
                    "sent": "In each of material of this material, we collected the meta data which is the title, the description, the URL.",
                    "label": 0
                },
                {
                    "sent": "So where the material is located, the type.",
                    "label": 0
                },
                {
                    "sent": "So the type of material.",
                    "label": 0
                },
                {
                    "sent": "So if it's a text with the audio and so on, and then also the language in which it was written or spoken for video and audio files, and who is the provider?",
                    "label": 0
                },
                {
                    "sent": "So here the provider is one of these two.",
                    "label": 0
                },
                {
                    "sent": "One of these four.",
                    "label": 0
                },
                {
                    "sent": "Repository's.",
                    "label": 0
                },
                {
                    "sent": "And because there are different repositories and because of for each positive repository, we get different types of data, different metadata.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We had to format the data in such a way that so here we designated.",
                    "label": 0
                },
                {
                    "sent": "What are the material attributes that are required.",
                    "label": 1
                },
                {
                    "sent": "So what are the fields that we must have so that we can do then further analysis and to set up when setting up this?",
                    "label": 1
                },
                {
                    "sent": "Setting up this schema So what are the fields that are required?",
                    "label": 0
                },
                {
                    "sent": "We also have some setting for checking missing material attributes so at the end of the pipeline we check if all of these materials are price or all of these attributes of the material are present and then we store in the data.",
                    "label": 1
                },
                {
                    "sent": "Otherwise we have it somewhere else.",
                    "label": 0
                },
                {
                    "sent": "Located so that we can then in the future we would then go through and check what went wrong.",
                    "label": 0
                },
                {
                    "sent": "And once we have this format.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We extract the content from the material in a textual form white actual form because we have other, because text is quite easy to analyze, its we have still.",
                    "label": 1
                },
                {
                    "sent": "I mean we still have like different methods.",
                    "label": 0
                },
                {
                    "sent": "How to analyze the text?",
                    "label": 1
                },
                {
                    "sent": "And here based on the type of field type of file we handle, each type of file type separately.",
                    "label": 0
                },
                {
                    "sent": "For text we used extract which is library for extracting the content of the actual file out in a raw text it omits figures, but it also, but currently as far as we know it doesn't know how to represent mathematical equations and other symbols.",
                    "label": 0
                },
                {
                    "sent": "So we need to also handle this.",
                    "label": 0
                },
                {
                    "sent": "Problems or this this problem?",
                    "label": 0
                },
                {
                    "sent": "While for the video and audio files we use translators, which is a service which for a given file it returns a transcripts or translation translation of the file.",
                    "label": 0
                },
                {
                    "sent": "In RDF XP format, which is the standard for for captions, an for subtitles and this service is for now at least as far as we have access it.",
                    "label": 0
                },
                {
                    "sent": "It supports 4 languages, English, Spanish, German and Slovene.",
                    "label": 0
                },
                {
                    "sent": "But because of this limited language coverage we will also need to find out what are the other possibilities for other languages.",
                    "label": 0
                },
                {
                    "sent": "But for now for this prototype it's.",
                    "label": 0
                },
                {
                    "sent": "Sufficient enough?",
                    "label": 0
                },
                {
                    "sent": "And from the this captions and subtitles we then extract the whole subtitles in raw text, which is then same format As for when we handle the actual files.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we go through the week ification process.",
                    "label": 0
                },
                {
                    "sent": "So here we take the raw text and put it into this process and this.",
                    "label": 0
                },
                {
                    "sent": "This actually does the following.",
                    "label": 0
                },
                {
                    "sent": "It links the material texture components to the corresponding Wikipedia page.",
                    "label": 1
                },
                {
                    "sent": "So within the textual file or within the raw text, we extract out water.",
                    "label": 0
                },
                {
                    "sent": "The Wikipedia concepts that are related to this material and also this Wikipedia concepts can be used as an abstract or abstract of the file.",
                    "label": 0
                },
                {
                    "sent": "And for this we use Wiki, wiki fire Services Service, which finds Wikipedia concepts that are related to textual input.",
                    "label": 1
                },
                {
                    "sent": "And it also supports crossing Multilinguality, so it automatically detects in which language the text is written in.",
                    "label": 0
                },
                {
                    "sent": "The only limitation that we found is that wiki fire has a limit of 20,000 characters, for which this is the limit for us, because some of the files are or some of some of the files are quite long and we have a lot of long textual files that.",
                    "label": 0
                },
                {
                    "sent": "Go over 20,000 characters, so for that we need to split this whole.",
                    "label": 0
                },
                {
                    "sent": "Content into chunks, and then preprocess sent each chunk to the weekly fire process and then aggregate this concept so that we have some represent Wikipedia concept representation for the whole material.",
                    "label": 0
                },
                {
                    "sent": "So in the end we validate the attributes and then we store it into the database.",
                    "label": 0
                },
                {
                    "sent": "So for the four repositories that was that were mentioned in the previous slides.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We acquired in preprocess approximately 90,000 items and in which we found that this repository is cover about 103 languages and this graph shows the languages that have at least 100 materials, so the most dominant language this graph is in log scale.",
                    "label": 1
                },
                {
                    "sent": "And here we see that the most dominant language is English followed by Italian and Slovenian, and for the unknown languages.",
                    "label": 0
                },
                {
                    "sent": "We need to handle some some other way because it seems that some of these files or some yeah some of these files had a lot of noise for which the services that we used for language detection couldn't identify what what, what is the actual language.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We also have, like we did the statistics for the file type, so we saw that there are in different formats that the most dominant one is the text which is we can see in PDF, PowerPoint and word documents, but followed.",
                    "label": 1
                },
                {
                    "sent": "Then it's followed by videos.",
                    "label": 0
                },
                {
                    "sent": "And if we generalize this graph, we can do all of the forearm material.",
                    "label": 0
                },
                {
                    "sent": "We can then say the text is the dominant.",
                    "label": 0
                },
                {
                    "sent": "Textual files are the dominant learning materials out there.",
                    "label": 0
                },
                {
                    "sent": "So for the preprocessing pipeline we would then need to handle or more specialized for this kind of materials.",
                    "label": 0
                },
                {
                    "sent": "And now that we have like we saw what this pipeline can do, and now I'm going to present.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Application of the recommend, which is the recommended engine.",
                    "label": 0
                },
                {
                    "sent": "Here we developed a simple content based recommender engine using K nearest neighbor algorithm and 1st first of all we tried to the bag of words approach where we found out that this approach doesn't support or doesn't provide any.",
                    "label": 0
                },
                {
                    "sent": "Cross lingual materials.",
                    "label": 0
                },
                {
                    "sent": "So if you provide an text in English, it will provide back materials that are written in English while here.",
                    "label": 0
                },
                {
                    "sent": "When we use Wikipedia concepts so we make bag of concepts.",
                    "label": 0
                },
                {
                    "sent": "And use this to compare materials.",
                    "label": 0
                },
                {
                    "sent": "We found out that it will provide cross lingual recommendations.",
                    "label": 0
                },
                {
                    "sent": "We can see here from the graph where this one is in English.",
                    "label": 0
                },
                {
                    "sent": "This is in German and this is also in Italian.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, because the Wikipedia concepts were extracted from the material content, it doesn't like it doesn't.",
                    "label": 1
                },
                {
                    "sent": "Matter if the file is in text or video or audio and so on.",
                    "label": 0
                },
                {
                    "sent": "So using Wikipedia concepts is also provide multimodel results.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "We found out that by using the preprocessor pipe preprocessing pipeline, we can then handle both multimodel materials and also cross lingual materials.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We presented to be developed a methodology for preprocessing, multimodel and cross lingual items and in the future what we want to do is improve the textual extraction methods and also handle missing material attributes so automatically so that it would automatically handle this kind of stuff and also add new features which would then help into the textual analysis for the learning materials, in this case to determine the quality and also the topic of the material.",
                    "label": 1
                },
                {
                    "sent": "So this is it.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your.",
                    "label": 0
                },
                {
                    "sent": "Thank you and yeah I can take questions.",
                    "label": 0
                }
            ]
        }
    }
}