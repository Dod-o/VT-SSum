{
    "id": "ogndepekix2g3cm767y6u2bseyhjmi2p",
    "title": "Inductive Rule Learning in a Nutshell",
    "info": {
        "author": [
            "Johannes F\u00fcrnkranz, Department of Computer Science, Darmstadt University of Technology"
        ],
        "published": "Sept. 25, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/ktsymposium2013_fuernkranz_rule_learning/",
    "segmentation": [
        [
            "So thank you for the introduction.",
            "As you said, one of the problems was that your book has not appeared previously is because it took us six years to write in.",
            "That was the one of the problems, but we managed.",
            "We do now in the 30 minutes or so is I'll try to give a very basic introduction into learning, and on the example of learning an introduction to machine learning in general to some of the fundamental concepts there."
        ],
        [
            "So one frequently or the most frequently studied scenario in machine learning is the induction of classifiers.",
            "You can think of it as.",
            "The problem where you have a couple of examples of some sort and they have some property attached to it.",
            "Like here their color, their green bonds, red ones, yellow ones and they are used for training a thing called a classifier, and this classifier is able then to take a new example.",
            "This has the color Gray, which stands for unknown here.",
            "And determine this color.",
            "So it automatically is able to assign categories to new objects based on the experience it has covered from previously categorized objects.",
            "From these here, these are called the training set."
        ],
        [
            "And in this setting of classification and even simpler setting is concept learning.",
            "There you essentially have two classes, positive and negative, and you can think of them as examples for concept.",
            "The positive examples that are examples for the concept and counter examples, negative examples that are examples.",
            "That do not hold for this concept and.",
            "The goal of concept learning is from these positive and negative examples to induce a description of this concept.",
            "So a description that this ideally also understandable but also computable.",
            "So.",
            "We also called this concept in a hypothesis and what we want to find is the target hypothesis and the hypothesis that are considered in the in the process are candidate hypothesis and in the end one of these candidates should turn out to be the target.",
            "In the ideal situation."
        ],
        [
            "So we want to do that with rules, so our concept should be rules.",
            "There are many other concept descriptions in machine learning.",
            "Decision trees, neural network, support vector machines.",
            "We want him to be rules.",
            "What is a rule look like?",
            "So here's a simple conjunctive rule.",
            "It has a body that is the if part.",
            "It describes these objects this that if part of the rule refers to some categories, some descriptions that we know about these objects.",
            "And the head of the rule that is the conclusion that would be the color, or in our case positive or negative.",
            "So the body characterizes the object and the head contains the classification.",
            "We also say that a rule covers an object or an example.",
            "If this example fulfills these conditions of the rule.",
            "And the prediction then is if a rule covers an example, then the conclusion, positive or negative, is predicted for this example."
        ],
        [
            "So here's a very simple and classical example task in machine learning you have.",
            "You observe your neighbor, let's say, and you observe the weather conditions and you want to know whether your neighbor goes out or not.",
            "You want to know whether he is home tomorrow or not for some reason.",
            "So you observe for two weeks the weather conditions.",
            "For example, if it's hot out and the outlook is sunny and it is humid and it's not windy, then he doesn't go out.",
            "He doesn't go.",
            "Usually, he only goes out for playing golf, so he doesn't go play.",
            "Golf in these conditions, or this was two weeks ago yesterday.",
            "Temperature was mild.",
            "It was rainy.",
            "It was humid and it was not windy and there he did go out.",
            "So you have some observations and the goal is now you want to find a rule that lets you predict whether he goes out tomorrow or not.",
            "So if you get the weather forecast for tomorrow and you have your rule, you can predict whether he goes out or not."
        ],
        [
            "There's a very simple rule based solution that at least explains these training examples and simple rule based solution is you just turn every one of these positive examples.",
            "So here we are."
        ],
        [
            "Here we have 3456789 examples where we predict that he goes out.",
            "This is the positive class.",
            "This is the one we want to learn a description for.",
            "And."
        ],
        [
            "You can turn every one of these into a rule.",
            "So this rule set will explain your training examples perfectly well, but of course this is not a good solution.",
            "You haven't learned anything, you have just repeated what you have observed."
        ],
        [
            "A much better solution would be a very small and compact.",
            "A simple rule set like this one.",
            "There you have only three rules and T3 rules explained.",
            "The 14 examples for all of the nine cases where you predict yes one of these rules predict yes, and for all of the five cases where we want to predict no, none of these rules predicts yes, so that's a much better so."
        ],
        [
            "Lucien, how can we find such a solution?",
            "Very classical strategy is called separate and conquer rule learning or covering rule learning.",
            "The idea here is that we learn these rules one rule at the time and.",
            "Every time we have learned one rule that explains parts of the examples.",
            "We remove the examples that are explained by this rule and then we have a smaller problem where there are fewer examples remaining and these are then later covered by the neck by the next iteration of this loop.",
            "So we learn another rule that explains more examples and the one that's a very old strategy, but it is still in Rule learning frequently."
        ],
        [
            "Used so far we have said that we want to find a rule that explains the positive examples and it doesn't cover any of the negative examples.",
            "That is our goal.",
            "We want to have rules that predict all yeses and don't predict any of the nose.",
            "Actually, it turns out that it is usually quite good to relax these constraints a little bit.",
            "We do not want to be super precise, because this is also part of a phenomenon that is called overfitting and I will come to two will come back to that a little bit later in this talk.",
            "So for the moment the short example, if we have a training set that has 200 examples, hundred of them are positive, 100 or negative and we find.",
            "One rule set like the first one I showed you that has 100 rules, each one covering one of the positive examples that explains these training data perfectly well.",
            "And then we have another theorie that consists of a single rule and this single rule covers 99 of the positive examples, so it only misses one.",
            "And it also covers one of the negative examples, so it makes one false prediction.",
            "Still, people would say that this theory beat single rule that covers that explains all of the examples with two mistakes is better than this very complex theory where you explain all of the examples but make.",
            "But have one rule for each example."
        ],
        [
            "Graphically, this separate and conquer strategy can be imagined in in a 2 dimensional space.",
            "So here we have maybe two numerical descriptors and X value and the Y value and hear the pluses and minuses indicate very an example of this class lies in this 2 dimensional space, so this is the data set.",
            "Then we would find one rule that explains part of the example space.",
            "This is a rule because you can describe rules with intervals and intervals, so the rule would be something like if X is smaller than equal than a value here and greater than equal, then the value here and the same for Y.",
            "Then you have two intervals and you have.",
            "Defined a rule that describes such a rectangle here.",
            "So here is a rule, and then we remove all these examples and.",
            "Is the first rule, then we have a second one.",
            "If you remove all the examples and found this second another rule and.",
            "This is the third rule that has to be found then in the end.",
            "See that this rule, for example, would cover two of the negative examples, but as we just explained."
        ],
        [
            "This is not always a problem.",
            "Some of terminology we have.",
            "Training examples positive and negative training examples.",
            "We I denote them with capital P and capital N. We have examples covered by the rule.",
            "Small P are the true positives.",
            "Those are the ones that are covered and should be covered and small N are the examples that are covered by the rule but should not be covered.",
            "These are negative examples covered by a rule that predicts positive and the others are the negative.",
            "The false negatives into true negatives.",
            "These are true negatives or negative examples that should be should not be covered, and false negatives are positive examples.",
            "That should not.",
            "That should actually be covered by a rule, but or not."
        ],
        [
            "This is maybe a little bit clearer if we try to visualize this in a space which we call the coverage space in this coverage space.",
            "We have two axis, these are the negative examples.",
            "These are the positive examples.",
            "And here is the point where we don't cover any examples, so this would be an empty theory theory that is always false that never predicts positive would be here.",
            "Up here is a universal theory that would be the theory that always predicts positive no matter what the example is.",
            "It always says this is a positive example.",
            "And up here, this is where we actually want to get to.",
            "This is the perfect theory.",
            "Here we would cover all of the positive examples and none of the negative examples, so the negative 0 positive, so it's maximum possible value.",
            "And down here is a theory that might be on first sight considered perfectly bad, so it does everything wrong.",
            "But if you start to think of it, that's actually not a bad theory because you have a theory that always predicts wrong.",
            "You can just invert its predictions and you have a theory that always is right, so there really bad theories are those that are here on the diagonal.",
            "They essentially only make random predictions.",
            "And.",
            "These are lines that.",
            "Have the same accuracy, so all the rules on this line here.",
            "So he ruled that covers one exam positive and one negative, or hear any rule that covers 5 positive and five negative.",
            "See if you evaluate them with accuracy, they all."
        ],
        [
            "Produce the same result so they are all equally good.",
            "The covering strategy that I just showed you before the separate and conquer strategy moves through this space by starting at an empty theory.",
            "Here in the beginning it doesn't have a rule, then it adds the first rule and it moves up in this space to appoint here covering some.",
            "Negatives, but many positives.",
            "Then it adds a second rule it.",
            "Then it covers more positives and maybe also a little bit more negatives and so on until it is at the end at the rule where it cover at the ruleset where it covers all examples and one of these points should be the final rule set then.",
            "So probably we will don't want to choose this one here where we cover all examples, but maybe this one could be the best strategy that depends only a little bit on how you.",
            "Evaluate.",
            "And the interesting thing is, after you remove after you learn one rule, you remove all examples that are covered by this rule and you have another coverage space that.",
            "Corresponds to the.",
            "To the new problem where you have fewer examples, you have removed all the positive and negative examples that are covered and then in this new problem you want to find the next rule.",
            "You have found the next rule.",
            "Then you have an even smaller problem and so on."
        ],
        [
            "How are these rules found now?",
            "A simple strategy for finding these rules is top down Hill climbing sounds a little bit counter intuitive if you want to climb a Hill, but you start from the top.",
            "Did their explanation for that is that these are actually two different concepts.",
            "Top down means that you start from the very general concept and you specialize it by adding conditions.",
            "This means that we start here with a rule that explains everything, and then we successively add conditions.",
            "So we essentially go in the other direction as the covering strategy does for finding a single rule for finding a single rule, we go down and for and for going up again.",
            "We add other rules.",
            "So this is the top down Barton, the Hill climbing parties that for each of these steps we evaluate all candidates with some sort of measure.",
            "And we take the one that gives you the highest increase in this measure in this heuristic function."
        ],
        [
            "So there are several heuristic functions that you can use here.",
            "The simplest one you can think of is maybe precision.",
            "That is, the percentage of positive examples that are covered by a rule.",
            "So the more positive examples you cover, the better their rules.",
            "So if you have 100% positive examples, that might be a very good rule.",
            "On the other hand, it is not because we have seen that rules that cover only a single example, and this is positive.",
            "Are not good rules, but they would have 100% position, so it better strategy is a simple small modification here that is called the LA plus heuristic.",
            "It just adds one to the positives and the negatives.",
            "So we start counting not zero, but we start counting at one both for the positives and for the negatives and then.",
            "This heuristic results in the limit.",
            "This is the same as this one if P and then go to Infinity, but for small values of P and then this makes a difference and this difference."
        ],
        [
            "Is important.",
            "So here you see what would happen on this training set.",
            "We have positive and we have negative examples.",
            "We have here evaluated by Precision Pilot Plus and the third measure that is even simpler.",
            "That just takes the difference between the cover of positives and the covered negatives.",
            "And here you see that if you have a rule that covers 4 positives and no negatives for precision and Laplace, this would be the best rule.",
            "But four P -- N for this weighted relative accuracy measure.",
            "I'll come back to that in a minute.",
            "This rule here that covers 6 positives and one negative would be better.",
            "So different choices in the heuristics will give you different rules."
        ],
        [
            "So it's an important thing to consider which heuristics to use, and one way of comparing heuristics is to look at them in this coverage space that I defined before.",
            "There you have something called isometrics.",
            "We have seen one for accuracy already in the first slide, but maybe it become in the first slide where I showed the coverage space.",
            "Maybe it becomes a little bit clearer on a simpler example, imagine you have a heuristic that just counts the positive examples.",
            "It completely ignores whether recover any negative examples.",
            "Then these would be the isometrics of these heuristics.",
            "This essentially means.",
            "All rules on these red lines are equally evaluated, so here we cover one negative, one positive example and no negatives, and this gets the same value as this rule here where we cover 1 positive example and and all negatives so they all have the equal values.",
            "Of course that's not a good heuristic here because this point is maybe a fairly good point.",
            "Not very good, but it's certainly better than this point where we cover all of the negative examples.",
            "Another heuristic might be excluding all negative, so that would be vertical is a metrics here, so here we only cover one negative example and here also one negative example and this rule is of course much better than this one.",
            "But with this heuristic they would get the same.",
            "Well, you OK."
        ],
        [
            "Not good heuristics, but how do these is a metrics look in?",
            "In other cases, so we will see a couple of examples.",
            "After this slide here.",
            "So this slide simply explains again the top down strategy we start at the top at the rule that covers all examples.",
            "The universal rule that covers all that has the condition true and then we successively at one condition at a time.",
            "And for adding one conditions, we try out all possible conditions and we choose the best one according to whatever heuristic we have selected.",
            "So."
        ],
        [
            "So now let's look at some of these heuristics.",
            "For example, position.",
            "As we have seen, it's maybe not a good measure, but how do these metrics look like?",
            "They look like like they rotate in this space.",
            "So here is the origin.",
            "If you have all values zero then this value is not defined and on these lines are all the rules that I have the same precision value.",
            "So here for example this is not red, but they all also all have the value.",
            "100% position here.",
            "They all have.",
            "Zero percent precision, and here they all have 50% precision.",
            "So we see why this is not bad, because why this is a bad heuristic because all values on this axis are all rules on this axis are the same, but we actually want to prefer this one over these down here.",
            "So."
        ],
        [
            "Accuracy we have seen on these slides P -- N that's actually the same as this def definition of accuracy.",
            "Accuracy.",
            "Here is how many percent of the examples do you get right?",
            "That's this formula essentially, and because capital P and capital N are constants, you can essentially reduce it to this.",
            "Here.",
            "This is of course a completely different values, but value, but it gives you the same ordering of the rules.",
            "So P -- 1 is equivalent to.",
            "Accuracy when it comes to ordering rules that has access parallel heuristics here.",
            "So the higher the closer we get to this point, the higher the value on these isometrics and down here they are lower.",
            "So I forgot to mention maybe one a good way of thinking about this user metrics if you if you think about these altitude lines in your Maps, you see you have lines that compare that connect points with the same altitude in Maps and this is essentially the same, but we do not have Hills, but we have heuristic values for rules and these are the altitude lines.",
            "So that's for accuracy."
        ],
        [
            "Weighted relative accuracy is a variant of that and that.",
            "Tilts these parallel lines a little bit so that they are parallel to this diagonal.",
            "The reason is, as I said in the beginning, that there agonal are the points that you can get without any information.",
            "That is what you get if you randomly guess you will be somewhere on this diagonal.",
            "Depending on how many of on how large the percentage of positive examples is that you get the more positive examples.",
            "You guess the higher you will be up here, the smaller the fewer positive examples you will guess today.",
            "The more you will be near this point and this measure gives the value of 0 to all points of the diagonal and all other isometrics are parallel to this."
        ],
        [
            "Then there are also examples of of non linear or or measures that have nonlinear isometrics, one that performs fairly well is correlation and, and this is the structure that you see here.",
            "It's again pair of diagonal is diverse point, and then they tilt towards this.",
            "This point where you actually want to.",
            "Get to."
        ],
        [
            "OK, I don't want to go into too much detail here.",
            "There are many more measures that you can compare.",
            "But correlation, for example, is one that performs fairly well in practice.",
            "I also mentioned that I want to go to talk a little bit more about overfitting.",
            "Overfitting is a very fundamental concept in machine learning, that is something that occurs in every machine learning algorithm and you somehow have to respect this phenomenon phenomenon.",
            "The phenomenon is simply that you do not want to fit the data perfectly.",
            "You want to make some mistakes.",
            "Or you want to allow for some mistakes?",
            "If you can gain simplicity in the learned concepts by making these mistakes."
        ],
        [
            "I'll show you this on any example, so here you have maybe five data points.",
            "Let's say these are your stock.",
            "Uh.",
            "I don't know you you bought the stock and these are the stock market prices in the last five days and you want to predict how will it be on the next day.",
            "And then you can take a simple strategy and say, OK, I fit a curve perfectly through these.",
            "Examples here.",
            "So here you have a perfect fit on these five points.",
            "Everything in the past has been explained with 100% precision.",
            "And then you make a prediction.",
            "For this value, this is tomorrow, and the prediction would be here.",
            "Now if you think about it.",
            "Actually their stocks have been increasing day by day and then you predict a very sudden drop.",
            "Doesn't seem right somehow, although you have explained all of the points in the past perfectly.",
            "A better prediction might be up here.",
            "You predict another increase because it has increased five times in the past.",
            "And that you would get if you do not use such a jagged curve here, which is a polynomial of degree 4.",
            "But you use a simple one polynomial of degree 1A linear function.",
            "So here you make mistakes on every one of the points in that in the past.",
            "And you probably will not get this one right either.",
            "But it is quite likely that the true value here will be closer to this value.",
            "Then it will be do this value.",
            "So that's a illustration of.",
            "Uh.",
            "Overfitting.",
            "And overfitting happens if you take a two complex function to fit your data points, you can always find if you have five points, you can always find a polynomial of degree four that fits these five points perfectly.",
            "Or in general, if you have endpoints, take a polynomial of N -- 1.",
            "And you will fit these endpoints perfectly.",
            "But we usually want to restrict the parameters and use simpler polynomials that will not fit perfectly, but they will be much better able to capture the trend."
        ],
        [
            "And the same hat."
        ],
        [
            "Comes with rule learning.",
            "There we have the case.",
            "What is the complexity of the rules?",
            "It's just a number of rules and the number of conditions in their rules.",
            "So here we have an example where we want to.",
            "Cover the positive examples with rules.",
            "Again, rules are rectangles, and if you think about it for a minute, we have at least we need 4 rectangles here to cover this exactly.",
            "But if we allow for mistakes, as I said in the beginning, is a good idea.",
            "We might be able to cover it with a single rule."
        ],
        [
            "So there are several methods for trying to.",
            "Implement this idea of finding simpler rules that do not fit the data perfectly well, and there are summarized, usually under the concept of pruning.",
            "Pre pruning happens during learning post pruning happens after learning and then there are some."
        ],
        [
            "Intermediate concepts pre pruning is you learn a rule and then you make a decision here that this rule is already complex enough you do not cover.",
            "You still cover a few negative examples, but nevertheless you stop learning these rule.",
            "This rule here.",
            "So you decide when to add rules during to add conditions.",
            "You decide when to stop adding conditions during learning.",
            "Earth.",
            "And you also.",
            "Decide when to stop adding rules during learning."
        ],
        [
            "Post pruning is another idea where you first learn a very complex ruleset and then you try to simplify it.",
            "So here you have a very big data ruleset that explains all of the examples perfectly well, and then you try to simplify it and you can come up with a simple rule you can."
        ],
        [
            "Also illustrate this on this example.",
            "Here we have rules again for this one data set, we start with a very complex theory.",
            "One rule for for every example, and then we can see that they have common conditions here and we remove the remaining conditions and the remaining rules."
        ],
        [
            "And would be with would get a very simple set by postponing.",
            "That is essentially the idea of post pruning.",
            "Uh.",
            "With the edit thing that here we did not lose any accuracy because both rulesets were 100% accurate.",
            "In postponing, you allowed that you you have some criteria that would allow you to reduce the accuracy on the training examples.",
            "For example by computing the accuracy on the.",
            "On this set, a new validation set that you have previously put."
        ],
        [
            "Aside incremental reduced error pruning is then another strategy that combines pre and post pruning and this is then the idea that you learn one rule.",
            "Perfectly well, and then you simplify this rule after immediately after you have learned it.",
            "So in a way you have implemented pre pruning here by using a post pruning criterion.",
            "And this turns out to be much more efficient and much more accurate than any of these two.",
            "So the final thing we only have talked about binary concept so far.",
            "We want to discriminate positive from negative examples.",
            "Uh.",
            "What if we have multiple classes like red, green, blue, yellow as we had in the beginning?"
        ],
        [
            "We have an example where we have six classes, circles, pluses, access.",
            "Till this hashes and we want to discriminate them from each other.",
            "The solution that you can hear use here for rule learning is there are several solutions but one year solution is that you reduce them to binary problems.",
            "Process called class binary."
        ],
        [
            "Station and the classical way of doing that is to use one against all binarization.",
            "That means you take a single class.",
            "These are the positive examples for this class.",
            "In this case, they are the pluses.",
            "That's just a coincidence.",
            "These are the positives.",
            "All other examples are labeled as negative, and then you learn.",
            "One predictor that recognizes the pluses.",
            "Or here for the circles you want to label all circles as positive.",
            "All other examples as negative.",
            "And then you learn a single rule set for recognizing circles.",
            "So then you have six rule sets, one for circles, one 4 + 1 for access and so on, and then you can infer prediction for a multiclass problem by just trying all of them.",
            "And ideally one will give you an answer and all others will say this is negative.",
            "Then you're fine.",
            "In more complex cases you have something like.",
            "Two of them will say they're minds, or maybe the circle will say this is a circle and the plus will say this is a plus.",
            "Then you have to do some sort of type breaking, for example by using a certainty measure how certain is the ruling its prediction."
        ],
        [
            "Another strategy, and this performs a little bit better in practice, is pairwise classification.",
            "Here you have to say, my dear, but you do not learn one theorie for one.",
            "For each class, but you learn one theory for each pair of classes, so you have one theory that separates the hashes.",
            "From the pluses you have another one that separates the axis from the circles.",
            "And one of the reasons why this performs well is because this is usually simpler.",
            "The concepts that you have to learn for pairs of classes are usually simpler then the concept.",
            "You have to learn to separate one class from all other classes.",
            "The downside is you have to learn a quadratic number of classifiers instead of a linear one, but actually turns out that this is not slower than learning to the linear number because you have more classifiers, but each one of them is trained on fewer examples.",
            "So in total this is actually even faster and."
        ],
        [
            "You can prove that, and it's also more accurate, at least for rule learning.",
            "This is a set of experiments that demonstrate this.",
            "This is the standard strategy one against all, and these are the results for the pair by strategy.",
            "These are error rates.",
            "Quite unfortunate that you have data set here with an 80% error rate in the beginning, but there are others where you see that you can perform much better.",
            "And what is important is the comparison between these two values, so here.",
            "7% error and 3% error.",
            "This, and essentially this pairwise strategy brought an improvement in every."
        ],
        [
            "Face.",
            "OK, so I'm writing time.",
            "I think a little bit over 30 minutes, but still within the set boundary.",
            "The summary is rules can be learned with top down Hill climbing, one condition at a time, and then you learn more rules to form a rule sets for choosing their.",
            "Select the conditions you use, heuristics, you can visualize them through his own metrics in coverage space.",
            "While choosing these heuristics, you have to consider that overfitting is a very important problem and this cannot only be addressed with.",
            "The choice of the heuristics, but also why are pruning pre pruning and post pruning and and they can be efficiently combined and finally multiclass problems can be addressed by reducing them to binary classification.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "As you said, one of the problems was that your book has not appeared previously is because it took us six years to write in.",
                    "label": 0
                },
                {
                    "sent": "That was the one of the problems, but we managed.",
                    "label": 0
                },
                {
                    "sent": "We do now in the 30 minutes or so is I'll try to give a very basic introduction into learning, and on the example of learning an introduction to machine learning in general to some of the fundamental concepts there.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one frequently or the most frequently studied scenario in machine learning is the induction of classifiers.",
                    "label": 1
                },
                {
                    "sent": "You can think of it as.",
                    "label": 0
                },
                {
                    "sent": "The problem where you have a couple of examples of some sort and they have some property attached to it.",
                    "label": 0
                },
                {
                    "sent": "Like here their color, their green bonds, red ones, yellow ones and they are used for training a thing called a classifier, and this classifier is able then to take a new example.",
                    "label": 0
                },
                {
                    "sent": "This has the color Gray, which stands for unknown here.",
                    "label": 0
                },
                {
                    "sent": "And determine this color.",
                    "label": 1
                },
                {
                    "sent": "So it automatically is able to assign categories to new objects based on the experience it has covered from previously categorized objects.",
                    "label": 1
                },
                {
                    "sent": "From these here, these are called the training set.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this setting of classification and even simpler setting is concept learning.",
                    "label": 1
                },
                {
                    "sent": "There you essentially have two classes, positive and negative, and you can think of them as examples for concept.",
                    "label": 0
                },
                {
                    "sent": "The positive examples that are examples for the concept and counter examples, negative examples that are examples.",
                    "label": 1
                },
                {
                    "sent": "That do not hold for this concept and.",
                    "label": 0
                },
                {
                    "sent": "The goal of concept learning is from these positive and negative examples to induce a description of this concept.",
                    "label": 0
                },
                {
                    "sent": "So a description that this ideally also understandable but also computable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "We also called this concept in a hypothesis and what we want to find is the target hypothesis and the hypothesis that are considered in the in the process are candidate hypothesis and in the end one of these candidates should turn out to be the target.",
                    "label": 0
                },
                {
                    "sent": "In the ideal situation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to do that with rules, so our concept should be rules.",
                    "label": 0
                },
                {
                    "sent": "There are many other concept descriptions in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Decision trees, neural network, support vector machines.",
                    "label": 0
                },
                {
                    "sent": "We want him to be rules.",
                    "label": 0
                },
                {
                    "sent": "What is a rule look like?",
                    "label": 0
                },
                {
                    "sent": "So here's a simple conjunctive rule.",
                    "label": 0
                },
                {
                    "sent": "It has a body that is the if part.",
                    "label": 0
                },
                {
                    "sent": "It describes these objects this that if part of the rule refers to some categories, some descriptions that we know about these objects.",
                    "label": 0
                },
                {
                    "sent": "And the head of the rule that is the conclusion that would be the color, or in our case positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So the body characterizes the object and the head contains the classification.",
                    "label": 0
                },
                {
                    "sent": "We also say that a rule covers an object or an example.",
                    "label": 0
                },
                {
                    "sent": "If this example fulfills these conditions of the rule.",
                    "label": 1
                },
                {
                    "sent": "And the prediction then is if a rule covers an example, then the conclusion, positive or negative, is predicted for this example.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a very simple and classical example task in machine learning you have.",
                    "label": 0
                },
                {
                    "sent": "You observe your neighbor, let's say, and you observe the weather conditions and you want to know whether your neighbor goes out or not.",
                    "label": 0
                },
                {
                    "sent": "You want to know whether he is home tomorrow or not for some reason.",
                    "label": 0
                },
                {
                    "sent": "So you observe for two weeks the weather conditions.",
                    "label": 0
                },
                {
                    "sent": "For example, if it's hot out and the outlook is sunny and it is humid and it's not windy, then he doesn't go out.",
                    "label": 0
                },
                {
                    "sent": "He doesn't go.",
                    "label": 0
                },
                {
                    "sent": "Usually, he only goes out for playing golf, so he doesn't go play.",
                    "label": 0
                },
                {
                    "sent": "Golf in these conditions, or this was two weeks ago yesterday.",
                    "label": 0
                },
                {
                    "sent": "Temperature was mild.",
                    "label": 0
                },
                {
                    "sent": "It was rainy.",
                    "label": 0
                },
                {
                    "sent": "It was humid and it was not windy and there he did go out.",
                    "label": 0
                },
                {
                    "sent": "So you have some observations and the goal is now you want to find a rule that lets you predict whether he goes out tomorrow or not.",
                    "label": 0
                },
                {
                    "sent": "So if you get the weather forecast for tomorrow and you have your rule, you can predict whether he goes out or not.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a very simple rule based solution that at least explains these training examples and simple rule based solution is you just turn every one of these positive examples.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have 3456789 examples where we predict that he goes out.",
                    "label": 0
                },
                {
                    "sent": "This is the positive class.",
                    "label": 0
                },
                {
                    "sent": "This is the one we want to learn a description for.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can turn every one of these into a rule.",
                    "label": 0
                },
                {
                    "sent": "So this rule set will explain your training examples perfectly well, but of course this is not a good solution.",
                    "label": 0
                },
                {
                    "sent": "You haven't learned anything, you have just repeated what you have observed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A much better solution would be a very small and compact.",
                    "label": 0
                },
                {
                    "sent": "A simple rule set like this one.",
                    "label": 0
                },
                {
                    "sent": "There you have only three rules and T3 rules explained.",
                    "label": 0
                },
                {
                    "sent": "The 14 examples for all of the nine cases where you predict yes one of these rules predict yes, and for all of the five cases where we want to predict no, none of these rules predicts yes, so that's a much better so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lucien, how can we find such a solution?",
                    "label": 0
                },
                {
                    "sent": "Very classical strategy is called separate and conquer rule learning or covering rule learning.",
                    "label": 1
                },
                {
                    "sent": "The idea here is that we learn these rules one rule at the time and.",
                    "label": 0
                },
                {
                    "sent": "Every time we have learned one rule that explains parts of the examples.",
                    "label": 0
                },
                {
                    "sent": "We remove the examples that are explained by this rule and then we have a smaller problem where there are fewer examples remaining and these are then later covered by the neck by the next iteration of this loop.",
                    "label": 0
                },
                {
                    "sent": "So we learn another rule that explains more examples and the one that's a very old strategy, but it is still in Rule learning frequently.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used so far we have said that we want to find a rule that explains the positive examples and it doesn't cover any of the negative examples.",
                    "label": 1
                },
                {
                    "sent": "That is our goal.",
                    "label": 0
                },
                {
                    "sent": "We want to have rules that predict all yeses and don't predict any of the nose.",
                    "label": 0
                },
                {
                    "sent": "Actually, it turns out that it is usually quite good to relax these constraints a little bit.",
                    "label": 0
                },
                {
                    "sent": "We do not want to be super precise, because this is also part of a phenomenon that is called overfitting and I will come to two will come back to that a little bit later in this talk.",
                    "label": 1
                },
                {
                    "sent": "So for the moment the short example, if we have a training set that has 200 examples, hundred of them are positive, 100 or negative and we find.",
                    "label": 0
                },
                {
                    "sent": "One rule set like the first one I showed you that has 100 rules, each one covering one of the positive examples that explains these training data perfectly well.",
                    "label": 1
                },
                {
                    "sent": "And then we have another theorie that consists of a single rule and this single rule covers 99 of the positive examples, so it only misses one.",
                    "label": 0
                },
                {
                    "sent": "And it also covers one of the negative examples, so it makes one false prediction.",
                    "label": 1
                },
                {
                    "sent": "Still, people would say that this theory beat single rule that covers that explains all of the examples with two mistakes is better than this very complex theory where you explain all of the examples but make.",
                    "label": 0
                },
                {
                    "sent": "But have one rule for each example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphically, this separate and conquer strategy can be imagined in in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here we have maybe two numerical descriptors and X value and the Y value and hear the pluses and minuses indicate very an example of this class lies in this 2 dimensional space, so this is the data set.",
                    "label": 0
                },
                {
                    "sent": "Then we would find one rule that explains part of the example space.",
                    "label": 0
                },
                {
                    "sent": "This is a rule because you can describe rules with intervals and intervals, so the rule would be something like if X is smaller than equal than a value here and greater than equal, then the value here and the same for Y.",
                    "label": 0
                },
                {
                    "sent": "Then you have two intervals and you have.",
                    "label": 0
                },
                {
                    "sent": "Defined a rule that describes such a rectangle here.",
                    "label": 0
                },
                {
                    "sent": "So here is a rule, and then we remove all these examples and.",
                    "label": 0
                },
                {
                    "sent": "Is the first rule, then we have a second one.",
                    "label": 0
                },
                {
                    "sent": "If you remove all the examples and found this second another rule and.",
                    "label": 0
                },
                {
                    "sent": "This is the third rule that has to be found then in the end.",
                    "label": 0
                },
                {
                    "sent": "See that this rule, for example, would cover two of the negative examples, but as we just explained.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is not always a problem.",
                    "label": 0
                },
                {
                    "sent": "Some of terminology we have.",
                    "label": 0
                },
                {
                    "sent": "Training examples positive and negative training examples.",
                    "label": 1
                },
                {
                    "sent": "We I denote them with capital P and capital N. We have examples covered by the rule.",
                    "label": 1
                },
                {
                    "sent": "Small P are the true positives.",
                    "label": 1
                },
                {
                    "sent": "Those are the ones that are covered and should be covered and small N are the examples that are covered by the rule but should not be covered.",
                    "label": 1
                },
                {
                    "sent": "These are negative examples covered by a rule that predicts positive and the others are the negative.",
                    "label": 0
                },
                {
                    "sent": "The false negatives into true negatives.",
                    "label": 1
                },
                {
                    "sent": "These are true negatives or negative examples that should be should not be covered, and false negatives are positive examples.",
                    "label": 0
                },
                {
                    "sent": "That should not.",
                    "label": 0
                },
                {
                    "sent": "That should actually be covered by a rule, but or not.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is maybe a little bit clearer if we try to visualize this in a space which we call the coverage space in this coverage space.",
                    "label": 0
                },
                {
                    "sent": "We have two axis, these are the negative examples.",
                    "label": 1
                },
                {
                    "sent": "These are the positive examples.",
                    "label": 0
                },
                {
                    "sent": "And here is the point where we don't cover any examples, so this would be an empty theory theory that is always false that never predicts positive would be here.",
                    "label": 0
                },
                {
                    "sent": "Up here is a universal theory that would be the theory that always predicts positive no matter what the example is.",
                    "label": 0
                },
                {
                    "sent": "It always says this is a positive example.",
                    "label": 0
                },
                {
                    "sent": "And up here, this is where we actually want to get to.",
                    "label": 0
                },
                {
                    "sent": "This is the perfect theory.",
                    "label": 1
                },
                {
                    "sent": "Here we would cover all of the positive examples and none of the negative examples, so the negative 0 positive, so it's maximum possible value.",
                    "label": 1
                },
                {
                    "sent": "And down here is a theory that might be on first sight considered perfectly bad, so it does everything wrong.",
                    "label": 1
                },
                {
                    "sent": "But if you start to think of it, that's actually not a bad theory because you have a theory that always predicts wrong.",
                    "label": 0
                },
                {
                    "sent": "You can just invert its predictions and you have a theory that always is right, so there really bad theories are those that are here on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "They essentially only make random predictions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are lines that.",
                    "label": 0
                },
                {
                    "sent": "Have the same accuracy, so all the rules on this line here.",
                    "label": 1
                },
                {
                    "sent": "So he ruled that covers one exam positive and one negative, or hear any rule that covers 5 positive and five negative.",
                    "label": 0
                },
                {
                    "sent": "See if you evaluate them with accuracy, they all.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Produce the same result so they are all equally good.",
                    "label": 0
                },
                {
                    "sent": "The covering strategy that I just showed you before the separate and conquer strategy moves through this space by starting at an empty theory.",
                    "label": 1
                },
                {
                    "sent": "Here in the beginning it doesn't have a rule, then it adds the first rule and it moves up in this space to appoint here covering some.",
                    "label": 0
                },
                {
                    "sent": "Negatives, but many positives.",
                    "label": 0
                },
                {
                    "sent": "Then it adds a second rule it.",
                    "label": 0
                },
                {
                    "sent": "Then it covers more positives and maybe also a little bit more negatives and so on until it is at the end at the rule where it cover at the ruleset where it covers all examples and one of these points should be the final rule set then.",
                    "label": 0
                },
                {
                    "sent": "So probably we will don't want to choose this one here where we cover all examples, but maybe this one could be the best strategy that depends only a little bit on how you.",
                    "label": 0
                },
                {
                    "sent": "Evaluate.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is, after you remove after you learn one rule, you remove all examples that are covered by this rule and you have another coverage space that.",
                    "label": 1
                },
                {
                    "sent": "Corresponds to the.",
                    "label": 0
                },
                {
                    "sent": "To the new problem where you have fewer examples, you have removed all the positive and negative examples that are covered and then in this new problem you want to find the next rule.",
                    "label": 0
                },
                {
                    "sent": "You have found the next rule.",
                    "label": 0
                },
                {
                    "sent": "Then you have an even smaller problem and so on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How are these rules found now?",
                    "label": 0
                },
                {
                    "sent": "A simple strategy for finding these rules is top down Hill climbing sounds a little bit counter intuitive if you want to climb a Hill, but you start from the top.",
                    "label": 0
                },
                {
                    "sent": "Did their explanation for that is that these are actually two different concepts.",
                    "label": 0
                },
                {
                    "sent": "Top down means that you start from the very general concept and you specialize it by adding conditions.",
                    "label": 0
                },
                {
                    "sent": "This means that we start here with a rule that explains everything, and then we successively add conditions.",
                    "label": 0
                },
                {
                    "sent": "So we essentially go in the other direction as the covering strategy does for finding a single rule for finding a single rule, we go down and for and for going up again.",
                    "label": 0
                },
                {
                    "sent": "We add other rules.",
                    "label": 0
                },
                {
                    "sent": "So this is the top down Barton, the Hill climbing parties that for each of these steps we evaluate all candidates with some sort of measure.",
                    "label": 0
                },
                {
                    "sent": "And we take the one that gives you the highest increase in this measure in this heuristic function.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several heuristic functions that you can use here.",
                    "label": 0
                },
                {
                    "sent": "The simplest one you can think of is maybe precision.",
                    "label": 0
                },
                {
                    "sent": "That is, the percentage of positive examples that are covered by a rule.",
                    "label": 1
                },
                {
                    "sent": "So the more positive examples you cover, the better their rules.",
                    "label": 0
                },
                {
                    "sent": "So if you have 100% positive examples, that might be a very good rule.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, it is not because we have seen that rules that cover only a single example, and this is positive.",
                    "label": 0
                },
                {
                    "sent": "Are not good rules, but they would have 100% position, so it better strategy is a simple small modification here that is called the LA plus heuristic.",
                    "label": 0
                },
                {
                    "sent": "It just adds one to the positives and the negatives.",
                    "label": 0
                },
                {
                    "sent": "So we start counting not zero, but we start counting at one both for the positives and for the negatives and then.",
                    "label": 0
                },
                {
                    "sent": "This heuristic results in the limit.",
                    "label": 0
                },
                {
                    "sent": "This is the same as this one if P and then go to Infinity, but for small values of P and then this makes a difference and this difference.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is important.",
                    "label": 0
                },
                {
                    "sent": "So here you see what would happen on this training set.",
                    "label": 0
                },
                {
                    "sent": "We have positive and we have negative examples.",
                    "label": 0
                },
                {
                    "sent": "We have here evaluated by Precision Pilot Plus and the third measure that is even simpler.",
                    "label": 0
                },
                {
                    "sent": "That just takes the difference between the cover of positives and the covered negatives.",
                    "label": 0
                },
                {
                    "sent": "And here you see that if you have a rule that covers 4 positives and no negatives for precision and Laplace, this would be the best rule.",
                    "label": 0
                },
                {
                    "sent": "But four P -- N for this weighted relative accuracy measure.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that in a minute.",
                    "label": 0
                },
                {
                    "sent": "This rule here that covers 6 positives and one negative would be better.",
                    "label": 0
                },
                {
                    "sent": "So different choices in the heuristics will give you different rules.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's an important thing to consider which heuristics to use, and one way of comparing heuristics is to look at them in this coverage space that I defined before.",
                    "label": 0
                },
                {
                    "sent": "There you have something called isometrics.",
                    "label": 0
                },
                {
                    "sent": "We have seen one for accuracy already in the first slide, but maybe it become in the first slide where I showed the coverage space.",
                    "label": 0
                },
                {
                    "sent": "Maybe it becomes a little bit clearer on a simpler example, imagine you have a heuristic that just counts the positive examples.",
                    "label": 0
                },
                {
                    "sent": "It completely ignores whether recover any negative examples.",
                    "label": 0
                },
                {
                    "sent": "Then these would be the isometrics of these heuristics.",
                    "label": 0
                },
                {
                    "sent": "This essentially means.",
                    "label": 0
                },
                {
                    "sent": "All rules on these red lines are equally evaluated, so here we cover one negative, one positive example and no negatives, and this gets the same value as this rule here where we cover 1 positive example and and all negatives so they all have the equal values.",
                    "label": 0
                },
                {
                    "sent": "Of course that's not a good heuristic here because this point is maybe a fairly good point.",
                    "label": 0
                },
                {
                    "sent": "Not very good, but it's certainly better than this point where we cover all of the negative examples.",
                    "label": 0
                },
                {
                    "sent": "Another heuristic might be excluding all negative, so that would be vertical is a metrics here, so here we only cover one negative example and here also one negative example and this rule is of course much better than this one.",
                    "label": 0
                },
                {
                    "sent": "But with this heuristic they would get the same.",
                    "label": 0
                },
                {
                    "sent": "Well, you OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not good heuristics, but how do these is a metrics look in?",
                    "label": 0
                },
                {
                    "sent": "In other cases, so we will see a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "After this slide here.",
                    "label": 0
                },
                {
                    "sent": "So this slide simply explains again the top down strategy we start at the top at the rule that covers all examples.",
                    "label": 0
                },
                {
                    "sent": "The universal rule that covers all that has the condition true and then we successively at one condition at a time.",
                    "label": 1
                },
                {
                    "sent": "And for adding one conditions, we try out all possible conditions and we choose the best one according to whatever heuristic we have selected.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look at some of these heuristics.",
                    "label": 0
                },
                {
                    "sent": "For example, position.",
                    "label": 0
                },
                {
                    "sent": "As we have seen, it's maybe not a good measure, but how do these metrics look like?",
                    "label": 0
                },
                {
                    "sent": "They look like like they rotate in this space.",
                    "label": 0
                },
                {
                    "sent": "So here is the origin.",
                    "label": 0
                },
                {
                    "sent": "If you have all values zero then this value is not defined and on these lines are all the rules that I have the same precision value.",
                    "label": 0
                },
                {
                    "sent": "So here for example this is not red, but they all also all have the value.",
                    "label": 0
                },
                {
                    "sent": "100% position here.",
                    "label": 0
                },
                {
                    "sent": "They all have.",
                    "label": 0
                },
                {
                    "sent": "Zero percent precision, and here they all have 50% precision.",
                    "label": 0
                },
                {
                    "sent": "So we see why this is not bad, because why this is a bad heuristic because all values on this axis are all rules on this axis are the same, but we actually want to prefer this one over these down here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy we have seen on these slides P -- N that's actually the same as this def definition of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "Here is how many percent of the examples do you get right?",
                    "label": 0
                },
                {
                    "sent": "That's this formula essentially, and because capital P and capital N are constants, you can essentially reduce it to this.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is of course a completely different values, but value, but it gives you the same ordering of the rules.",
                    "label": 0
                },
                {
                    "sent": "So P -- 1 is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Accuracy when it comes to ordering rules that has access parallel heuristics here.",
                    "label": 0
                },
                {
                    "sent": "So the higher the closer we get to this point, the higher the value on these isometrics and down here they are lower.",
                    "label": 0
                },
                {
                    "sent": "So I forgot to mention maybe one a good way of thinking about this user metrics if you if you think about these altitude lines in your Maps, you see you have lines that compare that connect points with the same altitude in Maps and this is essentially the same, but we do not have Hills, but we have heuristic values for rules and these are the altitude lines.",
                    "label": 0
                },
                {
                    "sent": "So that's for accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weighted relative accuracy is a variant of that and that.",
                    "label": 1
                },
                {
                    "sent": "Tilts these parallel lines a little bit so that they are parallel to this diagonal.",
                    "label": 0
                },
                {
                    "sent": "The reason is, as I said in the beginning, that there agonal are the points that you can get without any information.",
                    "label": 0
                },
                {
                    "sent": "That is what you get if you randomly guess you will be somewhere on this diagonal.",
                    "label": 0
                },
                {
                    "sent": "Depending on how many of on how large the percentage of positive examples is that you get the more positive examples.",
                    "label": 0
                },
                {
                    "sent": "You guess the higher you will be up here, the smaller the fewer positive examples you will guess today.",
                    "label": 0
                },
                {
                    "sent": "The more you will be near this point and this measure gives the value of 0 to all points of the diagonal and all other isometrics are parallel to this.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then there are also examples of of non linear or or measures that have nonlinear isometrics, one that performs fairly well is correlation and, and this is the structure that you see here.",
                    "label": 0
                },
                {
                    "sent": "It's again pair of diagonal is diverse point, and then they tilt towards this.",
                    "label": 0
                },
                {
                    "sent": "This point where you actually want to.",
                    "label": 0
                },
                {
                    "sent": "Get to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I don't want to go into too much detail here.",
                    "label": 0
                },
                {
                    "sent": "There are many more measures that you can compare.",
                    "label": 0
                },
                {
                    "sent": "But correlation, for example, is one that performs fairly well in practice.",
                    "label": 1
                },
                {
                    "sent": "I also mentioned that I want to go to talk a little bit more about overfitting.",
                    "label": 0
                },
                {
                    "sent": "Overfitting is a very fundamental concept in machine learning, that is something that occurs in every machine learning algorithm and you somehow have to respect this phenomenon phenomenon.",
                    "label": 0
                },
                {
                    "sent": "The phenomenon is simply that you do not want to fit the data perfectly.",
                    "label": 1
                },
                {
                    "sent": "You want to make some mistakes.",
                    "label": 0
                },
                {
                    "sent": "Or you want to allow for some mistakes?",
                    "label": 0
                },
                {
                    "sent": "If you can gain simplicity in the learned concepts by making these mistakes.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll show you this on any example, so here you have maybe five data points.",
                    "label": 0
                },
                {
                    "sent": "Let's say these are your stock.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "I don't know you you bought the stock and these are the stock market prices in the last five days and you want to predict how will it be on the next day.",
                    "label": 0
                },
                {
                    "sent": "And then you can take a simple strategy and say, OK, I fit a curve perfectly through these.",
                    "label": 0
                },
                {
                    "sent": "Examples here.",
                    "label": 0
                },
                {
                    "sent": "So here you have a perfect fit on these five points.",
                    "label": 0
                },
                {
                    "sent": "Everything in the past has been explained with 100% precision.",
                    "label": 0
                },
                {
                    "sent": "And then you make a prediction.",
                    "label": 0
                },
                {
                    "sent": "For this value, this is tomorrow, and the prediction would be here.",
                    "label": 1
                },
                {
                    "sent": "Now if you think about it.",
                    "label": 0
                },
                {
                    "sent": "Actually their stocks have been increasing day by day and then you predict a very sudden drop.",
                    "label": 0
                },
                {
                    "sent": "Doesn't seem right somehow, although you have explained all of the points in the past perfectly.",
                    "label": 0
                },
                {
                    "sent": "A better prediction might be up here.",
                    "label": 0
                },
                {
                    "sent": "You predict another increase because it has increased five times in the past.",
                    "label": 0
                },
                {
                    "sent": "And that you would get if you do not use such a jagged curve here, which is a polynomial of degree 4.",
                    "label": 0
                },
                {
                    "sent": "But you use a simple one polynomial of degree 1A linear function.",
                    "label": 0
                },
                {
                    "sent": "So here you make mistakes on every one of the points in that in the past.",
                    "label": 0
                },
                {
                    "sent": "And you probably will not get this one right either.",
                    "label": 0
                },
                {
                    "sent": "But it is quite likely that the true value here will be closer to this value.",
                    "label": 0
                },
                {
                    "sent": "Then it will be do this value.",
                    "label": 0
                },
                {
                    "sent": "So that's a illustration of.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Overfitting.",
                    "label": 0
                },
                {
                    "sent": "And overfitting happens if you take a two complex function to fit your data points, you can always find if you have five points, you can always find a polynomial of degree four that fits these five points perfectly.",
                    "label": 0
                },
                {
                    "sent": "Or in general, if you have endpoints, take a polynomial of N -- 1.",
                    "label": 0
                },
                {
                    "sent": "And you will fit these endpoints perfectly.",
                    "label": 0
                },
                {
                    "sent": "But we usually want to restrict the parameters and use simpler polynomials that will not fit perfectly, but they will be much better able to capture the trend.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the same hat.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comes with rule learning.",
                    "label": 0
                },
                {
                    "sent": "There we have the case.",
                    "label": 0
                },
                {
                    "sent": "What is the complexity of the rules?",
                    "label": 0
                },
                {
                    "sent": "It's just a number of rules and the number of conditions in their rules.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example where we want to.",
                    "label": 0
                },
                {
                    "sent": "Cover the positive examples with rules.",
                    "label": 1
                },
                {
                    "sent": "Again, rules are rectangles, and if you think about it for a minute, we have at least we need 4 rectangles here to cover this exactly.",
                    "label": 0
                },
                {
                    "sent": "But if we allow for mistakes, as I said in the beginning, is a good idea.",
                    "label": 0
                },
                {
                    "sent": "We might be able to cover it with a single rule.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several methods for trying to.",
                    "label": 0
                },
                {
                    "sent": "Implement this idea of finding simpler rules that do not fit the data perfectly well, and there are summarized, usually under the concept of pruning.",
                    "label": 1
                },
                {
                    "sent": "Pre pruning happens during learning post pruning happens after learning and then there are some.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intermediate concepts pre pruning is you learn a rule and then you make a decision here that this rule is already complex enough you do not cover.",
                    "label": 0
                },
                {
                    "sent": "You still cover a few negative examples, but nevertheless you stop learning these rule.",
                    "label": 0
                },
                {
                    "sent": "This rule here.",
                    "label": 0
                },
                {
                    "sent": "So you decide when to add rules during to add conditions.",
                    "label": 0
                },
                {
                    "sent": "You decide when to stop adding conditions during learning.",
                    "label": 1
                },
                {
                    "sent": "Earth.",
                    "label": 0
                },
                {
                    "sent": "And you also.",
                    "label": 0
                },
                {
                    "sent": "Decide when to stop adding rules during learning.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Post pruning is another idea where you first learn a very complex ruleset and then you try to simplify it.",
                    "label": 0
                },
                {
                    "sent": "So here you have a very big data ruleset that explains all of the examples perfectly well, and then you try to simplify it and you can come up with a simple rule you can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also illustrate this on this example.",
                    "label": 0
                },
                {
                    "sent": "Here we have rules again for this one data set, we start with a very complex theory.",
                    "label": 0
                },
                {
                    "sent": "One rule for for every example, and then we can see that they have common conditions here and we remove the remaining conditions and the remaining rules.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And would be with would get a very simple set by postponing.",
                    "label": 0
                },
                {
                    "sent": "That is essentially the idea of post pruning.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "With the edit thing that here we did not lose any accuracy because both rulesets were 100% accurate.",
                    "label": 0
                },
                {
                    "sent": "In postponing, you allowed that you you have some criteria that would allow you to reduce the accuracy on the training examples.",
                    "label": 0
                },
                {
                    "sent": "For example by computing the accuracy on the.",
                    "label": 0
                },
                {
                    "sent": "On this set, a new validation set that you have previously put.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Aside incremental reduced error pruning is then another strategy that combines pre and post pruning and this is then the idea that you learn one rule.",
                    "label": 1
                },
                {
                    "sent": "Perfectly well, and then you simplify this rule after immediately after you have learned it.",
                    "label": 0
                },
                {
                    "sent": "So in a way you have implemented pre pruning here by using a post pruning criterion.",
                    "label": 0
                },
                {
                    "sent": "And this turns out to be much more efficient and much more accurate than any of these two.",
                    "label": 0
                },
                {
                    "sent": "So the final thing we only have talked about binary concept so far.",
                    "label": 0
                },
                {
                    "sent": "We want to discriminate positive from negative examples.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "What if we have multiple classes like red, green, blue, yellow as we had in the beginning?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have an example where we have six classes, circles, pluses, access.",
                    "label": 0
                },
                {
                    "sent": "Till this hashes and we want to discriminate them from each other.",
                    "label": 1
                },
                {
                    "sent": "The solution that you can hear use here for rule learning is there are several solutions but one year solution is that you reduce them to binary problems.",
                    "label": 0
                },
                {
                    "sent": "Process called class binary.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station and the classical way of doing that is to use one against all binarization.",
                    "label": 0
                },
                {
                    "sent": "That means you take a single class.",
                    "label": 0
                },
                {
                    "sent": "These are the positive examples for this class.",
                    "label": 0
                },
                {
                    "sent": "In this case, they are the pluses.",
                    "label": 0
                },
                {
                    "sent": "That's just a coincidence.",
                    "label": 0
                },
                {
                    "sent": "These are the positives.",
                    "label": 0
                },
                {
                    "sent": "All other examples are labeled as negative, and then you learn.",
                    "label": 0
                },
                {
                    "sent": "One predictor that recognizes the pluses.",
                    "label": 0
                },
                {
                    "sent": "Or here for the circles you want to label all circles as positive.",
                    "label": 0
                },
                {
                    "sent": "All other examples as negative.",
                    "label": 0
                },
                {
                    "sent": "And then you learn a single rule set for recognizing circles.",
                    "label": 0
                },
                {
                    "sent": "So then you have six rule sets, one for circles, one 4 + 1 for access and so on, and then you can infer prediction for a multiclass problem by just trying all of them.",
                    "label": 1
                },
                {
                    "sent": "And ideally one will give you an answer and all others will say this is negative.",
                    "label": 0
                },
                {
                    "sent": "Then you're fine.",
                    "label": 0
                },
                {
                    "sent": "In more complex cases you have something like.",
                    "label": 0
                },
                {
                    "sent": "Two of them will say they're minds, or maybe the circle will say this is a circle and the plus will say this is a plus.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do some sort of type breaking, for example by using a certainty measure how certain is the ruling its prediction.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another strategy, and this performs a little bit better in practice, is pairwise classification.",
                    "label": 1
                },
                {
                    "sent": "Here you have to say, my dear, but you do not learn one theorie for one.",
                    "label": 1
                },
                {
                    "sent": "For each class, but you learn one theory for each pair of classes, so you have one theory that separates the hashes.",
                    "label": 0
                },
                {
                    "sent": "From the pluses you have another one that separates the axis from the circles.",
                    "label": 0
                },
                {
                    "sent": "And one of the reasons why this performs well is because this is usually simpler.",
                    "label": 0
                },
                {
                    "sent": "The concepts that you have to learn for pairs of classes are usually simpler then the concept.",
                    "label": 0
                },
                {
                    "sent": "You have to learn to separate one class from all other classes.",
                    "label": 0
                },
                {
                    "sent": "The downside is you have to learn a quadratic number of classifiers instead of a linear one, but actually turns out that this is not slower than learning to the linear number because you have more classifiers, but each one of them is trained on fewer examples.",
                    "label": 0
                },
                {
                    "sent": "So in total this is actually even faster and.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can prove that, and it's also more accurate, at least for rule learning.",
                    "label": 0
                },
                {
                    "sent": "This is a set of experiments that demonstrate this.",
                    "label": 0
                },
                {
                    "sent": "This is the standard strategy one against all, and these are the results for the pair by strategy.",
                    "label": 0
                },
                {
                    "sent": "These are error rates.",
                    "label": 0
                },
                {
                    "sent": "Quite unfortunate that you have data set here with an 80% error rate in the beginning, but there are others where you see that you can perform much better.",
                    "label": 0
                },
                {
                    "sent": "And what is important is the comparison between these two values, so here.",
                    "label": 0
                },
                {
                    "sent": "7% error and 3% error.",
                    "label": 0
                },
                {
                    "sent": "This, and essentially this pairwise strategy brought an improvement in every.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Face.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm writing time.",
                    "label": 0
                },
                {
                    "sent": "I think a little bit over 30 minutes, but still within the set boundary.",
                    "label": 0
                },
                {
                    "sent": "The summary is rules can be learned with top down Hill climbing, one condition at a time, and then you learn more rules to form a rule sets for choosing their.",
                    "label": 1
                },
                {
                    "sent": "Select the conditions you use, heuristics, you can visualize them through his own metrics in coverage space.",
                    "label": 0
                },
                {
                    "sent": "While choosing these heuristics, you have to consider that overfitting is a very important problem and this cannot only be addressed with.",
                    "label": 1
                },
                {
                    "sent": "The choice of the heuristics, but also why are pruning pre pruning and post pruning and and they can be efficiently combined and finally multiclass problems can be addressed by reducing them to binary classification.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}