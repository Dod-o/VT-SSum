{
    "id": "hyijnvouunvdmeqplkamtdbeanrku6x7",
    "title": "Manifold Alignment using Procrustes Analysis",
    "info": {
        "author": [
            "Chang Wang, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_wang_map/",
    "segmentation": [
        [
            "Thanks, thanks for attending the talk.",
            "My name is Wang Chung.",
            "I come from University of Massachusetts.",
            "I will present the work on manifold alignment using procrustes analysis.",
            "Is the joint work with Sheridan Hardware?"
        ],
        [
            "You know, in real life we always need to do knowledge transfer.",
            "For example, if you come here from the US, you will realize that the."
        ],
        [
            "The box and the parking meter here are quite different from the US, but this is not a problem because we know how to transfer the knowledge learned from the US to fly.",
            "So this transfer is simple, is straightforward."
        ],
        [
            "But how about this?",
            "So you are very interested in the document returning English and they want to find the most relevant document written in Arabic.",
            "So this transfer is much more complex compared to the transfer shown in the previous slide.",
            "And this is a problem that we want to solve in the paper.",
            "An the problem is the transfer is between two seemingly quite different datasets.",
            "So we want to transform for such problem."
        ],
        [
            "There are some simple applications where the knowledge transfer is very useful.",
            "I just mentioned automatic machine translation.",
            "Anne Cross lingual information retrieval and in representation and control policy, transfer between two different MDP's are also using the knowledge transfer."
        ],
        [
            "Outline of my talk."
        ],
        [
            "First part background.",
            "The basic concept, but a manifold.",
            "So Manifold is an abstract mathematical space in which every point has a neighborhood which resembles Euclidean space, but in which the global structure is much more complicated.",
            "So one perfect example about manifold is the Earth.",
            "If we want to measure the surface distance between two locations, if the two locations are very close to each other, we just use Euclidean distance to measure the distance between them.",
            "However, if two locations are far away from each other, we have to consider the fact that the Earth is not flat, so the distance on the surface is much larger compared to the Euclidean distance between the two locations.",
            "And there are some other examples about the manifold.",
            "As state space in MDP process can be thought as a manifold, a collection of documents can be thought as a manifold.",
            "And a set of images collected by a robot can also be thought as a manifold.",
            "Manifolds are usually invited in high dimensional space.",
            "But their intrinsic dimensionality might be much lower because we have fewer degrees of freedom.",
            "And we can collect data points from the manifold.",
            "Something like running around walk so we."
        ],
        [
            "And collect data points.",
            "Manifold alignment approaches or line for manifolds using a set of landmark points.",
            "To my knowledge, the pioneer work in the field is called semi supervised manifold alignment.",
            "It Maps the points of two data sites through the same space by solving a constrained inviting problem where the embedding of the corresponding points from the different sets are constrained to be identical."
        ],
        [
            "OK our."
        ],
        [
            "Algorithm.",
            "Here we are given two websites, each of which comes from a manifold.",
            "And each side has two subsets, for example, as one has two subsets, one unlabeled and S1 labeled.",
            "And we are given some correspondence, so each point in as well labeled is corresponding to point in S2 label.",
            "So we want to learn something from the correspondence and we want to apply the knowledge we learned to the whole data set and the framework of the algorithm is here in the first step we map the two data size to lower dimensional spaces reflecting their intrinsic structure.",
            "And in the second step we do procrustes analysis to align the two low dimensional embedding based on the landmark points in the correspondence.",
            "And the procrustes analysis can remove the translational, rotational and scaling components so that we can get the optimal alignment."
        ],
        [
            "A little bit more details about the first step, so it's in fact a dimensionality reduction step.",
            "All the existing approaches on dimensionality reduction can be used here, like Isomap plus single map, PCA, LP.",
            "And here we have something to mention when we are using the 1st three approaches.",
            "We are computing the lower dimensional embedding directly from the input.",
            "We don't know what the mapping looks like, so when we want to extend the knowledge to some new examples we might run into problems because we don't know the mapping.",
            "For PCA and IOP.",
            "We can solve the problem becausw using these two approaches.",
            "We learn the mapping F and once the F is given, we can map the higher dimensional data to lower dimensional space.",
            "And F is defined everywhere, so without any problem, we can extend the mapping to new data points.",
            "At the bottom of the slide I put the method to use Laplacian Eigen map to do dimensionality reduction.",
            "It's just the regular approach so I will skip that.",
            "In the second step we do.",
            "But alignment as I just mentioned.",
            "This approach can remove the translational, rotational and scaling components from one side so that we can achieve optimal alignment between the two sides.",
            "And here assume the lower dimensional inviting of S1 labeled is X lower dimensional embedding of S2 labeled is Y. Anne, first we translate the configures of X&Y so that their suckers are at a region point.",
            "So here in the first step we do the translation.",
            "And then the second step we do.",
            "Singular value decomposition of the matrix while transpose times X.",
            "This decomposition return returns us with three matrices.",
            "Orthonormal matrix U of normal matrix V and diagonal matrix Sigma.",
            "And the optimal rotation kill will be just a few times we transpose.",
            "And the optimal rescale factor.",
            "Is very simple trace of Sigma over trace of Y transpose times Y.",
            "It's very clean results.",
            "And once we have K&Q, we can do the knowledge transfer.",
            "Here we want to align Y 2X.",
            "So we compute White Star which is equal to K * y * Q.",
            "And we can guarantee that this one will be the optimal result regarding the Frobenius norm.",
            "And Frobenius norm is very popularly used.",
            "The definition is also on the slide."
        ],
        [
            "And give you some rough idea about what's going on.",
            "Here in figure A and figure P will show to lower dimensional embeddings.",
            "And from the two figures, it seems like the two ships are quite different.",
            "And they also pull out the two structures on the same figure figure see.",
            "So you can see that.",
            "A is much larger than B.",
            "And we apply this approach.",
            "And we got the K which is equal to 4.3 for this case and the rotation matrix is also listed on the slide.",
            "We will play key on the Q2 structure B and we pull out the alignments in Figure D so you can see that the two structures are aligned very well."
        ],
        [
            "Some justifications I.",
            "Didn't put too much technical details in the slides, I just want to give you some high level explanation.",
            "So on the slides there is no proof in the paper."
        ],
        [
            "There is a lot.",
            "Uh.",
            "Yes, we can achieve optimal alignment.",
            "Yeah, that's the only sentence I want to mention.",
            "On this slide.",
            "Procrustes analysis achieves optimal alignment regarding for."
        ],
        [
            "Opinions now.",
            "The second theorem is about under what conditions the two manifolds can be similar.",
            "You know many state of the art approaches to dimensionality reduction or something like the following.",
            "We first compute a relationship matrix and then we compute the top eigenvectors.",
            "The top eigenvectors will be used as the lower dimensional embedding.",
            "Of the original data, so here using Laplace anger map as example, the relationship matrix is graph Laplacian and lower dimensional embedding will just be the smallest eigenvectors of the graph Laplacian matrix.",
            "Theorem poo use the difference between 2:00.",
            "Input relationship matrices to bound the difference of the two lower dimensional embeddings.",
            "So if you are interested in theory work, you can read it.",
            "If you just want to apply the approach, you don't need to understand it.",
            "And some result from the theorem is if we want to compute lower dimensional inviting, you really need to pick the top K eigenvectors.",
            "But how to pick K Theorem tells us the eigen gap is very important.",
            "So if the number K eigenvalue and the number K + 1 eigenvalue is quite different.",
            "The system will be very stable, so other words, even if the original relationship matrix is somehow perturbed.",
            "The lower dimensional inviting will not be significantly changed, so when we do the nationality reduction, Eggen Gap is something we."
        ],
        [
            "Still consider.",
            "Expire."
        ],
        [
            "Dental results.",
            "This one I just briefly describe it so."
        ],
        [
            "You skip it.",
            "Second one is about cross lingual information retrieval.",
            "And the problem is we want to find the exact correspondence between documents in different languages.",
            "And we treat each collection as a manifold.",
            "For this case, our death site includes two collections, one in English, one in Arabic, each of which has roughly 2100 documents, and they are manually translated.",
            "We use about 20% of them too.",
            "As the training example and the remaining 70% is used to test the results, test the."
        ],
        [
            "Approaches.",
            "And the document vector is represented by language model similarity between documents return in same language.",
            "Is represented by the distributional affinity, and we don't need the similarity between documents written in different languages.",
            "We want to do transfer.",
            "And the correspondence between a small number of the documents is given.",
            "So for this case, 25%.",
            "And the data and the approach used here are from Fernando's educate paper."
        ],
        [
            "In our approach, in the first step we tested Q dimensionality reduction approaches, losinger map and IOP.",
            "And we also tested the semi supervised alignment and the baseline approach, which is very simple."
        ],
        [
            "And the results are summarized in the figure.",
            "The red line comes from Procrustes analysis plus.",
            "Last thing I can map the blue line comes from LP and Procrustes analysis and the green and Purple line from the baseline algorithm and semi supervised manifold alignment.",
            "So from the figure we can see that.",
            "Given any RBC document, if we just retrieve the top three most relevant English documents, the true match has a 60% probability of being among the top three.",
            "If we pick up the top 10, the probability goes up to 80%.",
            "And the performance of LP is worse compared to Laplacian Eigen map.",
            "It's easy to understand the cause when we are doing the nationality reduction.",
            "We only consider the training examples."
        ],
        [
            "Another application is about the representation transform in MDP's.",
            "Here we are using the proto value function framework.",
            "The so called protocol function is something like this.",
            "So given any state X.",
            "We use a linear combination of the dot product of proto value function and the state to approximate the value function.",
            "So here clearly is a coefficient FI is a provider function.",
            "In fact it's just the eigenvector of normalized graph Laplacian.",
            "And acts is a space as state.",
            "So given some training examples, we can learn the coefficients and the provider functions.",
            "What we want to do here is to transfer the old protocol functions through the new space.",
            "If the transfer is done with can pretty much transfer everything to the new domain and in the paper we did perturbation analysis.",
            "In other words, we assume the state space.",
            "Is only slightly different and we apply the old provider functions in the new space.",
            "We learned very good control."
        ],
        [
            "But something more interesting is how about the two domains are very, very different.",
            "For example, one is the inverted pendulum domain, another one is mountain card domain.",
            "How can we do the transfer?",
            "In the paper we gave the framework, but we didn't provide any experimental results.",
            "The framework is here assume Y represents the provider functions of the current time DP and interesting Lee, why can also be thought as lower dimensional inviting of the old state space.",
            "And the film X is the lower dimensional embedding of the new space.",
            "Using our approach, the new protocol functions will just be y * Q.",
            "In other words, we rotate the old profile functions.",
            "They will be the new protocol functions.",
            "Some work that my wife my lab mates is working on, so I put future work here is not my future work.",
            "Is her future work.",
            "And there are some open problems like how to compute X and how to find correspondences between two quite diff."
        ],
        [
            "Optimize.",
            "A very quick."
        ],
        [
            "Summary So here we have a method to do manifold alignment and we first map the data size to lower dimensional space.",
            "Then we do alignment.",
            "And we can learn the risk factor and the rotation so we can transfer the knowledge using the two factors.",
            "We mentioned that applications on cross lingual information retrieval and representation transferring them DPS.",
            "The future work includes transfer learning MVP's, but that's all.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, thanks for attending the talk.",
                    "label": 0
                },
                {
                    "sent": "My name is Wang Chung.",
                    "label": 0
                },
                {
                    "sent": "I come from University of Massachusetts.",
                    "label": 0
                },
                {
                    "sent": "I will present the work on manifold alignment using procrustes analysis.",
                    "label": 1
                },
                {
                    "sent": "Is the joint work with Sheridan Hardware?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, in real life we always need to do knowledge transfer.",
                    "label": 0
                },
                {
                    "sent": "For example, if you come here from the US, you will realize that the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The box and the parking meter here are quite different from the US, but this is not a problem because we know how to transfer the knowledge learned from the US to fly.",
                    "label": 0
                },
                {
                    "sent": "So this transfer is simple, is straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But how about this?",
                    "label": 0
                },
                {
                    "sent": "So you are very interested in the document returning English and they want to find the most relevant document written in Arabic.",
                    "label": 0
                },
                {
                    "sent": "So this transfer is much more complex compared to the transfer shown in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And this is a problem that we want to solve in the paper.",
                    "label": 0
                },
                {
                    "sent": "An the problem is the transfer is between two seemingly quite different datasets.",
                    "label": 1
                },
                {
                    "sent": "So we want to transform for such problem.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are some simple applications where the knowledge transfer is very useful.",
                    "label": 0
                },
                {
                    "sent": "I just mentioned automatic machine translation.",
                    "label": 1
                },
                {
                    "sent": "Anne Cross lingual information retrieval and in representation and control policy, transfer between two different MDP's are also using the knowledge transfer.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outline of my talk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First part background.",
                    "label": 0
                },
                {
                    "sent": "The basic concept, but a manifold.",
                    "label": 0
                },
                {
                    "sent": "So Manifold is an abstract mathematical space in which every point has a neighborhood which resembles Euclidean space, but in which the global structure is much more complicated.",
                    "label": 1
                },
                {
                    "sent": "So one perfect example about manifold is the Earth.",
                    "label": 0
                },
                {
                    "sent": "If we want to measure the surface distance between two locations, if the two locations are very close to each other, we just use Euclidean distance to measure the distance between them.",
                    "label": 0
                },
                {
                    "sent": "However, if two locations are far away from each other, we have to consider the fact that the Earth is not flat, so the distance on the surface is much larger compared to the Euclidean distance between the two locations.",
                    "label": 0
                },
                {
                    "sent": "And there are some other examples about the manifold.",
                    "label": 1
                },
                {
                    "sent": "As state space in MDP process can be thought as a manifold, a collection of documents can be thought as a manifold.",
                    "label": 1
                },
                {
                    "sent": "And a set of images collected by a robot can also be thought as a manifold.",
                    "label": 1
                },
                {
                    "sent": "Manifolds are usually invited in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But their intrinsic dimensionality might be much lower because we have fewer degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And we can collect data points from the manifold.",
                    "label": 0
                },
                {
                    "sent": "Something like running around walk so we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And collect data points.",
                    "label": 0
                },
                {
                    "sent": "Manifold alignment approaches or line for manifolds using a set of landmark points.",
                    "label": 1
                },
                {
                    "sent": "To my knowledge, the pioneer work in the field is called semi supervised manifold alignment.",
                    "label": 0
                },
                {
                    "sent": "It Maps the points of two data sites through the same space by solving a constrained inviting problem where the embedding of the corresponding points from the different sets are constrained to be identical.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK our.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here we are given two websites, each of which comes from a manifold.",
                    "label": 0
                },
                {
                    "sent": "And each side has two subsets, for example, as one has two subsets, one unlabeled and S1 labeled.",
                    "label": 0
                },
                {
                    "sent": "And we are given some correspondence, so each point in as well labeled is corresponding to point in S2 label.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn something from the correspondence and we want to apply the knowledge we learned to the whole data set and the framework of the algorithm is here in the first step we map the two data size to lower dimensional spaces reflecting their intrinsic structure.",
                    "label": 1
                },
                {
                    "sent": "And in the second step we do procrustes analysis to align the two low dimensional embedding based on the landmark points in the correspondence.",
                    "label": 1
                },
                {
                    "sent": "And the procrustes analysis can remove the translational, rotational and scaling components so that we can get the optimal alignment.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit more details about the first step, so it's in fact a dimensionality reduction step.",
                    "label": 0
                },
                {
                    "sent": "All the existing approaches on dimensionality reduction can be used here, like Isomap plus single map, PCA, LP.",
                    "label": 1
                },
                {
                    "sent": "And here we have something to mention when we are using the 1st three approaches.",
                    "label": 0
                },
                {
                    "sent": "We are computing the lower dimensional embedding directly from the input.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the mapping looks like, so when we want to extend the knowledge to some new examples we might run into problems because we don't know the mapping.",
                    "label": 0
                },
                {
                    "sent": "For PCA and IOP.",
                    "label": 0
                },
                {
                    "sent": "We can solve the problem becausw using these two approaches.",
                    "label": 1
                },
                {
                    "sent": "We learn the mapping F and once the F is given, we can map the higher dimensional data to lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And F is defined everywhere, so without any problem, we can extend the mapping to new data points.",
                    "label": 1
                },
                {
                    "sent": "At the bottom of the slide I put the method to use Laplacian Eigen map to do dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "It's just the regular approach so I will skip that.",
                    "label": 0
                },
                {
                    "sent": "In the second step we do.",
                    "label": 0
                },
                {
                    "sent": "But alignment as I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "This approach can remove the translational, rotational and scaling components from one side so that we can achieve optimal alignment between the two sides.",
                    "label": 0
                },
                {
                    "sent": "And here assume the lower dimensional inviting of S1 labeled is X lower dimensional embedding of S2 labeled is Y. Anne, first we translate the configures of X&Y so that their suckers are at a region point.",
                    "label": 0
                },
                {
                    "sent": "So here in the first step we do the translation.",
                    "label": 0
                },
                {
                    "sent": "And then the second step we do.",
                    "label": 0
                },
                {
                    "sent": "Singular value decomposition of the matrix while transpose times X.",
                    "label": 0
                },
                {
                    "sent": "This decomposition return returns us with three matrices.",
                    "label": 0
                },
                {
                    "sent": "Orthonormal matrix U of normal matrix V and diagonal matrix Sigma.",
                    "label": 0
                },
                {
                    "sent": "And the optimal rotation kill will be just a few times we transpose.",
                    "label": 0
                },
                {
                    "sent": "And the optimal rescale factor.",
                    "label": 0
                },
                {
                    "sent": "Is very simple trace of Sigma over trace of Y transpose times Y.",
                    "label": 0
                },
                {
                    "sent": "It's very clean results.",
                    "label": 0
                },
                {
                    "sent": "And once we have K&Q, we can do the knowledge transfer.",
                    "label": 0
                },
                {
                    "sent": "Here we want to align Y 2X.",
                    "label": 0
                },
                {
                    "sent": "So we compute White Star which is equal to K * y * Q.",
                    "label": 0
                },
                {
                    "sent": "And we can guarantee that this one will be the optimal result regarding the Frobenius norm.",
                    "label": 1
                },
                {
                    "sent": "And Frobenius norm is very popularly used.",
                    "label": 0
                },
                {
                    "sent": "The definition is also on the slide.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And give you some rough idea about what's going on.",
                    "label": 0
                },
                {
                    "sent": "Here in figure A and figure P will show to lower dimensional embeddings.",
                    "label": 0
                },
                {
                    "sent": "And from the two figures, it seems like the two ships are quite different.",
                    "label": 0
                },
                {
                    "sent": "And they also pull out the two structures on the same figure figure see.",
                    "label": 0
                },
                {
                    "sent": "So you can see that.",
                    "label": 0
                },
                {
                    "sent": "A is much larger than B.",
                    "label": 0
                },
                {
                    "sent": "And we apply this approach.",
                    "label": 0
                },
                {
                    "sent": "And we got the K which is equal to 4.3 for this case and the rotation matrix is also listed on the slide.",
                    "label": 0
                },
                {
                    "sent": "We will play key on the Q2 structure B and we pull out the alignments in Figure D so you can see that the two structures are aligned very well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some justifications I.",
                    "label": 0
                },
                {
                    "sent": "Didn't put too much technical details in the slides, I just want to give you some high level explanation.",
                    "label": 0
                },
                {
                    "sent": "So on the slides there is no proof in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a lot.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can achieve optimal alignment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the only sentence I want to mention.",
                    "label": 0
                },
                {
                    "sent": "On this slide.",
                    "label": 0
                },
                {
                    "sent": "Procrustes analysis achieves optimal alignment regarding for.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opinions now.",
                    "label": 0
                },
                {
                    "sent": "The second theorem is about under what conditions the two manifolds can be similar.",
                    "label": 1
                },
                {
                    "sent": "You know many state of the art approaches to dimensionality reduction or something like the following.",
                    "label": 0
                },
                {
                    "sent": "We first compute a relationship matrix and then we compute the top eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "The top eigenvectors will be used as the lower dimensional embedding.",
                    "label": 1
                },
                {
                    "sent": "Of the original data, so here using Laplace anger map as example, the relationship matrix is graph Laplacian and lower dimensional embedding will just be the smallest eigenvectors of the graph Laplacian matrix.",
                    "label": 1
                },
                {
                    "sent": "Theorem poo use the difference between 2:00.",
                    "label": 0
                },
                {
                    "sent": "Input relationship matrices to bound the difference of the two lower dimensional embeddings.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested in theory work, you can read it.",
                    "label": 0
                },
                {
                    "sent": "If you just want to apply the approach, you don't need to understand it.",
                    "label": 0
                },
                {
                    "sent": "And some result from the theorem is if we want to compute lower dimensional inviting, you really need to pick the top K eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "But how to pick K Theorem tells us the eigen gap is very important.",
                    "label": 0
                },
                {
                    "sent": "So if the number K eigenvalue and the number K + 1 eigenvalue is quite different.",
                    "label": 0
                },
                {
                    "sent": "The system will be very stable, so other words, even if the original relationship matrix is somehow perturbed.",
                    "label": 0
                },
                {
                    "sent": "The lower dimensional inviting will not be significantly changed, so when we do the nationality reduction, Eggen Gap is something we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still consider.",
                    "label": 0
                },
                {
                    "sent": "Expire.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dental results.",
                    "label": 0
                },
                {
                    "sent": "This one I just briefly describe it so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You skip it.",
                    "label": 0
                },
                {
                    "sent": "Second one is about cross lingual information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And the problem is we want to find the exact correspondence between documents in different languages.",
                    "label": 1
                },
                {
                    "sent": "And we treat each collection as a manifold.",
                    "label": 1
                },
                {
                    "sent": "For this case, our death site includes two collections, one in English, one in Arabic, each of which has roughly 2100 documents, and they are manually translated.",
                    "label": 0
                },
                {
                    "sent": "We use about 20% of them too.",
                    "label": 0
                },
                {
                    "sent": "As the training example and the remaining 70% is used to test the results, test the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approaches.",
                    "label": 0
                },
                {
                    "sent": "And the document vector is represented by language model similarity between documents return in same language.",
                    "label": 1
                },
                {
                    "sent": "Is represented by the distributional affinity, and we don't need the similarity between documents written in different languages.",
                    "label": 0
                },
                {
                    "sent": "We want to do transfer.",
                    "label": 1
                },
                {
                    "sent": "And the correspondence between a small number of the documents is given.",
                    "label": 1
                },
                {
                    "sent": "So for this case, 25%.",
                    "label": 0
                },
                {
                    "sent": "And the data and the approach used here are from Fernando's educate paper.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our approach, in the first step we tested Q dimensionality reduction approaches, losinger map and IOP.",
                    "label": 0
                },
                {
                    "sent": "And we also tested the semi supervised alignment and the baseline approach, which is very simple.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the results are summarized in the figure.",
                    "label": 0
                },
                {
                    "sent": "The red line comes from Procrustes analysis plus.",
                    "label": 0
                },
                {
                    "sent": "Last thing I can map the blue line comes from LP and Procrustes analysis and the green and Purple line from the baseline algorithm and semi supervised manifold alignment.",
                    "label": 0
                },
                {
                    "sent": "So from the figure we can see that.",
                    "label": 0
                },
                {
                    "sent": "Given any RBC document, if we just retrieve the top three most relevant English documents, the true match has a 60% probability of being among the top three.",
                    "label": 1
                },
                {
                    "sent": "If we pick up the top 10, the probability goes up to 80%.",
                    "label": 0
                },
                {
                    "sent": "And the performance of LP is worse compared to Laplacian Eigen map.",
                    "label": 0
                },
                {
                    "sent": "It's easy to understand the cause when we are doing the nationality reduction.",
                    "label": 0
                },
                {
                    "sent": "We only consider the training examples.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another application is about the representation transform in MDP's.",
                    "label": 0
                },
                {
                    "sent": "Here we are using the proto value function framework.",
                    "label": 0
                },
                {
                    "sent": "The so called protocol function is something like this.",
                    "label": 0
                },
                {
                    "sent": "So given any state X.",
                    "label": 0
                },
                {
                    "sent": "We use a linear combination of the dot product of proto value function and the state to approximate the value function.",
                    "label": 1
                },
                {
                    "sent": "So here clearly is a coefficient FI is a provider function.",
                    "label": 1
                },
                {
                    "sent": "In fact it's just the eigenvector of normalized graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "And acts is a space as state.",
                    "label": 0
                },
                {
                    "sent": "So given some training examples, we can learn the coefficients and the provider functions.",
                    "label": 0
                },
                {
                    "sent": "What we want to do here is to transfer the old protocol functions through the new space.",
                    "label": 0
                },
                {
                    "sent": "If the transfer is done with can pretty much transfer everything to the new domain and in the paper we did perturbation analysis.",
                    "label": 1
                },
                {
                    "sent": "In other words, we assume the state space.",
                    "label": 1
                },
                {
                    "sent": "Is only slightly different and we apply the old provider functions in the new space.",
                    "label": 0
                },
                {
                    "sent": "We learned very good control.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But something more interesting is how about the two domains are very, very different.",
                    "label": 1
                },
                {
                    "sent": "For example, one is the inverted pendulum domain, another one is mountain card domain.",
                    "label": 0
                },
                {
                    "sent": "How can we do the transfer?",
                    "label": 0
                },
                {
                    "sent": "In the paper we gave the framework, but we didn't provide any experimental results.",
                    "label": 0
                },
                {
                    "sent": "The framework is here assume Y represents the provider functions of the current time DP and interesting Lee, why can also be thought as lower dimensional inviting of the old state space.",
                    "label": 1
                },
                {
                    "sent": "And the film X is the lower dimensional embedding of the new space.",
                    "label": 0
                },
                {
                    "sent": "Using our approach, the new protocol functions will just be y * Q.",
                    "label": 1
                },
                {
                    "sent": "In other words, we rotate the old profile functions.",
                    "label": 0
                },
                {
                    "sent": "They will be the new protocol functions.",
                    "label": 0
                },
                {
                    "sent": "Some work that my wife my lab mates is working on, so I put future work here is not my future work.",
                    "label": 0
                },
                {
                    "sent": "Is her future work.",
                    "label": 0
                },
                {
                    "sent": "And there are some open problems like how to compute X and how to find correspondences between two quite diff.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimize.",
                    "label": 0
                },
                {
                    "sent": "A very quick.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary So here we have a method to do manifold alignment and we first map the data size to lower dimensional space.",
                    "label": 1
                },
                {
                    "sent": "Then we do alignment.",
                    "label": 0
                },
                {
                    "sent": "And we can learn the risk factor and the rotation so we can transfer the knowledge using the two factors.",
                    "label": 0
                },
                {
                    "sent": "We mentioned that applications on cross lingual information retrieval and representation transferring them DPS.",
                    "label": 1
                },
                {
                    "sent": "The future work includes transfer learning MVP's, but that's all.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}