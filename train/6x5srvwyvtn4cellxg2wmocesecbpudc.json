{
    "id": "6x5srvwyvtn4cellxg2wmocesecbpudc",
    "title": "Passive Learning with Target Risk",
    "info": {
        "author": [
            "Mehrdad Mahdavi, Department of Computer Science and Engineering, Michigan State University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_mahdavi_risk/",
    "segmentation": [
        [
            "Hi good morning everybody.",
            "So this is a joint work with my advisor rank Gina.",
            "It's about passive learning with target risk society.",
            "It's it's supposed to be risk."
        ],
        [
            "So we consider the standard setting of statical learning.",
            "So we have a instance space data points and labels and we have a distribution over this.",
            "This time is in science space and we have a hypothesis class H and we have a last function to measure the performance of the any hypothesis from the class.",
            "And we we are given as input a bunch of training examples instance with the level and our goal is to minimize the expected loss or risk over randomly chosen sample from distribution.",
            "And the sample complexity of any learning algorithm is the number of examples.",
            "To achieve accuracy epsilon with high probability."
        ],
        [
            "So one simple method to solve this problem is the empirical risk minimization, or ERM, which tries to sample and data points from the expected less and try to minimize this.",
            "And then based on the uniform convergence assumptions and some restrictions on the hypothesis space or some other assumptions, we can actually get this fundamental theorem of learning theory, which implies that if the we have a finite VC dimension or Rademacher complexity, then uniform convergence holds and the problem is learnable with DRM.",
            "And then so this actually implies that uniform convergence assumption an essay for.",
            "Learning without a problem."
        ],
        [
            "So in this paper actually.",
            "We ask this question if the target accuracy is known to the learner in advanced.",
            "So can we gain from this assumption?",
            "So.",
            "Actually, one main difference between this assumption and previous assumptions on the hypothesis space like margin sparsity or find out which dimensions that we can actually utilize.",
            "This prior knowledge in the learning process.",
            "Actually, we show that we can significantly improve the sample complexity."
        ],
        [
            "So this is a five I will actually review the known lower bounds and upper bounds on the sample complexity then.",
            "I will motivate our problem and then.",
            "Actually introduced the proposed method."
        ],
        [
            "So if you if you check the classical literature in stats coloring, you will see that in the pack setting we have this wow epsilon lower bound and agnostic pack setting.",
            "We have one over Epsilon square sample complexity."
        ],
        [
            "And actually recently.",
            "A few more exactly sure that if we put more assumptions on the loss function, like strong convexity or smoothness outlines function, we can improve the convergence.",
            "Actually sample complexity.",
            "Few works I actually listed over here so that some convex S accounts from the new analysis in the online learning.",
            "Then we show that actually we can get 1 / T convergency and in this in the 2nd paper actually buy this Moosa Sumption.",
            "Naughty actually Sir Han and how much do I show that?",
            "If the last function is smooth, we can get this improved sample complexity, which is tight for this setting."
        ],
        [
            "And in this paper we show that if we know the target risk in advanced and we utilized this prior knowledge in the learning process, we can get Laguan Rep salon accuracy.",
            "So an exponential improvement.",
            "So how?"
        ],
        [
            "OK so I firstly status option that I made in the analysis.",
            "The first one is the strong convexity and smoothness and the third one is actually we assume that epsilon prior which is given to the learner is visible.",
            "So it's larger than the optimal accuracy we can actually."
        ],
        [
            "So before going to the algorithm, I actually.",
            "Introduce briefly the convex lens ability, which is proposed.",
            "Actually discuss fears of going cold."
        ],
        [
            "So actually 12 durations.",
            "Shy or had actually not.",
            "Actually they observed that.",
            "Unique without uniform convergence, we have learnable problems, so they show examples that we can learn some problems with convex optimization, but no uniform convergence holds.",
            "So they motivate their published few papers on the relationship between stochastic convex optimization and linearity problem in general learning problems."
        ],
        [
            "So exactly an alternative viewpoint of empirical simulation and we try to directly minimize the expected loss functions, which is the stochastic gradients in the simple case we update based on the stochastic gradient descent project back to the hypothesis space.",
            "Actually, they show that.",
            "Even with a something empirical is memorization.",
            "We can show that the problem is learnable and this is more general family of learning problems."
        ],
        [
            "So to see how can we get actually exponential improvement, we go 40 years back for lower bound, prove our numerous scan using in their paper in their books.",
            "So actually this is the lower bound for any convex stochastic optimization algorithm for a smooth and strongly convex loss function minimization.",
            "So if we look at this lower bound, we have two times.",
            "First time is logarithmically and the second one is.",
            "Sigma Sigma Square epsilon convention.",
            "This term is dominating.",
            "It depends on the variance of the stochastic Oracle, which provides the gradients for the learner.",
            "So if you assume.",
            "The Oracle is very nice and variance is bounded by the accuracy we're looking for.",
            "Then the second term would be constant and we get the lag rhythmic one over epsilon.",
            "Actually, I crazy, but the problem is that we cannot make any assumption on the Oracle which provides stochastic gradient descent.",
            "So we try to modify stochastic gradient descent to tolerate the noise as an alternative because we don't have any contraband.",
            "Plastic or."
        ],
        [
            "So to do this, we propose this stochastic gradient descent with target risk."
        ],
        [
            "So there are three main ideas in the proposed algorithm.",
            "1st is we change the stochastic gradient descent by running it in multi stage.",
            "Is the 2nd is we clip the gradients to reduce the variance but actually disclipine comes with a cost which actually we don't have unbiased estimates of the true gradient.",
            "So we shake the domain of hypothesis space to actually cope with the ambassadors of the gradients."
        ],
        [
            "So in detail, keeping is just simple.",
            "We keep the gradient by some constant which is changing.",
            "Changing from the stage to this stage.",
            "Good news about this is actually it reduces the variance, but bad news is that it's not unbiased estimate of the true gradient at all, because we treat the gradient."
        ],
        [
            "In shrinking at this stage actually machine the domain of the hypothesis states we move forward.",
            "Putting this aside."
        ],
        [
            "Yes, together we have this multistage stochastic gradient descent algorithm so.",
            "It's run, actually, Zac works, so at the beginning of each airport we set the initial solution to the average series Iris solution.",
            "In the last stage, and we determine the.",
            "Keeping constant, then we just run the simple stochastic gradient descent, but now we use the clip version of the gradients.",
            "And then at the end of the stage.",
            "We actually.",
            "Shake the domain and.",
            "Determined in new hypothesis States and go to the next stage.",
            "So.",
            "It's in the same as without this stochastic gradient descent, but shrinking multi staging and clip gradients are the modification we made to this algorithm."
        ],
        [
            "OK, so actually we show that the convergence rate of the proposed algorithm has two terms, the first time it's just constant depends on number of States and then.",
            "The second term depends on epsilon prior and then we."
        ],
        [
            "Transform this.",
            "The sample complexity which gives large lag lag one over epsilon, plus like one over the Delta.",
            "An copper is the condition number of the last function, which is better or Alpha?"
        ],
        [
            "So I just give the.",
            "Profession to slide.",
            "So the main proof is based on the induction and we show that.",
            "So at that stage K the final solution actually departs from the optimal solution with Delta K in the next stage.",
            "Actually, it depends on the Delta K ^2 plus.",
            "Some constant times the same prayer we have.",
            "Then why this muscle function we can get easily the convergence rate of the algorithm."
        ],
        [
            "Actually, to prove this coverage that we so some ideas from Elan Shy, the first one is the deviation of crypt random variable which shows that the expected value of clipped gradient deviates from the expected value by some constant times the variance of the random variable.",
            "And the second actually deserve boundless properties.",
            "Must function which has been used by Cheyenne's.",
            "PhD thesis.",
            "And then using person quality for martingales, an applied appealing process or discretization which which is actually famous technique in statistical learning theory we can prove the convergence rate."
        ],
        [
            "So.",
            "So in summary, actually.",
            "Be sure that actually.",
            "If you have a prior knowledge on the learning problem, so why not use it in the learning process?",
            "For example, if you assume there is a large margin classifier.",
            "All the previous assumptions actually enter into the generalization bounds, so if if the assumption holds, so why not using the learning process and improve the sample complexity so this actually prior risk use this opportunity to ask to use in the learning process and stochastic guide in the same framework for learning actually is a very good option to utilize this information.",
            "The learning process and we show that actually by modifying the stochastic value on this end we can improve.",
            "Sample complexity to log one over epsilon.",
            "And there are actually fewer problems.",
            "Which I think it was to invest some time.",
            "The first one is actually we have.",
            "If you check the bond we have a dependency B which actually makes our algorithm hard to extend for nonparametric setting.",
            "So it would be interesting to see what happens if we use this prior knowledge in nonparametric setting.",
            "Second, one actually is that.",
            "Based on few works on active learning, if low margin condition holds then.",
            "It happens that our algorithm, an active learning, same shared the same sample complexes.",
            "So it would be interesting to see if there any connection between our assumption and.",
            "Low noise assumption in the active learning.",
            "And we have actually worse dependence on the condition number in the lower round.",
            "Actually it is.",
            "Skirt out better off about our dependencies condition number power four so.",
            "It would be interesting to see can we improve this dependency on the condition number."
        ],
        [
            "And I thank the your attention and I see."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "So this is a joint work with my advisor rank Gina.",
                    "label": 0
                },
                {
                    "sent": "It's about passive learning with target risk society.",
                    "label": 1
                },
                {
                    "sent": "It's it's supposed to be risk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider the standard setting of statical learning.",
                    "label": 0
                },
                {
                    "sent": "So we have a instance space data points and labels and we have a distribution over this.",
                    "label": 0
                },
                {
                    "sent": "This time is in science space and we have a hypothesis class H and we have a last function to measure the performance of the any hypothesis from the class.",
                    "label": 0
                },
                {
                    "sent": "And we we are given as input a bunch of training examples instance with the level and our goal is to minimize the expected loss or risk over randomly chosen sample from distribution.",
                    "label": 0
                },
                {
                    "sent": "And the sample complexity of any learning algorithm is the number of examples.",
                    "label": 1
                },
                {
                    "sent": "To achieve accuracy epsilon with high probability.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one simple method to solve this problem is the empirical risk minimization, or ERM, which tries to sample and data points from the expected less and try to minimize this.",
                    "label": 1
                },
                {
                    "sent": "And then based on the uniform convergence assumptions and some restrictions on the hypothesis space or some other assumptions, we can actually get this fundamental theorem of learning theory, which implies that if the we have a finite VC dimension or Rademacher complexity, then uniform convergence holds and the problem is learnable with DRM.",
                    "label": 1
                },
                {
                    "sent": "And then so this actually implies that uniform convergence assumption an essay for.",
                    "label": 0
                },
                {
                    "sent": "Learning without a problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper actually.",
                    "label": 0
                },
                {
                    "sent": "We ask this question if the target accuracy is known to the learner in advanced.",
                    "label": 1
                },
                {
                    "sent": "So can we gain from this assumption?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Actually, one main difference between this assumption and previous assumptions on the hypothesis space like margin sparsity or find out which dimensions that we can actually utilize.",
                    "label": 1
                },
                {
                    "sent": "This prior knowledge in the learning process.",
                    "label": 1
                },
                {
                    "sent": "Actually, we show that we can significantly improve the sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a five I will actually review the known lower bounds and upper bounds on the sample complexity then.",
                    "label": 0
                },
                {
                    "sent": "I will motivate our problem and then.",
                    "label": 0
                },
                {
                    "sent": "Actually introduced the proposed method.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you if you check the classical literature in stats coloring, you will see that in the pack setting we have this wow epsilon lower bound and agnostic pack setting.",
                    "label": 0
                },
                {
                    "sent": "We have one over Epsilon square sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually recently.",
                    "label": 0
                },
                {
                    "sent": "A few more exactly sure that if we put more assumptions on the loss function, like strong convexity or smoothness outlines function, we can improve the convergence.",
                    "label": 0
                },
                {
                    "sent": "Actually sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Few works I actually listed over here so that some convex S accounts from the new analysis in the online learning.",
                    "label": 0
                },
                {
                    "sent": "Then we show that actually we can get 1 / T convergency and in this in the 2nd paper actually buy this Moosa Sumption.",
                    "label": 0
                },
                {
                    "sent": "Naughty actually Sir Han and how much do I show that?",
                    "label": 0
                },
                {
                    "sent": "If the last function is smooth, we can get this improved sample complexity, which is tight for this setting.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this paper we show that if we know the target risk in advanced and we utilized this prior knowledge in the learning process, we can get Laguan Rep salon accuracy.",
                    "label": 1
                },
                {
                    "sent": "So an exponential improvement.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I firstly status option that I made in the analysis.",
                    "label": 0
                },
                {
                    "sent": "The first one is the strong convexity and smoothness and the third one is actually we assume that epsilon prior which is given to the learner is visible.",
                    "label": 0
                },
                {
                    "sent": "So it's larger than the optimal accuracy we can actually.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before going to the algorithm, I actually.",
                    "label": 0
                },
                {
                    "sent": "Introduce briefly the convex lens ability, which is proposed.",
                    "label": 0
                },
                {
                    "sent": "Actually discuss fears of going cold.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually 12 durations.",
                    "label": 0
                },
                {
                    "sent": "Shy or had actually not.",
                    "label": 0
                },
                {
                    "sent": "Actually they observed that.",
                    "label": 0
                },
                {
                    "sent": "Unique without uniform convergence, we have learnable problems, so they show examples that we can learn some problems with convex optimization, but no uniform convergence holds.",
                    "label": 1
                },
                {
                    "sent": "So they motivate their published few papers on the relationship between stochastic convex optimization and linearity problem in general learning problems.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So exactly an alternative viewpoint of empirical simulation and we try to directly minimize the expected loss functions, which is the stochastic gradients in the simple case we update based on the stochastic gradient descent project back to the hypothesis space.",
                    "label": 1
                },
                {
                    "sent": "Actually, they show that.",
                    "label": 0
                },
                {
                    "sent": "Even with a something empirical is memorization.",
                    "label": 0
                },
                {
                    "sent": "We can show that the problem is learnable and this is more general family of learning problems.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to see how can we get actually exponential improvement, we go 40 years back for lower bound, prove our numerous scan using in their paper in their books.",
                    "label": 0
                },
                {
                    "sent": "So actually this is the lower bound for any convex stochastic optimization algorithm for a smooth and strongly convex loss function minimization.",
                    "label": 1
                },
                {
                    "sent": "So if we look at this lower bound, we have two times.",
                    "label": 0
                },
                {
                    "sent": "First time is logarithmically and the second one is.",
                    "label": 0
                },
                {
                    "sent": "Sigma Sigma Square epsilon convention.",
                    "label": 0
                },
                {
                    "sent": "This term is dominating.",
                    "label": 1
                },
                {
                    "sent": "It depends on the variance of the stochastic Oracle, which provides the gradients for the learner.",
                    "label": 0
                },
                {
                    "sent": "So if you assume.",
                    "label": 0
                },
                {
                    "sent": "The Oracle is very nice and variance is bounded by the accuracy we're looking for.",
                    "label": 0
                },
                {
                    "sent": "Then the second term would be constant and we get the lag rhythmic one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Actually, I crazy, but the problem is that we cannot make any assumption on the Oracle which provides stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So we try to modify stochastic gradient descent to tolerate the noise as an alternative because we don't have any contraband.",
                    "label": 0
                },
                {
                    "sent": "Plastic or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to do this, we propose this stochastic gradient descent with target risk.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are three main ideas in the proposed algorithm.",
                    "label": 1
                },
                {
                    "sent": "1st is we change the stochastic gradient descent by running it in multi stage.",
                    "label": 0
                },
                {
                    "sent": "Is the 2nd is we clip the gradients to reduce the variance but actually disclipine comes with a cost which actually we don't have unbiased estimates of the true gradient.",
                    "label": 1
                },
                {
                    "sent": "So we shake the domain of hypothesis space to actually cope with the ambassadors of the gradients.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in detail, keeping is just simple.",
                    "label": 0
                },
                {
                    "sent": "We keep the gradient by some constant which is changing.",
                    "label": 0
                },
                {
                    "sent": "Changing from the stage to this stage.",
                    "label": 0
                },
                {
                    "sent": "Good news about this is actually it reduces the variance, but bad news is that it's not unbiased estimate of the true gradient at all, because we treat the gradient.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In shrinking at this stage actually machine the domain of the hypothesis states we move forward.",
                    "label": 0
                },
                {
                    "sent": "Putting this aside.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, together we have this multistage stochastic gradient descent algorithm so.",
                    "label": 0
                },
                {
                    "sent": "It's run, actually, Zac works, so at the beginning of each airport we set the initial solution to the average series Iris solution.",
                    "label": 0
                },
                {
                    "sent": "In the last stage, and we determine the.",
                    "label": 0
                },
                {
                    "sent": "Keeping constant, then we just run the simple stochastic gradient descent, but now we use the clip version of the gradients.",
                    "label": 0
                },
                {
                    "sent": "And then at the end of the stage.",
                    "label": 0
                },
                {
                    "sent": "We actually.",
                    "label": 0
                },
                {
                    "sent": "Shake the domain and.",
                    "label": 0
                },
                {
                    "sent": "Determined in new hypothesis States and go to the next stage.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's in the same as without this stochastic gradient descent, but shrinking multi staging and clip gradients are the modification we made to this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so actually we show that the convergence rate of the proposed algorithm has two terms, the first time it's just constant depends on number of States and then.",
                    "label": 0
                },
                {
                    "sent": "The second term depends on epsilon prior and then we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Transform this.",
                    "label": 0
                },
                {
                    "sent": "The sample complexity which gives large lag lag one over epsilon, plus like one over the Delta.",
                    "label": 0
                },
                {
                    "sent": "An copper is the condition number of the last function, which is better or Alpha?",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just give the.",
                    "label": 0
                },
                {
                    "sent": "Profession to slide.",
                    "label": 0
                },
                {
                    "sent": "So the main proof is based on the induction and we show that.",
                    "label": 0
                },
                {
                    "sent": "So at that stage K the final solution actually departs from the optimal solution with Delta K in the next stage.",
                    "label": 0
                },
                {
                    "sent": "Actually, it depends on the Delta K ^2 plus.",
                    "label": 0
                },
                {
                    "sent": "Some constant times the same prayer we have.",
                    "label": 0
                },
                {
                    "sent": "Then why this muscle function we can get easily the convergence rate of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, to prove this coverage that we so some ideas from Elan Shy, the first one is the deviation of crypt random variable which shows that the expected value of clipped gradient deviates from the expected value by some constant times the variance of the random variable.",
                    "label": 0
                },
                {
                    "sent": "And the second actually deserve boundless properties.",
                    "label": 0
                },
                {
                    "sent": "Must function which has been used by Cheyenne's.",
                    "label": 0
                },
                {
                    "sent": "PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "And then using person quality for martingales, an applied appealing process or discretization which which is actually famous technique in statistical learning theory we can prove the convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in summary, actually.",
                    "label": 0
                },
                {
                    "sent": "Be sure that actually.",
                    "label": 0
                },
                {
                    "sent": "If you have a prior knowledge on the learning problem, so why not use it in the learning process?",
                    "label": 0
                },
                {
                    "sent": "For example, if you assume there is a large margin classifier.",
                    "label": 0
                },
                {
                    "sent": "All the previous assumptions actually enter into the generalization bounds, so if if the assumption holds, so why not using the learning process and improve the sample complexity so this actually prior risk use this opportunity to ask to use in the learning process and stochastic guide in the same framework for learning actually is a very good option to utilize this information.",
                    "label": 0
                },
                {
                    "sent": "The learning process and we show that actually by modifying the stochastic value on this end we can improve.",
                    "label": 0
                },
                {
                    "sent": "Sample complexity to log one over epsilon.",
                    "label": 1
                },
                {
                    "sent": "And there are actually fewer problems.",
                    "label": 0
                },
                {
                    "sent": "Which I think it was to invest some time.",
                    "label": 0
                },
                {
                    "sent": "The first one is actually we have.",
                    "label": 0
                },
                {
                    "sent": "If you check the bond we have a dependency B which actually makes our algorithm hard to extend for nonparametric setting.",
                    "label": 0
                },
                {
                    "sent": "So it would be interesting to see what happens if we use this prior knowledge in nonparametric setting.",
                    "label": 1
                },
                {
                    "sent": "Second, one actually is that.",
                    "label": 0
                },
                {
                    "sent": "Based on few works on active learning, if low margin condition holds then.",
                    "label": 0
                },
                {
                    "sent": "It happens that our algorithm, an active learning, same shared the same sample complexes.",
                    "label": 0
                },
                {
                    "sent": "So it would be interesting to see if there any connection between our assumption and.",
                    "label": 0
                },
                {
                    "sent": "Low noise assumption in the active learning.",
                    "label": 1
                },
                {
                    "sent": "And we have actually worse dependence on the condition number in the lower round.",
                    "label": 1
                },
                {
                    "sent": "Actually it is.",
                    "label": 0
                },
                {
                    "sent": "Skirt out better off about our dependencies condition number power four so.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to see can we improve this dependency on the condition number.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I thank the your attention and I see.",
                    "label": 0
                }
            ]
        }
    }
}