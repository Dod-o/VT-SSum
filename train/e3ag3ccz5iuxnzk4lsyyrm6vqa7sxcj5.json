{
    "id": "e3ag3ccz5iuxnzk4lsyyrm6vqa7sxcj5",
    "title": "Toward Dependency Path based Entailment",
    "info": {
        "author": [
            "Rodney D. Nielsen, University of Colorado Boulder"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/pcw06_nielsen_tdpbe/",
    "segmentation": [
        [
            "Wasn't where I'm from, so I'm gonna skip this talking about dependency pacbase entailment.",
            "So long term goal of what I want to do is is to basically enhance the dirt system, which is basically an unsupervised method to discover inference rules such as you know from."
        ],
        [
            "This is the author of why You can infer that Axe wrote why.",
            "Anne.",
            "The underlying rationale in their system is essentially if two dependency paths tend to link the same set of words, then they hypothesize that their meanings are similar.",
            "So I didn't get that far.",
            "What I did is I guess a lot looser than that, but I did take it dependency based approach in trying to solve the problem so it's a machine learning classification approach and I guess I would characterize it as being somewhere between the bag of words approach, such as the Glickman bar illan.",
            "Approach last year and say the graph matching of Stanford or UIUC last year or even I guess I would throw in now what Lucy discussed as far as the syntactic matching.",
            "So the idea is to try to avoid some of the brittleness of those really."
        ],
        [
            "You know with the graph matching and syntactic approaches, but yet get alittle bit more structure into the system.",
            "So the features, all the features that I use or drive from corpus statistics, unigram statistics, surface form, bigram statistics and dependency derived bigram statistics.",
            "Use a mixture of 18 machine learning classifiers from the WeChat toolkit and classify in one run by majority vote, the other one by average probability, and that's what I'll talk about today.",
            "The results are not showing today.",
            "My corpora consisted of 7.4 million articles, 2 1/2 billion words, and that's giga word, Reuters and tipster used Lucene for the queries and Jenna."
        ],
        [
            "My indices, I have two indices, one with the surface form of the word and the second one with the quarter stem of the words.",
            "OK, I'm not going to be able to do this."
        ],
        [
            "Try these in detail, but I essentially I've got nine or features that are repeated in 25 different contexts, so I've got a total of like just over 225 features that are used by the machine learning classifiers to make a decision and they are very similar in nature to the features that, for example, Glickman had last year where there was just they have the one feature that was the product of the probabilities associated with.",
            "Each word, so I do that based on dependencies etc.",
            "K so.",
            "Do I actually?"
        ],
        [
            "Run many par on my 7.4 million documents to generate the corpus statistics on dependencies.",
            "No, what I do is I approximate that by taking I take dependencies are UN mini bar on the hypothesis attacks and I take the dependencies that I find there and I convert those into just simple bigrams by.",
            "Ordering the two words in the dependency in the same order that they occur within the hypothesis or the tax, whichever I've taken from and then I do a corpus query on that bigram and use that as an approximation for how often that dependency occurs, for example.",
            "So.",
            "This set of features.",
            "Is essentially trying to approximate the probability of seeing the dependency between some content or WC.",
            "And its parent given the text.",
            "And the approximation for that is the Co occurrence of the two bigrams that I discuss in my corpora divided by the count for just the number of times that the bigram from the text occurs in corpus."
        ],
        [
            "So then the way that I generate the features then is by taking those probabilities and in the case of my average maximum likelihood, I just average all the numbers across all the content words in the hypothesis.",
            "Alright then, most of the rest of."
        ],
        [
            "Features feature sets are based on what I'm calling descendant relation statistics.",
            "And there.",
            "What I try to do is essentially approximate.",
            "The probability of seeing a subtree, say S, rooted at the word W given the text.",
            "Anne.",
            "Effectively what I'm trying to do with this sort of the algorithm that I use for this is basically a function of all of the dependencies of that word, so I'm going to just go through a couple of."
        ],
        [
            "Examples give you a sort of a sense of what it is that I'm doing.",
            "So for the word of there's only one child, and that child doesn't have a.",
            "Descending tree or subtree below it.",
            "So it's basically just.",
            "Just amounts to using that single dependency.",
            "We go up the tree to cost.",
            "Then I'm taking the function basically an average over the dependency, assuming that the was a content word, which it's not, but just for the purposes of this discussion, let's assume that it is, I take.",
            "Basically, statistics generated from that branch of the subtree and combine them with statistics from the other branch."
        ],
        [
            "And then similarly for Verizon.",
            "So those are just statistics that I generate for the for basically all of the words in the tree and then the features that I output are based, some of them the first couple of sets are based on the verbs, so I take all of the verbs in the hypoth."
        ],
        [
            "This is and to generate features I-1 set of features combines.",
            "In an averaging, for example, all of those maximum likelihood estimates that I got in the on the previous slides for those verb nodes, all right.",
            "And then the other set of features is just based on."
        ],
        [
            "Each of the verbs ended up having the lowest product of probabilities.",
            "Now I do a similar thing for all of the subjects.",
            "So basically all of the nodes in the dependency tree that have a subject relation with their parent.",
            "So I I generate those same sets of features for that.",
            "But then I also generate a set of features based on the path from the subject to the verb.",
            "And then I do the same thing."
        ],
        [
            "And for the object for the head noun, prepositional complement.",
            "And then I just throw everything else together.",
            "So anything else, any other branches below the verb or also considered."
        ],
        [
            "And the results.",
            "These are a little bit higher than the official results, 'cause it turned out in my rush.",
            "I had a couple of small bulbs in the system, so these results are I think 1 1/2% better.",
            "An I guess the first thing to know is that there's a huge difference between the training set and test set.",
            "An that's not actually it.",
            "It appears to me that that's not.",
            "At all because of overfitting.",
            "And the reason that I say that is because first of all, if you look at the RT one data.",
            "There was almost no drop in performance from the training set to the test set using the exact same procedure to generate my results OK. And also I the way that I could potentially have gotten overfitting, was it?",
            "I am fitting the classifiers fitting some of the parameters based on the cross validation on the training sets.",
            "You would expect it to do better on the training set, but it turns out you know I dropped a lot of those parameters and use defaults and basically got the same thing.",
            "The other, I guess.",
            "The other thing I was going to point out here is just that relative to last year's results where we where I see the biggest increase is on.",
            "The non CD portion of the data set.",
            "So if you partition the data set you know CD and then everything else the best results last year.",
            "As far as I could tell were 52.8% an I think the medium and the average were actually both below chance and I got 56.8% on that portion of the data set.",
            "And a little bit better overall, like 3.2%.",
            "Um?"
        ],
        [
            "A little bit of feature analysis and it appears that all of the feature sets are contributing, at least marginally.",
            "I'm I wouldn't put a lot of weight on that because there are so few.",
            "The data set is just so small, but it's hard to tell for sure.",
            "I'm sort of skeptical.",
            "I didn't expect this.",
            "I really assumed that you know there are a lot of features that I have that are incredibly redundant, and I assumed that I was going to get better.",
            "Results by doing feature selection, but I couldn't, I just couldn't do that.",
            "So the most significant features, or the unigrams down based alignment feature set and of the core features the average maximum likelihood estimate was the most signif."
        ],
        [
            "Questions.",
            "Can you quantify what the impact of using their car stickers over just dividers impossible next?",
            "So I guess I didn't do an analysis that throughout all of the dependence and related features I should have done that.",
            "But I did an analysis where I individually took each set from those 25 feature sets that I generated.",
            "I took out each feature set individually and in each case performance got worse.",
            "So they do seem to be having a positive effect, though it isn't a huge effect.",
            "Actually, well, no.",
            "I guess I would say that I probably don't.",
            "I do have some bigram features, but they're not quite the same, so the way that the reason that I had generated those bigram features were really to do a word based alignment, and so the bigrams that I generated, I used the same second word.",
            "Appended to the hypothesis and the taps.",
            "So it wasn't a bigram from the hypothesis compared to a bigram from the text, but rather a word from the hypothesis and a word from the text, and then I take, I get the bigram by first doing the word following the one in the hypothesis.",
            "I put that behind both of them, and do a corpus analysis of that, etc.",
            "You said it.",
            "So the difference above.",
            "My baseline, which was basically the Glickman ET al approach.",
            "I think about half of that roughly was based on that set of features.",
            "Support line.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wasn't where I'm from, so I'm gonna skip this talking about dependency pacbase entailment.",
                    "label": 0
                },
                {
                    "sent": "So long term goal of what I want to do is is to basically enhance the dirt system, which is basically an unsupervised method to discover inference rules such as you know from.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the author of why You can infer that Axe wrote why.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The underlying rationale in their system is essentially if two dependency paths tend to link the same set of words, then they hypothesize that their meanings are similar.",
                    "label": 1
                },
                {
                    "sent": "So I didn't get that far.",
                    "label": 0
                },
                {
                    "sent": "What I did is I guess a lot looser than that, but I did take it dependency based approach in trying to solve the problem so it's a machine learning classification approach and I guess I would characterize it as being somewhere between the bag of words approach, such as the Glickman bar illan.",
                    "label": 0
                },
                {
                    "sent": "Approach last year and say the graph matching of Stanford or UIUC last year or even I guess I would throw in now what Lucy discussed as far as the syntactic matching.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to try to avoid some of the brittleness of those really.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know with the graph matching and syntactic approaches, but yet get alittle bit more structure into the system.",
                    "label": 0
                },
                {
                    "sent": "So the features, all the features that I use or drive from corpus statistics, unigram statistics, surface form, bigram statistics and dependency derived bigram statistics.",
                    "label": 1
                },
                {
                    "sent": "Use a mixture of 18 machine learning classifiers from the WeChat toolkit and classify in one run by majority vote, the other one by average probability, and that's what I'll talk about today.",
                    "label": 0
                },
                {
                    "sent": "The results are not showing today.",
                    "label": 0
                },
                {
                    "sent": "My corpora consisted of 7.4 million articles, 2 1/2 billion words, and that's giga word, Reuters and tipster used Lucene for the queries and Jenna.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My indices, I have two indices, one with the surface form of the word and the second one with the quarter stem of the words.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not going to be able to do this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try these in detail, but I essentially I've got nine or features that are repeated in 25 different contexts, so I've got a total of like just over 225 features that are used by the machine learning classifiers to make a decision and they are very similar in nature to the features that, for example, Glickman had last year where there was just they have the one feature that was the product of the probabilities associated with.",
                    "label": 0
                },
                {
                    "sent": "Each word, so I do that based on dependencies etc.",
                    "label": 0
                },
                {
                    "sent": "K so.",
                    "label": 0
                },
                {
                    "sent": "Do I actually?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run many par on my 7.4 million documents to generate the corpus statistics on dependencies.",
                    "label": 0
                },
                {
                    "sent": "No, what I do is I approximate that by taking I take dependencies are UN mini bar on the hypothesis attacks and I take the dependencies that I find there and I convert those into just simple bigrams by.",
                    "label": 0
                },
                {
                    "sent": "Ordering the two words in the dependency in the same order that they occur within the hypothesis or the tax, whichever I've taken from and then I do a corpus query on that bigram and use that as an approximation for how often that dependency occurs, for example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This set of features.",
                    "label": 0
                },
                {
                    "sent": "Is essentially trying to approximate the probability of seeing the dependency between some content or WC.",
                    "label": 0
                },
                {
                    "sent": "And its parent given the text.",
                    "label": 0
                },
                {
                    "sent": "And the approximation for that is the Co occurrence of the two bigrams that I discuss in my corpora divided by the count for just the number of times that the bigram from the text occurs in corpus.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then the way that I generate the features then is by taking those probabilities and in the case of my average maximum likelihood, I just average all the numbers across all the content words in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Alright then, most of the rest of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Features feature sets are based on what I'm calling descendant relation statistics.",
                    "label": 0
                },
                {
                    "sent": "And there.",
                    "label": 0
                },
                {
                    "sent": "What I try to do is essentially approximate.",
                    "label": 0
                },
                {
                    "sent": "The probability of seeing a subtree, say S, rooted at the word W given the text.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Effectively what I'm trying to do with this sort of the algorithm that I use for this is basically a function of all of the dependencies of that word, so I'm going to just go through a couple of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples give you a sort of a sense of what it is that I'm doing.",
                    "label": 0
                },
                {
                    "sent": "So for the word of there's only one child, and that child doesn't have a.",
                    "label": 0
                },
                {
                    "sent": "Descending tree or subtree below it.",
                    "label": 0
                },
                {
                    "sent": "So it's basically just.",
                    "label": 0
                },
                {
                    "sent": "Just amounts to using that single dependency.",
                    "label": 0
                },
                {
                    "sent": "We go up the tree to cost.",
                    "label": 0
                },
                {
                    "sent": "Then I'm taking the function basically an average over the dependency, assuming that the was a content word, which it's not, but just for the purposes of this discussion, let's assume that it is, I take.",
                    "label": 0
                },
                {
                    "sent": "Basically, statistics generated from that branch of the subtree and combine them with statistics from the other branch.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then similarly for Verizon.",
                    "label": 0
                },
                {
                    "sent": "So those are just statistics that I generate for the for basically all of the words in the tree and then the features that I output are based, some of them the first couple of sets are based on the verbs, so I take all of the verbs in the hypoth.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is and to generate features I-1 set of features combines.",
                    "label": 0
                },
                {
                    "sent": "In an averaging, for example, all of those maximum likelihood estimates that I got in the on the previous slides for those verb nodes, all right.",
                    "label": 0
                },
                {
                    "sent": "And then the other set of features is just based on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each of the verbs ended up having the lowest product of probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now I do a similar thing for all of the subjects.",
                    "label": 0
                },
                {
                    "sent": "So basically all of the nodes in the dependency tree that have a subject relation with their parent.",
                    "label": 0
                },
                {
                    "sent": "So I I generate those same sets of features for that.",
                    "label": 0
                },
                {
                    "sent": "But then I also generate a set of features based on the path from the subject to the verb.",
                    "label": 0
                },
                {
                    "sent": "And then I do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the object for the head noun, prepositional complement.",
                    "label": 0
                },
                {
                    "sent": "And then I just throw everything else together.",
                    "label": 0
                },
                {
                    "sent": "So anything else, any other branches below the verb or also considered.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the results.",
                    "label": 0
                },
                {
                    "sent": "These are a little bit higher than the official results, 'cause it turned out in my rush.",
                    "label": 0
                },
                {
                    "sent": "I had a couple of small bulbs in the system, so these results are I think 1 1/2% better.",
                    "label": 0
                },
                {
                    "sent": "An I guess the first thing to know is that there's a huge difference between the training set and test set.",
                    "label": 1
                },
                {
                    "sent": "An that's not actually it.",
                    "label": 0
                },
                {
                    "sent": "It appears to me that that's not.",
                    "label": 0
                },
                {
                    "sent": "At all because of overfitting.",
                    "label": 0
                },
                {
                    "sent": "And the reason that I say that is because first of all, if you look at the RT one data.",
                    "label": 1
                },
                {
                    "sent": "There was almost no drop in performance from the training set to the test set using the exact same procedure to generate my results OK. And also I the way that I could potentially have gotten overfitting, was it?",
                    "label": 0
                },
                {
                    "sent": "I am fitting the classifiers fitting some of the parameters based on the cross validation on the training sets.",
                    "label": 0
                },
                {
                    "sent": "You would expect it to do better on the training set, but it turns out you know I dropped a lot of those parameters and use defaults and basically got the same thing.",
                    "label": 0
                },
                {
                    "sent": "The other, I guess.",
                    "label": 0
                },
                {
                    "sent": "The other thing I was going to point out here is just that relative to last year's results where we where I see the biggest increase is on.",
                    "label": 0
                },
                {
                    "sent": "The non CD portion of the data set.",
                    "label": 0
                },
                {
                    "sent": "So if you partition the data set you know CD and then everything else the best results last year.",
                    "label": 0
                },
                {
                    "sent": "As far as I could tell were 52.8% an I think the medium and the average were actually both below chance and I got 56.8% on that portion of the data set.",
                    "label": 0
                },
                {
                    "sent": "And a little bit better overall, like 3.2%.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit of feature analysis and it appears that all of the feature sets are contributing, at least marginally.",
                    "label": 1
                },
                {
                    "sent": "I'm I wouldn't put a lot of weight on that because there are so few.",
                    "label": 0
                },
                {
                    "sent": "The data set is just so small, but it's hard to tell for sure.",
                    "label": 0
                },
                {
                    "sent": "I'm sort of skeptical.",
                    "label": 0
                },
                {
                    "sent": "I didn't expect this.",
                    "label": 0
                },
                {
                    "sent": "I really assumed that you know there are a lot of features that I have that are incredibly redundant, and I assumed that I was going to get better.",
                    "label": 0
                },
                {
                    "sent": "Results by doing feature selection, but I couldn't, I just couldn't do that.",
                    "label": 0
                },
                {
                    "sent": "So the most significant features, or the unigrams down based alignment feature set and of the core features the average maximum likelihood estimate was the most signif.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Can you quantify what the impact of using their car stickers over just dividers impossible next?",
                    "label": 0
                },
                {
                    "sent": "So I guess I didn't do an analysis that throughout all of the dependence and related features I should have done that.",
                    "label": 0
                },
                {
                    "sent": "But I did an analysis where I individually took each set from those 25 feature sets that I generated.",
                    "label": 0
                },
                {
                    "sent": "I took out each feature set individually and in each case performance got worse.",
                    "label": 0
                },
                {
                    "sent": "So they do seem to be having a positive effect, though it isn't a huge effect.",
                    "label": 0
                },
                {
                    "sent": "Actually, well, no.",
                    "label": 0
                },
                {
                    "sent": "I guess I would say that I probably don't.",
                    "label": 0
                },
                {
                    "sent": "I do have some bigram features, but they're not quite the same, so the way that the reason that I had generated those bigram features were really to do a word based alignment, and so the bigrams that I generated, I used the same second word.",
                    "label": 0
                },
                {
                    "sent": "Appended to the hypothesis and the taps.",
                    "label": 0
                },
                {
                    "sent": "So it wasn't a bigram from the hypothesis compared to a bigram from the text, but rather a word from the hypothesis and a word from the text, and then I take, I get the bigram by first doing the word following the one in the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "I put that behind both of them, and do a corpus analysis of that, etc.",
                    "label": 0
                },
                {
                    "sent": "You said it.",
                    "label": 0
                },
                {
                    "sent": "So the difference above.",
                    "label": 0
                },
                {
                    "sent": "My baseline, which was basically the Glickman ET al approach.",
                    "label": 0
                },
                {
                    "sent": "I think about half of that roughly was based on that set of features.",
                    "label": 0
                },
                {
                    "sent": "Support line.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}