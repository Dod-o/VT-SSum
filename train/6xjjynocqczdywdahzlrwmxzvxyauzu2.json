{
    "id": "6xjjynocqczdywdahzlrwmxzvxyauzu2",
    "title": "Guaranteed Non-convex Learning Algorithms through Tensor Factorization",
    "info": {
        "author": [
            "Animashree Anandkumar, University of California, Irvine"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_anandkumar_nonconvex_learning/",
    "segmentation": [
        [
            "Brian and thank the organizers for the invitation.",
            "It's a pleasure to be here.",
            "It's so great to see so much of energy in machine learning.",
            "So as we all know, we encounter nonconvex learning problems.",
            "You know almost always, right?",
            "So how do we overcome this?",
            "How do we overcome the problem of local Optima?",
            "How do we?",
            "When can we have guaranteed solutions?",
            "I'll show how tensor methods can offer net effective answer to this and how the lessons learned from tensor factorization could be extended to more general non convex optimization problems.",
            "So since we first started this work on tensor methods for probabilistic modeling, and lately we've also seen some interesting emerging applications of that in deep learning, so I'll try to cover a lot of ground in this talk without going into the, you know too many details, but I hope that'll convince you that tensors or something that everybody should think about, and you know, use in a variety of different creative ways.",
            "I think we've just touched the tip of the iceberg here."
        ],
        [
            "So as we all know, these are exciting times to be a machine learner.",
            "We have enormous datasets.",
            "We have growth in computational power and we're looking to solve very challenging problems.",
            "We've seen a lot of progress in the arena of supervised learning through mainly deep learning, but I would argue that there are still many open problems.",
            "One of the primary one being one of unsupervised learning, so we still have some ways to go to have.",
            "Machines making automated discoveries and to be able to automatically discover hidden information from data.",
            "So as I said."
        ],
        [
            "In the first part of the talk, Let's see how we can use the framework of probabilistic modeling to solve the problem of unsupervised learning.",
            "So here what you have is you first posit probabilistic model.",
            "In terms of like how you have hidden variables that relate to the observed data, right?",
            "For instance, a popular one is topic modeling where you want to learn about the hidden topics and documents based on observed words.",
            "So the challenging task is one of learning what is the right set of parameters that fits the data well and can make predictions in terms of what are the likely topics."
        ],
        [
            "So this becomes challenging as you have high dimensions, which is the regime of modern machine learning.",
            "So here shall typically what you have is a high dimensional observation right in the example of topic modeling, it's the size of the entire world vocabulary which can be of hundreds of thousands.",
            "And you can also have larger dimensions than that, whereas the information that you would like to extract is of typically much lower dimension, right so?",
            "This is the problem of finding a needle in a huge haystack, and this is what makes learning hard both in a computational sense and a statistical sense.",
            "So in general, you need lots of computation as well as lots of training samples.",
            "So are there ways to overcome this?",
            "Are there mechanisms that can award this explosion?"
        ],
        [
            "So the traditional approach has been typically to optimize some fixed objective function in the case of unsupervised learning, it's been the one of maximizing likelihood, right?",
            "But the problem is one of local Optima.",
            "So once you have hidden variables you have, you know, typically a large number of local Optima and local search methods such as gradient descent will tend to get stuck there, and this is specially bad in high dimensions.",
            "With increasing dimensions you can have an exponential number of critical points.",
            "So, are there ways to overcome this right?",
            "So are there ways to do better than some of the existing heuristics?",
            "So if you think about expectation maximization variational inference, they all aim to improve upon likelihood through different means.",
            "And at the same time, I would argue that many others learning mechanisms are also non convex and hence there is still the issue of local optimum.",
            "So are they."
        ],
        [
            "Means to overcome that."
        ],
        [
            "So what I'll show in this talk is for many problems you can overcome this by replacing the objective function.",
            "So one thing that differentiates machine learning from one of the things that differentiates machine learning from power, you know other fields such as optimization is we're not working on one fixed problem, right?",
            "We can be a lot more creative.",
            "We can use different kinds of structures and exploit even different kinds of information.",
            "To make the problem tractable, so one aspect would be to replace the objective function.",
            "So instead of looking at likelihood, we'll look at the notion of tensor decomposition.",
            "And what you can show is for many cases are in fact the global optimum is preserved with infinite samples, meaning the solution that you obtain from maximum likelihood would match the solution you would obtain by doing tensor decomposition.",
            "And hence you have a consistent solution.",
            "So here T hat will be the empirical tensor that's computed from data and T of Theatre will be a tensor that depends on the parameters and it typically has constraints such as being low rank, an matching.",
            "Those two tensors will allow us to learn the parameters.",
            "So now the question is, this is great.",
            "We've replaced the optimization problem, but even this new optimization problem of tensor decomposition is still nonconvex.",
            "So in what scenarios can we actually solve this?",
            "Can we actually obtain the globally optimal solution?",
            "Anne, what we've shown is that very simple algorithms that are very fast to implement will succeed in getting the globally optimal solution.",
            "And there are set of transparent conditions.",
            "And I'll argue that these conditions are natural for learning problems.",
            "To give you one example of such a condition, under the assumption that the data set has a small approximation error with the model class, we are fitting to.",
            "Then we show that you can obtain the globally optimal solution and this is quite natural to look for model classes that have low approximation error rather than a high one."
        ],
        [
            "So now let's dive into what tensor decomposition algorithms are.",
            "How do we go about designing that?",
            "And then I'll show how we can apply them in a variety of scenarios.",
            "You know, for probabilistic modeling as well as it's now emerging in deep learning as well."
        ],
        [
            "So before we get into tensors, we can do a lot with matrices, right?",
            "So why do we need to go beyond the matrix?",
            "So the most classical problem you can do with matrix decomposition is one of finding hidden factors.",
            "So this goes all the way back to Spearman in 1900s where he wanted to learn about the hidden factors such as verbal and mathematical intelligence of students by looking at a set of like scores in different tests.",
            "And so you have this core matrix and you want to break it down into various factors.",
            "So the natural way to do this is to look at each rank one component and say that that's one of the factors for intelligence."
        ],
        [
            "Right, so we can frame it in this manner, but what is the drawback?",
            "So you."
        ],
        [
            "So you can compute the sufficiently.",
            "The drawback is there are multiple possible representations.",
            "So you only have uniqueness and a very strong constraints of the components being orthogonal and having an eigen gap.",
            "Right, and that is, and so if these components are not orthogonal, you cannot recover them.",
            "On the other."
        ],
        [
            "Call back is you cannot have an overcomplete representation, meaning you cannot have more latent factors than the dimensionality of the matrix.",
            "That's because the rank is constrained by dimensionality.",
            "So now can we overcome these constraints right?",
            "And how do we do this?"
        ],
        [
            "So naturally there is no free lunch.",
            "So to overcome this we indeed know need more information as well.",
            "So one way to get more information is to design different kinds of tests.",
            "So in addition to like you know, now you have an oral exam and say a written exam, so you have multiple score matrices and now you can ask if I posit that there are common latent factors that appear for these different tests.",
            "You know, then I can have the notion of shared decomposition.",
            "So have the same factors appearing for both the decompositions.",
            "So now we can.",
            "This is one way to formalize this into a tensor decomposition.",
            "So essentially you have shared decomposition of different matrices.",
            "Now what you can do is collect them all together into a tensor.",
            "Right, so by doing this?"
        ],
        [
            "Now you can use the notion of shared decomposition to ask for a decomposition of the tensor.",
            "Where each component is rank one.",
            "So this is now an extension of the notion of matrix rank.",
            "Right, so here the rank one tensor is the outer product of three vectors in the same way that the rank one matrix is the outer product of two vectors.",
            "So you can now generalize the notion of matrix decomposition to one of tensor decomposition.",
            "So now naturally we have more constraints and more information than the case of a single matrix decomposition.",
            "So the question is, does this lead to better identify ability?",
            "So are there weaker conditions to have uniqueness of the factors of this decomposition?",
            "It turns out this is indeed."
        ],
        [
            "The case and this was studied all the way back by Kruskal in the 70s, and one of these sufficient conditions is for these components to be just linearly independent, whereas for this case of the matrix you have orthogonality constraints.",
            "So you have much weaker constraints for uniqueness of tensor decomposition compared to matrices.",
            "So this is great news.",
            "We can potentially extract a lot more information from tensors, but the question is."
        ],
        [
            "What about the algorithms, right?",
            "So do we have efficient algorithms to do that?",
            "We know matrix decomposition is nonconvex, but we can do it efficiently.",
            "Can we say the same about tensor decomposition?",
            "So as I said, there is no free lunch if you ask about general tensor decomposition.",
            "That's NP hard, but now we'll see for a natural class of tensors you can actually get efficient algorithms."
        ],
        [
            "So for this, we'll need the notion of tensor contractions, and this extends the notion of matrix product.",
            "So the idea of now a tensor contraction is you can multiply along different directions of the tensor.",
            "So here you're multiplying along two of the directions and what you get is a result that's a multi linear combination of those vectors.",
            "So now you can also extend this to higher order tensors, and in the most general Sir case you can transform one tensor to another by using this notion of tensor contraction.",
            "And in fact, you can think of many of the deep learning operations as well.",
            "In terms of these tensor contractions.",
            "So these are natural ways to look at multiple products in a compact manner."
        ],
        [
            "So let us now see how we can use the notion of tensor contraction to solve for the decomposition problem, right?",
            "So the goal is to find these rank one components that fit my input tensor."
        ],
        [
            "So the natural one is the power method.",
            "I think you are all aware of the matrix power method, where you take the matrix multiplied with a random vector, normalize and repeat, and we know with a random initialization you can get the top eigenvector.",
            "You can subtract it out an rippey tan, get the entire decomposition.",
            "So now the question is, can we say the same about tensors?",
            "So we have the natural notion of a tensor power method where we contract the tensor along two of the directions and we want to ask if we repeat this.",
            "Where does this converge to?",
            "Can I recover all the components?",
            "So it turns out if you take general tensors, this is not the case, so will first limit orthogonal tensors.",
            "So here the components will assume to be orthogonal.",
            "So my original goal was to go beyond orthogonality, but we'll see how solving for orthogonality will give us tools to solve the general case as well.",
            "So if I have an orthogonal tensor, so now what you can show is if I start with any of the components, say I initialized with the blue component.",
            "In that case I stay at the same place, right?",
            "Because the inner product with the other components is 0.",
            "So for orthogonal tensors we have the nice property that all the components of the decomposition are stationary points.",
            "But now the issue is if I randomly initialized, will I converge to these stationary points?",
            "So this is always challenging once you have nonlinear iterations and the tensor iteration is a nonlinear one."
        ],
        [
            "So it turns out that if you look for all stationary points, indeed there is an exponential number of them.",
            "So you see, already we are far away from the matrix regime.",
            "In the case of a matrix, we only have linear number of solutions and now we have an exponential number for tensors, and this is indeed the case for general nonconvex optimization as well.",
            "But now, for orthogonal tensors we have lot more structure and in fact we can analyze and argue that the only stable stationary points are the components.",
            "All the others are unstable, meaning there exist directions that will repel away from that point.",
            "So we can prove that random initialization will succeed in getting these stable components.",
            "So as you can see, even in this quite structured problem, it's a lot of subtle analysis to look at one nonconvex method succeed.",
            "And that's the window where you know trying to get with by looking at tensors, because they naturally extend some of the matrix methods, but still require a lot.",
            "Some more subtle analysis.",
            "So this is great news that for orthogonal tensors we can now recover this with the power method, right?",
            "So I can like randomly initialize, I will reach one of the components, I can subtract it out and proceed.",
            "So this way I can recover the orthogonal decomposition."
        ],
        [
            "So turns out you can now."
        ],
        [
            "So from here to a general tensor decomposition that need not be orthogonal, if you recall that was our goal to go to more general representations."
        ],
        [
            "And you can do this by first preprocessing the input tensor.",
            "So you have this input tensor.",
            "What you do is you multiply with a certain may."
        ],
        [
            "Tricks along each of the direction to get a new tensor that will be orthogonal.",
            "So what we are in effect doing is these are the original components.",
            "Here the red and the blue will find this orthogonal Ising matrix W that will generate knew vectors that are orthogonal.",
            "And once we have an orthogonal tensor, we can find the components through the power method.",
            "But what is the challenge?",
            "The challenge is, my goal is to find these components right, so I can't explicitly compute this matrix W. So can I now compute this orthogonal Ising transform from the tensor itself?",
            "So it turns out we can do that by looking again at the property of shared decompositions.",
            "So if you take any meat slice a matrix slice of the tensor, right that's represented like this by contracting it along one of the directions.",
            "What you get is a decomposition where the same components appear right because that's the property of having the shared decomposition.",
            "And now if you look at the singular value decomposition of that matrix, you're effectively orthogonalize ING these components so by looking at the SVD we can now compute this matrix W that will lead to this orthogonalization.",
            "So hence it's possible to transform this input into an orthogonal form and at the same time you can also do dimensionality reduction.",
            "So in many examples the rank is much smaller than dimension and you can now reduce this to a smaller tensor and do decomposition in this lower dimensional space.",
            "So this is effective both in a computational and statistical sense.",
            "Right, so this is, that's another benefit.",
            "So now the challenge is what?",
            "So we've got in this decomposition in the lower dimensional space we want to go back to the original space and for that we want the transformation to be invertible."
        ],
        [
            "And that happens precisely when the components are linearly independent.",
            "So if I have linear set of independent so set of linearly independent components, then I can recover that decomposition.",
            "So now you can see that we have set of efficient operations right?",
            "The singular value decomposition.",
            "There's this power iterations there matrix multiplications, so these set of simple operations allow us to give the best tensor decomposition.",
            "When you have this property of linear independence, and for many learning problems that turns out to be a mild condition.",
            "So now let's see how we can apply this algorithm in a range of different applications.",
            "So here I'll be somewhat quick because I want to show you the breadth of possibilities with tensors.",
            "So."
        ],
        [
            "First part will be to solve some of the popular probabilistic models, right?",
            "The one of the."
        ],
        [
            "Popular is topic modeling.",
            "You have a hidden set of topics in each document and the words are generated according to this topic.",
            "Word matrix.",
            "Can you recover the topic word matrix from the data?"
        ],
        [
            "So what we showed was if you look at Co occurrence of word triplets and decompose this tensor, you can talk your guarantee to recover a consistent set of parameters for the latent bearishly allocation model."
        ],
        [
            "Similarly, you can extract hidden communities in social networks are popular model.",
            "Is this mixed membership stochastic block model where nodes can belong to multiple communities and now by extracting tensor that looks at common friendships among triplets of nodes and decomposing it, we can find the hidden communities."
        ],
        [
            "Just to quickly show you how this is effective in practice, you know we had nice theoretical result, but what are the practical implications?",
            "We have the.",
            "Tensor methods available on Spark platform and we showed that for learning topics compared to variational inference, we are doing much better in terms of running time as well as likelihood.",
            "So and similarly is also the case for social networks across a range of different networks.",
            "We see that we are much faster and more accurate than variational inference and we have far more experiments in our papers.",
            "So the idea is that these tensor methods can vastly speed up probabilistic model."
        ],
        [
            "So what else can tensor methods do?",
            "So one of the you know we users of unsupervised learning, is to learn representations, right?",
            "This conference is about learning representations.",
            "So now how can we use tensor methods for that?",
            "A very popular model is one of sparse coding.",
            "You know there's strong evidence that neurons conduct sparse coding in its most simplified form.",
            "You can have this linear model where every sample is a sparse combination of dictionary elements."
        ],
        [
            "And what we showed in some of our works was we can correctly learn these overcomplete representations when the dictionary elements are incoherent, meaning they have weak correlation between them.",
            "And this is a natural assumption because you want the dictionary elements to be not redundant.",
            "And this requires analyzing now a decomposition of the tensor where the number of components can exceed the dimension.",
            "Right, where is the matrix?",
            "That's impossible, but with tensors we show that we can recover these components."
        ],
        [
            "You can also take this notion further.",
            "In many applications such as vision, you want the notion of shifting variance.",
            "Our other invariances, along with the notion of having a dictionary right, the natural model, is a convolutional one.",
            "So with this convolutional constraint will translate to one of having decomposition of the tensor where the components are tight to one another, so there."
        ],
        [
            "So one another and we can resolve these structures efficiently through FFT operations."
        ],
        [
            "Just to show some quick application of training this convolutional models with tensor methods.",
            "We applied this for learning sentence embeddings and see how it performs in paraphrase detection.",
            "So we tried this on the Microsoft Research corpus and what we did was we first reduce dimension and then fitted a convolutional model in this lower dimensional space and we use tensor methods for finding the filters of the model.",
            "And then decode the activations and use that for, you know through a linear SVM for looking at the paraphrase."
        ],
        [
            "And what we saw was we get a quite a close score F score compared to the recent Skip thought paper, but the difference is there is no out of corpus training, right?",
            "We've trained it on an extremely small set of examples.",
            "And you know able to get good performance.",
            "There are also other interesting notions of using tensor methods for embedding such as holographic embeddings for knowledge bases by Max, Nickel, and others, so there is a lot of we know interesting potential for generating embeddings through this notion of higher order relationships."
        ],
        [
            "So what else can we do with the hidden variable models?",
            "Another interesting application is one of reinforcement learning.",
            "Right?",
            "We all know this is an extremely challenging problem, and there's been a lot of exciting progress in recently, so one challenging aspect is to think about partially observable processes.",
            "So if they're if we model the His environment as a hidden state, you know this becomes more challenging.",
            "And what we show is if we now incorporate a palm DP framework, you can actually solve for these hidden variables through tensor methods.",
            "So we use the notion of memoryless policies to reduce complexity, and in many practical applications indeed you know you do not want a long history because designing policies becomes extremely hard.",
            "So we incorporate the tensor methods into an episodic learning framework and show the first regret bounds that you can actually learn.",
            "I know you can do efficient exploration exploitation with these tensor methods, and this will appear in call to this year.",
            "So to show so."
        ],
        [
            "Preliminary results for this popular Atari gameplay.",
            "What we're seeing is that tensor methods can potentially have a better reward compared to convolutional networks, and the intuition of why this could be possible is that these tensor methods are looking to 1st learn the latent representations and planning in the hidden space of a lower dimension could be more effective."
        ],
        [
            "So, so there is potential for tensor methods.",
            "You know, solve some of the challenging problems in reinforcement learning.",
            "So, so this is great.",
            "Tensor methods are effective in discovering hidden variables in a variety of applications.",
            "Now I want to."
        ],
        [
            "So some of the very recent work emerging results on how tensors are effective in a variety of deep learning frameworks."
        ],
        [
            "So one natural question is backpropagation is working great in a lot of practical applications, but is it really?",
            "You know, the only thing we can do can we go further than that?",
            "In fact, a paper that appeared here even states that few researchers would dare to train their models from scratch.",
            "So the issue of local Optima is strong one.",
            "So this toy example here shows that it's quite easy to construct local Optima.",
            "Right, so here there is a very seemingly simple classification task.",
            "You have this positive examples here and the negative examples.",
            "And in addition to the global optimum in green, which has zero training error, there is the local optimum in drag that has a substantial training error and you can actually increase the dimensions and show that there can be exponential number of such local optimum.",
            "So, are there mechanisms to overcome this local optimal?"
        ],
        [
            "So can we also use the notions of moments for training neural networks?",
            "So in a recent work, what we showed was by looking at different relationships between the input and output.",
            "It's possible to train a one layer neural network with one hidden layer with guarantees.",
            "So the idea is to look at transformations of the input using the so-called score functions.",
            "So if you look at the correlation between the output label Y and the score function of the input, what will show is we'll get a linear combination of the weight vectors of the first layer, so these are the weight vectors of the first layer.",
            "And what we can do is now construct a hierarchy of them.",
            "If we go to 2nd order score functions, we'll get a decomposition where each rank one component corresponds to one of the weight vectors.",
            "And similarly, now if you go to 3rd order moments, you can look at a decomposition where each component corresponds to again one of the weight vectors.",
            "So by resolving the tensor decomposition, it's possible to learn the weight vectors of the first layer of this two layer network.",
            "So the you know So what you know there is.",
            "As I said, there is always like requirement for something more.",
            "If we have to give the guarantees right.",
            "So here what we require is this notion of score functions to do the transformations.",
            "The idea is to overcome the nonlinearity of these neurons.",
            "Here you also need to transform the input and this transformation is coming in the form of these core functions and for that you.",
            "Marnie the generators of your input model, so this also gives a mechanism to transfer knowledge from generative models into discriminative training of neural networks.",
            "And to give you an example, if we have Gaussian input, you get the transformation in terms of Hermite polynomials.",
            "So at a basic level, the intuition is by looking at correlations between polynomial features of your input and correlating that with the output label.",
            "You get useful information about what weights would be effective in, you know, in a good neural network.",
            "So, so again, this is only the first step.",
            "I mean there would be.",
            "It would be interesting to see how this can extend to more layers or if there are other mechanisms to study the nonconvexity in neural networks."
        ],
        [
            "So I'll mention briefly some of the other recent works as well that have been looking at the use of tensors in deep learning, so one of the works that appeared in NIPS this year was to look at compression of the dense layers of neural networks using tensor representations.",
            "So this notion of a tensor train format effectively compresses these weights, and the idea is you can get a lot more compression than looking at the basic.",
            "Low rank representation of these weight matrices and you know there is potential for a huge compression rate, but based on looking at these tensor representations and the idea is I'm in, tensors are really a way to think about higher order or multilinear representations right?",
            "And that's why they could discover ritual dependencies in the weights he ran compress it more effectively."
        ],
        [
            "Another recent work that will appear in called this year is been to look at the analysis of, you know, based on tensor factorization to look at the expressive power of different neural networks.",
            "So here they used a different notion of tensor decomposition called the Tucker decomposition.",
            "So what I described so far is known as the CP decomposition right?",
            "Whereas the Tucker one is different from the fact that there can be an arbitrary code turn Sir.",
            "So the idea is you look at your input tensor and you decompose it into having a core tensor and its transformation with different factor matrices.",
            "Whereas in the in the case of the CP decomposition, this call tensor will be a diagonal one.",
            "And you can do a hierarchical Tucker decomposition by progressively decomposing this core into other representations, and so on.",
            "And I won't go into the details here, but you know they have algebraic argument to show that deep is better than shallow.",
            "The idea is if you want to represent a deep network based on these hierarchical Tucker decompositions into a shallow one, you would need exponentially more parameters, and so this is one of the analysis to show how you can argue for representational.",
            "Power off deeper networks.",
            "And to me that these connections are very interesting."
        ],
        [
            "So tensors have also been, you know, there have been some intriguing notions of using tensors for memory models.",
            "You know there's a recent work by Volker Tresp and others, and how you can use that for semantic decoding.",
            "An would be interesting to explore these directions further."
        ],
        [
            "So let me quickly wrap up to show like how we can go further in terms of bringing tensors, deep learning and nonconvex optimization together.",
            "What are the further steps?"
        ],
        [
            "So as I said, one natural direction is to really make these tensor methods scalable and have strong library support.",
            "So one aspect that would be very interesting to explore further is the idea of randomized catching.",
            "So we had a paper in NIPS where we show that through sketching you can avoid the exponential blowup as you increase the order of tensor, right?",
            "So if you naively computed a cube or a third order tensor, you would have cubic time and similarly for higher order, but it's possible to avoid this by doing.",
            "You know approximate calculations through randomized sketching.",
            "The other interesting aspect would be to come up with communication efficient schemes you know blocked tensor computations that extend naturally some of the matrix wants.",
            "So one strong, you know boost will be to think about extending the blast kernels.",
            "You know Blast has been around for a long time.",
            "They've been very effective for linear algebra, but answers are really extensions of, you know, linear algebra, right?",
            "So one of the recent works we've been exploring with NVIDIA researcher Chris Checker is to ask, can we extend this?",
            "Can we go?",
            "You know expose interfaces across different platforms that can really.",
            "Accelerat you know the hardware performance?",
            "And given that deep learning operations also involve tensor contractions, this would, I think, benefit a range of different applications.",
            "The other aspect is, you know, having strong library support for these tensor methods.",
            "Me and other researchers have released software, but I think we can still benefit from a much more.",
            "Like amount of participation and support for these methods."
        ],
        [
            "So going forward I describe like for tensor decomposition that the simple iterative schemes can be successful in recovery, but is there a more unified notion for nonconvex optimization?",
            "So one notion I've been recently interested in is to, you know, instead of trying to solve this nonconvex problem directly, can we have this notion of smoothing?",
            "So the idea of smoothing is to compute a certain transformation.",
            "Usually you know you smooth with the Gaussian kernel.",
            "And if you have a large enough amount of smoothing, you get a convex function, so you can use that as the initialization and follow the path back.",
            "I mean, this is also known as continuation method homotopy method.",
            "This method is old, right?",
            "But can we combine now of the power of this method with the local search techniques and come up with instances where there are guarantees, for instance one natural setting when this would be successful is when the global optimize stable enough.",
            "So if you require a large amount of noise to destroy the global optimum, then you know you can expect the homotopy methods to succeed there, and that all those are precisely the conditions we require for learning as well.",
            "You know, that's where generalization is good, so in a way like the experience that I've had so far is that many of the assumptions that you make to make the problem tractable are also precisely the kinds of problems we want to solve in learning, so they align well.",
            "In a natural way."
        ],
        [
            "And to see a show, some preliminary experiments by Hussain Mobile he from MIT.",
            "He showed that you know, this blue here is the nice stochastic gradient descent and the red.",
            "Here is the diffusion process to train recurrent neural networks and you can get a lot more improvement by using this notion of diffusion to speed up training as well as get a better generalization bound."
        ],
        [
            "Another aspect of Nonconvexity that has recently caught attention is one of saddle points, right?",
            "So far I've been focusing on how to overcome local Optima, but another issue that's also highly problematic and that indeed has been pointed out by other researchers, such as Bengio, Surya, Ganguly and others is one of saddle points.",
            "So these are points where there exist you know the gradient is 0, but there exists directions.",
            "Where you can escape, meaning the objective function can improve, But the problem is in high dimensions.",
            "In high dimensions, finding these directions of escape is challenging and that's why you can have stochastic gradient descent be slowing down in high dimensions.",
            "So is there a way to speed up stochastic gradient descent?",
            "An show that you can escape the saddle points in bounded time?",
            "So one aspect we recently addressed in our Cold Paper is to look at how to overcome higher order saddle.",
            "Points so these arise when there are degenerate critical points and these are an issue if you have over specified models.",
            "So if you have more model capacity than that's required to fit, you can run into these Tietjen."
        ],
        [
            "No seas, so it'll be interesting to see how we can incorporate some of those to speed up.",
            "You know non convex optimization."
        ],
        [
            "So I think I'll end the talk here.",
            "Over the years I've had the privilege to work with a number of highly lust Rias collaborators, as well as excellent students.",
            "Only a limited list is shown here, and you can access details about the various works that I provided at this website here.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brian and thank the organizers for the invitation.",
                    "label": 0
                },
                {
                    "sent": "It's a pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "It's so great to see so much of energy in machine learning.",
                    "label": 1
                },
                {
                    "sent": "So as we all know, we encounter nonconvex learning problems.",
                    "label": 0
                },
                {
                    "sent": "You know almost always, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we overcome this?",
                    "label": 0
                },
                {
                    "sent": "How do we overcome the problem of local Optima?",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "When can we have guaranteed solutions?",
                    "label": 0
                },
                {
                    "sent": "I'll show how tensor methods can offer net effective answer to this and how the lessons learned from tensor factorization could be extended to more general non convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "So since we first started this work on tensor methods for probabilistic modeling, and lately we've also seen some interesting emerging applications of that in deep learning, so I'll try to cover a lot of ground in this talk without going into the, you know too many details, but I hope that'll convince you that tensors or something that everybody should think about, and you know, use in a variety of different creative ways.",
                    "label": 0
                },
                {
                    "sent": "I think we've just touched the tip of the iceberg here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we all know, these are exciting times to be a machine learner.",
                    "label": 0
                },
                {
                    "sent": "We have enormous datasets.",
                    "label": 0
                },
                {
                    "sent": "We have growth in computational power and we're looking to solve very challenging problems.",
                    "label": 1
                },
                {
                    "sent": "We've seen a lot of progress in the arena of supervised learning through mainly deep learning, but I would argue that there are still many open problems.",
                    "label": 1
                },
                {
                    "sent": "One of the primary one being one of unsupervised learning, so we still have some ways to go to have.",
                    "label": 0
                },
                {
                    "sent": "Machines making automated discoveries and to be able to automatically discover hidden information from data.",
                    "label": 0
                },
                {
                    "sent": "So as I said.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the first part of the talk, Let's see how we can use the framework of probabilistic modeling to solve the problem of unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So here what you have is you first posit probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "In terms of like how you have hidden variables that relate to the observed data, right?",
                    "label": 0
                },
                {
                    "sent": "For instance, a popular one is topic modeling where you want to learn about the hidden topics and documents based on observed words.",
                    "label": 0
                },
                {
                    "sent": "So the challenging task is one of learning what is the right set of parameters that fits the data well and can make predictions in terms of what are the likely topics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this becomes challenging as you have high dimensions, which is the regime of modern machine learning.",
                    "label": 0
                },
                {
                    "sent": "So here shall typically what you have is a high dimensional observation right in the example of topic modeling, it's the size of the entire world vocabulary which can be of hundreds of thousands.",
                    "label": 1
                },
                {
                    "sent": "And you can also have larger dimensions than that, whereas the information that you would like to extract is of typically much lower dimension, right so?",
                    "label": 0
                },
                {
                    "sent": "This is the problem of finding a needle in a huge haystack, and this is what makes learning hard both in a computational sense and a statistical sense.",
                    "label": 1
                },
                {
                    "sent": "So in general, you need lots of computation as well as lots of training samples.",
                    "label": 0
                },
                {
                    "sent": "So are there ways to overcome this?",
                    "label": 0
                },
                {
                    "sent": "Are there mechanisms that can award this explosion?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the traditional approach has been typically to optimize some fixed objective function in the case of unsupervised learning, it's been the one of maximizing likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "But the problem is one of local Optima.",
                    "label": 1
                },
                {
                    "sent": "So once you have hidden variables you have, you know, typically a large number of local Optima and local search methods such as gradient descent will tend to get stuck there, and this is specially bad in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "With increasing dimensions you can have an exponential number of critical points.",
                    "label": 1
                },
                {
                    "sent": "So, are there ways to overcome this right?",
                    "label": 0
                },
                {
                    "sent": "So are there ways to do better than some of the existing heuristics?",
                    "label": 0
                },
                {
                    "sent": "So if you think about expectation maximization variational inference, they all aim to improve upon likelihood through different means.",
                    "label": 1
                },
                {
                    "sent": "And at the same time, I would argue that many others learning mechanisms are also non convex and hence there is still the issue of local optimum.",
                    "label": 0
                },
                {
                    "sent": "So are they.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means to overcome that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'll show in this talk is for many problems you can overcome this by replacing the objective function.",
                    "label": 0
                },
                {
                    "sent": "So one thing that differentiates machine learning from one of the things that differentiates machine learning from power, you know other fields such as optimization is we're not working on one fixed problem, right?",
                    "label": 0
                },
                {
                    "sent": "We can be a lot more creative.",
                    "label": 0
                },
                {
                    "sent": "We can use different kinds of structures and exploit even different kinds of information.",
                    "label": 0
                },
                {
                    "sent": "To make the problem tractable, so one aspect would be to replace the objective function.",
                    "label": 1
                },
                {
                    "sent": "So instead of looking at likelihood, we'll look at the notion of tensor decomposition.",
                    "label": 0
                },
                {
                    "sent": "And what you can show is for many cases are in fact the global optimum is preserved with infinite samples, meaning the solution that you obtain from maximum likelihood would match the solution you would obtain by doing tensor decomposition.",
                    "label": 1
                },
                {
                    "sent": "And hence you have a consistent solution.",
                    "label": 0
                },
                {
                    "sent": "So here T hat will be the empirical tensor that's computed from data and T of Theatre will be a tensor that depends on the parameters and it typically has constraints such as being low rank, an matching.",
                    "label": 0
                },
                {
                    "sent": "Those two tensors will allow us to learn the parameters.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, this is great.",
                    "label": 0
                },
                {
                    "sent": "We've replaced the optimization problem, but even this new optimization problem of tensor decomposition is still nonconvex.",
                    "label": 0
                },
                {
                    "sent": "So in what scenarios can we actually solve this?",
                    "label": 0
                },
                {
                    "sent": "Can we actually obtain the globally optimal solution?",
                    "label": 0
                },
                {
                    "sent": "Anne, what we've shown is that very simple algorithms that are very fast to implement will succeed in getting the globally optimal solution.",
                    "label": 1
                },
                {
                    "sent": "And there are set of transparent conditions.",
                    "label": 0
                },
                {
                    "sent": "And I'll argue that these conditions are natural for learning problems.",
                    "label": 0
                },
                {
                    "sent": "To give you one example of such a condition, under the assumption that the data set has a small approximation error with the model class, we are fitting to.",
                    "label": 0
                },
                {
                    "sent": "Then we show that you can obtain the globally optimal solution and this is quite natural to look for model classes that have low approximation error rather than a high one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's dive into what tensor decomposition algorithms are.",
                    "label": 0
                },
                {
                    "sent": "How do we go about designing that?",
                    "label": 0
                },
                {
                    "sent": "And then I'll show how we can apply them in a variety of scenarios.",
                    "label": 0
                },
                {
                    "sent": "You know, for probabilistic modeling as well as it's now emerging in deep learning as well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before we get into tensors, we can do a lot with matrices, right?",
                    "label": 0
                },
                {
                    "sent": "So why do we need to go beyond the matrix?",
                    "label": 0
                },
                {
                    "sent": "So the most classical problem you can do with matrix decomposition is one of finding hidden factors.",
                    "label": 0
                },
                {
                    "sent": "So this goes all the way back to Spearman in 1900s where he wanted to learn about the hidden factors such as verbal and mathematical intelligence of students by looking at a set of like scores in different tests.",
                    "label": 1
                },
                {
                    "sent": "And so you have this core matrix and you want to break it down into various factors.",
                    "label": 0
                },
                {
                    "sent": "So the natural way to do this is to look at each rank one component and say that that's one of the factors for intelligence.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so we can frame it in this manner, but what is the drawback?",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can compute the sufficiently.",
                    "label": 0
                },
                {
                    "sent": "The drawback is there are multiple possible representations.",
                    "label": 0
                },
                {
                    "sent": "So you only have uniqueness and a very strong constraints of the components being orthogonal and having an eigen gap.",
                    "label": 0
                },
                {
                    "sent": "Right, and that is, and so if these components are not orthogonal, you cannot recover them.",
                    "label": 0
                },
                {
                    "sent": "On the other.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call back is you cannot have an overcomplete representation, meaning you cannot have more latent factors than the dimensionality of the matrix.",
                    "label": 0
                },
                {
                    "sent": "That's because the rank is constrained by dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So now can we overcome these constraints right?",
                    "label": 0
                },
                {
                    "sent": "And how do we do this?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So naturally there is no free lunch.",
                    "label": 0
                },
                {
                    "sent": "So to overcome this we indeed know need more information as well.",
                    "label": 0
                },
                {
                    "sent": "So one way to get more information is to design different kinds of tests.",
                    "label": 0
                },
                {
                    "sent": "So in addition to like you know, now you have an oral exam and say a written exam, so you have multiple score matrices and now you can ask if I posit that there are common latent factors that appear for these different tests.",
                    "label": 0
                },
                {
                    "sent": "You know, then I can have the notion of shared decomposition.",
                    "label": 0
                },
                {
                    "sent": "So have the same factors appearing for both the decompositions.",
                    "label": 0
                },
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "This is one way to formalize this into a tensor decomposition.",
                    "label": 1
                },
                {
                    "sent": "So essentially you have shared decomposition of different matrices.",
                    "label": 1
                },
                {
                    "sent": "Now what you can do is collect them all together into a tensor.",
                    "label": 0
                },
                {
                    "sent": "Right, so by doing this?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you can use the notion of shared decomposition to ask for a decomposition of the tensor.",
                    "label": 0
                },
                {
                    "sent": "Where each component is rank one.",
                    "label": 0
                },
                {
                    "sent": "So this is now an extension of the notion of matrix rank.",
                    "label": 0
                },
                {
                    "sent": "Right, so here the rank one tensor is the outer product of three vectors in the same way that the rank one matrix is the outer product of two vectors.",
                    "label": 1
                },
                {
                    "sent": "So you can now generalize the notion of matrix decomposition to one of tensor decomposition.",
                    "label": 1
                },
                {
                    "sent": "So now naturally we have more constraints and more information than the case of a single matrix decomposition.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does this lead to better identify ability?",
                    "label": 0
                },
                {
                    "sent": "So are there weaker conditions to have uniqueness of the factors of this decomposition?",
                    "label": 0
                },
                {
                    "sent": "It turns out this is indeed.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The case and this was studied all the way back by Kruskal in the 70s, and one of these sufficient conditions is for these components to be just linearly independent, whereas for this case of the matrix you have orthogonality constraints.",
                    "label": 0
                },
                {
                    "sent": "So you have much weaker constraints for uniqueness of tensor decomposition compared to matrices.",
                    "label": 1
                },
                {
                    "sent": "So this is great news.",
                    "label": 0
                },
                {
                    "sent": "We can potentially extract a lot more information from tensors, but the question is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about the algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "So do we have efficient algorithms to do that?",
                    "label": 0
                },
                {
                    "sent": "We know matrix decomposition is nonconvex, but we can do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "Can we say the same about tensor decomposition?",
                    "label": 1
                },
                {
                    "sent": "So as I said, there is no free lunch if you ask about general tensor decomposition.",
                    "label": 0
                },
                {
                    "sent": "That's NP hard, but now we'll see for a natural class of tensors you can actually get efficient algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this, we'll need the notion of tensor contractions, and this extends the notion of matrix product.",
                    "label": 1
                },
                {
                    "sent": "So the idea of now a tensor contraction is you can multiply along different directions of the tensor.",
                    "label": 0
                },
                {
                    "sent": "So here you're multiplying along two of the directions and what you get is a result that's a multi linear combination of those vectors.",
                    "label": 0
                },
                {
                    "sent": "So now you can also extend this to higher order tensors, and in the most general Sir case you can transform one tensor to another by using this notion of tensor contraction.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you can think of many of the deep learning operations as well.",
                    "label": 0
                },
                {
                    "sent": "In terms of these tensor contractions.",
                    "label": 0
                },
                {
                    "sent": "So these are natural ways to look at multiple products in a compact manner.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us now see how we can use the notion of tensor contraction to solve for the decomposition problem, right?",
                    "label": 0
                },
                {
                    "sent": "So the goal is to find these rank one components that fit my input tensor.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the natural one is the power method.",
                    "label": 0
                },
                {
                    "sent": "I think you are all aware of the matrix power method, where you take the matrix multiplied with a random vector, normalize and repeat, and we know with a random initialization you can get the top eigenvector.",
                    "label": 0
                },
                {
                    "sent": "You can subtract it out an rippey tan, get the entire decomposition.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, can we say the same about tensors?",
                    "label": 0
                },
                {
                    "sent": "So we have the natural notion of a tensor power method where we contract the tensor along two of the directions and we want to ask if we repeat this.",
                    "label": 1
                },
                {
                    "sent": "Where does this converge to?",
                    "label": 0
                },
                {
                    "sent": "Can I recover all the components?",
                    "label": 0
                },
                {
                    "sent": "So it turns out if you take general tensors, this is not the case, so will first limit orthogonal tensors.",
                    "label": 0
                },
                {
                    "sent": "So here the components will assume to be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So my original goal was to go beyond orthogonality, but we'll see how solving for orthogonality will give us tools to solve the general case as well.",
                    "label": 0
                },
                {
                    "sent": "So if I have an orthogonal tensor, so now what you can show is if I start with any of the components, say I initialized with the blue component.",
                    "label": 0
                },
                {
                    "sent": "In that case I stay at the same place, right?",
                    "label": 0
                },
                {
                    "sent": "Because the inner product with the other components is 0.",
                    "label": 1
                },
                {
                    "sent": "So for orthogonal tensors we have the nice property that all the components of the decomposition are stationary points.",
                    "label": 0
                },
                {
                    "sent": "But now the issue is if I randomly initialized, will I converge to these stationary points?",
                    "label": 0
                },
                {
                    "sent": "So this is always challenging once you have nonlinear iterations and the tensor iteration is a nonlinear one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out that if you look for all stationary points, indeed there is an exponential number of them.",
                    "label": 0
                },
                {
                    "sent": "So you see, already we are far away from the matrix regime.",
                    "label": 0
                },
                {
                    "sent": "In the case of a matrix, we only have linear number of solutions and now we have an exponential number for tensors, and this is indeed the case for general nonconvex optimization as well.",
                    "label": 0
                },
                {
                    "sent": "But now, for orthogonal tensors we have lot more structure and in fact we can analyze and argue that the only stable stationary points are the components.",
                    "label": 0
                },
                {
                    "sent": "All the others are unstable, meaning there exist directions that will repel away from that point.",
                    "label": 0
                },
                {
                    "sent": "So we can prove that random initialization will succeed in getting these stable components.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, even in this quite structured problem, it's a lot of subtle analysis to look at one nonconvex method succeed.",
                    "label": 0
                },
                {
                    "sent": "And that's the window where you know trying to get with by looking at tensors, because they naturally extend some of the matrix methods, but still require a lot.",
                    "label": 0
                },
                {
                    "sent": "Some more subtle analysis.",
                    "label": 0
                },
                {
                    "sent": "So this is great news that for orthogonal tensors we can now recover this with the power method, right?",
                    "label": 0
                },
                {
                    "sent": "So I can like randomly initialize, I will reach one of the components, I can subtract it out and proceed.",
                    "label": 0
                },
                {
                    "sent": "So this way I can recover the orthogonal decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So turns out you can now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from here to a general tensor decomposition that need not be orthogonal, if you recall that was our goal to go to more general representations.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can do this by first preprocessing the input tensor.",
                    "label": 0
                },
                {
                    "sent": "So you have this input tensor.",
                    "label": 1
                },
                {
                    "sent": "What you do is you multiply with a certain may.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks along each of the direction to get a new tensor that will be orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So what we are in effect doing is these are the original components.",
                    "label": 0
                },
                {
                    "sent": "Here the red and the blue will find this orthogonal Ising matrix W that will generate knew vectors that are orthogonal.",
                    "label": 0
                },
                {
                    "sent": "And once we have an orthogonal tensor, we can find the components through the power method.",
                    "label": 0
                },
                {
                    "sent": "But what is the challenge?",
                    "label": 0
                },
                {
                    "sent": "The challenge is, my goal is to find these components right, so I can't explicitly compute this matrix W. So can I now compute this orthogonal Ising transform from the tensor itself?",
                    "label": 0
                },
                {
                    "sent": "So it turns out we can do that by looking again at the property of shared decompositions.",
                    "label": 0
                },
                {
                    "sent": "So if you take any meat slice a matrix slice of the tensor, right that's represented like this by contracting it along one of the directions.",
                    "label": 0
                },
                {
                    "sent": "What you get is a decomposition where the same components appear right because that's the property of having the shared decomposition.",
                    "label": 0
                },
                {
                    "sent": "And now if you look at the singular value decomposition of that matrix, you're effectively orthogonalize ING these components so by looking at the SVD we can now compute this matrix W that will lead to this orthogonalization.",
                    "label": 0
                },
                {
                    "sent": "So hence it's possible to transform this input into an orthogonal form and at the same time you can also do dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So in many examples the rank is much smaller than dimension and you can now reduce this to a smaller tensor and do decomposition in this lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is effective both in a computational and statistical sense.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is, that's another benefit.",
                    "label": 0
                },
                {
                    "sent": "So now the challenge is what?",
                    "label": 0
                },
                {
                    "sent": "So we've got in this decomposition in the lower dimensional space we want to go back to the original space and for that we want the transformation to be invertible.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that happens precisely when the components are linearly independent.",
                    "label": 1
                },
                {
                    "sent": "So if I have linear set of independent so set of linearly independent components, then I can recover that decomposition.",
                    "label": 0
                },
                {
                    "sent": "So now you can see that we have set of efficient operations right?",
                    "label": 1
                },
                {
                    "sent": "The singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "There's this power iterations there matrix multiplications, so these set of simple operations allow us to give the best tensor decomposition.",
                    "label": 0
                },
                {
                    "sent": "When you have this property of linear independence, and for many learning problems that turns out to be a mild condition.",
                    "label": 0
                },
                {
                    "sent": "So now let's see how we can apply this algorithm in a range of different applications.",
                    "label": 0
                },
                {
                    "sent": "So here I'll be somewhat quick because I want to show you the breadth of possibilities with tensors.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First part will be to solve some of the popular probabilistic models, right?",
                    "label": 0
                },
                {
                    "sent": "The one of the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Popular is topic modeling.",
                    "label": 0
                },
                {
                    "sent": "You have a hidden set of topics in each document and the words are generated according to this topic.",
                    "label": 0
                },
                {
                    "sent": "Word matrix.",
                    "label": 0
                },
                {
                    "sent": "Can you recover the topic word matrix from the data?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we showed was if you look at Co occurrence of word triplets and decompose this tensor, you can talk your guarantee to recover a consistent set of parameters for the latent bearishly allocation model.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, you can extract hidden communities in social networks are popular model.",
                    "label": 0
                },
                {
                    "sent": "Is this mixed membership stochastic block model where nodes can belong to multiple communities and now by extracting tensor that looks at common friendships among triplets of nodes and decomposing it, we can find the hidden communities.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to quickly show you how this is effective in practice, you know we had nice theoretical result, but what are the practical implications?",
                    "label": 0
                },
                {
                    "sent": "We have the.",
                    "label": 0
                },
                {
                    "sent": "Tensor methods available on Spark platform and we showed that for learning topics compared to variational inference, we are doing much better in terms of running time as well as likelihood.",
                    "label": 1
                },
                {
                    "sent": "So and similarly is also the case for social networks across a range of different networks.",
                    "label": 0
                },
                {
                    "sent": "We see that we are much faster and more accurate than variational inference and we have far more experiments in our papers.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that these tensor methods can vastly speed up probabilistic model.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what else can tensor methods do?",
                    "label": 0
                },
                {
                    "sent": "So one of the you know we users of unsupervised learning, is to learn representations, right?",
                    "label": 0
                },
                {
                    "sent": "This conference is about learning representations.",
                    "label": 1
                },
                {
                    "sent": "So now how can we use tensor methods for that?",
                    "label": 1
                },
                {
                    "sent": "A very popular model is one of sparse coding.",
                    "label": 0
                },
                {
                    "sent": "You know there's strong evidence that neurons conduct sparse coding in its most simplified form.",
                    "label": 0
                },
                {
                    "sent": "You can have this linear model where every sample is a sparse combination of dictionary elements.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we showed in some of our works was we can correctly learn these overcomplete representations when the dictionary elements are incoherent, meaning they have weak correlation between them.",
                    "label": 0
                },
                {
                    "sent": "And this is a natural assumption because you want the dictionary elements to be not redundant.",
                    "label": 0
                },
                {
                    "sent": "And this requires analyzing now a decomposition of the tensor where the number of components can exceed the dimension.",
                    "label": 0
                },
                {
                    "sent": "Right, where is the matrix?",
                    "label": 0
                },
                {
                    "sent": "That's impossible, but with tensors we show that we can recover these components.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also take this notion further.",
                    "label": 0
                },
                {
                    "sent": "In many applications such as vision, you want the notion of shifting variance.",
                    "label": 0
                },
                {
                    "sent": "Our other invariances, along with the notion of having a dictionary right, the natural model, is a convolutional one.",
                    "label": 0
                },
                {
                    "sent": "So with this convolutional constraint will translate to one of having decomposition of the tensor where the components are tight to one another, so there.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one another and we can resolve these structures efficiently through FFT operations.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to show some quick application of training this convolutional models with tensor methods.",
                    "label": 1
                },
                {
                    "sent": "We applied this for learning sentence embeddings and see how it performs in paraphrase detection.",
                    "label": 0
                },
                {
                    "sent": "So we tried this on the Microsoft Research corpus and what we did was we first reduce dimension and then fitted a convolutional model in this lower dimensional space and we use tensor methods for finding the filters of the model.",
                    "label": 0
                },
                {
                    "sent": "And then decode the activations and use that for, you know through a linear SVM for looking at the paraphrase.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we saw was we get a quite a close score F score compared to the recent Skip thought paper, but the difference is there is no out of corpus training, right?",
                    "label": 0
                },
                {
                    "sent": "We've trained it on an extremely small set of examples.",
                    "label": 0
                },
                {
                    "sent": "And you know able to get good performance.",
                    "label": 0
                },
                {
                    "sent": "There are also other interesting notions of using tensor methods for embedding such as holographic embeddings for knowledge bases by Max, Nickel, and others, so there is a lot of we know interesting potential for generating embeddings through this notion of higher order relationships.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what else can we do with the hidden variable models?",
                    "label": 0
                },
                {
                    "sent": "Another interesting application is one of reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 1
                },
                {
                    "sent": "We all know this is an extremely challenging problem, and there's been a lot of exciting progress in recently, so one challenging aspect is to think about partially observable processes.",
                    "label": 1
                },
                {
                    "sent": "So if they're if we model the His environment as a hidden state, you know this becomes more challenging.",
                    "label": 0
                },
                {
                    "sent": "And what we show is if we now incorporate a palm DP framework, you can actually solve for these hidden variables through tensor methods.",
                    "label": 1
                },
                {
                    "sent": "So we use the notion of memoryless policies to reduce complexity, and in many practical applications indeed you know you do not want a long history because designing policies becomes extremely hard.",
                    "label": 0
                },
                {
                    "sent": "So we incorporate the tensor methods into an episodic learning framework and show the first regret bounds that you can actually learn.",
                    "label": 0
                },
                {
                    "sent": "I know you can do efficient exploration exploitation with these tensor methods, and this will appear in call to this year.",
                    "label": 0
                },
                {
                    "sent": "So to show so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Preliminary results for this popular Atari gameplay.",
                    "label": 0
                },
                {
                    "sent": "What we're seeing is that tensor methods can potentially have a better reward compared to convolutional networks, and the intuition of why this could be possible is that these tensor methods are looking to 1st learn the latent representations and planning in the hidden space of a lower dimension could be more effective.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so there is potential for tensor methods.",
                    "label": 1
                },
                {
                    "sent": "You know, solve some of the challenging problems in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So, so this is great.",
                    "label": 0
                },
                {
                    "sent": "Tensor methods are effective in discovering hidden variables in a variety of applications.",
                    "label": 0
                },
                {
                    "sent": "Now I want to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some of the very recent work emerging results on how tensors are effective in a variety of deep learning frameworks.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one natural question is backpropagation is working great in a lot of practical applications, but is it really?",
                    "label": 0
                },
                {
                    "sent": "You know, the only thing we can do can we go further than that?",
                    "label": 0
                },
                {
                    "sent": "In fact, a paper that appeared here even states that few researchers would dare to train their models from scratch.",
                    "label": 1
                },
                {
                    "sent": "So the issue of local Optima is strong one.",
                    "label": 0
                },
                {
                    "sent": "So this toy example here shows that it's quite easy to construct local Optima.",
                    "label": 0
                },
                {
                    "sent": "Right, so here there is a very seemingly simple classification task.",
                    "label": 0
                },
                {
                    "sent": "You have this positive examples here and the negative examples.",
                    "label": 0
                },
                {
                    "sent": "And in addition to the global optimum in green, which has zero training error, there is the local optimum in drag that has a substantial training error and you can actually increase the dimensions and show that there can be exponential number of such local optimum.",
                    "label": 0
                },
                {
                    "sent": "So, are there mechanisms to overcome this local optimal?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So can we also use the notions of moments for training neural networks?",
                    "label": 0
                },
                {
                    "sent": "So in a recent work, what we showed was by looking at different relationships between the input and output.",
                    "label": 0
                },
                {
                    "sent": "It's possible to train a one layer neural network with one hidden layer with guarantees.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to look at transformations of the input using the so-called score functions.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the correlation between the output label Y and the score function of the input, what will show is we'll get a linear combination of the weight vectors of the first layer, so these are the weight vectors of the first layer.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is now construct a hierarchy of them.",
                    "label": 0
                },
                {
                    "sent": "If we go to 2nd order score functions, we'll get a decomposition where each rank one component corresponds to one of the weight vectors.",
                    "label": 0
                },
                {
                    "sent": "And similarly, now if you go to 3rd order moments, you can look at a decomposition where each component corresponds to again one of the weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So by resolving the tensor decomposition, it's possible to learn the weight vectors of the first layer of this two layer network.",
                    "label": 0
                },
                {
                    "sent": "So the you know So what you know there is.",
                    "label": 0
                },
                {
                    "sent": "As I said, there is always like requirement for something more.",
                    "label": 0
                },
                {
                    "sent": "If we have to give the guarantees right.",
                    "label": 1
                },
                {
                    "sent": "So here what we require is this notion of score functions to do the transformations.",
                    "label": 0
                },
                {
                    "sent": "The idea is to overcome the nonlinearity of these neurons.",
                    "label": 0
                },
                {
                    "sent": "Here you also need to transform the input and this transformation is coming in the form of these core functions and for that you.",
                    "label": 1
                },
                {
                    "sent": "Marnie the generators of your input model, so this also gives a mechanism to transfer knowledge from generative models into discriminative training of neural networks.",
                    "label": 0
                },
                {
                    "sent": "And to give you an example, if we have Gaussian input, you get the transformation in terms of Hermite polynomials.",
                    "label": 0
                },
                {
                    "sent": "So at a basic level, the intuition is by looking at correlations between polynomial features of your input and correlating that with the output label.",
                    "label": 0
                },
                {
                    "sent": "You get useful information about what weights would be effective in, you know, in a good neural network.",
                    "label": 0
                },
                {
                    "sent": "So, so again, this is only the first step.",
                    "label": 0
                },
                {
                    "sent": "I mean there would be.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to see how this can extend to more layers or if there are other mechanisms to study the nonconvexity in neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll mention briefly some of the other recent works as well that have been looking at the use of tensors in deep learning, so one of the works that appeared in NIPS this year was to look at compression of the dense layers of neural networks using tensor representations.",
                    "label": 1
                },
                {
                    "sent": "So this notion of a tensor train format effectively compresses these weights, and the idea is you can get a lot more compression than looking at the basic.",
                    "label": 0
                },
                {
                    "sent": "Low rank representation of these weight matrices and you know there is potential for a huge compression rate, but based on looking at these tensor representations and the idea is I'm in, tensors are really a way to think about higher order or multilinear representations right?",
                    "label": 1
                },
                {
                    "sent": "And that's why they could discover ritual dependencies in the weights he ran compress it more effectively.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another recent work that will appear in called this year is been to look at the analysis of, you know, based on tensor factorization to look at the expressive power of different neural networks.",
                    "label": 1
                },
                {
                    "sent": "So here they used a different notion of tensor decomposition called the Tucker decomposition.",
                    "label": 0
                },
                {
                    "sent": "So what I described so far is known as the CP decomposition right?",
                    "label": 0
                },
                {
                    "sent": "Whereas the Tucker one is different from the fact that there can be an arbitrary code turn Sir.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you look at your input tensor and you decompose it into having a core tensor and its transformation with different factor matrices.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the in the case of the CP decomposition, this call tensor will be a diagonal one.",
                    "label": 0
                },
                {
                    "sent": "And you can do a hierarchical Tucker decomposition by progressively decomposing this core into other representations, and so on.",
                    "label": 0
                },
                {
                    "sent": "And I won't go into the details here, but you know they have algebraic argument to show that deep is better than shallow.",
                    "label": 1
                },
                {
                    "sent": "The idea is if you want to represent a deep network based on these hierarchical Tucker decompositions into a shallow one, you would need exponentially more parameters, and so this is one of the analysis to show how you can argue for representational.",
                    "label": 0
                },
                {
                    "sent": "Power off deeper networks.",
                    "label": 0
                },
                {
                    "sent": "And to me that these connections are very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So tensors have also been, you know, there have been some intriguing notions of using tensors for memory models.",
                    "label": 0
                },
                {
                    "sent": "You know there's a recent work by Volker Tresp and others, and how you can use that for semantic decoding.",
                    "label": 0
                },
                {
                    "sent": "An would be interesting to explore these directions further.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me quickly wrap up to show like how we can go further in terms of bringing tensors, deep learning and nonconvex optimization together.",
                    "label": 0
                },
                {
                    "sent": "What are the further steps?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, one natural direction is to really make these tensor methods scalable and have strong library support.",
                    "label": 0
                },
                {
                    "sent": "So one aspect that would be very interesting to explore further is the idea of randomized catching.",
                    "label": 0
                },
                {
                    "sent": "So we had a paper in NIPS where we show that through sketching you can avoid the exponential blowup as you increase the order of tensor, right?",
                    "label": 0
                },
                {
                    "sent": "So if you naively computed a cube or a third order tensor, you would have cubic time and similarly for higher order, but it's possible to avoid this by doing.",
                    "label": 0
                },
                {
                    "sent": "You know approximate calculations through randomized sketching.",
                    "label": 0
                },
                {
                    "sent": "The other interesting aspect would be to come up with communication efficient schemes you know blocked tensor computations that extend naturally some of the matrix wants.",
                    "label": 0
                },
                {
                    "sent": "So one strong, you know boost will be to think about extending the blast kernels.",
                    "label": 0
                },
                {
                    "sent": "You know Blast has been around for a long time.",
                    "label": 0
                },
                {
                    "sent": "They've been very effective for linear algebra, but answers are really extensions of, you know, linear algebra, right?",
                    "label": 1
                },
                {
                    "sent": "So one of the recent works we've been exploring with NVIDIA researcher Chris Checker is to ask, can we extend this?",
                    "label": 0
                },
                {
                    "sent": "Can we go?",
                    "label": 0
                },
                {
                    "sent": "You know expose interfaces across different platforms that can really.",
                    "label": 0
                },
                {
                    "sent": "Accelerat you know the hardware performance?",
                    "label": 0
                },
                {
                    "sent": "And given that deep learning operations also involve tensor contractions, this would, I think, benefit a range of different applications.",
                    "label": 1
                },
                {
                    "sent": "The other aspect is, you know, having strong library support for these tensor methods.",
                    "label": 0
                },
                {
                    "sent": "Me and other researchers have released software, but I think we can still benefit from a much more.",
                    "label": 0
                },
                {
                    "sent": "Like amount of participation and support for these methods.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So going forward I describe like for tensor decomposition that the simple iterative schemes can be successful in recovery, but is there a more unified notion for nonconvex optimization?",
                    "label": 1
                },
                {
                    "sent": "So one notion I've been recently interested in is to, you know, instead of trying to solve this nonconvex problem directly, can we have this notion of smoothing?",
                    "label": 0
                },
                {
                    "sent": "So the idea of smoothing is to compute a certain transformation.",
                    "label": 0
                },
                {
                    "sent": "Usually you know you smooth with the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "And if you have a large enough amount of smoothing, you get a convex function, so you can use that as the initialization and follow the path back.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is also known as continuation method homotopy method.",
                    "label": 0
                },
                {
                    "sent": "This method is old, right?",
                    "label": 0
                },
                {
                    "sent": "But can we combine now of the power of this method with the local search techniques and come up with instances where there are guarantees, for instance one natural setting when this would be successful is when the global optimize stable enough.",
                    "label": 1
                },
                {
                    "sent": "So if you require a large amount of noise to destroy the global optimum, then you know you can expect the homotopy methods to succeed there, and that all those are precisely the conditions we require for learning as well.",
                    "label": 0
                },
                {
                    "sent": "You know, that's where generalization is good, so in a way like the experience that I've had so far is that many of the assumptions that you make to make the problem tractable are also precisely the kinds of problems we want to solve in learning, so they align well.",
                    "label": 0
                },
                {
                    "sent": "In a natural way.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to see a show, some preliminary experiments by Hussain Mobile he from MIT.",
                    "label": 0
                },
                {
                    "sent": "He showed that you know, this blue here is the nice stochastic gradient descent and the red.",
                    "label": 0
                },
                {
                    "sent": "Here is the diffusion process to train recurrent neural networks and you can get a lot more improvement by using this notion of diffusion to speed up training as well as get a better generalization bound.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another aspect of Nonconvexity that has recently caught attention is one of saddle points, right?",
                    "label": 1
                },
                {
                    "sent": "So far I've been focusing on how to overcome local Optima, but another issue that's also highly problematic and that indeed has been pointed out by other researchers, such as Bengio, Surya, Ganguly and others is one of saddle points.",
                    "label": 0
                },
                {
                    "sent": "So these are points where there exist you know the gradient is 0, but there exists directions.",
                    "label": 1
                },
                {
                    "sent": "Where you can escape, meaning the objective function can improve, But the problem is in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "In high dimensions, finding these directions of escape is challenging and that's why you can have stochastic gradient descent be slowing down in high dimensions.",
                    "label": 1
                },
                {
                    "sent": "So is there a way to speed up stochastic gradient descent?",
                    "label": 0
                },
                {
                    "sent": "An show that you can escape the saddle points in bounded time?",
                    "label": 1
                },
                {
                    "sent": "So one aspect we recently addressed in our Cold Paper is to look at how to overcome higher order saddle.",
                    "label": 0
                },
                {
                    "sent": "Points so these arise when there are degenerate critical points and these are an issue if you have over specified models.",
                    "label": 0
                },
                {
                    "sent": "So if you have more model capacity than that's required to fit, you can run into these Tietjen.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No seas, so it'll be interesting to see how we can incorporate some of those to speed up.",
                    "label": 0
                },
                {
                    "sent": "You know non convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I'll end the talk here.",
                    "label": 0
                },
                {
                    "sent": "Over the years I've had the privilege to work with a number of highly lust Rias collaborators, as well as excellent students.",
                    "label": 0
                },
                {
                    "sent": "Only a limited list is shown here, and you can access details about the various works that I provided at this website here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}