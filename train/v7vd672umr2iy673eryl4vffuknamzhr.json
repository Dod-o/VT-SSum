{
    "id": "v7vd672umr2iy673eryl4vffuknamzhr",
    "title": "Beyond the line of sight: labeling the underlying surfaces",
    "info": {
        "author": [
            "Ruiqi Guo, Department of Computer Science at Illinois"
        ],
        "chairman": [
            "Aude Oliva, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT",
            "Silvio Savarese, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2012_guo_labeling/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "My name is Richie and this is joint work with my advisor, Professor Deborah Coyne."
        ],
        [
            "Now be presenting this paper.",
            "Usually vision systems treat the problem of seeing understanding as apartment of labeling visible part of the surface is in two different object categories, such in this case we are labeling visible part of the sidewalk into yellow and visible part of the road into grain.",
            "But we can see there's a lot of things missing here because of the occlusions that comes from the bicycles and the vehicles, and if based on this kind of parsing result we."
        ],
        [
            "To ask some questions like whether this person is standing on the sidewalk or how do we get there, then we run into problems because we do not know what's behind those occlusions.",
            "To facilitate this kind of inference problem, we think we need another kind of scene representation."
        ],
        [
            "That would look like this.",
            "So this is labeling the complete extent of the underlying surface is, and now we can clearly see what's behind the occlusion and the inference problem will be a lot more straightforward."
        ],
        [
            "Let's see another example in the image space of this image.",
            "If we label the visible surfaces then we get around 5:00 or 10% of the occlusions.",
            "But if we project this into the overhead view, then we can see there are a lot of in navigable areas which will make problems for applications like robotics.",
            "If we have the complete extent of the assistive services, however, then we are free of this problem.",
            "That's why we prefer to label the surface is into its complete extent instead of just the visible part.",
            "That brings us to."
        ],
        [
            "The problem statement, so we will start with some image parsing and object.",
            "Detections may or may not be noisy and then we want to infer the whole extent of the online background surface is what we also want is some structural prediction of the whole theme giving a rough idea about what the layout looks like using those Polygon."
        ],
        [
            "There are works that relate to our problem, especially image parsing papers, which deals with some labeling.",
            "The visible surface is there also seeing layout us estimation papers which which take advantage of handcrafted sometimes like some specific prior such as boxy constraint in our indoor room and use that to infer the surface is we want something more general so.",
            "Here's our framework."
        ],
        [
            "We have the."
        ],
        [
            "Image and then we extract some features from the image, including detector responses and the parsing result."
        ],
        [
            "Next we will apply a context to influence, so this is using the idea that spatial surrounding contexts can tell us about what's behind the occlusion.",
            "So we can look at each of the position and image, take the features from the neighborhood and use that to predict the background label."
        ],
        [
            "After the contextual inference will get this label map and then we will explore our second idea which says complete regions have some certain ships.",
            "Usually we think of background services stuff as shapeless.",
            "It's quite true if they are often included, but if you look at the uncluded complete extent of those services, but they will actually have some shape, so an included piece of sidewalk or NPC included.",
            "Road we will look like triangles and buildings if they are not included then they will have this quarter radical look so we can find from the training images the polygons that matched the label map.",
            "We just predict it and then we will transfer those polygons.",
            "Use that as a shape prior for the image we are interested."
        ],
        [
            "We will use this shape prior to refine our result.",
            "Do another round of contextual inference and give out."
        ],
        [
            "Final result.",
            "Now let's go into the details for the visual cues we use off the shelf image parsing algorithms as well as pre trained object detectors.",
            "We use their responses as labeled confidence map as features and we have one of those feature Maps for each of the parsing labels and object categories we care about."
        ],
        [
            "Next we apply a contexual feedforward algorithm.",
            "So this is a modified version of auto context.",
            "That has been published a couple years ago.",
            "We start with visual cues for each position in the image.",
            "We will look at its radio neighborhood.",
            "And aggregate the features from the feature map.",
            "Use that trainer Discriptive classifier an predict the background label."
        ],
        [
            "Starting from the next round, not only we have the visual cues, we also have the feedforward context from the previous round of background prediction.",
            "Now we're doing contextual influence on both the visual cues and the feed for context, will train another descriptive classifier and predict the background prediction.",
            "Because we add a new information here, so this prediction will be different from what it is in the previous round.",
            "Will do this for a couple of iterations until it converges."
        ],
        [
            "By the end of the context inference we have the label map.",
            "Now what we're going to do is try to find polygons that describe the shape of this label map.",
            "What we do is look at the probability of sidewalk in this area and find that it really looks like one of the polygons that appear in the training images and we can retrieve those polygons using the criteria of weighted intersection over Union.",
            "We use this kind of retrieved Polygon as the shape prior, for example in the.",
            "Sidewalk region.",
            "Here we retrieve polygons for each of the classes separately and overlaid."
        ],
        [
            "Those polygons to create a rough estimation of the same layout.",
            "This will be used as a shape prior to improve the final result."
        ],
        [
            "So now let's see how things change during the different iterations.",
            "To begin with, we only have the visual cues, where occlusion is fairly obvious.",
            "Now it's the contextual inference goes.",
            "We become more and more confident about what's being included.",
            "And finally we apply our shape prior, which give the label map more simpler.",
            "Look, look like triangles compared to what it was before."
        ],
        [
            "And that brings us to our final result.",
            "We have the complete labeling of the underlying services along with the top one and two.",
            "My estimation for the scene.",
            "Note that this kind of structure prediction of the scene layout really is really preserve the simplicity of the original human annotators, and sometimes it gives different interpretations.",
            "From those different gases.",
            "So in this case the 1st guest say there's only one side walk region here, and the second guess say there might be 2 pieces of different sidewalk on both sides of the."
        ],
        [
            "For evaluation we did on three datasets.",
            "Stressing indoors and sell online.",
            "These datasets are not initially created for the purpose of underlying surface prediction, but we can use them because they annotate things in polygons.",
            "Secondly, they when they annotate the background regions, they annotate to complete the extent as if the foreground objects are not there.",
            "So when we do training and testing, we can just ignore the foreground polygons and use the background polygons to create our ground truth for the online services."
        ],
        [
            "Here's how things look like.",
            "The switching data set.",
            "The first column is a ground truth and we can see.",
            "Although there are cars on the street, the road are indeed labeled to its full extent, as if the cars are not there.",
            "An second column is a visualization of the visible cues we use, including parsing and detection.",
            "An third column for Skull and our our underlying Surface predictions overlaid with the Polygon estimation of the layout."
        ],
        [
            "Act this is a result from the ground truth data of the indoor scene data."
        ],
        [
            "And this is from the summer line.",
            "Datasets online has a lot more foreground clusters, but we can handle that."
        ],
        [
            "For quantitative evaluation, we created three base lines, all of which use the same features as we use in our algorithm.",
            "The most confident one predicts directly from the visual information, the most confident background label nears approach, fusing all the pixels that has been labeled as foreground objects using the nearest background label and graph cut does contrast sensitive smoothing and enforces label smoothness."
        ],
        [
            "Here's the pixel accuracy on the streets in data set.",
            "Although baselines we described perform similarly, an hour algorithm outperform and an overall accuracy by almost 4%, and the improvement is really drastic on the occluded part where we get almost 10% improvement.",
            "In terms of what things helped?",
            "Contacts and shape prior each contribute to some improvement, and they work the best when they are combined."
        ],
        [
            "His other two datasets.",
            "So the trend is similar.",
            "We have reasonable improvement in overall and the visible part and we got big improvement on the occluded."
        ],
        [
            "This is per class accuracy and we can look at what things got improved most.",
            "They are the regions like road and sidewalk in the outdoor areas and floor and walls in the indoor areas, so these are kind of the regions that often get included.",
            "That's why they got improved."
        ],
        [
            "So that brings us to our conclusion.",
            "We try to solve a problem that is slightly different from traditional image parsing.",
            "Now we are labeling the online services to its complete extent.",
            "We are also predicting some of the rough Polygon layout estimation.",
            "We had a general algorithm for this and it works well on all the three datasets we used and the code is available on our webpage so."
        ],
        [
            "You're welcome to check them out.",
            "That's it, thanks.",
            "I was wondering if your ship priors to which degree are view.",
            "Be invariant to the viewpoints.",
            "So anyway by suppose that you are able to estimate the viewpoint of the observer if there is a way to make those priors even simpler.",
            "Yes, exactly.",
            "I think those those Polygon layout we estimate may not be like consistent in terms of like geometry viewpoint, and it is absolutely valuable to users kind of viewpoint estimation to further improve the result.",
            "But this is like a.",
            "More general, like more robust things so well.",
            "Anyway, I would think that if you don't know you point you money to use a larger number of suppliers to accommodate all possible variations which.",
            "Might be affecting the overall performance of your framework, so I will will thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Richie and this is joint work with my advisor, Professor Deborah Coyne.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now be presenting this paper.",
                    "label": 0
                },
                {
                    "sent": "Usually vision systems treat the problem of seeing understanding as apartment of labeling visible part of the surface is in two different object categories, such in this case we are labeling visible part of the sidewalk into yellow and visible part of the road into grain.",
                    "label": 0
                },
                {
                    "sent": "But we can see there's a lot of things missing here because of the occlusions that comes from the bicycles and the vehicles, and if based on this kind of parsing result we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To ask some questions like whether this person is standing on the sidewalk or how do we get there, then we run into problems because we do not know what's behind those occlusions.",
                    "label": 0
                },
                {
                    "sent": "To facilitate this kind of inference problem, we think we need another kind of scene representation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would look like this.",
                    "label": 0
                },
                {
                    "sent": "So this is labeling the complete extent of the underlying surface is, and now we can clearly see what's behind the occlusion and the inference problem will be a lot more straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see another example in the image space of this image.",
                    "label": 0
                },
                {
                    "sent": "If we label the visible surfaces then we get around 5:00 or 10% of the occlusions.",
                    "label": 0
                },
                {
                    "sent": "But if we project this into the overhead view, then we can see there are a lot of in navigable areas which will make problems for applications like robotics.",
                    "label": 0
                },
                {
                    "sent": "If we have the complete extent of the assistive services, however, then we are free of this problem.",
                    "label": 0
                },
                {
                    "sent": "That's why we prefer to label the surface is into its complete extent instead of just the visible part.",
                    "label": 0
                },
                {
                    "sent": "That brings us to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem statement, so we will start with some image parsing and object.",
                    "label": 0
                },
                {
                    "sent": "Detections may or may not be noisy and then we want to infer the whole extent of the online background surface is what we also want is some structural prediction of the whole theme giving a rough idea about what the layout looks like using those Polygon.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are works that relate to our problem, especially image parsing papers, which deals with some labeling.",
                    "label": 0
                },
                {
                    "sent": "The visible surface is there also seeing layout us estimation papers which which take advantage of handcrafted sometimes like some specific prior such as boxy constraint in our indoor room and use that to infer the surface is we want something more general so.",
                    "label": 0
                },
                {
                    "sent": "Here's our framework.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image and then we extract some features from the image, including detector responses and the parsing result.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next we will apply a context to influence, so this is using the idea that spatial surrounding contexts can tell us about what's behind the occlusion.",
                    "label": 0
                },
                {
                    "sent": "So we can look at each of the position and image, take the features from the neighborhood and use that to predict the background label.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After the contextual inference will get this label map and then we will explore our second idea which says complete regions have some certain ships.",
                    "label": 1
                },
                {
                    "sent": "Usually we think of background services stuff as shapeless.",
                    "label": 0
                },
                {
                    "sent": "It's quite true if they are often included, but if you look at the uncluded complete extent of those services, but they will actually have some shape, so an included piece of sidewalk or NPC included.",
                    "label": 0
                },
                {
                    "sent": "Road we will look like triangles and buildings if they are not included then they will have this quarter radical look so we can find from the training images the polygons that matched the label map.",
                    "label": 0
                },
                {
                    "sent": "We just predict it and then we will transfer those polygons.",
                    "label": 1
                },
                {
                    "sent": "Use that as a shape prior for the image we are interested.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will use this shape prior to refine our result.",
                    "label": 0
                },
                {
                    "sent": "Do another round of contextual inference and give out.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Final result.",
                    "label": 0
                },
                {
                    "sent": "Now let's go into the details for the visual cues we use off the shelf image parsing algorithms as well as pre trained object detectors.",
                    "label": 1
                },
                {
                    "sent": "We use their responses as labeled confidence map as features and we have one of those feature Maps for each of the parsing labels and object categories we care about.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we apply a contexual feedforward algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a modified version of auto context.",
                    "label": 0
                },
                {
                    "sent": "That has been published a couple years ago.",
                    "label": 0
                },
                {
                    "sent": "We start with visual cues for each position in the image.",
                    "label": 1
                },
                {
                    "sent": "We will look at its radio neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And aggregate the features from the feature map.",
                    "label": 0
                },
                {
                    "sent": "Use that trainer Discriptive classifier an predict the background label.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting from the next round, not only we have the visual cues, we also have the feedforward context from the previous round of background prediction.",
                    "label": 0
                },
                {
                    "sent": "Now we're doing contextual influence on both the visual cues and the feed for context, will train another descriptive classifier and predict the background prediction.",
                    "label": 0
                },
                {
                    "sent": "Because we add a new information here, so this prediction will be different from what it is in the previous round.",
                    "label": 0
                },
                {
                    "sent": "Will do this for a couple of iterations until it converges.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the end of the context inference we have the label map.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do is try to find polygons that describe the shape of this label map.",
                    "label": 0
                },
                {
                    "sent": "What we do is look at the probability of sidewalk in this area and find that it really looks like one of the polygons that appear in the training images and we can retrieve those polygons using the criteria of weighted intersection over Union.",
                    "label": 0
                },
                {
                    "sent": "We use this kind of retrieved Polygon as the shape prior, for example in the.",
                    "label": 0
                },
                {
                    "sent": "Sidewalk region.",
                    "label": 0
                },
                {
                    "sent": "Here we retrieve polygons for each of the classes separately and overlaid.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those polygons to create a rough estimation of the same layout.",
                    "label": 0
                },
                {
                    "sent": "This will be used as a shape prior to improve the final result.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's see how things change during the different iterations.",
                    "label": 0
                },
                {
                    "sent": "To begin with, we only have the visual cues, where occlusion is fairly obvious.",
                    "label": 0
                },
                {
                    "sent": "Now it's the contextual inference goes.",
                    "label": 0
                },
                {
                    "sent": "We become more and more confident about what's being included.",
                    "label": 0
                },
                {
                    "sent": "And finally we apply our shape prior, which give the label map more simpler.",
                    "label": 1
                },
                {
                    "sent": "Look, look like triangles compared to what it was before.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that brings us to our final result.",
                    "label": 0
                },
                {
                    "sent": "We have the complete labeling of the underlying services along with the top one and two.",
                    "label": 0
                },
                {
                    "sent": "My estimation for the scene.",
                    "label": 0
                },
                {
                    "sent": "Note that this kind of structure prediction of the scene layout really is really preserve the simplicity of the original human annotators, and sometimes it gives different interpretations.",
                    "label": 0
                },
                {
                    "sent": "From those different gases.",
                    "label": 0
                },
                {
                    "sent": "So in this case the 1st guest say there's only one side walk region here, and the second guess say there might be 2 pieces of different sidewalk on both sides of the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For evaluation we did on three datasets.",
                    "label": 0
                },
                {
                    "sent": "Stressing indoors and sell online.",
                    "label": 0
                },
                {
                    "sent": "These datasets are not initially created for the purpose of underlying surface prediction, but we can use them because they annotate things in polygons.",
                    "label": 0
                },
                {
                    "sent": "Secondly, they when they annotate the background regions, they annotate to complete the extent as if the foreground objects are not there.",
                    "label": 0
                },
                {
                    "sent": "So when we do training and testing, we can just ignore the foreground polygons and use the background polygons to create our ground truth for the online services.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's how things look like.",
                    "label": 0
                },
                {
                    "sent": "The switching data set.",
                    "label": 0
                },
                {
                    "sent": "The first column is a ground truth and we can see.",
                    "label": 0
                },
                {
                    "sent": "Although there are cars on the street, the road are indeed labeled to its full extent, as if the cars are not there.",
                    "label": 0
                },
                {
                    "sent": "An second column is a visualization of the visible cues we use, including parsing and detection.",
                    "label": 0
                },
                {
                    "sent": "An third column for Skull and our our underlying Surface predictions overlaid with the Polygon estimation of the layout.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Act this is a result from the ground truth data of the indoor scene data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is from the summer line.",
                    "label": 0
                },
                {
                    "sent": "Datasets online has a lot more foreground clusters, but we can handle that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For quantitative evaluation, we created three base lines, all of which use the same features as we use in our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The most confident one predicts directly from the visual information, the most confident background label nears approach, fusing all the pixels that has been labeled as foreground objects using the nearest background label and graph cut does contrast sensitive smoothing and enforces label smoothness.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the pixel accuracy on the streets in data set.",
                    "label": 1
                },
                {
                    "sent": "Although baselines we described perform similarly, an hour algorithm outperform and an overall accuracy by almost 4%, and the improvement is really drastic on the occluded part where we get almost 10% improvement.",
                    "label": 0
                },
                {
                    "sent": "In terms of what things helped?",
                    "label": 0
                },
                {
                    "sent": "Contacts and shape prior each contribute to some improvement, and they work the best when they are combined.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His other two datasets.",
                    "label": 0
                },
                {
                    "sent": "So the trend is similar.",
                    "label": 0
                },
                {
                    "sent": "We have reasonable improvement in overall and the visible part and we got big improvement on the occluded.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is per class accuracy and we can look at what things got improved most.",
                    "label": 0
                },
                {
                    "sent": "They are the regions like road and sidewalk in the outdoor areas and floor and walls in the indoor areas, so these are kind of the regions that often get included.",
                    "label": 0
                },
                {
                    "sent": "That's why they got improved.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that brings us to our conclusion.",
                    "label": 0
                },
                {
                    "sent": "We try to solve a problem that is slightly different from traditional image parsing.",
                    "label": 0
                },
                {
                    "sent": "Now we are labeling the online services to its complete extent.",
                    "label": 0
                },
                {
                    "sent": "We are also predicting some of the rough Polygon layout estimation.",
                    "label": 0
                },
                {
                    "sent": "We had a general algorithm for this and it works well on all the three datasets we used and the code is available on our webpage so.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're welcome to check them out.",
                    "label": 0
                },
                {
                    "sent": "That's it, thanks.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if your ship priors to which degree are view.",
                    "label": 0
                },
                {
                    "sent": "Be invariant to the viewpoints.",
                    "label": 0
                },
                {
                    "sent": "So anyway by suppose that you are able to estimate the viewpoint of the observer if there is a way to make those priors even simpler.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "I think those those Polygon layout we estimate may not be like consistent in terms of like geometry viewpoint, and it is absolutely valuable to users kind of viewpoint estimation to further improve the result.",
                    "label": 0
                },
                {
                    "sent": "But this is like a.",
                    "label": 0
                },
                {
                    "sent": "More general, like more robust things so well.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I would think that if you don't know you point you money to use a larger number of suppliers to accommodate all possible variations which.",
                    "label": 0
                },
                {
                    "sent": "Might be affecting the overall performance of your framework, so I will will thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}