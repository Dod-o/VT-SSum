{
    "id": "2uyyykfu744fx32heqgnltmtaqjxftkj",
    "title": "Variational Bayesian methods for audio indexing",
    "info": {
        "author": [
            "Fabio Valente, IDIAP Research Institute"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlmi04uk_valente_vbmai/",
    "segmentation": [
        [
            "Let's have a look at the outline of this talk.",
            "I'm going to deal with a very classical problems that speaker clustering, so I'm going to give some very well known generalities.",
            "After data I'm going to discuss a little bit the model selection problem and very classical solution basic information criterion that's actually used by almost all state of the art system.",
            "Then I'm going to reduce the novelty of the paper.",
            "That's actually the use of the variational learning.",
            "And finally I'm going to show is it possible to make variational model selection jointly with model learning?",
            "Fine, I'm going to show some results we obtained on broadcast news database."
        ],
        [
            "So some well known generalities in many application, like speaker indexing with speech recognition, we actually do require speaker clustering.",
            "That means grouping together speech segments coming from the same speaker.",
            "The very classical topology always used in those cases is a fully connected age.",
            "Amends when I got engaged men in which each state represent the speaker.",
            "Actually, we put in this topology duration constraint in each.",
            "On each emission probability.",
            "Now, as long as the number of speaker is not a priority over from from from the data.",
            "We have to estimate this number of speaker using a model selection criterion.",
            "So we're basically reformulating the problem as the model selection criterion and the general solution.",
            "Previous pretty used, his invasion information is the BSE.",
            "So."
        ],
        [
            "So let's give some mathematical formulation of the problem.",
            "Given some data Y and the model M, the best model is the model that optimize actually the posterior probability of the model given the data.",
            "Simply applying Bayes rule we can find out that's proportional to the probability of the data given the models time.",
            "The prior probability of the model.",
            "If now we assume that the prior probability of the model is uniform over all possible models, the decision will always depend will simply depend on.",
            "Heel wide, even app that's also known as marginal likelihood of the data.",
            "So now both we estimate marginal likelihood with.",
            "But with numerical technique or nonparametric technique both, we assume as we always do in those kind of situations of parametric model with some parameters Theta and we assume it distribution over those parameters we designate with PT to given M. So now in this case the marginal likelihood can be computed, marginalizing out the joint probability of the data and the parameters with respect to the parameters.",
            "No, as were shown before in the in the introductory talk, this task can be.",
            "Prohibitive for some models.",
            "We actually do like very much like even Markov model of Goshen mixture model, in which we have even variable as well, for which we have to consider together with the probability over the data the joint probability over the data and the parameters.",
            "Also the joint probability over the inner variables and then becomes extremely complex and primitive task in terms of computational times this."
        ],
        [
            "The reason why the simplest way we we had to handle this problem is approximating the market.",
            "The marginal likelihood as well done by Schwartz in 78 so long time ago using an A plus approximation and making up some manipulation we can find out the well known vics that consist in the log probability of the data given a parameter estimation.",
            "So typically a map parameter summation minus a penalty term.",
            "Let's just proportional.",
            "On the logarithm of the amount of data on the number of free parameters.",
            "Now generally this method is extremely ineffective for the reason we generally multiply the penalty term by a constant by threshold language that may be adapted to a particular situation.",
            "So particular data set in order to make it more effective.",
            "Something interesting is that BIC does not depend on parameter distribution.",
            "That's good and bad at the same time.",
            "It depends on the situation, while it's.",
            "We should be at this.",
            "It's an action approximation of the marginal likelihood that does depend on the prior parameter distribution.",
            "Anyway, asymptotically that means for large amount of data they VSE actually converges to the lower margin likely, so variational methods actually aims."
        ],
        [
            "Logically bounding the load, the log marginal likelihood so the Bayesian integral bounding using an approximated distribution we introduce and we call variation distribution cutie to that.",
            "Actually, if it's chosen in a smart way, can can lead to a tractable problem.",
            "So some simple mathematical manipulation.",
            "Let's consider the log probability of the data that can be obtained marginalizing out.",
            "The joint probability of latent parameters with respect to parameters.",
            "Let's multiply and divide by the.",
            "Variation distribution wins.",
            "Will use cutie to and.",
            "Let's apply Jensen inequality.",
            "We find out an expression like this that's actually called variational free energy of fan and the goal of variational learning is maximizing the every energy offend.",
            "That's actually a lot where bound that's always verified on the log marginal likelihood.",
            "Instead of maximizing the larger, the marginal likely now the deal is choosing a variational distribution cutie to simple enough and effective enough to make this bound tractable.",
            "Where the approach becomes."
        ],
        [
            "Extremely interesting is in case of hidden variables, because if we have another variable set we call X.",
            "We can introduce as well variation distribution over the joint probability of Tieton and X obtaining a new form of free energy written like this that contains cutie to X.",
            "Now the variation Repatha SIS we make.",
            "This is a strong about.",
            "This is the Max.",
            "This fear retractable is assuming independent the variational posterior distribution over para meters and over.",
            "Even variables if we make this kind of assumption, we can write down the free energy in this new form in which we can recognize two different terms.",
            "So attached that dependent actually on the probability of the data of the hidden variables of the parameters.",
            "This smells like like you turn, and then another terms that actually the chaldee versions in between variational posterior distribution and prior distribution over the model.",
            "Now this term act a little bit like a penalty term in the sense that.",
            "Divergent is always positive, and if the model contains lot of parameters it will contain lot of care divergences.",
            "So a bigger model will contain larger Carol Divergents.",
            "Actually because we have minus the car divergents, we are going to have a penalty that becomes larger with the size of the model.",
            "If."
        ],
        [
            "We derive the previous expression FN with respect to cutie tank weeks.",
            "We can find out an algorithm that looks pretty much like like the EM algorithm.",
            "This called them like algorithm or variational Bayesian.",
            "Am lot of names for this algorithm that actually consisting in iteratively update of the variational distribution over the Eden data and the variational distribution over the para meters.",
            "Now this algorithm is inexpensive modification of the classical EM algorithm.",
            "In the sense that we are just computing the integral appear, not the integral appear with respect to parameters with respect to other variables, so the complexity of this algorithm compared to the classical EM algorithm is exactly the same.",
            "We are just computing one more integral.",
            "So now we have so."
        ],
        [
            "The problem of finding a reasonable bound from the log marginal likelihood and how is it possible to find variation distribution that optimize this bound so intuitively, if free energy is bound of log marginal likelihood, it may have some model selection property as well.",
            "So making discussion analogous to what we have done before, mathematically speaking, we can introduce a variational posterior distribution over the model itself.",
            "Let's make the same trick we have done before, so let's consider the logarithm of the probability of the data this time obtain marginalizing with respect to all possible models.",
            "And let's apply again, Jensen inequality, multiplying, dividing.",
            "By QM, we will find out.",
            "Lower bound again on the log probability of the data.",
            "Optimizing this lower bound, we're going to find out that optimal posterior distribution is proportional to the exponential.",
            "The free Energy Times the prior over the model.",
            "Now again, if the prior over the model is uniform, the best model will simply be inferred using the free energy.",
            "That means that we have the same quantity that can be used as an objective function for learning the model parameter distribution for making the model selection.",
            "This is pretty different what we do with the BSE, in which we are learning the model with the given criteria in general, like the map of the of the maximum language and then we are appending a penalty term.",
            "We're appending and penalty term tests.",
            "Actually nothing to do with model without here the penalty term is inside the free energy, so it's being trained together with the model."
        ],
        [
            "OK, so now let's move to the experimental framework.",
            "The experiment, the database we use is the broadcast news.",
            "96 up for evaluation data set that actually consists of four different files or almost half an hour each, in which we have both speech and speech data.",
            "So I've said before that apologies fully connected with the Markov model where the duration constraint we initialized with model with N speaker.",
            "That means and states in which we suppose that time is pretty big so that we can we can over cluster over, cluster the data and then we compare the variational Bayesian with two different approaches.",
            "One is the maximum likelihood BSE and one is the map BSE.",
            "So when we are comparing the VB with maximum likelihood.",
            "We are initializing the prior distribution as flat as possible in order not to give any prior information to the system.",
            "We will almost always say non informative even if we are using.",
            "Proper prior they cannot be non informative and in the case that via map we initializing prior distribution with the universal background model.",
            "So we're almost doing adaptation.",
            "We are doing adaptation.",
            "After that we are progressively reducing the number of speakers from Antoine, and we're scoring each intermediate system with both the free energy, the Bayesian information criteria.",
            "So I'm going to present three different scores.",
            "Q cheating schools.",
            "Actually, the first one is is actually the best car we can obtain using labels and the second cheating score is the score.",
            "Obtain initializing the system with real with the actual speaker member that comes again from the labor.",
            "We don't have it.",
            "And then I'm going to present the actual the actual score of the system.",
            "That's the one selected by the variation wayson Overvie See results are given in terms of average cluster purity and average speaker purity and their geometrical mean K. So now lot of now."
        ],
        [
            "For the first 2 fireworks important to retain it that actually on online the VB is doing better than the than the maximum likelihood, both for the Oracle experiments and for the real data for the real experiments.",
            "Actually for the actual experience and same conclusion for."
        ],
        [
            "My three and five for the actually have exactly the same performance.",
            "The in term of ACP SPMC.",
            "No here here I'm talking about BSE.",
            "But actually we know."
        ],
        [
            "BSE is extremely sensitive on the trishal Lambda we use, so in the under Fishel number we use, yeah.",
            "So in the previous slide what I presented was the best results we actually obtain with the best trishal Lambda here you have the real dependence and you can see that the best fischels actually change from file to file.",
            "Here we have the clustering score function of nutrition.",
            "Here we have the speaker number function Patricia.",
            "So this this this optimal Trish or change the file to file.",
            "From five to five, it is pretty easy to say, and this is pretty annoying.",
            "While we don't have this actually this problem, using the variational Bayesian.",
            "Surf."
        ],
        [
            "Even more interesting, I want to show you is how the clustering score out there be icy or the free energy follows the clustering Sir.",
            "So here in green we have the clustering score for the maximum likelihood system and then in blue and red and black we have the BIC for Lambda equal to 1 to 2123.",
            "So for Lambda equal to 1 Lambda Lambda equal to two.",
            "Actually the BS is not following at all the clustering score.",
            "In order to have a good approximation, we must go up to Lambda equal to three.",
            "You can see the difference with the free energy.",
            "So here you have in green you have again the clustering scoring blue.",
            "You have the free energy, so they're closing each other pretty close in a very good way.",
            "Actually, this is very satisfactory results.",
            "And finally."
        ],
        [
            "Another set of experiments in which we compare the VB with the map BC we come out with the same conclusion, but this time the difference is maybe still outperforming map, but this time the difference is not as big as in maximum likelihood framework and this time we're gay."
        ],
        [
            "Eating better results for.",
            "For the four 544485 South of Florida.",
            "So let me give."
        ],
        [
            "Some conclusion about my work.",
            "So basically to summarize with version Bayesian framework, we can use the free energy for making at the same time the parameter learning and the model selection with the same objective function.",
            "Version is actually a generalization of both.",
            "Maximum likely the map learning, so we expect actually to work better and experimentally we verify that it outperforms both Maxim like BSE and map BSE.",
            "So some future works.",
            "We want to get in is that actually this database we have running experiments on is pretty old, we don't.",
            "We haven't used actually the nice metric for the speaker clustering for the Speaker theorization task because at that time we wanted to compare with us with.",
            "What people I don't is database at this time.",
            "This magic didn't exist.",
            "So in the next future we would ever like to test the same approach on some nice data using the nice metric to see if even in this case we can have such good improvement."
        ],
        [
            "And that's pretty much once a thank you for your attention.",
            "Questions please.",
            "Everybody get it up, please.",
            "Address whatever man very interesting.",
            "I think you compare the maximum likelihood with Bayesian approach.",
            "I think the advantage of using a.",
            "The major approach can be observed when the amount of training data is very small.",
            "Yeah, sure.",
            "Well I can.",
            "I can show you another slide with four time reason.",
            "I was not showing so this is what happened."
        ],
        [
            "When we have so in this graph we have a classname cluster number and then the the blue line is actually the amount of data we have for each cluster OK. And this line represents the number of Goshen final number of Goshen that are in Ferd by the Bayesian framework so you can see what's when.",
            "We have very small amount of data like here.",
            "The Bayesian framework is not keeping the organization is avoiding overfitting.",
            "Cutting download of components and this is not doable.",
            "Actually where the maximum likely well, unless we put attritional on the covariance matrix and so on and so.",
            "So actually when we have very few data in each cluster, we definitely gotta improvement using the.",
            "The Bayesian framework, even if we're not adding actually any information because we are using flat priors, we initialize the system using non well flat priors, more priors we could say almost noninformative priors.",
            "Anymore questions.",
            "Well then let's like the speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's have a look at the outline of this talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to deal with a very classical problems that speaker clustering, so I'm going to give some very well known generalities.",
                    "label": 0
                },
                {
                    "sent": "After data I'm going to discuss a little bit the model selection problem and very classical solution basic information criterion that's actually used by almost all state of the art system.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to reduce the novelty of the paper.",
                    "label": 0
                },
                {
                    "sent": "That's actually the use of the variational learning.",
                    "label": 1
                },
                {
                    "sent": "And finally I'm going to show is it possible to make variational model selection jointly with model learning?",
                    "label": 1
                },
                {
                    "sent": "Fine, I'm going to show some results we obtained on broadcast news database.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some well known generalities in many application, like speaker indexing with speech recognition, we actually do require speaker clustering.",
                    "label": 1
                },
                {
                    "sent": "That means grouping together speech segments coming from the same speaker.",
                    "label": 1
                },
                {
                    "sent": "The very classical topology always used in those cases is a fully connected age.",
                    "label": 0
                },
                {
                    "sent": "Amends when I got engaged men in which each state represent the speaker.",
                    "label": 0
                },
                {
                    "sent": "Actually, we put in this topology duration constraint in each.",
                    "label": 0
                },
                {
                    "sent": "On each emission probability.",
                    "label": 0
                },
                {
                    "sent": "Now, as long as the number of speaker is not a priority over from from from the data.",
                    "label": 1
                },
                {
                    "sent": "We have to estimate this number of speaker using a model selection criterion.",
                    "label": 0
                },
                {
                    "sent": "So we're basically reformulating the problem as the model selection criterion and the general solution.",
                    "label": 0
                },
                {
                    "sent": "Previous pretty used, his invasion information is the BSE.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's give some mathematical formulation of the problem.",
                    "label": 0
                },
                {
                    "sent": "Given some data Y and the model M, the best model is the model that optimize actually the posterior probability of the model given the data.",
                    "label": 1
                },
                {
                    "sent": "Simply applying Bayes rule we can find out that's proportional to the probability of the data given the models time.",
                    "label": 0
                },
                {
                    "sent": "The prior probability of the model.",
                    "label": 0
                },
                {
                    "sent": "If now we assume that the prior probability of the model is uniform over all possible models, the decision will always depend will simply depend on.",
                    "label": 0
                },
                {
                    "sent": "Heel wide, even app that's also known as marginal likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "So now both we estimate marginal likelihood with.",
                    "label": 0
                },
                {
                    "sent": "But with numerical technique or nonparametric technique both, we assume as we always do in those kind of situations of parametric model with some parameters Theta and we assume it distribution over those parameters we designate with PT to given M. So now in this case the marginal likelihood can be computed, marginalizing out the joint probability of the data and the parameters with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "No, as were shown before in the in the introductory talk, this task can be.",
                    "label": 0
                },
                {
                    "sent": "Prohibitive for some models.",
                    "label": 0
                },
                {
                    "sent": "We actually do like very much like even Markov model of Goshen mixture model, in which we have even variable as well, for which we have to consider together with the probability over the data the joint probability over the data and the parameters.",
                    "label": 0
                },
                {
                    "sent": "Also the joint probability over the inner variables and then becomes extremely complex and primitive task in terms of computational times this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason why the simplest way we we had to handle this problem is approximating the market.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood as well done by Schwartz in 78 so long time ago using an A plus approximation and making up some manipulation we can find out the well known vics that consist in the log probability of the data given a parameter estimation.",
                    "label": 0
                },
                {
                    "sent": "So typically a map parameter summation minus a penalty term.",
                    "label": 0
                },
                {
                    "sent": "Let's just proportional.",
                    "label": 0
                },
                {
                    "sent": "On the logarithm of the amount of data on the number of free parameters.",
                    "label": 0
                },
                {
                    "sent": "Now generally this method is extremely ineffective for the reason we generally multiply the penalty term by a constant by threshold language that may be adapted to a particular situation.",
                    "label": 0
                },
                {
                    "sent": "So particular data set in order to make it more effective.",
                    "label": 0
                },
                {
                    "sent": "Something interesting is that BIC does not depend on parameter distribution.",
                    "label": 0
                },
                {
                    "sent": "That's good and bad at the same time.",
                    "label": 0
                },
                {
                    "sent": "It depends on the situation, while it's.",
                    "label": 0
                },
                {
                    "sent": "We should be at this.",
                    "label": 0
                },
                {
                    "sent": "It's an action approximation of the marginal likelihood that does depend on the prior parameter distribution.",
                    "label": 0
                },
                {
                    "sent": "Anyway, asymptotically that means for large amount of data they VSE actually converges to the lower margin likely, so variational methods actually aims.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Logically bounding the load, the log marginal likelihood so the Bayesian integral bounding using an approximated distribution we introduce and we call variation distribution cutie to that.",
                    "label": 0
                },
                {
                    "sent": "Actually, if it's chosen in a smart way, can can lead to a tractable problem.",
                    "label": 0
                },
                {
                    "sent": "So some simple mathematical manipulation.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the log probability of the data that can be obtained marginalizing out.",
                    "label": 0
                },
                {
                    "sent": "The joint probability of latent parameters with respect to parameters.",
                    "label": 0
                },
                {
                    "sent": "Let's multiply and divide by the.",
                    "label": 0
                },
                {
                    "sent": "Variation distribution wins.",
                    "label": 0
                },
                {
                    "sent": "Will use cutie to and.",
                    "label": 0
                },
                {
                    "sent": "Let's apply Jensen inequality.",
                    "label": 0
                },
                {
                    "sent": "We find out an expression like this that's actually called variational free energy of fan and the goal of variational learning is maximizing the every energy offend.",
                    "label": 0
                },
                {
                    "sent": "That's actually a lot where bound that's always verified on the log marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "Instead of maximizing the larger, the marginal likely now the deal is choosing a variational distribution cutie to simple enough and effective enough to make this bound tractable.",
                    "label": 0
                },
                {
                    "sent": "Where the approach becomes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Extremely interesting is in case of hidden variables, because if we have another variable set we call X.",
                    "label": 1
                },
                {
                    "sent": "We can introduce as well variation distribution over the joint probability of Tieton and X obtaining a new form of free energy written like this that contains cutie to X.",
                    "label": 0
                },
                {
                    "sent": "Now the variation Repatha SIS we make.",
                    "label": 0
                },
                {
                    "sent": "This is a strong about.",
                    "label": 0
                },
                {
                    "sent": "This is the Max.",
                    "label": 0
                },
                {
                    "sent": "This fear retractable is assuming independent the variational posterior distribution over para meters and over.",
                    "label": 1
                },
                {
                    "sent": "Even variables if we make this kind of assumption, we can write down the free energy in this new form in which we can recognize two different terms.",
                    "label": 1
                },
                {
                    "sent": "So attached that dependent actually on the probability of the data of the hidden variables of the parameters.",
                    "label": 0
                },
                {
                    "sent": "This smells like like you turn, and then another terms that actually the chaldee versions in between variational posterior distribution and prior distribution over the model.",
                    "label": 0
                },
                {
                    "sent": "Now this term act a little bit like a penalty term in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Divergent is always positive, and if the model contains lot of parameters it will contain lot of care divergences.",
                    "label": 0
                },
                {
                    "sent": "So a bigger model will contain larger Carol Divergents.",
                    "label": 0
                },
                {
                    "sent": "Actually because we have minus the car divergents, we are going to have a penalty that becomes larger with the size of the model.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We derive the previous expression FN with respect to cutie tank weeks.",
                    "label": 0
                },
                {
                    "sent": "We can find out an algorithm that looks pretty much like like the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "This called them like algorithm or variational Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Am lot of names for this algorithm that actually consisting in iteratively update of the variational distribution over the Eden data and the variational distribution over the para meters.",
                    "label": 0
                },
                {
                    "sent": "Now this algorithm is inexpensive modification of the classical EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the sense that we are just computing the integral appear, not the integral appear with respect to parameters with respect to other variables, so the complexity of this algorithm compared to the classical EM algorithm is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "We are just computing one more integral.",
                    "label": 0
                },
                {
                    "sent": "So now we have so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of finding a reasonable bound from the log marginal likelihood and how is it possible to find variation distribution that optimize this bound so intuitively, if free energy is bound of log marginal likelihood, it may have some model selection property as well.",
                    "label": 0
                },
                {
                    "sent": "So making discussion analogous to what we have done before, mathematically speaking, we can introduce a variational posterior distribution over the model itself.",
                    "label": 1
                },
                {
                    "sent": "Let's make the same trick we have done before, so let's consider the logarithm of the probability of the data this time obtain marginalizing with respect to all possible models.",
                    "label": 0
                },
                {
                    "sent": "And let's apply again, Jensen inequality, multiplying, dividing.",
                    "label": 0
                },
                {
                    "sent": "By QM, we will find out.",
                    "label": 0
                },
                {
                    "sent": "Lower bound again on the log probability of the data.",
                    "label": 0
                },
                {
                    "sent": "Optimizing this lower bound, we're going to find out that optimal posterior distribution is proportional to the exponential.",
                    "label": 0
                },
                {
                    "sent": "The free Energy Times the prior over the model.",
                    "label": 0
                },
                {
                    "sent": "Now again, if the prior over the model is uniform, the best model will simply be inferred using the free energy.",
                    "label": 0
                },
                {
                    "sent": "That means that we have the same quantity that can be used as an objective function for learning the model parameter distribution for making the model selection.",
                    "label": 1
                },
                {
                    "sent": "This is pretty different what we do with the BSE, in which we are learning the model with the given criteria in general, like the map of the of the maximum language and then we are appending a penalty term.",
                    "label": 0
                },
                {
                    "sent": "We're appending and penalty term tests.",
                    "label": 0
                },
                {
                    "sent": "Actually nothing to do with model without here the penalty term is inside the free energy, so it's being trained together with the model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's move to the experimental framework.",
                    "label": 0
                },
                {
                    "sent": "The experiment, the database we use is the broadcast news.",
                    "label": 0
                },
                {
                    "sent": "96 up for evaluation data set that actually consists of four different files or almost half an hour each, in which we have both speech and speech data.",
                    "label": 0
                },
                {
                    "sent": "So I've said before that apologies fully connected with the Markov model where the duration constraint we initialized with model with N speaker.",
                    "label": 1
                },
                {
                    "sent": "That means and states in which we suppose that time is pretty big so that we can we can over cluster over, cluster the data and then we compare the variational Bayesian with two different approaches.",
                    "label": 0
                },
                {
                    "sent": "One is the maximum likelihood BSE and one is the map BSE.",
                    "label": 0
                },
                {
                    "sent": "So when we are comparing the VB with maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "We are initializing the prior distribution as flat as possible in order not to give any prior information to the system.",
                    "label": 0
                },
                {
                    "sent": "We will almost always say non informative even if we are using.",
                    "label": 0
                },
                {
                    "sent": "Proper prior they cannot be non informative and in the case that via map we initializing prior distribution with the universal background model.",
                    "label": 0
                },
                {
                    "sent": "So we're almost doing adaptation.",
                    "label": 0
                },
                {
                    "sent": "We are doing adaptation.",
                    "label": 0
                },
                {
                    "sent": "After that we are progressively reducing the number of speakers from Antoine, and we're scoring each intermediate system with both the free energy, the Bayesian information criteria.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to present three different scores.",
                    "label": 0
                },
                {
                    "sent": "Q cheating schools.",
                    "label": 0
                },
                {
                    "sent": "Actually, the first one is is actually the best car we can obtain using labels and the second cheating score is the score.",
                    "label": 0
                },
                {
                    "sent": "Obtain initializing the system with real with the actual speaker member that comes again from the labor.",
                    "label": 0
                },
                {
                    "sent": "We don't have it.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to present the actual the actual score of the system.",
                    "label": 0
                },
                {
                    "sent": "That's the one selected by the variation wayson Overvie See results are given in terms of average cluster purity and average speaker purity and their geometrical mean K. So now lot of now.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the first 2 fireworks important to retain it that actually on online the VB is doing better than the than the maximum likelihood, both for the Oracle experiments and for the real data for the real experiments.",
                    "label": 0
                },
                {
                    "sent": "Actually for the actual experience and same conclusion for.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My three and five for the actually have exactly the same performance.",
                    "label": 0
                },
                {
                    "sent": "The in term of ACP SPMC.",
                    "label": 0
                },
                {
                    "sent": "No here here I'm talking about BSE.",
                    "label": 0
                },
                {
                    "sent": "But actually we know.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "BSE is extremely sensitive on the trishal Lambda we use, so in the under Fishel number we use, yeah.",
                    "label": 0
                },
                {
                    "sent": "So in the previous slide what I presented was the best results we actually obtain with the best trishal Lambda here you have the real dependence and you can see that the best fischels actually change from file to file.",
                    "label": 0
                },
                {
                    "sent": "Here we have the clustering score function of nutrition.",
                    "label": 1
                },
                {
                    "sent": "Here we have the speaker number function Patricia.",
                    "label": 1
                },
                {
                    "sent": "So this this this optimal Trish or change the file to file.",
                    "label": 0
                },
                {
                    "sent": "From five to five, it is pretty easy to say, and this is pretty annoying.",
                    "label": 0
                },
                {
                    "sent": "While we don't have this actually this problem, using the variational Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Surf.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even more interesting, I want to show you is how the clustering score out there be icy or the free energy follows the clustering Sir.",
                    "label": 0
                },
                {
                    "sent": "So here in green we have the clustering score for the maximum likelihood system and then in blue and red and black we have the BIC for Lambda equal to 1 to 2123.",
                    "label": 0
                },
                {
                    "sent": "So for Lambda equal to 1 Lambda Lambda equal to two.",
                    "label": 0
                },
                {
                    "sent": "Actually the BS is not following at all the clustering score.",
                    "label": 0
                },
                {
                    "sent": "In order to have a good approximation, we must go up to Lambda equal to three.",
                    "label": 0
                },
                {
                    "sent": "You can see the difference with the free energy.",
                    "label": 0
                },
                {
                    "sent": "So here you have in green you have again the clustering scoring blue.",
                    "label": 0
                },
                {
                    "sent": "You have the free energy, so they're closing each other pretty close in a very good way.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is very satisfactory results.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another set of experiments in which we compare the VB with the map BC we come out with the same conclusion, but this time the difference is maybe still outperforming map, but this time the difference is not as big as in maximum likelihood framework and this time we're gay.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eating better results for.",
                    "label": 0
                },
                {
                    "sent": "For the four 544485 South of Florida.",
                    "label": 0
                },
                {
                    "sent": "So let me give.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some conclusion about my work.",
                    "label": 0
                },
                {
                    "sent": "So basically to summarize with version Bayesian framework, we can use the free energy for making at the same time the parameter learning and the model selection with the same objective function.",
                    "label": 1
                },
                {
                    "sent": "Version is actually a generalization of both.",
                    "label": 1
                },
                {
                    "sent": "Maximum likely the map learning, so we expect actually to work better and experimentally we verify that it outperforms both Maxim like BSE and map BSE.",
                    "label": 0
                },
                {
                    "sent": "So some future works.",
                    "label": 0
                },
                {
                    "sent": "We want to get in is that actually this database we have running experiments on is pretty old, we don't.",
                    "label": 0
                },
                {
                    "sent": "We haven't used actually the nice metric for the speaker clustering for the Speaker theorization task because at that time we wanted to compare with us with.",
                    "label": 0
                },
                {
                    "sent": "What people I don't is database at this time.",
                    "label": 0
                },
                {
                    "sent": "This magic didn't exist.",
                    "label": 0
                },
                {
                    "sent": "So in the next future we would ever like to test the same approach on some nice data using the nice metric to see if even in this case we can have such good improvement.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's pretty much once a thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Questions please.",
                    "label": 0
                },
                {
                    "sent": "Everybody get it up, please.",
                    "label": 0
                },
                {
                    "sent": "Address whatever man very interesting.",
                    "label": 0
                },
                {
                    "sent": "I think you compare the maximum likelihood with Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "I think the advantage of using a.",
                    "label": 0
                },
                {
                    "sent": "The major approach can be observed when the amount of training data is very small.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Well I can.",
                    "label": 0
                },
                {
                    "sent": "I can show you another slide with four time reason.",
                    "label": 0
                },
                {
                    "sent": "I was not showing so this is what happened.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we have so in this graph we have a classname cluster number and then the the blue line is actually the amount of data we have for each cluster OK. And this line represents the number of Goshen final number of Goshen that are in Ferd by the Bayesian framework so you can see what's when.",
                    "label": 0
                },
                {
                    "sent": "We have very small amount of data like here.",
                    "label": 1
                },
                {
                    "sent": "The Bayesian framework is not keeping the organization is avoiding overfitting.",
                    "label": 0
                },
                {
                    "sent": "Cutting download of components and this is not doable.",
                    "label": 0
                },
                {
                    "sent": "Actually where the maximum likely well, unless we put attritional on the covariance matrix and so on and so.",
                    "label": 0
                },
                {
                    "sent": "So actually when we have very few data in each cluster, we definitely gotta improvement using the.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian framework, even if we're not adding actually any information because we are using flat priors, we initialize the system using non well flat priors, more priors we could say almost noninformative priors.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Well then let's like the speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}