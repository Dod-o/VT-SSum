{
    "id": "5ynrtcxc4natwp573qqrghettcnurrap",
    "title": "Putting Things in Order: On the Fundamental Role of Ranking in Classification and Probability Estimation",
    "info": {
        "author": [
            "Peter A. Flach, University of Bristol"
        ],
        "published": "Jan. 28, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecml07_flach_pto/",
    "segmentation": [
        [
            "Thanks very much Yost.",
            "Good morning everybody.",
            "I really appreciate your coming, particularly after after yesterday's lavish dinner, so I hope to make it worth your while."
        ],
        [
            "So.",
            "I'd like to start with a couple of slides of of why I am talking about this and what is the motivation behind this work, so I'm going to talk about 3 typical and quite well known machine learning tasks.",
            "One of them is classification binary classification, so from labeled training data, positives and negatives you are trying to learn to induce a model that can distinguish between the positives and the negatives, and this is a very well known task.",
            "Then a slightly different task is the ranking task, and in my case I'm talking about bipartite ranking, which means that your training data isn't ranked examples with your training data is still positive and negative.",
            "So for instance there can be movies you like and movies you don't like and you try to come up with a model that puts the movies in order such that all the positives are proceeding the negatives.",
            "And then a third related talk task could be probability estimation, where you actually want to come up with a model for the posterior probability of the positive class given the description of an example.",
            "So here's a couple of questions that I'm not pretending to answer them all in this talk, and I think several of them are still open questions.",
            "But just to get your mind going about in which direction I want to go.",
            "So.",
            "One question could be is there a natural hierarchy among these tasks?",
            "And if you think about it, then maybe one possible answer could be is actually the hierarchy is from bottom to the top.",
            "So because if you can do probability estimation then you can do ranking by just using the probabilities to rank things.",
            "Then if you can do ranking you can do classification and if you would think that you would be forgiven, at least by me because that's what I used to think, but the title of my talk will give you maybe a hint that actually I've reached a different conclusion and that actually the fundamental.",
            "One is the middle one.",
            "So another question you could ask is.",
            "You can most machine learning models.",
            "You can somehow use to do each of these these tasks and I will show that for instance that you can use a decision tree to do classification ranking and probability estimation.",
            "But when you train these things.",
            "Is it simply a matter of adjusting the loss function or the heuristic in order to train a decision tree that is a good rancor?",
            "For instance, how does it work with SVM's?",
            "Or do we really need specific algorithms to learn rancors rather than classic?"
        ],
        [
            "Tires.",
            "There is increasing evidence in the literature and there was actually a very nice talk by Harold Steck yesterday that various models which actually are designed to be good classifiers also in practice are good rancors and so they achieve.",
            "High area under the curve.",
            "I will explain what area under curve is.",
            "If you haven't seen it before and there is a similar quite intriguing analysis that.",
            "So we all know the ADA Boost rank boosting algorithm for classification.",
            "This was adapted to rank boost for ranking, but it turns out that even the original Adaboost, under certain circumstances produces a good rank, are in fact produces the same rancor as rank boost, although not necessarily with the same speed of convergence.",
            "So these are a lot of intriguing questions.",
            "The final question that I want to throw up has to do with probability estimation.",
            "So there is a pickle evidence that some models produce good and well calibrated probability estimates while some others don't.",
            "And they have specific distortions.",
            "And again the question is, can we understand that Moran and what can we learn from?"
        ],
        [
            "And finally, this is my third and final motivational slide, and this is also to get your brains going a little bit this morning.",
            "Here is a couple of very simple examples.",
            "The simplest examples I could think of to illustrate some of these points.",
            "So the first example is intended to show that good probability estimation is not necessarily aligned with good ranking, so if you.",
            "Look here so we go from probability one for the positive class to probability zero an.",
            "Suppose that we have 4 examples.",
            "They all have probability roughly .5, but they are in the right order.",
            "OK, let's say .5 two point 5.",
            "One point four 9.48 or something like that.",
            "OK, so we're not making any ranking errors where a ranking error is defined as a positive in front of a negative sorry and negative in front of a positive.",
            "So no ranking errors, perfect ranking.",
            "However, given that all the probabilities are around .5, if you take as your measure of the quality of the probability estimates, the mean squared error where you want the positive to be predicted, wanna negative predictor zero?",
            "Then each of these has squared error .25, so the mean squared error is .25 OK. Now look at this second example we I have now moved this positive to one, so it doesn't incur any squared error.",
            "Neither does this negative because it gets 0.",
            "The other two are still around .5, so they still incur each of them roughly .25.",
            "So 2 * 2.25 / 4.",
            "So I have now halved the probability error.",
            "However, in the process I have introduced the ranking error.",
            "So you still have swapped.",
            "So if I go from this one to this one, I have decreased ranking performance an in terms of mean squared error.",
            "I have improved the probability estimates so just to show that these things are not necessarily aligned.",
            "Here is a similar example with respect to ranking.",
            "So when I draw a ranking like this, what I mean is most likely to be positive to least likely to be positive if I draw them so behind each other, it means a tie.",
            "So actually this is a random ranking because the rancor count distinguish between this positive and this negative.",
            "This positive in this negative and is possibly is negative.",
            "It incurs four and a half ranking errors.",
            "OK, how can you introduce half ranking errors?",
            "Well, that's exactly because of the tides, because this positive and the negative is not a whole ranking error because the negative is not in front of the positive.",
            "But it's also not behind the positive, so you can't 1/2.",
            "Another way to understand this four and a half is saying well, if I have three negatives and three positives, I can potentially make 9 ranking errors an I make half of those.",
            "OK, so this is pretty bad as a rancor.",
            "As a classifier, I have to sort of cut this ranking somewhere and you will immediately see wherever I do that, I will incur three classification errors.",
            "OK, whatever, I do it so it's a totally random model, OK?",
            "Now look at this one.",
            "As a rancor, this one actually makes more errors OK, because each of these negatives is in front of two positives, so 3 * 2 is 6 ranking errors, so we've gone from four and a half ranking errors to six ranking errors.",
            "However, the best class of the best accuracy that I can achieve here is if I cut here, I correctly classified this.",
            "Plus, I correctly classify the minus is and I only make errors on the two pluses.",
            "So again we have a similar phenomenon we can.",
            "Decrease ranking performance and at the same time increase classification performance, so the relation between all these things is perhaps a little bit less clearcut then than you think, and I thought it worthwhile to go through this for becausw.",
            "In one of the referees for one of my email papers here said exactly this thing isn't a good probability estimator necessarily a good rank are an, so the answer is.",
            "The answer is at the end of my talk.",
            "The answer is that depends on how you measure the quality of your probability estimates.",
            "That will be one of the things that I will be addressing, so this is almost.",
            "I call this a Clintonian answer.",
            "Did you have sex with that woman?",
            "Depends on how you define sex well."
        ],
        [
            "OK, so this is the the broad outline.",
            "I have three parts, three main parts.",
            "I will talk about building models and these will be tree based models as well.",
            "I have three models for you and I will.",
            "I will sort of analyze the process of how these models are being built and what you can do with them.",
            "Then I will look at the relationship between classification and ranking and in the third part I will look at the relationship between ranking.",
            "And probability estimation.",
            "And here is a very brief executive summary for each of."
        ],
        [
            "It's parts.",
            "Now before I do that, I thought it might be a good idea to put a little health warning here because I know that some people in this audience will get very uneasy with some things that I do, because what I'll do is I'll mostly talk about the Roc curves as evaluated on the training set.",
            "And from experience again in referee report, I notice that this is enough.",
            "You know this is like a red rag to a bull for some people.",
            "I think this is justified because I'm not so much not necessarily talking about evaluating models.",
            "I'm talking about training models, yeah, so if somebody would be training a decision tree.",
            "And they would use the training data to decide the top split.",
            "You wouldn't say to them.",
            "You can't do that.",
            "That leads to overfitting.",
            "You need to use a validation set for that.",
            "That's what the training set is for.",
            "So all I'm doing here is analyzed that process and.",
            "And see what happens.",
            "Actually, when we train these models, so this is not to say that generalization and overfitting aren't issues.",
            "It's just to say that I don't have any particularly clever solutions for that in this talk, so it applies here as much as everywhere.",
            "So yes, it is a real issue whether these rockers that I look at will generalize from training set to test set, but that's not something that concerns me very much."
        ],
        [
            "During this talk.",
            "OK, so.",
            "The first part of the talk is about building models.",
            "I will consider three types of models.",
            "The first two are very well known decision trees and Naive Bayes.",
            "The third one we actually introduced in a poster paper here at this conference, it's called Lex Rank.",
            "It does lexicographic ranking, which is strictly as a strictly stronger bias than naive Bayes, so everything the Lex rank and do naive base can do, but not vice versa.",
            "So if we go down, the bias increases, and if we go down the variance decreases.",
            "What do these models have in common?",
            "Well, actually you can understand them all as tree based models.",
            "That may not be entirely obvious if you think about naive Bayes, for instance, because that's certainly not how I used to think of Naive Bayes, but this will become clear."
        ],
        [
            "Fedramp, so, let's have a look at the good old decision tree.",
            "This is kind of a running example through the 1st part, so I have two binary.",
            "Attributes A&B value zero and one.",
            "And so, for instance, here for a = 0 I have five positives and six negatives where I equals one.",
            "I have four positives and sorry five positives and four negatives and so on.",
            "If you look at this for a little bit, you see maybe that's a kind of.",
            "It's certainly not linearly separable.",
            "It's not even separable by Decision Tree.",
            "It looks a little bit like an X or kind of concept, so there's mostly plus is there, and there must be minus is there and there, so it wouldn't be a surprise that if you train a decision tree on it, you get something which is a complete decision tree, which means it uses.",
            "It uses all the attributes, so in this case we first split on B and then we split in both cases on A and then we get here.",
            "The the joint distributions in each leaves are as follows, so if you want to turn this into a classifier, what can you do?",
            "Well, you need a decision rule to turn these leaves to label them with classes.",
            "So what one obvious possibility is using simply majority class."
        ],
        [
            "We get in that case.",
            "Plus minus minus plus because we have majority plus here majority minus majority minus majority plus an instant space, that's what it looks like.",
            "So you recognize this kind of bimodal or or X or type of concept.",
            "OK, that's all well and good.",
            "Now how would we use a decision tree as a rancor?"
        ],
        [
            "Well, what we could look at is the posterior odds in each in each leaves, which is which is simply the ratio of positive to negative.",
            "So the posterior odds here is 4 to one here.",
            "Two to three here, one to five and here 3 to 1 positive to negative.",
            "So if you just rank those then you will say this leave goes first.",
            "This leaf goes second.",
            "This leave goes third.",
            "This leave goes forth.",
            "In instance space that looks like this.",
            "And as a ranking it looks like this.",
            "OK, so basically this leave goes there.",
            "This leave goes there.",
            "This leave goes there and this leave goes there so we have ties in the ranking naturally because all the instances that go into the same leave are tide in the ranking, we can't distinguish between them and so.",
            "And this is quite a nice ranking because you know you go from mostly positives on the left side of the positives.",
            "Go down when you go to the right and the.",
            "The negatives go up so that, so that's that looks fine.",
            "And Thirdly, what would we do if we use the decision tree as a probability estimate while we would use the?"
        ],
        [
            "Syria rods themselves.",
            "So we still have in blue the posterior odds and you convert them into probabilities if you want, although I actually think that very often it's much easier to calculate in terms of posterior odds rather than probabilities.",
            "If you want to turn it into a probability, what you do is you say 4 / 4 + 1 is .8.",
            "2 / 2 + 3 is .4 and so on.",
            "So I visualized that there's a probability estimator in this way, so these these columns are the same as with the Rancor, but they are now distributed on the real axis from from one to zero.",
            "OK.",
            "Right, so this is all nothing new when and you may wonder why I tell you all this.",
            "OK, let's look at this a little bit.",
            "Free."
        ],
        [
            "My rock analysis perspective so.",
            "I use Roc analysis a lot for visualizing, not necessarily evaluation, but just visualizing what's going on.",
            "By the way, rock analysis I always get this question I always forget to mention it stands for receiver operating characteristic.",
            "It's totally obsolete this this.",
            "This comes from signal detection theory has its roots in the Second World War.",
            "Like very many things do, but it's best just to treat it as as an abbreviation without.",
            "The real meaning.",
            "So what you can do is basically.",
            "If you want to visualize the performance, the main idea is that we take the class distribution in each leave as a vector.",
            "Yeah, so for instance, this leave has a vector 4 app for app and one to the right, and this leave is visualized as a vector two up and three to the right.",
            "And if you stack those in the ranking order, you get a curve like this.",
            "And this is a bit like a lorence curve.",
            "You know, these kind of curves, of which talk about wealth distribution.",
            "So they say 20% of the people earn 80% of the income, and it's similar here.",
            "So when you have this curve, you can say something like.",
            "You can say things like the numbers don't apply here, but you can say things like 40% of the negatives are put together with 60% of the positives.",
            "And of course you want that difference to be to be big.",
            "So that's basically where the Roc curves come from.",
            "So in this curve what you see is on the vertical axis, you have positive examples on the horizontal axis.",
            "You have negative examples, and somehow this curve visualizes.",
            "Essentially, the ranking performance of this of this classifier."
        ],
        [
            "Now.",
            "You may be more used to actually not having absolute numbers on the axis, but relative numbers, namely.",
            "You're probably used to having a true positive rate here, which is the absolute numbers of positive normalized by the actual number of positive right?",
            "So instead of"
        ],
        [
            "In the previous graph, we would say 4."
        ],
        [
            "In the next graph, we will say 4 out of 10 positive examples, so we have .4 and that's the true positive rate and you do something similar on the horizontal axis.",
            "This has confused me for a long time, but here is how I got to grips with this.",
            "I now switch very freely between these two perspectives and the reason is that if you do the typical rock analysis thing, you essentially take the class distribution out of the equation an your slopes in your space are likelihood ratios.",
            "OK, so it's the thing that you multiply with the prior odds to get the posterior odds.",
            "So The upshot of this is.",
            "If you want likelihood ratios because you somehow want to take the prior odds to class distribution out of the equation.",
            "This Roc curve is your thing."
        ],
        [
            "If you instead want posterior odds.",
            "Then and you want to take the class distribution in, then this space is is the right space to work in.",
            "Now you will have noticed that I've cunningly used an example with an equal class distribution, so it doesn't really matter, but you can imagine that if you have unequal class distribution, this space will become a rectangle, whereas the other one is still a square.",
            "OK, so just I I just choose whatever is seems most appropriate at the occasion, and most of this talk I will actually use.",
            "The absolute numbers because I'm interested in posterior odds.",
            "This is sometimes called coverage space an before that it was called."
        ],
        [
            "In space, but it's I. I basically loosely call all this rock space, but it is important."
        ],
        [
            "To distinguish between the two.",
            "OK, so let's think about this a little bit more.",
            "How can we use this space to visualize the performance of these kind of trees?",
            "Well, here's the way I like to think about it.",
            "If I have a tree with four leaves, then think about all possible ways that I can label the tree for leaves two possible classes.",
            "So I have two to the four.",
            "Is 16 possible labelings and all of these can be actually visualized as points in this rock space.",
            "So for instance, the labeling plus minus, minus minus would correspond to this point here.",
            "OK, the labeling minus plus plus plus which doesn't make.",
            "An awful lot of sensors are labeling, but at least in theory you can do that minus plus plus plus would as sort of correctly classify one negative an 236 positives.",
            "So did I do this correctly?",
            "So what did I say minus minus plus plus plus where are we?",
            "Where are we?",
            "There we are and actually.",
            "I won't take you long to figure that out.",
            "That is symmetrically opposite, so it's it's point mirror through the center of gravity of the space, so it's point mirror through the through the midpoint on the diagonal.",
            "So you have a lot of symmetry in this in this space, but the so the reason why I bring this up is that somehow all possible rancors and classifiers that I can construct from this tree have to somehow go through some of these 16 points.",
            "There are no more points in.",
            "In Roc space, this is what this tree can achieve on this.",
            "On this data set on this training set.",
            "And the actual if I use the posterior as of this ranking order, I use some of these points, and in fact I use the outermost point which which is the convex Hull of this of these points and that is not."
        ],
        [
            "A coincidence?",
            "Now let's use this idea to think about how would this tree be built?",
            "So Decision tree building is a nice recursive process.",
            "So what do we do?",
            "We find the best split at the root and then we recursively split the left tree and the right tree until until we finished and we can actually visualize that very nicely in Roc space.",
            "So because choosing the top split is nothing else, then basically we have a decision stump here and that.",
            "If we use this as a rank are we would basically go through this point because the left leave has six positives and four negatives and the right leaf has four positives and six negative.",
            "So actually this tree is visualized.",
            "By this Roc curve and what we are now going to do by growing the tree in the left by growing the tree at the left, we're going to refine the curve here."
        ],
        [
            "So in the next step we get something like this.",
            "And then fine."
        ],
        [
            "Early in the third step, we grow the right subtree and we got something like this.",
            "OK, now there is.",
            "I can see sort of a question coming up with some of you, which is.",
            "This doesn't quite look like the Roc curve that you showed us before.",
            "So what's going on here?",
            "And the answer is, well, what I show you here is not the ranking order, it is just the order in which the tree is being built.",
            "It is the recursive order of the trees, but decision trees of course don't always order their leaves from left to right.",
            "So we have an additional step here, and this is a crucial."
        ],
        [
            "Crucial thing we take this Roc curve and then.",
            "Using the posterior odds, we're going to reorder those leaves and we."
        ],
        [
            "At something like this.",
            "I'm very fond of that.",
            "This is my very first animation.",
            "I'm sure you want to see it again.",
            "So here we go.",
            "So this is the key bit.",
            "Decision trees have some kind of syntactic structure, but that is not what defines the ranking order.",
            "We then have the posterior odds in the leaves and dead defines the ranking order, so we have those vectors, but we put them in a different order and actually you can quite easily proven this has been known for some time.",
            "That on the training set you will always get the convex Roc curve.",
            "So actually the decision tree that you get is the best rank are on the training set, so there is already such a link between good classification."
        ],
        [
            "Good ranking.",
            "OK, let's move on to naive Bayes so.",
            "It may not be immediately obvious that naive Bayes is a tree, but certainly if you have discrete attributes it is a tree like structure.",
            "Why?",
            "Well, naive Bayes needs to estimate the probability for each possible combination of attribute values, and that's what you can do with the decision tree.",
            "That's exactly what the decision tree does.",
            "Of course, it is always a complete decision tree, so there is no pruning there because you need all the all the possible combinations of attribute values.",
            "Also, of course there are many different ways in which to draw complete decision trees, and here is just I just used the same three structures before.",
            "So what is happening here?",
            "Well, naive Bayes as a probability estimator.",
            "Doesn't use the joint probabilities, but it uses marginal probabilities to estimate the joint probabilities so you can understand this as follows.",
            "I have the prior odds here, which is uniform, so it doesn't really do anything.",
            "Then I have.",
            "The marginal for a for a = 0.",
            "Sorry, the marginal for B = 0 is 6 positives.",
            "For negatives, the marginal for B = 1 is for positive 6 negatives one level down.",
            "You don't use the joint as in decision trees, but you use again the marginal.",
            "So the marginal for a = 0.",
            "Oh sorry, I was pointing at the wrong one, so a = 0.",
            "Is this one 5 positive 6 negatives?",
            "And here we have 5 positive, four negatives and that's repeated there.",
            "And what naive Bayes then does is simply multiply these.",
            "Marginal likelihoods, likelihood ratios to obtain an estimate of the joint likelihood ratio combined with the prior odds gives you the posterior odds.",
            "If it's my mother, I'm not here.",
            "So, and you can visualize this as a rancor, so you can you can.",
            "You can transform these probabilities.",
            "Sorry, these ratios into probabilities as we did before.",
            "We can visualize this is the Rancor and as you see, Naive Bayes makes one quite big mistake because the marginals in this case are misleading, right?",
            "The marginal say 1 1/2 times more positives than negatives here and again more positive than negative here.",
            "So if you multiply those, you get something like twice as many.",
            "Positives and negatives here, but in effect.",
            "There are more negatives and positives, so naive Bayes commits a ranking error which which we all know."
        ],
        [
            "That it is likely to do that, and you can visualize that in Roc space by this concavity here.",
            "Yeah, so again, we have this only these 16 points to work with, so the rocker for Naive Bayes has to go through these points OK. And in blue, I've actually indicated the marginals because the marginals are decision stumps and again they must be somewhere among those 16 points.",
            "So actually I think this is B = 0.",
            "This is a equals one I think.",
            "This is a = 0 and this is B equals.",
            "1.",
            "OK, so basically naive Bayes, all naive Bayes has is those four blue vectors and it tries to estimate where you will end up in Roc space, and it sometimes makes makes mistakes there.",
            "So we have misleading marginal probabilities.",
            "We can't do anything about that because the only way to repair that is if we have access to the true joint probabilities, which we don't.",
            "So we have this ranking order here.",
            "But there's something interesting about this ranking order becausw by a simple restructuring of the tree.",
            "It actually becomes a left to right ordering of that tree.",
            "So it says 2143."
        ],
        [
            "If I just swap the values of a around in both cases I get 1234.",
            "Now this is what I call a lexicographic ranking.",
            "It is a simple left to right ordering of the tree which we get if we put B before a an we always put B = 0 B 4B equals one an A equals 1B, four A = 0.",
            "So why do I?"
        ],
        [
            "All this lexicographic ranking well think think about lexicographic ranking is what you do to find people in in the phone book for instance.",
            "So if you want to know whether Bar comes before flash, then you look at they say why they start with different letters and becomes before F, so I know now the relative ranking.",
            "But if you want to know whether.",
            "Flag comes before flag with CK.",
            "One of the very many amputations of my name that I have to suffer in England.",
            "Then you use canned two names until you find the first position where they differ, and then you say OK, Ha H comes before K right and then you ignore whatever comes after that, right?",
            "So lexicographic ranking is basically finding the first point where two things differ and use that point as a criterion which to order and ignore the rest.",
            "And that's exactly what we're doing here.",
            "So if we want to compare 2 examples if they differ in attribute B. I just need to know which one of them has B = 0 and I know that that one goes before the other one and I can ignore the rest OK and then if they are equal for BI, need to go deeper in the tree.",
            "Of course, in practice you wouldn't do this with the tree, because that requires exponential space.",
            "But this is.",
            "This is kind of the principle.",
            "And in practice, we use the odds ratio two to order the attributes because that basically gives us the fact that lexical graphic ranking has strictly stronger bias than naive base.",
            "So I will do this very quickly."
        ],
        [
            "The odds ratio can be understood as yet another splitting criterion, but it actually has quite nice nice properties.",
            "So basically the odds ratio is P / N, which is the the examples that go left divided by the ratio of the examples that go right and one of the reasons why I like this so much is that unlike something like information gain, for instance, the odds ratio has a closed form.",
            "If you want to draw these nice red lines, which are the easy metrics in rock space.",
            "So one way to understand it actually, if you look if you look at this.",
            "Race showed and actually P."
        ],
        [
            "Times N -- N is something like the area of this guy here.",
            "And the other one is the area of that guy there.",
            "So a point.",
            "This point here has odds ratio 5 because this guy is 5 times bigger.",
            "Is that one?"
        ],
        [
            "And if if I go along this curve."
        ],
        [
            "If I was, I will keep that property so it has a nice geometric interpretation and it's easy to implement.",
            "So in what I'm about to show you, we actually use the odds ratio also as a decision."
        ],
        [
            "Splitting criteria OK. Now comes the moment of truth.",
            "I've been foolish enough to decide to give Demo, told my students Terra Capital with an Epson Takashima to borrow.",
            "Have spent long and often nightly hours to to produce this.",
            "It's still in beta, which means anything can happen.",
            "Maybe it's the leaving in Alpha.",
            "So I just want to visualize some of these ideas here.",
            "The program is called prog rock.",
            "If you're old enough to understand the pond and maybe you even recognize the logo there, but best not to say too much about that.",
            "Anne.",
            "OK.",
            "So far so good, so I'll.",
            "I'll start it up.",
            "It's a it's a Java archive.",
            "It actually incorporates some of the up the classifiers of waka, so let me just show you what it does.",
            "I can open an ARF file so waka.",
            "Data file, let's say diabetes, binarized version of diabetes, because, for visualization purposes we only have binary attributes.",
            "So what you're looking at now is all the.",
            "Is basically the recursive structure of the decision tree that is built, so let's me let me just with this slide.",
            "There I can go further down or higher up in the tree.",
            "So I'm essentially pruning the tree.",
            "So essentially maybe it will be clearer here.",
            "What happens here is that at the top level we split here and that gives us two new spaces to work in and then basically recursively we keep on splitting those.",
            "Those those subspaces in the way that they explained what you also can do is.",
            "Actually, look at the alternative.",
            "So at this level these square these rectangle points are actually alternative decision stump splits at the top level.",
            "And what we also can do is show why did we choose that one?",
            "Well, that will become immediately clear if we draw the asymmetric.",
            "Because that's the odds ratio is a metric, and that is the point that is on the highest is a metric basically.",
            "So we can understand which one we choose, and we can also understand that maybe it's a close call here, so it's maybe not so obvious which one of the two to choose.",
            "Another thing that you can do is actually.",
            "Show the Roc curve that you would get if you cut off the tree at that level.",
            "OK, now at the decision stump level.",
            "That isn't very interesting, but.",
            "Once we go deeper into the tree, we actually refine the Roc curve, but also notice that because of the reordering effect, I don't necessarily go through these points anymore.",
            "Remember, those points are purely following from the recursive structure of the tree, but the Roc curve comes from a ranking, so we can basically see how we can understand how the decision tree kind of obtains his Roc curve by reordering their segments.",
            "So let's look at another model here.",
            "At the other extreme, we have Lex rank.",
            "There we go and let's rank as I told you, use is purely the left or right ordering of the tree.",
            "So there there is no reordering and we have a Roc curve that is exactly following all those points.",
            "Although split points that we have in the tree and again we can sort of cut off the tree at higher up levels and see what rocker we would get there and finally 4.",
            "Naive bayes.",
            "The Naive Bayes is a little bit in between, so it kind of can be understood as building the same tree as Lex rank, but it does take some reordering decisions.",
            "And sometimes these reordering decisions make things better, and sometimes they make things worse, OK, which which can happen?",
            "This as we know we can have concavities in the in a naive Bayes rocker so.",
            "I'm going to leave the demo at that.",
            "So, as I say, this is currently still in there in beta, but we've put up a rudimentary web page.",
            "It's actually the beta can be downloaded from there.",
            "If you want to want to play with it, then we are planning to release it."
        ],
        [
            "In the public domain.",
            "OK, so to summarize this part, when we talk about building models, we have sort of a range of models.",
            "We have decision trees that have full control over how things are ranked because they have access to the joint probabilities.",
            "On the other extreme, we have Lex rank which does things in a purely syntactic manner and follows their recursive structure of the tree, and we have naive Bayes which does something in between.",
            "So here's an example.",
            "I won't explain it, but this is something that is not lexical graphic, but can be represented by by now."
        ],
        [
            "Space.",
            "OK, so let's move on to classification and ranking.",
            "Um?",
            "Here's a quote from a paper that I recently saw.",
            "In the paper says, AUC aggregates the models behavior for all possible decision thresholds.",
            "I'm not actually quite sure what that means.",
            "And I'm allowed to say that because it's from one of my papers.",
            "So I'm saying I'm actually not quite sure what I mean there right?",
            "So I decided to actually try to analyze this question a bit more so that we actually mean that in some sense.",
            "Accuracy is an expected value.",
            "Sorry, AUC is an expected value of accuracy and what is the random experiment there?",
            "How does it work?",
            "So what I'm going to do is?",
            "Look at that a little bit deeper and sort of I'm I'm looking at it.",
            "They're very simple arguments, but I'm looking at it in two directions, so can I go from AUC to accuracy an?",
            "Can I go from?"
        ],
        [
            "He received the way you see, so the first over here is some notation.",
            "Basically, the notation is pretty obvious.",
            "What you just need to remember is that I try to adopt the custom that if something starts with an initial capital, it is meant to be an absolute frequency, and if it starts with if it's all lower case, it's meant to be a relative frequency.",
            "So for instance posses the actual number of positives and lower case pause is the relative number of positives.",
            "That's roughly what I tried to do anyway.",
            "Then then we have the usual things like true positives and true negative."
        ],
        [
            "And so on.",
            "And OK, so here is a little bit machine learning 101.",
            "Very briefly, if you have this rankings, how do you get to two Roc curve?",
            "I explained that through a decision tree, but here is basically if you just have a ranking which you basically do, which you draw the curve as follows, you go from left to right through the ranking and if it is a positive you go up and if it's a negative you go to the right and you normalize these things so that you end up in one one at the end."
        ],
        [
            "And if you have ties in the ranking, then you make diagonal moves.",
            "OK, so given that we just had a little bit."
        ],
        [
            "Machine learning 101.",
            "Let's have machine learning 101 exam OK so here is question 42 of machine learning 101.",
            "AUC is.",
            "OK, it's a multiple choice question, so don't worry.",
            "So here's the example before AUC.",
            "For those of you haven't seen it, simply the area under this curve.",
            "Yeah, so the proportion of the unit square that is under this curve area.",
            "AOC is 1 minus AUC is the area over the curve."
        ],
        [
            "OK, so you see is a.",
            "The expectation that the uniformly drawn random positive is ranked before a uniformly drawn random negative.",
            "Sounds may be familiar to some of you."
        ],
        [
            "Be.",
            "The expected proportion of positives ranked before uniformly drawn negative.",
            "OK, so uniformly drawn negative.",
            "Let's say that one I have, in this case 24689 positives in front of it.",
            "So what is the expected value expected proportion of?"
        ],
        [
            "Positives in front of it.",
            "See the expected true positive rate.",
            "If the ranking is split just before a uniformly drawn random negative."
        ],
        [
            "D. The expected proportion of negatives ranked after a uniformly drawn random positive."
        ],
        [
            "E. 1 minus the expected false positive rate if the ranking is split just after uniformly drawn random positive, anybody dare to venture something?",
            "You see?",
            "Sorry."
        ],
        [
            "Very good RF.",
            "All of the above, absolutely.",
            "And the way to see this is actually it's very instructive to actually see this.",
            "So first of all, A is the typical one, and it's sort of.",
            "In this unit square you know you have these little cells, and each cell represents a coupling of a positive and a negative.",
            "OK, so if you randomly just put your finger somewhere, then the bigger the area is on the curve, the higher the probability is that you'll end under the curve, which is where they is.",
            "But B&C are actually something very similar.",
            "So if I uniformly select a negative, it means that they uniformly selected point on this axis, right?",
            "And what I'm asking for, what is the expected height?",
            "Of the column, right?"
        ],
        [
            "So if a uniformly select one of these axes, and I do that many times, then what will I get?",
            "On average I will get the average height of the column, which is again the."
        ],
        [
            "Area under the curve and you can do something very similar with positives in area over the curve and so on.",
            "The reason why I bring this up is that there is indeed something like.",
            "For instance, C says.",
            "AUC equals the expected value of the true positive rate under some you know experiment, so maybe that gives us a link to what this what is a connect."
        ],
        [
            "Between AUC and accuracy?",
            "And actually the connection is is very simple, so.",
            "What you can do.",
            "Somebody gives you a ranking.",
            "An you do exactly that, you say if I randomly split it somewhere, what is my expected accuracy?",
            "And if you do that I won't bore you with the details.",
            "You get something like that.",
            "And if you simplify it, a uniform class distribution, you get something like this.",
            "So there is a linear relationship.",
            "Also, we have a constant term here.",
            "Where does that come from?",
            "Well, if my ranking has AUC zeros, it's the worst possible ranking all the negatives before all the positives.",
            "If I land on a negative, on average, I will land in the middle and that will give me.",
            "I will correctly classify half of those negatives, right?",
            "So that's where the quarter comes from.",
            "So even with the worst possible ranking on average, you get still something out of it.",
            "But also notice that this can have very high variance, because if you ask yourself what are the maximum and minimum values?",
            "Well, if you have the worst possible ranking and you happen to split on the point where negatives goes to positive, you will get accuracy 0.",
            "And the best possible case is majority class, so you may have a very wide range of value.",
            "So even though there is a linear relationship in expectation.",
            "This probably doesn't mean that if if we use one lesson optimization criterion, that means that we also optimize the other, which is an argument that has been made before.",
            "So the argument is yes, if you look at the expected values there, maybe it may be a linear relationship, but actually there is so much variance that you can't really exploit it."
        ],
        [
            "Relationship you can do something similar if you go in the opposite direction, although it is a little bit more complicated in the interest of time I will only talk about this very briefly, but you can think of the following.",
            "Suppose I tell you that my classifier has a certain accuracy and you know the class distribution.",
            "This is like almost like knowing one point on the Roc curve and I ask you the question, what is the expected value of the area under?",
            "The curve that goes through that point and you have to somehow average overall curves that go through that point, and one way that you can.",
            "So basically what it means is that you know accuracy, so you know that you're somewhere on this.",
            "Accuracy is a metric, but you don't know where.",
            "So the and and one simplification that I make is that let's just look at three point rockers, so rocker, which basically ties everything before the split point and everything after the split."
        ],
        [
            "So we could have."
        ],
        [
            "Roc curve like this."
        ],
        [
            "If we could have a rocker."
        ],
        [
            "Like this like this like."
        ],
        [
            "Is on average.",
            "We would have a Roc curve that goes."
        ],
        [
            "Through the midpoint on that line and with equal class distribution we would have something like that.",
            "So in that case, you could say that the expected value of AUC in that case, if I tell you here is the value of accuracy with this uniform class distribution.",
            "Your best answer would probably be, well, I'm guessing that the AUC is equal to the accuracy, but again, this."
        ],
        [
            "Has large variance.",
            "Because the worst possible rocker through that point would be this one which has AUC, accuracy square."
        ],
        [
            "The best possible Roc curve that goes through that point would be this one, which has a you see 1 -- 1 minus accurate square, 1 minus the error square.",
            "So again, there is sort of some kind of linear relationship, but because of the large variance."
        ],
        [
            "We can't really exploit this.",
            "So if we have situations like.",
            "Adaboost seem to optimize AUC or SVM seem to optimize AUC.",
            "Then we need a model specific argument to explain this point.",
            "And as I mentioned, there was a very nice paper here which did that for SVM's."
        ],
        [
            "OK, so I'm moving on to the third and final part, which is about the relation between ranking and probability estimation.",
            "Why do we need calibrated probability estimates?",
            "Well, I like to say that if my probability estimates are not calibrated in some sense, and in fact we don't have a probability estimator at all, we have are anchor.",
            "So I like to say naive Bayes is not a probability estimator.",
            "It happens to output numbers between zero and one.",
            "But what it does is ranking if you want probabilities you need to calibrate them."
        ],
        [
            "So that's why we're going to talk about here.",
            "And one reason why you won't calibrate the probability estimates is that that gives you to justification to, for instance, put your threshold at .5.",
            "If your probability estimates are not calibrated, there is no justification for such a threshold and you have to learn the threshold from the data.",
            "So what calibration essentially means is that suppose I have my model and I have a test set and I record all the predictions of the model of the test set, and now I look at the examples that have a particular predicted probability.",
            "In a well calibrated classifier, the proportion of positives to negatives would correspond to that probability.",
            "So if my if I condition the examples on predicted probability .5.",
            "For a well calibrated classifier, I would expect as many positives as negatives in that case.",
            "And then actually, without going through in through all the details.",
            "Basically, this proportion of positives to negatives is essentially the slope of your Roc curve.",
            "So a well calibrated classifier which uses it scores to order the segments of the rocker.",
            "If the score corresponds to the slope of the segment, then it means that the curve is convex, right?",
            "Because the score is monotonically decreasing along the curve and then the slopes are monotonically decreasing as well.",
            "So this suggests a very simple calibration procedure.",
            "I.",
            "Because we can take outcome Roc Curve and if it isn't convex we can construct the convex Hull, which essentially means introducing ties in the ranking and then we can just read off the probability from the slopes of the segments.",
            "Now this is one of these moments in machine learning.",
            "Somebody said then machine learning that it's easy to pick out the good ideas in machine learning because they get invented, reinvented over and over again.",
            "And this is one of those moments.",
            "So when I discovered this, not only that, I discovered that actually this procedure already existed under the name of his atomic regression and the pair adjacent violators.",
            "Paradise and violators itself was, I think, invented somewhere in the 50s or 60s.",
            "It was introduced to machine learning by Bianca's address and Charles Elcome.",
            "The connection with the rock convex Hull was made first by Tom Fawcett, an Alex Nicolesco, and they published it as a.",
            "It's a very nice research note in the Machine Learning Journal.",
            "They just beat us to it.",
            "Honestly, I could say we did it independently, but they get the credit because they published it first.",
            "Notice also that this is exactly what decision trees do, by the way, so this is a very easy argument that decision trees are well calibrated.",
            "First of all, we don't need the raw convex Hull because the curves are convex already an it uses the slope of each segment to assign the probabilities, so this is I feel not so well known.",
            "Decision trees are by construction perfectly calibrated on the training set.",
            "Now there are issues here, particularly the large variance decision trees so.",
            "So there is a question of how well it generalizes, but.",
            "I think this is something that."
        ],
        [
            "That deserves wider wider knowledge.",
            "So how does this rock convex Hull calibration method work?",
            "Well, here I tried to put it in one picture and I hope it is relatively clear here.",
            "We have the ranking that we had before with its associated Roc curve and this ranking has concavities.",
            "So if you look at what the convex Hull of this ranking is it is this red line here right?",
            "And that means that we're going to put our.",
            "Ranking into bins.",
            "So all the positives got all the 1st four positives go into one bin and then we have another segment here, so that's there.",
            "And then we have another segment there and so on and we simply read of the predicted probabilities from the slope of these segments of the slope is 1.",
            "Then we you predict .5 and so in other words we get this calibration map because if you if you want to plot it as a calibration map, you somehow need to have scores to start from.",
            "Although the procedure works.",
            "Perfectly well if you only have a ranking in our scores, but the essential point is, is that your calibration map has these piecewise constant segments?",
            "Now, maybe you think, well, that's a bit nasty computationally, and so, but I would argue this is a feature, not a bug, because it is the piecewise constant bits that improve the ranking because they construct the convex Hull.",
            "If it was a monotonic curve, the Roc curve wouldn't change.",
            "OK, so now let's look at another."
        ],
        [
            "Your calibration method.",
            "Which is actually quite well known as well, which would be something like, you know, pushing your scores through a sigmoid like you would do in logistic regression, for instance.",
            "What is the justification for that?",
            "Well, the justification is that your scores must be normally distributed.",
            "Both of them actually with the same variance.",
            "OK, So what I'm saying is this is this this this red and blue curve is kind of the density of the score, so we have lots of scores around.",
            "Lots of negative scores around .4, lots of positive scores around six and so.",
            "If your scores approaching normal distribution is well known from signal detection theory that your Roc curve starting to look very nice.",
            "Basically the only parameter here is the normalized distance between the means of these two things, and if you have that and this is something that that I thought that if you have that your scores are calibrated, but the answer is they are not calibrated, but they can be calibrated by pushing it through a sigmoid.",
            "Basically, if you push this course through a sigmoid, they end up like the dotted lines there.",
            "And they are nicely calibrated because the density of the blue score linearly increases to the right and the density of the red scores linearly increases to the left, which is essentially why what you want with calibrated scores.",
            "The fact that it drops off is just because you have symmetric Gaussians.",
            "If you would just cut off the Gaussian in the middle, you wouldn't have that drop off points, which is right.",
            "So now we have two calibration methods, and I'm not claiming that any of these are.",
            "RR My invention but.",
            "What are the similarities and the differences here?",
            "Well, notice that this is a monotonic curve.",
            "So this calibration method does not change the Roc curve, right?",
            "So the ranking won't change, it's just a distribution of scores on the.",
            "So it doesn't have that power that that is a tonic.",
            "Regression would have by improving the ranking by creating these bins.",
            "On the other hand, of course it's a smooth differentiable function, so it would be.",
            "Would be less brittle and maybe you need less data to estimate this, but of course things can also go wrong if your scores are not Gaussian.",
            "There's no guarantee that this does anything useful.",
            "It may make matters worse, of course."
        ],
        [
            "OK, so I'm going to end with with a small analysis if I still have enough time.",
            "Yeah OK, I'm I think I can do that.",
            "And that is something that is that I think we we contribute through to this analysis.",
            "Let's go back to this idea of how are we measuring the quality of probability estimates and what we did so far was mean squared error.",
            "But there is something wrong with mean squared error.",
            "If we compare the squared error as deviations from zero and 1 zero and one are ideal scores, but in no sense are they actually true scores.",
            "There's no reason why would every positive require score woman.",
            "What random event does that correspond to?",
            "So there is something funny here, and actually what you can do is a decomposition that has been known in some form in forecasting theorie an.",
            "It's a simple rewrite if you have P positives and negatives which all get the same score.",
            "You can actually rewrite this so you have P terms which have the square deviation to one an N terms which have the square deviation to zero.",
            "You can rewrite this to something nice.",
            "S prime is the proportion of positives in that segment.",
            "So P divided people then, and what you get if you rewrite it like this is.",
            "A different squared error term where actually S prime now takes the role of the true probabilities, and this is the proportion of positives in that segment that you're talking about.",
            "There's a second term here, but it is.",
            "It doesn't refer to S, so it doesn't refer to the score, it's an impurity term.",
            "It's basically Gini index, but it only talks about the slope of that rock segment."
        ],
        [
            "So to make a Long story short, you can do this decomposition exactly, which is the different from forecasting theory, because they only knew how to do it approximately.",
            "By dividing the scores in equal bins using the rock.",
            "A curve to to basically decide the boundaries.",
            "You can exactly decompose these two things, and this is now something very useful, but because it gives this decomposition, gives you much more information than the Brier score or mean squared error itself gives you.",
            "So to show."
        ],
        [
            "That I have a plot here of some actual models that we basically this is 10 fold cross validation, so this is performance on.",
            "Each of the 10 folds.",
            "So what I want you to look at is."
        ],
        [
            "These points here are Lex rank.",
            "Which is clearly uncalibrated in fact, Lex rank doesn't really calculate probabilities, it just calculates binary numbers, so it's not not a surprise that this is not calibrated, but by doing the rock convex Hull calibration we can get the calibration loss close to 0.",
            "But of course we incur a little bit refinement loss because we are creating the convex Hull.",
            "So maybe previously we had only horizontal and vertical segments, which means that the refinement loss was zero.",
            "But by introducing these ties we will now get diagonal segments, so we get the calibration loss close to 0 at the expense of a slight increase of the of the."
        ],
        [
            "Refinement, loss and he is another example.",
            "Here we have the calibration with the same method of naive Bayes.",
            "And actually, you could argue that maybe in Brier score, this calibration doesn't make an awful difference, because basically that is the line of constant Brier score, and we're kind of moving parallel to that line.",
            "So what we win in calibration loss we we lose in refinement loss."
        ],
        [
            "So just to wrap up.",
            "We want calibrated probability estimators becausw we want to we want to essentially nowhere to threshold the score if you don't have calibrated probability estimate that you need rock analysis to find the best score threshold.",
            "If you have a calibrated probability estimate, you can simply say I want the posterior odds P / 1 -- P. So I want four times as many.",
            "Positives, negatives, and therefore I can just threshold it at that value.",
            "You can only do that with calibrated probability estimators.",
            "But the quality of probability estimators comes after.",
            "Evaluating the quality of a rancor so you can't have a good probability estimator if you don't have a rancor.",
            "So something like mean squared error."
        ],
        [
            "Alone is not enough OK, but we do need to do the ranking.",
            "That is the most fundamental thing.",
            "Then we have different calibration methods with different properties.",
            "So if you do logistic calibration, you basically have a parametric model which works as long as the assumptions of your model are satisfied, so your scores are Gaussian.",
            "You don't improve ranking, but you may get calibrated probability estimates, whereas doing it with the rock convex Hull is atonic regression potentially improves the ranking performance as well.",
            "The problem there is to estimate these points where the calibration map jumps up."
        ],
        [
            "So one open question that that I thought of is.",
            "Maybe it might be a good idea to somehow smooth these jumps in the calibration map, you don't want to get rid of these horizontal segments completely because they are what allows the improvement in ranking performance.",
            "But maybe you want to put some kind of smoothing up there so that you're not so sensitive to where you put these these boundaries.",
            "And here's a couple of.",
            "Other questions that that that that might be interesting to work on, and with that I would like to conclude and thank you for your attention.",
            "Start for few questions questions.",
            "Or just say that.",
            "The definition of a rancor that I often use is something that.",
            "Um?",
            "The ties are important here, right so?",
            "You can you can call it something that gives you a total order with ties.",
            "If you think about that, that's kind of.",
            "It's not a partial order because you you know you can't go via several parts, but the ties are important, right?",
            "So a total rancor is something that puts any set of examples in a total order with ties.",
            "Some people call that the bucket order.",
            "Just to see that one point the other you could do that, but then you have to add consistency requirements, right?",
            "Because you don't want to allow Rancor that puts a in front of BB in front of.",
            "See it, see in front of a.",
            "And you get, you get that with saying it's a bucket order.",
            "Turn 5 final floor dripping wet.",
            "Snakes.",
            "OK. Paper doll.",
            "Right?",
            "OK, so you're saying there are advantages in in, in in sort of viewing it as.",
            "Yes, OK, might be interested to discuss that.",
            "Oh yes, yeah.",
            "So when is one thing is just a question.",
            "The application which progressions I think it's lightning speed is that is not an assumption of Gaussian assumption of exponential family.",
            "The same OK.",
            "Applications right?",
            "But the other thing is kind of following up on this idea.",
            "Thinking this in terms of ranking, what did somebody actually came through instead of 1 zeros?",
            "They said here are some things of this example schedule in front of the other right and had a series of these binary decisions at this example in front of a given.",
            "Any thought to generalizing our receipts, that type space.",
            "I haven't personally, so the way I understand is that what you say is you could.",
            "You could generalize the bipartite drank the bipartite ranking setting is very restricted because it's basically somebody says good movies, bad movies and that's all you got and you.",
            "So for instance, you don't care at all how to rank the good movies amongst themselves, and you're saying, well, you could generalize that and have constraints and say this one should be go before that one.",
            "This one should go after that one, and I think I would need to look again in the literature.",
            "I think some people are starting to look at that.",
            "I haven't looked at it.",
            "But I think that's a very interesting direction to go.",
            "Yeah, thank you.",
            "Forecast.",
            "Look further into different scoring functions, proper scoring functions and how that could analyze that and potentially different compositions.",
            "What they would mean.",
            "I haven't looked at different decompositions.",
            "I don't know if you have any particular ones in mind with proper scoring functions.",
            "They basically all say.",
            "Constructing the convex Hull is a good idea.",
            "It's, you know it it because you want to be easy, tonic you.",
            "You don't want to mess up the ranking.",
            "You may, you may make the ranking coarser by putting things in bins, but you don't want to change the order and then basically any proper scoring function is minimized by, you know, doing the convex Hull basically, so I'm not sure how much interest there is for going beyond that.",
            "Looking at proper scoring functions, but if there are any, if there are other decompositions today, I'm.",
            "Probably just unaware of them and I would welcome any suggestions.",
            "Last question.",
            "About two classes, most of the time I was wondering what happened if you had more classes and what the state of the art here, and that's a very good question.",
            "What you end up with if you have more than two classes, the this kind of analysis very quickly becomes intractable, so.",
            "It's often not so much a theoretical question for International Srinivasan.",
            "In 1999 had a technical note where he shows that it all generalizes.",
            "You know you can do.",
            "You can have a rock polytope and sort of you're looking for the optimal point on the polytope by sliding and a surface with a certain orientation and so so with all generalizes, it just blows up.",
            "So you need to approximate it.",
            "So that means that you cannot guarantee to have the actual optimum.",
            "So what you can do is, if you have multiclass Roc analysis, you project it down to one versus one or one versus rest and you tried to do something that is good enough on the average without, without being necessarily optimal.",
            "So what you lose is optimality.",
            "But what you gain is a potentially still tractable method.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much Yost.",
                    "label": 0
                },
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "I really appreciate your coming, particularly after after yesterday's lavish dinner, so I hope to make it worth your while.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'd like to start with a couple of slides of of why I am talking about this and what is the motivation behind this work, so I'm going to talk about 3 typical and quite well known machine learning tasks.",
                    "label": 0
                },
                {
                    "sent": "One of them is classification binary classification, so from labeled training data, positives and negatives you are trying to learn to induce a model that can distinguish between the positives and the negatives, and this is a very well known task.",
                    "label": 1
                },
                {
                    "sent": "Then a slightly different task is the ranking task, and in my case I'm talking about bipartite ranking, which means that your training data isn't ranked examples with your training data is still positive and negative.",
                    "label": 0
                },
                {
                    "sent": "So for instance there can be movies you like and movies you don't like and you try to come up with a model that puts the movies in order such that all the positives are proceeding the negatives.",
                    "label": 0
                },
                {
                    "sent": "And then a third related talk task could be probability estimation, where you actually want to come up with a model for the posterior probability of the positive class given the description of an example.",
                    "label": 0
                },
                {
                    "sent": "So here's a couple of questions that I'm not pretending to answer them all in this talk, and I think several of them are still open questions.",
                    "label": 0
                },
                {
                    "sent": "But just to get your mind going about in which direction I want to go.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One question could be is there a natural hierarchy among these tasks?",
                    "label": 1
                },
                {
                    "sent": "And if you think about it, then maybe one possible answer could be is actually the hierarchy is from bottom to the top.",
                    "label": 0
                },
                {
                    "sent": "So because if you can do probability estimation then you can do ranking by just using the probabilities to rank things.",
                    "label": 0
                },
                {
                    "sent": "Then if you can do ranking you can do classification and if you would think that you would be forgiven, at least by me because that's what I used to think, but the title of my talk will give you maybe a hint that actually I've reached a different conclusion and that actually the fundamental.",
                    "label": 0
                },
                {
                    "sent": "One is the middle one.",
                    "label": 1
                },
                {
                    "sent": "So another question you could ask is.",
                    "label": 0
                },
                {
                    "sent": "You can most machine learning models.",
                    "label": 0
                },
                {
                    "sent": "You can somehow use to do each of these these tasks and I will show that for instance that you can use a decision tree to do classification ranking and probability estimation.",
                    "label": 0
                },
                {
                    "sent": "But when you train these things.",
                    "label": 0
                },
                {
                    "sent": "Is it simply a matter of adjusting the loss function or the heuristic in order to train a decision tree that is a good rancor?",
                    "label": 1
                },
                {
                    "sent": "For instance, how does it work with SVM's?",
                    "label": 0
                },
                {
                    "sent": "Or do we really need specific algorithms to learn rancors rather than classic?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tires.",
                    "label": 0
                },
                {
                    "sent": "There is increasing evidence in the literature and there was actually a very nice talk by Harold Steck yesterday that various models which actually are designed to be good classifiers also in practice are good rancors and so they achieve.",
                    "label": 1
                },
                {
                    "sent": "High area under the curve.",
                    "label": 0
                },
                {
                    "sent": "I will explain what area under curve is.",
                    "label": 0
                },
                {
                    "sent": "If you haven't seen it before and there is a similar quite intriguing analysis that.",
                    "label": 0
                },
                {
                    "sent": "So we all know the ADA Boost rank boosting algorithm for classification.",
                    "label": 0
                },
                {
                    "sent": "This was adapted to rank boost for ranking, but it turns out that even the original Adaboost, under certain circumstances produces a good rank, are in fact produces the same rancor as rank boost, although not necessarily with the same speed of convergence.",
                    "label": 0
                },
                {
                    "sent": "So these are a lot of intriguing questions.",
                    "label": 0
                },
                {
                    "sent": "The final question that I want to throw up has to do with probability estimation.",
                    "label": 0
                },
                {
                    "sent": "So there is a pickle evidence that some models produce good and well calibrated probability estimates while some others don't.",
                    "label": 1
                },
                {
                    "sent": "And they have specific distortions.",
                    "label": 0
                },
                {
                    "sent": "And again the question is, can we understand that Moran and what can we learn from?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, this is my third and final motivational slide, and this is also to get your brains going a little bit this morning.",
                    "label": 0
                },
                {
                    "sent": "Here is a couple of very simple examples.",
                    "label": 0
                },
                {
                    "sent": "The simplest examples I could think of to illustrate some of these points.",
                    "label": 0
                },
                {
                    "sent": "So the first example is intended to show that good probability estimation is not necessarily aligned with good ranking, so if you.",
                    "label": 0
                },
                {
                    "sent": "Look here so we go from probability one for the positive class to probability zero an.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have 4 examples.",
                    "label": 0
                },
                {
                    "sent": "They all have probability roughly .5, but they are in the right order.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say .5 two point 5.",
                    "label": 0
                },
                {
                    "sent": "One point four 9.48 or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're not making any ranking errors where a ranking error is defined as a positive in front of a negative sorry and negative in front of a positive.",
                    "label": 0
                },
                {
                    "sent": "So no ranking errors, perfect ranking.",
                    "label": 1
                },
                {
                    "sent": "However, given that all the probabilities are around .5, if you take as your measure of the quality of the probability estimates, the mean squared error where you want the positive to be predicted, wanna negative predictor zero?",
                    "label": 0
                },
                {
                    "sent": "Then each of these has squared error .25, so the mean squared error is .25 OK. Now look at this second example we I have now moved this positive to one, so it doesn't incur any squared error.",
                    "label": 1
                },
                {
                    "sent": "Neither does this negative because it gets 0.",
                    "label": 0
                },
                {
                    "sent": "The other two are still around .5, so they still incur each of them roughly .25.",
                    "label": 0
                },
                {
                    "sent": "So 2 * 2.25 / 4.",
                    "label": 1
                },
                {
                    "sent": "So I have now halved the probability error.",
                    "label": 0
                },
                {
                    "sent": "However, in the process I have introduced the ranking error.",
                    "label": 0
                },
                {
                    "sent": "So you still have swapped.",
                    "label": 0
                },
                {
                    "sent": "So if I go from this one to this one, I have decreased ranking performance an in terms of mean squared error.",
                    "label": 0
                },
                {
                    "sent": "I have improved the probability estimates so just to show that these things are not necessarily aligned.",
                    "label": 0
                },
                {
                    "sent": "Here is a similar example with respect to ranking.",
                    "label": 0
                },
                {
                    "sent": "So when I draw a ranking like this, what I mean is most likely to be positive to least likely to be positive if I draw them so behind each other, it means a tie.",
                    "label": 0
                },
                {
                    "sent": "So actually this is a random ranking because the rancor count distinguish between this positive and this negative.",
                    "label": 0
                },
                {
                    "sent": "This positive in this negative and is possibly is negative.",
                    "label": 0
                },
                {
                    "sent": "It incurs four and a half ranking errors.",
                    "label": 0
                },
                {
                    "sent": "OK, how can you introduce half ranking errors?",
                    "label": 0
                },
                {
                    "sent": "Well, that's exactly because of the tides, because this positive and the negative is not a whole ranking error because the negative is not in front of the positive.",
                    "label": 0
                },
                {
                    "sent": "But it's also not behind the positive, so you can't 1/2.",
                    "label": 0
                },
                {
                    "sent": "Another way to understand this four and a half is saying well, if I have three negatives and three positives, I can potentially make 9 ranking errors an I make half of those.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is pretty bad as a rancor.",
                    "label": 0
                },
                {
                    "sent": "As a classifier, I have to sort of cut this ranking somewhere and you will immediately see wherever I do that, I will incur three classification errors.",
                    "label": 0
                },
                {
                    "sent": "OK, whatever, I do it so it's a totally random model, OK?",
                    "label": 0
                },
                {
                    "sent": "Now look at this one.",
                    "label": 0
                },
                {
                    "sent": "As a rancor, this one actually makes more errors OK, because each of these negatives is in front of two positives, so 3 * 2 is 6 ranking errors, so we've gone from four and a half ranking errors to six ranking errors.",
                    "label": 1
                },
                {
                    "sent": "However, the best class of the best accuracy that I can achieve here is if I cut here, I correctly classified this.",
                    "label": 0
                },
                {
                    "sent": "Plus, I correctly classify the minus is and I only make errors on the two pluses.",
                    "label": 0
                },
                {
                    "sent": "So again we have a similar phenomenon we can.",
                    "label": 0
                },
                {
                    "sent": "Decrease ranking performance and at the same time increase classification performance, so the relation between all these things is perhaps a little bit less clearcut then than you think, and I thought it worthwhile to go through this for becausw.",
                    "label": 0
                },
                {
                    "sent": "In one of the referees for one of my email papers here said exactly this thing isn't a good probability estimator necessarily a good rank are an, so the answer is.",
                    "label": 0
                },
                {
                    "sent": "The answer is at the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "The answer is that depends on how you measure the quality of your probability estimates.",
                    "label": 0
                },
                {
                    "sent": "That will be one of the things that I will be addressing, so this is almost.",
                    "label": 0
                },
                {
                    "sent": "I call this a Clintonian answer.",
                    "label": 0
                },
                {
                    "sent": "Did you have sex with that woman?",
                    "label": 0
                },
                {
                    "sent": "Depends on how you define sex well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the the broad outline.",
                    "label": 0
                },
                {
                    "sent": "I have three parts, three main parts.",
                    "label": 0
                },
                {
                    "sent": "I will talk about building models and these will be tree based models as well.",
                    "label": 1
                },
                {
                    "sent": "I have three models for you and I will.",
                    "label": 0
                },
                {
                    "sent": "I will sort of analyze the process of how these models are being built and what you can do with them.",
                    "label": 0
                },
                {
                    "sent": "Then I will look at the relationship between classification and ranking and in the third part I will look at the relationship between ranking.",
                    "label": 1
                },
                {
                    "sent": "And probability estimation.",
                    "label": 0
                },
                {
                    "sent": "And here is a very brief executive summary for each of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's parts.",
                    "label": 0
                },
                {
                    "sent": "Now before I do that, I thought it might be a good idea to put a little health warning here because I know that some people in this audience will get very uneasy with some things that I do, because what I'll do is I'll mostly talk about the Roc curves as evaluated on the training set.",
                    "label": 0
                },
                {
                    "sent": "And from experience again in referee report, I notice that this is enough.",
                    "label": 0
                },
                {
                    "sent": "You know this is like a red rag to a bull for some people.",
                    "label": 0
                },
                {
                    "sent": "I think this is justified because I'm not so much not necessarily talking about evaluating models.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about training models, yeah, so if somebody would be training a decision tree.",
                    "label": 0
                },
                {
                    "sent": "And they would use the training data to decide the top split.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't say to them.",
                    "label": 0
                },
                {
                    "sent": "You can't do that.",
                    "label": 0
                },
                {
                    "sent": "That leads to overfitting.",
                    "label": 0
                },
                {
                    "sent": "You need to use a validation set for that.",
                    "label": 0
                },
                {
                    "sent": "That's what the training set is for.",
                    "label": 1
                },
                {
                    "sent": "So all I'm doing here is analyzed that process and.",
                    "label": 0
                },
                {
                    "sent": "And see what happens.",
                    "label": 0
                },
                {
                    "sent": "Actually, when we train these models, so this is not to say that generalization and overfitting aren't issues.",
                    "label": 1
                },
                {
                    "sent": "It's just to say that I don't have any particularly clever solutions for that in this talk, so it applies here as much as everywhere.",
                    "label": 0
                },
                {
                    "sent": "So yes, it is a real issue whether these rockers that I look at will generalize from training set to test set, but that's not something that concerns me very much.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "During this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The first part of the talk is about building models.",
                    "label": 0
                },
                {
                    "sent": "I will consider three types of models.",
                    "label": 1
                },
                {
                    "sent": "The first two are very well known decision trees and Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "The third one we actually introduced in a poster paper here at this conference, it's called Lex Rank.",
                    "label": 0
                },
                {
                    "sent": "It does lexicographic ranking, which is strictly as a strictly stronger bias than naive Bayes, so everything the Lex rank and do naive base can do, but not vice versa.",
                    "label": 0
                },
                {
                    "sent": "So if we go down, the bias increases, and if we go down the variance decreases.",
                    "label": 0
                },
                {
                    "sent": "What do these models have in common?",
                    "label": 0
                },
                {
                    "sent": "Well, actually you can understand them all as tree based models.",
                    "label": 0
                },
                {
                    "sent": "That may not be entirely obvious if you think about naive Bayes, for instance, because that's certainly not how I used to think of Naive Bayes, but this will become clear.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fedramp, so, let's have a look at the good old decision tree.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a running example through the 1st part, so I have two binary.",
                    "label": 0
                },
                {
                    "sent": "Attributes A&B value zero and one.",
                    "label": 0
                },
                {
                    "sent": "And so, for instance, here for a = 0 I have five positives and six negatives where I equals one.",
                    "label": 0
                },
                {
                    "sent": "I have four positives and sorry five positives and four negatives and so on.",
                    "label": 0
                },
                {
                    "sent": "If you look at this for a little bit, you see maybe that's a kind of.",
                    "label": 0
                },
                {
                    "sent": "It's certainly not linearly separable.",
                    "label": 0
                },
                {
                    "sent": "It's not even separable by Decision Tree.",
                    "label": 1
                },
                {
                    "sent": "It looks a little bit like an X or kind of concept, so there's mostly plus is there, and there must be minus is there and there, so it wouldn't be a surprise that if you train a decision tree on it, you get something which is a complete decision tree, which means it uses.",
                    "label": 0
                },
                {
                    "sent": "It uses all the attributes, so in this case we first split on B and then we split in both cases on A and then we get here.",
                    "label": 0
                },
                {
                    "sent": "The the joint distributions in each leaves are as follows, so if you want to turn this into a classifier, what can you do?",
                    "label": 0
                },
                {
                    "sent": "Well, you need a decision rule to turn these leaves to label them with classes.",
                    "label": 0
                },
                {
                    "sent": "So what one obvious possibility is using simply majority class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get in that case.",
                    "label": 0
                },
                {
                    "sent": "Plus minus minus plus because we have majority plus here majority minus majority minus majority plus an instant space, that's what it looks like.",
                    "label": 0
                },
                {
                    "sent": "So you recognize this kind of bimodal or or X or type of concept.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all well and good.",
                    "label": 0
                },
                {
                    "sent": "Now how would we use a decision tree as a rancor?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what we could look at is the posterior odds in each in each leaves, which is which is simply the ratio of positive to negative.",
                    "label": 0
                },
                {
                    "sent": "So the posterior odds here is 4 to one here.",
                    "label": 0
                },
                {
                    "sent": "Two to three here, one to five and here 3 to 1 positive to negative.",
                    "label": 0
                },
                {
                    "sent": "So if you just rank those then you will say this leave goes first.",
                    "label": 0
                },
                {
                    "sent": "This leaf goes second.",
                    "label": 0
                },
                {
                    "sent": "This leave goes third.",
                    "label": 0
                },
                {
                    "sent": "This leave goes forth.",
                    "label": 0
                },
                {
                    "sent": "In instance space that looks like this.",
                    "label": 0
                },
                {
                    "sent": "And as a ranking it looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically this leave goes there.",
                    "label": 0
                },
                {
                    "sent": "This leave goes there.",
                    "label": 0
                },
                {
                    "sent": "This leave goes there and this leave goes there so we have ties in the ranking naturally because all the instances that go into the same leave are tide in the ranking, we can't distinguish between them and so.",
                    "label": 0
                },
                {
                    "sent": "And this is quite a nice ranking because you know you go from mostly positives on the left side of the positives.",
                    "label": 0
                },
                {
                    "sent": "Go down when you go to the right and the.",
                    "label": 0
                },
                {
                    "sent": "The negatives go up so that, so that's that looks fine.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, what would we do if we use the decision tree as a probability estimate while we would use the?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Syria rods themselves.",
                    "label": 0
                },
                {
                    "sent": "So we still have in blue the posterior odds and you convert them into probabilities if you want, although I actually think that very often it's much easier to calculate in terms of posterior odds rather than probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you want to turn it into a probability, what you do is you say 4 / 4 + 1 is .8.",
                    "label": 1
                },
                {
                    "sent": "2 / 2 + 3 is .4 and so on.",
                    "label": 1
                },
                {
                    "sent": "So I visualized that there's a probability estimator in this way, so these these columns are the same as with the Rancor, but they are now distributed on the real axis from from one to zero.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is all nothing new when and you may wonder why I tell you all this.",
                    "label": 0
                },
                {
                    "sent": "OK, let's look at this a little bit.",
                    "label": 0
                },
                {
                    "sent": "Free.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My rock analysis perspective so.",
                    "label": 0
                },
                {
                    "sent": "I use Roc analysis a lot for visualizing, not necessarily evaluation, but just visualizing what's going on.",
                    "label": 0
                },
                {
                    "sent": "By the way, rock analysis I always get this question I always forget to mention it stands for receiver operating characteristic.",
                    "label": 0
                },
                {
                    "sent": "It's totally obsolete this this.",
                    "label": 0
                },
                {
                    "sent": "This comes from signal detection theory has its roots in the Second World War.",
                    "label": 0
                },
                {
                    "sent": "Like very many things do, but it's best just to treat it as as an abbreviation without.",
                    "label": 0
                },
                {
                    "sent": "The real meaning.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is basically.",
                    "label": 0
                },
                {
                    "sent": "If you want to visualize the performance, the main idea is that we take the class distribution in each leave as a vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for instance, this leave has a vector 4 app for app and one to the right, and this leave is visualized as a vector two up and three to the right.",
                    "label": 0
                },
                {
                    "sent": "And if you stack those in the ranking order, you get a curve like this.",
                    "label": 1
                },
                {
                    "sent": "And this is a bit like a lorence curve.",
                    "label": 0
                },
                {
                    "sent": "You know, these kind of curves, of which talk about wealth distribution.",
                    "label": 0
                },
                {
                    "sent": "So they say 20% of the people earn 80% of the income, and it's similar here.",
                    "label": 0
                },
                {
                    "sent": "So when you have this curve, you can say something like.",
                    "label": 0
                },
                {
                    "sent": "You can say things like the numbers don't apply here, but you can say things like 40% of the negatives are put together with 60% of the positives.",
                    "label": 0
                },
                {
                    "sent": "And of course you want that difference to be to be big.",
                    "label": 0
                },
                {
                    "sent": "So that's basically where the Roc curves come from.",
                    "label": 0
                },
                {
                    "sent": "So in this curve what you see is on the vertical axis, you have positive examples on the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "You have negative examples, and somehow this curve visualizes.",
                    "label": 1
                },
                {
                    "sent": "Essentially, the ranking performance of this of this classifier.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "You may be more used to actually not having absolute numbers on the axis, but relative numbers, namely.",
                    "label": 1
                },
                {
                    "sent": "You're probably used to having a true positive rate here, which is the absolute numbers of positive normalized by the actual number of positive right?",
                    "label": 0
                },
                {
                    "sent": "So instead of",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous graph, we would say 4.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next graph, we will say 4 out of 10 positive examples, so we have .4 and that's the true positive rate and you do something similar on the horizontal axis.",
                    "label": 1
                },
                {
                    "sent": "This has confused me for a long time, but here is how I got to grips with this.",
                    "label": 0
                },
                {
                    "sent": "I now switch very freely between these two perspectives and the reason is that if you do the typical rock analysis thing, you essentially take the class distribution out of the equation an your slopes in your space are likelihood ratios.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the thing that you multiply with the prior odds to get the posterior odds.",
                    "label": 1
                },
                {
                    "sent": "So The upshot of this is.",
                    "label": 1
                },
                {
                    "sent": "If you want likelihood ratios because you somehow want to take the prior odds to class distribution out of the equation.",
                    "label": 0
                },
                {
                    "sent": "This Roc curve is your thing.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you instead want posterior odds.",
                    "label": 0
                },
                {
                    "sent": "Then and you want to take the class distribution in, then this space is is the right space to work in.",
                    "label": 0
                },
                {
                    "sent": "Now you will have noticed that I've cunningly used an example with an equal class distribution, so it doesn't really matter, but you can imagine that if you have unequal class distribution, this space will become a rectangle, whereas the other one is still a square.",
                    "label": 0
                },
                {
                    "sent": "OK, so just I I just choose whatever is seems most appropriate at the occasion, and most of this talk I will actually use.",
                    "label": 0
                },
                {
                    "sent": "The absolute numbers because I'm interested in posterior odds.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called coverage space an before that it was called.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In space, but it's I. I basically loosely call all this rock space, but it is important.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To distinguish between the two.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think about this a little bit more.",
                    "label": 0
                },
                {
                    "sent": "How can we use this space to visualize the performance of these kind of trees?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the way I like to think about it.",
                    "label": 0
                },
                {
                    "sent": "If I have a tree with four leaves, then think about all possible ways that I can label the tree for leaves two possible classes.",
                    "label": 1
                },
                {
                    "sent": "So I have two to the four.",
                    "label": 0
                },
                {
                    "sent": "Is 16 possible labelings and all of these can be actually visualized as points in this rock space.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the labeling plus minus, minus minus would correspond to this point here.",
                    "label": 0
                },
                {
                    "sent": "OK, the labeling minus plus plus plus which doesn't make.",
                    "label": 0
                },
                {
                    "sent": "An awful lot of sensors are labeling, but at least in theory you can do that minus plus plus plus would as sort of correctly classify one negative an 236 positives.",
                    "label": 0
                },
                {
                    "sent": "So did I do this correctly?",
                    "label": 0
                },
                {
                    "sent": "So what did I say minus minus plus plus plus where are we?",
                    "label": 0
                },
                {
                    "sent": "Where are we?",
                    "label": 0
                },
                {
                    "sent": "There we are and actually.",
                    "label": 0
                },
                {
                    "sent": "I won't take you long to figure that out.",
                    "label": 0
                },
                {
                    "sent": "That is symmetrically opposite, so it's it's point mirror through the center of gravity of the space, so it's point mirror through the through the midpoint on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So you have a lot of symmetry in this in this space, but the so the reason why I bring this up is that somehow all possible rancors and classifiers that I can construct from this tree have to somehow go through some of these 16 points.",
                    "label": 0
                },
                {
                    "sent": "There are no more points in.",
                    "label": 1
                },
                {
                    "sent": "In Roc space, this is what this tree can achieve on this.",
                    "label": 0
                },
                {
                    "sent": "On this data set on this training set.",
                    "label": 0
                },
                {
                    "sent": "And the actual if I use the posterior as of this ranking order, I use some of these points, and in fact I use the outermost point which which is the convex Hull of this of these points and that is not.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A coincidence?",
                    "label": 0
                },
                {
                    "sent": "Now let's use this idea to think about how would this tree be built?",
                    "label": 0
                },
                {
                    "sent": "So Decision tree building is a nice recursive process.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We find the best split at the root and then we recursively split the left tree and the right tree until until we finished and we can actually visualize that very nicely in Roc space.",
                    "label": 0
                },
                {
                    "sent": "So because choosing the top split is nothing else, then basically we have a decision stump here and that.",
                    "label": 0
                },
                {
                    "sent": "If we use this as a rank are we would basically go through this point because the left leave has six positives and four negatives and the right leaf has four positives and six negative.",
                    "label": 0
                },
                {
                    "sent": "So actually this tree is visualized.",
                    "label": 0
                },
                {
                    "sent": "By this Roc curve and what we are now going to do by growing the tree in the left by growing the tree at the left, we're going to refine the curve here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the next step we get something like this.",
                    "label": 0
                },
                {
                    "sent": "And then fine.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Early in the third step, we grow the right subtree and we got something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, now there is.",
                    "label": 0
                },
                {
                    "sent": "I can see sort of a question coming up with some of you, which is.",
                    "label": 0
                },
                {
                    "sent": "This doesn't quite look like the Roc curve that you showed us before.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "And the answer is, well, what I show you here is not the ranking order, it is just the order in which the tree is being built.",
                    "label": 0
                },
                {
                    "sent": "It is the recursive order of the trees, but decision trees of course don't always order their leaves from left to right.",
                    "label": 0
                },
                {
                    "sent": "So we have an additional step here, and this is a crucial.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crucial thing we take this Roc curve and then.",
                    "label": 0
                },
                {
                    "sent": "Using the posterior odds, we're going to reorder those leaves and we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At something like this.",
                    "label": 0
                },
                {
                    "sent": "I'm very fond of that.",
                    "label": 0
                },
                {
                    "sent": "This is my very first animation.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you want to see it again.",
                    "label": 0
                },
                {
                    "sent": "So here we go.",
                    "label": 0
                },
                {
                    "sent": "So this is the key bit.",
                    "label": 0
                },
                {
                    "sent": "Decision trees have some kind of syntactic structure, but that is not what defines the ranking order.",
                    "label": 0
                },
                {
                    "sent": "We then have the posterior odds in the leaves and dead defines the ranking order, so we have those vectors, but we put them in a different order and actually you can quite easily proven this has been known for some time.",
                    "label": 0
                },
                {
                    "sent": "That on the training set you will always get the convex Roc curve.",
                    "label": 1
                },
                {
                    "sent": "So actually the decision tree that you get is the best rank are on the training set, so there is already such a link between good classification.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, let's move on to naive Bayes so.",
                    "label": 0
                },
                {
                    "sent": "It may not be immediately obvious that naive Bayes is a tree, but certainly if you have discrete attributes it is a tree like structure.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Well, naive Bayes needs to estimate the probability for each possible combination of attribute values, and that's what you can do with the decision tree.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what the decision tree does.",
                    "label": 0
                },
                {
                    "sent": "Of course, it is always a complete decision tree, so there is no pruning there because you need all the all the possible combinations of attribute values.",
                    "label": 0
                },
                {
                    "sent": "Also, of course there are many different ways in which to draw complete decision trees, and here is just I just used the same three structures before.",
                    "label": 0
                },
                {
                    "sent": "So what is happening here?",
                    "label": 0
                },
                {
                    "sent": "Well, naive Bayes as a probability estimator.",
                    "label": 1
                },
                {
                    "sent": "Doesn't use the joint probabilities, but it uses marginal probabilities to estimate the joint probabilities so you can understand this as follows.",
                    "label": 0
                },
                {
                    "sent": "I have the prior odds here, which is uniform, so it doesn't really do anything.",
                    "label": 0
                },
                {
                    "sent": "Then I have.",
                    "label": 0
                },
                {
                    "sent": "The marginal for a for a = 0.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the marginal for B = 0 is 6 positives.",
                    "label": 0
                },
                {
                    "sent": "For negatives, the marginal for B = 1 is for positive 6 negatives one level down.",
                    "label": 0
                },
                {
                    "sent": "You don't use the joint as in decision trees, but you use again the marginal.",
                    "label": 0
                },
                {
                    "sent": "So the marginal for a = 0.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, I was pointing at the wrong one, so a = 0.",
                    "label": 0
                },
                {
                    "sent": "Is this one 5 positive 6 negatives?",
                    "label": 0
                },
                {
                    "sent": "And here we have 5 positive, four negatives and that's repeated there.",
                    "label": 0
                },
                {
                    "sent": "And what naive Bayes then does is simply multiply these.",
                    "label": 0
                },
                {
                    "sent": "Marginal likelihoods, likelihood ratios to obtain an estimate of the joint likelihood ratio combined with the prior odds gives you the posterior odds.",
                    "label": 0
                },
                {
                    "sent": "If it's my mother, I'm not here.",
                    "label": 0
                },
                {
                    "sent": "So, and you can visualize this as a rancor, so you can you can.",
                    "label": 0
                },
                {
                    "sent": "You can transform these probabilities.",
                    "label": 0
                },
                {
                    "sent": "Sorry, these ratios into probabilities as we did before.",
                    "label": 0
                },
                {
                    "sent": "We can visualize this is the Rancor and as you see, Naive Bayes makes one quite big mistake because the marginals in this case are misleading, right?",
                    "label": 0
                },
                {
                    "sent": "The marginal say 1 1/2 times more positives than negatives here and again more positive than negative here.",
                    "label": 0
                },
                {
                    "sent": "So if you multiply those, you get something like twice as many.",
                    "label": 0
                },
                {
                    "sent": "Positives and negatives here, but in effect.",
                    "label": 0
                },
                {
                    "sent": "There are more negatives and positives, so naive Bayes commits a ranking error which which we all know.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That it is likely to do that, and you can visualize that in Roc space by this concavity here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so again, we have this only these 16 points to work with, so the rocker for Naive Bayes has to go through these points OK. And in blue, I've actually indicated the marginals because the marginals are decision stumps and again they must be somewhere among those 16 points.",
                    "label": 0
                },
                {
                    "sent": "So actually I think this is B = 0.",
                    "label": 0
                },
                {
                    "sent": "This is a equals one I think.",
                    "label": 0
                },
                {
                    "sent": "This is a = 0 and this is B equals.",
                    "label": 1
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically naive Bayes, all naive Bayes has is those four blue vectors and it tries to estimate where you will end up in Roc space, and it sometimes makes makes mistakes there.",
                    "label": 0
                },
                {
                    "sent": "So we have misleading marginal probabilities.",
                    "label": 1
                },
                {
                    "sent": "We can't do anything about that because the only way to repair that is if we have access to the true joint probabilities, which we don't.",
                    "label": 1
                },
                {
                    "sent": "So we have this ranking order here.",
                    "label": 0
                },
                {
                    "sent": "But there's something interesting about this ranking order becausw by a simple restructuring of the tree.",
                    "label": 0
                },
                {
                    "sent": "It actually becomes a left to right ordering of that tree.",
                    "label": 0
                },
                {
                    "sent": "So it says 2143.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If I just swap the values of a around in both cases I get 1234.",
                    "label": 0
                },
                {
                    "sent": "Now this is what I call a lexicographic ranking.",
                    "label": 0
                },
                {
                    "sent": "It is a simple left to right ordering of the tree which we get if we put B before a an we always put B = 0 B 4B equals one an A equals 1B, four A = 0.",
                    "label": 1
                },
                {
                    "sent": "So why do I?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All this lexicographic ranking well think think about lexicographic ranking is what you do to find people in in the phone book for instance.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know whether Bar comes before flash, then you look at they say why they start with different letters and becomes before F, so I know now the relative ranking.",
                    "label": 0
                },
                {
                    "sent": "But if you want to know whether.",
                    "label": 0
                },
                {
                    "sent": "Flag comes before flag with CK.",
                    "label": 0
                },
                {
                    "sent": "One of the very many amputations of my name that I have to suffer in England.",
                    "label": 0
                },
                {
                    "sent": "Then you use canned two names until you find the first position where they differ, and then you say OK, Ha H comes before K right and then you ignore whatever comes after that, right?",
                    "label": 0
                },
                {
                    "sent": "So lexicographic ranking is basically finding the first point where two things differ and use that point as a criterion which to order and ignore the rest.",
                    "label": 1
                },
                {
                    "sent": "And that's exactly what we're doing here.",
                    "label": 1
                },
                {
                    "sent": "So if we want to compare 2 examples if they differ in attribute B. I just need to know which one of them has B = 0 and I know that that one goes before the other one and I can ignore the rest OK and then if they are equal for BI, need to go deeper in the tree.",
                    "label": 0
                },
                {
                    "sent": "Of course, in practice you wouldn't do this with the tree, because that requires exponential space.",
                    "label": 1
                },
                {
                    "sent": "But this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the principle.",
                    "label": 0
                },
                {
                    "sent": "And in practice, we use the odds ratio two to order the attributes because that basically gives us the fact that lexical graphic ranking has strictly stronger bias than naive base.",
                    "label": 0
                },
                {
                    "sent": "So I will do this very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The odds ratio can be understood as yet another splitting criterion, but it actually has quite nice nice properties.",
                    "label": 0
                },
                {
                    "sent": "So basically the odds ratio is P / N, which is the the examples that go left divided by the ratio of the examples that go right and one of the reasons why I like this so much is that unlike something like information gain, for instance, the odds ratio has a closed form.",
                    "label": 1
                },
                {
                    "sent": "If you want to draw these nice red lines, which are the easy metrics in rock space.",
                    "label": 0
                },
                {
                    "sent": "So one way to understand it actually, if you look if you look at this.",
                    "label": 0
                },
                {
                    "sent": "Race showed and actually P.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times N -- N is something like the area of this guy here.",
                    "label": 0
                },
                {
                    "sent": "And the other one is the area of that guy there.",
                    "label": 0
                },
                {
                    "sent": "So a point.",
                    "label": 0
                },
                {
                    "sent": "This point here has odds ratio 5 because this guy is 5 times bigger.",
                    "label": 0
                },
                {
                    "sent": "Is that one?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if if I go along this curve.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I was, I will keep that property so it has a nice geometric interpretation and it's easy to implement.",
                    "label": 0
                },
                {
                    "sent": "So in what I'm about to show you, we actually use the odds ratio also as a decision.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Splitting criteria OK. Now comes the moment of truth.",
                    "label": 0
                },
                {
                    "sent": "I've been foolish enough to decide to give Demo, told my students Terra Capital with an Epson Takashima to borrow.",
                    "label": 0
                },
                {
                    "sent": "Have spent long and often nightly hours to to produce this.",
                    "label": 0
                },
                {
                    "sent": "It's still in beta, which means anything can happen.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's the leaving in Alpha.",
                    "label": 0
                },
                {
                    "sent": "So I just want to visualize some of these ideas here.",
                    "label": 0
                },
                {
                    "sent": "The program is called prog rock.",
                    "label": 0
                },
                {
                    "sent": "If you're old enough to understand the pond and maybe you even recognize the logo there, but best not to say too much about that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So far so good, so I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll start it up.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a Java archive.",
                    "label": 0
                },
                {
                    "sent": "It actually incorporates some of the up the classifiers of waka, so let me just show you what it does.",
                    "label": 0
                },
                {
                    "sent": "I can open an ARF file so waka.",
                    "label": 0
                },
                {
                    "sent": "Data file, let's say diabetes, binarized version of diabetes, because, for visualization purposes we only have binary attributes.",
                    "label": 0
                },
                {
                    "sent": "So what you're looking at now is all the.",
                    "label": 0
                },
                {
                    "sent": "Is basically the recursive structure of the decision tree that is built, so let's me let me just with this slide.",
                    "label": 0
                },
                {
                    "sent": "There I can go further down or higher up in the tree.",
                    "label": 0
                },
                {
                    "sent": "So I'm essentially pruning the tree.",
                    "label": 0
                },
                {
                    "sent": "So essentially maybe it will be clearer here.",
                    "label": 0
                },
                {
                    "sent": "What happens here is that at the top level we split here and that gives us two new spaces to work in and then basically recursively we keep on splitting those.",
                    "label": 0
                },
                {
                    "sent": "Those those subspaces in the way that they explained what you also can do is.",
                    "label": 0
                },
                {
                    "sent": "Actually, look at the alternative.",
                    "label": 0
                },
                {
                    "sent": "So at this level these square these rectangle points are actually alternative decision stump splits at the top level.",
                    "label": 0
                },
                {
                    "sent": "And what we also can do is show why did we choose that one?",
                    "label": 0
                },
                {
                    "sent": "Well, that will become immediately clear if we draw the asymmetric.",
                    "label": 0
                },
                {
                    "sent": "Because that's the odds ratio is a metric, and that is the point that is on the highest is a metric basically.",
                    "label": 0
                },
                {
                    "sent": "So we can understand which one we choose, and we can also understand that maybe it's a close call here, so it's maybe not so obvious which one of the two to choose.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you can do is actually.",
                    "label": 0
                },
                {
                    "sent": "Show the Roc curve that you would get if you cut off the tree at that level.",
                    "label": 1
                },
                {
                    "sent": "OK, now at the decision stump level.",
                    "label": 0
                },
                {
                    "sent": "That isn't very interesting, but.",
                    "label": 0
                },
                {
                    "sent": "Once we go deeper into the tree, we actually refine the Roc curve, but also notice that because of the reordering effect, I don't necessarily go through these points anymore.",
                    "label": 0
                },
                {
                    "sent": "Remember, those points are purely following from the recursive structure of the tree, but the Roc curve comes from a ranking, so we can basically see how we can understand how the decision tree kind of obtains his Roc curve by reordering their segments.",
                    "label": 0
                },
                {
                    "sent": "So let's look at another model here.",
                    "label": 0
                },
                {
                    "sent": "At the other extreme, we have Lex rank.",
                    "label": 0
                },
                {
                    "sent": "There we go and let's rank as I told you, use is purely the left or right ordering of the tree.",
                    "label": 0
                },
                {
                    "sent": "So there there is no reordering and we have a Roc curve that is exactly following all those points.",
                    "label": 0
                },
                {
                    "sent": "Although split points that we have in the tree and again we can sort of cut off the tree at higher up levels and see what rocker we would get there and finally 4.",
                    "label": 0
                },
                {
                    "sent": "Naive bayes.",
                    "label": 0
                },
                {
                    "sent": "The Naive Bayes is a little bit in between, so it kind of can be understood as building the same tree as Lex rank, but it does take some reordering decisions.",
                    "label": 0
                },
                {
                    "sent": "And sometimes these reordering decisions make things better, and sometimes they make things worse, OK, which which can happen?",
                    "label": 0
                },
                {
                    "sent": "This as we know we can have concavities in the in a naive Bayes rocker so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to leave the demo at that.",
                    "label": 0
                },
                {
                    "sent": "So, as I say, this is currently still in there in beta, but we've put up a rudimentary web page.",
                    "label": 0
                },
                {
                    "sent": "It's actually the beta can be downloaded from there.",
                    "label": 0
                },
                {
                    "sent": "If you want to want to play with it, then we are planning to release it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the public domain.",
                    "label": 0
                },
                {
                    "sent": "OK, so to summarize this part, when we talk about building models, we have sort of a range of models.",
                    "label": 0
                },
                {
                    "sent": "We have decision trees that have full control over how things are ranked because they have access to the joint probabilities.",
                    "label": 1
                },
                {
                    "sent": "On the other extreme, we have Lex rank which does things in a purely syntactic manner and follows their recursive structure of the tree, and we have naive Bayes which does something in between.",
                    "label": 1
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "I won't explain it, but this is something that is not lexical graphic, but can be represented by by now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's move on to classification and ranking.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here's a quote from a paper that I recently saw.",
                    "label": 0
                },
                {
                    "sent": "In the paper says, AUC aggregates the models behavior for all possible decision thresholds.",
                    "label": 1
                },
                {
                    "sent": "I'm not actually quite sure what that means.",
                    "label": 0
                },
                {
                    "sent": "And I'm allowed to say that because it's from one of my papers.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying I'm actually not quite sure what I mean there right?",
                    "label": 1
                },
                {
                    "sent": "So I decided to actually try to analyze this question a bit more so that we actually mean that in some sense.",
                    "label": 0
                },
                {
                    "sent": "Accuracy is an expected value.",
                    "label": 1
                },
                {
                    "sent": "Sorry, AUC is an expected value of accuracy and what is the random experiment there?",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is?",
                    "label": 0
                },
                {
                    "sent": "Look at that a little bit deeper and sort of I'm I'm looking at it.",
                    "label": 0
                },
                {
                    "sent": "They're very simple arguments, but I'm looking at it in two directions, so can I go from AUC to accuracy an?",
                    "label": 0
                },
                {
                    "sent": "Can I go from?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He received the way you see, so the first over here is some notation.",
                    "label": 1
                },
                {
                    "sent": "Basically, the notation is pretty obvious.",
                    "label": 0
                },
                {
                    "sent": "What you just need to remember is that I try to adopt the custom that if something starts with an initial capital, it is meant to be an absolute frequency, and if it starts with if it's all lower case, it's meant to be a relative frequency.",
                    "label": 0
                },
                {
                    "sent": "So for instance posses the actual number of positives and lower case pause is the relative number of positives.",
                    "label": 1
                },
                {
                    "sent": "That's roughly what I tried to do anyway.",
                    "label": 0
                },
                {
                    "sent": "Then then we have the usual things like true positives and true negative.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And OK, so here is a little bit machine learning 101.",
                    "label": 0
                },
                {
                    "sent": "Very briefly, if you have this rankings, how do you get to two Roc curve?",
                    "label": 1
                },
                {
                    "sent": "I explained that through a decision tree, but here is basically if you just have a ranking which you basically do, which you draw the curve as follows, you go from left to right through the ranking and if it is a positive you go up and if it's a negative you go to the right and you normalize these things so that you end up in one one at the end.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you have ties in the ranking, then you make diagonal moves.",
                    "label": 0
                },
                {
                    "sent": "OK, so given that we just had a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning 101.",
                    "label": 0
                },
                {
                    "sent": "Let's have machine learning 101 exam OK so here is question 42 of machine learning 101.",
                    "label": 1
                },
                {
                    "sent": "AUC is.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a multiple choice question, so don't worry.",
                    "label": 0
                },
                {
                    "sent": "So here's the example before AUC.",
                    "label": 0
                },
                {
                    "sent": "For those of you haven't seen it, simply the area under this curve.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the proportion of the unit square that is under this curve area.",
                    "label": 0
                },
                {
                    "sent": "AOC is 1 minus AUC is the area over the curve.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you see is a.",
                    "label": 0
                },
                {
                    "sent": "The expectation that the uniformly drawn random positive is ranked before a uniformly drawn random negative.",
                    "label": 1
                },
                {
                    "sent": "Sounds may be familiar to some of you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be.",
                    "label": 0
                },
                {
                    "sent": "The expected proportion of positives ranked before uniformly drawn negative.",
                    "label": 1
                },
                {
                    "sent": "OK, so uniformly drawn negative.",
                    "label": 0
                },
                {
                    "sent": "Let's say that one I have, in this case 24689 positives in front of it.",
                    "label": 0
                },
                {
                    "sent": "So what is the expected value expected proportion of?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Positives in front of it.",
                    "label": 0
                },
                {
                    "sent": "See the expected true positive rate.",
                    "label": 1
                },
                {
                    "sent": "If the ranking is split just before a uniformly drawn random negative.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "D. The expected proportion of negatives ranked after a uniformly drawn random positive.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "E. 1 minus the expected false positive rate if the ranking is split just after uniformly drawn random positive, anybody dare to venture something?",
                    "label": 1
                },
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very good RF.",
                    "label": 0
                },
                {
                    "sent": "All of the above, absolutely.",
                    "label": 1
                },
                {
                    "sent": "And the way to see this is actually it's very instructive to actually see this.",
                    "label": 0
                },
                {
                    "sent": "So first of all, A is the typical one, and it's sort of.",
                    "label": 0
                },
                {
                    "sent": "In this unit square you know you have these little cells, and each cell represents a coupling of a positive and a negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you randomly just put your finger somewhere, then the bigger the area is on the curve, the higher the probability is that you'll end under the curve, which is where they is.",
                    "label": 0
                },
                {
                    "sent": "But B&C are actually something very similar.",
                    "label": 0
                },
                {
                    "sent": "So if I uniformly select a negative, it means that they uniformly selected point on this axis, right?",
                    "label": 0
                },
                {
                    "sent": "And what I'm asking for, what is the expected height?",
                    "label": 0
                },
                {
                    "sent": "Of the column, right?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if a uniformly select one of these axes, and I do that many times, then what will I get?",
                    "label": 0
                },
                {
                    "sent": "On average I will get the average height of the column, which is again the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area under the curve and you can do something very similar with positives in area over the curve and so on.",
                    "label": 0
                },
                {
                    "sent": "The reason why I bring this up is that there is indeed something like.",
                    "label": 0
                },
                {
                    "sent": "For instance, C says.",
                    "label": 0
                },
                {
                    "sent": "AUC equals the expected value of the true positive rate under some you know experiment, so maybe that gives us a link to what this what is a connect.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Between AUC and accuracy?",
                    "label": 0
                },
                {
                    "sent": "And actually the connection is is very simple, so.",
                    "label": 0
                },
                {
                    "sent": "What you can do.",
                    "label": 0
                },
                {
                    "sent": "Somebody gives you a ranking.",
                    "label": 1
                },
                {
                    "sent": "An you do exactly that, you say if I randomly split it somewhere, what is my expected accuracy?",
                    "label": 0
                },
                {
                    "sent": "And if you do that I won't bore you with the details.",
                    "label": 0
                },
                {
                    "sent": "You get something like that.",
                    "label": 1
                },
                {
                    "sent": "And if you simplify it, a uniform class distribution, you get something like this.",
                    "label": 0
                },
                {
                    "sent": "So there is a linear relationship.",
                    "label": 0
                },
                {
                    "sent": "Also, we have a constant term here.",
                    "label": 0
                },
                {
                    "sent": "Where does that come from?",
                    "label": 0
                },
                {
                    "sent": "Well, if my ranking has AUC zeros, it's the worst possible ranking all the negatives before all the positives.",
                    "label": 0
                },
                {
                    "sent": "If I land on a negative, on average, I will land in the middle and that will give me.",
                    "label": 0
                },
                {
                    "sent": "I will correctly classify half of those negatives, right?",
                    "label": 0
                },
                {
                    "sent": "So that's where the quarter comes from.",
                    "label": 1
                },
                {
                    "sent": "So even with the worst possible ranking on average, you get still something out of it.",
                    "label": 0
                },
                {
                    "sent": "But also notice that this can have very high variance, because if you ask yourself what are the maximum and minimum values?",
                    "label": 0
                },
                {
                    "sent": "Well, if you have the worst possible ranking and you happen to split on the point where negatives goes to positive, you will get accuracy 0.",
                    "label": 0
                },
                {
                    "sent": "And the best possible case is majority class, so you may have a very wide range of value.",
                    "label": 1
                },
                {
                    "sent": "So even though there is a linear relationship in expectation.",
                    "label": 0
                },
                {
                    "sent": "This probably doesn't mean that if if we use one lesson optimization criterion, that means that we also optimize the other, which is an argument that has been made before.",
                    "label": 0
                },
                {
                    "sent": "So the argument is yes, if you look at the expected values there, maybe it may be a linear relationship, but actually there is so much variance that you can't really exploit it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relationship you can do something similar if you go in the opposite direction, although it is a little bit more complicated in the interest of time I will only talk about this very briefly, but you can think of the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose I tell you that my classifier has a certain accuracy and you know the class distribution.",
                    "label": 0
                },
                {
                    "sent": "This is like almost like knowing one point on the Roc curve and I ask you the question, what is the expected value of the area under?",
                    "label": 0
                },
                {
                    "sent": "The curve that goes through that point and you have to somehow average overall curves that go through that point, and one way that you can.",
                    "label": 0
                },
                {
                    "sent": "So basically what it means is that you know accuracy, so you know that you're somewhere on this.",
                    "label": 0
                },
                {
                    "sent": "Accuracy is a metric, but you don't know where.",
                    "label": 0
                },
                {
                    "sent": "So the and and one simplification that I make is that let's just look at three point rockers, so rocker, which basically ties everything before the split point and everything after the split.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we could have.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roc curve like this.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we could have a rocker.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this like this like.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is on average.",
                    "label": 0
                },
                {
                    "sent": "We would have a Roc curve that goes.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Through the midpoint on that line and with equal class distribution we would have something like that.",
                    "label": 0
                },
                {
                    "sent": "So in that case, you could say that the expected value of AUC in that case, if I tell you here is the value of accuracy with this uniform class distribution.",
                    "label": 1
                },
                {
                    "sent": "Your best answer would probably be, well, I'm guessing that the AUC is equal to the accuracy, but again, this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Has large variance.",
                    "label": 0
                },
                {
                    "sent": "Because the worst possible rocker through that point would be this one which has AUC, accuracy square.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The best possible Roc curve that goes through that point would be this one, which has a you see 1 -- 1 minus accurate square, 1 minus the error square.",
                    "label": 0
                },
                {
                    "sent": "So again, there is sort of some kind of linear relationship, but because of the large variance.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can't really exploit this.",
                    "label": 0
                },
                {
                    "sent": "So if we have situations like.",
                    "label": 0
                },
                {
                    "sent": "Adaboost seem to optimize AUC or SVM seem to optimize AUC.",
                    "label": 0
                },
                {
                    "sent": "Then we need a model specific argument to explain this point.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, there was a very nice paper here which did that for SVM's.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm moving on to the third and final part, which is about the relation between ranking and probability estimation.",
                    "label": 1
                },
                {
                    "sent": "Why do we need calibrated probability estimates?",
                    "label": 0
                },
                {
                    "sent": "Well, I like to say that if my probability estimates are not calibrated in some sense, and in fact we don't have a probability estimator at all, we have are anchor.",
                    "label": 1
                },
                {
                    "sent": "So I like to say naive Bayes is not a probability estimator.",
                    "label": 0
                },
                {
                    "sent": "It happens to output numbers between zero and one.",
                    "label": 0
                },
                {
                    "sent": "But what it does is ranking if you want probabilities you need to calibrate them.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's why we're going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "And one reason why you won't calibrate the probability estimates is that that gives you to justification to, for instance, put your threshold at .5.",
                    "label": 0
                },
                {
                    "sent": "If your probability estimates are not calibrated, there is no justification for such a threshold and you have to learn the threshold from the data.",
                    "label": 0
                },
                {
                    "sent": "So what calibration essentially means is that suppose I have my model and I have a test set and I record all the predictions of the model of the test set, and now I look at the examples that have a particular predicted probability.",
                    "label": 0
                },
                {
                    "sent": "In a well calibrated classifier, the proportion of positives to negatives would correspond to that probability.",
                    "label": 0
                },
                {
                    "sent": "So if my if I condition the examples on predicted probability .5.",
                    "label": 1
                },
                {
                    "sent": "For a well calibrated classifier, I would expect as many positives as negatives in that case.",
                    "label": 0
                },
                {
                    "sent": "And then actually, without going through in through all the details.",
                    "label": 0
                },
                {
                    "sent": "Basically, this proportion of positives to negatives is essentially the slope of your Roc curve.",
                    "label": 1
                },
                {
                    "sent": "So a well calibrated classifier which uses it scores to order the segments of the rocker.",
                    "label": 0
                },
                {
                    "sent": "If the score corresponds to the slope of the segment, then it means that the curve is convex, right?",
                    "label": 0
                },
                {
                    "sent": "Because the score is monotonically decreasing along the curve and then the slopes are monotonically decreasing as well.",
                    "label": 1
                },
                {
                    "sent": "So this suggests a very simple calibration procedure.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Because we can take outcome Roc Curve and if it isn't convex we can construct the convex Hull, which essentially means introducing ties in the ranking and then we can just read off the probability from the slopes of the segments.",
                    "label": 0
                },
                {
                    "sent": "Now this is one of these moments in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Somebody said then machine learning that it's easy to pick out the good ideas in machine learning because they get invented, reinvented over and over again.",
                    "label": 0
                },
                {
                    "sent": "And this is one of those moments.",
                    "label": 0
                },
                {
                    "sent": "So when I discovered this, not only that, I discovered that actually this procedure already existed under the name of his atomic regression and the pair adjacent violators.",
                    "label": 0
                },
                {
                    "sent": "Paradise and violators itself was, I think, invented somewhere in the 50s or 60s.",
                    "label": 0
                },
                {
                    "sent": "It was introduced to machine learning by Bianca's address and Charles Elcome.",
                    "label": 0
                },
                {
                    "sent": "The connection with the rock convex Hull was made first by Tom Fawcett, an Alex Nicolesco, and they published it as a.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice research note in the Machine Learning Journal.",
                    "label": 0
                },
                {
                    "sent": "They just beat us to it.",
                    "label": 0
                },
                {
                    "sent": "Honestly, I could say we did it independently, but they get the credit because they published it first.",
                    "label": 0
                },
                {
                    "sent": "Notice also that this is exactly what decision trees do, by the way, so this is a very easy argument that decision trees are well calibrated.",
                    "label": 1
                },
                {
                    "sent": "First of all, we don't need the raw convex Hull because the curves are convex already an it uses the slope of each segment to assign the probabilities, so this is I feel not so well known.",
                    "label": 0
                },
                {
                    "sent": "Decision trees are by construction perfectly calibrated on the training set.",
                    "label": 0
                },
                {
                    "sent": "Now there are issues here, particularly the large variance decision trees so.",
                    "label": 0
                },
                {
                    "sent": "So there is a question of how well it generalizes, but.",
                    "label": 0
                },
                {
                    "sent": "I think this is something that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That deserves wider wider knowledge.",
                    "label": 0
                },
                {
                    "sent": "So how does this rock convex Hull calibration method work?",
                    "label": 0
                },
                {
                    "sent": "Well, here I tried to put it in one picture and I hope it is relatively clear here.",
                    "label": 0
                },
                {
                    "sent": "We have the ranking that we had before with its associated Roc curve and this ranking has concavities.",
                    "label": 0
                },
                {
                    "sent": "So if you look at what the convex Hull of this ranking is it is this red line here right?",
                    "label": 0
                },
                {
                    "sent": "And that means that we're going to put our.",
                    "label": 0
                },
                {
                    "sent": "Ranking into bins.",
                    "label": 0
                },
                {
                    "sent": "So all the positives got all the 1st four positives go into one bin and then we have another segment here, so that's there.",
                    "label": 0
                },
                {
                    "sent": "And then we have another segment there and so on and we simply read of the predicted probabilities from the slope of these segments of the slope is 1.",
                    "label": 0
                },
                {
                    "sent": "Then we you predict .5 and so in other words we get this calibration map because if you if you want to plot it as a calibration map, you somehow need to have scores to start from.",
                    "label": 0
                },
                {
                    "sent": "Although the procedure works.",
                    "label": 0
                },
                {
                    "sent": "Perfectly well if you only have a ranking in our scores, but the essential point is, is that your calibration map has these piecewise constant segments?",
                    "label": 0
                },
                {
                    "sent": "Now, maybe you think, well, that's a bit nasty computationally, and so, but I would argue this is a feature, not a bug, because it is the piecewise constant bits that improve the ranking because they construct the convex Hull.",
                    "label": 1
                },
                {
                    "sent": "If it was a monotonic curve, the Roc curve wouldn't change.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at another.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your calibration method.",
                    "label": 0
                },
                {
                    "sent": "Which is actually quite well known as well, which would be something like, you know, pushing your scores through a sigmoid like you would do in logistic regression, for instance.",
                    "label": 0
                },
                {
                    "sent": "What is the justification for that?",
                    "label": 0
                },
                {
                    "sent": "Well, the justification is that your scores must be normally distributed.",
                    "label": 0
                },
                {
                    "sent": "Both of them actually with the same variance.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm saying is this is this this this red and blue curve is kind of the density of the score, so we have lots of scores around.",
                    "label": 0
                },
                {
                    "sent": "Lots of negative scores around .4, lots of positive scores around six and so.",
                    "label": 0
                },
                {
                    "sent": "If your scores approaching normal distribution is well known from signal detection theory that your Roc curve starting to look very nice.",
                    "label": 0
                },
                {
                    "sent": "Basically the only parameter here is the normalized distance between the means of these two things, and if you have that and this is something that that I thought that if you have that your scores are calibrated, but the answer is they are not calibrated, but they can be calibrated by pushing it through a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you push this course through a sigmoid, they end up like the dotted lines there.",
                    "label": 0
                },
                {
                    "sent": "And they are nicely calibrated because the density of the blue score linearly increases to the right and the density of the red scores linearly increases to the left, which is essentially why what you want with calibrated scores.",
                    "label": 0
                },
                {
                    "sent": "The fact that it drops off is just because you have symmetric Gaussians.",
                    "label": 0
                },
                {
                    "sent": "If you would just cut off the Gaussian in the middle, you wouldn't have that drop off points, which is right.",
                    "label": 0
                },
                {
                    "sent": "So now we have two calibration methods, and I'm not claiming that any of these are.",
                    "label": 0
                },
                {
                    "sent": "RR My invention but.",
                    "label": 0
                },
                {
                    "sent": "What are the similarities and the differences here?",
                    "label": 0
                },
                {
                    "sent": "Well, notice that this is a monotonic curve.",
                    "label": 0
                },
                {
                    "sent": "So this calibration method does not change the Roc curve, right?",
                    "label": 0
                },
                {
                    "sent": "So the ranking won't change, it's just a distribution of scores on the.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't have that power that that is a tonic.",
                    "label": 0
                },
                {
                    "sent": "Regression would have by improving the ranking by creating these bins.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, of course it's a smooth differentiable function, so it would be.",
                    "label": 0
                },
                {
                    "sent": "Would be less brittle and maybe you need less data to estimate this, but of course things can also go wrong if your scores are not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "There's no guarantee that this does anything useful.",
                    "label": 0
                },
                {
                    "sent": "It may make matters worse, of course.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to end with with a small analysis if I still have enough time.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, I'm I think I can do that.",
                    "label": 0
                },
                {
                    "sent": "And that is something that is that I think we we contribute through to this analysis.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to this idea of how are we measuring the quality of probability estimates and what we did so far was mean squared error.",
                    "label": 0
                },
                {
                    "sent": "But there is something wrong with mean squared error.",
                    "label": 0
                },
                {
                    "sent": "If we compare the squared error as deviations from zero and 1 zero and one are ideal scores, but in no sense are they actually true scores.",
                    "label": 1
                },
                {
                    "sent": "There's no reason why would every positive require score woman.",
                    "label": 0
                },
                {
                    "sent": "What random event does that correspond to?",
                    "label": 0
                },
                {
                    "sent": "So there is something funny here, and actually what you can do is a decomposition that has been known in some form in forecasting theorie an.",
                    "label": 1
                },
                {
                    "sent": "It's a simple rewrite if you have P positives and negatives which all get the same score.",
                    "label": 0
                },
                {
                    "sent": "You can actually rewrite this so you have P terms which have the square deviation to one an N terms which have the square deviation to zero.",
                    "label": 0
                },
                {
                    "sent": "You can rewrite this to something nice.",
                    "label": 1
                },
                {
                    "sent": "S prime is the proportion of positives in that segment.",
                    "label": 0
                },
                {
                    "sent": "So P divided people then, and what you get if you rewrite it like this is.",
                    "label": 0
                },
                {
                    "sent": "A different squared error term where actually S prime now takes the role of the true probabilities, and this is the proportion of positives in that segment that you're talking about.",
                    "label": 1
                },
                {
                    "sent": "There's a second term here, but it is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't refer to S, so it doesn't refer to the score, it's an impurity term.",
                    "label": 0
                },
                {
                    "sent": "It's basically Gini index, but it only talks about the slope of that rock segment.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to make a Long story short, you can do this decomposition exactly, which is the different from forecasting theory, because they only knew how to do it approximately.",
                    "label": 1
                },
                {
                    "sent": "By dividing the scores in equal bins using the rock.",
                    "label": 0
                },
                {
                    "sent": "A curve to to basically decide the boundaries.",
                    "label": 1
                },
                {
                    "sent": "You can exactly decompose these two things, and this is now something very useful, but because it gives this decomposition, gives you much more information than the Brier score or mean squared error itself gives you.",
                    "label": 0
                },
                {
                    "sent": "So to show.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I have a plot here of some actual models that we basically this is 10 fold cross validation, so this is performance on.",
                    "label": 0
                },
                {
                    "sent": "Each of the 10 folds.",
                    "label": 0
                },
                {
                    "sent": "So what I want you to look at is.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These points here are Lex rank.",
                    "label": 0
                },
                {
                    "sent": "Which is clearly uncalibrated in fact, Lex rank doesn't really calculate probabilities, it just calculates binary numbers, so it's not not a surprise that this is not calibrated, but by doing the rock convex Hull calibration we can get the calibration loss close to 0.",
                    "label": 0
                },
                {
                    "sent": "But of course we incur a little bit refinement loss because we are creating the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "So maybe previously we had only horizontal and vertical segments, which means that the refinement loss was zero.",
                    "label": 0
                },
                {
                    "sent": "But by introducing these ties we will now get diagonal segments, so we get the calibration loss close to 0 at the expense of a slight increase of the of the.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Refinement, loss and he is another example.",
                    "label": 0
                },
                {
                    "sent": "Here we have the calibration with the same method of naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "And actually, you could argue that maybe in Brier score, this calibration doesn't make an awful difference, because basically that is the line of constant Brier score, and we're kind of moving parallel to that line.",
                    "label": 0
                },
                {
                    "sent": "So what we win in calibration loss we we lose in refinement loss.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "We want calibrated probability estimators becausw we want to we want to essentially nowhere to threshold the score if you don't have calibrated probability estimate that you need rock analysis to find the best score threshold.",
                    "label": 1
                },
                {
                    "sent": "If you have a calibrated probability estimate, you can simply say I want the posterior odds P / 1 -- P. So I want four times as many.",
                    "label": 0
                },
                {
                    "sent": "Positives, negatives, and therefore I can just threshold it at that value.",
                    "label": 1
                },
                {
                    "sent": "You can only do that with calibrated probability estimators.",
                    "label": 1
                },
                {
                    "sent": "But the quality of probability estimators comes after.",
                    "label": 0
                },
                {
                    "sent": "Evaluating the quality of a rancor so you can't have a good probability estimator if you don't have a rancor.",
                    "label": 0
                },
                {
                    "sent": "So something like mean squared error.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alone is not enough OK, but we do need to do the ranking.",
                    "label": 1
                },
                {
                    "sent": "That is the most fundamental thing.",
                    "label": 1
                },
                {
                    "sent": "Then we have different calibration methods with different properties.",
                    "label": 1
                },
                {
                    "sent": "So if you do logistic calibration, you basically have a parametric model which works as long as the assumptions of your model are satisfied, so your scores are Gaussian.",
                    "label": 1
                },
                {
                    "sent": "You don't improve ranking, but you may get calibrated probability estimates, whereas doing it with the rock convex Hull is atonic regression potentially improves the ranking performance as well.",
                    "label": 1
                },
                {
                    "sent": "The problem there is to estimate these points where the calibration map jumps up.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one open question that that I thought of is.",
                    "label": 0
                },
                {
                    "sent": "Maybe it might be a good idea to somehow smooth these jumps in the calibration map, you don't want to get rid of these horizontal segments completely because they are what allows the improvement in ranking performance.",
                    "label": 0
                },
                {
                    "sent": "But maybe you want to put some kind of smoothing up there so that you're not so sensitive to where you put these these boundaries.",
                    "label": 0
                },
                {
                    "sent": "And here's a couple of.",
                    "label": 0
                },
                {
                    "sent": "Other questions that that that that might be interesting to work on, and with that I would like to conclude and thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Start for few questions questions.",
                    "label": 0
                },
                {
                    "sent": "Or just say that.",
                    "label": 0
                },
                {
                    "sent": "The definition of a rancor that I often use is something that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The ties are important here, right so?",
                    "label": 0
                },
                {
                    "sent": "You can you can call it something that gives you a total order with ties.",
                    "label": 0
                },
                {
                    "sent": "If you think about that, that's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's not a partial order because you you know you can't go via several parts, but the ties are important, right?",
                    "label": 0
                },
                {
                    "sent": "So a total rancor is something that puts any set of examples in a total order with ties.",
                    "label": 0
                },
                {
                    "sent": "Some people call that the bucket order.",
                    "label": 0
                },
                {
                    "sent": "Just to see that one point the other you could do that, but then you have to add consistency requirements, right?",
                    "label": 0
                },
                {
                    "sent": "Because you don't want to allow Rancor that puts a in front of BB in front of.",
                    "label": 0
                },
                {
                    "sent": "See it, see in front of a.",
                    "label": 0
                },
                {
                    "sent": "And you get, you get that with saying it's a bucket order.",
                    "label": 0
                },
                {
                    "sent": "Turn 5 final floor dripping wet.",
                    "label": 0
                },
                {
                    "sent": "Snakes.",
                    "label": 0
                },
                {
                    "sent": "OK. Paper doll.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so you're saying there are advantages in in, in in sort of viewing it as.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, might be interested to discuss that.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "So when is one thing is just a question.",
                    "label": 0
                },
                {
                    "sent": "The application which progressions I think it's lightning speed is that is not an assumption of Gaussian assumption of exponential family.",
                    "label": 0
                },
                {
                    "sent": "The same OK.",
                    "label": 0
                },
                {
                    "sent": "Applications right?",
                    "label": 0
                },
                {
                    "sent": "But the other thing is kind of following up on this idea.",
                    "label": 0
                },
                {
                    "sent": "Thinking this in terms of ranking, what did somebody actually came through instead of 1 zeros?",
                    "label": 0
                },
                {
                    "sent": "They said here are some things of this example schedule in front of the other right and had a series of these binary decisions at this example in front of a given.",
                    "label": 0
                },
                {
                    "sent": "Any thought to generalizing our receipts, that type space.",
                    "label": 0
                },
                {
                    "sent": "I haven't personally, so the way I understand is that what you say is you could.",
                    "label": 0
                },
                {
                    "sent": "You could generalize the bipartite drank the bipartite ranking setting is very restricted because it's basically somebody says good movies, bad movies and that's all you got and you.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you don't care at all how to rank the good movies amongst themselves, and you're saying, well, you could generalize that and have constraints and say this one should be go before that one.",
                    "label": 0
                },
                {
                    "sent": "This one should go after that one, and I think I would need to look again in the literature.",
                    "label": 1
                },
                {
                    "sent": "I think some people are starting to look at that.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at it.",
                    "label": 0
                },
                {
                    "sent": "But I think that's a very interesting direction to go.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Forecast.",
                    "label": 0
                },
                {
                    "sent": "Look further into different scoring functions, proper scoring functions and how that could analyze that and potentially different compositions.",
                    "label": 0
                },
                {
                    "sent": "What they would mean.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at different decompositions.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have any particular ones in mind with proper scoring functions.",
                    "label": 0
                },
                {
                    "sent": "They basically all say.",
                    "label": 0
                },
                {
                    "sent": "Constructing the convex Hull is a good idea.",
                    "label": 0
                },
                {
                    "sent": "It's, you know it it because you want to be easy, tonic you.",
                    "label": 0
                },
                {
                    "sent": "You don't want to mess up the ranking.",
                    "label": 0
                },
                {
                    "sent": "You may, you may make the ranking coarser by putting things in bins, but you don't want to change the order and then basically any proper scoring function is minimized by, you know, doing the convex Hull basically, so I'm not sure how much interest there is for going beyond that.",
                    "label": 0
                },
                {
                    "sent": "Looking at proper scoring functions, but if there are any, if there are other decompositions today, I'm.",
                    "label": 0
                },
                {
                    "sent": "Probably just unaware of them and I would welcome any suggestions.",
                    "label": 0
                },
                {
                    "sent": "Last question.",
                    "label": 0
                },
                {
                    "sent": "About two classes, most of the time I was wondering what happened if you had more classes and what the state of the art here, and that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "What you end up with if you have more than two classes, the this kind of analysis very quickly becomes intractable, so.",
                    "label": 1
                },
                {
                    "sent": "It's often not so much a theoretical question for International Srinivasan.",
                    "label": 0
                },
                {
                    "sent": "In 1999 had a technical note where he shows that it all generalizes.",
                    "label": 0
                },
                {
                    "sent": "You know you can do.",
                    "label": 0
                },
                {
                    "sent": "You can have a rock polytope and sort of you're looking for the optimal point on the polytope by sliding and a surface with a certain orientation and so so with all generalizes, it just blows up.",
                    "label": 0
                },
                {
                    "sent": "So you need to approximate it.",
                    "label": 0
                },
                {
                    "sent": "So that means that you cannot guarantee to have the actual optimum.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is, if you have multiclass Roc analysis, you project it down to one versus one or one versus rest and you tried to do something that is good enough on the average without, without being necessarily optimal.",
                    "label": 0
                },
                {
                    "sent": "So what you lose is optimality.",
                    "label": 0
                },
                {
                    "sent": "But what you gain is a potentially still tractable method.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}