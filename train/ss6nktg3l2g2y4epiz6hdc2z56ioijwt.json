{
    "id": "ss6nktg3l2g2y4epiz6hdc2z56ioijwt",
    "title": "Restricted Deep Belief Networks for Multi-View Learning",
    "info": {
        "author": [
            "Yoonseop Kang, Pohang University of Science and Technology (POSTECH)"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_kang_restricted/",
    "segmentation": [
        [
            "No, I'm user account and this work is joint work with my advisors change in Troy.",
            "And my work title is the restricted deep belief networks for multiple learning.",
            "So my work is about applying the deep belief network framework to the multi view learning.",
            "Problem."
        ],
        [
            "OK.",
            "So.",
            "Before talking about multiple learning, let's talk about what is multiple data.",
            "So multi view data is a data with more than one possible representation, so it will have multiple disjoint feature set.",
            "So for example so each different representation is called a view.",
            "An you can have many examples of multiple data.",
            "So for example, a video clip will have each frame of video clip will have an image fragment and also a audio fragment, and for example of web page, a web page can be represented using a bag of words an the links connecting toward it or connecting out of it.",
            "An also the image is annotated with tags can be seen as a two view data, so its image features and also its annotations.",
            "So that was multiple data."
        ],
        [
            "So to learn from mounting data, people have devised many methods.",
            "So we call these methods multiple learning.",
            "So the beginning of the multiple learning was a code training which is a semi supervised method method for multiple data.",
            "So it was started in 1998.",
            "So they exploited the redundancy between views, so they assume that the views are conditionally independent given the labels or some latent variables.",
            "So this is how they have been done.",
            "So that's why some simple concatenation of features often fails.",
            "But in real scenarios, views are not completely independent or there, nor they're not completely correlated, so that's why some attempts to find just completely shared representations like latent variable might fail."
        ],
        [
            "So that's why we came up with our new approach.",
            "So we model this partial correlation between views with some separate latent model.",
            "So what we did is that.",
            "You can see the.",
            "Uh.",
            "On the picture above, most previous methods explain the multiple data using only one shared common latent variable.",
            "But in our model we.",
            "Introduced to view specific latent variables to explain the multiple data.",
            "And also we want our model to be fast so we.",
            "Or adaptive the exponential family harmony harmonium so our model.",
            "Our model is undirected graphical model.",
            "So we our model can infer the latent variables very fast."
        ],
        [
            "So let's talk about our building blocks first, which is exponential exponential family, harmonium.",
            "So this model is proposed by the enrolling at all and in 2005.",
            "It is just a two layer graphical model.",
            "Anne.",
            "So it has some visible nodes and also some hidden nodes.",
            "So it is defined using their marginal distributions of each layer margin.",
            "District distribution of X&H and also are there.",
            "Interaction terms, so we have the joint distribution of X&H in the exponential family.",
            "Form.",
            "Yep.",
            "So.",
            "SD Model is a bipartite graph.",
            "So we can.",
            "We can get a hidden nodes conditional independence given the visible nodes and also the nodes gets conditionally independent given hidden nodes.",
            "So we can do sampling of each each layer very fast.",
            "So that's why our.",
            "Influence the influence on the FHR very faster than the directed models."
        ],
        [
            "And to apply this exponential family harmonium for the multiple data, or.",
            "People came up with more with the model name dueling harmonium.",
            "So during harmonium just.",
            "Separated the.",
            "Visible nodes into multiple sets and assigned each set to each view of the data.",
            "So that's why we have.",
            "So this is presented by the EPC at all at 2005.",
            "And the.",
            "It's yeah it is.",
            "Multiplication of extension of FH.",
            "And that's an as we have multiple sets of visible visible nodes, so we have multiple terms for the connection between resume nodes and hidden nodes."
        ],
        [
            "Yep.",
            "And this is our proposed model, which we call a multiple harmonium.",
            "So we introduced a few specific hidden nodes for each view of our data.",
            "So we have.",
            "Yeah, modify model.",
            "And also via get we have some additional connections between the visible nodes and use specific hidden nodes.",
            "Yes, so with our new model we represent the.",
            "Each view of data using shared hidden node and also if you specifically don't know so weak we have much.",
            "Richer representation of our data.",
            "So.",
            "This specific hidden nodes will encode some uncorrelated information of each views."
        ],
        [
            "So how do we?",
            "How do we train our multiple harmonium?",
            "So it is.",
            "Done using the.",
            "Log log gradient descent on the log likelihood function.",
            "So this.",
            "This is done by first ensuring integrating out the hidden nodes.",
            "So it is.",
            "The likelihood is described like this.",
            "And then we do some approximation on the gradient.",
            "Of the log like log likelihood.",
            "So.",
            "The log gradient of the log likelihood log likelihood function over the parameter is explained by the expected expectation of the derivative derivative on data distribution and also the drip drip tips on the.",
            "Data distribution, but.",
            "A model distribution, but calculating this term takes a lot of time, so we just approximate model distribution using the.",
            "It's some number of steps up the Gibbs sampling.",
            "So."
        ],
        [
            "This is how we train our multiple harmonium.",
            "And in practice, we used stochastic gradient descent.",
            "So that means we use some mini batches of small sizes.",
            "Ann this algorithm is easily parallelizable using GPU programming because.",
            "Each view are the each nodes are conditionally independent given the other layer.",
            "So we can do sampling simultaneously."
        ],
        [
            "An we can extend our multiyear harmonium to the multilayer.",
            "So.",
            "This is this.",
            "These two are our multiple harmonium.",
            "We can stack our multiple harmonium.",
            "On you up.",
            "Over our multi behind menu and we can do it again to form a nice nice multilayer network.",
            "So in our multi layer model we feed our view specific hidden nodes to the higher layer as a training sample and then we train the higher layers and.",
            "Then we again we feed the hidden nodes to the over higher layer so we can repeat this step through our train.",
            "Our multi Multi layer Network which is called restricted piplup network.",
            "So.",
            "By doing this, our model can model a more complex relationship between the views, so when.",
            "Or after we further analyzed the.",
            "The uncorrelated part of our data, so we seek for more relationship between these uncorrelated parts.",
            "So we find further relationship on the higher layer."
        ],
        [
            "And we can use our model.",
            "For you to view convergent tasks, for example, we are given a value for X, but we don't have the value for Y, so we want to know it.",
            "So we start from.",
            "Sampling through the.",
            "View one so U of X.",
            "So we sample H of X given HX from the conditional distribution.",
            "And we do it.",
            "We repeat this procedure until we reach the top layer.",
            "And then we fix these two nodes and we do a sampling on these five.",
            "Notice that so that we get the values for HC in HY&Y.",
            "And then we can do similar.",
            "Procedure for the lower layer.",
            "So we go down through the layers until we reach the bottom layer.",
            "Then we we get the value for Y.",
            "So this is our systematic inference procedure."
        ],
        [
            "Now let's talk about our experimental data, experimental result.",
            "So we made a some synthetic data.",
            "Using so three set of latent variables so each latent variables are parameterized using the.",
            "Value T&X is.",
            "Obtained by applying a sine function in Z1 and Z2 are obtained by using some constant cosine function on the parameter T. Then we augment XN, Z1 to create a.",
            "First view of our data and we augment X and Z2 to create our second view of our data.",
            "So what we want to do in our experiment is that we give this.",
            "Two view data and we want our model to infer the X and Z1 and Z2 correctly."
        ],
        [
            "Yeah.",
            "And this is is the result.",
            "So we trained CCA an for false GP LVM and multi multi wing harmonium that I mentioned before and multiview harmonium which is our model.",
            "And you can see that our model is the only model which find finds the latent variables correctly.",
            "While other latent variables latent variable models fails."
        ],
        [
            "Anne.",
            "The second experiment is about image reconstruction on a.",
            "A toy data set.",
            "So we are given a set of airplane images and car images which are taken in the same angles.",
            "So we have a bunch of these pairs.",
            "So we train our model using these pairs and then we give a new airplane image taken in a new angle and we want to infer the car image taken in the same angle and then we.",
            "Compare the imported image and the ground truth image.",
            "So we calculate the RMS error.",
            "Between these inferred and ground truth images, so we evaluated to the performance."
        ],
        [
            "The algorithms.",
            "So.",
            "This is the result.",
            "So to compare.",
            "To make a fair comparison, we built a three layer model for the multilink harmonium an also before layer.",
            "For the multi multi harmonium.",
            "And the result shows that our four layer model.",
            "Shows the least reconstruction error.",
            "Yeah, and these models for multi harmoniums fail too.",
            "Get a higher performance."
        ],
        [
            "And this is our last experiment.",
            "So the image the setting of the experiment is just saying, as the previous one, so we prepare a pair of the image and the tag occurrence vectors.",
            "So we.",
            "We train our model using.",
            "These pairs and then.",
            "Or we pick an image from the test data set.",
            "And we do a view convert beauty view convergent so we give images X and we infer the value of Y.",
            "Then we get some probability values for each tag tags occurring in addition dictionary then.",
            "We choose the tags with some highest probability as a as a annotation result."
        ],
        [
            "So.",
            "Yep.",
            "So with those results we calculated precision and recall and we drew some graph.",
            "And you can see that our our DBN model shows the.",
            "Highest shows the highest.",
            "The area under this curve, so which means that our model are performed the other computers."
        ],
        [
            "So that was my work.",
            "So to conclude.",
            "We divide a harmonium based model for partially correlated multi data and we also showed some it's multilayer extension which is called restrictive belief network.",
            "An hour methods are performed.",
            "The existing model during harmonium on some datasets.",
            "And our future work will be.",
            "Devising it better inference methods for ardens and some maximum margin objective function for classification or to apply it on some time series data.",
            "Thank you.",
            "Questions from floor.",
            "No question.",
            "Maybe let me ask questions so when you say higher or upper, does it mean a deeper deeper right which means?",
            "Beneath the surface, no no is over the surface over the surface.",
            "So like when I say higher layer, it means the layers above.",
            "What does it mean?",
            "Is more abstract?",
            "Yeah it is.",
            "We as we go through the layers, the features extracted from these layers OK then so once you have this model we want to understand what's happening an after you.",
            "Guarantees.",
            "Model, is it possible to interpret what each layer actually means?",
            "We didn't try.",
            "OK, we didn't try some.",
            "Probably adding more than three or four layers that makes sense.",
            "But it is well known that the.",
            "The.",
            "Using higher higher number of layers gives us more.",
            "We can ask responses and some underlying mechanism, right?",
            "Yeah.",
            "OK. And another question is you used undirected edges between hidden and visible and this makes inference faster.",
            "Yeah, it seems to be more natural to have link from hidden to visible.",
            "If you think of some mechanism.",
            "It's just a matter of computational convenience.",
            "And as far as I as I know it is more conventional convenience.",
            "OK?",
            "Because if you have directed edges, the computation of these latent variable will be OK.",
            "Understand.",
            "How does this scale with a larger number of examples?",
            "So if you have 10 fold number of examples are 100 fold training algorithm, the complexity of the training algorithm depends on the.",
            "Depends linearly to the number of examples, so it scales linearly with also with respect to convergence of with respect to one iteration.",
            "I tried that and.",
            "I got the result that with more.",
            "Examples we got faster convergence because we have more examples to run.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, I'm user account and this work is joint work with my advisors change in Troy.",
                    "label": 0
                },
                {
                    "sent": "And my work title is the restricted deep belief networks for multiple learning.",
                    "label": 1
                },
                {
                    "sent": "So my work is about applying the deep belief network framework to the multi view learning.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Before talking about multiple learning, let's talk about what is multiple data.",
                    "label": 0
                },
                {
                    "sent": "So multi view data is a data with more than one possible representation, so it will have multiple disjoint feature set.",
                    "label": 1
                },
                {
                    "sent": "So for example so each different representation is called a view.",
                    "label": 0
                },
                {
                    "sent": "An you can have many examples of multiple data.",
                    "label": 0
                },
                {
                    "sent": "So for example, a video clip will have each frame of video clip will have an image fragment and also a audio fragment, and for example of web page, a web page can be represented using a bag of words an the links connecting toward it or connecting out of it.",
                    "label": 0
                },
                {
                    "sent": "An also the image is annotated with tags can be seen as a two view data, so its image features and also its annotations.",
                    "label": 0
                },
                {
                    "sent": "So that was multiple data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to learn from mounting data, people have devised many methods.",
                    "label": 0
                },
                {
                    "sent": "So we call these methods multiple learning.",
                    "label": 0
                },
                {
                    "sent": "So the beginning of the multiple learning was a code training which is a semi supervised method method for multiple data.",
                    "label": 0
                },
                {
                    "sent": "So it was started in 1998.",
                    "label": 0
                },
                {
                    "sent": "So they exploited the redundancy between views, so they assume that the views are conditionally independent given the labels or some latent variables.",
                    "label": 1
                },
                {
                    "sent": "So this is how they have been done.",
                    "label": 0
                },
                {
                    "sent": "So that's why some simple concatenation of features often fails.",
                    "label": 1
                },
                {
                    "sent": "But in real scenarios, views are not completely independent or there, nor they're not completely correlated, so that's why some attempts to find just completely shared representations like latent variable might fail.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's why we came up with our new approach.",
                    "label": 0
                },
                {
                    "sent": "So we model this partial correlation between views with some separate latent model.",
                    "label": 1
                },
                {
                    "sent": "So what we did is that.",
                    "label": 0
                },
                {
                    "sent": "You can see the.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "On the picture above, most previous methods explain the multiple data using only one shared common latent variable.",
                    "label": 0
                },
                {
                    "sent": "But in our model we.",
                    "label": 0
                },
                {
                    "sent": "Introduced to view specific latent variables to explain the multiple data.",
                    "label": 0
                },
                {
                    "sent": "And also we want our model to be fast so we.",
                    "label": 0
                },
                {
                    "sent": "Or adaptive the exponential family harmony harmonium so our model.",
                    "label": 1
                },
                {
                    "sent": "Our model is undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "So we our model can infer the latent variables very fast.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about our building blocks first, which is exponential exponential family, harmonium.",
                    "label": 1
                },
                {
                    "sent": "So this model is proposed by the enrolling at all and in 2005.",
                    "label": 1
                },
                {
                    "sent": "It is just a two layer graphical model.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So it has some visible nodes and also some hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "So it is defined using their marginal distributions of each layer margin.",
                    "label": 1
                },
                {
                    "sent": "District distribution of X&H and also are there.",
                    "label": 0
                },
                {
                    "sent": "Interaction terms, so we have the joint distribution of X&H in the exponential family.",
                    "label": 0
                },
                {
                    "sent": "Form.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "SD Model is a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can get a hidden nodes conditional independence given the visible nodes and also the nodes gets conditionally independent given hidden nodes.",
                    "label": 1
                },
                {
                    "sent": "So we can do sampling of each each layer very fast.",
                    "label": 0
                },
                {
                    "sent": "So that's why our.",
                    "label": 0
                },
                {
                    "sent": "Influence the influence on the FHR very faster than the directed models.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to apply this exponential family harmonium for the multiple data, or.",
                    "label": 0
                },
                {
                    "sent": "People came up with more with the model name dueling harmonium.",
                    "label": 0
                },
                {
                    "sent": "So during harmonium just.",
                    "label": 0
                },
                {
                    "sent": "Separated the.",
                    "label": 0
                },
                {
                    "sent": "Visible nodes into multiple sets and assigned each set to each view of the data.",
                    "label": 1
                },
                {
                    "sent": "So that's why we have.",
                    "label": 0
                },
                {
                    "sent": "So this is presented by the EPC at all at 2005.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "It's yeah it is.",
                    "label": 1
                },
                {
                    "sent": "Multiplication of extension of FH.",
                    "label": 1
                },
                {
                    "sent": "And that's an as we have multiple sets of visible visible nodes, so we have multiple terms for the connection between resume nodes and hidden nodes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "And this is our proposed model, which we call a multiple harmonium.",
                    "label": 1
                },
                {
                    "sent": "So we introduced a few specific hidden nodes for each view of our data.",
                    "label": 1
                },
                {
                    "sent": "So we have.",
                    "label": 1
                },
                {
                    "sent": "Yeah, modify model.",
                    "label": 1
                },
                {
                    "sent": "And also via get we have some additional connections between the visible nodes and use specific hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so with our new model we represent the.",
                    "label": 0
                },
                {
                    "sent": "Each view of data using shared hidden node and also if you specifically don't know so weak we have much.",
                    "label": 0
                },
                {
                    "sent": "Richer representation of our data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This specific hidden nodes will encode some uncorrelated information of each views.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we train our multiple harmonium?",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "Done using the.",
                    "label": 0
                },
                {
                    "sent": "Log log gradient descent on the log likelihood function.",
                    "label": 1
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This is done by first ensuring integrating out the hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "The likelihood is described like this.",
                    "label": 0
                },
                {
                    "sent": "And then we do some approximation on the gradient.",
                    "label": 0
                },
                {
                    "sent": "Of the log like log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The log gradient of the log likelihood log likelihood function over the parameter is explained by the expected expectation of the derivative derivative on data distribution and also the drip drip tips on the.",
                    "label": 1
                },
                {
                    "sent": "Data distribution, but.",
                    "label": 0
                },
                {
                    "sent": "A model distribution, but calculating this term takes a lot of time, so we just approximate model distribution using the.",
                    "label": 1
                },
                {
                    "sent": "It's some number of steps up the Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is how we train our multiple harmonium.",
                    "label": 0
                },
                {
                    "sent": "And in practice, we used stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So that means we use some mini batches of small sizes.",
                    "label": 0
                },
                {
                    "sent": "Ann this algorithm is easily parallelizable using GPU programming because.",
                    "label": 1
                },
                {
                    "sent": "Each view are the each nodes are conditionally independent given the other layer.",
                    "label": 0
                },
                {
                    "sent": "So we can do sampling simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An we can extend our multiyear harmonium to the multilayer.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "These two are our multiple harmonium.",
                    "label": 0
                },
                {
                    "sent": "We can stack our multiple harmonium.",
                    "label": 0
                },
                {
                    "sent": "On you up.",
                    "label": 0
                },
                {
                    "sent": "Over our multi behind menu and we can do it again to form a nice nice multilayer network.",
                    "label": 0
                },
                {
                    "sent": "So in our multi layer model we feed our view specific hidden nodes to the higher layer as a training sample and then we train the higher layers and.",
                    "label": 1
                },
                {
                    "sent": "Then we again we feed the hidden nodes to the over higher layer so we can repeat this step through our train.",
                    "label": 0
                },
                {
                    "sent": "Our multi Multi layer Network which is called restricted piplup network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "By doing this, our model can model a more complex relationship between the views, so when.",
                    "label": 0
                },
                {
                    "sent": "Or after we further analyzed the.",
                    "label": 1
                },
                {
                    "sent": "The uncorrelated part of our data, so we seek for more relationship between these uncorrelated parts.",
                    "label": 0
                },
                {
                    "sent": "So we find further relationship on the higher layer.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can use our model.",
                    "label": 0
                },
                {
                    "sent": "For you to view convergent tasks, for example, we are given a value for X, but we don't have the value for Y, so we want to know it.",
                    "label": 0
                },
                {
                    "sent": "So we start from.",
                    "label": 0
                },
                {
                    "sent": "Sampling through the.",
                    "label": 0
                },
                {
                    "sent": "View one so U of X.",
                    "label": 0
                },
                {
                    "sent": "So we sample H of X given HX from the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And we do it.",
                    "label": 0
                },
                {
                    "sent": "We repeat this procedure until we reach the top layer.",
                    "label": 0
                },
                {
                    "sent": "And then we fix these two nodes and we do a sampling on these five.",
                    "label": 0
                },
                {
                    "sent": "Notice that so that we get the values for HC in HY&Y.",
                    "label": 0
                },
                {
                    "sent": "And then we can do similar.",
                    "label": 0
                },
                {
                    "sent": "Procedure for the lower layer.",
                    "label": 0
                },
                {
                    "sent": "So we go down through the layers until we reach the bottom layer.",
                    "label": 0
                },
                {
                    "sent": "Then we we get the value for Y.",
                    "label": 0
                },
                {
                    "sent": "So this is our systematic inference procedure.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's talk about our experimental data, experimental result.",
                    "label": 0
                },
                {
                    "sent": "So we made a some synthetic data.",
                    "label": 1
                },
                {
                    "sent": "Using so three set of latent variables so each latent variables are parameterized using the.",
                    "label": 1
                },
                {
                    "sent": "Value T&X is.",
                    "label": 1
                },
                {
                    "sent": "Obtained by applying a sine function in Z1 and Z2 are obtained by using some constant cosine function on the parameter T. Then we augment XN, Z1 to create a.",
                    "label": 0
                },
                {
                    "sent": "First view of our data and we augment X and Z2 to create our second view of our data.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do in our experiment is that we give this.",
                    "label": 0
                },
                {
                    "sent": "Two view data and we want our model to infer the X and Z1 and Z2 correctly.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And this is is the result.",
                    "label": 0
                },
                {
                    "sent": "So we trained CCA an for false GP LVM and multi multi wing harmonium that I mentioned before and multiview harmonium which is our model.",
                    "label": 0
                },
                {
                    "sent": "And you can see that our model is the only model which find finds the latent variables correctly.",
                    "label": 0
                },
                {
                    "sent": "While other latent variables latent variable models fails.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The second experiment is about image reconstruction on a.",
                    "label": 1
                },
                {
                    "sent": "A toy data set.",
                    "label": 0
                },
                {
                    "sent": "So we are given a set of airplane images and car images which are taken in the same angles.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of these pairs.",
                    "label": 0
                },
                {
                    "sent": "So we train our model using these pairs and then we give a new airplane image taken in a new angle and we want to infer the car image taken in the same angle and then we.",
                    "label": 1
                },
                {
                    "sent": "Compare the imported image and the ground truth image.",
                    "label": 0
                },
                {
                    "sent": "So we calculate the RMS error.",
                    "label": 0
                },
                {
                    "sent": "Between these inferred and ground truth images, so we evaluated to the performance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithms.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the result.",
                    "label": 0
                },
                {
                    "sent": "So to compare.",
                    "label": 0
                },
                {
                    "sent": "To make a fair comparison, we built a three layer model for the multilink harmonium an also before layer.",
                    "label": 0
                },
                {
                    "sent": "For the multi multi harmonium.",
                    "label": 0
                },
                {
                    "sent": "And the result shows that our four layer model.",
                    "label": 0
                },
                {
                    "sent": "Shows the least reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and these models for multi harmoniums fail too.",
                    "label": 0
                },
                {
                    "sent": "Get a higher performance.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is our last experiment.",
                    "label": 0
                },
                {
                    "sent": "So the image the setting of the experiment is just saying, as the previous one, so we prepare a pair of the image and the tag occurrence vectors.",
                    "label": 1
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We train our model using.",
                    "label": 0
                },
                {
                    "sent": "These pairs and then.",
                    "label": 0
                },
                {
                    "sent": "Or we pick an image from the test data set.",
                    "label": 0
                },
                {
                    "sent": "And we do a view convert beauty view convergent so we give images X and we infer the value of Y.",
                    "label": 0
                },
                {
                    "sent": "Then we get some probability values for each tag tags occurring in addition dictionary then.",
                    "label": 0
                },
                {
                    "sent": "We choose the tags with some highest probability as a as a annotation result.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So with those results we calculated precision and recall and we drew some graph.",
                    "label": 0
                },
                {
                    "sent": "And you can see that our our DBN model shows the.",
                    "label": 0
                },
                {
                    "sent": "Highest shows the highest.",
                    "label": 0
                },
                {
                    "sent": "The area under this curve, so which means that our model are performed the other computers.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was my work.",
                    "label": 0
                },
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "We divide a harmonium based model for partially correlated multi data and we also showed some it's multilayer extension which is called restrictive belief network.",
                    "label": 1
                },
                {
                    "sent": "An hour methods are performed.",
                    "label": 0
                },
                {
                    "sent": "The existing model during harmonium on some datasets.",
                    "label": 1
                },
                {
                    "sent": "And our future work will be.",
                    "label": 1
                },
                {
                    "sent": "Devising it better inference methods for ardens and some maximum margin objective function for classification or to apply it on some time series data.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions from floor.",
                    "label": 0
                },
                {
                    "sent": "No question.",
                    "label": 0
                },
                {
                    "sent": "Maybe let me ask questions so when you say higher or upper, does it mean a deeper deeper right which means?",
                    "label": 0
                },
                {
                    "sent": "Beneath the surface, no no is over the surface over the surface.",
                    "label": 0
                },
                {
                    "sent": "So like when I say higher layer, it means the layers above.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "Is more abstract?",
                    "label": 0
                },
                {
                    "sent": "Yeah it is.",
                    "label": 0
                },
                {
                    "sent": "We as we go through the layers, the features extracted from these layers OK then so once you have this model we want to understand what's happening an after you.",
                    "label": 0
                },
                {
                    "sent": "Guarantees.",
                    "label": 0
                },
                {
                    "sent": "Model, is it possible to interpret what each layer actually means?",
                    "label": 0
                },
                {
                    "sent": "We didn't try.",
                    "label": 0
                },
                {
                    "sent": "OK, we didn't try some.",
                    "label": 0
                },
                {
                    "sent": "Probably adding more than three or four layers that makes sense.",
                    "label": 0
                },
                {
                    "sent": "But it is well known that the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Using higher higher number of layers gives us more.",
                    "label": 0
                },
                {
                    "sent": "We can ask responses and some underlying mechanism, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK. And another question is you used undirected edges between hidden and visible and this makes inference faster.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it seems to be more natural to have link from hidden to visible.",
                    "label": 0
                },
                {
                    "sent": "If you think of some mechanism.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of computational convenience.",
                    "label": 0
                },
                {
                    "sent": "And as far as I as I know it is more conventional convenience.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Because if you have directed edges, the computation of these latent variable will be OK.",
                    "label": 0
                },
                {
                    "sent": "Understand.",
                    "label": 0
                },
                {
                    "sent": "How does this scale with a larger number of examples?",
                    "label": 0
                },
                {
                    "sent": "So if you have 10 fold number of examples are 100 fold training algorithm, the complexity of the training algorithm depends on the.",
                    "label": 0
                },
                {
                    "sent": "Depends linearly to the number of examples, so it scales linearly with also with respect to convergence of with respect to one iteration.",
                    "label": 0
                },
                {
                    "sent": "I tried that and.",
                    "label": 0
                },
                {
                    "sent": "I got the result that with more.",
                    "label": 0
                },
                {
                    "sent": "Examples we got faster convergence because we have more examples to run.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}