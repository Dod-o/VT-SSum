{
    "id": "fpu7y3w4iwhwsqtlyujdcwhri247tlmw",
    "title": "Model-Free Reinforcement Learning as Mixture Learning",
    "info": {
        "author": [
            "Nikos Vlassis, Department of Production Engineering and Management, Technical University of Crete"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_vlassis_mfr/",
    "segmentation": [
        [
            "OK. My name is Nicholas.",
            "Thanks for being here.",
            "This is John work with Mark Descent with sitting back there.",
            "So sometimes I get difficult questions.",
            "I might refer to him to answer them.",
            "OK, this work is about model fitting, reinforcement learning.",
            "It's an approach in which we want to cast model for reinforcement learning is a problem."
        ],
        [
            "Exposing the lack of a mixed model and I will explain how this is possible."
        ],
        [
            "So to be consistent with the title of the session is the menu, so I will say a few things about the general idea how to cast stochastic optimal controllers."
        ],
        [
            "Picture learning problem.",
            "And then I will focus on the Model 3 reinforcement learning case and I will propose the stochastic email EM algorithm for solving.",
            "For optimizing the value function.",
            "One of the contributions of this work is new mix representation and new model, which allows a more efficient in algorithm.",
            "Another interesting contribution is that there is an interesting relationship between the stochastic EM algorithm that we propose and the family of methods in moderate moderate enforcement learning, which is called optimistic policy iteration for the instances are so one for those of you who are familiar with that is a member of that family, and I will give very some very preliminary results.",
            "Experiment results.",
            "OK, before I start I dig into the math.",
            "This is the algorithm that eventually comes out of all the theory, so I want to start with that.",
            "It's a very simple algorithm.",
            "We want to maximize the discounted value, so this is an interaction problem with the discount factor gamma.",
            "So the algorithm works as follows.",
            "We choose a scalar Delta which is from zero to one.",
            "We initialize the policies stochastic policy P, and then we repeat the following steps until we converge.",
            "We simply batch of trajectories from the MDP by using the policy P, an special characteristic of our our algorithm is this projectors have random length.",
            "So the length T of its trajectory is drawn from some geometric distribution with parameter Delta.",
            "That's a new thing.",
            "Then we estimate the Q function for all X for Allstate action pairs with standard batch every visit Monte Carlo, and then we updated the stochastic policy by making it proportional to the Q values, which is also somewhat unusual.",
            "We don't see it very often in the literature.",
            "OK, so that's it basically that's the algorithm.",
            "And let's see now how we get there."
        ],
        [
            "So there is a.",
            "There's been some.",
            "I work lately trying to relate planning or optimal control problems with probabilistic inference and learning problems.",
            "There are series of papers recently.",
            "The idea is that.",
            "First we get the fresh look at the problem by casting.",
            "Planning.",
            "Two probabilistic inference we can use existing algorithms for inference learning to solve our planning problems.",
            "It can give us new tools to analyze algorithms and allows more easy extensions to structure domain structure, policies and little bit harder M DPS.",
            "The the case where this here is the interaction model free reinforcement in case there are two papers.",
            "The paper by to science talking in 2000, seven 2006.",
            "They undressed.",
            "There.",
            "There Steven to rise and model based case and there's a recent paper by covering Peters.",
            "They addressed the finite or as a Model 3 case.",
            "So we're sort of compliment this two.",
            "Play this work."
        ],
        [
            "OK, so as I said, this tab is a classical infinite horizon MDP we assume.",
            "Discrete MDP at the moment.",
            "We states XT actions, yutian rewards scaled to 0 between zero and one.",
            "We assume that the process starts from a certain distribution.",
            "POX Zero and the task is to find the stochastic policies that maximize the value function, which is an infant horizon.",
            "Discounted value function.",
            "And we assume that we are giving a discount factor gamma, which is between 01.",
            "So this is a classical standard enforcement problem."
        ],
        [
            "So here's the idea how to cast the value function as a mixture likelihood.",
            "So to send a story, they showed that V actually can be expressed in the local function of an infinite mixture model, so it makes more within the number of components, and there are two key ideas that allow allow us to do this.",
            "The first is that we take the discount factor gamma to the power T and we treat it as a geometric distribution.",
            "Overtime steps, so this PFT.",
            "And the second idea, which actually is not new IT dates back to 1988 and also 97 is to treat the rewards as Bernoulli para meters.",
            "So here we introduce some fictitious random variable R and we assume that the reward obtained it's time.",
            "Some time step T is the probability of.",
            "Actually this is the probability that this fictitious random variable takes the value one.",
            "Now if we incorporate these two ideas in the definition of the value function and we slightly related the value function.",
            "So this is the sum of discount."
        ],
        [
            "Version of the expected reward.",
            "The time step T. Now if we use the.",
            "So first of all this this expectation is the sum of all possible trajectory is with lengthy.",
            "Um of the rewarded time 16 and this is the distribution of trajectories of lengthy through the MDP forgiven policy, which is given by the standard MDP dynamics.",
            "This is a factor that depends on the dynamics of the MDP, and this is a component that depends on our policy.",
            "OK, so now if we incorporated two ideas that I mentioned in the definition."
        ],
        [
            "The value function so treat.",
            "This is a geometric prior and treat this as a Bernoulli probability what we get.",
            "Is we get this mixture?",
            "So this mixture is this a mixture likelihood which is proportional to the value function of the original problem.",
            "Um so.",
            "An interesting thing here is that the observation likelihood is in fact identical to the terminal reward of a trajectory.",
            "So we can think of this as a generative process in which we first draw a length of a trajectory T, and then we draw a trajectory of lengthy from the MDP using policy P, and then we draw from this Bernoulli model and then the probability that are that the variable R takes value one.",
            "Is actually proportional to the value function of the MDP.",
            "And the chorus.",
            "This is a mixture.",
            "Lack of corresponding joint model which involves.",
            "Two latent variables T&C.",
            "So it is the length exceeds direct.",
            "The trajectory of that length and this fictitious random variable R. And this parameterized by the policy parameters of the one point.",
            "OK, so that was there originally.",
            "Derivation of the sentence talking.",
            "Anne."
        ],
        [
            "So let's see now how we can go model free.",
            "So we want to Model 3.",
            "So first of all, how are you?"
        ],
        [
            "6:00 PM most of you, I presume, are familiar with them.",
            "So yeah, iterates between 2 steps.",
            "They step them.",
            "Step in.",
            "This step will compute the posterior distribution over the latent variables given the observed quantities.",
            "Here the observed are not actually observed.",
            "Its effect issues observed random variable R, but we did it as observed.",
            "So we need to find distribution or sample from distribution of the latent variables given R. These factors as follows.",
            "And in the M step we have to maximize the energy.",
            "The expected joint look like on function.",
            "The energy function of our policy.",
            "That involves the log of the factors of our joint distribution that contain the unknown parameters P. So basically this is just the probability of.",
            "The lengths objectors.",
            "So this is pretty standard stuff that comes out from the definition of the joint model and the mixture like.",
            "Now if we have access to the model of the MDP, we can actually do this, yeah?",
            "In an exact manner, we can compute this distribution analytically and we can do them step analytically, but if we don't have access to the empty dynamics, we have to sample from this distribution.",
            "This is the stochastic yam family of methods.",
            "So then the question is how we actually drove samples from this posterior distribution of latent variables given the observed quantities.",
            "So why?"
        ],
        [
            "Relatively straightforward way to to draw from this distribution is to perform a forward simulation implementation in which we first draw length, length of length of the trajectory, and then we draw a trajectory of that length using the previous policy previous step policy.",
            "And assigned to this trajectory CI an importance weight which equals the reward of this trajectory.",
            "The last time step.",
            "Remember that this was identical to the reward at some time step, and in this case it's the terminal that awarded the last time step of the trajectory, see.",
            "So we could plug this sampling into their stochastic EM and we would have a convergence theorem algorithm.",
            "But the problem is.",
            "That's that's an earmark.",
            "It would only utilize the terminal reward of a trajectory, which is clearly a waste, because that would require a huge sample complexity in order to produce any meaningful results.",
            "So they make a decision of this work is actually to propose a new mixer representation, a new mixer model.",
            "That would make the step little bit more efficient."
        ],
        [
            "So here is the new mixer representation that we propose, so this is a main theme of this work.",
            "We have shown that we can actually.",
            "Actually, there is an infinite.",
            "There are infinitely many ways to write."
        ],
        [
            "Value function is a mixture and there's a family that is parameterized by the scalar Delta that we can choose between gamma and one.",
            "And then we introduce a new random variable, capital T and then we can show that we can add the value function as this mixture that now is similar to the previous one slide definition of their observed.",
            "The observation model is slightly different and also this new.",
            "There's a new also distribution over this latent variable T and here in this model there are three latent variables.",
            "There is also capital T and also small teensy.",
            "It's not perhaps very intuitive to look at this.",
            "An representation will function, but there is some."
        ],
        [
            "It's simpler the presentation, which is the limit of far.",
            "The mixer when we take the Delta parameter going to, In this case we get that, so this part is like the original to sense Torquay representation, But here we get that instead of using the reward only at the last time step of some projects Dixie, we actually we can use all the reward the some of the rewards of dent in the course of the trajectory.",
            "Now this turns out to be identical to the stochastic shortest path formulation of anything Verizon value function, which is known since 96 and even earlier this actually a small paragraph in the book.",
            "A bit sick acid please.",
            "And actually, this morning I realized there is a paper by Hoffman and others in a few days that actually also proved the same.",
            "Mixer representation for the value function.",
            "So one way to think about freedom is the generalization of this stochastic sort shortest path formulation of an infant infant horizon value function.",
            "So what is the benefit of having?"
        ],
        [
            "This new model is now that we can have a more efficient TM algorithm, in particular now.",
            "What we can do is we can we can sample First Capital T, which is the length of some trajectory.",
            "Then we draw a trajectory of that length.",
            "And then what we can do is we can reuse all the subjects of this trajectory, so all the length one length to length etc.",
            "Until capital T sub trajectories of C and we can assign them important weight proportional to their corresponding reward times where that comes out of our theory.",
            "When the game equals equals Delta, this drops this Council, so the importance weighted just reward.",
            "And then step the energy function that we have to maximize the expectation of the sum of all the.",
            "All time steps between zero and the length of its trajectory.",
            "These are the rewards or rewards multiplied by some content is the log of the.",
            "Probability of the trajector itself.",
            "So."
        ],
        [
            "Let's see how this is related to optimistic policy iteration algorithms for model reinforcement learning, and this is in this case we assume that here that the state and action spaces are discrete and the policies must multinomial.",
            "Anne."
        ],
        [
            "So this is standard stuff.",
            "The probability the low probability of projective lengthy can be written as the sum of all possible X and you or possible States and actions log of the.",
            "Probability parameters for the policy parameters and these are the.",
            "These are the counts of how many times state action pair appears in some project.",
            "Trixie up to sometimes step.",
            "And then step is.",
            "Involves a quantity that I will explain in a minute, what it, what's its meaning is and this is the parameters that we have to maximize over.",
            "Now let's see this quantity here.",
            "Takes a very interesting interpretation in terms of known in terms of known.",
            "No."
        ],
        [
            "Functioning reinforcement learning every Q function so we can call this a Q function for every state action pair.",
            "We can define the Q function by taking."
        ],
        [
            "This part of the energy function.",
            "And this is indeed well known to function."
        ],
        [
            "Sensitive, we think a bit about that.",
            "It actually does every visit Montecarlo, so it's a it's a partial evaluation of the currently followed policy.",
            "By every visit, Monte Carlo in a batch manner, so there's a bunch of trajectories.",
            "These are rewards.",
            "This can be ignored.",
            "This form.",
            "These are rewards, and this accounts an that.",
            "Perhaps the difference here is that we normalize.",
            "Using the length of its trajectory.",
            "So its trajectory is set before has different length, it's not finalized in every trajectory the length of its object trajectory is drawn from a geometric distribution, but for the rest is standard.",
            "Every visit Monte Carlo.",
            "And then step the maximization of energy function gives us.",
            "If we use a LaGrange multiplier to ensure that these sums to one overall actions for every state.",
            "This gives us that the policy is proportional to the Q values.",
            "So again."
        ],
        [
            "Algorithm.",
            "That I showed at the beginning, so we only have to sample a bunch of random length trajectories.",
            "The length of every trajectory is a draw from geometric distribution.",
            "With this parameter Delta that we have to choose at the beginning.",
            "But we can just choose it equal to the gamma to the discount factor.",
            "And then we estimate the Q function is a set by every visit Monte Carlo and we update the policy by just making the policy proportional to the Q value itself.",
            "They're not, there's no learning parimeter other than that, only the Delta is the only free parameter.",
            "But in the paper we have a different version where there is actually slightly different version of the stochastic game, but.",
            "It's irrelevant at the moment.",
            "It's also important.",
            "OK, so a nice also aspect of this algorithm is that this non bootstrapping.",
            "There is no Bellman equation.",
            "There is no.",
            "Value there's no Bellman backups, and therefore can also be applied to inform the piece.",
            "It's basically as long as it relies on Monte Carlo.",
            "Actually, we can.",
            "We know that we can use them from the peace, and in fact it's reminiscent of some other algorithms very similar to some other algorithms that have appeared in the literature.",
            "Is there's a paper by apologizing and Jordan 95 that presents actually very, very similar algorithm which was derived somewhat ad hoc.",
            "The argument they prove that it converges, which optimizes average reward and which requires a learning rate in the policy update.",
            "There's also a lead work by Perkins.",
            "In 2002, they were using a slightly different."
        ],
        [
            "From the Q function to make it convergent on Palm DP's.",
            "Anne.",
            "There's a paper by putting some pick up in 2003 with salsa very similar to that one, but it requires that every the policy is fully evaluated in every time step before we do the policy update operator and perhaps.",
            "There are similarities with the recent work of Rich which.",
            "I haven't actually.",
            "I don't know if there are."
        ],
        [
            "Perhaps there are OK very quickly the this naturally extends to continue States and actions, and for the finite horizon and without getting into the details, I only want to mention that in this case by using a controller parameterization similar to the one that was used by covering Peters at NIPS last year.",
            "Then we get precisely the power algorithm of copper Peters.",
            "So in fact our framework extends the power algorithm to the infant horizon setting.",
            "That's yet another way to view it.",
            "So here in that algorithm, we're using a fixed horizon T. In our case, we will have to randomize over the horizon of every trajectory.",
            "OK, so here this would be.",
            "This is an average of the trajectories.",
            "In our case, every trajectory we had a different length.",
            "Some very preliminary.",
            "We have only two slides, so they are very pretty.",
            "Some very preliminary results this."
        ],
        [
            "The classical chain MTP.",
            "We initialize uniform the stochastic model multinomial policy here with the Delta we said the Delta equal to Grammas or withdraw.",
            "Projectors with length proportional to according to a geometric distribution with fundamental gamma.",
            "We use about size 50 and here we see that the algorithm converges.",
            "It always converge to the optimal policy.",
            "And this is much more interesting."
        ],
        [
            "Experiment, this is the whole way Palm Desert classical benchmark in the palm DP literature.",
            "This is a great robot navigation problem.",
            "With this robot starts from some unknown state and has to read some target goal state, but this is there's hidden information.",
            "It's partially observable.",
            "So what we do here we use a memory is memoryless policy, stochastic memoryless policy, actually taking the observations of the palm DP as states of an MVP we initialize uniform or use Delta bigger than gamma, which means that we draw slightly longer trajectories.",
            "And for large, but size is what we saw, is that the algorithm converts to an optimal policy.",
            "What we see here is.",
            "So these are the iterations of him.",
            "It is the value of the policy, and we see very small error bars and the value here the point 58 actually turns out to be the value from optimal policy, which we verified this after the final acceptance of the paper, and we verified this by running.",
            "An algorithm by Amato and others.",
            "By solving a quadratically constrained linear program so our argument commits."
        ],
        [
            "This is good news.",
            "OK, so here's the summary.",
            "We saw how to cast an interaction model for reinforcement problem as a mixer learning problem.",
            "We proposed a new mixer modeling a new algorithm which does more efficient sampling.",
            "There's some interesting links between stochastic game and optimistic policy iteration.",
            "The algorithm can be applied to palm DP's and it gave some good results on the hallway, but more experimental results would be included soon, hopefully.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. My name is Nicholas.",
                    "label": 0
                },
                {
                    "sent": "Thanks for being here.",
                    "label": 0
                },
                {
                    "sent": "This is John work with Mark Descent with sitting back there.",
                    "label": 0
                },
                {
                    "sent": "So sometimes I get difficult questions.",
                    "label": 0
                },
                {
                    "sent": "I might refer to him to answer them.",
                    "label": 0
                },
                {
                    "sent": "OK, this work is about model fitting, reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "It's an approach in which we want to cast model for reinforcement learning is a problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exposing the lack of a mixed model and I will explain how this is possible.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to be consistent with the title of the session is the menu, so I will say a few things about the general idea how to cast stochastic optimal controllers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Picture learning problem.",
                    "label": 0
                },
                {
                    "sent": "And then I will focus on the Model 3 reinforcement learning case and I will propose the stochastic email EM algorithm for solving.",
                    "label": 1
                },
                {
                    "sent": "For optimizing the value function.",
                    "label": 0
                },
                {
                    "sent": "One of the contributions of this work is new mix representation and new model, which allows a more efficient in algorithm.",
                    "label": 1
                },
                {
                    "sent": "Another interesting contribution is that there is an interesting relationship between the stochastic EM algorithm that we propose and the family of methods in moderate moderate enforcement learning, which is called optimistic policy iteration for the instances are so one for those of you who are familiar with that is a member of that family, and I will give very some very preliminary results.",
                    "label": 0
                },
                {
                    "sent": "Experiment results.",
                    "label": 0
                },
                {
                    "sent": "OK, before I start I dig into the math.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm that eventually comes out of all the theory, so I want to start with that.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize the discounted value, so this is an interaction problem with the discount factor gamma.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm works as follows.",
                    "label": 0
                },
                {
                    "sent": "We choose a scalar Delta which is from zero to one.",
                    "label": 0
                },
                {
                    "sent": "We initialize the policies stochastic policy P, and then we repeat the following steps until we converge.",
                    "label": 0
                },
                {
                    "sent": "We simply batch of trajectories from the MDP by using the policy P, an special characteristic of our our algorithm is this projectors have random length.",
                    "label": 0
                },
                {
                    "sent": "So the length T of its trajectory is drawn from some geometric distribution with parameter Delta.",
                    "label": 0
                },
                {
                    "sent": "That's a new thing.",
                    "label": 0
                },
                {
                    "sent": "Then we estimate the Q function for all X for Allstate action pairs with standard batch every visit Monte Carlo, and then we updated the stochastic policy by making it proportional to the Q values, which is also somewhat unusual.",
                    "label": 0
                },
                {
                    "sent": "We don't see it very often in the literature.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it basically that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And let's see now how we get there.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "There's been some.",
                    "label": 0
                },
                {
                    "sent": "I work lately trying to relate planning or optimal control problems with probabilistic inference and learning problems.",
                    "label": 0
                },
                {
                    "sent": "There are series of papers recently.",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                },
                {
                    "sent": "First we get the fresh look at the problem by casting.",
                    "label": 1
                },
                {
                    "sent": "Planning.",
                    "label": 1
                },
                {
                    "sent": "Two probabilistic inference we can use existing algorithms for inference learning to solve our planning problems.",
                    "label": 0
                },
                {
                    "sent": "It can give us new tools to analyze algorithms and allows more easy extensions to structure domain structure, policies and little bit harder M DPS.",
                    "label": 0
                },
                {
                    "sent": "The the case where this here is the interaction model free reinforcement in case there are two papers.",
                    "label": 0
                },
                {
                    "sent": "The paper by to science talking in 2000, seven 2006.",
                    "label": 0
                },
                {
                    "sent": "They undressed.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "There Steven to rise and model based case and there's a recent paper by covering Peters.",
                    "label": 0
                },
                {
                    "sent": "They addressed the finite or as a Model 3 case.",
                    "label": 0
                },
                {
                    "sent": "So we're sort of compliment this two.",
                    "label": 0
                },
                {
                    "sent": "Play this work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, this tab is a classical infinite horizon MDP we assume.",
                    "label": 0
                },
                {
                    "sent": "Discrete MDP at the moment.",
                    "label": 0
                },
                {
                    "sent": "We states XT actions, yutian rewards scaled to 0 between zero and one.",
                    "label": 1
                },
                {
                    "sent": "We assume that the process starts from a certain distribution.",
                    "label": 0
                },
                {
                    "sent": "POX Zero and the task is to find the stochastic policies that maximize the value function, which is an infant horizon.",
                    "label": 0
                },
                {
                    "sent": "Discounted value function.",
                    "label": 0
                },
                {
                    "sent": "And we assume that we are giving a discount factor gamma, which is between 01.",
                    "label": 0
                },
                {
                    "sent": "So this is a classical standard enforcement problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the idea how to cast the value function as a mixture likelihood.",
                    "label": 1
                },
                {
                    "sent": "So to send a story, they showed that V actually can be expressed in the local function of an infinite mixture model, so it makes more within the number of components, and there are two key ideas that allow allow us to do this.",
                    "label": 1
                },
                {
                    "sent": "The first is that we take the discount factor gamma to the power T and we treat it as a geometric distribution.",
                    "label": 0
                },
                {
                    "sent": "Overtime steps, so this PFT.",
                    "label": 1
                },
                {
                    "sent": "And the second idea, which actually is not new IT dates back to 1988 and also 97 is to treat the rewards as Bernoulli para meters.",
                    "label": 0
                },
                {
                    "sent": "So here we introduce some fictitious random variable R and we assume that the reward obtained it's time.",
                    "label": 0
                },
                {
                    "sent": "Some time step T is the probability of.",
                    "label": 0
                },
                {
                    "sent": "Actually this is the probability that this fictitious random variable takes the value one.",
                    "label": 0
                },
                {
                    "sent": "Now if we incorporate these two ideas in the definition of the value function and we slightly related the value function.",
                    "label": 0
                },
                {
                    "sent": "So this is the sum of discount.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Version of the expected reward.",
                    "label": 0
                },
                {
                    "sent": "The time step T. Now if we use the.",
                    "label": 0
                },
                {
                    "sent": "So first of all this this expectation is the sum of all possible trajectory is with lengthy.",
                    "label": 0
                },
                {
                    "sent": "Um of the rewarded time 16 and this is the distribution of trajectories of lengthy through the MDP forgiven policy, which is given by the standard MDP dynamics.",
                    "label": 1
                },
                {
                    "sent": "This is a factor that depends on the dynamics of the MDP, and this is a component that depends on our policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if we incorporated two ideas that I mentioned in the definition.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The value function so treat.",
                    "label": 1
                },
                {
                    "sent": "This is a geometric prior and treat this as a Bernoulli probability what we get.",
                    "label": 0
                },
                {
                    "sent": "Is we get this mixture?",
                    "label": 0
                },
                {
                    "sent": "So this mixture is this a mixture likelihood which is proportional to the value function of the original problem.",
                    "label": 1
                },
                {
                    "sent": "Um so.",
                    "label": 1
                },
                {
                    "sent": "An interesting thing here is that the observation likelihood is in fact identical to the terminal reward of a trajectory.",
                    "label": 0
                },
                {
                    "sent": "So we can think of this as a generative process in which we first draw a length of a trajectory T, and then we draw a trajectory of lengthy from the MDP using policy P, and then we draw from this Bernoulli model and then the probability that are that the variable R takes value one.",
                    "label": 0
                },
                {
                    "sent": "Is actually proportional to the value function of the MDP.",
                    "label": 1
                },
                {
                    "sent": "And the chorus.",
                    "label": 1
                },
                {
                    "sent": "This is a mixture.",
                    "label": 0
                },
                {
                    "sent": "Lack of corresponding joint model which involves.",
                    "label": 0
                },
                {
                    "sent": "Two latent variables T&C.",
                    "label": 0
                },
                {
                    "sent": "So it is the length exceeds direct.",
                    "label": 0
                },
                {
                    "sent": "The trajectory of that length and this fictitious random variable R. And this parameterized by the policy parameters of the one point.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was there originally.",
                    "label": 0
                },
                {
                    "sent": "Derivation of the sentence talking.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see now how we can go model free.",
                    "label": 0
                },
                {
                    "sent": "So we want to Model 3.",
                    "label": 0
                },
                {
                    "sent": "So first of all, how are you?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "6:00 PM most of you, I presume, are familiar with them.",
                    "label": 0
                },
                {
                    "sent": "So yeah, iterates between 2 steps.",
                    "label": 0
                },
                {
                    "sent": "They step them.",
                    "label": 0
                },
                {
                    "sent": "Step in.",
                    "label": 0
                },
                {
                    "sent": "This step will compute the posterior distribution over the latent variables given the observed quantities.",
                    "label": 1
                },
                {
                    "sent": "Here the observed are not actually observed.",
                    "label": 0
                },
                {
                    "sent": "Its effect issues observed random variable R, but we did it as observed.",
                    "label": 0
                },
                {
                    "sent": "So we need to find distribution or sample from distribution of the latent variables given R. These factors as follows.",
                    "label": 0
                },
                {
                    "sent": "And in the M step we have to maximize the energy.",
                    "label": 1
                },
                {
                    "sent": "The expected joint look like on function.",
                    "label": 0
                },
                {
                    "sent": "The energy function of our policy.",
                    "label": 0
                },
                {
                    "sent": "That involves the log of the factors of our joint distribution that contain the unknown parameters P. So basically this is just the probability of.",
                    "label": 0
                },
                {
                    "sent": "The lengths objectors.",
                    "label": 1
                },
                {
                    "sent": "So this is pretty standard stuff that comes out from the definition of the joint model and the mixture like.",
                    "label": 0
                },
                {
                    "sent": "Now if we have access to the model of the MDP, we can actually do this, yeah?",
                    "label": 0
                },
                {
                    "sent": "In an exact manner, we can compute this distribution analytically and we can do them step analytically, but if we don't have access to the empty dynamics, we have to sample from this distribution.",
                    "label": 0
                },
                {
                    "sent": "This is the stochastic yam family of methods.",
                    "label": 0
                },
                {
                    "sent": "So then the question is how we actually drove samples from this posterior distribution of latent variables given the observed quantities.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relatively straightforward way to to draw from this distribution is to perform a forward simulation implementation in which we first draw length, length of length of the trajectory, and then we draw a trajectory of that length using the previous policy previous step policy.",
                    "label": 0
                },
                {
                    "sent": "And assigned to this trajectory CI an importance weight which equals the reward of this trajectory.",
                    "label": 0
                },
                {
                    "sent": "The last time step.",
                    "label": 0
                },
                {
                    "sent": "Remember that this was identical to the reward at some time step, and in this case it's the terminal that awarded the last time step of the trajectory, see.",
                    "label": 0
                },
                {
                    "sent": "So we could plug this sampling into their stochastic EM and we would have a convergence theorem algorithm.",
                    "label": 0
                },
                {
                    "sent": "But the problem is.",
                    "label": 0
                },
                {
                    "sent": "That's that's an earmark.",
                    "label": 0
                },
                {
                    "sent": "It would only utilize the terminal reward of a trajectory, which is clearly a waste, because that would require a huge sample complexity in order to produce any meaningful results.",
                    "label": 1
                },
                {
                    "sent": "So they make a decision of this work is actually to propose a new mixer representation, a new mixer model.",
                    "label": 0
                },
                {
                    "sent": "That would make the step little bit more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the new mixer representation that we propose, so this is a main theme of this work.",
                    "label": 0
                },
                {
                    "sent": "We have shown that we can actually.",
                    "label": 0
                },
                {
                    "sent": "Actually, there is an infinite.",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many ways to write.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value function is a mixture and there's a family that is parameterized by the scalar Delta that we can choose between gamma and one.",
                    "label": 0
                },
                {
                    "sent": "And then we introduce a new random variable, capital T and then we can show that we can add the value function as this mixture that now is similar to the previous one slide definition of their observed.",
                    "label": 0
                },
                {
                    "sent": "The observation model is slightly different and also this new.",
                    "label": 0
                },
                {
                    "sent": "There's a new also distribution over this latent variable T and here in this model there are three latent variables.",
                    "label": 1
                },
                {
                    "sent": "There is also capital T and also small teensy.",
                    "label": 0
                },
                {
                    "sent": "It's not perhaps very intuitive to look at this.",
                    "label": 0
                },
                {
                    "sent": "An representation will function, but there is some.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's simpler the presentation, which is the limit of far.",
                    "label": 0
                },
                {
                    "sent": "The mixer when we take the Delta parameter going to, In this case we get that, so this part is like the original to sense Torquay representation, But here we get that instead of using the reward only at the last time step of some projects Dixie, we actually we can use all the reward the some of the rewards of dent in the course of the trajectory.",
                    "label": 0
                },
                {
                    "sent": "Now this turns out to be identical to the stochastic shortest path formulation of anything Verizon value function, which is known since 96 and even earlier this actually a small paragraph in the book.",
                    "label": 0
                },
                {
                    "sent": "A bit sick acid please.",
                    "label": 0
                },
                {
                    "sent": "And actually, this morning I realized there is a paper by Hoffman and others in a few days that actually also proved the same.",
                    "label": 0
                },
                {
                    "sent": "Mixer representation for the value function.",
                    "label": 1
                },
                {
                    "sent": "So one way to think about freedom is the generalization of this stochastic sort shortest path formulation of an infant infant horizon value function.",
                    "label": 1
                },
                {
                    "sent": "So what is the benefit of having?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This new model is now that we can have a more efficient TM algorithm, in particular now.",
                    "label": 1
                },
                {
                    "sent": "What we can do is we can we can sample First Capital T, which is the length of some trajectory.",
                    "label": 0
                },
                {
                    "sent": "Then we draw a trajectory of that length.",
                    "label": 1
                },
                {
                    "sent": "And then what we can do is we can reuse all the subjects of this trajectory, so all the length one length to length etc.",
                    "label": 0
                },
                {
                    "sent": "Until capital T sub trajectories of C and we can assign them important weight proportional to their corresponding reward times where that comes out of our theory.",
                    "label": 0
                },
                {
                    "sent": "When the game equals equals Delta, this drops this Council, so the importance weighted just reward.",
                    "label": 0
                },
                {
                    "sent": "And then step the energy function that we have to maximize the expectation of the sum of all the.",
                    "label": 0
                },
                {
                    "sent": "All time steps between zero and the length of its trajectory.",
                    "label": 0
                },
                {
                    "sent": "These are the rewards or rewards multiplied by some content is the log of the.",
                    "label": 0
                },
                {
                    "sent": "Probability of the trajector itself.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how this is related to optimistic policy iteration algorithms for model reinforcement learning, and this is in this case we assume that here that the state and action spaces are discrete and the policies must multinomial.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is standard stuff.",
                    "label": 0
                },
                {
                    "sent": "The probability the low probability of projective lengthy can be written as the sum of all possible X and you or possible States and actions log of the.",
                    "label": 1
                },
                {
                    "sent": "Probability parameters for the policy parameters and these are the.",
                    "label": 0
                },
                {
                    "sent": "These are the counts of how many times state action pair appears in some project.",
                    "label": 0
                },
                {
                    "sent": "Trixie up to sometimes step.",
                    "label": 0
                },
                {
                    "sent": "And then step is.",
                    "label": 0
                },
                {
                    "sent": "Involves a quantity that I will explain in a minute, what it, what's its meaning is and this is the parameters that we have to maximize over.",
                    "label": 0
                },
                {
                    "sent": "Now let's see this quantity here.",
                    "label": 0
                },
                {
                    "sent": "Takes a very interesting interpretation in terms of known in terms of known.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functioning reinforcement learning every Q function so we can call this a Q function for every state action pair.",
                    "label": 0
                },
                {
                    "sent": "We can define the Q function by taking.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This part of the energy function.",
                    "label": 0
                },
                {
                    "sent": "And this is indeed well known to function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensitive, we think a bit about that.",
                    "label": 0
                },
                {
                    "sent": "It actually does every visit Montecarlo, so it's a it's a partial evaluation of the currently followed policy.",
                    "label": 0
                },
                {
                    "sent": "By every visit, Monte Carlo in a batch manner, so there's a bunch of trajectories.",
                    "label": 0
                },
                {
                    "sent": "These are rewards.",
                    "label": 0
                },
                {
                    "sent": "This can be ignored.",
                    "label": 0
                },
                {
                    "sent": "This form.",
                    "label": 0
                },
                {
                    "sent": "These are rewards, and this accounts an that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the difference here is that we normalize.",
                    "label": 0
                },
                {
                    "sent": "Using the length of its trajectory.",
                    "label": 0
                },
                {
                    "sent": "So its trajectory is set before has different length, it's not finalized in every trajectory the length of its object trajectory is drawn from a geometric distribution, but for the rest is standard.",
                    "label": 0
                },
                {
                    "sent": "Every visit Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And then step the maximization of energy function gives us.",
                    "label": 0
                },
                {
                    "sent": "If we use a LaGrange multiplier to ensure that these sums to one overall actions for every state.",
                    "label": 0
                },
                {
                    "sent": "This gives us that the policy is proportional to the Q values.",
                    "label": 0
                },
                {
                    "sent": "So again.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "That I showed at the beginning, so we only have to sample a bunch of random length trajectories.",
                    "label": 0
                },
                {
                    "sent": "The length of every trajectory is a draw from geometric distribution.",
                    "label": 0
                },
                {
                    "sent": "With this parameter Delta that we have to choose at the beginning.",
                    "label": 0
                },
                {
                    "sent": "But we can just choose it equal to the gamma to the discount factor.",
                    "label": 0
                },
                {
                    "sent": "And then we estimate the Q function is a set by every visit Monte Carlo and we update the policy by just making the policy proportional to the Q value itself.",
                    "label": 0
                },
                {
                    "sent": "They're not, there's no learning parimeter other than that, only the Delta is the only free parameter.",
                    "label": 0
                },
                {
                    "sent": "But in the paper we have a different version where there is actually slightly different version of the stochastic game, but.",
                    "label": 0
                },
                {
                    "sent": "It's irrelevant at the moment.",
                    "label": 0
                },
                {
                    "sent": "It's also important.",
                    "label": 0
                },
                {
                    "sent": "OK, so a nice also aspect of this algorithm is that this non bootstrapping.",
                    "label": 1
                },
                {
                    "sent": "There is no Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                },
                {
                    "sent": "Value there's no Bellman backups, and therefore can also be applied to inform the piece.",
                    "label": 0
                },
                {
                    "sent": "It's basically as long as it relies on Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can.",
                    "label": 0
                },
                {
                    "sent": "We know that we can use them from the peace, and in fact it's reminiscent of some other algorithms very similar to some other algorithms that have appeared in the literature.",
                    "label": 0
                },
                {
                    "sent": "Is there's a paper by apologizing and Jordan 95 that presents actually very, very similar algorithm which was derived somewhat ad hoc.",
                    "label": 0
                },
                {
                    "sent": "The argument they prove that it converges, which optimizes average reward and which requires a learning rate in the policy update.",
                    "label": 1
                },
                {
                    "sent": "There's also a lead work by Perkins.",
                    "label": 0
                },
                {
                    "sent": "In 2002, they were using a slightly different.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the Q function to make it convergent on Palm DP's.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There's a paper by putting some pick up in 2003 with salsa very similar to that one, but it requires that every the policy is fully evaluated in every time step before we do the policy update operator and perhaps.",
                    "label": 0
                },
                {
                    "sent": "There are similarities with the recent work of Rich which.",
                    "label": 0
                },
                {
                    "sent": "I haven't actually.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there are.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perhaps there are OK very quickly the this naturally extends to continue States and actions, and for the finite horizon and without getting into the details, I only want to mention that in this case by using a controller parameterization similar to the one that was used by covering Peters at NIPS last year.",
                    "label": 0
                },
                {
                    "sent": "Then we get precisely the power algorithm of copper Peters.",
                    "label": 0
                },
                {
                    "sent": "So in fact our framework extends the power algorithm to the infant horizon setting.",
                    "label": 1
                },
                {
                    "sent": "That's yet another way to view it.",
                    "label": 0
                },
                {
                    "sent": "So here in that algorithm, we're using a fixed horizon T. In our case, we will have to randomize over the horizon of every trajectory.",
                    "label": 0
                },
                {
                    "sent": "OK, so here this would be.",
                    "label": 0
                },
                {
                    "sent": "This is an average of the trajectories.",
                    "label": 0
                },
                {
                    "sent": "In our case, every trajectory we had a different length.",
                    "label": 0
                },
                {
                    "sent": "Some very preliminary.",
                    "label": 0
                },
                {
                    "sent": "We have only two slides, so they are very pretty.",
                    "label": 0
                },
                {
                    "sent": "Some very preliminary results this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The classical chain MTP.",
                    "label": 0
                },
                {
                    "sent": "We initialize uniform the stochastic model multinomial policy here with the Delta we said the Delta equal to Grammas or withdraw.",
                    "label": 0
                },
                {
                    "sent": "Projectors with length proportional to according to a geometric distribution with fundamental gamma.",
                    "label": 0
                },
                {
                    "sent": "We use about size 50 and here we see that the algorithm converges.",
                    "label": 0
                },
                {
                    "sent": "It always converge to the optimal policy.",
                    "label": 1
                },
                {
                    "sent": "And this is much more interesting.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment, this is the whole way Palm Desert classical benchmark in the palm DP literature.",
                    "label": 0
                },
                {
                    "sent": "This is a great robot navigation problem.",
                    "label": 0
                },
                {
                    "sent": "With this robot starts from some unknown state and has to read some target goal state, but this is there's hidden information.",
                    "label": 0
                },
                {
                    "sent": "It's partially observable.",
                    "label": 0
                },
                {
                    "sent": "So what we do here we use a memory is memoryless policy, stochastic memoryless policy, actually taking the observations of the palm DP as states of an MVP we initialize uniform or use Delta bigger than gamma, which means that we draw slightly longer trajectories.",
                    "label": 1
                },
                {
                    "sent": "And for large, but size is what we saw, is that the algorithm converts to an optimal policy.",
                    "label": 1
                },
                {
                    "sent": "What we see here is.",
                    "label": 0
                },
                {
                    "sent": "So these are the iterations of him.",
                    "label": 0
                },
                {
                    "sent": "It is the value of the policy, and we see very small error bars and the value here the point 58 actually turns out to be the value from optimal policy, which we verified this after the final acceptance of the paper, and we verified this by running.",
                    "label": 0
                },
                {
                    "sent": "An algorithm by Amato and others.",
                    "label": 0
                },
                {
                    "sent": "By solving a quadratically constrained linear program so our argument commits.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is good news.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the summary.",
                    "label": 0
                },
                {
                    "sent": "We saw how to cast an interaction model for reinforcement problem as a mixer learning problem.",
                    "label": 0
                },
                {
                    "sent": "We proposed a new mixer modeling a new algorithm which does more efficient sampling.",
                    "label": 1
                },
                {
                    "sent": "There's some interesting links between stochastic game and optimistic policy iteration.",
                    "label": 1
                },
                {
                    "sent": "The algorithm can be applied to palm DP's and it gave some good results on the hallway, but more experimental results would be included soon, hopefully.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}