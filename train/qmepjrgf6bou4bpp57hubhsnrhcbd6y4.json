{
    "id": "qmepjrgf6bou4bpp57hubhsnrhcbd6y4",
    "title": "MPI & OpenMP (Part 3)",
    "info": {
        "author": [
            "David Henty, EPCC, University of Edinburgh"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_henty_MPI_openMP_part3/",
    "segmentation": [
        [
            "So I was my plan was."
        ],
        [
            "To do the Open MP overview and the walk through the Pi example, I don't think I'll have time to do this, I think, so I'll probably do the open MP overview now and then do this.",
            "Do this after lunch is just a small shift.",
            "I think I'd rather spend more time on this on this tour, so I'm just going to give a brief.",
            "Overview Of open MP.",
            "Initially and then we'll get onto the details this afternoon, hopefully even if you've used open MP before, like yesterday with the."
        ],
        [
            "With the MPI you know it's not as simple as well.",
            "The problem about whoops.",
            "The problem with my open MP is that if you see a working open MP code, it looks trivial, but it's a lot in some senses it's more complicated than you think, and in some sense it's it's a lot simpler that the problem is that open MP requires the compiler and open MP.",
            "The compiler does stuff stuff for you, so it looks like magic, but it isn't magic.",
            "OK, compilers are pretty dumb really.",
            "They do very mechanical things, so if you understand what open MP is actually doing, it really helps.",
            "It is not magic, OK?",
            "Compilers a piece of software written by people.",
            "OK, there's no reason to, you know, so you know they're not magic things.",
            "People sometimes think that compilers are magical, mystical, pitner.",
            "Their programs were written by people, some bit, some are good, some it's my bad.",
            "Compiling program is very difficult.",
            "You're asking somebody to write a program which is a compiler which is supposed to compile any random piece of text generated by any of the 7 billion people on the planet, and do a good job.",
            "It's clearly impossible task, so you know.",
            "Compilers tend to be quite so, but the problem with open MP is that the compiler does it for you, and it can look like magic, but it is not magic.",
            "And people who think it's magic End up writing wrong programs.",
            "Incorrect programs, which is your dealer simulation, and you probably took incorrect paper then, hey, how if you're writing a piece of software to fly a plane or something, you write an incorrect program, then that's probably a bit more important, and I gotta go through the concept here and they are quite important, so I'll talk about shared variables parallelism, which which uses threads, not processes.",
            "OK.",
            "So I'm saying that what I'm going to do is I'm going to discuss the basic just before lunch.",
            "I'll discuss the basic concepts of open MP and threads and then I'll.",
            "I'll do some programming.",
            "I'll do.",
            "This specific example had planned to do before lunch or now do it now.",
            "Do it after lunch, so shared variables and open MP and threads are all intricately connected.",
            "And I'll talk about that.",
            "So shared variables is parallel programming using threads."
        ],
        [
            "I'll talk about what shared variables parallelism is, what threads are, and how they fit to shared memory architectures.",
            "Threads are not processes, MPI message passing.",
            "Programming uses multiple processes.",
            "People often say threads.",
            "But they.",
            "Excuse me but they shouldn't.",
            "Threads and processes are not the same thing.",
            "And then we'll talk about practicalities.",
            "How operating systems fit in and how you use these things on real machines and that will be so."
        ],
        [
            "Add variables threads based parallelism."
        ],
        [
            "So many applications contain a single each process as a single thread.",
            "When you write normal program and you compile it and you run it, you have one thread.",
            "So you just you just something that it just runs to the end.",
            "But a single process can actually contain multiple threads.",
            "Threads are like mini programs, so after execution your program runs and it can create multiple threads which are like mini programs within the parent process.",
            "OK, and each chat thread is like is like a child process contained within a parent process.",
            "It's not quite the same, but but threads belong.",
            "Book have a parent process.",
            "You launch a single process and at runtime you say right?",
            "I would now like to create multiple threads and they become like running multiple programs.",
            "But there is a hierarchy and threads weren't in like everything.",
            "Threads weren't invented for HPC.",
            "If I wrong wrong machine threads were invented.",
            "For doing things like you know.",
            "Web browsers have a Webdriver web.",
            "You have a web.",
            "I have a web browser which is a process.",
            "When I run it, I run Firefox.",
            "It's a single process, but at runtime Firefox says well, I've got.",
            "I've got this little ticker tape down here, that's like that would be a separate thread.",
            "And this movie here will be a separate thread to run the movie player and then and then maybe a thread monitoring for mouse clicks.",
            "So this process has lots of threads doing different things with threads were invented so you could do lots of different things at the same time.",
            "And the reason that they belong to the parent processes.",
            "All these threads need to write to the screen they need.",
            "They need to share the memory with apparent processor.",
            "Two processes cannot share memory, but threads within a parent process all have access to the same memory, and it was designed to allow you to have little things you know, monitor mouse clicks and updating this banner and doing all kinds of stuff.",
            "But as usual, we're going to hijack them to try and do parallel programming."
        ],
        [
            "The shared made concept well so this is threads can all see the date of the parent process.",
            "That is the absolutely key thing or every thread has access to all the data of the parent process.",
            "And so as John described on the first day, it's like having a large map of the US in an office and you've got four people all in the same office and they say look, you look at the top left hand corner of the map.",
            "You look at the top right hand corner.",
            "You may decide that the access different areas of this shared memory, but they in principle have access to all of it.",
            "The threads can run on different cores, and that's how you get speedup, and we'll see how that works later on, But again.",
            "Threads on in threads aren't introduced into things like web browsers for performances.",
            "It's there for freeze of programming, but when you have multiple core machines that then the threads can run on different cores to substantial parallel speedup.",
            "So for example, if I thought I had the traffic model with the eight cells 01234567, I could decide that this thread operates on the 1st four and this thread operates on the second form, so that's different from MPI and MPI.",
            "You would say physically.",
            "These fourth, these four cells are stored on the 1st process and these four cells are stored on the second process.",
            "Not true here.",
            "They're all stored in the same memory on the same process.",
            "You've just decided that one thread might like to do the 1st for another might do the next floor."
        ],
        [
            "So the analogy, and it's a very good analogy, is one large white board in the two person office, which is the shared memory, and two people are working on the same problem.",
            "That's like 2 threads running on different cores attached to the same memory.",
            "So again, if you're working with your office mate, you've got a big white board.",
            "It's great, you can just write stuff wherever you want.",
            "You can all access the data.",
            "So, but the problem is.",
            "You don't want people to interfere with each other.",
            "OK, that's actually the problem.",
            "In shared variables, Open MP programming is that it's kind of anarchy.",
            "Any thread can access any of the memory, which sounds great, but what happens if somebody up you're working on something or someone comes and rubs it out?",
            "OK, that's a problem, so the way it works, the way that open MP solves it says, well, there is shared data, but you need private data.",
            "Clearly need private data to record your own stuff, and so the most important concept about open MP is that it has shared data, which is the default.",
            "But you can have private data and of course all that you've done is said.",
            "This bottom left hand corner is mine and this bottom right hand corner is yours.",
            "OK, but but conceptually, MPI open MP distinguish being shared data and private data.",
            "There is no such thing.",
            "Of course, there's just memory, but you know you can carve it up like this.",
            "So we do need private data, so that's one of those things that you don't have this choice in.",
            "At MPI and MPI, all data is private.",
            "All data is private MPI.",
            "No other process can access your data in open MP threads can access the same data, so you have to decide.",
            "Is it shared or is it private?"
        ],
        [
            "So this is sort of a diagram was saying that we have some shared data.",
            "We have saved three threads and they can all access the shared data.",
            "So this red thread can access all this data, but they have their own private data here, but they also have their own PC, which means program counter.",
            "What this is saying is that threads could be doing different things at different times.",
            "One different threads can be operating different, running different functions, different subroutines at the same time.",
            "And that's not quite what we do in open MP, but different threads can have their own private data and they also are operating on potentially different completely different instructions at the same time."
        ],
        [
            "So we saw this diagram yesterday for MPI.",
            "What I said in MP wanted data.",
            "What's that communicate data from process one to process too?",
            "And that was that was done by message passing, which is like sending an email or making a phone call in open MP.",
            "There are no messages.",
            "You collaborate by reading and writing to shared memory.",
            "If you've gone off for lunch and you want to tell back to your to your office mate in message passing, you send them an email and shared memory in open MP, you'd write on the board.",
            "I have gone to the I have gone to lunch and when they come in they can read that and it's there.",
            "So we want to communicate a value from thread one to thread two.",
            "So thread one says my equals."
        ],
        [
            "23 and then you immediately after wait a second, you said that data could be shared or private.",
            "OK, what is this one?",
            "But my I've tried to indicate that A is a private variable, but the way it works in open MP is when you declare a well you mark variables are being a shared or private.",
            "So when you see something like my equals 23 in an open MP program, you have to go and look up and say it's my a shared or private variable.",
            "But here my a supposed to is supposed to indicate it's private.",
            "OK so my equals 23."
        ],
        [
            "My private data has my equals 23 then to write."
        ],
        [
            "The shared data.",
            "All I say is equals my a, so as long as a has been declared to be a shared variable, which is the default.",
            "What happens is that that is just a copy."
        ],
        [
            "What happens is your private copy of my A is copied from the shared the private region to the shared region.",
            "So that's like doing a little calculation in your notebook and say, OK, well I've got the answer.",
            "Now I want my officemate to know, so you write it on the on the shared on the shared space.",
            "Now now your office mate might want to read that and all they do to read, as they say my."
        ],
        [
            "Equals 8 + 1, so they have available may, but it's private so they have a different.",
            "They're both called my A, but they are separate variables because their private, so everyone has their independent copy of a private variable so they can have different values.",
            "So this does two things.",
            "This illustrates two things a it shows that the data is copied from the shared space, the private space because a is a private variable.",
            "My is a private variable and A is a shared variable."
        ],
        [
            "And Secondly, it shows that the two my eyes can have different values and what we'll see if we use this for loops when you're doing a loop, you might say there's a big data set.",
            "I want thread zero to two items, one to 10 thread, one todo items, 11 to 20 to break up the loop there you need to have a private variable, the loop counter.",
            "I need to be private, 'cause by definition each thread is on a different iteration, a different time.",
            "However, there's something I haven't that this isn't guaranteed to work.",
            "There's something I've assumed here, which I haven't made explicit.",
            "So if there's something I'd went through it, but you know, there's an assumption here.",
            "Yeah, I assume that this statement eight was my A was executed before this statement my it was a plus one and there's nothing in this program to mandate that.",
            "So now this isn't a problem in message passing because when you pass a message you you send it and then if thread two is a bit slow, it waits a live thread.",
            "One is a bit fast, it's waiting for them to come in.",
            "But in Beacon threaded program because you can read and write the same membrane.",
            "Mutual synchronization is a big issue, so this code is not guaranteed to work.",
            "OK, so this is the kind of thing people write this kind of code.",
            "It works 99 times out of 100 and then 100 times it doesn't work and you get the wrong answer.",
            "So so it's really it's very easy to write incorrect shared memory, open MP programs.",
            "And this is the reason so."
        ],
        [
            "Synchronization is crucial for the shared variables approach.",
            "Thread two code must execute after thread one another, but the most commonly thing to do as a global barrier.",
            "So as I said, you never almost never said you almost never need global barriers in MPI for correctness, but you need them all the time and shared variables or some synchronization.",
            "You need to say look, I want you guys if you're going to read data from me, you have to execute your instructions after I've executed mine.",
            "So we're going to do a barrier.",
            "So what you say is.",
            "You would do."
        ],
        [
            "My equals 23.",
            "Eight was my a barrier and thread two would have barrier my equals a + 1.",
            "So you draw a line across here that says that none of these instructions can happen after this instruction.",
            "OK, so that's that's a common Now you won't see them often open MP, but open MP puts them in by default, so open MP puts.",
            "It may not be obvious, but open MP puts barriers in lots of places.",
            "To try and help you with correctness."
        ],
        [
            "OK, so writing parallel, so most common global basic message.",
            "Other mechanisms exist such as locks.",
            "So you can have.",
            "This is the usual thing.",
            "You can lock variables and you can, you can say nobody can access this variable until I'm finished with it.",
            "But barrier is the most common one.",
            "Writing parallel code is relatively straightforward 'cause you can access the shared data as and when it's needed.",
            "It's really quite nice.",
            "You can just, you know.",
            "If you have a shared big shared array like you're the road, or that the map that you're working on any thread can access any value anytime it wants, so that that makes parallel programming easier.",
            "But it's very easy to write incorrect code.",
            "Getting correct code can be difficult."
        ],
        [
            "So here's a specific example.",
            "I've got an array a which has got 888 elements.",
            "I want to compute the sum and we saw in the pie example yesterday which I did in MPI.",
            "How to do this?",
            "And conceptually, it's quite similar in open MP, except that you have to really well at least in threaded rod, you have to realize that although I've split, I'm said that that thread zero is operating on A0A1A2 and A3 and thread one is operating on a 4A5A6 and A7 K the array is not split.",
            "The array is just somewhere on the whiteboard in the shared space.",
            "We've just decided that this particular loop they will iterate over these spaces, but the array is not distributed between the threads.",
            "This is a shared array, just sits in the space, but we just decided that will work on different parts of it.",
            "So if I want to compute this summation, the usual classic observation is summation is associative, so thread zero consume the 1st four elements.",
            "I've called them thread one and two have no threads.",
            "Fraser Construction on the 1st four elements thread, one consumed the next four elements, they can add them together.",
            "So if I have a loop, every code, this is looking very likely that the MPI code every code would look for Mike as I start to I stop.",
            "They didn't increment there.",
            "There my a sum and they then the loop.",
            "So you have to always asking in a shared variables open MP program what?",
            "What is the classification of these variables?",
            "Are they shared or private?",
            "Well clearly or a is shared a is that big arrays are always share the thing you're working on.",
            "OK, you can't duplicate bigger Asian run out of memory to start with, but I mean you know conceptually you have a big large array operating on.",
            "It just sits there in the shared space, OK?",
            "Um?",
            "The result is shared 'cause there is only one result you want to come up with a single result and you wanted to be able to everybody, but to compute that you need some private variables, you need the loop counter loop counters are always private.",
            "If you're splitting a loop up, by definition, different threads will have different values of high at the same time, so it makes no sense to have a shared loop counter.",
            "We need to have loop limits.",
            "Every thread is running the same piece of code, but they can have.",
            "They can do different things by having different loop limits.",
            "So you have a nice start and then I stop there private.",
            "And the local some in my asum is also private.",
            "'cause you're accumulating a local value and then you add them together.",
            "So this looks quite easy, but the killer comes in the at the end.",
            "How do you add these variables together too?",
            "How do you add these variables together to get a common result?",
            "So you might just say a sum plus equals myisam.",
            "OK, you just say everybody adds their local value to the global value.",
            "OK, that does not work and this is the thing which is which is the thing which catches most people in open MP that is not guaranteed to work OK.",
            "The reason is there is no such thing as adding and adding a value to a variable.",
            "OK, computers, very stupid.",
            "They can do three things.",
            "They can read data from memory, they can add numbers and they can write it back again.",
            "It's basically all they can do.",
            "So to add a number to a variable you have to read the variable into register.",
            "You have to add to it and you have to write it back again.",
            "OK and the problem is if you have two threads doing this at once, OK One thread reads the variable and adds to it and writes it back again at the same time.",
            "Somebody else could have read the same variable.",
            "And then you don't get the right answer, OK?",
            "It's like saying I want everybody I want to compute the total age of.",
            "I want to compute the sum of every stage in this room, and I'll write on this whiteboard.",
            "K alright on this whiteboard a total I want you all your Azure your age that total.",
            "The problem is you know you say over the total 53 I need to add my age so I take it away and you have to use a Calculator go 53 plus my age is 100 and something and then you go back but why you've been doing that?",
            "Somebody else is done the same thing.",
            "OK so whenever you have multiple threads accessing shared variables or reading them, writing them at least you have this you have to worry about synchronization.",
            "Now all these things are in principle worries because open MP is designed.",
            "For scientific and technical programming, where these these are very commonplace and open MP has constructs which do all this for you, but it's essential to understand that that what it is doing it is not magic.",
            "It's just doing stuff which is just fairly straightforward but but crucial.",
            "So what I could do is I could say thread 0 does a sum plus equal to my a sum than a barrier and then do thread 118 plus equal to my son.",
            "OK, you could do something like that that would be and there are other ways of doing it, but you have to be careful when you have multiple threads access.",
            "Writing to share data or modifying it that you control any race conditions.",
            "Anything that could clash on each other.",
            "OK, and then we use this as a specific example after after lunch."
        ],
        [
            "Hardware, I'm saying you've got multiple people in an office sharing the same whiteboard.",
            "What we want is multiple threads, or at least multiple physical processors or cores accessing the same memory.",
            "You require a shared memory architecture like this laptop.",
            "OK, this laptop is a shared memory architecture has 54 cores.",
            "I've called them processors here they can all access the same memory and the way have is that they have a shared bus.",
            "They basically they can all read and write the same memory because it's all mediated by some bus, so they can all read.",
            "Write to the same memory.",
            "You can immediately see this is a problem.",
            "Because every read and write from all the processes has to go through this link.",
            "OK, you might say, isn't that a bottleneck?",
            "But it is OK.",
            "So modern computers are incredibly bad at processing data.",
            "The Act, the data speeds, the speeds here are a factor of 10 or 100 slower than the speeds of the processors so.",
            "But there are ways that we try and get around that by.",
            "Having caches so you when you read data you store it locally and stuff like that.",
            "But you know this is.",
            "This is why it's very hard.",
            "This is why you can build distributed memory computers with millions of processes.",
            "You just buy a million laptops and stick them altogether.",
            "But it's very hard to build shared memory architectures with more than maybe a few 10s or 100 cores because at least in the most naive approach like this, you get this button there.",
            "Clearly there clever ways to get around this, but you know this is the fundamental problem, yeah?",
            "If you have 100 people trying to get that door at once, it takes a long time.",
            "So."
        ],
        [
            "And the other point is, these are controlled by a single operating system that a single and multi.",
            "That's the basic difference between the distributed memory computer, shared memory, computer a shared memory computer.",
            "You have one copy of the operating system, be it Mac or Windows or Linux, which sees the whole sees everything.",
            "So it can decide whether to run thread here or thread hero process here or process here.",
            "OK, it's the operating system which schedules the threads or the processes onto these cores, and it might move them around until we're done.",
            "Wonderful things and people always ask.",
            "Oh, where is my thread running?",
            "OK, well.",
            "I don't know.",
            "I mean I'm running PowerPoint, I'm running Firefox.",
            "I'm running a Mail client.",
            "I don't know which core they're running on.",
            "Why would I care?",
            "Well, unfortunately Headphones Committee we do care, but you're at the mercy of the operating system here.",
            "You basically tell the operating system.",
            "Please run 10 threads.",
            "Please run five processes and hopefully if it's clever it will say I'll put the processes on different physical cores to try and balance the load, but OK.",
            "Depends how.",
            "I mean, yeah.",
            "That is not necessarily.",
            "You can't guarantee the operating system is going to do what you want it to do, particularly as you found on the web and download it for free.",
            "You know 'cause it was lyrics.",
            "I mean, you know you get what you pay for, but."
        ],
        [
            "Being slightly disingenuous there.",
            "So the way that shared memory works is, as you user, you say."
        ],
        [
            "I want to run lots of threads please.",
            "So how many figure 812345678 and the operating systems is fine.",
            "I'll create 8 threads for you and this is actually the same with processes.",
            "It's no different and it might."
        ],
        [
            "Come on to these core."
        ],
        [
            "Course this is a 1234567 ten 1112 core machine, but then it might decide to."
        ],
        [
            "Move them, it might."
        ],
        [
            "Say oh, OK, well I'd rather move that thread over there so it might move it across.",
            "So you know, threads can threads and processes can migrate round and you don't really have any control.",
            "While you can give hints the operating system and will see in the exercises how to do that, at least on Intel processors, but you know you.",
            "I remember on a general purpose computer your your operating system is running hundreds of processes, possibly thousands of threads, so it's the shared things and moving them around all the time.",
            "OK, what we try and do in HPC is we would make sure that we only around 12 threads here and hope that the operating system stuck them on these cores and then then just left them there OK.",
            "But then maybe an email comes in.",
            "You know things have to run somewhere.",
            "One of your threads would be D schedules and such like, so that's why.",
            "High performance computers have compute nodes which you don't log onto there much more specialized.",
            "They do, you know that you just interact with a batch system, just try and get rid of all this.",
            "This this this kind of garbage you have to run your desktop.",
            "So."
        ],
        [
            "Threads existed before parallel.",
            "Computers there designed for concurrency right now to make it look like things were running at the same time, but actually it was just running them quickly and scheduling them, you know.",
            "And so it looked like 4 things were running at once, but it wasn't.",
            "It was just running in order and so many more threads running them physical cores.",
            "And they scheduled the scheduled as and when needed, and I think it's you know, I don't know how.",
            "Typically the operating system will look at things every few milliseconds, 10 milliseconds, maybe.",
            "You know, give you 10 milliseconds to run, and then maybe share your lip.",
            "But for parallel computing, one single thread per core, because we're interested in performance, you can run multiple threads.",
            "You could run 100 threads on your laptop, which is a very good debugging exercise, but you won't get speedup beyond the physical cores we want them to run at the same time we want to run all the time, all to run all the time, so optimizations, typically.",
            "Operating systems which are on high performance computing are optimized place threads on selected course, stop them from migrating.",
            "You know all this stuff you you cut the operating system down that that's various optimizations we can play around with."
        ],
        [
            "So practicalities.",
            "Threading can only operate within a single node.",
            "As I said, for two reasons a all the cores have to physically be attached to the same memory and be they have to be within the same operating system, 'cause it's only the operating system that can control them.",
            "Each node is a shared memory computer, so on bridges you can't run an open MP program on more than 28 cores because the controller single operating system.",
            "Yep.",
            "Oh sorry, well I said did I say yeah, the yeah there may be other standard nodes.",
            "OK, fine, so there are.",
            "There are there are nodes which have lots and lots of cores, but I'm on the standard ones.",
            "28 quarts here.",
            "Simple parallelization.",
            "You could speed up a serial program using threads.",
            "Actually this is.",
            "Yeah, so I mean what will do will do will do this you can.",
            "You can combine open MP with message passing.",
            "I'll come back to that so you know you could decide you could run one process per node and 28 threads per process.",
            "That would be an open MP program, but you could also run two processors per node and 40 threads per process.",
            "Use up on the course.",
            "This is hybrid MPI Open MP where you use up all the physical cores by a combination of so many processes.",
            "Which sport and so many threads?",
            "So as long as they multiply up to the total number of cores, then you're OK.",
            "But what we'll do is we'll just looking at speeding up the serial program using threads."
        ],
        [
            "So in summary, this shared blackboard is a good analogy for thread parallelism.",
            "It requires a shared memory architecture, so in high performance computing terms, that means you can't scale threads beyond a single node.",
            "So we have threads operating independently on the shared data.",
            "We also have private data for local variables, but we need to ensure they don't interfere, so synchronization is crucial.",
            "So whenever threads modify or or write to or modify shared variables or elements of shared arrays, you have to think to yourself is this.",
            "Is this legitimate?",
            "That is just what I want to do.",
            "Again, open MP kind of insulates you from that because it does a lot of this automatically, but you have to understand that there is an issue in principle.",
            "I'm threading.",
            "HPC usually uses Open MP directors, so.",
            "Open MP.",
            "Is requires compiler support, so and you put in magic lines into your code, it could support common parallel patterns such as reductions, so you can see that even to do something very simple which is to paralyze.",
            "Summing up in an array."
        ],
        [
            "Looks quite complicated.",
            "I have to do all this stuff OK, which makes my serial program look like ruins my serial program.",
            "Well, open MP recognizes that you want to do this.",
            "Everybody wants to do this so there are.",
            "There are simple constructs and open MP to do this for you.",
            "And so.",
            "Yeah, loop limits computed by the compiler or something varies across threads done automatically.",
            "So what I'll do actually I've got.",
            "Are there any questions?",
            "I don't have a nice quiz like it did yesterday.",
            "I'm afraid I don't have any questions.",
            "Yeah.",
            "So I know that there are a few different kinds of thread libraries available, like we're talking about open MP right now, but I remember reading that some other stuff I use gives you the option to use OS threads or another type of very lightweight threads that they've got embedded into their own runtime.",
            "Yeah, do the do all threads?",
            "Can all threads be moved around by the OS like that, or is it just conventional OS threads that are scheduled in descheduled?",
            "By the OS.",
            "I don't know people write their own thread libraries, maybe do their own scheduling.",
            "I don't know.",
            "I mean, that's that would be one of the advantages, you know well what the advantages and disadvantages that you have to do your own scheduling, but then you can do what you want.",
            "So I would guess that you could exploit that.",
            "I don't know this guy sitting next you have written there and I don't know.",
            "I mean, I presume that's something you can you can.",
            "You can control, I don't know.",
            "So in fact you have the same mechanism at the pit relatives to manage all the threads.",
            "But all of these lightweight threads are executed on map and scheduled on the user space.",
            "So the operating system is not aware about what is happening.",
            "So I thought I got a fight.",
            "What I would do is I would do the first part so you should be able to.",
            "You'll see in the instructions it's just on the instruction sheet I gave yesterday and actually in the pie example which you downloaded yesterday for it.",
            "Open MP for MPI you will see that there is an open MP directory.",
            "So what I'll show you is, I'll do the C1 first.",
            "Actually, let's go back to the list of the Serial 1 first.",
            "So if I this is the serial program.",
            "And it's very simple, it's just computing.",
            "It's just computing \u03c0.",
            "In this very simple way.",
            "An is 840.",
            "It just loops over.",
            "And then does this this this expansion and then prints the answer.",
            "OK so not a big deal and if I if I compile it.",
            "And run it.",
            "I get the right answer, so you'll see I get 3.141592 with a very small percentage error.",
            "If I go to the open MP code and I set up, I'm jumping ahead here, but I thought this would be useful.",
            "The way it works in open MP.",
            "Let's see if I can get this on.",
            "Is that basic groups?",
            "So what I've done here?",
            "Is that?",
            "I'm doing some print Hello World's so you have functions in open MP to get your OMP get thread NUM which is a light which is like MPI Comm rank and OMP get NUM threads isn't.",
            "This tells you what thread number you are and how many threads are running and then in open MP.",
            "The fundamental construct is a parallel region OMP parallel so.",
            "At this stage the parent processes is run at the parent prices.",
            "Gotta Fork join model so you have a single process running and at this stage here it actually creates multiple threads, so multiple threads are created here, so I'd expect to get one print here, but lots of prints here, but the real crucial one that's just a bit of print stuff.",
            "Open MP works by directives, so I have the same code here.",
            "This is the same code as the serial code, but hash pragma OMP is a directive saying this is a special construct which I want you to interpret, and so an open MP compiler will interpret that and an open MP compiler will just ignore it.",
            "So if you're lucky, you can write a parallel code, which is both, which is a valid serial code and will run in parallel.",
            "And I'll cover these in detail after while you're saying this here, I want to do a parallel loop.",
            "This is a four I want to parallelize that and by default it will split the loop space up in some way and it turns out to be the obvious way to split it up evenly.",
            "Default none says I want to scope everything.",
            "OK so that there are some default rules for whether variables are private or shared, but you shouldn't.",
            "You shouldn't use them because if you default and then it will.",
            "It will force you to declare the scope of every variable, and the scoping is only valid for that for the extent of the parallel region, which is here.",
            "So I said Priyesh private because I is the loop variable.",
            "By definition we're splitting the loop up the I variable.",
            "We different on different threads.",
            "And the upper limit is shared to everyone to read the same value of N an reduction pie.",
            "This says this is.",
            "This is basically a special clause saying I'm going to be adding this number up, so I want you to that special thing that I want everyone to add up locally.",
            "At the end I want them to add up together so this list looks like magic and it works and the code goes faster.",
            "But you need to understand this isn't magic.",
            "Need to understand what's going on, but it's actually doing So what I'll do after the break is I'll go through how you would do this naively to show you what's actually happening, but this is the kind of thing you will see that just looks like magic.",
            "And there's some open MP people courses actually.",
            "So just stick OMP parallel four in every loop and it will go faster and well, no no.",
            "That that that madness that their their madness lies.",
            "You have to understand what you're doing.",
            "So if I run this.",
            "And I run it.",
            "I run it just normally.",
            "I just run a serial program, but at runtime I actually.",
            "So the way you control the number of threads is through an environment variable.",
            "So you have to set this environment variable OMP, NUM threads.",
            "It was previously set to a big number, but if I do that and then I run it.",
            "You can see what's happened.",
            "The first print was in a serial region, so I only got one of them.",
            "Hello from Thread, Zero out of one through the serial region there is only one thread running and it's red Zero.",
            "OK, so this might be confusing.",
            "When you print how many threads are running, it tells you not how many threads could there be running.",
            "It tells you how many threads are running at that particular point in time, so outside the parallel region it says one.",
            "So the parent process is actually thread zero.",
            "Thread zero is special, it's actually the parent process.",
            "The original process within a parallel region.",
            "I get hello from Thread Zero to four and they get the right answer here, but I'll go through in detail how this works, but unlike open it, in MPI all the processes are running all the time in open MP.",
            "The threads come and go in these parallel regions.",
            "I'll just show you the Fortran one.",
            "Fortran is exactly the same except.",
            "Except the.",
            "Umm?",
            "You control the the Sentinel.",
            "It's called!",
            "Dollar OMP.",
            "So if I if I'm a normal Fortran compiler, what does that look like to me?",
            "Just a Fortran compiler, what's that?",
            "It's a comment, yeah, but if you turn on open MP it's a special comment.",
            "It understands it, so that's in Fortran.",
            "So hash pragma in C is an instruction to the compiler that it's allowed to ignore if it doesn't understand it.",
            "In Fortran, that doesn't exist or what you do you have any special comments?",
            "So these are comments, but a good open MP compiler will interpret them.",
            "So in fact the syntax is almost identical.",
            "And again, yeah, so since I spelt approximation wrong anyway, there you go so and I'll cover this this afternoon.",
            "So that was all I had to do in that were at lunchtime.",
            "So any questions just briefly.",
            "Yep.",
            "I noticed in your C code is there any?",
            "Is there any pragma to end the open MP with?",
            "Yeah, so that so Fortran is nicer than.",
            "Well, you can argue it with weight in Fortran, because Fortran doesn't have braces.",
            "You can't.",
            "There's no concept of a code block in Fortran.",
            "You have to explicitly say parallel, end, parallel, parallel, do an end parallel to.",
            "So that's quite nice.",
            "In C that in C. It applies to the.",
            "The next structured block to this directive applies to that block, so I've had to put in here.",
            "But the parallel four apply.",
            "Well, parallel four is different supplies next 4 loop, but in a special case.",
            "In general you have to brace.",
            "It applies.",
            "I didn't need it here, but just for explicitness and open, MP directive applies to the following structured block.",
            "And if that's more than one line, you need to put braces and that means you have more braces and then you'll have a philosophical argument yourself whether you dead in them or not.",
            "So yeah, that's kind of that.",
            "That kind of worries, but this is still a valid serial code, because this is still although it's crazy to do this totally, this is valid C, You know, to designate this structure the block.",
            "Yeah, that's a good question.",
            "OK, so we'll see you again.",
            "I think it's 1:30 yeah, OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I was my plan was.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do the Open MP overview and the walk through the Pi example, I don't think I'll have time to do this, I think, so I'll probably do the open MP overview now and then do this.",
                    "label": 0
                },
                {
                    "sent": "Do this after lunch is just a small shift.",
                    "label": 0
                },
                {
                    "sent": "I think I'd rather spend more time on this on this tour, so I'm just going to give a brief.",
                    "label": 0
                },
                {
                    "sent": "Overview Of open MP.",
                    "label": 0
                },
                {
                    "sent": "Initially and then we'll get onto the details this afternoon, hopefully even if you've used open MP before, like yesterday with the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the MPI you know it's not as simple as well.",
                    "label": 0
                },
                {
                    "sent": "The problem about whoops.",
                    "label": 0
                },
                {
                    "sent": "The problem with my open MP is that if you see a working open MP code, it looks trivial, but it's a lot in some senses it's more complicated than you think, and in some sense it's it's a lot simpler that the problem is that open MP requires the compiler and open MP.",
                    "label": 0
                },
                {
                    "sent": "The compiler does stuff stuff for you, so it looks like magic, but it isn't magic.",
                    "label": 0
                },
                {
                    "sent": "OK, compilers are pretty dumb really.",
                    "label": 0
                },
                {
                    "sent": "They do very mechanical things, so if you understand what open MP is actually doing, it really helps.",
                    "label": 0
                },
                {
                    "sent": "It is not magic, OK?",
                    "label": 0
                },
                {
                    "sent": "Compilers a piece of software written by people.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no reason to, you know, so you know they're not magic things.",
                    "label": 0
                },
                {
                    "sent": "People sometimes think that compilers are magical, mystical, pitner.",
                    "label": 0
                },
                {
                    "sent": "Their programs were written by people, some bit, some are good, some it's my bad.",
                    "label": 0
                },
                {
                    "sent": "Compiling program is very difficult.",
                    "label": 0
                },
                {
                    "sent": "You're asking somebody to write a program which is a compiler which is supposed to compile any random piece of text generated by any of the 7 billion people on the planet, and do a good job.",
                    "label": 0
                },
                {
                    "sent": "It's clearly impossible task, so you know.",
                    "label": 0
                },
                {
                    "sent": "Compilers tend to be quite so, but the problem with open MP is that the compiler does it for you, and it can look like magic, but it is not magic.",
                    "label": 0
                },
                {
                    "sent": "And people who think it's magic End up writing wrong programs.",
                    "label": 0
                },
                {
                    "sent": "Incorrect programs, which is your dealer simulation, and you probably took incorrect paper then, hey, how if you're writing a piece of software to fly a plane or something, you write an incorrect program, then that's probably a bit more important, and I gotta go through the concept here and they are quite important, so I'll talk about shared variables parallelism, which which uses threads, not processes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying that what I'm going to do is I'm going to discuss the basic just before lunch.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss the basic concepts of open MP and threads and then I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll do some programming.",
                    "label": 0
                },
                {
                    "sent": "I'll do.",
                    "label": 0
                },
                {
                    "sent": "This specific example had planned to do before lunch or now do it now.",
                    "label": 0
                },
                {
                    "sent": "Do it after lunch, so shared variables and open MP and threads are all intricately connected.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "So shared variables is parallel programming using threads.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll talk about what shared variables parallelism is, what threads are, and how they fit to shared memory architectures.",
                    "label": 0
                },
                {
                    "sent": "Threads are not processes, MPI message passing.",
                    "label": 0
                },
                {
                    "sent": "Programming uses multiple processes.",
                    "label": 0
                },
                {
                    "sent": "People often say threads.",
                    "label": 0
                },
                {
                    "sent": "But they.",
                    "label": 0
                },
                {
                    "sent": "Excuse me but they shouldn't.",
                    "label": 0
                },
                {
                    "sent": "Threads and processes are not the same thing.",
                    "label": 0
                },
                {
                    "sent": "And then we'll talk about practicalities.",
                    "label": 0
                },
                {
                    "sent": "How operating systems fit in and how you use these things on real machines and that will be so.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add variables threads based parallelism.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So many applications contain a single each process as a single thread.",
                    "label": 0
                },
                {
                    "sent": "When you write normal program and you compile it and you run it, you have one thread.",
                    "label": 0
                },
                {
                    "sent": "So you just you just something that it just runs to the end.",
                    "label": 0
                },
                {
                    "sent": "But a single process can actually contain multiple threads.",
                    "label": 0
                },
                {
                    "sent": "Threads are like mini programs, so after execution your program runs and it can create multiple threads which are like mini programs within the parent process.",
                    "label": 0
                },
                {
                    "sent": "OK, and each chat thread is like is like a child process contained within a parent process.",
                    "label": 0
                },
                {
                    "sent": "It's not quite the same, but but threads belong.",
                    "label": 0
                },
                {
                    "sent": "Book have a parent process.",
                    "label": 0
                },
                {
                    "sent": "You launch a single process and at runtime you say right?",
                    "label": 0
                },
                {
                    "sent": "I would now like to create multiple threads and they become like running multiple programs.",
                    "label": 0
                },
                {
                    "sent": "But there is a hierarchy and threads weren't in like everything.",
                    "label": 0
                },
                {
                    "sent": "Threads weren't invented for HPC.",
                    "label": 0
                },
                {
                    "sent": "If I wrong wrong machine threads were invented.",
                    "label": 0
                },
                {
                    "sent": "For doing things like you know.",
                    "label": 0
                },
                {
                    "sent": "Web browsers have a Webdriver web.",
                    "label": 0
                },
                {
                    "sent": "You have a web.",
                    "label": 0
                },
                {
                    "sent": "I have a web browser which is a process.",
                    "label": 0
                },
                {
                    "sent": "When I run it, I run Firefox.",
                    "label": 0
                },
                {
                    "sent": "It's a single process, but at runtime Firefox says well, I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got this little ticker tape down here, that's like that would be a separate thread.",
                    "label": 0
                },
                {
                    "sent": "And this movie here will be a separate thread to run the movie player and then and then maybe a thread monitoring for mouse clicks.",
                    "label": 0
                },
                {
                    "sent": "So this process has lots of threads doing different things with threads were invented so you could do lots of different things at the same time.",
                    "label": 0
                },
                {
                    "sent": "And the reason that they belong to the parent processes.",
                    "label": 0
                },
                {
                    "sent": "All these threads need to write to the screen they need.",
                    "label": 0
                },
                {
                    "sent": "They need to share the memory with apparent processor.",
                    "label": 0
                },
                {
                    "sent": "Two processes cannot share memory, but threads within a parent process all have access to the same memory, and it was designed to allow you to have little things you know, monitor mouse clicks and updating this banner and doing all kinds of stuff.",
                    "label": 0
                },
                {
                    "sent": "But as usual, we're going to hijack them to try and do parallel programming.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The shared made concept well so this is threads can all see the date of the parent process.",
                    "label": 1
                },
                {
                    "sent": "That is the absolutely key thing or every thread has access to all the data of the parent process.",
                    "label": 0
                },
                {
                    "sent": "And so as John described on the first day, it's like having a large map of the US in an office and you've got four people all in the same office and they say look, you look at the top left hand corner of the map.",
                    "label": 0
                },
                {
                    "sent": "You look at the top right hand corner.",
                    "label": 0
                },
                {
                    "sent": "You may decide that the access different areas of this shared memory, but they in principle have access to all of it.",
                    "label": 0
                },
                {
                    "sent": "The threads can run on different cores, and that's how you get speedup, and we'll see how that works later on, But again.",
                    "label": 0
                },
                {
                    "sent": "Threads on in threads aren't introduced into things like web browsers for performances.",
                    "label": 0
                },
                {
                    "sent": "It's there for freeze of programming, but when you have multiple core machines that then the threads can run on different cores to substantial parallel speedup.",
                    "label": 1
                },
                {
                    "sent": "So for example, if I thought I had the traffic model with the eight cells 01234567, I could decide that this thread operates on the 1st four and this thread operates on the second form, so that's different from MPI and MPI.",
                    "label": 0
                },
                {
                    "sent": "You would say physically.",
                    "label": 0
                },
                {
                    "sent": "These fourth, these four cells are stored on the 1st process and these four cells are stored on the second process.",
                    "label": 0
                },
                {
                    "sent": "Not true here.",
                    "label": 0
                },
                {
                    "sent": "They're all stored in the same memory on the same process.",
                    "label": 0
                },
                {
                    "sent": "You've just decided that one thread might like to do the 1st for another might do the next floor.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the analogy, and it's a very good analogy, is one large white board in the two person office, which is the shared memory, and two people are working on the same problem.",
                    "label": 1
                },
                {
                    "sent": "That's like 2 threads running on different cores attached to the same memory.",
                    "label": 1
                },
                {
                    "sent": "So again, if you're working with your office mate, you've got a big white board.",
                    "label": 0
                },
                {
                    "sent": "It's great, you can just write stuff wherever you want.",
                    "label": 0
                },
                {
                    "sent": "You can all access the data.",
                    "label": 0
                },
                {
                    "sent": "So, but the problem is.",
                    "label": 0
                },
                {
                    "sent": "You don't want people to interfere with each other.",
                    "label": 0
                },
                {
                    "sent": "OK, that's actually the problem.",
                    "label": 0
                },
                {
                    "sent": "In shared variables, Open MP programming is that it's kind of anarchy.",
                    "label": 0
                },
                {
                    "sent": "Any thread can access any of the memory, which sounds great, but what happens if somebody up you're working on something or someone comes and rubs it out?",
                    "label": 0
                },
                {
                    "sent": "OK, that's a problem, so the way it works, the way that open MP solves it says, well, there is shared data, but you need private data.",
                    "label": 0
                },
                {
                    "sent": "Clearly need private data to record your own stuff, and so the most important concept about open MP is that it has shared data, which is the default.",
                    "label": 0
                },
                {
                    "sent": "But you can have private data and of course all that you've done is said.",
                    "label": 0
                },
                {
                    "sent": "This bottom left hand corner is mine and this bottom right hand corner is yours.",
                    "label": 0
                },
                {
                    "sent": "OK, but but conceptually, MPI open MP distinguish being shared data and private data.",
                    "label": 0
                },
                {
                    "sent": "There is no such thing.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's just memory, but you know you can carve it up like this.",
                    "label": 0
                },
                {
                    "sent": "So we do need private data, so that's one of those things that you don't have this choice in.",
                    "label": 0
                },
                {
                    "sent": "At MPI and MPI, all data is private.",
                    "label": 0
                },
                {
                    "sent": "All data is private MPI.",
                    "label": 0
                },
                {
                    "sent": "No other process can access your data in open MP threads can access the same data, so you have to decide.",
                    "label": 0
                },
                {
                    "sent": "Is it shared or is it private?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is sort of a diagram was saying that we have some shared data.",
                    "label": 0
                },
                {
                    "sent": "We have saved three threads and they can all access the shared data.",
                    "label": 1
                },
                {
                    "sent": "So this red thread can access all this data, but they have their own private data here, but they also have their own PC, which means program counter.",
                    "label": 0
                },
                {
                    "sent": "What this is saying is that threads could be doing different things at different times.",
                    "label": 0
                },
                {
                    "sent": "One different threads can be operating different, running different functions, different subroutines at the same time.",
                    "label": 0
                },
                {
                    "sent": "And that's not quite what we do in open MP, but different threads can have their own private data and they also are operating on potentially different completely different instructions at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we saw this diagram yesterday for MPI.",
                    "label": 0
                },
                {
                    "sent": "What I said in MP wanted data.",
                    "label": 0
                },
                {
                    "sent": "What's that communicate data from process one to process too?",
                    "label": 0
                },
                {
                    "sent": "And that was that was done by message passing, which is like sending an email or making a phone call in open MP.",
                    "label": 0
                },
                {
                    "sent": "There are no messages.",
                    "label": 0
                },
                {
                    "sent": "You collaborate by reading and writing to shared memory.",
                    "label": 0
                },
                {
                    "sent": "If you've gone off for lunch and you want to tell back to your to your office mate in message passing, you send them an email and shared memory in open MP, you'd write on the board.",
                    "label": 0
                },
                {
                    "sent": "I have gone to the I have gone to lunch and when they come in they can read that and it's there.",
                    "label": 0
                },
                {
                    "sent": "So we want to communicate a value from thread one to thread two.",
                    "label": 0
                },
                {
                    "sent": "So thread one says my equals.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "23 and then you immediately after wait a second, you said that data could be shared or private.",
                    "label": 0
                },
                {
                    "sent": "OK, what is this one?",
                    "label": 0
                },
                {
                    "sent": "But my I've tried to indicate that A is a private variable, but the way it works in open MP is when you declare a well you mark variables are being a shared or private.",
                    "label": 0
                },
                {
                    "sent": "So when you see something like my equals 23 in an open MP program, you have to go and look up and say it's my a shared or private variable.",
                    "label": 0
                },
                {
                    "sent": "But here my a supposed to is supposed to indicate it's private.",
                    "label": 0
                },
                {
                    "sent": "OK so my equals 23.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My private data has my equals 23 then to write.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The shared data.",
                    "label": 0
                },
                {
                    "sent": "All I say is equals my a, so as long as a has been declared to be a shared variable, which is the default.",
                    "label": 0
                },
                {
                    "sent": "What happens is that that is just a copy.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens is your private copy of my A is copied from the shared the private region to the shared region.",
                    "label": 0
                },
                {
                    "sent": "So that's like doing a little calculation in your notebook and say, OK, well I've got the answer.",
                    "label": 0
                },
                {
                    "sent": "Now I want my officemate to know, so you write it on the on the shared on the shared space.",
                    "label": 0
                },
                {
                    "sent": "Now now your office mate might want to read that and all they do to read, as they say my.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equals 8 + 1, so they have available may, but it's private so they have a different.",
                    "label": 0
                },
                {
                    "sent": "They're both called my A, but they are separate variables because their private, so everyone has their independent copy of a private variable so they can have different values.",
                    "label": 0
                },
                {
                    "sent": "So this does two things.",
                    "label": 0
                },
                {
                    "sent": "This illustrates two things a it shows that the data is copied from the shared space, the private space because a is a private variable.",
                    "label": 0
                },
                {
                    "sent": "My is a private variable and A is a shared variable.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Secondly, it shows that the two my eyes can have different values and what we'll see if we use this for loops when you're doing a loop, you might say there's a big data set.",
                    "label": 0
                },
                {
                    "sent": "I want thread zero to two items, one to 10 thread, one todo items, 11 to 20 to break up the loop there you need to have a private variable, the loop counter.",
                    "label": 0
                },
                {
                    "sent": "I need to be private, 'cause by definition each thread is on a different iteration, a different time.",
                    "label": 0
                },
                {
                    "sent": "However, there's something I haven't that this isn't guaranteed to work.",
                    "label": 0
                },
                {
                    "sent": "There's something I've assumed here, which I haven't made explicit.",
                    "label": 0
                },
                {
                    "sent": "So if there's something I'd went through it, but you know, there's an assumption here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I assume that this statement eight was my A was executed before this statement my it was a plus one and there's nothing in this program to mandate that.",
                    "label": 0
                },
                {
                    "sent": "So now this isn't a problem in message passing because when you pass a message you you send it and then if thread two is a bit slow, it waits a live thread.",
                    "label": 0
                },
                {
                    "sent": "One is a bit fast, it's waiting for them to come in.",
                    "label": 0
                },
                {
                    "sent": "But in Beacon threaded program because you can read and write the same membrane.",
                    "label": 0
                },
                {
                    "sent": "Mutual synchronization is a big issue, so this code is not guaranteed to work.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the kind of thing people write this kind of code.",
                    "label": 0
                },
                {
                    "sent": "It works 99 times out of 100 and then 100 times it doesn't work and you get the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "So so it's really it's very easy to write incorrect shared memory, open MP programs.",
                    "label": 0
                },
                {
                    "sent": "And this is the reason so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Synchronization is crucial for the shared variables approach.",
                    "label": 1
                },
                {
                    "sent": "Thread two code must execute after thread one another, but the most commonly thing to do as a global barrier.",
                    "label": 1
                },
                {
                    "sent": "So as I said, you never almost never said you almost never need global barriers in MPI for correctness, but you need them all the time and shared variables or some synchronization.",
                    "label": 0
                },
                {
                    "sent": "You need to say look, I want you guys if you're going to read data from me, you have to execute your instructions after I've executed mine.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do a barrier.",
                    "label": 0
                },
                {
                    "sent": "So what you say is.",
                    "label": 0
                },
                {
                    "sent": "You would do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My equals 23.",
                    "label": 0
                },
                {
                    "sent": "Eight was my a barrier and thread two would have barrier my equals a + 1.",
                    "label": 0
                },
                {
                    "sent": "So you draw a line across here that says that none of these instructions can happen after this instruction.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's a common Now you won't see them often open MP, but open MP puts them in by default, so open MP puts.",
                    "label": 0
                },
                {
                    "sent": "It may not be obvious, but open MP puts barriers in lots of places.",
                    "label": 0
                },
                {
                    "sent": "To try and help you with correctness.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so writing parallel, so most common global basic message.",
                    "label": 0
                },
                {
                    "sent": "Other mechanisms exist such as locks.",
                    "label": 0
                },
                {
                    "sent": "So you can have.",
                    "label": 0
                },
                {
                    "sent": "This is the usual thing.",
                    "label": 0
                },
                {
                    "sent": "You can lock variables and you can, you can say nobody can access this variable until I'm finished with it.",
                    "label": 0
                },
                {
                    "sent": "But barrier is the most common one.",
                    "label": 0
                },
                {
                    "sent": "Writing parallel code is relatively straightforward 'cause you can access the shared data as and when it's needed.",
                    "label": 1
                },
                {
                    "sent": "It's really quite nice.",
                    "label": 0
                },
                {
                    "sent": "You can just, you know.",
                    "label": 0
                },
                {
                    "sent": "If you have a shared big shared array like you're the road, or that the map that you're working on any thread can access any value anytime it wants, so that that makes parallel programming easier.",
                    "label": 0
                },
                {
                    "sent": "But it's very easy to write incorrect code.",
                    "label": 1
                },
                {
                    "sent": "Getting correct code can be difficult.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a specific example.",
                    "label": 1
                },
                {
                    "sent": "I've got an array a which has got 888 elements.",
                    "label": 0
                },
                {
                    "sent": "I want to compute the sum and we saw in the pie example yesterday which I did in MPI.",
                    "label": 0
                },
                {
                    "sent": "How to do this?",
                    "label": 0
                },
                {
                    "sent": "And conceptually, it's quite similar in open MP, except that you have to really well at least in threaded rod, you have to realize that although I've split, I'm said that that thread zero is operating on A0A1A2 and A3 and thread one is operating on a 4A5A6 and A7 K the array is not split.",
                    "label": 0
                },
                {
                    "sent": "The array is just somewhere on the whiteboard in the shared space.",
                    "label": 0
                },
                {
                    "sent": "We've just decided that this particular loop they will iterate over these spaces, but the array is not distributed between the threads.",
                    "label": 0
                },
                {
                    "sent": "This is a shared array, just sits in the space, but we just decided that will work on different parts of it.",
                    "label": 0
                },
                {
                    "sent": "So if I want to compute this summation, the usual classic observation is summation is associative, so thread zero consume the 1st four elements.",
                    "label": 0
                },
                {
                    "sent": "I've called them thread one and two have no threads.",
                    "label": 0
                },
                {
                    "sent": "Fraser Construction on the 1st four elements thread, one consumed the next four elements, they can add them together.",
                    "label": 0
                },
                {
                    "sent": "So if I have a loop, every code, this is looking very likely that the MPI code every code would look for Mike as I start to I stop.",
                    "label": 0
                },
                {
                    "sent": "They didn't increment there.",
                    "label": 0
                },
                {
                    "sent": "There my a sum and they then the loop.",
                    "label": 0
                },
                {
                    "sent": "So you have to always asking in a shared variables open MP program what?",
                    "label": 0
                },
                {
                    "sent": "What is the classification of these variables?",
                    "label": 0
                },
                {
                    "sent": "Are they shared or private?",
                    "label": 0
                },
                {
                    "sent": "Well clearly or a is shared a is that big arrays are always share the thing you're working on.",
                    "label": 0
                },
                {
                    "sent": "OK, you can't duplicate bigger Asian run out of memory to start with, but I mean you know conceptually you have a big large array operating on.",
                    "label": 0
                },
                {
                    "sent": "It just sits there in the shared space, OK?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The result is shared 'cause there is only one result you want to come up with a single result and you wanted to be able to everybody, but to compute that you need some private variables, you need the loop counter loop counters are always private.",
                    "label": 0
                },
                {
                    "sent": "If you're splitting a loop up, by definition, different threads will have different values of high at the same time, so it makes no sense to have a shared loop counter.",
                    "label": 1
                },
                {
                    "sent": "We need to have loop limits.",
                    "label": 0
                },
                {
                    "sent": "Every thread is running the same piece of code, but they can have.",
                    "label": 0
                },
                {
                    "sent": "They can do different things by having different loop limits.",
                    "label": 0
                },
                {
                    "sent": "So you have a nice start and then I stop there private.",
                    "label": 0
                },
                {
                    "sent": "And the local some in my asum is also private.",
                    "label": 0
                },
                {
                    "sent": "'cause you're accumulating a local value and then you add them together.",
                    "label": 0
                },
                {
                    "sent": "So this looks quite easy, but the killer comes in the at the end.",
                    "label": 0
                },
                {
                    "sent": "How do you add these variables together too?",
                    "label": 0
                },
                {
                    "sent": "How do you add these variables together to get a common result?",
                    "label": 0
                },
                {
                    "sent": "So you might just say a sum plus equals myisam.",
                    "label": 0
                },
                {
                    "sent": "OK, you just say everybody adds their local value to the global value.",
                    "label": 0
                },
                {
                    "sent": "OK, that does not work and this is the thing which is which is the thing which catches most people in open MP that is not guaranteed to work OK.",
                    "label": 0
                },
                {
                    "sent": "The reason is there is no such thing as adding and adding a value to a variable.",
                    "label": 0
                },
                {
                    "sent": "OK, computers, very stupid.",
                    "label": 0
                },
                {
                    "sent": "They can do three things.",
                    "label": 0
                },
                {
                    "sent": "They can read data from memory, they can add numbers and they can write it back again.",
                    "label": 0
                },
                {
                    "sent": "It's basically all they can do.",
                    "label": 0
                },
                {
                    "sent": "So to add a number to a variable you have to read the variable into register.",
                    "label": 0
                },
                {
                    "sent": "You have to add to it and you have to write it back again.",
                    "label": 0
                },
                {
                    "sent": "OK and the problem is if you have two threads doing this at once, OK One thread reads the variable and adds to it and writes it back again at the same time.",
                    "label": 0
                },
                {
                    "sent": "Somebody else could have read the same variable.",
                    "label": 0
                },
                {
                    "sent": "And then you don't get the right answer, OK?",
                    "label": 0
                },
                {
                    "sent": "It's like saying I want everybody I want to compute the total age of.",
                    "label": 0
                },
                {
                    "sent": "I want to compute the sum of every stage in this room, and I'll write on this whiteboard.",
                    "label": 0
                },
                {
                    "sent": "K alright on this whiteboard a total I want you all your Azure your age that total.",
                    "label": 0
                },
                {
                    "sent": "The problem is you know you say over the total 53 I need to add my age so I take it away and you have to use a Calculator go 53 plus my age is 100 and something and then you go back but why you've been doing that?",
                    "label": 0
                },
                {
                    "sent": "Somebody else is done the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK so whenever you have multiple threads accessing shared variables or reading them, writing them at least you have this you have to worry about synchronization.",
                    "label": 0
                },
                {
                    "sent": "Now all these things are in principle worries because open MP is designed.",
                    "label": 0
                },
                {
                    "sent": "For scientific and technical programming, where these these are very commonplace and open MP has constructs which do all this for you, but it's essential to understand that that what it is doing it is not magic.",
                    "label": 0
                },
                {
                    "sent": "It's just doing stuff which is just fairly straightforward but but crucial.",
                    "label": 0
                },
                {
                    "sent": "So what I could do is I could say thread 0 does a sum plus equal to my a sum than a barrier and then do thread 118 plus equal to my son.",
                    "label": 0
                },
                {
                    "sent": "OK, you could do something like that that would be and there are other ways of doing it, but you have to be careful when you have multiple threads access.",
                    "label": 0
                },
                {
                    "sent": "Writing to share data or modifying it that you control any race conditions.",
                    "label": 0
                },
                {
                    "sent": "Anything that could clash on each other.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we use this as a specific example after after lunch.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hardware, I'm saying you've got multiple people in an office sharing the same whiteboard.",
                    "label": 0
                },
                {
                    "sent": "What we want is multiple threads, or at least multiple physical processors or cores accessing the same memory.",
                    "label": 0
                },
                {
                    "sent": "You require a shared memory architecture like this laptop.",
                    "label": 0
                },
                {
                    "sent": "OK, this laptop is a shared memory architecture has 54 cores.",
                    "label": 0
                },
                {
                    "sent": "I've called them processors here they can all access the same memory and the way have is that they have a shared bus.",
                    "label": 1
                },
                {
                    "sent": "They basically they can all read and write the same memory because it's all mediated by some bus, so they can all read.",
                    "label": 0
                },
                {
                    "sent": "Write to the same memory.",
                    "label": 0
                },
                {
                    "sent": "You can immediately see this is a problem.",
                    "label": 0
                },
                {
                    "sent": "Because every read and write from all the processes has to go through this link.",
                    "label": 0
                },
                {
                    "sent": "OK, you might say, isn't that a bottleneck?",
                    "label": 0
                },
                {
                    "sent": "But it is OK.",
                    "label": 0
                },
                {
                    "sent": "So modern computers are incredibly bad at processing data.",
                    "label": 0
                },
                {
                    "sent": "The Act, the data speeds, the speeds here are a factor of 10 or 100 slower than the speeds of the processors so.",
                    "label": 0
                },
                {
                    "sent": "But there are ways that we try and get around that by.",
                    "label": 0
                },
                {
                    "sent": "Having caches so you when you read data you store it locally and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "But you know this is.",
                    "label": 0
                },
                {
                    "sent": "This is why it's very hard.",
                    "label": 0
                },
                {
                    "sent": "This is why you can build distributed memory computers with millions of processes.",
                    "label": 0
                },
                {
                    "sent": "You just buy a million laptops and stick them altogether.",
                    "label": 0
                },
                {
                    "sent": "But it's very hard to build shared memory architectures with more than maybe a few 10s or 100 cores because at least in the most naive approach like this, you get this button there.",
                    "label": 0
                },
                {
                    "sent": "Clearly there clever ways to get around this, but you know this is the fundamental problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "If you have 100 people trying to get that door at once, it takes a long time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other point is, these are controlled by a single operating system that a single and multi.",
                    "label": 1
                },
                {
                    "sent": "That's the basic difference between the distributed memory computer, shared memory, computer a shared memory computer.",
                    "label": 0
                },
                {
                    "sent": "You have one copy of the operating system, be it Mac or Windows or Linux, which sees the whole sees everything.",
                    "label": 0
                },
                {
                    "sent": "So it can decide whether to run thread here or thread hero process here or process here.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the operating system which schedules the threads or the processes onto these cores, and it might move them around until we're done.",
                    "label": 0
                },
                {
                    "sent": "Wonderful things and people always ask.",
                    "label": 0
                },
                {
                    "sent": "Oh, where is my thread running?",
                    "label": 0
                },
                {
                    "sent": "OK, well.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean I'm running PowerPoint, I'm running Firefox.",
                    "label": 0
                },
                {
                    "sent": "I'm running a Mail client.",
                    "label": 0
                },
                {
                    "sent": "I don't know which core they're running on.",
                    "label": 0
                },
                {
                    "sent": "Why would I care?",
                    "label": 0
                },
                {
                    "sent": "Well, unfortunately Headphones Committee we do care, but you're at the mercy of the operating system here.",
                    "label": 0
                },
                {
                    "sent": "You basically tell the operating system.",
                    "label": 0
                },
                {
                    "sent": "Please run 10 threads.",
                    "label": 0
                },
                {
                    "sent": "Please run five processes and hopefully if it's clever it will say I'll put the processes on different physical cores to try and balance the load, but OK.",
                    "label": 0
                },
                {
                    "sent": "Depends how.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah.",
                    "label": 0
                },
                {
                    "sent": "That is not necessarily.",
                    "label": 0
                },
                {
                    "sent": "You can't guarantee the operating system is going to do what you want it to do, particularly as you found on the web and download it for free.",
                    "label": 0
                },
                {
                    "sent": "You know 'cause it was lyrics.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know you get what you pay for, but.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Being slightly disingenuous there.",
                    "label": 0
                },
                {
                    "sent": "So the way that shared memory works is, as you user, you say.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to run lots of threads please.",
                    "label": 0
                },
                {
                    "sent": "So how many figure 812345678 and the operating systems is fine.",
                    "label": 0
                },
                {
                    "sent": "I'll create 8 threads for you and this is actually the same with processes.",
                    "label": 0
                },
                {
                    "sent": "It's no different and it might.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come on to these core.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Course this is a 1234567 ten 1112 core machine, but then it might decide to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move them, it might.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say oh, OK, well I'd rather move that thread over there so it might move it across.",
                    "label": 0
                },
                {
                    "sent": "So you know, threads can threads and processes can migrate round and you don't really have any control.",
                    "label": 0
                },
                {
                    "sent": "While you can give hints the operating system and will see in the exercises how to do that, at least on Intel processors, but you know you.",
                    "label": 0
                },
                {
                    "sent": "I remember on a general purpose computer your your operating system is running hundreds of processes, possibly thousands of threads, so it's the shared things and moving them around all the time.",
                    "label": 0
                },
                {
                    "sent": "OK, what we try and do in HPC is we would make sure that we only around 12 threads here and hope that the operating system stuck them on these cores and then then just left them there OK.",
                    "label": 0
                },
                {
                    "sent": "But then maybe an email comes in.",
                    "label": 0
                },
                {
                    "sent": "You know things have to run somewhere.",
                    "label": 0
                },
                {
                    "sent": "One of your threads would be D schedules and such like, so that's why.",
                    "label": 0
                },
                {
                    "sent": "High performance computers have compute nodes which you don't log onto there much more specialized.",
                    "label": 0
                },
                {
                    "sent": "They do, you know that you just interact with a batch system, just try and get rid of all this.",
                    "label": 0
                },
                {
                    "sent": "This this this kind of garbage you have to run your desktop.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Threads existed before parallel.",
                    "label": 0
                },
                {
                    "sent": "Computers there designed for concurrency right now to make it look like things were running at the same time, but actually it was just running them quickly and scheduling them, you know.",
                    "label": 0
                },
                {
                    "sent": "And so it looked like 4 things were running at once, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It was just running in order and so many more threads running them physical cores.",
                    "label": 1
                },
                {
                    "sent": "And they scheduled the scheduled as and when needed, and I think it's you know, I don't know how.",
                    "label": 0
                },
                {
                    "sent": "Typically the operating system will look at things every few milliseconds, 10 milliseconds, maybe.",
                    "label": 0
                },
                {
                    "sent": "You know, give you 10 milliseconds to run, and then maybe share your lip.",
                    "label": 0
                },
                {
                    "sent": "But for parallel computing, one single thread per core, because we're interested in performance, you can run multiple threads.",
                    "label": 1
                },
                {
                    "sent": "You could run 100 threads on your laptop, which is a very good debugging exercise, but you won't get speedup beyond the physical cores we want them to run at the same time we want to run all the time, all to run all the time, so optimizations, typically.",
                    "label": 0
                },
                {
                    "sent": "Operating systems which are on high performance computing are optimized place threads on selected course, stop them from migrating.",
                    "label": 1
                },
                {
                    "sent": "You know all this stuff you you cut the operating system down that that's various optimizations we can play around with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So practicalities.",
                    "label": 0
                },
                {
                    "sent": "Threading can only operate within a single node.",
                    "label": 1
                },
                {
                    "sent": "As I said, for two reasons a all the cores have to physically be attached to the same memory and be they have to be within the same operating system, 'cause it's only the operating system that can control them.",
                    "label": 1
                },
                {
                    "sent": "Each node is a shared memory computer, so on bridges you can't run an open MP program on more than 28 cores because the controller single operating system.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, well I said did I say yeah, the yeah there may be other standard nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, fine, so there are.",
                    "label": 0
                },
                {
                    "sent": "There are there are nodes which have lots and lots of cores, but I'm on the standard ones.",
                    "label": 0
                },
                {
                    "sent": "28 quarts here.",
                    "label": 1
                },
                {
                    "sent": "Simple parallelization.",
                    "label": 0
                },
                {
                    "sent": "You could speed up a serial program using threads.",
                    "label": 0
                },
                {
                    "sent": "Actually this is.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so I mean what will do will do will do this you can.",
                    "label": 0
                },
                {
                    "sent": "You can combine open MP with message passing.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that so you know you could decide you could run one process per node and 28 threads per process.",
                    "label": 0
                },
                {
                    "sent": "That would be an open MP program, but you could also run two processors per node and 40 threads per process.",
                    "label": 0
                },
                {
                    "sent": "Use up on the course.",
                    "label": 0
                },
                {
                    "sent": "This is hybrid MPI Open MP where you use up all the physical cores by a combination of so many processes.",
                    "label": 0
                },
                {
                    "sent": "Which sport and so many threads?",
                    "label": 0
                },
                {
                    "sent": "So as long as they multiply up to the total number of cores, then you're OK.",
                    "label": 0
                },
                {
                    "sent": "But what we'll do is we'll just looking at speeding up the serial program using threads.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, this shared blackboard is a good analogy for thread parallelism.",
                    "label": 1
                },
                {
                    "sent": "It requires a shared memory architecture, so in high performance computing terms, that means you can't scale threads beyond a single node.",
                    "label": 1
                },
                {
                    "sent": "So we have threads operating independently on the shared data.",
                    "label": 0
                },
                {
                    "sent": "We also have private data for local variables, but we need to ensure they don't interfere, so synchronization is crucial.",
                    "label": 1
                },
                {
                    "sent": "So whenever threads modify or or write to or modify shared variables or elements of shared arrays, you have to think to yourself is this.",
                    "label": 0
                },
                {
                    "sent": "Is this legitimate?",
                    "label": 0
                },
                {
                    "sent": "That is just what I want to do.",
                    "label": 1
                },
                {
                    "sent": "Again, open MP kind of insulates you from that because it does a lot of this automatically, but you have to understand that there is an issue in principle.",
                    "label": 0
                },
                {
                    "sent": "I'm threading.",
                    "label": 0
                },
                {
                    "sent": "HPC usually uses Open MP directors, so.",
                    "label": 0
                },
                {
                    "sent": "Open MP.",
                    "label": 0
                },
                {
                    "sent": "Is requires compiler support, so and you put in magic lines into your code, it could support common parallel patterns such as reductions, so you can see that even to do something very simple which is to paralyze.",
                    "label": 0
                },
                {
                    "sent": "Summing up in an array.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks quite complicated.",
                    "label": 0
                },
                {
                    "sent": "I have to do all this stuff OK, which makes my serial program look like ruins my serial program.",
                    "label": 0
                },
                {
                    "sent": "Well, open MP recognizes that you want to do this.",
                    "label": 0
                },
                {
                    "sent": "Everybody wants to do this so there are.",
                    "label": 0
                },
                {
                    "sent": "There are simple constructs and open MP to do this for you.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, loop limits computed by the compiler or something varies across threads done automatically.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do actually I've got.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "I don't have a nice quiz like it did yesterday.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid I don't have any questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I know that there are a few different kinds of thread libraries available, like we're talking about open MP right now, but I remember reading that some other stuff I use gives you the option to use OS threads or another type of very lightweight threads that they've got embedded into their own runtime.",
                    "label": 0
                },
                {
                    "sent": "Yeah, do the do all threads?",
                    "label": 0
                },
                {
                    "sent": "Can all threads be moved around by the OS like that, or is it just conventional OS threads that are scheduled in descheduled?",
                    "label": 0
                },
                {
                    "sent": "By the OS.",
                    "label": 0
                },
                {
                    "sent": "I don't know people write their own thread libraries, maybe do their own scheduling.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's that would be one of the advantages, you know well what the advantages and disadvantages that you have to do your own scheduling, but then you can do what you want.",
                    "label": 0
                },
                {
                    "sent": "So I would guess that you could exploit that.",
                    "label": 0
                },
                {
                    "sent": "I don't know this guy sitting next you have written there and I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, I presume that's something you can you can.",
                    "label": 0
                },
                {
                    "sent": "You can control, I don't know.",
                    "label": 0
                },
                {
                    "sent": "So in fact you have the same mechanism at the pit relatives to manage all the threads.",
                    "label": 0
                },
                {
                    "sent": "But all of these lightweight threads are executed on map and scheduled on the user space.",
                    "label": 0
                },
                {
                    "sent": "So the operating system is not aware about what is happening.",
                    "label": 0
                },
                {
                    "sent": "So I thought I got a fight.",
                    "label": 0
                },
                {
                    "sent": "What I would do is I would do the first part so you should be able to.",
                    "label": 0
                },
                {
                    "sent": "You'll see in the instructions it's just on the instruction sheet I gave yesterday and actually in the pie example which you downloaded yesterday for it.",
                    "label": 0
                },
                {
                    "sent": "Open MP for MPI you will see that there is an open MP directory.",
                    "label": 0
                },
                {
                    "sent": "So what I'll show you is, I'll do the C1 first.",
                    "label": 0
                },
                {
                    "sent": "Actually, let's go back to the list of the Serial 1 first.",
                    "label": 0
                },
                {
                    "sent": "So if I this is the serial program.",
                    "label": 0
                },
                {
                    "sent": "And it's very simple, it's just computing.",
                    "label": 0
                },
                {
                    "sent": "It's just computing \u03c0.",
                    "label": 0
                },
                {
                    "sent": "In this very simple way.",
                    "label": 0
                },
                {
                    "sent": "An is 840.",
                    "label": 0
                },
                {
                    "sent": "It just loops over.",
                    "label": 0
                },
                {
                    "sent": "And then does this this this expansion and then prints the answer.",
                    "label": 0
                },
                {
                    "sent": "OK so not a big deal and if I if I compile it.",
                    "label": 0
                },
                {
                    "sent": "And run it.",
                    "label": 0
                },
                {
                    "sent": "I get the right answer, so you'll see I get 3.141592 with a very small percentage error.",
                    "label": 0
                },
                {
                    "sent": "If I go to the open MP code and I set up, I'm jumping ahead here, but I thought this would be useful.",
                    "label": 0
                },
                {
                    "sent": "The way it works in open MP.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I can get this on.",
                    "label": 0
                },
                {
                    "sent": "Is that basic groups?",
                    "label": 0
                },
                {
                    "sent": "So what I've done here?",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "I'm doing some print Hello World's so you have functions in open MP to get your OMP get thread NUM which is a light which is like MPI Comm rank and OMP get NUM threads isn't.",
                    "label": 0
                },
                {
                    "sent": "This tells you what thread number you are and how many threads are running and then in open MP.",
                    "label": 0
                },
                {
                    "sent": "The fundamental construct is a parallel region OMP parallel so.",
                    "label": 0
                },
                {
                    "sent": "At this stage the parent processes is run at the parent prices.",
                    "label": 0
                },
                {
                    "sent": "Gotta Fork join model so you have a single process running and at this stage here it actually creates multiple threads, so multiple threads are created here, so I'd expect to get one print here, but lots of prints here, but the real crucial one that's just a bit of print stuff.",
                    "label": 0
                },
                {
                    "sent": "Open MP works by directives, so I have the same code here.",
                    "label": 0
                },
                {
                    "sent": "This is the same code as the serial code, but hash pragma OMP is a directive saying this is a special construct which I want you to interpret, and so an open MP compiler will interpret that and an open MP compiler will just ignore it.",
                    "label": 0
                },
                {
                    "sent": "So if you're lucky, you can write a parallel code, which is both, which is a valid serial code and will run in parallel.",
                    "label": 0
                },
                {
                    "sent": "And I'll cover these in detail after while you're saying this here, I want to do a parallel loop.",
                    "label": 0
                },
                {
                    "sent": "This is a four I want to parallelize that and by default it will split the loop space up in some way and it turns out to be the obvious way to split it up evenly.",
                    "label": 0
                },
                {
                    "sent": "Default none says I want to scope everything.",
                    "label": 0
                },
                {
                    "sent": "OK so that there are some default rules for whether variables are private or shared, but you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't use them because if you default and then it will.",
                    "label": 0
                },
                {
                    "sent": "It will force you to declare the scope of every variable, and the scoping is only valid for that for the extent of the parallel region, which is here.",
                    "label": 0
                },
                {
                    "sent": "So I said Priyesh private because I is the loop variable.",
                    "label": 0
                },
                {
                    "sent": "By definition we're splitting the loop up the I variable.",
                    "label": 0
                },
                {
                    "sent": "We different on different threads.",
                    "label": 0
                },
                {
                    "sent": "And the upper limit is shared to everyone to read the same value of N an reduction pie.",
                    "label": 0
                },
                {
                    "sent": "This says this is.",
                    "label": 0
                },
                {
                    "sent": "This is basically a special clause saying I'm going to be adding this number up, so I want you to that special thing that I want everyone to add up locally.",
                    "label": 0
                },
                {
                    "sent": "At the end I want them to add up together so this list looks like magic and it works and the code goes faster.",
                    "label": 0
                },
                {
                    "sent": "But you need to understand this isn't magic.",
                    "label": 0
                },
                {
                    "sent": "Need to understand what's going on, but it's actually doing So what I'll do after the break is I'll go through how you would do this naively to show you what's actually happening, but this is the kind of thing you will see that just looks like magic.",
                    "label": 0
                },
                {
                    "sent": "And there's some open MP people courses actually.",
                    "label": 0
                },
                {
                    "sent": "So just stick OMP parallel four in every loop and it will go faster and well, no no.",
                    "label": 0
                },
                {
                    "sent": "That that that madness that their their madness lies.",
                    "label": 0
                },
                {
                    "sent": "You have to understand what you're doing.",
                    "label": 0
                },
                {
                    "sent": "So if I run this.",
                    "label": 0
                },
                {
                    "sent": "And I run it.",
                    "label": 0
                },
                {
                    "sent": "I run it just normally.",
                    "label": 0
                },
                {
                    "sent": "I just run a serial program, but at runtime I actually.",
                    "label": 0
                },
                {
                    "sent": "So the way you control the number of threads is through an environment variable.",
                    "label": 0
                },
                {
                    "sent": "So you have to set this environment variable OMP, NUM threads.",
                    "label": 0
                },
                {
                    "sent": "It was previously set to a big number, but if I do that and then I run it.",
                    "label": 0
                },
                {
                    "sent": "You can see what's happened.",
                    "label": 0
                },
                {
                    "sent": "The first print was in a serial region, so I only got one of them.",
                    "label": 0
                },
                {
                    "sent": "Hello from Thread, Zero out of one through the serial region there is only one thread running and it's red Zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this might be confusing.",
                    "label": 0
                },
                {
                    "sent": "When you print how many threads are running, it tells you not how many threads could there be running.",
                    "label": 0
                },
                {
                    "sent": "It tells you how many threads are running at that particular point in time, so outside the parallel region it says one.",
                    "label": 0
                },
                {
                    "sent": "So the parent process is actually thread zero.",
                    "label": 0
                },
                {
                    "sent": "Thread zero is special, it's actually the parent process.",
                    "label": 0
                },
                {
                    "sent": "The original process within a parallel region.",
                    "label": 0
                },
                {
                    "sent": "I get hello from Thread Zero to four and they get the right answer here, but I'll go through in detail how this works, but unlike open it, in MPI all the processes are running all the time in open MP.",
                    "label": 0
                },
                {
                    "sent": "The threads come and go in these parallel regions.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you the Fortran one.",
                    "label": 0
                },
                {
                    "sent": "Fortran is exactly the same except.",
                    "label": 0
                },
                {
                    "sent": "Except the.",
                    "label": 0
                },
                {
                    "sent": "Umm?",
                    "label": 0
                },
                {
                    "sent": "You control the the Sentinel.",
                    "label": 0
                },
                {
                    "sent": "It's called!",
                    "label": 0
                },
                {
                    "sent": "Dollar OMP.",
                    "label": 0
                },
                {
                    "sent": "So if I if I'm a normal Fortran compiler, what does that look like to me?",
                    "label": 0
                },
                {
                    "sent": "Just a Fortran compiler, what's that?",
                    "label": 0
                },
                {
                    "sent": "It's a comment, yeah, but if you turn on open MP it's a special comment.",
                    "label": 0
                },
                {
                    "sent": "It understands it, so that's in Fortran.",
                    "label": 0
                },
                {
                    "sent": "So hash pragma in C is an instruction to the compiler that it's allowed to ignore if it doesn't understand it.",
                    "label": 0
                },
                {
                    "sent": "In Fortran, that doesn't exist or what you do you have any special comments?",
                    "label": 0
                },
                {
                    "sent": "So these are comments, but a good open MP compiler will interpret them.",
                    "label": 0
                },
                {
                    "sent": "So in fact the syntax is almost identical.",
                    "label": 0
                },
                {
                    "sent": "And again, yeah, so since I spelt approximation wrong anyway, there you go so and I'll cover this this afternoon.",
                    "label": 0
                },
                {
                    "sent": "So that was all I had to do in that were at lunchtime.",
                    "label": 0
                },
                {
                    "sent": "So any questions just briefly.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "I noticed in your C code is there any?",
                    "label": 0
                },
                {
                    "sent": "Is there any pragma to end the open MP with?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that so Fortran is nicer than.",
                    "label": 0
                },
                {
                    "sent": "Well, you can argue it with weight in Fortran, because Fortran doesn't have braces.",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "There's no concept of a code block in Fortran.",
                    "label": 0
                },
                {
                    "sent": "You have to explicitly say parallel, end, parallel, parallel, do an end parallel to.",
                    "label": 0
                },
                {
                    "sent": "So that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "In C that in C. It applies to the.",
                    "label": 0
                },
                {
                    "sent": "The next structured block to this directive applies to that block, so I've had to put in here.",
                    "label": 0
                },
                {
                    "sent": "But the parallel four apply.",
                    "label": 0
                },
                {
                    "sent": "Well, parallel four is different supplies next 4 loop, but in a special case.",
                    "label": 0
                },
                {
                    "sent": "In general you have to brace.",
                    "label": 0
                },
                {
                    "sent": "It applies.",
                    "label": 0
                },
                {
                    "sent": "I didn't need it here, but just for explicitness and open, MP directive applies to the following structured block.",
                    "label": 0
                },
                {
                    "sent": "And if that's more than one line, you need to put braces and that means you have more braces and then you'll have a philosophical argument yourself whether you dead in them or not.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's kind of that.",
                    "label": 0
                },
                {
                    "sent": "That kind of worries, but this is still a valid serial code, because this is still although it's crazy to do this totally, this is valid C, You know, to designate this structure the block.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll see you again.",
                    "label": 0
                },
                {
                    "sent": "I think it's 1:30 yeah, OK.",
                    "label": 0
                }
            ]
        }
    }
}