{
    "id": "dba2e43ztj5wdqoupqq34evzxjunbn6f",
    "title": "Distance Metric Learning Using Dropout: A Structured Regularization Approach",
    "info": {
        "author": [
            "Juhua Hu, School of Computing Science, Simon Fraser University"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Knowledge Extraction",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2014_hu_regularization_approach/",
    "segmentation": [
        [
            "Afternoon everyone, my name is Julia, who from SFU this is a joint work with teaching from MSU.",
            "Our title is distance metric learning.",
            "Using dropout we handle this problem by regularization approach."
        ],
        [
            "In this presentation we will firstly given introduction to our problem and some recent work.",
            "Then we will describe our methods and show the empirical without followed by the conclusion.",
            "So."
        ],
        [
            "So as we know, distance metric learning is very important for those distance based methods like K means clustering can yes neighbor classifier.",
            "But for example if the classifier is given the left side data set, it must be crazy about that even you cannot see the clusters or the class class class distribution.",
            "But we can apply the distance metric learning and get the representation of the class.",
            "Relationship as right side.",
            "The classifier must be happy about that."
        ],
        [
            "But the property is like this, since in distance metric learning we need to learn the number of pairing because that we need to learn is quadratic to the number of features so it can easily overfit to the training data.",
            "It means the general generalization performance will go going down.",
            "Even the training error is going up.",
            "China is going down."
        ],
        [
            "So."
        ],
        [
            "There has been some work, some trying to overcome this problem.",
            "Most existing methods that use PSD constraint to alleviate the operating problem, but each PSD projection is cubic to the number of dimensions.",
            "So it's very time expensive.",
            "And recently some researchers that use one projection strategy.",
            "That means they only project once at the end of the method and this.",
            "Our strategy is also adopting our method.",
            "On the other hand, dropout has been successfully used in deep learning and later it is even used for regression as actual regularizer.",
            "But these methods they only doing the dropout on the data features, and this in this work we are applied the dropout both on the metric and data, and we find that they can mimic the LP norm and even the trace norm.",
            "Also they can improve the generalizing.",
            "Formance"
        ],
        [
            "So let's go to the method part."
        ],
        [
            "See the details and now we are given a data set X which is consist of N data points and each ways D dimensions.",
            "The DML is aiming to learn a good metric that based on that metric, the distance from the distance between the data points from the same class is different from those from different ones with a very large margin and the corresponding obtaining property is.",
            "Like this, so the last function we use here is a hinge loss, and 80 is the triplet constraint."
        ],
        [
            "OK, so firstly we tried the dropout on the metric.",
            "This is the.",
            "Algorithm for the stochastic gradient descent for DML and in each updating of the metric we can associate random rival to each element of the metric.",
            "Symmetrically, and we find that with different settings of the probability I mean the dropout property.",
            "It can mimic different norms.",
            "For example, if you set the probability that Delta equals yellow equals Q, then the expectation of the metric is equal to 1 -- Q times the original metric.",
            "So in this way to expectation it can mimic the feminist norm.",
            "And similarly you can use it.",
            "Another strategy to mimic the LP norm and if you set P = 1 then it will be the L1 norm.",
            "So more importantly, we find that if we applying dropout, this will not affect the convergence rate of the gradient descent."
        ],
        [
            "And actually, in distance metric learning, you may want to exploit the structure of the metric.",
            "Usually the diagonal elements are more important than off the off diagonal ones.",
            "So now given a Gaussian random Max Q, we can set up the random matrix like this and you have finally used the structure FF norm.",
            "You can emphasize more on the diagonal.",
            "Elements, then off techno ones.",
            "And similarly, we can use it another Japan.",
            "The strategy tool we make the structure, the L11."
        ],
        [
            "OK, let's try to do the perturbation on the data as previous method did and firstly we can warm up with additive noise.",
            "So the the methods elect that we can add some Gaussian noise to the first of the triplet constraint and the expectation of this knew triplicate string is equal to the previous 1 + 1 + Q times the identity and we find that if you put this back to the automatic problem we get to the trace norm on the metric."
        ],
        [
            "But if you add external noise, it may affect the solution.",
            "So we try the dropouts on data by a surge by associating random rival to each feature of the data, and we get the similar doubts as addictive noise and more importantly, it were also not affected.",
            "The gradient descent convergence rate."
        ],
        [
            "OK, let's go to the experiment batch.",
            "See how our method works."
        ],
        [
            "The real data set here are the datasets that we used, and for each device edge we randomly sample 100,000 that related constraints and we use three nearest neighbor classifier to evaluate the performance for each experiment.",
            "Will repeat 5 trials and we compare the performance by pairwise T tests at the 95% confidence an.",
            "We have four remnants of the dropout methods.",
            "The SG and one is japal geos, the structured business Norm and SG DM 2 where you strip out as the structured L11.",
            "The other two is on data.",
            "One is used, the Gaussian noise and another one is applying to part as trace norm."
        ],
        [
            "So the baseline is the SGD with only one projection at the end of the method and another one is the projection at every iteration.",
            "We find that on the accuracy comparison, our methods are significantly better than those baselines, and from these people we can see that if you do every projection projection at every iteration, it will not affect the performance.",
            "If you're just parts only once at end of method and it is compara bulto, the SGD PSD which project at every iteration, so this is consistent with the previous results."
        ],
        [
            "And we also want to see how our report XX like the role of the regularizer from the first figure, you can see that if you increase the probability of dropout.",
            "The L1 noise is decreasing as we expected.",
            "And also for the rank, the rank is a formation of the trace norm and you can see that if you increase the probability that race rank is decreasing.",
            "OK, and since we want to alleviate the overfitting problem in DML, you can see from this example.",
            "When the number of training triplets is increasing, the training error for all the methods are similar, but those baselines their training area is slightly going up.",
            "But Joe Pod can help improve the performance."
        ],
        [
            "OK, so and we also compare.",
            "Compare the dropout with other existing state of art distance metric learning methods and in most cases our.",
            "Message outperform them.",
            "We we can also wrap in the dropout strategy in the in these methods and improve the performance."
        ],
        [
            "So.",
            "Now let's go to the conclusion part.",
            "So in this work we apply dropout on both the metric and data for distance metric learning and we find that in expectation they then they can mimic the option on trace norm and in one important day they were not affected.",
            "The coverage is rate of the gradient, descent and weapon.",
            "This strategy in existing VMS is also helpful.",
            "So a possible future worker is.",
            "Combine these two strategies together.",
            "How that's all."
        ],
        [
            "Thank you any question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Afternoon everyone, my name is Julia, who from SFU this is a joint work with teaching from MSU.",
                    "label": 0
                },
                {
                    "sent": "Our title is distance metric learning.",
                    "label": 1
                },
                {
                    "sent": "Using dropout we handle this problem by regularization approach.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this presentation we will firstly given introduction to our problem and some recent work.",
                    "label": 0
                },
                {
                    "sent": "Then we will describe our methods and show the empirical without followed by the conclusion.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we know, distance metric learning is very important for those distance based methods like K means clustering can yes neighbor classifier.",
                    "label": 1
                },
                {
                    "sent": "But for example if the classifier is given the left side data set, it must be crazy about that even you cannot see the clusters or the class class class distribution.",
                    "label": 1
                },
                {
                    "sent": "But we can apply the distance metric learning and get the representation of the class.",
                    "label": 0
                },
                {
                    "sent": "Relationship as right side.",
                    "label": 0
                },
                {
                    "sent": "The classifier must be happy about that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the property is like this, since in distance metric learning we need to learn the number of pairing because that we need to learn is quadratic to the number of features so it can easily overfit to the training data.",
                    "label": 1
                },
                {
                    "sent": "It means the general generalization performance will go going down.",
                    "label": 0
                },
                {
                    "sent": "Even the training error is going up.",
                    "label": 0
                },
                {
                    "sent": "China is going down.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There has been some work, some trying to overcome this problem.",
                    "label": 0
                },
                {
                    "sent": "Most existing methods that use PSD constraint to alleviate the operating problem, but each PSD projection is cubic to the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So it's very time expensive.",
                    "label": 0
                },
                {
                    "sent": "And recently some researchers that use one projection strategy.",
                    "label": 0
                },
                {
                    "sent": "That means they only project once at the end of the method and this.",
                    "label": 0
                },
                {
                    "sent": "Our strategy is also adopting our method.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, dropout has been successfully used in deep learning and later it is even used for regression as actual regularizer.",
                    "label": 0
                },
                {
                    "sent": "But these methods they only doing the dropout on the data features, and this in this work we are applied the dropout both on the metric and data, and we find that they can mimic the LP norm and even the trace norm.",
                    "label": 0
                },
                {
                    "sent": "Also they can improve the generalizing.",
                    "label": 0
                },
                {
                    "sent": "Formance",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go to the method part.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See the details and now we are given a data set X which is consist of N data points and each ways D dimensions.",
                    "label": 0
                },
                {
                    "sent": "The DML is aiming to learn a good metric that based on that metric, the distance from the distance between the data points from the same class is different from those from different ones with a very large margin and the corresponding obtaining property is.",
                    "label": 0
                },
                {
                    "sent": "Like this, so the last function we use here is a hinge loss, and 80 is the triplet constraint.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so firstly we tried the dropout on the metric.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm for the stochastic gradient descent for DML and in each updating of the metric we can associate random rival to each element of the metric.",
                    "label": 0
                },
                {
                    "sent": "Symmetrically, and we find that with different settings of the probability I mean the dropout property.",
                    "label": 0
                },
                {
                    "sent": "It can mimic different norms.",
                    "label": 0
                },
                {
                    "sent": "For example, if you set the probability that Delta equals yellow equals Q, then the expectation of the metric is equal to 1 -- Q times the original metric.",
                    "label": 0
                },
                {
                    "sent": "So in this way to expectation it can mimic the feminist norm.",
                    "label": 0
                },
                {
                    "sent": "And similarly you can use it.",
                    "label": 0
                },
                {
                    "sent": "Another strategy to mimic the LP norm and if you set P = 1 then it will be the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So more importantly, we find that if we applying dropout, this will not affect the convergence rate of the gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, in distance metric learning, you may want to exploit the structure of the metric.",
                    "label": 0
                },
                {
                    "sent": "Usually the diagonal elements are more important than off the off diagonal ones.",
                    "label": 0
                },
                {
                    "sent": "So now given a Gaussian random Max Q, we can set up the random matrix like this and you have finally used the structure FF norm.",
                    "label": 0
                },
                {
                    "sent": "You can emphasize more on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Elements, then off techno ones.",
                    "label": 0
                },
                {
                    "sent": "And similarly, we can use it another Japan.",
                    "label": 0
                },
                {
                    "sent": "The strategy tool we make the structure, the L11.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's try to do the perturbation on the data as previous method did and firstly we can warm up with additive noise.",
                    "label": 0
                },
                {
                    "sent": "So the the methods elect that we can add some Gaussian noise to the first of the triplet constraint and the expectation of this knew triplicate string is equal to the previous 1 + 1 + Q times the identity and we find that if you put this back to the automatic problem we get to the trace norm on the metric.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you add external noise, it may affect the solution.",
                    "label": 1
                },
                {
                    "sent": "So we try the dropouts on data by a surge by associating random rival to each feature of the data, and we get the similar doubts as addictive noise and more importantly, it were also not affected.",
                    "label": 0
                },
                {
                    "sent": "The gradient descent convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's go to the experiment batch.",
                    "label": 0
                },
                {
                    "sent": "See how our method works.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The real data set here are the datasets that we used, and for each device edge we randomly sample 100,000 that related constraints and we use three nearest neighbor classifier to evaluate the performance for each experiment.",
                    "label": 0
                },
                {
                    "sent": "Will repeat 5 trials and we compare the performance by pairwise T tests at the 95% confidence an.",
                    "label": 0
                },
                {
                    "sent": "We have four remnants of the dropout methods.",
                    "label": 0
                },
                {
                    "sent": "The SG and one is japal geos, the structured business Norm and SG DM 2 where you strip out as the structured L11.",
                    "label": 0
                },
                {
                    "sent": "The other two is on data.",
                    "label": 0
                },
                {
                    "sent": "One is used, the Gaussian noise and another one is applying to part as trace norm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the baseline is the SGD with only one projection at the end of the method and another one is the projection at every iteration.",
                    "label": 1
                },
                {
                    "sent": "We find that on the accuracy comparison, our methods are significantly better than those baselines, and from these people we can see that if you do every projection projection at every iteration, it will not affect the performance.",
                    "label": 0
                },
                {
                    "sent": "If you're just parts only once at end of method and it is compara bulto, the SGD PSD which project at every iteration, so this is consistent with the previous results.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also want to see how our report XX like the role of the regularizer from the first figure, you can see that if you increase the probability of dropout.",
                    "label": 0
                },
                {
                    "sent": "The L1 noise is decreasing as we expected.",
                    "label": 0
                },
                {
                    "sent": "And also for the rank, the rank is a formation of the trace norm and you can see that if you increase the probability that race rank is decreasing.",
                    "label": 0
                },
                {
                    "sent": "OK, and since we want to alleviate the overfitting problem in DML, you can see from this example.",
                    "label": 0
                },
                {
                    "sent": "When the number of training triplets is increasing, the training error for all the methods are similar, but those baselines their training area is slightly going up.",
                    "label": 0
                },
                {
                    "sent": "But Joe Pod can help improve the performance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so and we also compare.",
                    "label": 0
                },
                {
                    "sent": "Compare the dropout with other existing state of art distance metric learning methods and in most cases our.",
                    "label": 0
                },
                {
                    "sent": "Message outperform them.",
                    "label": 0
                },
                {
                    "sent": "We we can also wrap in the dropout strategy in the in these methods and improve the performance.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now let's go to the conclusion part.",
                    "label": 0
                },
                {
                    "sent": "So in this work we apply dropout on both the metric and data for distance metric learning and we find that in expectation they then they can mimic the option on trace norm and in one important day they were not affected.",
                    "label": 0
                },
                {
                    "sent": "The coverage is rate of the gradient, descent and weapon.",
                    "label": 0
                },
                {
                    "sent": "This strategy in existing VMS is also helpful.",
                    "label": 0
                },
                {
                    "sent": "So a possible future worker is.",
                    "label": 0
                },
                {
                    "sent": "Combine these two strategies together.",
                    "label": 0
                },
                {
                    "sent": "How that's all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you any question.",
                    "label": 0
                }
            ]
        }
    }
}