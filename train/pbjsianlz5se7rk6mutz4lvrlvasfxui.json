{
    "id": "pbjsianlz5se7rk6mutz4lvrlvasfxui",
    "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies",
    "info": {
        "author": [
            "Shihao Ji, Intel Corporation"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_ji_recurrent_neural/",
    "segmentation": [
        [
            "My name is Yuji from Intel Labs.",
            "Today I'm going to talk about a simple approximation.",
            "Error isn't called blackout.",
            "To speed our our language model with a large vocabulary.",
            "So this is joint work with Professor Vision, UCSC and a couple of my colleagues at Intel Labs."
        ],
        [
            "Softmax output layer has gained a lot of popularity.",
            "Do to learn our state, our performance of deep learning, application to NLP and computer vision problem.",
            "So it has been used in popular.",
            "What you wack model our language model and also neural machine translation model.",
            "And so on, so forth.",
            "So in this type of application typically we want is selecting award from vocabulary of hundreds of thousands or even millions of words.",
            "And similarly, in the case of.",
            "Computer vision problem like image, net Object recognition task.",
            "We want to classify objects into 22,000 different classes so.",
            "Large stock Max output layer typically is one of the bottlenecks of the of the whole deep learning pipeline, so we propose blackout or.",
            "Approximation error is going to speed up the computation of softmax output layer."
        ],
        [
            "So it can be used to any networks with with a softmax output layer.",
            "So in this talk.",
            "We will describe black out in the context of our language model, so here is a is a simple.",
            "You know standard our next model architecture.",
            "We have a input layer.",
            "We have a hidden layer with recurrent connection to itself and we have output layer.",
            "So all these layers are fully connected with the model parameter represented by three matrix is.",
            "So the computation flow goes like this at input layer, we have one hot encoding of my word and then we project this world into hidden layer plus or transformation of from previous hidden States and then we apply sigmoid landing ality.",
            "And then we got the current tenant States and namely project hidden stays too.",
            "The output layer and then we apply softmax function to normalize the score.",
            "So we get the probability density function over the entire vocabulary.",
            "So the computation in this stage is simple because this is 1 hot encoding, even though this is a matrix vector, multiplication is essentially just table look up.",
            "And the matrix here is small.",
            "Learn computation bottleneck is at the output layer will learn the vector here, which corresponding to the hidden states.",
            "It's a dense vector.",
            "So nice Caesar dense matrix, dense vector multiplication and if we use mini batch for the training then it becomes a dense matrix.",
            "This matrix multiplication or SGM.",
            "And the list is a special type of SGN called tall skinny.",
            "And because of the shape of the matrix involved in the computation.",
            "The computation efficiency for this type of SGM is low because of learning.",
            "Low arithmetic intensity and a high memory bandwidth requirement.",
            "In addition, during forward and backward propagation.",
            "Transpose SJ are required so so matrix transport transpose incurs additional penalty.",
            "So we took a look at this case and optimized tall skinny as Gen Intel architecture.",
            "And if you take a look at the latest Intel MKL.",
            "You will observe significant performance improvement to this case.",
            "And and by the way, Intel MKL is free for academia now, so please get a copy and give it a try."
        ],
        [
            "Our next model called our attention because recently there couple of papers talk about claims that GPU's performance is about 20 X 30X or even 100 X faster than the CPU.",
            "So we took a look at this and optimize standard arm length model without any approximation first.",
            "On Intel architecture, so in this case we use Intel Haswell machine.",
            "And it turns out that our optimized code deliverers.",
            "Your performance then what they what they got.",
            "Leader GTX Titan.",
            "So this is all good and we were happy.",
            "However, if we want you run this our next model with millions of words as vocabulary.",
            "On Google's one building world benchmark until convergence, it will take several months.",
            "So we took."
        ],
        [
            "Additional approximation to speed up our next model even further.",
            "So we know the bottleneck bottleneck is the large softmax output layer.",
            "And there are many paper talk about strategies to speed up, you know, softmax output layer already, so such as a hierarchical submax sampling based approximation.",
            "Self normalization.",
            "And exact gradient unlimited loss function.",
            "For the sake of it, and I cannot describe this related method here, I just want to point out that.",
            "Breakout belongs to you in the second category.",
            "Is sampling based approximation.",
            "So in this case we select a subset of output layer instead of using the entire output layer."
        ],
        [
            "So here I'm going to talk about the background training.",
            "So before that I, let's review what the traditional maximal training.",
            "So in this case on the output nodes participate in the training.",
            "And we compute the softmax.",
            "And then we.",
            "Find model parameter to maximize the log likelihood of model parameter respect to their target word, and then we compute the gradient and then we back propagate the error to the previous states that previous as layers.",
            "Well, in black out training, first we select a subset of random words from sampling distribution QW.",
            "And then the computation will just constraint on these selected words and then we compute a weighted softmax.",
            "And with the weight equal to the inverse of learn sampling probability.",
            "And then we use a discriminative training function.",
            "So it has two terms.",
            "The first turn, just similar to the traditional you know maximum likelihood.",
            "We try to maximize their probability of target word and then second second term sets that we want to minimize the probability of random words explicitly.",
            "We call it explicitly because for the first time, if we want to maximize, if we maximize probability of target words.",
            "Since the probability are normalized, so it will implicitly minimize the likely under probability of random words.",
            "So in a second test instead, in addition to level, you know implicit penalty for random words.",
            "We want to explicitly penalize the score for random words even further, so we will show that in our experiment, that is, it's a very efficient way for this case, and then we compute the gradient and is it can be seen that they are strongly related.",
            "So if we ignore the second term, so these two goes away, and if we.",
            "Don't you simply use the entire upper layer K equal to 0?",
            "Then these two just degenerated to the standard, or you know gradient."
        ],
        [
            "So Blackout has a cloud collection, 2 important sampling, and it turns out that the weighted softmax corresponding to a important sampling based estimator of standards of Max.",
            "Well, import important sampling has been used for softmax before.",
            "Here let the difference are two folded.",
            "The first word is that we use a exponent exterminated unigram distribution.",
            "Well offer it's in the range between zero to 1.",
            "Well, previously, either you know uniform distribution corresponding about equal to 0, or unigram distribution about equal to 1 was used.",
            "Well, we well we we notice that and.",
            "Opera, you know, in between these two extremes.",
            "We can't find the Alpha lead components, you know.",
            "I'm.",
            "Bias and variance better.",
            "And the second second difference is that we use discriminative training."
        ],
        [
            "So Blackout can also formulate into NC framework and.",
            "You know, I cannot describe too much about the NC, but I just want to say that typically for NC application unigram is used at Lloyds Distribution.",
            "Here we propose uses stochastic volume of noise distribution.",
            "We draw samples from a sampling distribution and then we compute a mean based on this formulation and this theorem shows that expression.",
            "Indeed, it's a probability distribution function under the expectation and then follow the standard ANSI framework and we insert this noise distribution into a posterior and then we got this.",
            "Weighted softmax the advantage of this acoustic version of noise distribution is not the expensive partition function.",
            "Of softmax is cancelled out and locks.",
            "Trip can still be useful numerical stability.",
            "We will show data on this later."
        ],
        [
            "And I also want to draw the collection to drop out so in drop out, drop on.",
            "You know typical use for useful input and hidden layer will blackout used at the softmax output layer.",
            "So they provide you know some kind of complementary to each other to train the entire you know.",
            "Different Internet works and in dropout training we retain a load.",
            "These are fixed probability P and in the in the test we use the full network but we scale down weights PW right?",
            "So in in blackout we sample K nodes from a long uniform distribution.",
            "And then we compute a weighted softmax with discriminative training.",
            "In the test we used the entire network, but we just use the trend wait, so this shows the difference in connection between them.",
            "In terms of benefits, it quite similar, both of them speed up training, but not test an in blackout blackout is the regularization method.",
            "It does model averaging can avoid overfitting with some theoretical justification.",
            "Well, in all my experiment we showed that a blackout can also avoid overfitting, probably due to stochasticity.",
            "But we need more theoretical justification."
        ],
        [
            "Now I'm going to talk about experiments out first on small data set we have a complex real reason and see black out and except for Max we use this small small tests because we can run standard our next model.",
            "These with exact softmax within multiple time.",
            "Lisa result untrained itself needs results on the test data set and X axis is the lambos random samples for softmax approximation and one access perplexity lower the better.",
            "So in this case we show that training that on training data set standard language model reaches lowest popularity well in test data set, blackout reaches lowest populated.",
            "This shows that our standard.",
            "Has some kind of overfitting issue.",
            "Will black out at end and see can avoid orbiting.",
            "And."
        ],
        [
            "This shows the rate of convergence of NC and Blackout blackout converge faster.",
            "And this."
        ],
        [
            "Shows the impact of social issues on Google's 1 billion word benchmark.",
            "This shows impact of Alpha so are far is the implemented unigram distribution are equal to 0 uniform sampling by equal to 1 kilogram sampling.",
            "And in between it's actually gives the lowest perplexity you know about points 3 or .4, and this shows the benefit of discriminative training.",
            "We normally see 123 point Publix reduction due to discriminate training.",
            "So."
        ],
        [
            "Finally, I'm going to talk about a little bit about, you know comparison with state of our results obtained on GPU and CPU, CPU cluster so well.",
            "We just use one CPU.",
            "You know, in this case Intel Haswell, so this paper.",
            "The data from this paper so they optimize NC on a.",
            "And lead us GTX Titan machine and provide their you know perplexity and potential solution.",
            "We optimize blackout on Intel Haswell.",
            "And then, in terms of perplexity, that we normally get one to two point reduction except and on a large model.",
            "Probably because you know, you know recently this architecture will right close to the optimal can reach in terms of our total solution we normally faster.",
            "And this is comparison.",
            "These CPU cluster results.",
            "They run this on 32 machine about 60 hours with LTM language model.",
            "And we just use one CPU.",
            "And but we use the standard ARM language model.",
            "We achieve slightly better perplexity."
        ],
        [
            "So in this case, the purpose comparison is not.",
            "It's impropriate because we use different language model, but we just want to show that in terms of runtime performance we can just use one CPU and match the performance of GPU and CPU cluster.",
            "And finally we open source of implementation at GitHub.",
            "Yeah, that's.",
            "What I have?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Yuji from Intel Labs.",
                    "label": 1
                },
                {
                    "sent": "Today I'm going to talk about a simple approximation.",
                    "label": 0
                },
                {
                    "sent": "Error isn't called blackout.",
                    "label": 0
                },
                {
                    "sent": "To speed our our language model with a large vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Professor Vision, UCSC and a couple of my colleagues at Intel Labs.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Softmax output layer has gained a lot of popularity.",
                    "label": 1
                },
                {
                    "sent": "Do to learn our state, our performance of deep learning, application to NLP and computer vision problem.",
                    "label": 0
                },
                {
                    "sent": "So it has been used in popular.",
                    "label": 0
                },
                {
                    "sent": "What you wack model our language model and also neural machine translation model.",
                    "label": 1
                },
                {
                    "sent": "And so on, so forth.",
                    "label": 0
                },
                {
                    "sent": "So in this type of application typically we want is selecting award from vocabulary of hundreds of thousands or even millions of words.",
                    "label": 0
                },
                {
                    "sent": "And similarly, in the case of.",
                    "label": 0
                },
                {
                    "sent": "Computer vision problem like image, net Object recognition task.",
                    "label": 0
                },
                {
                    "sent": "We want to classify objects into 22,000 different classes so.",
                    "label": 0
                },
                {
                    "sent": "Large stock Max output layer typically is one of the bottlenecks of the of the whole deep learning pipeline, so we propose blackout or.",
                    "label": 0
                },
                {
                    "sent": "Approximation error is going to speed up the computation of softmax output layer.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it can be used to any networks with with a softmax output layer.",
                    "label": 0
                },
                {
                    "sent": "So in this talk.",
                    "label": 0
                },
                {
                    "sent": "We will describe black out in the context of our language model, so here is a is a simple.",
                    "label": 0
                },
                {
                    "sent": "You know standard our next model architecture.",
                    "label": 0
                },
                {
                    "sent": "We have a input layer.",
                    "label": 0
                },
                {
                    "sent": "We have a hidden layer with recurrent connection to itself and we have output layer.",
                    "label": 0
                },
                {
                    "sent": "So all these layers are fully connected with the model parameter represented by three matrix is.",
                    "label": 0
                },
                {
                    "sent": "So the computation flow goes like this at input layer, we have one hot encoding of my word and then we project this world into hidden layer plus or transformation of from previous hidden States and then we apply sigmoid landing ality.",
                    "label": 0
                },
                {
                    "sent": "And then we got the current tenant States and namely project hidden stays too.",
                    "label": 0
                },
                {
                    "sent": "The output layer and then we apply softmax function to normalize the score.",
                    "label": 0
                },
                {
                    "sent": "So we get the probability density function over the entire vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So the computation in this stage is simple because this is 1 hot encoding, even though this is a matrix vector, multiplication is essentially just table look up.",
                    "label": 0
                },
                {
                    "sent": "And the matrix here is small.",
                    "label": 0
                },
                {
                    "sent": "Learn computation bottleneck is at the output layer will learn the vector here, which corresponding to the hidden states.",
                    "label": 0
                },
                {
                    "sent": "It's a dense vector.",
                    "label": 0
                },
                {
                    "sent": "So nice Caesar dense matrix, dense vector multiplication and if we use mini batch for the training then it becomes a dense matrix.",
                    "label": 0
                },
                {
                    "sent": "This matrix multiplication or SGM.",
                    "label": 0
                },
                {
                    "sent": "And the list is a special type of SGN called tall skinny.",
                    "label": 0
                },
                {
                    "sent": "And because of the shape of the matrix involved in the computation.",
                    "label": 0
                },
                {
                    "sent": "The computation efficiency for this type of SGM is low because of learning.",
                    "label": 0
                },
                {
                    "sent": "Low arithmetic intensity and a high memory bandwidth requirement.",
                    "label": 1
                },
                {
                    "sent": "In addition, during forward and backward propagation.",
                    "label": 0
                },
                {
                    "sent": "Transpose SJ are required so so matrix transport transpose incurs additional penalty.",
                    "label": 0
                },
                {
                    "sent": "So we took a look at this case and optimized tall skinny as Gen Intel architecture.",
                    "label": 1
                },
                {
                    "sent": "And if you take a look at the latest Intel MKL.",
                    "label": 1
                },
                {
                    "sent": "You will observe significant performance improvement to this case.",
                    "label": 0
                },
                {
                    "sent": "And and by the way, Intel MKL is free for academia now, so please get a copy and give it a try.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next model called our attention because recently there couple of papers talk about claims that GPU's performance is about 20 X 30X or even 100 X faster than the CPU.",
                    "label": 0
                },
                {
                    "sent": "So we took a look at this and optimize standard arm length model without any approximation first.",
                    "label": 0
                },
                {
                    "sent": "On Intel architecture, so in this case we use Intel Haswell machine.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that our optimized code deliverers.",
                    "label": 0
                },
                {
                    "sent": "Your performance then what they what they got.",
                    "label": 0
                },
                {
                    "sent": "Leader GTX Titan.",
                    "label": 0
                },
                {
                    "sent": "So this is all good and we were happy.",
                    "label": 0
                },
                {
                    "sent": "However, if we want you run this our next model with millions of words as vocabulary.",
                    "label": 0
                },
                {
                    "sent": "On Google's one building world benchmark until convergence, it will take several months.",
                    "label": 0
                },
                {
                    "sent": "So we took.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Additional approximation to speed up our next model even further.",
                    "label": 0
                },
                {
                    "sent": "So we know the bottleneck bottleneck is the large softmax output layer.",
                    "label": 0
                },
                {
                    "sent": "And there are many paper talk about strategies to speed up, you know, softmax output layer already, so such as a hierarchical submax sampling based approximation.",
                    "label": 1
                },
                {
                    "sent": "Self normalization.",
                    "label": 1
                },
                {
                    "sent": "And exact gradient unlimited loss function.",
                    "label": 0
                },
                {
                    "sent": "For the sake of it, and I cannot describe this related method here, I just want to point out that.",
                    "label": 0
                },
                {
                    "sent": "Breakout belongs to you in the second category.",
                    "label": 0
                },
                {
                    "sent": "Is sampling based approximation.",
                    "label": 1
                },
                {
                    "sent": "So in this case we select a subset of output layer instead of using the entire output layer.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here I'm going to talk about the background training.",
                    "label": 0
                },
                {
                    "sent": "So before that I, let's review what the traditional maximal training.",
                    "label": 0
                },
                {
                    "sent": "So in this case on the output nodes participate in the training.",
                    "label": 0
                },
                {
                    "sent": "And we compute the softmax.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                },
                {
                    "sent": "Find model parameter to maximize the log likelihood of model parameter respect to their target word, and then we compute the gradient and then we back propagate the error to the previous states that previous as layers.",
                    "label": 0
                },
                {
                    "sent": "Well, in black out training, first we select a subset of random words from sampling distribution QW.",
                    "label": 0
                },
                {
                    "sent": "And then the computation will just constraint on these selected words and then we compute a weighted softmax.",
                    "label": 0
                },
                {
                    "sent": "And with the weight equal to the inverse of learn sampling probability.",
                    "label": 0
                },
                {
                    "sent": "And then we use a discriminative training function.",
                    "label": 0
                },
                {
                    "sent": "So it has two terms.",
                    "label": 0
                },
                {
                    "sent": "The first turn, just similar to the traditional you know maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "We try to maximize their probability of target word and then second second term sets that we want to minimize the probability of random words explicitly.",
                    "label": 1
                },
                {
                    "sent": "We call it explicitly because for the first time, if we want to maximize, if we maximize probability of target words.",
                    "label": 0
                },
                {
                    "sent": "Since the probability are normalized, so it will implicitly minimize the likely under probability of random words.",
                    "label": 0
                },
                {
                    "sent": "So in a second test instead, in addition to level, you know implicit penalty for random words.",
                    "label": 0
                },
                {
                    "sent": "We want to explicitly penalize the score for random words even further, so we will show that in our experiment, that is, it's a very efficient way for this case, and then we compute the gradient and is it can be seen that they are strongly related.",
                    "label": 0
                },
                {
                    "sent": "So if we ignore the second term, so these two goes away, and if we.",
                    "label": 0
                },
                {
                    "sent": "Don't you simply use the entire upper layer K equal to 0?",
                    "label": 0
                },
                {
                    "sent": "Then these two just degenerated to the standard, or you know gradient.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Blackout has a cloud collection, 2 important sampling, and it turns out that the weighted softmax corresponding to a important sampling based estimator of standards of Max.",
                    "label": 0
                },
                {
                    "sent": "Well, import important sampling has been used for softmax before.",
                    "label": 0
                },
                {
                    "sent": "Here let the difference are two folded.",
                    "label": 0
                },
                {
                    "sent": "The first word is that we use a exponent exterminated unigram distribution.",
                    "label": 0
                },
                {
                    "sent": "Well offer it's in the range between zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Well, previously, either you know uniform distribution corresponding about equal to 0, or unigram distribution about equal to 1 was used.",
                    "label": 1
                },
                {
                    "sent": "Well, we well we we notice that and.",
                    "label": 0
                },
                {
                    "sent": "Opera, you know, in between these two extremes.",
                    "label": 0
                },
                {
                    "sent": "We can't find the Alpha lead components, you know.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Bias and variance better.",
                    "label": 1
                },
                {
                    "sent": "And the second second difference is that we use discriminative training.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Blackout can also formulate into NC framework and.",
                    "label": 0
                },
                {
                    "sent": "You know, I cannot describe too much about the NC, but I just want to say that typically for NC application unigram is used at Lloyds Distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we propose uses stochastic volume of noise distribution.",
                    "label": 1
                },
                {
                    "sent": "We draw samples from a sampling distribution and then we compute a mean based on this formulation and this theorem shows that expression.",
                    "label": 0
                },
                {
                    "sent": "Indeed, it's a probability distribution function under the expectation and then follow the standard ANSI framework and we insert this noise distribution into a posterior and then we got this.",
                    "label": 0
                },
                {
                    "sent": "Weighted softmax the advantage of this acoustic version of noise distribution is not the expensive partition function.",
                    "label": 1
                },
                {
                    "sent": "Of softmax is cancelled out and locks.",
                    "label": 0
                },
                {
                    "sent": "Trip can still be useful numerical stability.",
                    "label": 0
                },
                {
                    "sent": "We will show data on this later.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I also want to draw the collection to drop out so in drop out, drop on.",
                    "label": 0
                },
                {
                    "sent": "You know typical use for useful input and hidden layer will blackout used at the softmax output layer.",
                    "label": 0
                },
                {
                    "sent": "So they provide you know some kind of complementary to each other to train the entire you know.",
                    "label": 0
                },
                {
                    "sent": "Different Internet works and in dropout training we retain a load.",
                    "label": 0
                },
                {
                    "sent": "These are fixed probability P and in the in the test we use the full network but we scale down weights PW right?",
                    "label": 0
                },
                {
                    "sent": "So in in blackout we sample K nodes from a long uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "And then we compute a weighted softmax with discriminative training.",
                    "label": 1
                },
                {
                    "sent": "In the test we used the entire network, but we just use the trend wait, so this shows the difference in connection between them.",
                    "label": 1
                },
                {
                    "sent": "In terms of benefits, it quite similar, both of them speed up training, but not test an in blackout blackout is the regularization method.",
                    "label": 1
                },
                {
                    "sent": "It does model averaging can avoid overfitting with some theoretical justification.",
                    "label": 0
                },
                {
                    "sent": "Well, in all my experiment we showed that a blackout can also avoid overfitting, probably due to stochasticity.",
                    "label": 0
                },
                {
                    "sent": "But we need more theoretical justification.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to talk about experiments out first on small data set we have a complex real reason and see black out and except for Max we use this small small tests because we can run standard our next model.",
                    "label": 0
                },
                {
                    "sent": "These with exact softmax within multiple time.",
                    "label": 0
                },
                {
                    "sent": "Lisa result untrained itself needs results on the test data set and X axis is the lambos random samples for softmax approximation and one access perplexity lower the better.",
                    "label": 0
                },
                {
                    "sent": "So in this case we show that training that on training data set standard language model reaches lowest popularity well in test data set, blackout reaches lowest populated.",
                    "label": 1
                },
                {
                    "sent": "This shows that our standard.",
                    "label": 0
                },
                {
                    "sent": "Has some kind of overfitting issue.",
                    "label": 0
                },
                {
                    "sent": "Will black out at end and see can avoid orbiting.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shows the rate of convergence of NC and Blackout blackout converge faster.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shows the impact of social issues on Google's 1 billion word benchmark.",
                    "label": 1
                },
                {
                    "sent": "This shows impact of Alpha so are far is the implemented unigram distribution are equal to 0 uniform sampling by equal to 1 kilogram sampling.",
                    "label": 0
                },
                {
                    "sent": "And in between it's actually gives the lowest perplexity you know about points 3 or .4, and this shows the benefit of discriminative training.",
                    "label": 0
                },
                {
                    "sent": "We normally see 123 point Publix reduction due to discriminate training.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, I'm going to talk about a little bit about, you know comparison with state of our results obtained on GPU and CPU, CPU cluster so well.",
                    "label": 0
                },
                {
                    "sent": "We just use one CPU.",
                    "label": 0
                },
                {
                    "sent": "You know, in this case Intel Haswell, so this paper.",
                    "label": 0
                },
                {
                    "sent": "The data from this paper so they optimize NC on a.",
                    "label": 0
                },
                {
                    "sent": "And lead us GTX Titan machine and provide their you know perplexity and potential solution.",
                    "label": 1
                },
                {
                    "sent": "We optimize blackout on Intel Haswell.",
                    "label": 0
                },
                {
                    "sent": "And then, in terms of perplexity, that we normally get one to two point reduction except and on a large model.",
                    "label": 0
                },
                {
                    "sent": "Probably because you know, you know recently this architecture will right close to the optimal can reach in terms of our total solution we normally faster.",
                    "label": 0
                },
                {
                    "sent": "And this is comparison.",
                    "label": 0
                },
                {
                    "sent": "These CPU cluster results.",
                    "label": 0
                },
                {
                    "sent": "They run this on 32 machine about 60 hours with LTM language model.",
                    "label": 0
                },
                {
                    "sent": "And we just use one CPU.",
                    "label": 0
                },
                {
                    "sent": "And but we use the standard ARM language model.",
                    "label": 0
                },
                {
                    "sent": "We achieve slightly better perplexity.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, the purpose comparison is not.",
                    "label": 0
                },
                {
                    "sent": "It's impropriate because we use different language model, but we just want to show that in terms of runtime performance we can just use one CPU and match the performance of GPU and CPU cluster.",
                    "label": 0
                },
                {
                    "sent": "And finally we open source of implementation at GitHub.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's.",
                    "label": 0
                },
                {
                    "sent": "What I have?",
                    "label": 0
                }
            ]
        }
    }
}