{
    "id": "hxr7hgxes4y77x5pjpkpvri4watiasi2",
    "title": "Machine Translation - Winds of Change",
    "info": {
        "author": [
            "Stephan Vogel, Qatar Computing Research Institute"
        ],
        "published": "July 31, 2016",
        "recorded": "July 2016",
        "category": [
            "Top->Computer Science",
            "Top->Technology"
        ]
    },
    "url": "http://videolectures.net/interACT2016_vogel_machine_translation/",
    "segmentation": [
        [
            "So I chose the topic machine translation, winds of change, and actually it's seems to be a good connection of what I already mentioned."
        ],
        [
            "So winds of change.",
            "People like Alex Waibel.",
            "They may actually recognize the title.",
            "So this oops, let's go back here.",
            "Can we play?",
            "No, it's not playing here.",
            "Unfortunately not, so I wanted to play this for Alex.",
            "I know for a fact that he likes to listen to Hotel California when he's flying his chopper.",
            "I thought winds of change might be inappropriate song to to start this.",
            "Unfortunately it's not playing right now, but many of you may actually know this song and it was at the time when actually there were systematic changes happening here in Germany."
        ],
        [
            "So winds of change could also be called paradigm shift, right?",
            "So this is a title.",
            "This is a term which was introduced by the Historian of Science Thomas Kuhn in his famous book The Structure of Scientific Revolutions to describe process and progress in Science.",
            "So paradigm is essentially what the Community, the member of a certain scientific community have in common.",
            "So this could be, you know, the examples in the taste textbooks, the techniques which are used to practices which are accepted and so on.",
            "We could also say what can be actually published at the conference and when we look at conferences, the recent knuckle or the upcoming ACL, we see that actually machine translation Paradigm Shift is happening right now.",
            "And we have heard already by Cai that this is already going into the into the companies so one.",
            "Example of a paradigm shift which is always used in history of science is, you know, the shift in the heliocentric from the cheer centric to the heliocentric view of the universe."
        ],
        [
            "So we have heard that there was rule based translation and then there was statistical machine translation.",
            "So these numbers are lipid raft.",
            "Obviously there is not always a shift which is happening within a few weeks and so on.",
            "But when we look back so I started in machine translation in 95 late 95 in Akron and so at that time we started to work on statistical machine translation which was already published a couple of years earlier by researchers at IBM.",
            "But then it took a couple of years.",
            "Till it actually really took off right?",
            "And so I indicated this.",
            "We still have statistical machine translation as a major major paradigm, but it seems to be fading away.",
            "We will see if it actually happens over the next couple of years.",
            "And now this new LMT is really taking off again.",
            "It just didn't pop out of nothing.",
            "So there has been work going on.",
            "But now what we see is really that the scientific community and obviously also companies are actually jumping on this."
        ],
        [
            "Alright, so the old paradigm here is statistical machine translation, and as heimer mentioned, there had been different flavors of it.",
            "The most prominent, probably the phrase based statistical translation.",
            "And so I just.",
            "Refer to this right now.",
            "So there you train this alignment model which you have seen.",
            "I think Hunts mentioned this extract phrase pairs or subtrees.",
            "If we actually go to syntax based Service Commission translation we learn phrase or word reordering patterns.",
            "We train N gram language models and then we use this entire inventory to actually translate your sentences."
        ],
        [
            "So here, just as a few images, so we learn from parallel data from human translators examples, kind of.",
            "Forwards between words on the source language on the target language.",
            "When we have an entire sentence, then we find the best path of these alignments and then we can draw these."
        ],
        [
            "Tangles about these alignment points, and each of these rectangles is now actually a phrase pair which we can extract.",
            "You know, overlap being embedded, and so on.",
            "So we get this entire list."
        ],
        [
            "Then we use this when we have a new sentence we fill.",
            "Now in all these partial translations and then we have essentially a craft search problem.",
            "We need to find the best path within this graph.",
            "And actually, it's not just monotone going through this graph, we can jump ahead and then jump back to fill some gaps which we left in earlier in earlier parts.",
            "So this is this typical translation which was now the predominant paradigm over the past.",
            "Let's say 1515 years."
        ],
        [
            "So this is essentially data driven.",
            "More data means better translations, which of course creates a problem in this low resource scenarios which hi, my boss was mentioning right and there's currently this project lower light going on which now addresses this particular issue.",
            "There is, however, a serious problem with this statistical machine translation, so essentially each word is an island, right?",
            "So if you have now related words like the colors green, blue, yellow and so on, so they have nothing.",
            "In common inside of the system.",
            "So essentially the system treats each word you know individually.",
            "Even worse, when you have now relate a morphological variants of the same word, like here table and tables.",
            "Inside this statistical empty system, they're just as different as 22 and cow, right?",
            "Which is not very appropriate for dealing actually with languages.",
            "And of course there have been some repair mechanisms like factored models, world class models.",
            "Or the abstract meaning representation and so on which all try to address this fundamental problem in statistical machine translation to actually bring the meaning of the words together.",
            "So when I say they are an island, you know for efficiency birds are mapped into numbers inside of the system, so it is really 17 and 99 and 220 which represent the individual words."
        ],
        [
            "So now there is a new paradigm.",
            "This neural empty, right?",
            "So here and of course you know it has predecessors and I will come back to this.",
            "But what is happening now that essentially the scientific community is jumping at this neural empty right?",
            "So here I have just probably the most simple example of an architecture for Newell empty.",
            "So what happens here is that we feed in the words through a vector where we just switch on, you know one cell in the vector.",
            "So this is the lowest roll.",
            "Alright, so then these words are mapped now into a high dimensional space, not as high dimensional as actually the input, right?",
            "So if I have now a vocabulary of 200,000 words, I'm a map it down too.",
            "500 dimensional space.",
            "And so I tried to actually capture some generalization right and these mappings are now part of the training program.",
            "So the idea here is that I actually find a representation of the words in a multi dimensional space such that related words come closer together in this page in this space.",
            "So the next step is then actually a recurrent neural network.",
            "So I accumulate the information as I feed in Word of the word.",
            "I accumulate it.",
            "Through these States and so the red state out there is not a representation of the entire sentence, so this is the encoding part.",
            "So I first collect the information encoded into one state, which is not just, you know one one dot, but essentially also in high dimensional representation.",
            "Now from this I start to generate the words for the target language.",
            "So this again is now step by step and now I can reuse the history.",
            "So the word which I have seen before.",
            "So I start out from this.",
            "Representation which encodes the source sentence and I generate the target sentence by also looking at what I have generated so far until I have generated the entire sentence and this is now the decoder part right.",
            "And of course this shows already one major problem with this kind of architecture I have to compress an entire sentence into this one state and then generate out of it."
        ],
        [
            "So.",
            "This new empty so the important parts are that actually the words are mapped into this high dimensional vector space.",
            "This is called embeddings, and as I already mentioned hopefully words closely related to each other actually occupy spaces related closed in close to each other at this time can be extended to phrase level and sentence level.",
            "So the neural empty is as data driven as the SMT, so it has the same flaw in a way you know that we need a lot of data to actually train it.",
            "So right now we were experimenting under the evaluation, which is going on related to this low resource scenario to the Lorelei project, right?",
            "And what are you doing when actually your training data covers only half of the words which you see now in your test data, right?",
            "You start out with.",
            "Every other word is unknown, and of course then you want to look into the related languages and so on.",
            "And maybe there is great hope using neural empty to cover.",
            "Now some of these aspects for low resource languages.",
            "Right now what we experience and what we hear is you need lots of data to train the embeddings to train.",
            "You know the Bates in this recurrent neural networks and so on.",
            "So from this part it may not be there.",
            "Actually this paradigm shift is really, you know, solving one of the problems which you have in the.",
            "Machine translation"
        ],
        [
            "So to some extent it's like back to the future, you know.",
            "Please go back to actually worked over translations.",
            "This is what the original IBM translation approach was right, and then we made huge progress in statistical translation when we went to the phrase level to capture local local phenomena and so on.",
            "So now we are feeding individual words into it into the neural network, right?",
            "And of course people are working very hard and there's amazing progress in now overcoming some of these shortcomings and innovate, pulling in and capturing some of what has been learned over the last 15 years in statistical translation.",
            "Right, so it's not just standard show standard recurrent neural network, so it's short long term memory.",
            "It's going in both directions.",
            "Attention, mechanism and so on."
        ],
        [
            "So of course, even if we can talk in a way about paradigm shift, here you know.",
            "There was always something which has happened in the past, right?",
            "And it was already mentioned, so neural networks or at that time also under the under the term connectionism actually was happening 30 years ago and was a big big thing.",
            "You know, there was a lot of hype about it.",
            "I remember you know that I played at that time with the Stuart Stuttgart neural Network simulator on a parallel computer.",
            "It was fun.",
            "Then suddenly you know 60 * 4 * 4 Windows popped up.",
            "In this simulator and we had to click them away 1 by 1 to clear the screen again.",
            "So a lot of this has happened and actually it has been applied in language processing in different areas.",
            "I just gave 3 examples here.",
            "You know on the passing passing complex structures in speech recognition where Alex viable boss heavily involved with time delay neural networks, but then also in machine translation.",
            "For example, the group in Valencia at the time.",
            "Of course there were problems at the time.",
            "Both in terms of limited hardware and also in terms of not yet fully developed mathematical underpinnings.",
            "So there has been progress that now we can come back and do actually things which were not possible 30 years ago."
        ],
        [
            "So when we look back at this picture right?",
            "So the rule based, you know, just some thoughts I have about this, so the rule based translation, both expert driven.",
            "You know they had the linguist too populated the dictionaries who wrote the rules, the transfer rules, the analysis and generation rules and so on, and words were handled as a bundle of features.",
            "You know you have the description of the individual words, how they operate in the context of other words.",
            "And so this achieves actually a pretty good level of generalization.",
            "Then we move to the SMT, which is now data driven, and this was probably the most important part of the paradigm shift going on there.",
            "But now the world was essentially an Atom, right?",
            "Just like one number is different from another number, and we built now collecting huge bags of examples which we could reuse to translating sentences.",
            "So when we are now going to Newell empty, it still data driven.",
            "So whatever is good from data driven, whatever is bad from day to turn.",
            "I think we are carried over.",
            "But now the word is not just this one number.",
            "We actually with this embedding, mapping the mapping them into a continuous space, and this allows for richer manipulation and richer representation of the content of of a sentence.",
            "The level of generalization maybe not as high, but actually this learned it becomes.",
            "Part of the learning mechanism, which is actually a good thing, and then you know we may expect that actually each phase in empty is shorter than the one before, so we don't know how long people will now be excited about the neural empty and then something new will come.",
            "And maybe I'm still around at the time."
        ],
        [
            "If it happens fast enough, OK, so there's another dimension in icy wind of change, and this is in empty business, right?",
            "So this again, in a way refers to what kind was presenting before, right?",
            "So translation, not machine learning translation has grown to 40 billion dollar business and there is an annual growth of 7%.",
            "Machine translation actually grows much faster, so it grows by 20 to 25.",
            "Percent per year, right?",
            "Observations we can make here.",
            "Companies are really embracing.",
            "Empty governments are adopting empty right?",
            "So the European Parliament.",
            "They have actually big efforts in pulling in.",
            "Empty translators are often unhappily accepting the reality that there is an empty right and they are using it to some extent.",
            "Translation memory providers, translation, memories on except the technology for human translators.",
            "They all have now their back doors to the machine translation online or specifically turned.",
            "And I would say machine translation is not so much replacing.",
            "Actually the human translation, but it's covering.",
            "Their translation was not done before."
        ],
        [
            "So when we look at this, you know Google reported recently that they are translating 140 billion words a day, right?",
            "We just heard you know, the two billion text translations on a daily basis from Facebook, which may be something like 20 thirty 40 billion words.",
            "I guess I reported it not inverts, but in in postings.",
            "Former colleague and friend from who's now at eBay couldn't come today.",
            "He mentioned in private communication they would need 200,000 translators to translate their postings, which they translate.",
            "This machine translation and I couldn't find any information from Microsoft, but Chris went.",
            "Probably can tell us also numbers which are in the higher high billions of words translated everyday."
        ],
        [
            "So when we look at it and this is really, you know, mind blowing to me.",
            "So the small dot is how much humans translate the Queen.",
            "Big plot is how much is already translated by machines, right?",
            "And of course, the machine translation goes a little bit into the space where the human translation is happening, but most of it is actually where humans would never have translated before, like the Facebook postings.",
            "I would not pay a translator to translate now the Bulgarian message of a colleague I'm interested in to understand, but I can use the Facebook translation."
        ],
        [
            "And we see also in the space of small companies you know a lot of startups are actually popping up over the last couple of years which are operating in the space of machine translation.",
            "So there was a report just last year which mentioned within 10 years 190 translation startups were identified and they probably missed a couple of those."
        ],
        [
            "So here is a listing you know you cannot see it, but you see that every year you know even dozens of translation companies were actually start."
        ],
        [
            "So just some examples from close by mobile technologies which was bought by Facebook right which we just learned about started by Alex Waibel and Mobile Technologies, was famous for cheap, eager.",
            "Probably, this is how Facebook became aware of them.",
            "Another startup so far by a former colleague at CMU which was recently bought by Amazon, right?",
            "Not a big app tech which was not entirely bought, but the empty section boss bought by eBay.",
            "So many of these big companies who are actually doing business.",
            "Multilingual prisoners are actually buying some of these successful startups.",
            "Recently we saw Unbabel, which is interesting company from a former student at LTI who is using machine translation and crowd source post editing and another example which is interesting out coming out of Stanford by spend Screenshot De Niro Interactive Machine translation.",
            "So a lot of exciting stuff is going on."
        ],
        [
            "So let me summarize machine translation and winds of change, so I see another paradigm shift happening right now of going from SMT to neural empty, which still has a couple.",
            "You know major features of the same, but then there are new aspects that machine translation is taking an increasing share in the translation industry because there's just faster growth on the empty side then on the human translation side and empty has just become a reality in everyday life in the web, in in social media and so on, which results in the 200 times more birds are actually translated by machines than actually by humans."
        ],
        [
            "So winds of change exciting times for empty researchers, so it's worth sticking around for another couple of years in this area."
        ],
        [
            "And so I also want to use this opportunity to congratulate and give my best wishes to interact, and especially to Alex Waibel, who 15 years ago you know, invited me to join CMU.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I chose the topic machine translation, winds of change, and actually it's seems to be a good connection of what I already mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So winds of change.",
                    "label": 0
                },
                {
                    "sent": "People like Alex Waibel.",
                    "label": 0
                },
                {
                    "sent": "They may actually recognize the title.",
                    "label": 0
                },
                {
                    "sent": "So this oops, let's go back here.",
                    "label": 0
                },
                {
                    "sent": "Can we play?",
                    "label": 0
                },
                {
                    "sent": "No, it's not playing here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately not, so I wanted to play this for Alex.",
                    "label": 0
                },
                {
                    "sent": "I know for a fact that he likes to listen to Hotel California when he's flying his chopper.",
                    "label": 0
                },
                {
                    "sent": "I thought winds of change might be inappropriate song to to start this.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it's not playing right now, but many of you may actually know this song and it was at the time when actually there were systematic changes happening here in Germany.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So winds of change could also be called paradigm shift, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a title.",
                    "label": 0
                },
                {
                    "sent": "This is a term which was introduced by the Historian of Science Thomas Kuhn in his famous book The Structure of Scientific Revolutions to describe process and progress in Science.",
                    "label": 1
                },
                {
                    "sent": "So paradigm is essentially what the Community, the member of a certain scientific community have in common.",
                    "label": 0
                },
                {
                    "sent": "So this could be, you know, the examples in the taste textbooks, the techniques which are used to practices which are accepted and so on.",
                    "label": 0
                },
                {
                    "sent": "We could also say what can be actually published at the conference and when we look at conferences, the recent knuckle or the upcoming ACL, we see that actually machine translation Paradigm Shift is happening right now.",
                    "label": 0
                },
                {
                    "sent": "And we have heard already by Cai that this is already going into the into the companies so one.",
                    "label": 0
                },
                {
                    "sent": "Example of a paradigm shift which is always used in history of science is, you know, the shift in the heliocentric from the cheer centric to the heliocentric view of the universe.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have heard that there was rule based translation and then there was statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "So these numbers are lipid raft.",
                    "label": 0
                },
                {
                    "sent": "Obviously there is not always a shift which is happening within a few weeks and so on.",
                    "label": 0
                },
                {
                    "sent": "But when we look back so I started in machine translation in 95 late 95 in Akron and so at that time we started to work on statistical machine translation which was already published a couple of years earlier by researchers at IBM.",
                    "label": 0
                },
                {
                    "sent": "But then it took a couple of years.",
                    "label": 0
                },
                {
                    "sent": "Till it actually really took off right?",
                    "label": 0
                },
                {
                    "sent": "And so I indicated this.",
                    "label": 0
                },
                {
                    "sent": "We still have statistical machine translation as a major major paradigm, but it seems to be fading away.",
                    "label": 0
                },
                {
                    "sent": "We will see if it actually happens over the next couple of years.",
                    "label": 0
                },
                {
                    "sent": "And now this new LMT is really taking off again.",
                    "label": 0
                },
                {
                    "sent": "It just didn't pop out of nothing.",
                    "label": 0
                },
                {
                    "sent": "So there has been work going on.",
                    "label": 0
                },
                {
                    "sent": "But now what we see is really that the scientific community and obviously also companies are actually jumping on this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the old paradigm here is statistical machine translation, and as heimer mentioned, there had been different flavors of it.",
                    "label": 0
                },
                {
                    "sent": "The most prominent, probably the phrase based statistical translation.",
                    "label": 0
                },
                {
                    "sent": "And so I just.",
                    "label": 0
                },
                {
                    "sent": "Refer to this right now.",
                    "label": 0
                },
                {
                    "sent": "So there you train this alignment model which you have seen.",
                    "label": 0
                },
                {
                    "sent": "I think Hunts mentioned this extract phrase pairs or subtrees.",
                    "label": 0
                },
                {
                    "sent": "If we actually go to syntax based Service Commission translation we learn phrase or word reordering patterns.",
                    "label": 0
                },
                {
                    "sent": "We train N gram language models and then we use this entire inventory to actually translate your sentences.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, just as a few images, so we learn from parallel data from human translators examples, kind of.",
                    "label": 0
                },
                {
                    "sent": "Forwards between words on the source language on the target language.",
                    "label": 0
                },
                {
                    "sent": "When we have an entire sentence, then we find the best path of these alignments and then we can draw these.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tangles about these alignment points, and each of these rectangles is now actually a phrase pair which we can extract.",
                    "label": 0
                },
                {
                    "sent": "You know, overlap being embedded, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we get this entire list.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we use this when we have a new sentence we fill.",
                    "label": 0
                },
                {
                    "sent": "Now in all these partial translations and then we have essentially a craft search problem.",
                    "label": 0
                },
                {
                    "sent": "We need to find the best path within this graph.",
                    "label": 0
                },
                {
                    "sent": "And actually, it's not just monotone going through this graph, we can jump ahead and then jump back to fill some gaps which we left in earlier in earlier parts.",
                    "label": 0
                },
                {
                    "sent": "So this is this typical translation which was now the predominant paradigm over the past.",
                    "label": 0
                },
                {
                    "sent": "Let's say 1515 years.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is essentially data driven.",
                    "label": 0
                },
                {
                    "sent": "More data means better translations, which of course creates a problem in this low resource scenarios which hi, my boss was mentioning right and there's currently this project lower light going on which now addresses this particular issue.",
                    "label": 1
                },
                {
                    "sent": "There is, however, a serious problem with this statistical machine translation, so essentially each word is an island, right?",
                    "label": 1
                },
                {
                    "sent": "So if you have now related words like the colors green, blue, yellow and so on, so they have nothing.",
                    "label": 0
                },
                {
                    "sent": "In common inside of the system.",
                    "label": 0
                },
                {
                    "sent": "So essentially the system treats each word you know individually.",
                    "label": 0
                },
                {
                    "sent": "Even worse, when you have now relate a morphological variants of the same word, like here table and tables.",
                    "label": 0
                },
                {
                    "sent": "Inside this statistical empty system, they're just as different as 22 and cow, right?",
                    "label": 0
                },
                {
                    "sent": "Which is not very appropriate for dealing actually with languages.",
                    "label": 1
                },
                {
                    "sent": "And of course there have been some repair mechanisms like factored models, world class models.",
                    "label": 0
                },
                {
                    "sent": "Or the abstract meaning representation and so on which all try to address this fundamental problem in statistical machine translation to actually bring the meaning of the words together.",
                    "label": 0
                },
                {
                    "sent": "So when I say they are an island, you know for efficiency birds are mapped into numbers inside of the system, so it is really 17 and 99 and 220 which represent the individual words.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now there is a new paradigm.",
                    "label": 1
                },
                {
                    "sent": "This neural empty, right?",
                    "label": 0
                },
                {
                    "sent": "So here and of course you know it has predecessors and I will come back to this.",
                    "label": 0
                },
                {
                    "sent": "But what is happening now that essentially the scientific community is jumping at this neural empty right?",
                    "label": 0
                },
                {
                    "sent": "So here I have just probably the most simple example of an architecture for Newell empty.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that we feed in the words through a vector where we just switch on, you know one cell in the vector.",
                    "label": 0
                },
                {
                    "sent": "So this is the lowest roll.",
                    "label": 0
                },
                {
                    "sent": "Alright, so then these words are mapped now into a high dimensional space, not as high dimensional as actually the input, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have now a vocabulary of 200,000 words, I'm a map it down too.",
                    "label": 0
                },
                {
                    "sent": "500 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And so I tried to actually capture some generalization right and these mappings are now part of the training program.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that I actually find a representation of the words in a multi dimensional space such that related words come closer together in this page in this space.",
                    "label": 0
                },
                {
                    "sent": "So the next step is then actually a recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "So I accumulate the information as I feed in Word of the word.",
                    "label": 0
                },
                {
                    "sent": "I accumulate it.",
                    "label": 0
                },
                {
                    "sent": "Through these States and so the red state out there is not a representation of the entire sentence, so this is the encoding part.",
                    "label": 0
                },
                {
                    "sent": "So I first collect the information encoded into one state, which is not just, you know one one dot, but essentially also in high dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "Now from this I start to generate the words for the target language.",
                    "label": 0
                },
                {
                    "sent": "So this again is now step by step and now I can reuse the history.",
                    "label": 0
                },
                {
                    "sent": "So the word which I have seen before.",
                    "label": 0
                },
                {
                    "sent": "So I start out from this.",
                    "label": 0
                },
                {
                    "sent": "Representation which encodes the source sentence and I generate the target sentence by also looking at what I have generated so far until I have generated the entire sentence and this is now the decoder part right.",
                    "label": 0
                },
                {
                    "sent": "And of course this shows already one major problem with this kind of architecture I have to compress an entire sentence into this one state and then generate out of it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This new empty so the important parts are that actually the words are mapped into this high dimensional vector space.",
                    "label": 0
                },
                {
                    "sent": "This is called embeddings, and as I already mentioned hopefully words closely related to each other actually occupy spaces related closed in close to each other at this time can be extended to phrase level and sentence level.",
                    "label": 0
                },
                {
                    "sent": "So the neural empty is as data driven as the SMT, so it has the same flaw in a way you know that we need a lot of data to actually train it.",
                    "label": 0
                },
                {
                    "sent": "So right now we were experimenting under the evaluation, which is going on related to this low resource scenario to the Lorelei project, right?",
                    "label": 0
                },
                {
                    "sent": "And what are you doing when actually your training data covers only half of the words which you see now in your test data, right?",
                    "label": 0
                },
                {
                    "sent": "You start out with.",
                    "label": 0
                },
                {
                    "sent": "Every other word is unknown, and of course then you want to look into the related languages and so on.",
                    "label": 0
                },
                {
                    "sent": "And maybe there is great hope using neural empty to cover.",
                    "label": 0
                },
                {
                    "sent": "Now some of these aspects for low resource languages.",
                    "label": 0
                },
                {
                    "sent": "Right now what we experience and what we hear is you need lots of data to train the embeddings to train.",
                    "label": 0
                },
                {
                    "sent": "You know the Bates in this recurrent neural networks and so on.",
                    "label": 0
                },
                {
                    "sent": "So from this part it may not be there.",
                    "label": 0
                },
                {
                    "sent": "Actually this paradigm shift is really, you know, solving one of the problems which you have in the.",
                    "label": 0
                },
                {
                    "sent": "Machine translation",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to some extent it's like back to the future, you know.",
                    "label": 1
                },
                {
                    "sent": "Please go back to actually worked over translations.",
                    "label": 0
                },
                {
                    "sent": "This is what the original IBM translation approach was right, and then we made huge progress in statistical translation when we went to the phrase level to capture local local phenomena and so on.",
                    "label": 0
                },
                {
                    "sent": "So now we are feeding individual words into it into the neural network, right?",
                    "label": 0
                },
                {
                    "sent": "And of course people are working very hard and there's amazing progress in now overcoming some of these shortcomings and innovate, pulling in and capturing some of what has been learned over the last 15 years in statistical translation.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's not just standard show standard recurrent neural network, so it's short long term memory.",
                    "label": 0
                },
                {
                    "sent": "It's going in both directions.",
                    "label": 1
                },
                {
                    "sent": "Attention, mechanism and so on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course, even if we can talk in a way about paradigm shift, here you know.",
                    "label": 0
                },
                {
                    "sent": "There was always something which has happened in the past, right?",
                    "label": 0
                },
                {
                    "sent": "And it was already mentioned, so neural networks or at that time also under the under the term connectionism actually was happening 30 years ago and was a big big thing.",
                    "label": 0
                },
                {
                    "sent": "You know, there was a lot of hype about it.",
                    "label": 0
                },
                {
                    "sent": "I remember you know that I played at that time with the Stuart Stuttgart neural Network simulator on a parallel computer.",
                    "label": 0
                },
                {
                    "sent": "It was fun.",
                    "label": 0
                },
                {
                    "sent": "Then suddenly you know 60 * 4 * 4 Windows popped up.",
                    "label": 0
                },
                {
                    "sent": "In this simulator and we had to click them away 1 by 1 to clear the screen again.",
                    "label": 0
                },
                {
                    "sent": "So a lot of this has happened and actually it has been applied in language processing in different areas.",
                    "label": 0
                },
                {
                    "sent": "I just gave 3 examples here.",
                    "label": 0
                },
                {
                    "sent": "You know on the passing passing complex structures in speech recognition where Alex viable boss heavily involved with time delay neural networks, but then also in machine translation.",
                    "label": 0
                },
                {
                    "sent": "For example, the group in Valencia at the time.",
                    "label": 0
                },
                {
                    "sent": "Of course there were problems at the time.",
                    "label": 0
                },
                {
                    "sent": "Both in terms of limited hardware and also in terms of not yet fully developed mathematical underpinnings.",
                    "label": 0
                },
                {
                    "sent": "So there has been progress that now we can come back and do actually things which were not possible 30 years ago.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we look back at this picture right?",
                    "label": 0
                },
                {
                    "sent": "So the rule based, you know, just some thoughts I have about this, so the rule based translation, both expert driven.",
                    "label": 0
                },
                {
                    "sent": "You know they had the linguist too populated the dictionaries who wrote the rules, the transfer rules, the analysis and generation rules and so on, and words were handled as a bundle of features.",
                    "label": 0
                },
                {
                    "sent": "You know you have the description of the individual words, how they operate in the context of other words.",
                    "label": 0
                },
                {
                    "sent": "And so this achieves actually a pretty good level of generalization.",
                    "label": 1
                },
                {
                    "sent": "Then we move to the SMT, which is now data driven, and this was probably the most important part of the paradigm shift going on there.",
                    "label": 1
                },
                {
                    "sent": "But now the world was essentially an Atom, right?",
                    "label": 0
                },
                {
                    "sent": "Just like one number is different from another number, and we built now collecting huge bags of examples which we could reuse to translating sentences.",
                    "label": 0
                },
                {
                    "sent": "So when we are now going to Newell empty, it still data driven.",
                    "label": 0
                },
                {
                    "sent": "So whatever is good from data driven, whatever is bad from day to turn.",
                    "label": 0
                },
                {
                    "sent": "I think we are carried over.",
                    "label": 0
                },
                {
                    "sent": "But now the word is not just this one number.",
                    "label": 0
                },
                {
                    "sent": "We actually with this embedding, mapping the mapping them into a continuous space, and this allows for richer manipulation and richer representation of the content of of a sentence.",
                    "label": 0
                },
                {
                    "sent": "The level of generalization maybe not as high, but actually this learned it becomes.",
                    "label": 0
                },
                {
                    "sent": "Part of the learning mechanism, which is actually a good thing, and then you know we may expect that actually each phase in empty is shorter than the one before, so we don't know how long people will now be excited about the neural empty and then something new will come.",
                    "label": 0
                },
                {
                    "sent": "And maybe I'm still around at the time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If it happens fast enough, OK, so there's another dimension in icy wind of change, and this is in empty business, right?",
                    "label": 0
                },
                {
                    "sent": "So this again, in a way refers to what kind was presenting before, right?",
                    "label": 0
                },
                {
                    "sent": "So translation, not machine learning translation has grown to 40 billion dollar business and there is an annual growth of 7%.",
                    "label": 1
                },
                {
                    "sent": "Machine translation actually grows much faster, so it grows by 20 to 25.",
                    "label": 0
                },
                {
                    "sent": "Percent per year, right?",
                    "label": 0
                },
                {
                    "sent": "Observations we can make here.",
                    "label": 0
                },
                {
                    "sent": "Companies are really embracing.",
                    "label": 1
                },
                {
                    "sent": "Empty governments are adopting empty right?",
                    "label": 0
                },
                {
                    "sent": "So the European Parliament.",
                    "label": 1
                },
                {
                    "sent": "They have actually big efforts in pulling in.",
                    "label": 0
                },
                {
                    "sent": "Empty translators are often unhappily accepting the reality that there is an empty right and they are using it to some extent.",
                    "label": 1
                },
                {
                    "sent": "Translation memory providers, translation, memories on except the technology for human translators.",
                    "label": 0
                },
                {
                    "sent": "They all have now their back doors to the machine translation online or specifically turned.",
                    "label": 0
                },
                {
                    "sent": "And I would say machine translation is not so much replacing.",
                    "label": 1
                },
                {
                    "sent": "Actually the human translation, but it's covering.",
                    "label": 0
                },
                {
                    "sent": "Their translation was not done before.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we look at this, you know Google reported recently that they are translating 140 billion words a day, right?",
                    "label": 0
                },
                {
                    "sent": "We just heard you know, the two billion text translations on a daily basis from Facebook, which may be something like 20 thirty 40 billion words.",
                    "label": 1
                },
                {
                    "sent": "I guess I reported it not inverts, but in in postings.",
                    "label": 0
                },
                {
                    "sent": "Former colleague and friend from who's now at eBay couldn't come today.",
                    "label": 1
                },
                {
                    "sent": "He mentioned in private communication they would need 200,000 translators to translate their postings, which they translate.",
                    "label": 0
                },
                {
                    "sent": "This machine translation and I couldn't find any information from Microsoft, but Chris went.",
                    "label": 0
                },
                {
                    "sent": "Probably can tell us also numbers which are in the higher high billions of words translated everyday.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we look at it and this is really, you know, mind blowing to me.",
                    "label": 0
                },
                {
                    "sent": "So the small dot is how much humans translate the Queen.",
                    "label": 0
                },
                {
                    "sent": "Big plot is how much is already translated by machines, right?",
                    "label": 1
                },
                {
                    "sent": "And of course, the machine translation goes a little bit into the space where the human translation is happening, but most of it is actually where humans would never have translated before, like the Facebook postings.",
                    "label": 0
                },
                {
                    "sent": "I would not pay a translator to translate now the Bulgarian message of a colleague I'm interested in to understand, but I can use the Facebook translation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see also in the space of small companies you know a lot of startups are actually popping up over the last couple of years which are operating in the space of machine translation.",
                    "label": 0
                },
                {
                    "sent": "So there was a report just last year which mentioned within 10 years 190 translation startups were identified and they probably missed a couple of those.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a listing you know you cannot see it, but you see that every year you know even dozens of translation companies were actually start.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just some examples from close by mobile technologies which was bought by Facebook right which we just learned about started by Alex Waibel and Mobile Technologies, was famous for cheap, eager.",
                    "label": 1
                },
                {
                    "sent": "Probably, this is how Facebook became aware of them.",
                    "label": 0
                },
                {
                    "sent": "Another startup so far by a former colleague at CMU which was recently bought by Amazon, right?",
                    "label": 0
                },
                {
                    "sent": "Not a big app tech which was not entirely bought, but the empty section boss bought by eBay.",
                    "label": 0
                },
                {
                    "sent": "So many of these big companies who are actually doing business.",
                    "label": 0
                },
                {
                    "sent": "Multilingual prisoners are actually buying some of these successful startups.",
                    "label": 0
                },
                {
                    "sent": "Recently we saw Unbabel, which is interesting company from a former student at LTI who is using machine translation and crowd source post editing and another example which is interesting out coming out of Stanford by spend Screenshot De Niro Interactive Machine translation.",
                    "label": 0
                },
                {
                    "sent": "So a lot of exciting stuff is going on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me summarize machine translation and winds of change, so I see another paradigm shift happening right now of going from SMT to neural empty, which still has a couple.",
                    "label": 0
                },
                {
                    "sent": "You know major features of the same, but then there are new aspects that machine translation is taking an increasing share in the translation industry because there's just faster growth on the empty side then on the human translation side and empty has just become a reality in everyday life in the web, in in social media and so on, which results in the 200 times more birds are actually translated by machines than actually by humans.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So winds of change exciting times for empty researchers, so it's worth sticking around for another couple of years in this area.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I also want to use this opportunity to congratulate and give my best wishes to interact, and especially to Alex Waibel, who 15 years ago you know, invited me to join CMU.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}