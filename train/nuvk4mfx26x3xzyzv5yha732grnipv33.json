{
    "id": "nuvk4mfx26x3xzyzv5yha732grnipv33",
    "title": "Gaussian processes for Bayesian Filtering",
    "info": {
        "author": [
            "Dieter Fox, Department of Computer Science and Engineering, University of Washington"
        ],
        "published": "Aug. 3, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/rraa09_fox_gpbf/",
    "segmentation": [
        [
            "So I will talk about kind of Bayesian state estimation, robotics and how we can use regression techniques and specially Gaussian processes in this context.",
            "That's work.",
            "That's mostly been done by my grad students and Co who's sitting up there.",
            "Wait high up.",
            "Yeah, that's him.",
            "He's also a.",
            "He also has a poster later today and he's giving a talk at the main conference on Tuesday on related issues and also Brian Ferris did some work that I'll show that's related to wireless signal strength."
        ],
        [
            "So in general, based in filtering is.",
            "High level view is it's kind of probabilistic framework for estimating the state of a dynamical system, and this is one of the key problems in robotics.",
            "So a robot, if it has to operate in the real world, it has to figure out kind of what is going on around it, and base filters have been at the core of many key problems in robotics.",
            "So for example we have robot localization where the state of the system that the robot is interested in is where it is in the world.",
            "Also robot mapping.",
            "Now the state is a bit more complicated where you try to estimate not only where the robot is, but at the same time where different features or different entities in the world are people tracking is another important application example for basin filtering.",
            "Also more and more coming up activity recognition, and if you think about planning under uncertainty and one thing that often comes up in that context are these palm DP is partially observable Markov decision processes.",
            "And then in the sensor based filter is typically the core component of that system that you use to keep track of the state and then you use the distribution of that base filter to make planning into the future.",
            "Many different instantiations of these space filters, so the general framework is just you estimate posterior distributions based on observations and control information an.",
            "Now the question is how do you represent and infer those distributions and some of the key techniques are the classical.",
            "The common filter for the linear system.",
            "If you go into.",
            "Nonlinear environments or nonlinear dynamical systems, then techniques such as the extended Kalman filter.",
            "More recently, the unscented Kalman filter also particle filters have been used quite a bit and also grid based filters for innocence, you just discretize your continuous state and then apply techniques on that discrete version."
        ],
        [
            "So he is in sensor graphical model view of a base filter where S is the state of the system at time K. We have observations that are generated by the state and we have control and there are two key components of such a base filter and again, this is not pretty much independent of the application, whether it's mapping or people tracking or things like that.",
            "So we have a dynamic model or also often called the process model that describes what is the probability of the system being in state S at time K. Given the previous time step and the control exerted, for example by the robot and the other key component, is the observation model which gives you the probability distribution of the over the observation space given or conditioned on the current state of the system and the key aspect related to this workshop is that in a sense we're using regression techniques to learn those conditional probability distributions."
        ],
        [
            "Here's an example of of how often you arrive at those observation dynamics models.",
            "Is that you incense?",
            "Try to model the underlying physical process that generates the data.",
            "So for example, here on the left you see one version of a laser based model, so you have a laser beam sensor innocence.",
            "And let's assume you know how far the next obstacle is away from the robot conditioned on where the robot is in the environment.",
            "Then you can use such a model and say if this is the true distance to the obstacle.",
            "Then your measurement will give you kind of a distribution.",
            "The likelihood model will look like this, where of course the most likely reading at the expected distance with some Gaussian noise and then you could have other aspects that innocence have an impact on the readings you're going to receive.",
            "Typically how you get in these models is you collect a lot of training data and try to estimate the parameters of your system.",
            "On the right side you see one of the also kind of a typical robot motion model where we have the position of the robot at certain point in time, and then you have a parametric description how it moves to the next state OK, and then you add in most cases some kind of Gaussian noise to the different parameters of your model and that gives you then your probabilistic motion model.",
            "And how that fits now into the context of this."
        ],
        [
            "Basin filtering is here.",
            "You can see.",
            "Innocence is kind of the setup that often a common filter uses where you have at the input you have the previous state.",
            "You have control and then you have your process or dynamics model and in a sense you feed that as input to your model.",
            "Then you get a predicted state at time K you add some noise to the state at that time and that annoys can of course be time dependent and then you can use that state.",
            "To predict innocencia observation, you're going to see this is again, you can use a physical model for that, and then you have some noise on those sensor measurements too.",
            "The problems here are that often if you, if you build such a physical model that somehow it's really hard to come up with with an exact description of what is going on.",
            "So there's always some slide aspects that you might miss.",
            "OK, and even if you then optimize your model with respect to training data, there might be some aspects that you can't really model within that parametric framework.",
            "And also if you don't have in a sense like a simple laser beam that you try to model, but you have some very high dimensional data.",
            "So for example, a person wearing a sensor system that gives you for example IMU readings, accelerometer readings, light sensors and things like that, then it's really really hard to come up with a parametric model.",
            "And in that case often what people do is you extract some high dimensional feature space is an, then try to learn a prediction or observation model for those.",
            "And in that case, it's extremely hard to come a really with a physics based model.",
            "So in a sense, the idea."
        ],
        [
            "Of these, what we call cheapie based filters GPS sensor, again using Gaussian processes in the context of Bayesian filtering is what we replace the process model and the observation model just by in the sensor Gaussian process regression.",
            "System where we have here the GP Dynamics model and the observation model, and also because the Gaussian process gives you some estimates on the noise.",
            "That is, in the system, you can replace those and also by GP noise models."
        ],
        [
            "And the Gaussian processes have been applied over the last years quite a bit actually, especially in robotics.",
            "This is clearly not an exhaustive list, but just gives you some ideas.",
            "I think we did one early work here.",
            "Darcy's actually where we model wireless signal strength using Gaussian processes.",
            "Then Christian has done many applications over the last years.",
            "For example, in the course of failure detection or modeling gas distribution.",
            "Also young Peter stood some work recently on.",
            "Modeling the dynamics of a robotic arm using Gaussian processes and actually the whole work, I think was motivated by Andreas Crouser, who's going to give the next talk when I talk to him at ICML, was the first time I've heard about Gaussian processes, and I thought there might actually be cool application in the context of robotics, so he did this work mostly as you'll see in the context of sensor networks.",
            "So Gaussian process just briefly."
        ],
        [
            "The idea is in, that's exactly what Christian said, so we have an output that's a noisy function of some input vector X and epsilon is just some with some white guys calcium noise.",
            "Gaussian process in the sense if you want so a prior distribution over functions in that space.",
            "And the idea is that the outputs, which means that the Y the output of that function is the different output points are jointly Gaussian distributed.",
            "It's a zero mean Gaussian.",
            "But I'll show you some examples that you can kind of overcome that, and the covariance of that high dimensional Gaussian is given by some kernel function, which means in the sense that points that are close in input space should also be close in output space, and that kernel function is sense.",
            "Models how how you define the closeness incense in that input space and the kernel function has some parameters.",
            "This is just one.",
            "Their squared exponential kernel.",
            "There many different types of kernels you can imagine and people have applied in different contexts.",
            "The different parameters.",
            "So for example, how wide if you want so the Gaussian drop off is in that kernel function and the noise of the of the model.",
            "He'll show you see some, except you can in a sense, from a Gaussian prior you can innocence sample functions over that space if you want.",
            "So he is.",
            "Just here's some examples of different Gaussian process of functions sample from a prior with different what is called hyper parameters parameters for that kernel function.",
            "These pictures are taken from Chris bishops."
        ],
        [
            "Pattern recognition and machine learning book and you can see that you can get a pretty wide range of different functions.",
            "You can sample.",
            "So for example here in the upper right you can see that you have a highly varying function and here much more smooth functions OK. Now the key question is you're not really interested in those priors per say, but you're much more interested."
        ],
        [
            "And if you have training data from a system, which means you have pairs of input, output XY, then you can use.",
            "In that framework you can in a sense make predictions for new point that you have not yet seen, which means you have the training data up there and then you get a new input text are and you would like to generate a prediction for what the Why store should look like.",
            "And that innocence boils down to nothing else then conditioning a Gaussian.",
            "OK, and then you get a Gaussian distribution for the output over there twice star over that, why Val?"
        ],
        [
            "Let me just give you an example here.",
            "What that looks like, here's a 1 dimensional example where is the input.",
            "Here we have the output and let's assume we have data drawn from this sinusoidal.",
            "Of course, the cheapie doesn't know.",
            "So initially, in the sense that cheap you prior what it gives you for every point, you can then ask it what would be the prediction."
        ],
        [
            "Because it's zero mean initially of course you have.",
            "The mean function is the red one, and then you have this blue gives you the variance.",
            "And that again depends also on these hyperparameters.",
            "Now if we start conditioning in a sense on training points, then that function starts also adapting to those points.",
            "And you can also see that in areas where you have.",
            "Many data points or high data point density that the uncertainty of that system is also smaller.",
            "So here in that area in the center section you have a larger uncertainty and that uncertainty consists of two takes 2 pieces in a sense into account, one is the signal noise that you learn, kind of as a hyperparameter of the model and the other is just the data sparsity.",
            "Here's an example of the input, for example, that the kernel which has on the Gaussian process predictions.",
            "So what I'll do here is I'll show in a sense.",
            "How the cheapie prediction changes if you change the width of the kernel, which means if you make the process more and more smooth, OK and we don't change the data points that we condition on.",
            "So initially you can see that it's a very non smooth function, so it has a very small kernel width, which means neighboring points don't have much impact on each other and you can see that the process quickly as soon as you get away from data points.",
            "In a sense it goes to the.",
            "Zero mean prior again.",
            "Now if we increase the kernel rate.",
            "Then you can see that the function just becomes smooth and smoother.",
            "Of course, if you overdo it.",
            "Then Justin since learns the average of the data.",
            "If you want so OK at the same time, if I showed it again, but you can also look at if you look at the upper right, you can see the log likelihood of the data, so you can measure how well your current parameter setting approximates these data points.",
            "So if you look at this so the log likelihood actually.",
            "Increases.",
            "And now it decreases again.",
            "OK, so that's already a hint that you can estimate those so called hyper."
        ],
        [
            "It is from the data and typically what you do is you just maximize the data log likelihood.",
            "And how you do this is to figure out people uses something like a conjugate gradient descent.",
            "The problem is often that because it's a high dimensional nonlinear problem that innocence, you might find local minima doing that optimization, so it's not a convex function that's an issue.",
            "So where you might actually want to try different parameter settings, OK."
        ],
        [
            "So here is not one example where we applied that using wireless signature.",
            "So imagine you have a laptop OK and the laptop measures in sensor signal strength of different access points that you see.",
            "OK so this is.",
            "An example, so here is 4 one access point.",
            "We walked through the Town Center.",
            "Actually, the computer science building there through one floor, and imagine you log the locations you are at and in every location you store the signal strength for an access point OK and then what you get is such a function of the problem is that it's really hard to come up with a parametric model for signal strength just based on because it depends on so many things like the walls and different access points, how close they are.",
            "So people have done a lot like work in trying to discretize the space and then learned discrete approximation.",
            "We use these Gaussian processes over innocence again.",
            "You walk through the building and you log that data and then you just filter Gaussian process through their data and what you see here is then up there.",
            "The mean function and at the same time also the nice thing is it gives you for every point in the continuous space.",
            "It also gives you a variance estimation.",
            "So you can see that bump in the middle.",
            "Does anybody have an idea where the bump in the variance comes from?",
            "Do you think about the registration?",
            "So that's the atrium innocence.",
            "So we did that on the 5th floor and we couldn't collect.",
            "Of course, any data in the atrium itself, and so you see that gap in the middle.",
            "And that is also then results.",
            "Obviously, in a larger uncertainty.",
            "OK, so if you have such a model, it's actually trivial to incorporate that into, for example, a particle filter, right?",
            "So what you do is you have for example emotion model for a person OK, and then you run a particle filter and for every particle at every point in time you just in the sense used that learned model to predict.",
            "The signal strength you would see for each access point you currently see.",
            "OK, and then you compare that with the actual measurement, and again, the prediction doesn't only give you an expected measurement, but also an uncertainty which is really nice for the probabilistic weighting that you need in your particle filter."
        ],
        [
            "OK, so here's a location model.",
            "Then we used actually for the Town Center.",
            "For example, if we want to do tracking.",
            "So we have this kind of hybrid representation where we have the lines are kind of you have to walk along the hallways, right?",
            "Or the blue line is actually the elevator and things like that.",
            "You have different rooms."
        ],
        [
            "And he's just one tracking example we get out of this now.",
            "These are two different buildings.",
            "This is the on center here.",
            "That's Mary Gates Hall, and this is.",
            "The water pond is sometimes the fountain, not today, obviously.",
            "OK, so this is also part of the campus.",
            "It's not only one building.",
            "So.",
            "The red are the particles in the yellow.",
            "One is actually work there.",
            "Brian Paris it.",
            "So this is Brian.",
            "As you can see.",
            "Some noise, so he's exiting the building.",
            "And again, we just learned the model by him walking through through these areas, right?",
            "And then logging at certain points, clicking on a map where he is and then just linear interpolating in order to get the ground truth locations.",
            "So this is just one example of how you can really easily use those Gaussian processes in the Sense S observation models.",
            "Now, if it's a more general framework, are these?"
        ],
        [
            "The GP based filter.",
            "So in a sense what you do is the following.",
            "You have a system and let's assume you can collect ground truth training data, which means you can operate your system in its normal environment and you can collect innocence ground truth for the state of the system and you store the observations and the control information.",
            "So for example, for the Wi-Fi system you assume that you can know that you know the location of the person during the training run.",
            "OK, if you have that.",
            "Then, in a sense, from that sequence of ground truth states with observations you can easily extract training data for a Gaussian process that models the dynamics of the system and the observations of the system.",
            "OK, so now once you have those two Gaussian processes, you can then combine that or incorporate them into a base filter and I won't go into details here, but it's trying to just has reason paper on that in a RJ coming up, but in a sense, if you want to incorporate it into a particle filter then the Gaussian process you take a particle and you just sample that you take the Gaussian process prediction for this particle and the prediction also gives you noise and then you just sample from that Gaussian noise.",
            "And that gives your next particle state.",
            "OK, yeah.",
            "Are you going on vacation?",
            "We have two year.",
            "Just for the prediction, right?",
            "And then and then for that particle?",
            "If you want to get the weight of the observation again, you just use the GP to predict the observation and that gives you a Gaussian prediction and you compare to the true one.",
            "OK, yeah?",
            "Most.",
            "We've been doing.",
            "Your mom is so many things, yes.",
            "So you mean overtime?",
            "Michael.",
            "Patient had seen her in the meeting.",
            "Independent you are free.",
            "We don't deal with it in any specific way right now, so it's just just like in most in the sense of these models, we just have a really kind of temporally independent model.",
            "So what I want, yeah, good point.",
            "I think the next step and I'll talk about that in the end is actually and you hinted at that when you mentioned discriminative models, right?",
            "So like Andrew or Peter Beale.",
            "He has done work using discriminative learning of Kalman filters, and we've done work discriminative learning of particle filters and then you can actually train.",
            "I think the cheap parameters in the sense hopefully to optimize the performance of the filter, yeah.",
            "But we haven't done that yet.",
            "OK, the nice thing is you can also really easily connect these into for example extended Kalman filters where what you do is incense for your prediction.",
            "You just take the mean of Eureka F you predicted forward and then you get the noise from the Gaussian process that keeps your prediction noise and in order doing a Taylor series expansion you can actually compute easily the derivative of your GP and that gives you then your linearisation that you need for your EKF and you do the same thing for the observations OK.",
            "So in a sense, you just plug in two or three.",
            "I think it's 4 lines into your standard CKF algorithm and replace them by cheapie models.",
            "OK, and the same, it's even easier for the ukf because it doesn't even need derivatives.",
            "So in a sense, you just apply these Gaussian process models for the individual signal points, OK?"
        ],
        [
            "Here's some work that transcended some evaluations in the concept tracking up limp, so we had a blimp going through a mobile app and wanted to check that."
        ],
        [
            "Incense with the camera so it's just fun.",
            "Example of tracking with the ukf and what the cheapie learned is in a sense of prediction model for the blimp emotion model of the dynamics and an observation model, which means from the state to predict the shape of the lips that you would expect to see with the camera.",
            "I'm not advocating that you should that this is how you should do blimp tracking.",
            "OK. Not going to get me that easily.",
            "This is just an example of trying to see a true value at kind of different aspects of these models and then."
        ],
        [
            "We did some comparisons to a parametric model and the parametric model was actually.",
            "I I don't know all the details right now, but it's a pretty sophisticated parametric model for a blimp.",
            "OK, so this is was developed by some people in error Astro and we took some training data in the environment and then learn the parameters of their dynamics model so it sort of models kind of things like thrust, drag and take those pieces into account and then also trained our GP and you can see in the sense that.",
            "You get some reduction of causing the error becausw.",
            "Still the parametric model doesn't take everything into account.",
            "That really impacts the motion of that blimp and the GP if you give it enough training data then it can actually give you a pretty good results.",
            "A key problem of GPS is of course that I'm online that you need to spend, so it's especially if you look at the particular because you have to run cheap prediction which is quadratic in the number of training data points for every particle.",
            "So it's actually not very efficient.",
            "And we can talk about that either at the panel or the end of my talk.",
            "We also showed, of course the best you can do, and I think you should actually do is you can combine the parametric model with the Gaussian process model.",
            "So what you do is you optimize your parametric model and then you don't have a Gaussian process.",
            "Zero mean Gaussian process.",
            "But the parametric model becomes the mean of your cheapie.",
            "OK.",
            "So what the cheapie then does is it learns in a sense the residual that your parametric model doesn't really take into account.",
            "I think that is actually the best combination.",
            "OK, so here's an example."
        ],
        [
            "Where we tried the following, we removed from the training data all the data points where the blimp did a right turn.",
            "OK, and then we we.",
            "We tracked the blimp and in the tracking data we had a right turn.",
            "OK, so look at the lower one here.",
            "This is the section where the blimp turn to the right.",
            "Now the problem that GP has never seen anything like a right turn.",
            "So what happens here is innocence pretty nicely that automatically.",
            "The uncertainty of your unscented Kalman filter in this case actually increases because the cheap is aware of that data sparsity.",
            "OK, so I think that's actually a pretty nice aspect of these models."
        ],
        [
            "Now let me just very quickly, because that's actually going to be Jonathan's talk on Tuesday is the problem.",
            "So far, we assume that in your training data you have ground truth data for the state of the system, but you, for example, for a blimp you don't always have a vicon motion capture lab, right?",
            "So what do you do if you don't have ground truth training data and the so there could be situations where you have maybe some sparse labels?",
            "You might know the state of the system at some sparse points in time.",
            "Or you might just have still noise in your data itself, and the idea is.",
            "And this is based very much on Neil Lawrence work using Gaussian process latent variable models that he introduced for using GPS for dimensionality reduction is you don't only optimize over your hyperparameters, but you also optimize over the latent points in your training data OK?",
            "Brian Ferris did some work where you can then, in a sense, use it."
        ],
        [
            "For wireless signal slam, where you in a sense you don't know the positions of the person as she's walking around."
        ],
        [
            "The problem with the GPL VM's is that they don't take into account the temporal sequence of the data and in the context of computer graphics and animation.",
            "Aaron Hartsman scripted some really nice work on innocence, combining GP LVM with the dynamics model that takes really the temporal nature of the data into account.",
            "And it also learns a dynamic."
        ],
        [
            "This model then.",
            "Now the what we did and again on Tuesday is we introduced in a sense these cheap for learning framework where you can in a sense learn Gaussian process based filter that takes also the control information into account and it doesn't require incense ground truth labels for your training data.",
            "And there are different settings.",
            "For example you could again have sparse labels.",
            "You could have noisy labels or there are cases where you don't have labels at all.",
            "OK."
        ],
        [
            "So here's an example application that we did so if you do a slot car that we drive around here on that Carrera track and we put an IMU on the car.",
            "And then the input data."
        ],
        [
            "Here is control, which is kind of how fast you want the car to go.",
            "And here you see in a sense, what we measure is from the IMU, the change in heading off the car from one time step to the other.",
            "OK, so it's not only kind of the change in orientation, but also if it goes through a bank curve in those kind of things right?",
            "Then you can register that.",
            "The goal was then to evaluate like how we can work with different setups for different labels, and one interesting piece is actually what you do."
        ],
        [
            "If you can learn a Gaussian process of base filter.",
            "If this is the only data you have, which means you have zero labels or anything like that, and that is something called in the control Community subspace identification.",
            "And what you do is you take that data and for example, this is a technique that is a linear model and then you learn a lower dimensional embedding for latent space.",
            "OK, so you learn a latent space such that it optimally models your dynamical system.",
            "OK, this is in a linear common filter framework and this is in a sense the latent space at our model then learns in order to generate the data that you just saw.",
            "OK, so in a sense you now have your car in a sense moving through this.",
            "2 dimensional latent space.",
            "OK, and you can also do tracking in that."
        ],
        [
            "Space we also do automatic time alignment, which means if you have different runs through the trace, OK, the blue the different curves are different.",
            "Kind of runs through the through the track OK, and they're not time aligned, so they have different durations because it's not the same speed at every time.",
            "Then after optimization of this Gaussian process model, we actually get all these traces really nicely on top of each other, which means time aligned.",
            "And you can use it, for example then for replay.",
            "Of trajectories, Peter Beale did some very nice work."
        ],
        [
            "In the context of helicopters, there also issues and I'm running, oh, I still have two 3 minutes.",
            "With so in a sense, this is totally independent of what kind of things you do to your Gaussian process.",
            "OK, so far we only talked.",
            "It was very kind of plain generic Gaussian processes.",
            "You can also go far beyond that.",
            "So for example, you can do heteroscedastic GPS when they sense the noise is right now the noise in the sense is independent of the state, but you can also learn and state dependent noise systems.",
            "You can make them much far more efficient by doing using sparse Gaussian processes, specially the NIPS community has developed many of those techniques where the idea is either you learn.",
            "Of course not an optimal, but you learn a subset of data points such that you're cheap.",
            "He still gives you a very good prediction, and then of course, because your prediction later depends on the size of the data set, you get much more efficient prediction results.",
            "OK, instead of subsets, you can also innocence learn the positions of points, optimal position, and if you want to go beyond just in continuous systems, then you can you just go to some of its called.",
            "Also, process classification is a lot of literature in that, and actually Christian did some work where he used GPS."
        ],
        [
            "That context to summarize, I think on the positive Gaussian process it's really a nice kind of flexible modeling framework because it's nonparametric technique.",
            "Key.",
            "Another key advantage is really that they take the noise and the data noise and the uncertainty due to data sparsity into account.",
            "OK, problem is clearly the efficiency right?",
            "But there are some remedies to that.",
            "I think actually the real key is you want to combine those with parametric models, right?",
            "So if you have a rough parametric model of your system, you should not throw that away and try to learn everything with the GP that you can write, you should model as much as you can.",
            "And then maybe if there are still things that you really need to take into account to increase performance, then you should learn the residual with the GP.",
            "I think.",
            "So in a sense, maybe one of the take home message that I think is also that these things should really only be used if you need them, right?",
            "If you don't need it or you have a parametric model, it doesn't make sense to waste all the computational complexities on using GPS, but I think there are many applications where it does make sense to apply these nonparametric regression techniques.",
            "And I can imagine you can get maybe on the 1st part, similar results with with the kind of K means related techniques.",
            "But the nice thing also about the Gaussian processes in combination with this latent variable model that you just have one nice framework for handling all the connections in the data.",
            "And with that, I think amount of time and thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will talk about kind of Bayesian state estimation, robotics and how we can use regression techniques and specially Gaussian processes in this context.",
                    "label": 0
                },
                {
                    "sent": "That's work.",
                    "label": 0
                },
                {
                    "sent": "That's mostly been done by my grad students and Co who's sitting up there.",
                    "label": 0
                },
                {
                    "sent": "Wait high up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's him.",
                    "label": 0
                },
                {
                    "sent": "He's also a.",
                    "label": 0
                },
                {
                    "sent": "He also has a poster later today and he's giving a talk at the main conference on Tuesday on related issues and also Brian Ferris did some work that I'll show that's related to wireless signal strength.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general, based in filtering is.",
                    "label": 0
                },
                {
                    "sent": "High level view is it's kind of probabilistic framework for estimating the state of a dynamical system, and this is one of the key problems in robotics.",
                    "label": 0
                },
                {
                    "sent": "So a robot, if it has to operate in the real world, it has to figure out kind of what is going on around it, and base filters have been at the core of many key problems in robotics.",
                    "label": 0
                },
                {
                    "sent": "So for example we have robot localization where the state of the system that the robot is interested in is where it is in the world.",
                    "label": 0
                },
                {
                    "sent": "Also robot mapping.",
                    "label": 0
                },
                {
                    "sent": "Now the state is a bit more complicated where you try to estimate not only where the robot is, but at the same time where different features or different entities in the world are people tracking is another important application example for basin filtering.",
                    "label": 0
                },
                {
                    "sent": "Also more and more coming up activity recognition, and if you think about planning under uncertainty and one thing that often comes up in that context are these palm DP is partially observable Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "And then in the sensor based filter is typically the core component of that system that you use to keep track of the state and then you use the distribution of that base filter to make planning into the future.",
                    "label": 0
                },
                {
                    "sent": "Many different instantiations of these space filters, so the general framework is just you estimate posterior distributions based on observations and control information an.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how do you represent and infer those distributions and some of the key techniques are the classical.",
                    "label": 0
                },
                {
                    "sent": "The common filter for the linear system.",
                    "label": 0
                },
                {
                    "sent": "If you go into.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear environments or nonlinear dynamical systems, then techniques such as the extended Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "More recently, the unscented Kalman filter also particle filters have been used quite a bit and also grid based filters for innocence, you just discretize your continuous state and then apply techniques on that discrete version.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he is in sensor graphical model view of a base filter where S is the state of the system at time K. We have observations that are generated by the state and we have control and there are two key components of such a base filter and again, this is not pretty much independent of the application, whether it's mapping or people tracking or things like that.",
                    "label": 0
                },
                {
                    "sent": "So we have a dynamic model or also often called the process model that describes what is the probability of the system being in state S at time K. Given the previous time step and the control exerted, for example by the robot and the other key component, is the observation model which gives you the probability distribution of the over the observation space given or conditioned on the current state of the system and the key aspect related to this workshop is that in a sense we're using regression techniques to learn those conditional probability distributions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of of how often you arrive at those observation dynamics models.",
                    "label": 0
                },
                {
                    "sent": "Is that you incense?",
                    "label": 0
                },
                {
                    "sent": "Try to model the underlying physical process that generates the data.",
                    "label": 0
                },
                {
                    "sent": "So for example, here on the left you see one version of a laser based model, so you have a laser beam sensor innocence.",
                    "label": 0
                },
                {
                    "sent": "And let's assume you know how far the next obstacle is away from the robot conditioned on where the robot is in the environment.",
                    "label": 0
                },
                {
                    "sent": "Then you can use such a model and say if this is the true distance to the obstacle.",
                    "label": 0
                },
                {
                    "sent": "Then your measurement will give you kind of a distribution.",
                    "label": 0
                },
                {
                    "sent": "The likelihood model will look like this, where of course the most likely reading at the expected distance with some Gaussian noise and then you could have other aspects that innocence have an impact on the readings you're going to receive.",
                    "label": 0
                },
                {
                    "sent": "Typically how you get in these models is you collect a lot of training data and try to estimate the parameters of your system.",
                    "label": 0
                },
                {
                    "sent": "On the right side you see one of the also kind of a typical robot motion model where we have the position of the robot at certain point in time, and then you have a parametric description how it moves to the next state OK, and then you add in most cases some kind of Gaussian noise to the different parameters of your model and that gives you then your probabilistic motion model.",
                    "label": 0
                },
                {
                    "sent": "And how that fits now into the context of this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basin filtering is here.",
                    "label": 0
                },
                {
                    "sent": "You can see.",
                    "label": 0
                },
                {
                    "sent": "Innocence is kind of the setup that often a common filter uses where you have at the input you have the previous state.",
                    "label": 0
                },
                {
                    "sent": "You have control and then you have your process or dynamics model and in a sense you feed that as input to your model.",
                    "label": 0
                },
                {
                    "sent": "Then you get a predicted state at time K you add some noise to the state at that time and that annoys can of course be time dependent and then you can use that state.",
                    "label": 0
                },
                {
                    "sent": "To predict innocencia observation, you're going to see this is again, you can use a physical model for that, and then you have some noise on those sensor measurements too.",
                    "label": 0
                },
                {
                    "sent": "The problems here are that often if you, if you build such a physical model that somehow it's really hard to come up with with an exact description of what is going on.",
                    "label": 0
                },
                {
                    "sent": "So there's always some slide aspects that you might miss.",
                    "label": 0
                },
                {
                    "sent": "OK, and even if you then optimize your model with respect to training data, there might be some aspects that you can't really model within that parametric framework.",
                    "label": 0
                },
                {
                    "sent": "And also if you don't have in a sense like a simple laser beam that you try to model, but you have some very high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So for example, a person wearing a sensor system that gives you for example IMU readings, accelerometer readings, light sensors and things like that, then it's really really hard to come up with a parametric model.",
                    "label": 0
                },
                {
                    "sent": "And in that case often what people do is you extract some high dimensional feature space is an, then try to learn a prediction or observation model for those.",
                    "label": 0
                },
                {
                    "sent": "And in that case, it's extremely hard to come a really with a physics based model.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, the idea.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of these, what we call cheapie based filters GPS sensor, again using Gaussian processes in the context of Bayesian filtering is what we replace the process model and the observation model just by in the sensor Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "System where we have here the GP Dynamics model and the observation model, and also because the Gaussian process gives you some estimates on the noise.",
                    "label": 0
                },
                {
                    "sent": "That is, in the system, you can replace those and also by GP noise models.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the Gaussian processes have been applied over the last years quite a bit actually, especially in robotics.",
                    "label": 1
                },
                {
                    "sent": "This is clearly not an exhaustive list, but just gives you some ideas.",
                    "label": 0
                },
                {
                    "sent": "I think we did one early work here.",
                    "label": 0
                },
                {
                    "sent": "Darcy's actually where we model wireless signal strength using Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Then Christian has done many applications over the last years.",
                    "label": 0
                },
                {
                    "sent": "For example, in the course of failure detection or modeling gas distribution.",
                    "label": 0
                },
                {
                    "sent": "Also young Peter stood some work recently on.",
                    "label": 0
                },
                {
                    "sent": "Modeling the dynamics of a robotic arm using Gaussian processes and actually the whole work, I think was motivated by Andreas Crouser, who's going to give the next talk when I talk to him at ICML, was the first time I've heard about Gaussian processes, and I thought there might actually be cool application in the context of robotics, so he did this work mostly as you'll see in the context of sensor networks.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process just briefly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is in, that's exactly what Christian said, so we have an output that's a noisy function of some input vector X and epsilon is just some with some white guys calcium noise.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process in the sense if you want so a prior distribution over functions in that space.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that the outputs, which means that the Y the output of that function is the different output points are jointly Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "It's a zero mean Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But I'll show you some examples that you can kind of overcome that, and the covariance of that high dimensional Gaussian is given by some kernel function, which means in the sense that points that are close in input space should also be close in output space, and that kernel function is sense.",
                    "label": 0
                },
                {
                    "sent": "Models how how you define the closeness incense in that input space and the kernel function has some parameters.",
                    "label": 0
                },
                {
                    "sent": "This is just one.",
                    "label": 0
                },
                {
                    "sent": "Their squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "There many different types of kernels you can imagine and people have applied in different contexts.",
                    "label": 0
                },
                {
                    "sent": "The different parameters.",
                    "label": 0
                },
                {
                    "sent": "So for example, how wide if you want so the Gaussian drop off is in that kernel function and the noise of the of the model.",
                    "label": 0
                },
                {
                    "sent": "He'll show you see some, except you can in a sense, from a Gaussian prior you can innocence sample functions over that space if you want.",
                    "label": 0
                },
                {
                    "sent": "So he is.",
                    "label": 0
                },
                {
                    "sent": "Just here's some examples of different Gaussian process of functions sample from a prior with different what is called hyper parameters parameters for that kernel function.",
                    "label": 0
                },
                {
                    "sent": "These pictures are taken from Chris bishops.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pattern recognition and machine learning book and you can see that you can get a pretty wide range of different functions.",
                    "label": 0
                },
                {
                    "sent": "You can sample.",
                    "label": 0
                },
                {
                    "sent": "So for example here in the upper right you can see that you have a highly varying function and here much more smooth functions OK. Now the key question is you're not really interested in those priors per say, but you're much more interested.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you have training data from a system, which means you have pairs of input, output XY, then you can use.",
                    "label": 0
                },
                {
                    "sent": "In that framework you can in a sense make predictions for new point that you have not yet seen, which means you have the training data up there and then you get a new input text are and you would like to generate a prediction for what the Why store should look like.",
                    "label": 0
                },
                {
                    "sent": "And that innocence boils down to nothing else then conditioning a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you get a Gaussian distribution for the output over there twice star over that, why Val?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just give you an example here.",
                    "label": 0
                },
                {
                    "sent": "What that looks like, here's a 1 dimensional example where is the input.",
                    "label": 0
                },
                {
                    "sent": "Here we have the output and let's assume we have data drawn from this sinusoidal.",
                    "label": 0
                },
                {
                    "sent": "Of course, the cheapie doesn't know.",
                    "label": 0
                },
                {
                    "sent": "So initially, in the sense that cheap you prior what it gives you for every point, you can then ask it what would be the prediction.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because it's zero mean initially of course you have.",
                    "label": 0
                },
                {
                    "sent": "The mean function is the red one, and then you have this blue gives you the variance.",
                    "label": 0
                },
                {
                    "sent": "And that again depends also on these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Now if we start conditioning in a sense on training points, then that function starts also adapting to those points.",
                    "label": 0
                },
                {
                    "sent": "And you can also see that in areas where you have.",
                    "label": 0
                },
                {
                    "sent": "Many data points or high data point density that the uncertainty of that system is also smaller.",
                    "label": 0
                },
                {
                    "sent": "So here in that area in the center section you have a larger uncertainty and that uncertainty consists of two takes 2 pieces in a sense into account, one is the signal noise that you learn, kind of as a hyperparameter of the model and the other is just the data sparsity.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of the input, for example, that the kernel which has on the Gaussian process predictions.",
                    "label": 0
                },
                {
                    "sent": "So what I'll do here is I'll show in a sense.",
                    "label": 0
                },
                {
                    "sent": "How the cheapie prediction changes if you change the width of the kernel, which means if you make the process more and more smooth, OK and we don't change the data points that we condition on.",
                    "label": 0
                },
                {
                    "sent": "So initially you can see that it's a very non smooth function, so it has a very small kernel width, which means neighboring points don't have much impact on each other and you can see that the process quickly as soon as you get away from data points.",
                    "label": 0
                },
                {
                    "sent": "In a sense it goes to the.",
                    "label": 0
                },
                {
                    "sent": "Zero mean prior again.",
                    "label": 0
                },
                {
                    "sent": "Now if we increase the kernel rate.",
                    "label": 0
                },
                {
                    "sent": "Then you can see that the function just becomes smooth and smoother.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you overdo it.",
                    "label": 0
                },
                {
                    "sent": "Then Justin since learns the average of the data.",
                    "label": 0
                },
                {
                    "sent": "If you want so OK at the same time, if I showed it again, but you can also look at if you look at the upper right, you can see the log likelihood of the data, so you can measure how well your current parameter setting approximates these data points.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this so the log likelihood actually.",
                    "label": 0
                },
                {
                    "sent": "Increases.",
                    "label": 0
                },
                {
                    "sent": "And now it decreases again.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's already a hint that you can estimate those so called hyper.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is from the data and typically what you do is you just maximize the data log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And how you do this is to figure out people uses something like a conjugate gradient descent.",
                    "label": 0
                },
                {
                    "sent": "The problem is often that because it's a high dimensional nonlinear problem that innocence, you might find local minima doing that optimization, so it's not a convex function that's an issue.",
                    "label": 0
                },
                {
                    "sent": "So where you might actually want to try different parameter settings, OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is not one example where we applied that using wireless signature.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a laptop OK and the laptop measures in sensor signal strength of different access points that you see.",
                    "label": 0
                },
                {
                    "sent": "OK so this is.",
                    "label": 0
                },
                {
                    "sent": "An example, so here is 4 one access point.",
                    "label": 0
                },
                {
                    "sent": "We walked through the Town Center.",
                    "label": 0
                },
                {
                    "sent": "Actually, the computer science building there through one floor, and imagine you log the locations you are at and in every location you store the signal strength for an access point OK and then what you get is such a function of the problem is that it's really hard to come up with a parametric model for signal strength just based on because it depends on so many things like the walls and different access points, how close they are.",
                    "label": 0
                },
                {
                    "sent": "So people have done a lot like work in trying to discretize the space and then learned discrete approximation.",
                    "label": 0
                },
                {
                    "sent": "We use these Gaussian processes over innocence again.",
                    "label": 0
                },
                {
                    "sent": "You walk through the building and you log that data and then you just filter Gaussian process through their data and what you see here is then up there.",
                    "label": 0
                },
                {
                    "sent": "The mean function and at the same time also the nice thing is it gives you for every point in the continuous space.",
                    "label": 0
                },
                {
                    "sent": "It also gives you a variance estimation.",
                    "label": 0
                },
                {
                    "sent": "So you can see that bump in the middle.",
                    "label": 0
                },
                {
                    "sent": "Does anybody have an idea where the bump in the variance comes from?",
                    "label": 0
                },
                {
                    "sent": "Do you think about the registration?",
                    "label": 0
                },
                {
                    "sent": "So that's the atrium innocence.",
                    "label": 0
                },
                {
                    "sent": "So we did that on the 5th floor and we couldn't collect.",
                    "label": 0
                },
                {
                    "sent": "Of course, any data in the atrium itself, and so you see that gap in the middle.",
                    "label": 0
                },
                {
                    "sent": "And that is also then results.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in a larger uncertainty.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have such a model, it's actually trivial to incorporate that into, for example, a particle filter, right?",
                    "label": 0
                },
                {
                    "sent": "So what you do is you have for example emotion model for a person OK, and then you run a particle filter and for every particle at every point in time you just in the sense used that learned model to predict.",
                    "label": 0
                },
                {
                    "sent": "The signal strength you would see for each access point you currently see.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you compare that with the actual measurement, and again, the prediction doesn't only give you an expected measurement, but also an uncertainty which is really nice for the probabilistic weighting that you need in your particle filter.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a location model.",
                    "label": 0
                },
                {
                    "sent": "Then we used actually for the Town Center.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to do tracking.",
                    "label": 0
                },
                {
                    "sent": "So we have this kind of hybrid representation where we have the lines are kind of you have to walk along the hallways, right?",
                    "label": 0
                },
                {
                    "sent": "Or the blue line is actually the elevator and things like that.",
                    "label": 0
                },
                {
                    "sent": "You have different rooms.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he's just one tracking example we get out of this now.",
                    "label": 0
                },
                {
                    "sent": "These are two different buildings.",
                    "label": 0
                },
                {
                    "sent": "This is the on center here.",
                    "label": 0
                },
                {
                    "sent": "That's Mary Gates Hall, and this is.",
                    "label": 0
                },
                {
                    "sent": "The water pond is sometimes the fountain, not today, obviously.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is also part of the campus.",
                    "label": 0
                },
                {
                    "sent": "It's not only one building.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The red are the particles in the yellow.",
                    "label": 0
                },
                {
                    "sent": "One is actually work there.",
                    "label": 0
                },
                {
                    "sent": "Brian Paris it.",
                    "label": 0
                },
                {
                    "sent": "So this is Brian.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "Some noise, so he's exiting the building.",
                    "label": 0
                },
                {
                    "sent": "And again, we just learned the model by him walking through through these areas, right?",
                    "label": 0
                },
                {
                    "sent": "And then logging at certain points, clicking on a map where he is and then just linear interpolating in order to get the ground truth locations.",
                    "label": 0
                },
                {
                    "sent": "So this is just one example of how you can really easily use those Gaussian processes in the Sense S observation models.",
                    "label": 0
                },
                {
                    "sent": "Now, if it's a more general framework, are these?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The GP based filter.",
                    "label": 0
                },
                {
                    "sent": "So in a sense what you do is the following.",
                    "label": 0
                },
                {
                    "sent": "You have a system and let's assume you can collect ground truth training data, which means you can operate your system in its normal environment and you can collect innocence ground truth for the state of the system and you store the observations and the control information.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the Wi-Fi system you assume that you can know that you know the location of the person during the training run.",
                    "label": 0
                },
                {
                    "sent": "OK, if you have that.",
                    "label": 0
                },
                {
                    "sent": "Then, in a sense, from that sequence of ground truth states with observations you can easily extract training data for a Gaussian process that models the dynamics of the system and the observations of the system.",
                    "label": 0
                },
                {
                    "sent": "OK, so now once you have those two Gaussian processes, you can then combine that or incorporate them into a base filter and I won't go into details here, but it's trying to just has reason paper on that in a RJ coming up, but in a sense, if you want to incorporate it into a particle filter then the Gaussian process you take a particle and you just sample that you take the Gaussian process prediction for this particle and the prediction also gives you noise and then you just sample from that Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "And that gives your next particle state.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Are you going on vacation?",
                    "label": 0
                },
                {
                    "sent": "We have two year.",
                    "label": 0
                },
                {
                    "sent": "Just for the prediction, right?",
                    "label": 0
                },
                {
                    "sent": "And then and then for that particle?",
                    "label": 0
                },
                {
                    "sent": "If you want to get the weight of the observation again, you just use the GP to predict the observation and that gives you a Gaussian prediction and you compare to the true one.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah?",
                    "label": 0
                },
                {
                    "sent": "Most.",
                    "label": 0
                },
                {
                    "sent": "We've been doing.",
                    "label": 0
                },
                {
                    "sent": "Your mom is so many things, yes.",
                    "label": 0
                },
                {
                    "sent": "So you mean overtime?",
                    "label": 0
                },
                {
                    "sent": "Michael.",
                    "label": 0
                },
                {
                    "sent": "Patient had seen her in the meeting.",
                    "label": 0
                },
                {
                    "sent": "Independent you are free.",
                    "label": 0
                },
                {
                    "sent": "We don't deal with it in any specific way right now, so it's just just like in most in the sense of these models, we just have a really kind of temporally independent model.",
                    "label": 0
                },
                {
                    "sent": "So what I want, yeah, good point.",
                    "label": 0
                },
                {
                    "sent": "I think the next step and I'll talk about that in the end is actually and you hinted at that when you mentioned discriminative models, right?",
                    "label": 0
                },
                {
                    "sent": "So like Andrew or Peter Beale.",
                    "label": 0
                },
                {
                    "sent": "He has done work using discriminative learning of Kalman filters, and we've done work discriminative learning of particle filters and then you can actually train.",
                    "label": 0
                },
                {
                    "sent": "I think the cheap parameters in the sense hopefully to optimize the performance of the filter, yeah.",
                    "label": 0
                },
                {
                    "sent": "But we haven't done that yet.",
                    "label": 0
                },
                {
                    "sent": "OK, the nice thing is you can also really easily connect these into for example extended Kalman filters where what you do is incense for your prediction.",
                    "label": 0
                },
                {
                    "sent": "You just take the mean of Eureka F you predicted forward and then you get the noise from the Gaussian process that keeps your prediction noise and in order doing a Taylor series expansion you can actually compute easily the derivative of your GP and that gives you then your linearisation that you need for your EKF and you do the same thing for the observations OK.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, you just plug in two or three.",
                    "label": 0
                },
                {
                    "sent": "I think it's 4 lines into your standard CKF algorithm and replace them by cheapie models.",
                    "label": 0
                },
                {
                    "sent": "OK, and the same, it's even easier for the ukf because it doesn't even need derivatives.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, you just apply these Gaussian process models for the individual signal points, OK?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some work that transcended some evaluations in the concept tracking up limp, so we had a blimp going through a mobile app and wanted to check that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incense with the camera so it's just fun.",
                    "label": 0
                },
                {
                    "sent": "Example of tracking with the ukf and what the cheapie learned is in a sense of prediction model for the blimp emotion model of the dynamics and an observation model, which means from the state to predict the shape of the lips that you would expect to see with the camera.",
                    "label": 0
                },
                {
                    "sent": "I'm not advocating that you should that this is how you should do blimp tracking.",
                    "label": 0
                },
                {
                    "sent": "OK. Not going to get me that easily.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of trying to see a true value at kind of different aspects of these models and then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did some comparisons to a parametric model and the parametric model was actually.",
                    "label": 0
                },
                {
                    "sent": "I I don't know all the details right now, but it's a pretty sophisticated parametric model for a blimp.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is was developed by some people in error Astro and we took some training data in the environment and then learn the parameters of their dynamics model so it sort of models kind of things like thrust, drag and take those pieces into account and then also trained our GP and you can see in the sense that.",
                    "label": 0
                },
                {
                    "sent": "You get some reduction of causing the error becausw.",
                    "label": 0
                },
                {
                    "sent": "Still the parametric model doesn't take everything into account.",
                    "label": 1
                },
                {
                    "sent": "That really impacts the motion of that blimp and the GP if you give it enough training data then it can actually give you a pretty good results.",
                    "label": 0
                },
                {
                    "sent": "A key problem of GPS is of course that I'm online that you need to spend, so it's especially if you look at the particular because you have to run cheap prediction which is quadratic in the number of training data points for every particle.",
                    "label": 0
                },
                {
                    "sent": "So it's actually not very efficient.",
                    "label": 0
                },
                {
                    "sent": "And we can talk about that either at the panel or the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "We also showed, of course the best you can do, and I think you should actually do is you can combine the parametric model with the Gaussian process model.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you optimize your parametric model and then you don't have a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Zero mean Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "But the parametric model becomes the mean of your cheapie.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what the cheapie then does is it learns in a sense the residual that your parametric model doesn't really take into account.",
                    "label": 0
                },
                {
                    "sent": "I think that is actually the best combination.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an example.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we tried the following, we removed from the training data all the data points where the blimp did a right turn.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we we.",
                    "label": 0
                },
                {
                    "sent": "We tracked the blimp and in the tracking data we had a right turn.",
                    "label": 0
                },
                {
                    "sent": "OK, so look at the lower one here.",
                    "label": 0
                },
                {
                    "sent": "This is the section where the blimp turn to the right.",
                    "label": 0
                },
                {
                    "sent": "Now the problem that GP has never seen anything like a right turn.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is innocence pretty nicely that automatically.",
                    "label": 0
                },
                {
                    "sent": "The uncertainty of your unscented Kalman filter in this case actually increases because the cheap is aware of that data sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that's actually a pretty nice aspect of these models.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let me just very quickly, because that's actually going to be Jonathan's talk on Tuesday is the problem.",
                    "label": 0
                },
                {
                    "sent": "So far, we assume that in your training data you have ground truth data for the state of the system, but you, for example, for a blimp you don't always have a vicon motion capture lab, right?",
                    "label": 0
                },
                {
                    "sent": "So what do you do if you don't have ground truth training data and the so there could be situations where you have maybe some sparse labels?",
                    "label": 0
                },
                {
                    "sent": "You might know the state of the system at some sparse points in time.",
                    "label": 0
                },
                {
                    "sent": "Or you might just have still noise in your data itself, and the idea is.",
                    "label": 0
                },
                {
                    "sent": "And this is based very much on Neil Lawrence work using Gaussian process latent variable models that he introduced for using GPS for dimensionality reduction is you don't only optimize over your hyperparameters, but you also optimize over the latent points in your training data OK?",
                    "label": 0
                },
                {
                    "sent": "Brian Ferris did some work where you can then, in a sense, use it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For wireless signal slam, where you in a sense you don't know the positions of the person as she's walking around.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem with the GPL VM's is that they don't take into account the temporal sequence of the data and in the context of computer graphics and animation.",
                    "label": 0
                },
                {
                    "sent": "Aaron Hartsman scripted some really nice work on innocence, combining GP LVM with the dynamics model that takes really the temporal nature of the data into account.",
                    "label": 0
                },
                {
                    "sent": "And it also learns a dynamic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model then.",
                    "label": 0
                },
                {
                    "sent": "Now the what we did and again on Tuesday is we introduced in a sense these cheap for learning framework where you can in a sense learn Gaussian process based filter that takes also the control information into account and it doesn't require incense ground truth labels for your training data.",
                    "label": 0
                },
                {
                    "sent": "And there are different settings.",
                    "label": 0
                },
                {
                    "sent": "For example you could again have sparse labels.",
                    "label": 0
                },
                {
                    "sent": "You could have noisy labels or there are cases where you don't have labels at all.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example application that we did so if you do a slot car that we drive around here on that Carrera track and we put an IMU on the car.",
                    "label": 0
                },
                {
                    "sent": "And then the input data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is control, which is kind of how fast you want the car to go.",
                    "label": 0
                },
                {
                    "sent": "And here you see in a sense, what we measure is from the IMU, the change in heading off the car from one time step to the other.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not only kind of the change in orientation, but also if it goes through a bank curve in those kind of things right?",
                    "label": 0
                },
                {
                    "sent": "Then you can register that.",
                    "label": 0
                },
                {
                    "sent": "The goal was then to evaluate like how we can work with different setups for different labels, and one interesting piece is actually what you do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you can learn a Gaussian process of base filter.",
                    "label": 0
                },
                {
                    "sent": "If this is the only data you have, which means you have zero labels or anything like that, and that is something called in the control Community subspace identification.",
                    "label": 0
                },
                {
                    "sent": "And what you do is you take that data and for example, this is a technique that is a linear model and then you learn a lower dimensional embedding for latent space.",
                    "label": 0
                },
                {
                    "sent": "OK, so you learn a latent space such that it optimally models your dynamical system.",
                    "label": 0
                },
                {
                    "sent": "OK, this is in a linear common filter framework and this is in a sense the latent space at our model then learns in order to generate the data that you just saw.",
                    "label": 0
                },
                {
                    "sent": "OK, so in a sense you now have your car in a sense moving through this.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional latent space.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can also do tracking in that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space we also do automatic time alignment, which means if you have different runs through the trace, OK, the blue the different curves are different.",
                    "label": 0
                },
                {
                    "sent": "Kind of runs through the through the track OK, and they're not time aligned, so they have different durations because it's not the same speed at every time.",
                    "label": 0
                },
                {
                    "sent": "Then after optimization of this Gaussian process model, we actually get all these traces really nicely on top of each other, which means time aligned.",
                    "label": 0
                },
                {
                    "sent": "And you can use it, for example then for replay.",
                    "label": 0
                },
                {
                    "sent": "Of trajectories, Peter Beale did some very nice work.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the context of helicopters, there also issues and I'm running, oh, I still have two 3 minutes.",
                    "label": 0
                },
                {
                    "sent": "With so in a sense, this is totally independent of what kind of things you do to your Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "OK, so far we only talked.",
                    "label": 0
                },
                {
                    "sent": "It was very kind of plain generic Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "You can also go far beyond that.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can do heteroscedastic GPS when they sense the noise is right now the noise in the sense is independent of the state, but you can also learn and state dependent noise systems.",
                    "label": 0
                },
                {
                    "sent": "You can make them much far more efficient by doing using sparse Gaussian processes, specially the NIPS community has developed many of those techniques where the idea is either you learn.",
                    "label": 0
                },
                {
                    "sent": "Of course not an optimal, but you learn a subset of data points such that you're cheap.",
                    "label": 0
                },
                {
                    "sent": "He still gives you a very good prediction, and then of course, because your prediction later depends on the size of the data set, you get much more efficient prediction results.",
                    "label": 0
                },
                {
                    "sent": "OK, instead of subsets, you can also innocence learn the positions of points, optimal position, and if you want to go beyond just in continuous systems, then you can you just go to some of its called.",
                    "label": 0
                },
                {
                    "sent": "Also, process classification is a lot of literature in that, and actually Christian did some work where he used GPS.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That context to summarize, I think on the positive Gaussian process it's really a nice kind of flexible modeling framework because it's nonparametric technique.",
                    "label": 0
                },
                {
                    "sent": "Key.",
                    "label": 0
                },
                {
                    "sent": "Another key advantage is really that they take the noise and the data noise and the uncertainty due to data sparsity into account.",
                    "label": 0
                },
                {
                    "sent": "OK, problem is clearly the efficiency right?",
                    "label": 0
                },
                {
                    "sent": "But there are some remedies to that.",
                    "label": 0
                },
                {
                    "sent": "I think actually the real key is you want to combine those with parametric models, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a rough parametric model of your system, you should not throw that away and try to learn everything with the GP that you can write, you should model as much as you can.",
                    "label": 0
                },
                {
                    "sent": "And then maybe if there are still things that you really need to take into account to increase performance, then you should learn the residual with the GP.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, maybe one of the take home message that I think is also that these things should really only be used if you need them, right?",
                    "label": 0
                },
                {
                    "sent": "If you don't need it or you have a parametric model, it doesn't make sense to waste all the computational complexities on using GPS, but I think there are many applications where it does make sense to apply these nonparametric regression techniques.",
                    "label": 0
                },
                {
                    "sent": "And I can imagine you can get maybe on the 1st part, similar results with with the kind of K means related techniques.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing also about the Gaussian processes in combination with this latent variable model that you just have one nice framework for handling all the connections in the data.",
                    "label": 0
                },
                {
                    "sent": "And with that, I think amount of time and thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}