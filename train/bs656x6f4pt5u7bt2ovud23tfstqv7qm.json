{
    "id": "bs656x6f4pt5u7bt2ovud23tfstqv7qm",
    "title": "Gaussian Processes and Process Convolutions from a Bayesian Perspective",
    "info": {
        "author": [
            "Dave Higdon, Los Alamos National Laboratory"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_higdon_gppcbp/",
    "segmentation": [
        [
            "Alright, thanks, you're not going to read the title of my talk.",
            "So alright, we better fix that.",
            "So to kill time in the classes when I would teach, so we'd have to guess which ones were my albums and which ones were the ones my kids had ordered on iTunes.",
            "So, and the students are pretty good.",
            "They can figure that out.",
            "If it was like older than 20 years ago, probably mine.",
            "Alright, so so I was going to talk about some work I have done in Gaussian processes.",
            "I'm not sure.",
            "I think my wife got that one, but a lot of the greatest hits often come in the party mix, and so usually that's a pretty good one to get.",
            "Yeah.",
            "Yeah, so so chime in if you have questions anytime.",
            "Chime in and go and do that.",
            "It'll be good.",
            "Luckily Luckily we recorded right so we can always go back and look at the real detailed interesting questions here.",
            "Alright, so I was going to talk about mostly Gaussian process models.",
            "An looking at him from a convolution setting, which I think has a lot of overlap with a lot of these radial basis function models and certainly they share a lot of the same ideas.",
            "So I think a lot of this won't be.",
            "Terribly new in terms of content, but maybe you'll get a little bit of insight about how a knucklehead statistician might have thought about these sorts of problems.",
            "And maybe there's something useful to be had there, but.",
            "Course I'll throw the disclaimer, maybe nothing useful will come with this, so each at least try to enjoy it and trying to learn something anyway, so ask questions anytime you want.",
            "This is just eye candy since I work for the government, I often have to talk with, you know government officials on different sorts of things, and will often complain if there's not enough eye candy and things like that so.",
            "I."
        ],
        [
            "I always just try to do that now just to.",
            "Just to make sure we can do that.",
            "I'll see if I can.",
            "So if I can advance this, otherwise we're just going to talk on this one, slide the whole time."
        ],
        [
            "Alright, so.",
            "So basically what I thought I'd do is I'll go through a real basic example and kind of go through a little too much detail so you don't have to pay attention to all this, but I thought it might be worth it on the first one just to sort of give you a sense of alright, here's what I'm thinking about going through this sort of problem, and I'll look at it from a Bayesian perspective.",
            "And then and then we kind of back off the resolution and look at a number of approaches for dealing with problems that involve these convolutional kernel types of methods.",
            "And so here is just a real simple spatial problem.",
            "So we've got a spatial dimension 1 dimensional here right now.",
            "And and what we're going to do is just have a bunch of basis functions here, so Gaussian kernels.",
            "And then what we're going to do is just represent spatial process.",
            "So something smooth going through this one dimensional space.",
            "Here as just a weighted combination of these kernels.",
            "OK, so so Gaussian kernel.",
            "The actual spatial process we're going to represent us something smooth, just a weighted combination of these things.",
            "And then if we go ahead and do it this way, you have this nice sort of regression like representation.",
            "You can represent the process as just.",
            "A matrix which is built up from the Kernels Times acts the weights that go with each of the kernels.",
            "Alright, so X will be the weights that go with the kernels.",
            "K will usually be the kernel in Kane.",
            "Will be this matrix we build up by saying what the kernel is at.",
            "Different spatial locations corresponding to different locations and so usually I'll use like the Omega to denote where the spatial location is for the kernel.",
            "So we just have a bunch of kernels that are spatially kind of next to each other, but they're living on a lattice, something nice and regular, and that's typically what I'm."
        ],
        [
            "Be thinking about here and hopefully we'll see wise as we move on.",
            "OK, so so if we pick a different set of values for the weights these ex Jays and so now the XJ's are basically proportional to these Heights that we see here is summer negative summer positive.",
            "We Adam together and then we get this spatial function here.",
            "OK, so just a linear combination of basis functions.",
            "That's just regression as far as far as we know that right now.",
            "Now it will be a little bit Bayesian will put some sort of prior on.",
            "These weights will say these weights and the simplest thing to put out there as IID normal.",
            "Zero with maybe some precision.",
            "Or maybe you specify it and this is going to shrink all the weights together so there's other ways to shrink weights down, right?",
            "Guys that do things like wavelet and stuff like that, like to zero out lots of the weights and just leave a few standing.",
            "And this prior is really just going to shrink them all in a common sort of way, so it's not going to reduce the number of weights that we have to work with.",
            "Alright, so and again we'll see it because this gives you lots of duality with the Gaussian processes.",
            "Why this model is coming about?",
            "OK so the 1st."
        ],
        [
            "Prior there is on those weights they have is common mean zero normal distribution.",
            "Alright, and then we're going to do is try to fit these things to data.",
            "So if this is our data here, we might see the data are centered at this unknown function Z or call it in Canada.",
            "We say zed in Canada.",
            "How may we do?",
            "I'm sure I'm not gonna be able to do that, so I'm still going to use going to talk in American still.",
            "So, so we have this process.",
            "Then we're going to the data, maybe are IID distributed about the about that smooth process.",
            "OK, and so we might have a regression sort of model here that says our data are what the smooth process tells us, plus some IID error.",
            "OK, and so given that data Now, we'd like to estimate what the axes are.",
            "Maybe some parameters about the X is how much shrinkage is there?",
            "What's the variance of those of that prior for the X is?",
            "And also stuff like what's the variance of the errors?"
        ],
        [
            "Alright, so here Oh yeah.",
            "OK, so we're just going to do this one time, so there will be other slides like this and we're going to rip through him, but we'll do this one time.",
            "OK so so so alright so I live in the stat world, right?",
            "We'll do likelihood functions and things like that or likelihood function basically says that data are IID normal with a variance one over Lambda Y. I'll use precisions and just turns out that.",
            "It depends on what you grow up with.",
            "Do you like to do precisions or variances?",
            "I'll do precisions so it has precision Lambda, why that controls the variation of the scatter about the about the smooth process, so we're just saying that the data are IID normal zero with precision Lambda Lambda, Y. Alright, and we have N data points.",
            "We'll have a kernel that we build up.",
            "We build up this K matrix from the kernels.",
            "And so then we're just going to have a few priors to finish things off.",
            "We're going to have the normal zero Lambda X prior for the precision, so we're going to shrink all those weights together.",
            "We're also going to have him live near 0 for right now, and so all these problems will be centered near 0.",
            "Just it makes the math a little bit easier.",
            "And conceptually, it's not a big deal if you want to send him someplace else.",
            "If you need to move the data somewhere, well, you had a mean function in there, or maybe you center the data so we're not going to worry about that here.",
            "Alright then.",
            "We also have a prior for the precision parameter for the weights Lambda X, so that's going to tell you how much do you shrink the weights down, and we also have a prior for the error variance.",
            "So really, it's just sort of three things there.",
            "There's weights.",
            "X is.",
            "There's a precision for those weights, Lambda X, and then there's the error precision Lambda.",
            "Why so real basic problem here?",
            "And then will summarize everything in terms of the posterior distribution.",
            "So we have the posterior distribution of these three things.",
            "The Axis is a vector, but the other two things are just scalars, and it just has a posterior distribution that's just proportional to the likelihood times the priors.",
            "Alright, so this is really the thing that if you want to point estimate you might think about optimizing this, or you might sample from this distribution and look at posterior means or something like that."
        ],
        [
            "OK, and so and So what will typically do is here's our posterior distribution.",
            "We live in the stat world.",
            "We often can recognize the full conditional distributions at that.",
            "If I look at the X is conditional on everything else I can figure out that oh, this thing is minus 1/2 something quadratic in X, so I know that that's going to be something that's normal, so I know conditionally that X is going to have a normal distribution given everything else.",
            "Likewise, I can recognize the distribution for the two precision parameters.",
            "That if I use a gamma prior for those things, the full conditional for the precisions is also going to be gamma.",
            "OK so so now I can say, OK, I can recognize these things.",
            "I've got a full conditional that's normal for X full conditionals that are gamma for Lambda X and Lambda Y.",
            "Why is this nice?",
            "'cause now we could just use this Markov chain Monte Carlo and actually just draw samples from this big posterior.",
            "And that's typically what we do for these sorts of things so."
        ],
        [
            "Might be true.",
            "Everyone knows this stuff, right?",
            "Famous last words.",
            "Alright, so here's just a Gibbs sampler for a bivariate normal.",
            "You start at some initial guess and maybe you can actually sample from the distribution, but probably not.",
            "So you take an initial guess and then you update one of the components, leaving the other one fixed and then update the next component.",
            "Leave the other one fixed and just keep cycling through like that.",
            "So if you have a bivariate normal distribution, you're going to have normal full conditionals and he just generate Z1 given Z2Z2 given Z1 and then you just start.",
            "Just go back and forth continually doing that.",
            "And you'll end up with a."
        ],
        [
            "Looking sample path through this thing.",
            "And then if you take this whole collection of points, right?",
            "That's a basically a correlated draw for your posterior distribution.",
            "The thing that you want, and so if you want to estimate the mean of this distribution, just the mean of the sample will do.",
            "If you want to estimate quantiles of quantiles, will do for that so.",
            "So that's typically how we can go ahead and estimate these things.",
            "There's other ways to do that, but here's a good generic approach for doing that.",
            "And so if we."
        ],
        [
            "Have like just going to do this once, right?",
            "It won't be painful one time, so here is just a real simple example where I've got five data points.",
            "I have a space that's just between zero and one.",
            "I have basis functions that are just these normal kernels centered at.",
            "Six points between minus .3 and 1.2.",
            "Alright, so I make the colonels go a little bit beyond the space I want to predict.",
            "That way I'm going to eliminate these edge effect so the kernels go beyond where.",
            "I have my data and want to build my model.",
            "And then I just go ahead and do that alright.",
            "And then I also say, alright, the kernels will be normal with a standard deviation of three.",
            "I'll specify priors because I know what this error precision is for this problem, I'm going to have a strong prior on what the error precision is, and I'll have a fairly weak prior on how much shrinkage we do on the kernels on the X is.",
            "And so now we can just go ahead and do that MCMC scheme estimating the multivariate normals for the X is sample those things and then sample the full conditionals for the precisions.",
            "And here is just a train or sequence of samples using MCMC for the X is.",
            "There are six of 'em here and then the two precisions.",
            "There's the MCMC chain for that, so I'm just running through and doing this MCMC scheme to estimate this stuff and hopefully I'll get some function that goes nice and smoothly through this data.",
            "Of course, I ought to.",
            "I have nice, smooth, wide kernels here, so that should probably work pretty well.",
            "And sure enough, here are if I go back and get those realizations of X is I can run up to the kernels and say, alright, here is the predicted disease or realizations of disease that I've got."
        ],
        [
            "And I could try to summarize that maybe pointwise posterior mean incredible intervals or something like that.",
            "OK, so there is a lot of detail for that one problem.",
            "Well, we won't do that so much anymore, but."
        ],
        [
            "Alright, well there's sort of a background.",
            "Here's how I.",
            "Have a look at this sort of problem.",
            "OK, and so.",
            "And so here's an example where I've got some data I'll come up with a normal kernel, the SDF 2 here.",
            "OK, so with the kernel is going about this big relative to the data.",
            "How do we choose that thing was going to make some important or how it effect on how things come out, but here's a resulted fitted process.",
            "Here is the posterior distribution for the shrinkage.",
            "Lambda X for that parameter and the precision Lambda Y here.",
            "So there it is for one choice of the kernel if I."
        ],
        [
            "A different choice of Colonel.",
            "I might get a fairly different answer so I can make that kernel a lot narrower, so it's with is only about you know, plus or minus two here and SD of one.",
            "Well, I could have a process that goes through the data pretty.",
            "Pretty strongly, maybe too much, and so that kernel width is basically saying how smooth is this process going to be?",
            "And certainly that's something we're going to want to estimate most the time when I haven't talked too much about that yet.",
            "Alright."
        ],
        [
            "OK, so there's sort of the basic idea here is kernels using for using kernels to estimate these smooth processes.",
            "And now I want to relate these a little more to a Gaussian processes here, so typically in a Gaussian process model, I'm going to think very discreetly here.",
            "I might want to predict the points at a set of spatial locations across.",
            "Here it is between zero and 10, so I have a bunch of spatial locations I want to predict this thing in a Gaussian process model is basically going to say this big vector, which is nice.",
            "Finally finally space thing.",
            "Is going to be normal with some sort of covariance?",
            "And so if we build the cover or if we build this this kernel process here with the case.",
            "We can go ahead and build this K matrix right.",
            "We said Z is equal to K * X and so if we just go ahead and follow this thing through, OK, well if I looked at Z so it's K times some normal 0.",
            "Here it's going to be normal 01 vector vector of IID normal 0 ones.",
            "Well, I can just go ahead and say OK Well Z is just going to be have a covariance.",
            "That's if I marginalized out the X.",
            "It's just going to have covariance.",
            "That's K * K transpose.",
            "Alright, so that's nothing new at all right here, but.",
            "But now I can go ahead and say, well, there's probably lots of ways I might be able to represent this.",
            "K * K transpose thing.",
            "And So what we typically do for this thing is we.",
            "Typically generate a normal distribution by doing something exactly like this.",
            "I'll generate IID normal 0 ones.",
            "I'll hit it with some matrix to make a Gaussian process with the covariance function I want.",
            "So if I know the covariance matrix, typically I do something like Cholesky or something like that to go ahead and do that, and so if I just do a Cholesky factorization of this thing, say Cholesky, with pivoting, the columns of K end up looking like these vectors here.",
            "And so I can think of it discretely.",
            "Or I could just go ahead and make it continuous version that really, if I'm generating a normal distribution, I'm really hitting a bunch of basis vectors with the normal with IID normal 0 ones, and so for a given covariance function or covariance matrix I basically have induced the matrix by saying I'm going to use Cholesky and I'm going to look at the Rose of the Cholesky or the columns of the Cholesky matrix.",
            "OK, and so typically we might not take a whole infinite range of these things, but we might take a small number of these basis functions and so if you have a covariance, can represent efficiently with maybe 5 basis or 10 basis functions, you can get away with doing things like this."
        ],
        [
            "Alright, so we have a covariance function that looks like this and this is a covariance that has real smooth spatial dependence.",
            "It says basically the process is going if I take an element here, it's going to be correlated to nearby things and then the correlation is going to die off like a big Gaussian.",
            "So here's a Gaussian covariance function.",
            "Find decompose that with Cholesky, say with pivoting I get basis functions that look like this, so these are different than these kernels.",
            "But there's other ways we can decompose this thing right?",
            "We can decompose this covariance matrix and say.",
            "OK, we've got this.",
            "We can use singular value decomposition and we get a slightly different looking basis.",
            "And the kernels are just another another way of generating this covariance function with different type of bases.",
            "OK, so if I use the kernels I can get this exact same covariance function.",
            "So really the same covariance matrix I could decompose in different sorts of ways, meaning I've got different basis representations for the same basic normal model for the ZZ is normal, mean zero, and covariance Sigma, and there's different ways to get at that, and that's probably useful if we're going to try to think about how do we want to model other sorts of things as we're going along here."
        ],
        [
            "Another question is, well, how dance?",
            "If we go back to the kernels, how dense do we need to make the kernels so I just separated the kernels about 1 standard deviation apart.",
            "I could have made him a lot denser and made the precision or the variance of those things go down and I get basically the same process so.",
            "So here is.",
            "So here's kernels separated by about 1 standard deviation.",
            "Here's the induced covariance function or covariance matrix that you get if I make the kernels twice as dense and make the variance go down by half.",
            "I get this picture and the covariance is almost identical.",
            "Same thing here.",
            "If I have a real dense set of kernels and I appropriately knock down the variance attached to those kernels, I basically get the same covariance again.",
            "So what this is saying is that I could basically thin out my basis matrix to a point where I can efficiently represent the same covariance with just say six basis functions instead of 20.",
            "And it's also saying that it's basically the same model that if I had six basis functions or 20 basis functions, the implied model on that Z is basically the same.",
            "So so if you know the width of these things.",
            "And you need to reduce.",
            "You need to get your basis functions down on the number of bases you want to work with.",
            "So this is essentially a much lower rank approximation.",
            "This matrix than this one.",
            "Oh yeah, absolutely yeah, yeah, all complaints are welcome too.",
            "That would be great.",
            "I guess the point is that and so the requesting is of course one can.",
            "You take Micah.",
            "You can basically, if you have some kind of everywhere and integrate it out, can actually get it right.",
            "Having sex isn't the same as having engine.",
            "If you have you have a number of different points, right?",
            "So you actually want.",
            "Yeah.",
            "Then you only have 6.",
            "Yeah, yeah, that's exactly right.",
            "So that I mean that that's a good point that.",
            "If you do something with the Gaussian process, you can make it interpolate any data set you wanted really for any function for any width of these kernels or bandwidth, and you could get it to go through and this thing is never going to do that.",
            "If you have 100 points to go through that are kind of all over the place.",
            "In fact, you're going to probably 100 basis to get there if it's very wild.",
            "So that's absolutely right then.",
            "It's never going to be an interpolator in that sort of sense, but it's also a warning that if.",
            "If this isn't doing the job.",
            "And you need to do something like this.",
            "Well, there's probably a mismatch between what you're picking as your bandwidth for your kernel and what the process is you're trying to fit.",
            "But yeah, you're absolutely right timing.",
            "Good complaint.",
            "Alright."
        ],
        [
            "OK, and you can do it.",
            "I mean you can do a continuous version of this sort of thing, and all I want to say here is that you can actually write down the induced covariance function that you've got, and as the number of points you've got here, it gets denser and denser.",
            "This induced covariance function approximates this continuous thing that you get.",
            "OK, so I talked about basis functions and adding them together.",
            "It's an equivalent way of thinking about this thing, which is going to be useful is we could talk about a bunch of white noise that we have an smoothing it out with the kernel.",
            "Turns out if we have the same kernel all the way through, so we're just convolving some white noise with the kernel.",
            "This is equivalent, so if we just had six places where we have white noise, that's just six basis functions really.",
            "And this is nice because if we think of it this way, then we can also do other sorts of things, like have the kernel width change as a function of spatial location, and this might give us a richer set of models to work with.",
            "Alright.",
            "Alright."
        ],
        [
            "Um?",
            "Yes, my artwork.",
            "I guess I better talk about it some so.",
            "So what sort of things might we do if we what's nice about this is whatever kernel specification you make you induced a valid covariance function.",
            "So that's a nice.",
            "It might be a handy way to mess with well.",
            "How do I pick a covariance function?",
            "Traditionally, if you work in covariances, you have to pick a covariance rule that's positive semidefinite and obeys all the rules.",
            "That covariance function must obey.",
            "But if you work with kernel, you can do anything to it, and it's going to induce a covariance, so it might be who've you or it might be nicer to work with this kernel mess with that and get it to fit the data well rather than trying to mess with the covariance function.",
            "There might be other multi variant specifications that you might be able to workout this way a little bit easier and so the idea is that you might have maybe 3 latent IID processes are doing something else.",
            "You convolve 'em, so I have the red kernel here involving two of these three processes to get a spatial field and they have this green kernel convolving a different two of these processes to get another spatial field.",
            "So these two guys will have dependence in am an it might be convenient to think about this link and process to try to figure out how do you put the dependence in there.",
            "And then a lot of physical processes have properties that are kind of strange, like they don't really obey this nice geode esic kind of rule.",
            "For instance, water temperature that if we have an island, we have wind coming through that the water temperature in the Cove here might be very different than the water temperature on the other side of the island, so we don't want to use just straight Euclidean distance.",
            "We might do something that tries to go around here and so thinking of these things in terms of convolutions or kernels might help you with that.",
            "I also had this nice a wildlife picture here that I thought OK, well it might be really important.",
            "The wildlife you know on this side of the island and this side of the island are somehow dependent.",
            "Their genetics are linked and things like that, and they can only sort of.",
            "They only interact with that real small isthmus in the island, and so this might put some interesting structure here that we want to model, and so I'm not really good at art, so I drew this bird here for the wildlife.",
            "And then I realized, oh, the bird would just fly right over to the other side of the island and wouldn't care about the isthmus.",
            "So it's got a ball and chain on it right here, so now it has to walk through the isthmus.",
            "Alright um.",
            "And the other thing is, like I said, you could have the kernel change with spatial location and this might also induce some non stationarity in your model.",
            "That might be more appropriate for the process you've got.",
            "And then also other more physical types of models of.",
            "Of transport, often these kernels are linked to maybe the Greens function of the PDS that go with those sorts of things or differential equations."
        ],
        [
            "Alright.",
            "Picture.",
            "Also, the basis functions right.",
            "Oh yeah, alright so this is.",
            "Yeah, so this is I've always called these basis functions kernels.",
            "And now I realize like it's the exact opposite and machine learning world, right?",
            "So I will slip that up once in awhile.",
            "And yeah, I mean to say what I mean, but not what I'm saying.",
            "Most of the time."
        ],
        [
            "So yeah, you're right.",
            "Yeah.",
            "Alright, I'm going to skip this for now, but the idea is that I want to say basis function here and covariance status inducing so for different basis functions you induce different types of covariances and some of these basis functions are going to be easier to work with than others, so someone just aren't practical, right?",
            "If I really want exponential covariance function.",
            "Boy, the basis function looks like a thumb tack.",
            "It's going to be an awful thing to have to work with, so so it's not going to be practical.",
            "If you want a real rough sort of process that's going to be practical.",
            "If you want things that have more smoothness attached to him."
        ],
        [
            "Alright."
        ],
        [
            "Oh, see, we get to skip all that.",
            "See that don't have to worry about that.",
            "So now I'm just going to go through some."
        ],
        [
            "Basic ideas here to sort of finish stuff off so, so here's a data set.",
            "If we use kernels that are about this wide, which is really too skinny for the data set.",
            "If you try to estimate the kernel, it's going to want something wider.",
            "If I pick something too skinny, I end up fitting a posterior posterior mean process for the thing that's going to have a lot of wiggle and in it, and it's going to be kind of noisy.",
            "Alright, if I use.",
            "A kernel that has the right width.",
            "So now I have a tuned kernel when estimated it when I'm not going to deal with that, it goes to the data.",
            "Nice smooth sort of way.",
            "Alright so here is the right kernel width.",
            "Here's the wrong one at 2 skinny, well oftentimes.",
            "It's hard to estimate this kernel with.",
            "It takes a lot of computation and maybe you don't have the computation to do that, and so one idea is to say that instead of saying I'm going to have IID weights on the kernels, I could put dependence on the kernels.",
            "I could say that the kernel weights are going to follow a dependent process going across here.",
            "So here I put in a random walk model for the kernels going across here.",
            "So this means that the kernel height here for this kernel or the wait here is going to be dependent is going to close the weight on this kernel, so this is effectively kind of.",
            "Even though the Colonels, a little bit narrow, it's effectively pushing out the width of the kernel some, and so if I use a different process instead of IID, I can come up with a process that mimics what we want to have a wider kernel without actually having to go and estimate the thing so.",
            "So that's an easy thing to do.",
            "It just takes one more parameter estimate numerically.",
            "It's a nice thing to do, and then he was just the right answer.",
            "If you just use the Gaussian process sort of model.",
            "Alright, So what we could do is mess around with that underlying dependent structure on the axes or on the weights, and that gives us a richer class of."
        ],
        [
            "Things.",
            "A second thing we can do is look at maybe multiresolution types of prophecies.",
            "Alright, so here's just a convolution model looking at ozone data, or.",
            "You know, we just have one kernel here, this being estimated.",
            "And here is the result that fit.",
            "For for the ozone map.",
            "As."
        ],
        [
            "Second idea might be saying well that that one is going to be a little bit smooth here, because if I have nots this spaced out I really can't make my kernel too narrow or I'm going to get sort of dead spots in between.",
            "But if I really do think there's more small scale structure here, I could instead of putting lots of knots in a real small kernel, I could make a sum of two processes, one that's fairly coarse or smooth, and one that's a lot more wiggly.",
            "And so here I've got a course process with a with a wide kernel, an affine process with a much narrower kernel.",
            "And I could just say that OK, then the resulting thing I want to estimate is going to be the sum of the course plus the fine thing.",
            "And so now if we just go ahead and estimate that, it turns out that's not much harder.",
            "That's you gotta put all this stuff together and go and do it.",
            "But you can go ahead and estimating course and flying process."
        ],
        [
            "And you really do it by just by combining the coarse and fine kernels together and really putting it back in the in the regression framework that we've got.",
            "And so we just."
        ],
        [
            "Go ahead and get well.",
            "Here's sort of the course representation.",
            "Here's the course.",
            "Plus fine has a little more detail in there, not a ton more.",
            "OK so alright.",
            "So a multiresolution sort of idea is something we could do.",
            "Also."
        ],
        [
            "You know this sort of stuff will work pretty well in binary classification types of problems.",
            "That's not really a binary classification, but here I've got data where the data are more blueish.",
            "It's suggesting one state at this process, and where the data are more purple, is suggesting another state.",
            "So here's a true binary field.",
            "And what we got are noisy realizations of this thing over here.",
            "OK, and so now now what we'd like to do is.",
            "Is trying to, you know, try to estimate what is the binary reconstruction for this thing given the data and so sort of a cheat, I've got a likelihood that is a lot harder.",
            "It's easy to specify it here.",
            "It's a lot harder in practice, but we got 10 measurements and will represent the spatial field using this convolution idea again.",
            "And basically, here's how it goes that."
        ],
        [
            "We'll put out a lattice of weights, right, and then will convolve these weights with the kernel will end up with a smooth process and then we'll just say the binary process is going to be.",
            "One when that process above 0 and N -- 1 or 0.",
            "The other thing when it's below 0.",
            "So we just sort of clip this thing and end up with the binary the binary field.",
            "So now it's the same basic idea.",
            "I have the same basic set of weights here that control this process which we clip to get the binary thing."
        ],
        [
            "OK, and so.",
            "So we've got a likelihood that just says what we have is a noisy version of the truth.",
            "Gaussian noisy version of the truth.",
            "We just have the IID model on the on the X is and then.",
            "And then we're going to say that the truth is basically the clipped version.",
            "So I want to put a star on this.",
            "See the smooth process when I'm clipping it to be zero and one.",
            "And so then we're just going to end up with a posterior distribution here, so that clipping is a nice nonlinear sort of thing, so it's going to make this resulting posterior not be nice and easy.",
            "It's not going to have this nice full conditional form.",
            "That we had before, and that's just Gaussian and.",
            "And gamma, so we'll have to do we have to do some metropolis updating on this."
        ],
        [
            "Sing there's Metropolis Los Alamos guy."
        ],
        [
            "Alright, and so Metropolis is just doing a different approach for sampling this."
        ],
        [
            "Posterior distribution, but it's very basic kind of thing.",
            "And so you can end up with as you're updating these X is you're going to induce binary fields here.",
            "And here is just a collection of posterior samples of this binary field given the data.",
            "And so we can."
        ],
        [
            "Go ahead and put that together.",
            "We can come up with a posterior mean from that field and compare it and do something with that.",
            "So really, that's.",
            "One is that that's just using the same spatial model and just clipping it.",
            "For a binary example."
        ],
        [
            "Rather than."
        ],
        [
            "Using something continuous here."
        ],
        [
            "Alright, so.",
            "So, so it's been useful to do this sort of convolution idea.",
            "This basis representation, it has been useful in a lot of inverse problems.",
            "If you wanted to sort of.",
            "Trying to estimate the 64 by 64 spatial field given some very remote data.",
            "So here it's.",
            "Here it's a transport problem that we put an injector well in the middle.",
            "We've got a bunch of wells around the outside that are going to be sucking out fluid at a constant pressure.",
            "And So what we do is we get get the system going into steady state.",
            "So we inject a constant amount overtime into the injector.",
            "Well in the middle, and then we suck out at a constant pressure around the outside.",
            "So there is a.",
            "8 wells around the outside here and so when you get this thing going at steady state, you put in a tracer in the middle and then you wait until the tracer see when the tracer hits the well out here and so this is basically the concentration of the tracer on this outside well so it starts out with nothing and then eventually the tracer reaches there and you have a high concentration.",
            "And then it dies back out.",
            "So you get this at all these wells or surrounding the injector and what you'd like to do is try to infer what's the underlying permeability of this field.",
            "To do this with a 64 by 64 grid is pretty demanding, but if we break it up and we just use a real smooth or coarse representation with these basis functions.",
            "And we might be able to estimate things."
        ],
        [
            "Alright, and so here was just an example with the real problem.",
            "If we compute for three months we get some answer like this, which using MCMC it didn't really converge and we didn't get anywhere.",
            "But if we did some much coarser basis representation, we could get a. Posterior that seemed to be fairly stable now correct?",
            "Is it tough question on that one, but at least it helps with that problem.",
            "It reduce the dimensionality so much that we did have a hope of inferring something about this permeability here, but we did have to assume a lot of smoothness to be able to do that.",
            "Alright, I'll talk about a couple more things before we end it here, nonstationarity so."
        ],
        [
            "Typically what we do is we take some white noise process, convolve it with the kernel and we end up with some process that has this smoothness attached to it.",
            "Alright, so we just use one kernel all the way through.",
            "Here's the process we get, and so I just drew that kernel everywhere, shrunk down by about a factor of 6 so I could fit it on each pixel.",
            "So there's the resulting field.",
            "One alternative"
        ],
        [
            "To say OK, what we could have is have that convolution kernel change with spatial location.",
            "So now the kernel is changing depending on where it is.",
            "Colonel is not really the size of these things as by 6 times as large, so it's much bigger.",
            "But just to show you the picture here shrunken down.",
            "So now it's the same basic process here, but we're having the kernel change smoothly over the spatial domain, and this allows you to see you know other types of dependence.",
            "So over here the dependence is very much in this direction, but over here it's going in a very different direction, so this might be a way to allow the spatial dependence to change.",
            "Over the region that you're looking at."
        ],
        [
            "One way we did this was just look at a weighted combination.",
            "Let's just have three kernels and at any spatial location will just have a weighted combination of those three.",
            "And so you have to cook up."
        ],
        [
            "One way to have this weighted combination change smoothly over space.",
            "And what we did is we had three smooth processes going around and then the then the weight was just going to be, you know.",
            "The.",
            "Either one process divided by the sum of Y to the other processes, so it's fairly standard way to come up with some sort of mixture spatial mixture over.",
            "Over spatial domain here.",
            "And then you can represent the Colonel like which weight is winning.",
            "So two is this kind of sideways one going this way.",
            "So over here 2 is high and most of the dependences in this direction over here 3 is high, most independence is in that direction.",
            "And then over here one is high.",
            "So most dependences horizontal direction.",
            "So here is just one way of trying to do that.",
            "The tricky thing is trying to."
        ],
        [
            "Automate all this stuff when you're doing it, but here is just one example where we're looking at dioxin concentrations, and sure enough, going through this whole estimation process.",
            "It does estimate stronger spatial dependence in this direction over here and over here at it's kind of changing this spatial dependence, so certainly there is information in that.",
            "The thing that we haven't found is that you get much better predictions at all that if you really compare your predictions on something very stationary to something nonstationary with this sort of data set, you don't see that much difference.",
            "So it does."
        ],
        [
            "Estimate the structure well.",
            "The resulting fit doesn't seem to be huge."
        ],
        [
            "Difference.",
            "Alright, a couple more ideas.",
            "Just if we're dealing with spacetime models we could have these latent weights live in some sort of space time domain, and then to specify a kernel over the space time domain.",
            "And then we can go ahead and convolve.",
            "This sort of latent wait process with the kernel in that end, up with the space time model.",
            "So sometimes that's a convenient sort of thing to do."
        ],
        [
            "Where is it worked well?",
            "So it works well in estimating ocean temperatures, so we have this.",
            "This data set over space and time, so it's about 70 years a little more almost 80 years of data over the spatial region."
        ],
        [
            "The data are living in this 3D space.",
            "So we end up specifying a grid of weight, or the kernels all live in this big grid here, and the kernels are different in different locations, so it's a hassle trying to estimate those sorts of things, but we could go ahead and do that.",
            "And now with those things, if we estimate the weights with the kernels, we can go ahead and estimated spacetime field for this."
        ],
        [
            "For this whole thing, given the data."
        ],
        [
            "And so there is a, so there's a space time model.",
            "And the big thing about doing these weights is that it would actually become.",
            "So it actually became estimable is a real tough problem without doing this sort of thing.",
            "So so here is 1 approach to doing that."
        ],
        [
            "And then you can look at deviances from the overall grand mean, and you know, is there evidence of things getting warmer overtime.",
            "And in that data with that MoD."
        ],
        [
            "That we use really.",
            "There's not much evidence that it, you know, seem to be warmer anywhere, not really."
        ],
        [
            "Right?",
            "Alright, here at a second way we could do this is you can imagine a latent process that lives spatially and then have this process evolve overtime.",
            "And then what you're going to do is just convolve that problem or convolve that process with just a spatial kernel.",
            "So we have this latent process.",
            "These weights that are evolving overtime and just a spatial kernel here.",
            "And so if we do that now, we're going to have a space time process that's going to evolve overtime.",
            "What's nice about that is that this often will fit into this dynamic model framework, so these dynamic linear models you can often estimate this way so you can cook up a space time model that has a lot of estimation machinery already attached to it.",
            "And so then you can handle some of these pretty big problems that otherwise we're going to be pretty difficult."
        ],
        [
            "And so here was just we're looking at ozone levels, daily ozone and so here is just nine days of ozone.",
            "And if we try to fit this sort of model to this thing, we end up with something that.",
            "Wow."
        ],
        [
            "That's quite a bit of structure, but here is.",
            "Here's the ozone map evolving overtime.",
            "And what we've got is basically this spatial set of weights that's changing overtime and some random random walk kind of fashion.",
            "I think the random walk was all we used here.",
            "And that seemed to model the data fairly well.",
            "If we go back and look at.",
            "Spatial ways?",
            "Well here is just."
        ],
        [
            "Three of the weights that we got in that big giant lattice, and if we look at three of the way, so we estimate an independent independent prior well, then the weights are kind of all over the board.",
            "If we put a prior that has structure overtime.",
            "So here's a random walk prior.",
            "Well, then the weights do seem to have a lot more continuity overtime.",
            "And it gives you slightly better.",
            "He just sort of one day and types of predictions.",
            "So so anyway, there is another way to try to get a spacetime model out of this thing.",
            "Alright."
        ],
        [
            "So I think.",
            "That's basically.",
            "All they really had to talk about so, So what is this?",
            "This is basically here's a guy that thinks a lot about Gaussian processes thinks about spatial models, maybe spacetime models.",
            "And and the kernels.",
            "I want to say the basis functions were really an idea or a way of getting around.",
            "Well.",
            "How can I mimic this covariance structure that I wanted?",
            "This kernel that I want when I can't do the usual Gaussian process kind of activity here and so using the kernels it allowed to allow me to speed up computations at times.",
            "It also allows you to model processes in different sorts of ways than you would have had you just thought about covariances.",
            "And so so it worked well in a number of these sorts of situations where.",
            "Especially that ozone case where you can actually do the model quickly enough and analytically enough that you could start asking questions about design.",
            "Where should we know where should you add new monitors?",
            "Here's a lesson about design though.",
            "Is that almost always if it involves a government, you're always asking questions like, which monitor should I remove?",
            "And that's always a lot easier question.",
            "Maybe doesn't model need as much model so?",
            "But we won't worry about that, and so things that, if you're trying to do optimization on top of this and design, then these this sort of structure actually gives you a computational boost that makes some problems that just weren't really doable.",
            "Doable again.",
            "And then the ocean one.",
            "So now we kind of worked with the ocean modelers at Los Alamos now.",
            "And so now we are looking at ways that how can we do data simulation with those with those big models.",
            "And thought is well, maybe some of these kernel approaches might be useful there.",
            "Alright so I am done torturing you with this talk.",
            "Thanks for listening and I'm sure we'll have time for discussion later.",
            "This is kind of.",
            "Going somewhere from A to B and back today.",
            "Funny thing is that.",
            "Two years ago I started working gas prices.",
            "We said, yeah, he's fine.",
            "Dimensional models are based on but hey we have to specify regular basis functions are.",
            "Yeah yeah that's kind of ugly.",
            "So how about we do this thing we actually have?",
            "Yeah.",
            "Bracelets It's true that we do have some flexibility in that way, right?",
            "Described in my library.",
            "You can do things like that please.",
            "That sounds fun.",
            "Yeah, yeah.",
            "Yeah.",
            "It is true that.",
            "If you have a smooth process.",
            "Can effectively deal with this as well.",
            "I would take you think about trying to get the eigenfunction.",
            "Yeah, if you really want this.",
            "Actually.",
            "Mouse.",
            "Right?",
            "Right, it's worth sticking around.",
            "I mean the eigenfunction idea, which I think.",
            "That's right, it's so it's nice to work with that, but at times when you're conditioning on data that the eigen representation is usually on this.",
            "I don't know if this is really right.",
            "I was saying it this way, but kind of this prior covariance that.",
            "The eigen function only looks at the covariance and doesn't care about what the data might be doing to it later, right and.",
            "And something that's more local might be a little more responsive to if the data is pushing to do something that maybe you didn't throw in the first 10 eigen Eigen functions.",
            "So this local bases.",
            "Game is that I give you.",
            "Some number K basis yeah.",
            "Which are best places?",
            "Yeah.",
            "Because PCI is basically a Ultima Linea.",
            "Racist.",
            "If you play some sort of average game then that will be the right place.",
            "Yeah, yeah.",
            "The dates are inputs.",
            "Yes yeah, average over the data location.",
            "No, no question first.",
            "Who is also.",
            "Displays consoles here is what he said it's.",
            "GPS came from.",
            "Number.",
            "What has been done in recent years like in?",
            "Horse applications and basically what you have there is.",
            "It's basically the basis function representation again, and when you show these Maps here and there's so much uses.",
            "Or through the English version version of it, see how I wanted Bullet?",
            "Yeah, basically what is happening is similar thing that with the basis functions that if you have a.",
            "Really Sports Creek.",
            "You're actually you're not able to capture anything else, but where is wrong or license anyway?",
            "Put it.",
            "With her right?",
            "People.",
            "And that's it.",
            "For me it seems a little bit like we've been doing, kind of like a loop and coming back.",
            "Like like this for stop listening when it's it's kind of like going back to the internals and again but from different directions.",
            "And is there any way you actually fell short, usurp ends, his or short pregnancies?",
            "That seems to be the most problem, yeah?",
            "Yeah, and and it seems like that's seems to be the hardest thing to do with the basis approach that voice lots of degrees of freedom for, you know, a little fuzz.",
            "On top of this big structure.",
            "That is not so.",
            "That's a great question, and my experience has been offer the crude part bases often do quite well.",
            "And for this for the rest of this part, if you really care about, you know small scale structure.",
            "Boy, it's hard.",
            "It's hard to do much better than some.",
            "You know, basic kind of you know, Gaussian process approach with, you know you know exponential covariance or something like that that.",
            "In some ways, for that boy, that's a good way of working with something that otherwise you'd have to have, you know, a million little basis functions to do that last little piece of your problem, so I don't have a good answer for that.",
            "It seems to be like, oh, you could do it fix spaces."
        ],
        [
            "Function.",
            "Plus comeback support based on variance, but yeah.",
            "Yeah.",
            "Short times I've never done it.",
            "Yeah, I mean so so so I know people that the small compact support is exactly the same even want to move into this covariance representation because that thing if you look at the covariance matrix or maybe the precision matrix of that thing, it tends to be mostly zeros and the sparse representations might do pretty likely to do very well in that arena, so that.",
            "You know that the small wiggle part.",
            "Maybe Gaussian process is the right way to go with that, because the covariance is going to have lots of zeros, it will just have stuff around the diagonal or very small sort of neighborhood structures.",
            "Things will change in movies.",
            "Resolution analysis about approaching for showed me yeah.",
            "You just said about global smoothness.",
            "You just have a different place and you push holding in space.",
            "Yeah, I'm sure like it's the wavelets, right?",
            "Yeah, I'm just wondering.",
            "The way to do it, or even do something.",
            "Same thing with GP.",
            "Assuming something else today.",
            "That's good, that's a good point already.",
            "So it's basically saying that OK, what do they do at the very small?",
            "Or at the very small wavelets?",
            "Just just have some?",
            "Yeah.",
            "Food drinking frequency goals with different space.",
            "If you see any low frequency here behind.",
            "So.",
            "Just wondering.",
            "Yeah.",
            "I'm allowed to have no answers for lots of things.",
            "This seems quite sensible, I think.",
            "So as quickly as I have wavelengths found a way into special statistics, or is it?",
            "Yeah yeah, I would say like there's there.",
            "There's a paper by I think Doug Niche.",
            "Gonna few other people that basically said, Oh yeah, if you use this wavelet basis.",
            "That here's how you can approximate kind of standard covariance models that you do in the spatial world, and it ends up being, you know, not quite diagonal types of things in the wavelet basis, but.",
            "It adds a little extra dependence.",
            "Trying to mimic those things.",
            "But yeah, I think mathematically you can always.",
            "You can do it.",
            "You can go back and forth with those things.",
            "And it seems like a lot of these answers or what you want is, you know, well computation or for the problem at hand.",
            "You know what's the way to do that, but yeah.",
            "So so those happen related.",
            "Website.",
            "Alright, so so here's.",
            "Make me a better feeling.",
            "Shapeless, I think shape lights are out there, too, right?",
            "Yeah, beyond that.",
            "Yeah, sure.",
            "Alright, so so problem that I know I know in the in the Gaussian process world I'm running across right now which I don't know if you guys have thought in more of the machine learning world.",
            "It's been looking at spatial dimensions using just two or three or something kind of simple.",
            "Often we're trying to interpolate what a big computational model would have said given an input space, and so the response tends to be pretty smooth.",
            "But now the input space is, you know 10 or 20 dimensional sort of thing.",
            "And so we have often alright.",
            "It's hard to know what can you do with bases in that sort of arena.",
            "We've sort of.",
            "Pon and gone with Gaussian processes there, which is good for some problems for other problems.",
            "Basically have no good answer on that and I don't know if you guys have wrestled with that problem at all or have any thoughts there.",
            "You can't do like.",
            "So if you do a digit recognition but dancing process, you have an 8 by 864 dimensions already, and then people are using them for images.",
            "I think that the.",
            "To be more open.",
            "Assuming away is a good one, yes.",
            "So one thing that Chris is looked at is where you you effectively have.",
            "I would interpret it as you got a linear projection of your data onto a low dimensional space.",
            "And then you operate on that and you optimize the covariance function with respect to that projection characterization.",
            "That was also.",
            "I think my worry would they most of the time it's probably nonlinear.",
            "I mean I don't believe in high dimension spaces, personality their high dimension of things which are really low dimensional, but I believe they're also nonlinear, so.",
            "This is a simple.",
            "Can I ask?",
            "And so you were talking about some problems about using the GPS in the television series?",
            "Can you give some examples of things that you feel is not working?",
            "Yeah.",
            "Basically, all right so, so I think this is a theorem or meta theorem for anything I've ever done that's been useful.",
            "Is that in hindsight it was a trivial problem.",
            "And then it only seemed.",
            "Maybe it wasn't that way when I was working on it, but then the back, oh, you only depend on 2 dimensions.",
            "So so yeah.",
            "So let on the computer model world.",
            "What typically happens is we might consider 20 dimensions.",
            "Anne, what happens and we will have a univariate response or looking at temperature or something that's output after that.",
            "So bunch of parameters, strength of materials, equations of state, and things like that.",
            "So lots of parameters controlling this thing, but and it wasn't, the guys were idiots that wrote the code.",
            "It turns out we're looking at it for one particular type of experiment or something like that, and so we're really exercising this code in a fairly narrow range.",
            "And so if we look at the response, the temperature of the system as we mess around with these parameters, it turns out.",
            "It's dominated by three or four dimensions, three or four inputs are really controlling everything, and they also control it in a very smooth sort of predictable kind of way, so that after we did this whole 20 dimensional Gaussian process, estimated ranges in each of the dimensions and things like that.",
            "It turns out 04 dimensions is all that's really going on.",
            "Just like you said Neil, that really there was just.",
            "It was just a four dimensional surface that we're trying to fit in.",
            "Our model was just barely smart enough to figure out it shouldn't try on the other 16 dimensions and then the problems that are hard are when.",
            "All right, all 20 are kind of acting about evenly in this problem.",
            "And so even if it's smooth and they're acting evenly.",
            "It becomes hard and if they act just in an additive sort of way, well, maybe we got a chance.",
            "But any real problems doesn't really happen that they actually start interacting in places of this kind of parameter space.",
            "You're searching, and it's it's those problems where everyone is kind of contributing some in a slightly nonlinear away in a slightly interactive way.",
            "And I don't know it may just mean that's an impossible problem.",
            "Increasing estimation on the.",
            "Kerbal feel.",
            "Meaning, for example, if you press 1, they're just for that matter, by used way, yeah.",
            "You actually observe that Doris Timation is work so bad.",
            "For a better use for, yeah, well, so usually what happens is somewhere in the model.",
            "You have parameters that are controlling which dimensions you're looking at, and it figures out not to look at those other 16.",
            "It figures it set something to 0 for 16 of AM of those numbers, and for the numbers it's around 1:00 or something like that.",
            "So your model is really figuring out that I only need to fit a four dimensional model, and that's when you really do have a 20 dimensional model is when.",
            "It seems like nothing is near anything in that in that case and so.",
            "You can make a Gaussian process.",
            "You look at how do you do on your holdouts and you might as well just guess the mean.",
            "In fact, usually you don't even do that well.",
            "So it's in some sense that you're not.",
            "I don't know what's happening in the Gaussian if you just thought you were a Gaussian process and thought about covariance, is that everything becomes far away because nothing is close in all dimensions that matter.",
            "Yeah, yeah.",
            "And in the fourth dimension case, it can be misleading because it may be the the other dimension.",
            "Something quite small scales going on and you just don't have the density of coverage to see it.",
            "So you're just saying, oh, that's part of my game lease term because I can't differentiate between that and noise, but it might be that there's actual structure.",
            "Yeah, yeah.",
            "But I think another extremist if you go to really high dimensions and would like something text classification, it turns out removing inputs doesn't help.",
            "I mean it hurts.",
            "You use all of them.",
            "The model had typically linear budget India.",
            "So interesting is we had lots of problems with really high dimensions.",
            "You really want to use all the dimensions because every dimension makes a little contribution to the estimation of noisy things.",
            "And so in these text models I mean are they what sort of models do people use?",
            "Are they kind of?",
            "Additive sorts of things or.",
            "He's like, um, Vectorable will extend the words they might get down to their past 6000 right now and then just count the number of.",
            "Each word, so they removed all the structure.",
            "But this document contains the word wrestling and WWF whatever.",
            "And so we've got some representation.",
            "And then they they might just put that in the kernel learning algorithm that could be against.",
            "Sometimes they also used a wide caution things I guess, but.",
            "So I think that you should think about that every every input makes a little contribution.",
            "Deciding on some classification service or something, yeah.",
            "I mean, you think simply function of each class right?",
            "And that option might be quite complicated like yeah.",
            "Quite implicated, so in principle, if you actually want to estimate the density, you might need more complicated schemes and you know how you mention like yeah.",
            "Next notification it boils down to just say one or minus one or so you don't really.",
            "I mean, as long as you get the boundary well.",
            "Yeah question, I think progression.",
            "He won this year, though there is this answer that you want because when you do the computer emulation, so you're predicting for an exact value which is noiseless and yeah.",
            "Where is when we're doing this texting?",
            "Maybe 80% full minutes anyway, you're not expecting to exactly emulate whatever the text prices you know you turn location when you're doing this emulation.",
            "You really expect the only on certain to be to do that with what the functions doing between your design, yeah?",
            "So you really want.",
            "Yeah, that's just trying to be.",
            "Right, so so I have to say that the emulation in that sense might be an easier problem because you know it has to be something smooth and it has to interpolate, and so you're not going to be that easily fooled by something that's close but doesn't interpolate that.",
            "Whereas.",
            "Whereas if these other noisier problems you have to consider, all these things right?",
            "And nothing you can't rule out close but not exact things at all.",
            "And so I think, yeah, this computer emulation world is in some sense sort of ideal for this Gaussian process.",
            "'cause if you just find something that was smooth and through that.",
            "Yeah.",
            "Yeah, so yeah, forget it.",
            "Yep.",
            "So I mean, it's also true in numerical tolerances and things like that that you know lots of stuff shows up as small amounts of noise.",
            "But I mean then again, at the problems we can't do.",
            "We totally can't do, and the ones we can are kind of trivial.",
            "And so you know, if I suspect if those things are a big problem.",
            "Those problems get hard fast.",
            "And then often because you're like I said, usually you're looking at it in some really restricted way.",
            "We're trying to estimate, you know, these types of problems, which are the code was made to feel this whole room, but we're really looking at that chair for the code within there.",
            "There's not all that much going on in terms of.",
            "You have some things like that.",
            "But I think you're right if.",
            "If it gets fairly nonlinear, then you're.",
            "I can't imagine we can do all that well.",
            "Do these they often component based, so in the possible system that feeding into other parts of the system where you could say actually, why don't I just look inside the system?",
            "So my inputs are only feeding into that, and then that's another input into the later.",
            "Later you get to decompose and then people try that.",
            "Yeah, alright, so that so it's hard because it's the one nice thing about this computer model world is some guy you know, some guy and his team spent 10 years building this model right?",
            "And then?",
            "If you could just run it a few times, you can look at it and do stuff with it.",
            "Actually getting inside and knowing the people that you know can can get inside those things is actually a much harder thing in practice to do then you might guess.",
            "So yeah, in a few cases where you can, it seems to be pretty beneficial and also things like can you get an joins, derivatives and things like that out or other types of information out.",
            "That can also be huge, it's just boy.",
            "It's so hard and practiced.",
            "To have this very decoupled problem.",
            "Get more kind of convolved within itself that it's just.",
            "It's hard in practice to get to that point.",
            "But yeah, I think that's right.",
            "And that does that.",
            "Can simplify a lot of things.",
            "Has it ever happened in my life on a real problem?",
            "No."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thanks, you're not going to read the title of my talk.",
                    "label": 0
                },
                {
                    "sent": "So alright, we better fix that.",
                    "label": 0
                },
                {
                    "sent": "So to kill time in the classes when I would teach, so we'd have to guess which ones were my albums and which ones were the ones my kids had ordered on iTunes.",
                    "label": 0
                },
                {
                    "sent": "So, and the students are pretty good.",
                    "label": 0
                },
                {
                    "sent": "They can figure that out.",
                    "label": 0
                },
                {
                    "sent": "If it was like older than 20 years ago, probably mine.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so I was going to talk about some work I have done in Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I think my wife got that one, but a lot of the greatest hits often come in the party mix, and so usually that's a pretty good one to get.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so chime in if you have questions anytime.",
                    "label": 0
                },
                {
                    "sent": "Chime in and go and do that.",
                    "label": 0
                },
                {
                    "sent": "It'll be good.",
                    "label": 0
                },
                {
                    "sent": "Luckily Luckily we recorded right so we can always go back and look at the real detailed interesting questions here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I was going to talk about mostly Gaussian process models.",
                    "label": 0
                },
                {
                    "sent": "An looking at him from a convolution setting, which I think has a lot of overlap with a lot of these radial basis function models and certainly they share a lot of the same ideas.",
                    "label": 0
                },
                {
                    "sent": "So I think a lot of this won't be.",
                    "label": 0
                },
                {
                    "sent": "Terribly new in terms of content, but maybe you'll get a little bit of insight about how a knucklehead statistician might have thought about these sorts of problems.",
                    "label": 0
                },
                {
                    "sent": "And maybe there's something useful to be had there, but.",
                    "label": 0
                },
                {
                    "sent": "Course I'll throw the disclaimer, maybe nothing useful will come with this, so each at least try to enjoy it and trying to learn something anyway, so ask questions anytime you want.",
                    "label": 0
                },
                {
                    "sent": "This is just eye candy since I work for the government, I often have to talk with, you know government officials on different sorts of things, and will often complain if there's not enough eye candy and things like that so.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I always just try to do that now just to.",
                    "label": 0
                },
                {
                    "sent": "Just to make sure we can do that.",
                    "label": 0
                },
                {
                    "sent": "I'll see if I can.",
                    "label": 0
                },
                {
                    "sent": "So if I can advance this, otherwise we're just going to talk on this one, slide the whole time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So basically what I thought I'd do is I'll go through a real basic example and kind of go through a little too much detail so you don't have to pay attention to all this, but I thought it might be worth it on the first one just to sort of give you a sense of alright, here's what I'm thinking about going through this sort of problem, and I'll look at it from a Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "And then and then we kind of back off the resolution and look at a number of approaches for dealing with problems that involve these convolutional kernel types of methods.",
                    "label": 0
                },
                {
                    "sent": "And so here is just a real simple spatial problem.",
                    "label": 0
                },
                {
                    "sent": "So we've got a spatial dimension 1 dimensional here right now.",
                    "label": 0
                },
                {
                    "sent": "And and what we're going to do is just have a bunch of basis functions here, so Gaussian kernels.",
                    "label": 1
                },
                {
                    "sent": "And then what we're going to do is just represent spatial process.",
                    "label": 0
                },
                {
                    "sent": "So something smooth going through this one dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Here as just a weighted combination of these kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so so Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "The actual spatial process we're going to represent us something smooth, just a weighted combination of these things.",
                    "label": 0
                },
                {
                    "sent": "And then if we go ahead and do it this way, you have this nice sort of regression like representation.",
                    "label": 0
                },
                {
                    "sent": "You can represent the process as just.",
                    "label": 1
                },
                {
                    "sent": "A matrix which is built up from the Kernels Times acts the weights that go with each of the kernels.",
                    "label": 0
                },
                {
                    "sent": "Alright, so X will be the weights that go with the kernels.",
                    "label": 0
                },
                {
                    "sent": "K will usually be the kernel in Kane.",
                    "label": 1
                },
                {
                    "sent": "Will be this matrix we build up by saying what the kernel is at.",
                    "label": 0
                },
                {
                    "sent": "Different spatial locations corresponding to different locations and so usually I'll use like the Omega to denote where the spatial location is for the kernel.",
                    "label": 0
                },
                {
                    "sent": "So we just have a bunch of kernels that are spatially kind of next to each other, but they're living on a lattice, something nice and regular, and that's typically what I'm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be thinking about here and hopefully we'll see wise as we move on.",
                    "label": 0
                },
                {
                    "sent": "OK, so so if we pick a different set of values for the weights these ex Jays and so now the XJ's are basically proportional to these Heights that we see here is summer negative summer positive.",
                    "label": 0
                },
                {
                    "sent": "We Adam together and then we get this spatial function here.",
                    "label": 0
                },
                {
                    "sent": "OK, so just a linear combination of basis functions.",
                    "label": 0
                },
                {
                    "sent": "That's just regression as far as far as we know that right now.",
                    "label": 0
                },
                {
                    "sent": "Now it will be a little bit Bayesian will put some sort of prior on.",
                    "label": 0
                },
                {
                    "sent": "These weights will say these weights and the simplest thing to put out there as IID normal.",
                    "label": 0
                },
                {
                    "sent": "Zero with maybe some precision.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you specify it and this is going to shrink all the weights together so there's other ways to shrink weights down, right?",
                    "label": 0
                },
                {
                    "sent": "Guys that do things like wavelet and stuff like that, like to zero out lots of the weights and just leave a few standing.",
                    "label": 0
                },
                {
                    "sent": "And this prior is really just going to shrink them all in a common sort of way, so it's not going to reduce the number of weights that we have to work with.",
                    "label": 0
                },
                {
                    "sent": "Alright, so and again we'll see it because this gives you lots of duality with the Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Why this model is coming about?",
                    "label": 0
                },
                {
                    "sent": "OK so the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prior there is on those weights they have is common mean zero normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright, and then we're going to do is try to fit these things to data.",
                    "label": 0
                },
                {
                    "sent": "So if this is our data here, we might see the data are centered at this unknown function Z or call it in Canada.",
                    "label": 0
                },
                {
                    "sent": "We say zed in Canada.",
                    "label": 0
                },
                {
                    "sent": "How may we do?",
                    "label": 0
                },
                {
                    "sent": "I'm sure I'm not gonna be able to do that, so I'm still going to use going to talk in American still.",
                    "label": 0
                },
                {
                    "sent": "So, so we have this process.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to the data, maybe are IID distributed about the about that smooth process.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we might have a regression sort of model here that says our data are what the smooth process tells us, plus some IID error.",
                    "label": 0
                },
                {
                    "sent": "OK, and so given that data Now, we'd like to estimate what the axes are.",
                    "label": 0
                },
                {
                    "sent": "Maybe some parameters about the X is how much shrinkage is there?",
                    "label": 0
                },
                {
                    "sent": "What's the variance of those of that prior for the X is?",
                    "label": 0
                },
                {
                    "sent": "And also stuff like what's the variance of the errors?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're just going to do this one time, so there will be other slides like this and we're going to rip through him, but we'll do this one time.",
                    "label": 0
                },
                {
                    "sent": "OK so so so alright so I live in the stat world, right?",
                    "label": 0
                },
                {
                    "sent": "We'll do likelihood functions and things like that or likelihood function basically says that data are IID normal with a variance one over Lambda Y. I'll use precisions and just turns out that.",
                    "label": 0
                },
                {
                    "sent": "It depends on what you grow up with.",
                    "label": 0
                },
                {
                    "sent": "Do you like to do precisions or variances?",
                    "label": 0
                },
                {
                    "sent": "I'll do precisions so it has precision Lambda, why that controls the variation of the scatter about the about the smooth process, so we're just saying that the data are IID normal zero with precision Lambda Lambda, Y. Alright, and we have N data points.",
                    "label": 0
                },
                {
                    "sent": "We'll have a kernel that we build up.",
                    "label": 0
                },
                {
                    "sent": "We build up this K matrix from the kernels.",
                    "label": 0
                },
                {
                    "sent": "And so then we're just going to have a few priors to finish things off.",
                    "label": 0
                },
                {
                    "sent": "We're going to have the normal zero Lambda X prior for the precision, so we're going to shrink all those weights together.",
                    "label": 0
                },
                {
                    "sent": "We're also going to have him live near 0 for right now, and so all these problems will be centered near 0.",
                    "label": 0
                },
                {
                    "sent": "Just it makes the math a little bit easier.",
                    "label": 0
                },
                {
                    "sent": "And conceptually, it's not a big deal if you want to send him someplace else.",
                    "label": 0
                },
                {
                    "sent": "If you need to move the data somewhere, well, you had a mean function in there, or maybe you center the data so we're not going to worry about that here.",
                    "label": 0
                },
                {
                    "sent": "Alright then.",
                    "label": 0
                },
                {
                    "sent": "We also have a prior for the precision parameter for the weights Lambda X, so that's going to tell you how much do you shrink the weights down, and we also have a prior for the error variance.",
                    "label": 0
                },
                {
                    "sent": "So really, it's just sort of three things there.",
                    "label": 0
                },
                {
                    "sent": "There's weights.",
                    "label": 0
                },
                {
                    "sent": "X is.",
                    "label": 0
                },
                {
                    "sent": "There's a precision for those weights, Lambda X, and then there's the error precision Lambda.",
                    "label": 0
                },
                {
                    "sent": "Why so real basic problem here?",
                    "label": 0
                },
                {
                    "sent": "And then will summarize everything in terms of the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have the posterior distribution of these three things.",
                    "label": 0
                },
                {
                    "sent": "The Axis is a vector, but the other two things are just scalars, and it just has a posterior distribution that's just proportional to the likelihood times the priors.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is really the thing that if you want to point estimate you might think about optimizing this, or you might sample from this distribution and look at posterior means or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so and So what will typically do is here's our posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "We live in the stat world.",
                    "label": 0
                },
                {
                    "sent": "We often can recognize the full conditional distributions at that.",
                    "label": 0
                },
                {
                    "sent": "If I look at the X is conditional on everything else I can figure out that oh, this thing is minus 1/2 something quadratic in X, so I know that that's going to be something that's normal, so I know conditionally that X is going to have a normal distribution given everything else.",
                    "label": 0
                },
                {
                    "sent": "Likewise, I can recognize the distribution for the two precision parameters.",
                    "label": 0
                },
                {
                    "sent": "That if I use a gamma prior for those things, the full conditional for the precisions is also going to be gamma.",
                    "label": 0
                },
                {
                    "sent": "OK so so now I can say, OK, I can recognize these things.",
                    "label": 0
                },
                {
                    "sent": "I've got a full conditional that's normal for X full conditionals that are gamma for Lambda X and Lambda Y.",
                    "label": 0
                },
                {
                    "sent": "Why is this nice?",
                    "label": 0
                },
                {
                    "sent": "'cause now we could just use this Markov chain Monte Carlo and actually just draw samples from this big posterior.",
                    "label": 0
                },
                {
                    "sent": "And that's typically what we do for these sorts of things so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Might be true.",
                    "label": 0
                },
                {
                    "sent": "Everyone knows this stuff, right?",
                    "label": 0
                },
                {
                    "sent": "Famous last words.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's just a Gibbs sampler for a bivariate normal.",
                    "label": 1
                },
                {
                    "sent": "You start at some initial guess and maybe you can actually sample from the distribution, but probably not.",
                    "label": 0
                },
                {
                    "sent": "So you take an initial guess and then you update one of the components, leaving the other one fixed and then update the next component.",
                    "label": 0
                },
                {
                    "sent": "Leave the other one fixed and just keep cycling through like that.",
                    "label": 0
                },
                {
                    "sent": "So if you have a bivariate normal distribution, you're going to have normal full conditionals and he just generate Z1 given Z2Z2 given Z1 and then you just start.",
                    "label": 0
                },
                {
                    "sent": "Just go back and forth continually doing that.",
                    "label": 0
                },
                {
                    "sent": "And you'll end up with a.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking sample path through this thing.",
                    "label": 0
                },
                {
                    "sent": "And then if you take this whole collection of points, right?",
                    "label": 0
                },
                {
                    "sent": "That's a basically a correlated draw for your posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "The thing that you want, and so if you want to estimate the mean of this distribution, just the mean of the sample will do.",
                    "label": 0
                },
                {
                    "sent": "If you want to estimate quantiles of quantiles, will do for that so.",
                    "label": 0
                },
                {
                    "sent": "So that's typically how we can go ahead and estimate these things.",
                    "label": 0
                },
                {
                    "sent": "There's other ways to do that, but here's a good generic approach for doing that.",
                    "label": 0
                },
                {
                    "sent": "And so if we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have like just going to do this once, right?",
                    "label": 0
                },
                {
                    "sent": "It won't be painful one time, so here is just a real simple example where I've got five data points.",
                    "label": 1
                },
                {
                    "sent": "I have a space that's just between zero and one.",
                    "label": 0
                },
                {
                    "sent": "I have basis functions that are just these normal kernels centered at.",
                    "label": 1
                },
                {
                    "sent": "Six points between minus .3 and 1.2.",
                    "label": 1
                },
                {
                    "sent": "Alright, so I make the colonels go a little bit beyond the space I want to predict.",
                    "label": 0
                },
                {
                    "sent": "That way I'm going to eliminate these edge effect so the kernels go beyond where.",
                    "label": 0
                },
                {
                    "sent": "I have my data and want to build my model.",
                    "label": 0
                },
                {
                    "sent": "And then I just go ahead and do that alright.",
                    "label": 0
                },
                {
                    "sent": "And then I also say, alright, the kernels will be normal with a standard deviation of three.",
                    "label": 0
                },
                {
                    "sent": "I'll specify priors because I know what this error precision is for this problem, I'm going to have a strong prior on what the error precision is, and I'll have a fairly weak prior on how much shrinkage we do on the kernels on the X is.",
                    "label": 0
                },
                {
                    "sent": "And so now we can just go ahead and do that MCMC scheme estimating the multivariate normals for the X is sample those things and then sample the full conditionals for the precisions.",
                    "label": 0
                },
                {
                    "sent": "And here is just a train or sequence of samples using MCMC for the X is.",
                    "label": 0
                },
                {
                    "sent": "There are six of 'em here and then the two precisions.",
                    "label": 0
                },
                {
                    "sent": "There's the MCMC chain for that, so I'm just running through and doing this MCMC scheme to estimate this stuff and hopefully I'll get some function that goes nice and smoothly through this data.",
                    "label": 0
                },
                {
                    "sent": "Of course, I ought to.",
                    "label": 0
                },
                {
                    "sent": "I have nice, smooth, wide kernels here, so that should probably work pretty well.",
                    "label": 0
                },
                {
                    "sent": "And sure enough, here are if I go back and get those realizations of X is I can run up to the kernels and say, alright, here is the predicted disease or realizations of disease that I've got.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I could try to summarize that maybe pointwise posterior mean incredible intervals or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a lot of detail for that one problem.",
                    "label": 0
                },
                {
                    "sent": "Well, we won't do that so much anymore, but.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, well there's sort of a background.",
                    "label": 0
                },
                {
                    "sent": "Here's how I.",
                    "label": 0
                },
                {
                    "sent": "Have a look at this sort of problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "And so here's an example where I've got some data I'll come up with a normal kernel, the SDF 2 here.",
                    "label": 0
                },
                {
                    "sent": "OK, so with the kernel is going about this big relative to the data.",
                    "label": 0
                },
                {
                    "sent": "How do we choose that thing was going to make some important or how it effect on how things come out, but here's a resulted fitted process.",
                    "label": 0
                },
                {
                    "sent": "Here is the posterior distribution for the shrinkage.",
                    "label": 0
                },
                {
                    "sent": "Lambda X for that parameter and the precision Lambda Y here.",
                    "label": 0
                },
                {
                    "sent": "So there it is for one choice of the kernel if I.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A different choice of Colonel.",
                    "label": 0
                },
                {
                    "sent": "I might get a fairly different answer so I can make that kernel a lot narrower, so it's with is only about you know, plus or minus two here and SD of one.",
                    "label": 0
                },
                {
                    "sent": "Well, I could have a process that goes through the data pretty.",
                    "label": 0
                },
                {
                    "sent": "Pretty strongly, maybe too much, and so that kernel width is basically saying how smooth is this process going to be?",
                    "label": 0
                },
                {
                    "sent": "And certainly that's something we're going to want to estimate most the time when I haven't talked too much about that yet.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's sort of the basic idea here is kernels using for using kernels to estimate these smooth processes.",
                    "label": 0
                },
                {
                    "sent": "And now I want to relate these a little more to a Gaussian processes here, so typically in a Gaussian process model, I'm going to think very discreetly here.",
                    "label": 0
                },
                {
                    "sent": "I might want to predict the points at a set of spatial locations across.",
                    "label": 1
                },
                {
                    "sent": "Here it is between zero and 10, so I have a bunch of spatial locations I want to predict this thing in a Gaussian process model is basically going to say this big vector, which is nice.",
                    "label": 0
                },
                {
                    "sent": "Finally finally space thing.",
                    "label": 0
                },
                {
                    "sent": "Is going to be normal with some sort of covariance?",
                    "label": 0
                },
                {
                    "sent": "And so if we build the cover or if we build this this kernel process here with the case.",
                    "label": 0
                },
                {
                    "sent": "We can go ahead and build this K matrix right.",
                    "label": 0
                },
                {
                    "sent": "We said Z is equal to K * X and so if we just go ahead and follow this thing through, OK, well if I looked at Z so it's K times some normal 0.",
                    "label": 0
                },
                {
                    "sent": "Here it's going to be normal 01 vector vector of IID normal 0 ones.",
                    "label": 0
                },
                {
                    "sent": "Well, I can just go ahead and say OK Well Z is just going to be have a covariance.",
                    "label": 0
                },
                {
                    "sent": "That's if I marginalized out the X.",
                    "label": 0
                },
                {
                    "sent": "It's just going to have covariance.",
                    "label": 0
                },
                {
                    "sent": "That's K * K transpose.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's nothing new at all right here, but.",
                    "label": 0
                },
                {
                    "sent": "But now I can go ahead and say, well, there's probably lots of ways I might be able to represent this.",
                    "label": 0
                },
                {
                    "sent": "K * K transpose thing.",
                    "label": 0
                },
                {
                    "sent": "And So what we typically do for this thing is we.",
                    "label": 0
                },
                {
                    "sent": "Typically generate a normal distribution by doing something exactly like this.",
                    "label": 0
                },
                {
                    "sent": "I'll generate IID normal 0 ones.",
                    "label": 0
                },
                {
                    "sent": "I'll hit it with some matrix to make a Gaussian process with the covariance function I want.",
                    "label": 1
                },
                {
                    "sent": "So if I know the covariance matrix, typically I do something like Cholesky or something like that to go ahead and do that, and so if I just do a Cholesky factorization of this thing, say Cholesky, with pivoting, the columns of K end up looking like these vectors here.",
                    "label": 0
                },
                {
                    "sent": "And so I can think of it discretely.",
                    "label": 0
                },
                {
                    "sent": "Or I could just go ahead and make it continuous version that really, if I'm generating a normal distribution, I'm really hitting a bunch of basis vectors with the normal with IID normal 0 ones, and so for a given covariance function or covariance matrix I basically have induced the matrix by saying I'm going to use Cholesky and I'm going to look at the Rose of the Cholesky or the columns of the Cholesky matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, and so typically we might not take a whole infinite range of these things, but we might take a small number of these basis functions and so if you have a covariance, can represent efficiently with maybe 5 basis or 10 basis functions, you can get away with doing things like this.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we have a covariance function that looks like this and this is a covariance that has real smooth spatial dependence.",
                    "label": 0
                },
                {
                    "sent": "It says basically the process is going if I take an element here, it's going to be correlated to nearby things and then the correlation is going to die off like a big Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So here's a Gaussian covariance function.",
                    "label": 0
                },
                {
                    "sent": "Find decompose that with Cholesky, say with pivoting I get basis functions that look like this, so these are different than these kernels.",
                    "label": 0
                },
                {
                    "sent": "But there's other ways we can decompose this thing right?",
                    "label": 0
                },
                {
                    "sent": "We can decompose this covariance matrix and say.",
                    "label": 0
                },
                {
                    "sent": "OK, we've got this.",
                    "label": 0
                },
                {
                    "sent": "We can use singular value decomposition and we get a slightly different looking basis.",
                    "label": 0
                },
                {
                    "sent": "And the kernels are just another another way of generating this covariance function with different type of bases.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I use the kernels I can get this exact same covariance function.",
                    "label": 0
                },
                {
                    "sent": "So really the same covariance matrix I could decompose in different sorts of ways, meaning I've got different basis representations for the same basic normal model for the ZZ is normal, mean zero, and covariance Sigma, and there's different ways to get at that, and that's probably useful if we're going to try to think about how do we want to model other sorts of things as we're going along here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another question is, well, how dance?",
                    "label": 0
                },
                {
                    "sent": "If we go back to the kernels, how dense do we need to make the kernels so I just separated the kernels about 1 standard deviation apart.",
                    "label": 0
                },
                {
                    "sent": "I could have made him a lot denser and made the precision or the variance of those things go down and I get basically the same process so.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                },
                {
                    "sent": "So here's kernels separated by about 1 standard deviation.",
                    "label": 0
                },
                {
                    "sent": "Here's the induced covariance function or covariance matrix that you get if I make the kernels twice as dense and make the variance go down by half.",
                    "label": 0
                },
                {
                    "sent": "I get this picture and the covariance is almost identical.",
                    "label": 0
                },
                {
                    "sent": "Same thing here.",
                    "label": 0
                },
                {
                    "sent": "If I have a real dense set of kernels and I appropriately knock down the variance attached to those kernels, I basically get the same covariance again.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that I could basically thin out my basis matrix to a point where I can efficiently represent the same covariance with just say six basis functions instead of 20.",
                    "label": 0
                },
                {
                    "sent": "And it's also saying that it's basically the same model that if I had six basis functions or 20 basis functions, the implied model on that Z is basically the same.",
                    "label": 0
                },
                {
                    "sent": "So so if you know the width of these things.",
                    "label": 0
                },
                {
                    "sent": "And you need to reduce.",
                    "label": 0
                },
                {
                    "sent": "You need to get your basis functions down on the number of bases you want to work with.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially a much lower rank approximation.",
                    "label": 0
                },
                {
                    "sent": "This matrix than this one.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, absolutely yeah, yeah, all complaints are welcome too.",
                    "label": 0
                },
                {
                    "sent": "That would be great.",
                    "label": 0
                },
                {
                    "sent": "I guess the point is that and so the requesting is of course one can.",
                    "label": 0
                },
                {
                    "sent": "You take Micah.",
                    "label": 0
                },
                {
                    "sent": "You can basically, if you have some kind of everywhere and integrate it out, can actually get it right.",
                    "label": 0
                },
                {
                    "sent": "Having sex isn't the same as having engine.",
                    "label": 0
                },
                {
                    "sent": "If you have you have a number of different points, right?",
                    "label": 0
                },
                {
                    "sent": "So you actually want.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Then you only have 6.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "So that I mean that that's a good point that.",
                    "label": 0
                },
                {
                    "sent": "If you do something with the Gaussian process, you can make it interpolate any data set you wanted really for any function for any width of these kernels or bandwidth, and you could get it to go through and this thing is never going to do that.",
                    "label": 0
                },
                {
                    "sent": "If you have 100 points to go through that are kind of all over the place.",
                    "label": 0
                },
                {
                    "sent": "In fact, you're going to probably 100 basis to get there if it's very wild.",
                    "label": 0
                },
                {
                    "sent": "So that's absolutely right then.",
                    "label": 0
                },
                {
                    "sent": "It's never going to be an interpolator in that sort of sense, but it's also a warning that if.",
                    "label": 0
                },
                {
                    "sent": "If this isn't doing the job.",
                    "label": 0
                },
                {
                    "sent": "And you need to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Well, there's probably a mismatch between what you're picking as your bandwidth for your kernel and what the process is you're trying to fit.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're absolutely right timing.",
                    "label": 0
                },
                {
                    "sent": "Good complaint.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and you can do it.",
                    "label": 0
                },
                {
                    "sent": "I mean you can do a continuous version of this sort of thing, and all I want to say here is that you can actually write down the induced covariance function that you've got, and as the number of points you've got here, it gets denser and denser.",
                    "label": 0
                },
                {
                    "sent": "This induced covariance function approximates this continuous thing that you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so I talked about basis functions and adding them together.",
                    "label": 0
                },
                {
                    "sent": "It's an equivalent way of thinking about this thing, which is going to be useful is we could talk about a bunch of white noise that we have an smoothing it out with the kernel.",
                    "label": 0
                },
                {
                    "sent": "Turns out if we have the same kernel all the way through, so we're just convolving some white noise with the kernel.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent, so if we just had six places where we have white noise, that's just six basis functions really.",
                    "label": 0
                },
                {
                    "sent": "And this is nice because if we think of it this way, then we can also do other sorts of things, like have the kernel width change as a function of spatial location, and this might give us a richer set of models to work with.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, my artwork.",
                    "label": 0
                },
                {
                    "sent": "I guess I better talk about it some so.",
                    "label": 0
                },
                {
                    "sent": "So what sort of things might we do if we what's nice about this is whatever kernel specification you make you induced a valid covariance function.",
                    "label": 0
                },
                {
                    "sent": "So that's a nice.",
                    "label": 0
                },
                {
                    "sent": "It might be a handy way to mess with well.",
                    "label": 0
                },
                {
                    "sent": "How do I pick a covariance function?",
                    "label": 0
                },
                {
                    "sent": "Traditionally, if you work in covariances, you have to pick a covariance rule that's positive semidefinite and obeys all the rules.",
                    "label": 0
                },
                {
                    "sent": "That covariance function must obey.",
                    "label": 0
                },
                {
                    "sent": "But if you work with kernel, you can do anything to it, and it's going to induce a covariance, so it might be who've you or it might be nicer to work with this kernel mess with that and get it to fit the data well rather than trying to mess with the covariance function.",
                    "label": 0
                },
                {
                    "sent": "There might be other multi variant specifications that you might be able to workout this way a little bit easier and so the idea is that you might have maybe 3 latent IID processes are doing something else.",
                    "label": 0
                },
                {
                    "sent": "You convolve 'em, so I have the red kernel here involving two of these three processes to get a spatial field and they have this green kernel convolving a different two of these processes to get another spatial field.",
                    "label": 0
                },
                {
                    "sent": "So these two guys will have dependence in am an it might be convenient to think about this link and process to try to figure out how do you put the dependence in there.",
                    "label": 0
                },
                {
                    "sent": "And then a lot of physical processes have properties that are kind of strange, like they don't really obey this nice geode esic kind of rule.",
                    "label": 0
                },
                {
                    "sent": "For instance, water temperature that if we have an island, we have wind coming through that the water temperature in the Cove here might be very different than the water temperature on the other side of the island, so we don't want to use just straight Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "We might do something that tries to go around here and so thinking of these things in terms of convolutions or kernels might help you with that.",
                    "label": 0
                },
                {
                    "sent": "I also had this nice a wildlife picture here that I thought OK, well it might be really important.",
                    "label": 0
                },
                {
                    "sent": "The wildlife you know on this side of the island and this side of the island are somehow dependent.",
                    "label": 0
                },
                {
                    "sent": "Their genetics are linked and things like that, and they can only sort of.",
                    "label": 0
                },
                {
                    "sent": "They only interact with that real small isthmus in the island, and so this might put some interesting structure here that we want to model, and so I'm not really good at art, so I drew this bird here for the wildlife.",
                    "label": 0
                },
                {
                    "sent": "And then I realized, oh, the bird would just fly right over to the other side of the island and wouldn't care about the isthmus.",
                    "label": 0
                },
                {
                    "sent": "So it's got a ball and chain on it right here, so now it has to walk through the isthmus.",
                    "label": 0
                },
                {
                    "sent": "Alright um.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, like I said, you could have the kernel change with spatial location and this might also induce some non stationarity in your model.",
                    "label": 0
                },
                {
                    "sent": "That might be more appropriate for the process you've got.",
                    "label": 0
                },
                {
                    "sent": "And then also other more physical types of models of.",
                    "label": 0
                },
                {
                    "sent": "Of transport, often these kernels are linked to maybe the Greens function of the PDS that go with those sorts of things or differential equations.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Picture.",
                    "label": 0
                },
                {
                    "sent": "Also, the basis functions right.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, alright so this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is I've always called these basis functions kernels.",
                    "label": 0
                },
                {
                    "sent": "And now I realize like it's the exact opposite and machine learning world, right?",
                    "label": 0
                },
                {
                    "sent": "So I will slip that up once in awhile.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I mean to say what I mean, but not what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "Most of the time.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm going to skip this for now, but the idea is that I want to say basis function here and covariance status inducing so for different basis functions you induce different types of covariances and some of these basis functions are going to be easier to work with than others, so someone just aren't practical, right?",
                    "label": 0
                },
                {
                    "sent": "If I really want exponential covariance function.",
                    "label": 0
                },
                {
                    "sent": "Boy, the basis function looks like a thumb tack.",
                    "label": 0
                },
                {
                    "sent": "It's going to be an awful thing to have to work with, so so it's not going to be practical.",
                    "label": 0
                },
                {
                    "sent": "If you want a real rough sort of process that's going to be practical.",
                    "label": 0
                },
                {
                    "sent": "If you want things that have more smoothness attached to him.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, see, we get to skip all that.",
                    "label": 0
                },
                {
                    "sent": "See that don't have to worry about that.",
                    "label": 0
                },
                {
                    "sent": "So now I'm just going to go through some.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basic ideas here to sort of finish stuff off so, so here's a data set.",
                    "label": 0
                },
                {
                    "sent": "If we use kernels that are about this wide, which is really too skinny for the data set.",
                    "label": 0
                },
                {
                    "sent": "If you try to estimate the kernel, it's going to want something wider.",
                    "label": 0
                },
                {
                    "sent": "If I pick something too skinny, I end up fitting a posterior posterior mean process for the thing that's going to have a lot of wiggle and in it, and it's going to be kind of noisy.",
                    "label": 0
                },
                {
                    "sent": "Alright, if I use.",
                    "label": 0
                },
                {
                    "sent": "A kernel that has the right width.",
                    "label": 0
                },
                {
                    "sent": "So now I have a tuned kernel when estimated it when I'm not going to deal with that, it goes to the data.",
                    "label": 0
                },
                {
                    "sent": "Nice smooth sort of way.",
                    "label": 0
                },
                {
                    "sent": "Alright so here is the right kernel width.",
                    "label": 0
                },
                {
                    "sent": "Here's the wrong one at 2 skinny, well oftentimes.",
                    "label": 0
                },
                {
                    "sent": "It's hard to estimate this kernel with.",
                    "label": 0
                },
                {
                    "sent": "It takes a lot of computation and maybe you don't have the computation to do that, and so one idea is to say that instead of saying I'm going to have IID weights on the kernels, I could put dependence on the kernels.",
                    "label": 0
                },
                {
                    "sent": "I could say that the kernel weights are going to follow a dependent process going across here.",
                    "label": 0
                },
                {
                    "sent": "So here I put in a random walk model for the kernels going across here.",
                    "label": 0
                },
                {
                    "sent": "So this means that the kernel height here for this kernel or the wait here is going to be dependent is going to close the weight on this kernel, so this is effectively kind of.",
                    "label": 0
                },
                {
                    "sent": "Even though the Colonels, a little bit narrow, it's effectively pushing out the width of the kernel some, and so if I use a different process instead of IID, I can come up with a process that mimics what we want to have a wider kernel without actually having to go and estimate the thing so.",
                    "label": 0
                },
                {
                    "sent": "So that's an easy thing to do.",
                    "label": 0
                },
                {
                    "sent": "It just takes one more parameter estimate numerically.",
                    "label": 0
                },
                {
                    "sent": "It's a nice thing to do, and then he was just the right answer.",
                    "label": 0
                },
                {
                    "sent": "If you just use the Gaussian process sort of model.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what we could do is mess around with that underlying dependent structure on the axes or on the weights, and that gives us a richer class of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "A second thing we can do is look at maybe multiresolution types of prophecies.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's just a convolution model looking at ozone data, or.",
                    "label": 0
                },
                {
                    "sent": "You know, we just have one kernel here, this being estimated.",
                    "label": 0
                },
                {
                    "sent": "And here is the result that fit.",
                    "label": 0
                },
                {
                    "sent": "For for the ozone map.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second idea might be saying well that that one is going to be a little bit smooth here, because if I have nots this spaced out I really can't make my kernel too narrow or I'm going to get sort of dead spots in between.",
                    "label": 0
                },
                {
                    "sent": "But if I really do think there's more small scale structure here, I could instead of putting lots of knots in a real small kernel, I could make a sum of two processes, one that's fairly coarse or smooth, and one that's a lot more wiggly.",
                    "label": 0
                },
                {
                    "sent": "And so here I've got a course process with a with a wide kernel, an affine process with a much narrower kernel.",
                    "label": 0
                },
                {
                    "sent": "And I could just say that OK, then the resulting thing I want to estimate is going to be the sum of the course plus the fine thing.",
                    "label": 0
                },
                {
                    "sent": "And so now if we just go ahead and estimate that, it turns out that's not much harder.",
                    "label": 0
                },
                {
                    "sent": "That's you gotta put all this stuff together and go and do it.",
                    "label": 0
                },
                {
                    "sent": "But you can go ahead and estimating course and flying process.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you really do it by just by combining the coarse and fine kernels together and really putting it back in the in the regression framework that we've got.",
                    "label": 0
                },
                {
                    "sent": "And so we just.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go ahead and get well.",
                    "label": 0
                },
                {
                    "sent": "Here's sort of the course representation.",
                    "label": 0
                },
                {
                    "sent": "Here's the course.",
                    "label": 0
                },
                {
                    "sent": "Plus fine has a little more detail in there, not a ton more.",
                    "label": 0
                },
                {
                    "sent": "OK so alright.",
                    "label": 0
                },
                {
                    "sent": "So a multiresolution sort of idea is something we could do.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know this sort of stuff will work pretty well in binary classification types of problems.",
                    "label": 0
                },
                {
                    "sent": "That's not really a binary classification, but here I've got data where the data are more blueish.",
                    "label": 0
                },
                {
                    "sent": "It's suggesting one state at this process, and where the data are more purple, is suggesting another state.",
                    "label": 0
                },
                {
                    "sent": "So here's a true binary field.",
                    "label": 0
                },
                {
                    "sent": "And what we got are noisy realizations of this thing over here.",
                    "label": 0
                },
                {
                    "sent": "OK, and so now now what we'd like to do is.",
                    "label": 0
                },
                {
                    "sent": "Is trying to, you know, try to estimate what is the binary reconstruction for this thing given the data and so sort of a cheat, I've got a likelihood that is a lot harder.",
                    "label": 0
                },
                {
                    "sent": "It's easy to specify it here.",
                    "label": 0
                },
                {
                    "sent": "It's a lot harder in practice, but we got 10 measurements and will represent the spatial field using this convolution idea again.",
                    "label": 0
                },
                {
                    "sent": "And basically, here's how it goes that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll put out a lattice of weights, right, and then will convolve these weights with the kernel will end up with a smooth process and then we'll just say the binary process is going to be.",
                    "label": 0
                },
                {
                    "sent": "One when that process above 0 and N -- 1 or 0.",
                    "label": 0
                },
                {
                    "sent": "The other thing when it's below 0.",
                    "label": 0
                },
                {
                    "sent": "So we just sort of clip this thing and end up with the binary the binary field.",
                    "label": 0
                },
                {
                    "sent": "So now it's the same basic idea.",
                    "label": 0
                },
                {
                    "sent": "I have the same basic set of weights here that control this process which we clip to get the binary thing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "So we've got a likelihood that just says what we have is a noisy version of the truth.",
                    "label": 0
                },
                {
                    "sent": "Gaussian noisy version of the truth.",
                    "label": 0
                },
                {
                    "sent": "We just have the IID model on the on the X is and then.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to say that the truth is basically the clipped version.",
                    "label": 0
                },
                {
                    "sent": "So I want to put a star on this.",
                    "label": 0
                },
                {
                    "sent": "See the smooth process when I'm clipping it to be zero and one.",
                    "label": 0
                },
                {
                    "sent": "And so then we're just going to end up with a posterior distribution here, so that clipping is a nice nonlinear sort of thing, so it's going to make this resulting posterior not be nice and easy.",
                    "label": 0
                },
                {
                    "sent": "It's not going to have this nice full conditional form.",
                    "label": 0
                },
                {
                    "sent": "That we had before, and that's just Gaussian and.",
                    "label": 0
                },
                {
                    "sent": "And gamma, so we'll have to do we have to do some metropolis updating on this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing there's Metropolis Los Alamos guy.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and so Metropolis is just doing a different approach for sampling this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Posterior distribution, but it's very basic kind of thing.",
                    "label": 0
                },
                {
                    "sent": "And so you can end up with as you're updating these X is you're going to induce binary fields here.",
                    "label": 0
                },
                {
                    "sent": "And here is just a collection of posterior samples of this binary field given the data.",
                    "label": 0
                },
                {
                    "sent": "And so we can.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go ahead and put that together.",
                    "label": 0
                },
                {
                    "sent": "We can come up with a posterior mean from that field and compare it and do something with that.",
                    "label": 0
                },
                {
                    "sent": "So really, that's.",
                    "label": 0
                },
                {
                    "sent": "One is that that's just using the same spatial model and just clipping it.",
                    "label": 0
                },
                {
                    "sent": "For a binary example.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rather than.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using something continuous here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So, so it's been useful to do this sort of convolution idea.",
                    "label": 0
                },
                {
                    "sent": "This basis representation, it has been useful in a lot of inverse problems.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to sort of.",
                    "label": 0
                },
                {
                    "sent": "Trying to estimate the 64 by 64 spatial field given some very remote data.",
                    "label": 0
                },
                {
                    "sent": "So here it's.",
                    "label": 0
                },
                {
                    "sent": "Here it's a transport problem that we put an injector well in the middle.",
                    "label": 0
                },
                {
                    "sent": "We've got a bunch of wells around the outside that are going to be sucking out fluid at a constant pressure.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is we get get the system going into steady state.",
                    "label": 0
                },
                {
                    "sent": "So we inject a constant amount overtime into the injector.",
                    "label": 0
                },
                {
                    "sent": "Well in the middle, and then we suck out at a constant pressure around the outside.",
                    "label": 0
                },
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "8 wells around the outside here and so when you get this thing going at steady state, you put in a tracer in the middle and then you wait until the tracer see when the tracer hits the well out here and so this is basically the concentration of the tracer on this outside well so it starts out with nothing and then eventually the tracer reaches there and you have a high concentration.",
                    "label": 0
                },
                {
                    "sent": "And then it dies back out.",
                    "label": 0
                },
                {
                    "sent": "So you get this at all these wells or surrounding the injector and what you'd like to do is try to infer what's the underlying permeability of this field.",
                    "label": 0
                },
                {
                    "sent": "To do this with a 64 by 64 grid is pretty demanding, but if we break it up and we just use a real smooth or coarse representation with these basis functions.",
                    "label": 0
                },
                {
                    "sent": "And we might be able to estimate things.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and so here was just an example with the real problem.",
                    "label": 0
                },
                {
                    "sent": "If we compute for three months we get some answer like this, which using MCMC it didn't really converge and we didn't get anywhere.",
                    "label": 0
                },
                {
                    "sent": "But if we did some much coarser basis representation, we could get a. Posterior that seemed to be fairly stable now correct?",
                    "label": 0
                },
                {
                    "sent": "Is it tough question on that one, but at least it helps with that problem.",
                    "label": 0
                },
                {
                    "sent": "It reduce the dimensionality so much that we did have a hope of inferring something about this permeability here, but we did have to assume a lot of smoothness to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'll talk about a couple more things before we end it here, nonstationarity so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typically what we do is we take some white noise process, convolve it with the kernel and we end up with some process that has this smoothness attached to it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we just use one kernel all the way through.",
                    "label": 0
                },
                {
                    "sent": "Here's the process we get, and so I just drew that kernel everywhere, shrunk down by about a factor of 6 so I could fit it on each pixel.",
                    "label": 0
                },
                {
                    "sent": "So there's the resulting field.",
                    "label": 0
                },
                {
                    "sent": "One alternative",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say OK, what we could have is have that convolution kernel change with spatial location.",
                    "label": 0
                },
                {
                    "sent": "So now the kernel is changing depending on where it is.",
                    "label": 0
                },
                {
                    "sent": "Colonel is not really the size of these things as by 6 times as large, so it's much bigger.",
                    "label": 0
                },
                {
                    "sent": "But just to show you the picture here shrunken down.",
                    "label": 0
                },
                {
                    "sent": "So now it's the same basic process here, but we're having the kernel change smoothly over the spatial domain, and this allows you to see you know other types of dependence.",
                    "label": 0
                },
                {
                    "sent": "So over here the dependence is very much in this direction, but over here it's going in a very different direction, so this might be a way to allow the spatial dependence to change.",
                    "label": 0
                },
                {
                    "sent": "Over the region that you're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way we did this was just look at a weighted combination.",
                    "label": 0
                },
                {
                    "sent": "Let's just have three kernels and at any spatial location will just have a weighted combination of those three.",
                    "label": 1
                },
                {
                    "sent": "And so you have to cook up.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way to have this weighted combination change smoothly over space.",
                    "label": 0
                },
                {
                    "sent": "And what we did is we had three smooth processes going around and then the then the weight was just going to be, you know.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Either one process divided by the sum of Y to the other processes, so it's fairly standard way to come up with some sort of mixture spatial mixture over.",
                    "label": 0
                },
                {
                    "sent": "Over spatial domain here.",
                    "label": 0
                },
                {
                    "sent": "And then you can represent the Colonel like which weight is winning.",
                    "label": 0
                },
                {
                    "sent": "So two is this kind of sideways one going this way.",
                    "label": 0
                },
                {
                    "sent": "So over here 2 is high and most of the dependences in this direction over here 3 is high, most independence is in that direction.",
                    "label": 0
                },
                {
                    "sent": "And then over here one is high.",
                    "label": 0
                },
                {
                    "sent": "So most dependences horizontal direction.",
                    "label": 0
                },
                {
                    "sent": "So here is just one way of trying to do that.",
                    "label": 0
                },
                {
                    "sent": "The tricky thing is trying to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Automate all this stuff when you're doing it, but here is just one example where we're looking at dioxin concentrations, and sure enough, going through this whole estimation process.",
                    "label": 0
                },
                {
                    "sent": "It does estimate stronger spatial dependence in this direction over here and over here at it's kind of changing this spatial dependence, so certainly there is information in that.",
                    "label": 0
                },
                {
                    "sent": "The thing that we haven't found is that you get much better predictions at all that if you really compare your predictions on something very stationary to something nonstationary with this sort of data set, you don't see that much difference.",
                    "label": 0
                },
                {
                    "sent": "So it does.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimate the structure well.",
                    "label": 0
                },
                {
                    "sent": "The resulting fit doesn't seem to be huge.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference.",
                    "label": 0
                },
                {
                    "sent": "Alright, a couple more ideas.",
                    "label": 0
                },
                {
                    "sent": "Just if we're dealing with spacetime models we could have these latent weights live in some sort of space time domain, and then to specify a kernel over the space time domain.",
                    "label": 0
                },
                {
                    "sent": "And then we can go ahead and convolve.",
                    "label": 0
                },
                {
                    "sent": "This sort of latent wait process with the kernel in that end, up with the space time model.",
                    "label": 0
                },
                {
                    "sent": "So sometimes that's a convenient sort of thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is it worked well?",
                    "label": 0
                },
                {
                    "sent": "So it works well in estimating ocean temperatures, so we have this.",
                    "label": 0
                },
                {
                    "sent": "This data set over space and time, so it's about 70 years a little more almost 80 years of data over the spatial region.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data are living in this 3D space.",
                    "label": 0
                },
                {
                    "sent": "So we end up specifying a grid of weight, or the kernels all live in this big grid here, and the kernels are different in different locations, so it's a hassle trying to estimate those sorts of things, but we could go ahead and do that.",
                    "label": 0
                },
                {
                    "sent": "And now with those things, if we estimate the weights with the kernels, we can go ahead and estimated spacetime field for this.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this whole thing, given the data.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there is a, so there's a space time model.",
                    "label": 0
                },
                {
                    "sent": "And the big thing about doing these weights is that it would actually become.",
                    "label": 0
                },
                {
                    "sent": "So it actually became estimable is a real tough problem without doing this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So so here is 1 approach to doing that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can look at deviances from the overall grand mean, and you know, is there evidence of things getting warmer overtime.",
                    "label": 0
                },
                {
                    "sent": "And in that data with that MoD.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we use really.",
                    "label": 0
                },
                {
                    "sent": "There's not much evidence that it, you know, seem to be warmer anywhere, not really.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Alright, here at a second way we could do this is you can imagine a latent process that lives spatially and then have this process evolve overtime.",
                    "label": 0
                },
                {
                    "sent": "And then what you're going to do is just convolve that problem or convolve that process with just a spatial kernel.",
                    "label": 0
                },
                {
                    "sent": "So we have this latent process.",
                    "label": 0
                },
                {
                    "sent": "These weights that are evolving overtime and just a spatial kernel here.",
                    "label": 0
                },
                {
                    "sent": "And so if we do that now, we're going to have a space time process that's going to evolve overtime.",
                    "label": 0
                },
                {
                    "sent": "What's nice about that is that this often will fit into this dynamic model framework, so these dynamic linear models you can often estimate this way so you can cook up a space time model that has a lot of estimation machinery already attached to it.",
                    "label": 0
                },
                {
                    "sent": "And so then you can handle some of these pretty big problems that otherwise we're going to be pretty difficult.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here was just we're looking at ozone levels, daily ozone and so here is just nine days of ozone.",
                    "label": 0
                },
                {
                    "sent": "And if we try to fit this sort of model to this thing, we end up with something that.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's quite a bit of structure, but here is.",
                    "label": 0
                },
                {
                    "sent": "Here's the ozone map evolving overtime.",
                    "label": 0
                },
                {
                    "sent": "And what we've got is basically this spatial set of weights that's changing overtime and some random random walk kind of fashion.",
                    "label": 0
                },
                {
                    "sent": "I think the random walk was all we used here.",
                    "label": 0
                },
                {
                    "sent": "And that seemed to model the data fairly well.",
                    "label": 0
                },
                {
                    "sent": "If we go back and look at.",
                    "label": 0
                },
                {
                    "sent": "Spatial ways?",
                    "label": 0
                },
                {
                    "sent": "Well here is just.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three of the weights that we got in that big giant lattice, and if we look at three of the way, so we estimate an independent independent prior well, then the weights are kind of all over the board.",
                    "label": 0
                },
                {
                    "sent": "If we put a prior that has structure overtime.",
                    "label": 0
                },
                {
                    "sent": "So here's a random walk prior.",
                    "label": 0
                },
                {
                    "sent": "Well, then the weights do seem to have a lot more continuity overtime.",
                    "label": 0
                },
                {
                    "sent": "And it gives you slightly better.",
                    "label": 0
                },
                {
                    "sent": "He just sort of one day and types of predictions.",
                    "label": 0
                },
                {
                    "sent": "So so anyway, there is another way to try to get a spacetime model out of this thing.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think.",
                    "label": 0
                },
                {
                    "sent": "That's basically.",
                    "label": 0
                },
                {
                    "sent": "All they really had to talk about so, So what is this?",
                    "label": 0
                },
                {
                    "sent": "This is basically here's a guy that thinks a lot about Gaussian processes thinks about spatial models, maybe spacetime models.",
                    "label": 0
                },
                {
                    "sent": "And and the kernels.",
                    "label": 0
                },
                {
                    "sent": "I want to say the basis functions were really an idea or a way of getting around.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "How can I mimic this covariance structure that I wanted?",
                    "label": 0
                },
                {
                    "sent": "This kernel that I want when I can't do the usual Gaussian process kind of activity here and so using the kernels it allowed to allow me to speed up computations at times.",
                    "label": 0
                },
                {
                    "sent": "It also allows you to model processes in different sorts of ways than you would have had you just thought about covariances.",
                    "label": 0
                },
                {
                    "sent": "And so so it worked well in a number of these sorts of situations where.",
                    "label": 0
                },
                {
                    "sent": "Especially that ozone case where you can actually do the model quickly enough and analytically enough that you could start asking questions about design.",
                    "label": 0
                },
                {
                    "sent": "Where should we know where should you add new monitors?",
                    "label": 0
                },
                {
                    "sent": "Here's a lesson about design though.",
                    "label": 0
                },
                {
                    "sent": "Is that almost always if it involves a government, you're always asking questions like, which monitor should I remove?",
                    "label": 0
                },
                {
                    "sent": "And that's always a lot easier question.",
                    "label": 0
                },
                {
                    "sent": "Maybe doesn't model need as much model so?",
                    "label": 0
                },
                {
                    "sent": "But we won't worry about that, and so things that, if you're trying to do optimization on top of this and design, then these this sort of structure actually gives you a computational boost that makes some problems that just weren't really doable.",
                    "label": 0
                },
                {
                    "sent": "Doable again.",
                    "label": 0
                },
                {
                    "sent": "And then the ocean one.",
                    "label": 0
                },
                {
                    "sent": "So now we kind of worked with the ocean modelers at Los Alamos now.",
                    "label": 0
                },
                {
                    "sent": "And so now we are looking at ways that how can we do data simulation with those with those big models.",
                    "label": 0
                },
                {
                    "sent": "And thought is well, maybe some of these kernel approaches might be useful there.",
                    "label": 0
                },
                {
                    "sent": "Alright so I am done torturing you with this talk.",
                    "label": 0
                },
                {
                    "sent": "Thanks for listening and I'm sure we'll have time for discussion later.",
                    "label": 0
                },
                {
                    "sent": "This is kind of.",
                    "label": 0
                },
                {
                    "sent": "Going somewhere from A to B and back today.",
                    "label": 0
                },
                {
                    "sent": "Funny thing is that.",
                    "label": 0
                },
                {
                    "sent": "Two years ago I started working gas prices.",
                    "label": 0
                },
                {
                    "sent": "We said, yeah, he's fine.",
                    "label": 0
                },
                {
                    "sent": "Dimensional models are based on but hey we have to specify regular basis functions are.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah that's kind of ugly.",
                    "label": 0
                },
                {
                    "sent": "So how about we do this thing we actually have?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Bracelets It's true that we do have some flexibility in that way, right?",
                    "label": 0
                },
                {
                    "sent": "Described in my library.",
                    "label": 0
                },
                {
                    "sent": "You can do things like that please.",
                    "label": 0
                },
                {
                    "sent": "That sounds fun.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It is true that.",
                    "label": 0
                },
                {
                    "sent": "If you have a smooth process.",
                    "label": 0
                },
                {
                    "sent": "Can effectively deal with this as well.",
                    "label": 0
                },
                {
                    "sent": "I would take you think about trying to get the eigenfunction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you really want this.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Mouse.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Right, it's worth sticking around.",
                    "label": 0
                },
                {
                    "sent": "I mean the eigenfunction idea, which I think.",
                    "label": 0
                },
                {
                    "sent": "That's right, it's so it's nice to work with that, but at times when you're conditioning on data that the eigen representation is usually on this.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this is really right.",
                    "label": 0
                },
                {
                    "sent": "I was saying it this way, but kind of this prior covariance that.",
                    "label": 0
                },
                {
                    "sent": "The eigen function only looks at the covariance and doesn't care about what the data might be doing to it later, right and.",
                    "label": 0
                },
                {
                    "sent": "And something that's more local might be a little more responsive to if the data is pushing to do something that maybe you didn't throw in the first 10 eigen Eigen functions.",
                    "label": 0
                },
                {
                    "sent": "So this local bases.",
                    "label": 0
                },
                {
                    "sent": "Game is that I give you.",
                    "label": 0
                },
                {
                    "sent": "Some number K basis yeah.",
                    "label": 0
                },
                {
                    "sent": "Which are best places?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Because PCI is basically a Ultima Linea.",
                    "label": 0
                },
                {
                    "sent": "Racist.",
                    "label": 0
                },
                {
                    "sent": "If you play some sort of average game then that will be the right place.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "The dates are inputs.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah, average over the data location.",
                    "label": 0
                },
                {
                    "sent": "No, no question first.",
                    "label": 0
                },
                {
                    "sent": "Who is also.",
                    "label": 0
                },
                {
                    "sent": "Displays consoles here is what he said it's.",
                    "label": 0
                },
                {
                    "sent": "GPS came from.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "What has been done in recent years like in?",
                    "label": 0
                },
                {
                    "sent": "Horse applications and basically what you have there is.",
                    "label": 0
                },
                {
                    "sent": "It's basically the basis function representation again, and when you show these Maps here and there's so much uses.",
                    "label": 0
                },
                {
                    "sent": "Or through the English version version of it, see how I wanted Bullet?",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically what is happening is similar thing that with the basis functions that if you have a.",
                    "label": 0
                },
                {
                    "sent": "Really Sports Creek.",
                    "label": 0
                },
                {
                    "sent": "You're actually you're not able to capture anything else, but where is wrong or license anyway?",
                    "label": 0
                },
                {
                    "sent": "Put it.",
                    "label": 0
                },
                {
                    "sent": "With her right?",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "For me it seems a little bit like we've been doing, kind of like a loop and coming back.",
                    "label": 0
                },
                {
                    "sent": "Like like this for stop listening when it's it's kind of like going back to the internals and again but from different directions.",
                    "label": 0
                },
                {
                    "sent": "And is there any way you actually fell short, usurp ends, his or short pregnancies?",
                    "label": 0
                },
                {
                    "sent": "That seems to be the most problem, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and and it seems like that's seems to be the hardest thing to do with the basis approach that voice lots of degrees of freedom for, you know, a little fuzz.",
                    "label": 0
                },
                {
                    "sent": "On top of this big structure.",
                    "label": 0
                },
                {
                    "sent": "That is not so.",
                    "label": 0
                },
                {
                    "sent": "That's a great question, and my experience has been offer the crude part bases often do quite well.",
                    "label": 0
                },
                {
                    "sent": "And for this for the rest of this part, if you really care about, you know small scale structure.",
                    "label": 0
                },
                {
                    "sent": "Boy, it's hard.",
                    "label": 0
                },
                {
                    "sent": "It's hard to do much better than some.",
                    "label": 0
                },
                {
                    "sent": "You know, basic kind of you know, Gaussian process approach with, you know you know exponential covariance or something like that that.",
                    "label": 0
                },
                {
                    "sent": "In some ways, for that boy, that's a good way of working with something that otherwise you'd have to have, you know, a million little basis functions to do that last little piece of your problem, so I don't have a good answer for that.",
                    "label": 0
                },
                {
                    "sent": "It seems to be like, oh, you could do it fix spaces.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Plus comeback support based on variance, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Short times I've never done it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean so so so I know people that the small compact support is exactly the same even want to move into this covariance representation because that thing if you look at the covariance matrix or maybe the precision matrix of that thing, it tends to be mostly zeros and the sparse representations might do pretty likely to do very well in that arena, so that.",
                    "label": 0
                },
                {
                    "sent": "You know that the small wiggle part.",
                    "label": 0
                },
                {
                    "sent": "Maybe Gaussian process is the right way to go with that, because the covariance is going to have lots of zeros, it will just have stuff around the diagonal or very small sort of neighborhood structures.",
                    "label": 0
                },
                {
                    "sent": "Things will change in movies.",
                    "label": 0
                },
                {
                    "sent": "Resolution analysis about approaching for showed me yeah.",
                    "label": 0
                },
                {
                    "sent": "You just said about global smoothness.",
                    "label": 0
                },
                {
                    "sent": "You just have a different place and you push holding in space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm sure like it's the wavelets, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm just wondering.",
                    "label": 0
                },
                {
                    "sent": "The way to do it, or even do something.",
                    "label": 0
                },
                {
                    "sent": "Same thing with GP.",
                    "label": 0
                },
                {
                    "sent": "Assuming something else today.",
                    "label": 0
                },
                {
                    "sent": "That's good, that's a good point already.",
                    "label": 0
                },
                {
                    "sent": "So it's basically saying that OK, what do they do at the very small?",
                    "label": 0
                },
                {
                    "sent": "Or at the very small wavelets?",
                    "label": 0
                },
                {
                    "sent": "Just just have some?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Food drinking frequency goals with different space.",
                    "label": 0
                },
                {
                    "sent": "If you see any low frequency here behind.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just wondering.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I'm allowed to have no answers for lots of things.",
                    "label": 0
                },
                {
                    "sent": "This seems quite sensible, I think.",
                    "label": 0
                },
                {
                    "sent": "So as quickly as I have wavelengths found a way into special statistics, or is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, I would say like there's there.",
                    "label": 0
                },
                {
                    "sent": "There's a paper by I think Doug Niche.",
                    "label": 0
                },
                {
                    "sent": "Gonna few other people that basically said, Oh yeah, if you use this wavelet basis.",
                    "label": 0
                },
                {
                    "sent": "That here's how you can approximate kind of standard covariance models that you do in the spatial world, and it ends up being, you know, not quite diagonal types of things in the wavelet basis, but.",
                    "label": 0
                },
                {
                    "sent": "It adds a little extra dependence.",
                    "label": 0
                },
                {
                    "sent": "Trying to mimic those things.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I think mathematically you can always.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "You can go back and forth with those things.",
                    "label": 0
                },
                {
                    "sent": "And it seems like a lot of these answers or what you want is, you know, well computation or for the problem at hand.",
                    "label": 0
                },
                {
                    "sent": "You know what's the way to do that, but yeah.",
                    "label": 0
                },
                {
                    "sent": "So so those happen related.",
                    "label": 0
                },
                {
                    "sent": "Website.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so here's.",
                    "label": 0
                },
                {
                    "sent": "Make me a better feeling.",
                    "label": 0
                },
                {
                    "sent": "Shapeless, I think shape lights are out there, too, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, beyond that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so problem that I know I know in the in the Gaussian process world I'm running across right now which I don't know if you guys have thought in more of the machine learning world.",
                    "label": 0
                },
                {
                    "sent": "It's been looking at spatial dimensions using just two or three or something kind of simple.",
                    "label": 0
                },
                {
                    "sent": "Often we're trying to interpolate what a big computational model would have said given an input space, and so the response tends to be pretty smooth.",
                    "label": 0
                },
                {
                    "sent": "But now the input space is, you know 10 or 20 dimensional sort of thing.",
                    "label": 0
                },
                {
                    "sent": "And so we have often alright.",
                    "label": 0
                },
                {
                    "sent": "It's hard to know what can you do with bases in that sort of arena.",
                    "label": 0
                },
                {
                    "sent": "We've sort of.",
                    "label": 0
                },
                {
                    "sent": "Pon and gone with Gaussian processes there, which is good for some problems for other problems.",
                    "label": 0
                },
                {
                    "sent": "Basically have no good answer on that and I don't know if you guys have wrestled with that problem at all or have any thoughts there.",
                    "label": 0
                },
                {
                    "sent": "You can't do like.",
                    "label": 0
                },
                {
                    "sent": "So if you do a digit recognition but dancing process, you have an 8 by 864 dimensions already, and then people are using them for images.",
                    "label": 0
                },
                {
                    "sent": "I think that the.",
                    "label": 0
                },
                {
                    "sent": "To be more open.",
                    "label": 0
                },
                {
                    "sent": "Assuming away is a good one, yes.",
                    "label": 0
                },
                {
                    "sent": "So one thing that Chris is looked at is where you you effectively have.",
                    "label": 0
                },
                {
                    "sent": "I would interpret it as you got a linear projection of your data onto a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And then you operate on that and you optimize the covariance function with respect to that projection characterization.",
                    "label": 0
                },
                {
                    "sent": "That was also.",
                    "label": 0
                },
                {
                    "sent": "I think my worry would they most of the time it's probably nonlinear.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't believe in high dimension spaces, personality their high dimension of things which are really low dimensional, but I believe they're also nonlinear, so.",
                    "label": 0
                },
                {
                    "sent": "This is a simple.",
                    "label": 0
                },
                {
                    "sent": "Can I ask?",
                    "label": 0
                },
                {
                    "sent": "And so you were talking about some problems about using the GPS in the television series?",
                    "label": 0
                },
                {
                    "sent": "Can you give some examples of things that you feel is not working?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Basically, all right so, so I think this is a theorem or meta theorem for anything I've ever done that's been useful.",
                    "label": 0
                },
                {
                    "sent": "Is that in hindsight it was a trivial problem.",
                    "label": 0
                },
                {
                    "sent": "And then it only seemed.",
                    "label": 0
                },
                {
                    "sent": "Maybe it wasn't that way when I was working on it, but then the back, oh, you only depend on 2 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So so yeah.",
                    "label": 0
                },
                {
                    "sent": "So let on the computer model world.",
                    "label": 0
                },
                {
                    "sent": "What typically happens is we might consider 20 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Anne, what happens and we will have a univariate response or looking at temperature or something that's output after that.",
                    "label": 0
                },
                {
                    "sent": "So bunch of parameters, strength of materials, equations of state, and things like that.",
                    "label": 0
                },
                {
                    "sent": "So lots of parameters controlling this thing, but and it wasn't, the guys were idiots that wrote the code.",
                    "label": 0
                },
                {
                    "sent": "It turns out we're looking at it for one particular type of experiment or something like that, and so we're really exercising this code in a fairly narrow range.",
                    "label": 0
                },
                {
                    "sent": "And so if we look at the response, the temperature of the system as we mess around with these parameters, it turns out.",
                    "label": 0
                },
                {
                    "sent": "It's dominated by three or four dimensions, three or four inputs are really controlling everything, and they also control it in a very smooth sort of predictable kind of way, so that after we did this whole 20 dimensional Gaussian process, estimated ranges in each of the dimensions and things like that.",
                    "label": 0
                },
                {
                    "sent": "It turns out 04 dimensions is all that's really going on.",
                    "label": 0
                },
                {
                    "sent": "Just like you said Neil, that really there was just.",
                    "label": 0
                },
                {
                    "sent": "It was just a four dimensional surface that we're trying to fit in.",
                    "label": 0
                },
                {
                    "sent": "Our model was just barely smart enough to figure out it shouldn't try on the other 16 dimensions and then the problems that are hard are when.",
                    "label": 0
                },
                {
                    "sent": "All right, all 20 are kind of acting about evenly in this problem.",
                    "label": 0
                },
                {
                    "sent": "And so even if it's smooth and they're acting evenly.",
                    "label": 0
                },
                {
                    "sent": "It becomes hard and if they act just in an additive sort of way, well, maybe we got a chance.",
                    "label": 0
                },
                {
                    "sent": "But any real problems doesn't really happen that they actually start interacting in places of this kind of parameter space.",
                    "label": 0
                },
                {
                    "sent": "You're searching, and it's it's those problems where everyone is kind of contributing some in a slightly nonlinear away in a slightly interactive way.",
                    "label": 0
                },
                {
                    "sent": "And I don't know it may just mean that's an impossible problem.",
                    "label": 0
                },
                {
                    "sent": "Increasing estimation on the.",
                    "label": 0
                },
                {
                    "sent": "Kerbal feel.",
                    "label": 0
                },
                {
                    "sent": "Meaning, for example, if you press 1, they're just for that matter, by used way, yeah.",
                    "label": 0
                },
                {
                    "sent": "You actually observe that Doris Timation is work so bad.",
                    "label": 0
                },
                {
                    "sent": "For a better use for, yeah, well, so usually what happens is somewhere in the model.",
                    "label": 0
                },
                {
                    "sent": "You have parameters that are controlling which dimensions you're looking at, and it figures out not to look at those other 16.",
                    "label": 0
                },
                {
                    "sent": "It figures it set something to 0 for 16 of AM of those numbers, and for the numbers it's around 1:00 or something like that.",
                    "label": 0
                },
                {
                    "sent": "So your model is really figuring out that I only need to fit a four dimensional model, and that's when you really do have a 20 dimensional model is when.",
                    "label": 0
                },
                {
                    "sent": "It seems like nothing is near anything in that in that case and so.",
                    "label": 0
                },
                {
                    "sent": "You can make a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "You look at how do you do on your holdouts and you might as well just guess the mean.",
                    "label": 0
                },
                {
                    "sent": "In fact, usually you don't even do that well.",
                    "label": 0
                },
                {
                    "sent": "So it's in some sense that you're not.",
                    "label": 0
                },
                {
                    "sent": "I don't know what's happening in the Gaussian if you just thought you were a Gaussian process and thought about covariance, is that everything becomes far away because nothing is close in all dimensions that matter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And in the fourth dimension case, it can be misleading because it may be the the other dimension.",
                    "label": 0
                },
                {
                    "sent": "Something quite small scales going on and you just don't have the density of coverage to see it.",
                    "label": 0
                },
                {
                    "sent": "So you're just saying, oh, that's part of my game lease term because I can't differentiate between that and noise, but it might be that there's actual structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "But I think another extremist if you go to really high dimensions and would like something text classification, it turns out removing inputs doesn't help.",
                    "label": 0
                },
                {
                    "sent": "I mean it hurts.",
                    "label": 0
                },
                {
                    "sent": "You use all of them.",
                    "label": 0
                },
                {
                    "sent": "The model had typically linear budget India.",
                    "label": 0
                },
                {
                    "sent": "So interesting is we had lots of problems with really high dimensions.",
                    "label": 0
                },
                {
                    "sent": "You really want to use all the dimensions because every dimension makes a little contribution to the estimation of noisy things.",
                    "label": 0
                },
                {
                    "sent": "And so in these text models I mean are they what sort of models do people use?",
                    "label": 0
                },
                {
                    "sent": "Are they kind of?",
                    "label": 0
                },
                {
                    "sent": "Additive sorts of things or.",
                    "label": 0
                },
                {
                    "sent": "He's like, um, Vectorable will extend the words they might get down to their past 6000 right now and then just count the number of.",
                    "label": 0
                },
                {
                    "sent": "Each word, so they removed all the structure.",
                    "label": 0
                },
                {
                    "sent": "But this document contains the word wrestling and WWF whatever.",
                    "label": 0
                },
                {
                    "sent": "And so we've got some representation.",
                    "label": 0
                },
                {
                    "sent": "And then they they might just put that in the kernel learning algorithm that could be against.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they also used a wide caution things I guess, but.",
                    "label": 0
                },
                {
                    "sent": "So I think that you should think about that every every input makes a little contribution.",
                    "label": 0
                },
                {
                    "sent": "Deciding on some classification service or something, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, you think simply function of each class right?",
                    "label": 0
                },
                {
                    "sent": "And that option might be quite complicated like yeah.",
                    "label": 0
                },
                {
                    "sent": "Quite implicated, so in principle, if you actually want to estimate the density, you might need more complicated schemes and you know how you mention like yeah.",
                    "label": 0
                },
                {
                    "sent": "Next notification it boils down to just say one or minus one or so you don't really.",
                    "label": 0
                },
                {
                    "sent": "I mean, as long as you get the boundary well.",
                    "label": 0
                },
                {
                    "sent": "Yeah question, I think progression.",
                    "label": 0
                },
                {
                    "sent": "He won this year, though there is this answer that you want because when you do the computer emulation, so you're predicting for an exact value which is noiseless and yeah.",
                    "label": 0
                },
                {
                    "sent": "Where is when we're doing this texting?",
                    "label": 0
                },
                {
                    "sent": "Maybe 80% full minutes anyway, you're not expecting to exactly emulate whatever the text prices you know you turn location when you're doing this emulation.",
                    "label": 0
                },
                {
                    "sent": "You really expect the only on certain to be to do that with what the functions doing between your design, yeah?",
                    "label": 0
                },
                {
                    "sent": "So you really want.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's just trying to be.",
                    "label": 0
                },
                {
                    "sent": "Right, so so I have to say that the emulation in that sense might be an easier problem because you know it has to be something smooth and it has to interpolate, and so you're not going to be that easily fooled by something that's close but doesn't interpolate that.",
                    "label": 0
                },
                {
                    "sent": "Whereas.",
                    "label": 0
                },
                {
                    "sent": "Whereas if these other noisier problems you have to consider, all these things right?",
                    "label": 0
                },
                {
                    "sent": "And nothing you can't rule out close but not exact things at all.",
                    "label": 0
                },
                {
                    "sent": "And so I think, yeah, this computer emulation world is in some sense sort of ideal for this Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "'cause if you just find something that was smooth and through that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah, forget it.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's also true in numerical tolerances and things like that that you know lots of stuff shows up as small amounts of noise.",
                    "label": 0
                },
                {
                    "sent": "But I mean then again, at the problems we can't do.",
                    "label": 0
                },
                {
                    "sent": "We totally can't do, and the ones we can are kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "And so you know, if I suspect if those things are a big problem.",
                    "label": 0
                },
                {
                    "sent": "Those problems get hard fast.",
                    "label": 0
                },
                {
                    "sent": "And then often because you're like I said, usually you're looking at it in some really restricted way.",
                    "label": 0
                },
                {
                    "sent": "We're trying to estimate, you know, these types of problems, which are the code was made to feel this whole room, but we're really looking at that chair for the code within there.",
                    "label": 0
                },
                {
                    "sent": "There's not all that much going on in terms of.",
                    "label": 0
                },
                {
                    "sent": "You have some things like that.",
                    "label": 0
                },
                {
                    "sent": "But I think you're right if.",
                    "label": 0
                },
                {
                    "sent": "If it gets fairly nonlinear, then you're.",
                    "label": 0
                },
                {
                    "sent": "I can't imagine we can do all that well.",
                    "label": 0
                },
                {
                    "sent": "Do these they often component based, so in the possible system that feeding into other parts of the system where you could say actually, why don't I just look inside the system?",
                    "label": 0
                },
                {
                    "sent": "So my inputs are only feeding into that, and then that's another input into the later.",
                    "label": 0
                },
                {
                    "sent": "Later you get to decompose and then people try that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, alright, so that so it's hard because it's the one nice thing about this computer model world is some guy you know, some guy and his team spent 10 years building this model right?",
                    "label": 0
                },
                {
                    "sent": "And then?",
                    "label": 0
                },
                {
                    "sent": "If you could just run it a few times, you can look at it and do stuff with it.",
                    "label": 0
                },
                {
                    "sent": "Actually getting inside and knowing the people that you know can can get inside those things is actually a much harder thing in practice to do then you might guess.",
                    "label": 0
                },
                {
                    "sent": "So yeah, in a few cases where you can, it seems to be pretty beneficial and also things like can you get an joins, derivatives and things like that out or other types of information out.",
                    "label": 0
                },
                {
                    "sent": "That can also be huge, it's just boy.",
                    "label": 0
                },
                {
                    "sent": "It's so hard and practiced.",
                    "label": 0
                },
                {
                    "sent": "To have this very decoupled problem.",
                    "label": 0
                },
                {
                    "sent": "Get more kind of convolved within itself that it's just.",
                    "label": 0
                },
                {
                    "sent": "It's hard in practice to get to that point.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I think that's right.",
                    "label": 0
                },
                {
                    "sent": "And that does that.",
                    "label": 0
                },
                {
                    "sent": "Can simplify a lot of things.",
                    "label": 0
                },
                {
                    "sent": "Has it ever happened in my life on a real problem?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        }
    }
}