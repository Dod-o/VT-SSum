{
    "id": "r7mrqmlvlrqm37rauyw7jj7lztipuj55",
    "title": "Patterns in Vector Spaces",
    "info": {
        "author": [
            "Elisa Ricci, University of Perugia"
        ],
        "published": "Dec. 3, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/aop09_ricci_pivs/",
    "segmentation": [
        [
            "I just want to recall a some few concept that never already introduced quite a lot of like what we mean by pattern analysis."
        ],
        [
            "By pattern analysis we mean the automatic detection of patterns into the attack and what is a pattern is a basically an instructor and irregularity.",
            "Any relations that it's present in the data and we want to be able to discover automatically the concept of pattern analysis is strongly connected with the concept of learning because we want to use learning technique in order to be able to detect the significant factor and to transfer the information that we learn.",
            "By making prediction on new data that you assume coming from the same source in this talk, I will simply talk about data as are represented as vector.",
            "The other tools will introduce other type of data like sequences.",
            "So basically I'm talking about statistical pattern recognition where we where we have data represented as vectors and we made some statistical assumption about their distribution before we.",
            "Into the talk, let's see."
        ],
        [
            "Let's assume that we want to find faces in images.",
            "OK, how do we handle this problem?",
            "I would say it's a typical classification task because we can learn to discriminate faces Patch containing faces from Patch containing background."
        ],
        [
            "So everybody is familiar already with the classification and we want to find in case of binary classification I function that is able to split up the data set in the circle.",
            "I will there's many techniques that can be used for that in this circle will concentrate on two of them that are official discriminant analysis answer."
        ],
        [
            "Vector machine.",
            "Let's analyze another task involving faces like the task of face recognition.",
            "We have a data set of phases of different subject and given a face knew phase as a probe, we want to be able to recognize which is the person in the field.",
            "This is a typically template matching problem and since in the most simple way images are represented as a vector.",
            "The Pixel at the image are concatenated.",
            "There's a problem due to the high dimensionality of data, so every faces can be typically in the normal computer vision processing task reconstructed by away this submission of some few basis synthetic faces that are called again faces.",
            "So Eigenfaces recall that we are used."
        ],
        [
            "Principal component analysis that is a well known dimensionality reduction technique.",
            "Actually, there are many more dimensionality reduction techniques in this circle will simply introduce a principal component analysis.",
            "What we want to do is to find low dimensional representation of our data."
        ],
        [
            "Let's see another task again involving faces.",
            "We have faces and the famous people, and we have text associated with the images.",
            "So we have the data that are present in multiple modalities.",
            "We have images and captures.",
            "So we ask ourself is there any latent aspects that relate the two modalities?",
            "How do we detect this latent aspect?",
            "We can use algorithms like for example Canonical correlation analysis or partial least squares.",
            "Laura algorithm that has been developed in order to find relationship between 2 pages."
        ],
        [
            "Data set.",
            "OK, I talk about classification, dimensionality reduction, finding relationship between two datasets, the last."
        ],
        [
            "A task we are going to consider is regression where we want to find the function that interpolates between the data points.",
            "I talk again about simple technique like least square regression and it's a regularised form that is called the regularising square or."
        ],
        [
            "Regulation.",
            "So in the first part of this talk, I'll simply introduce easy technique linear technique for vectorial data for various task classifications, dimensionality reduction, regression and finding relationship between data sets.",
            "I talk about linear technique that has been, I think everybody knows already but are very powerful and I believe that for three reasons.",
            "First of all, they are very intuitive because.",
            "Sometimes we need to be able to visualize the solution, but geometrically they work well may often because most of the natural function are smooth and we can ignore some loan small noise somehow and the good news is also that they are very faster and easy to solve because simply involves monitoring operation usually.",
            "So this is the last part of the talk that I'm gonna do before lunch and after lunch I gonna do.",
            "I would say more interesting things may be born you first I want to I ask."
        ],
        [
            "Myself, but what do we do if we have nonlinear function and I will introduce the notion of kernels, so I will canvas are used when we want to map our data in some high dimensional feature space where we can still use a linear technique after the mapping, and then there's part of the talker, and I think it's maybe the most knew and the most interesting is what what do we do if we don't have?",
            "Simple data, but we have structured data like sequences, graphs, images.",
            "How do we encode this into vectors?",
            "And which algorithm can we use to mine this data?"
        ],
        [
            "So a brief outline before lunch we we will talk about linear technique of our Potter analysis, some technique for regression, linear regression, least squares regression, Ridge regression introduced Fisher discriminant analysis for classification.",
            "Then I will talk about some problem that can be solved as again value problem.",
            "There are principal component analysis, partial least squares and Canonical correlation analysis.",
            "And then I will talk about.",
            "I will start to talk about the support vector machine."
        ],
        [
            "And the second part of the talk I will introduce Cannon method.",
            "I wish continued support vector machine but introducer or their Kennametal like Kernel Ridge regression cannot principal component analysis and kind of Canonical correlation analysis.",
            "The last part of the talk will discuss recent trend in machine learning.",
            "That is learning instructor at outer space that I like and I also worked a bit on it since I switched my search.",
            "Field in the last two years I will discuss some application model in computer vision, then in other fields.",
            "But you already know that the oldest technique are very powerful, widely used, and people from many fields rely on them."
        ],
        [
            "So.",
            "OK, let's talk about linear technique."
        ],
        [
            "Start with the least square problems and in case of regression briefer notation, we have data points represented as vector in the dimensional spaces and we have labels.",
            "Each label is associated to a data point labels come from.",
            "My lover is a real number, each level is.",
            "This color is alien near number.",
            "We want to find the linear decision function that basically what we want to learn.",
            "He's a parameter vector vector W that parameterized our decision function."
        ],
        [
            "An eye, our goal for regression is to find the linear function that approximate the label.",
            "Let's assume that the labeler are generated by this process, where we have a Gaussian noise with the diagonal covariance matrix and expected value equal to 0.",
            "When we look for the best double, the best W is the maximum likelihood estimator for W, and what we want to we want to get W. By solving these problems, once we get W. And we are able to do.",
            "Give the optimal label WHI too.",
            "When you test point X in the I want to do some homework, as Nello suggested in the in the in the store.",
            "This is a pattern function.",
            "I think you already see.",
            "I'm not doing any assumption, for example, about the capacity and no regularization, but this is a I'm trying to minimize something on W and this is.",
            "But the function.",
            "So just so do I have."
        ],
        [
            "Nutritional what we are doing is like with the list with the square regression we have for each data point a label and a prediction, and we measured for each training point there or the residual.",
            "So what we are summing up all the error and obtaining our cost function.",
            "So we are minimizing this cost function.",
            "OK.",
            "Speed.",
            "We are minimizing discuss function that is the sum of square rather wide is."
        ],
        [
            "Correspondence with the likelihood it's quite straightforward to see that we can brighten the lively with the in that way as a product of exponential, we can put the we can change the product in this submission if we go inside this potential and you can see that minimizing the likelihood that maximizing the likelihood is equal to minimizing the sum of square error."
        ],
        [
            "So how do we solve the least square regression problem we can?",
            "Get the discretion and put the gradient with respect to W equal to 0 and get W by inverting the covariance matrix an this is called the least square regression is it often is not a good idea to try to invert matrices, but it's better for numerical issue to solve the associated linear system that is written down."
        ],
        [
            "OK. And this quarter regression in is well known that tend to over to overfit data if we have data that are noisy and high dimensional.",
            "OK, I brought in a very intuitive way some concept that everybody is familiar with that are over fitting.",
            "So what is overheating?",
            "We're doing?",
            "Well on our training data, but we are doing very poorly on our test data, so we are not able to generalize.",
            "How do we end up with this problem?",
            "Since learning can be seen as a search in any part of the space, what we can do to add some control over fitting is to restrict the search of our hypothesis spaces in in order to to look for hypothesis that are not too much complex or flexible.",
            "So we look for iPod, any party space that has a low capacity.",
            "And if our training set is large enough, we have theoretical guarantee that we're doing well on similar test data, so this is just intuition not doing any bound.",
            "Any formula in this story?"
        ],
        [
            "So how do we control their capacity for our problem in this query?",
            "Regression we can minimize the loss that we see before at the same time put a control on the capacity.",
            "That means that we are restricting the search of our hypothesis spaces by imposing that the normal power vector W square should be less than one.",
            "This is known as a regularizer, least square or Ridge regression."
        ],
        [
            "OK, again I'll do.",
            "We solve this problem and I have to introduce the Lagrangian due to constraints.",
            "I put the gradient the gradient equal to 0 degraded with yeah.",
            "Any type of.",
            "Yeah, I had to to report an example.",
            "I'm sorry, it's just that I have so many things to to show in in the talk, but I yeah, I mean, it's better to visualize with the data just talking.",
            "It's difficult to explain.",
            "I mean you, you know how are nonlinear function over fields.",
            "Yeah yeah OK OK yeah but it's the same but I unfortunately I don't have.",
            "High dimensional space.",
            "Yeah, but I don't have a picture to show this.",
            "I'm sorry for this.",
            "I remove all the feature on the 1st part will give more space to the second part because I assume this is I would say.",
            "Warming up.",
            "Maybe I can look later if I have some picture for this.",
            "OK, I'm sorry.",
            "OK, again we rely on in matrix inversion where we have to.",
            "We rely on matrix inversion to solve."
        ],
        [
            "Our problem equating the gradient with respect to W equal to 0.",
            "Again, we have a probabilistic interpretation or and like for this square regression and basically what we are maximizes, we are maximizing the posterior that we assume proportional to the joint.",
            "We also assume that the.",
            "Prior on the vector W and you can see by simple algebra that minimizing the problem above is equal to maximizing the procedure.",
            "And let's see some.",
            "Let's introduce a bit more the notation."
        ],
        [
            "Fication we have a data points that are in a D dimensional space to each data point.",
            "A label is associated, but at this time the label can belong only to can be only plus 1 -- 1.",
            "We are looking for a linear decision function.",
            "I put that in the bias be inside the weight vector and in the following with a slight abuse of notation I just use W transpose X, including the bias inside the the parameter vector.",
            "So this is a four."
        ],
        [
            "Relation for Fisher discriminant analysis or linear discriminant analysis and you can see that simply changing the the set of the label we are solving the same problem and and solving different task.",
            "So from regression to classification we have we can solve exactly the same problem.",
            "In practice, what the Fisher discriminant analysis is doing is maximizing the function that represents the difference between the mean of the two classes, normalized by measure that.",
            "Represent the spread of the two of the data."
        ],
        [
            "So this is the typical relation that is minimized.",
            "Maximizing Fisher discriminant analysis, so we're maximizing the distance between the mean of the two classes normalized by the sum of standard deviation.",
            "We can rewrite this in term of the vector W by considering what are well known as.",
            "Between between class scatter metrics within class scatter metrics and this is the formulation of the problem of Fisher discriminant analysis.",
            "And we can admit for this also a regularised form, and the solution is the same up to a custom factor to what we see before.",
            "Later we will see another interpretation official."
        ],
        [
            "Agreement analysis.",
            "OK, I will talk now about some."
        ],
        [
            "Eigenvalues problem.",
            "The well known principal component analysis can be cast as a negative value problem or other problem that.",
            "Want to find the relationship between two paid data sets are also eigenvalue problems and, for example, principal component analysis on their data set its amount.",
            "Is it an invalid problem, partially square written again value problem.",
            "Canonical correlation analysis can be cast as another invalid problem for all of them we can admit to regularize form and we can stand also this technique to finding relationship between more than two data sources and we will see multiway Canonical correlation analysis."
        ],
        [
            "And this is a general form of an eigenvalue problems.",
            "This is pretty nice because we have nice properties on problem like that.",
            "First of all that for real and symmetric matrix, again value at negative vector.",
            "Ariel then also that the hidden vector are orthogonal and the good news is also that if we are able to cast our problem as an again value problem, it's fast to compute because there are.",
            "A lot of studying of iterative technique that can solve very easily this problem."
        ],
        [
            "So let's talk about the most famous one that is a principal component analysis in principal component analysis, we want to find direction of large variance in the data.",
            "And we are maximizing.",
            "These are function that represent the variance of the projection of the data set on the vector W, and again this is a pattern function and we can impose a capacity control as a.",
            "Just to recall the framework introduced by Nell imposing that the norm of WEC."
        ],
        [
            "But one.",
            "How do we solve this problem?",
            "Again, we can compute the Lagrangian and solve the following eigenvalue problem.",
            "So as you can see, this is a negative value problem as."
        ],
        [
            "Of this phone.",
            "Another way to."
        ],
        [
            "Formulate a principal component analysis is by the Russia on the right and the the Matrix X. Transpose X represented the empirical covariance matrix that is a positive semidefinite symmetric matrix and.",
            "The solution of this problem is given by the largest I get the eigenvector corresponding to the largest eigenvalue.",
            "The value of the ratio.",
            "Correspond to the largest eigenvalue.",
            "If we start to deflate this matrix with respect to W, then we get all the other eigenvectors."
        ],
        [
            "Principal component analysis is usually used for dimensionality reduction, where we have the need to approximate our data in a low dimensional subspace.",
            "So from K2D where case is much much smaller than the, why do we need dimensionality reduction for many things?",
            "First of all, for visualization purposes, imagine that we want to be able to project our data into.",
            "2 dimensional, three dimension.",
            "We can use the dimensionality reduction for data compression when we have the need to store amount of data like images, we can use a dimensionality reduction.",
            "For example for the noise.",
            "England because.",
            "We remove the small noise direction somehow.",
            "Dimensionality reduction very often imply information loss, but we have guaranteed at the PC a press serve as much information as possible.",
            "I say not always because let's assume that our debt I have for example one feature that is cost and for all the data points that if we project our vector to the dimension D -- 1.",
            "We don't lose any information, but this is in general the case.",
            "We are losing information so.",
            "How do we protect our data to a low dimensional subspace?",
            "How do we get these compact representation of our data by considering the agent vector corresponding to the K largest eigenvalues?"
        ],
        [
            "There's a question that usually arise.",
            "How many eigenvectors do we keep?",
            "How many principal components do we keep?",
            "Typically we studied eigenvalues spectrum, so we study the variance explained by all the components and this is an example of the eigenvalues spectral.",
            "We can for example user these.",
            "This rule to choose K where the ratio is between the sum of the first K eigenvalues divided by the sum of the old eigenvalues.",
            "If we put the threshold T equal to 0.9, we mean that we explain the 90% of the variance of the data by keeping only the first again vectors."
        ],
        [
            "Anne.",
            "PCA is being used a lot for dimensionality reduction, but in many problems, maybe it's not a good idea.",
            "For example, where we want to.",
            "To reduce the dimensionality of our data for classification purposes, you can see from the picture that if you project our data in the in the direction of the principal component, the data are not overlapped.",
            "If we find another way to project our data, like for example as a using a linear discriminant analysis, our data are well separated so.",
            "Also, Fisher discriminant analysis or linear discriminant analysis can be used to as a dimensionality reduction technique where this time and the metrics we contain the first degree in vector of the scatter, the product of the scatter matrix that I introduced before."
        ],
        [
            "This is quite a.",
            "All the style discussion ING in computer vision.",
            "Alot of people and we use a PCA, LDA for dimensionality reduction as a preprocessing step to do other things and there's a desert seminar favor that.",
            "Days do result that is better to keep in mind as a practical issue.",
            "I mean, when the training set is small, typically the scatter matrix, artificial discriminants analysis are not well characterized, so it's better to use PCA instead.",
            "When the number of sample is enough, we can get a good representation of our data.",
            "Representative for the class and we can use linear discriminant analysis, for example to perform dimensionality reduction."
        ],
        [
            "OK.",
            "So what if we have two data set and we want to find related direction on two paid data set?",
            "And there are many applications where we need to do that, like for example whether we want to do cross language retrieval of documents and we have English and French test.",
            "We want to discover the relationship between them or we have images an.",
            "And captions as I show you in the introduction or for example where we want to do speaker recognition.",
            "And we have some audio data and visual data with the data, for example describing the end it movement.",
            "So."
        ],
        [
            "What we can do?",
            "We can use a serious of eigenvalue problems.",
            "For example, we can use a principal component analysis on the joint data set.",
            "The joint data set created by the two pair data set X&Y.",
            "We can maximize the covariance by partially squared, or we can maximize the correlation between the two data set by Canonical correlation and."
        ],
        [
            "So this is a the same problem of principal component analysis that we saw before, but I plug but I put all the data together or X&Y as a whole data set.",
            "And so.",
            "We have exactly the same problem as before.",
            "The metrics in the objective function is made by covariance matrices and the cross covariance matrices on the.",
            "Opposite diagonal a.",
            "We want to compute the optimal projection WX&WY in order to maximize the variance on the joint data set.",
            "And we can do the same thing.",
            "You know computing the Lagrangian equation gradient to zero and get these again value problem exactly as before and solve this again value problem.",
            "The good news is that the result is essentially independent."
        ],
        [
            "End of our data has been split up.",
            "Another technique could be to maximize the covariance between the projection."
        ],
        [
            "In this case, we can solve a decent immigration problem.",
            "If we fix the scale, we can see that these two optimization problem, up and down are equivalent and this.",
            "By equivalent we mean that the solution is the same up to a constant factor."
        ],
        [
            "It's also equivalent to the following problem that maybe it's more easy to to solve to see how to solve, and we compute the Lagrangian.",
            "Again, we put the gradient and with respect to WX&WY equal to 0, and what we get is again an order a eigenvalue problem.",
            "Last second value problem is Canonical correlation analysis."
        ],
        [
            "We are maximizing the correlation, assuming that the variance is irrelevant, we will see after what does it mean in terms of result.",
            "This problem is equivalent.",
            "Again, fixing the scale to the problem below."
        ],
        [
            "We can solve this problem again with the same technique.",
            "Computing the Lagrangian, putting the gradient equal to 0, and we get another eigenvalue problem."
        ],
        [
            "I want to summarize.",
            "I did.",
            "It is exactly the same procedure for the three problems.",
            "Different objective function maximize correlation, covariance variance of the joint data set.",
            "The machine is exactly the same and they changed the.",
            "What are the value of the metrics A&B?"
        ],
        [
            "What happened in practice is that the PCA and partially square look at.",
            "Look at large variance direction so they ignore a small variance in the data and they are more robust to noise.",
            "Canonical correlation analysis.",
            "Assume that the variance of the data is irrelevant, so only looks at correlation in the data, and it's more sensitive to noise.",
            "That's why it's important in this case to introduce regularization to make CCA robus robust against noise and to introduce regularization.",
            "As for the other technique that we saw before we try to obtain solution WX&Y of small norm."
        ],
        [
            "This is the original problem on the top, below regularised problem where we have a tradeoff regulated by the parimeter C between our pattern function and our regularization term."
        ],
        [
            "We can solve the regularised problem exactly as before computing the Lagrangian, equating the gradient equal to 0, and again we get another again value problem."
        ],
        [
            "What it's interesting to see here is that indeed and Fisher discriminant analysis and Ridge regression are special cases of regularised Canonical correlation analysis.",
            "How do we see this?",
            "Let's, for example, substitute the second data set Y by one vector, that in case official discriminant analysis will be the vector of the label price minus one in case of Ridge, regression will be.",
            "The vector of the labels belonging to the real set of real number.",
            "We can solve this problem and get the same that the official discriminants solution so Fisher Discriminator is a special case of regularised Canonical correlation analysis and also read progression.",
            "We can also see that cannot at this point Canonical correlation analysis is a form of multivariate regression.",
            "So where we have not just one label but we want to regress on a multiple set of label.",
            "It's a good idea perhaps to use the regularize Canonical Coronation and."
        ],
        [
            "Lost things.",
            "What do we do if we want the identify relationship?",
            "A common factors between more than two data sets?",
            "It's we we can develop with some multi way extension of partially square root principal component analysis and Canonical correlation analysis.",
            "In this case we have a K data set and we want to compute the associated vector W. For example, just for Canonical."
        ],
        [
            "Election analysis this is how can we?",
            "And this problem solving this resulting session problem.",
            "But we have again a pattern function and capacity control.",
            "You can see that the problem up and now are we violent, because if you look at the last expression, the last term is controlled by the regularization function factor and is equal to 1.",
            "So the two problems are equivalent."
        ],
        [
            "At.",
            "Then we can see how solving this problem.",
            "We simply write it and we see that the the solution of the multiway CCA can be obtained again by solving an negative value problem of this.",
            "Metrics A&B are the metrics in the formula.",
            "And then this problem and meet regularised."
        ],
        [
            "Went just to conclude.",
            "And there are many important problems impacting analysis, dimensionality reduction, finding relationship between two sets of data classification, all of them.",
            "They can be reduced to eigenvalue problems and they can be started by simple linear algebra.",
            "They can be solved or approximated efficiently with Sander technique.",
            "But most importantly, all of them involve only the inner product between training data.",
            "Between the data and so we can easily canalized these approaches.",
            "We're gonna see this after lunch.",
            "Want to say just.",
            "Could have been a bit boring here because I just reported the former.",
            "I assume everybody is a bit.",
            "Familiar with these things, but the message is that.",
            "All of these techniques have a common framework behind.",
            "Once we learn one of them, we can easily switch from one to another one.",
            "It requires a bit of practice to see what is the problems related with noise or not.",
            "Why partial square sometime is better than Canonical correlation analysis, but in general we recommend the use of this technique for many applications.",
            "I think I'll do this after lunch.",
            "So what is the robustness of the parameter is important to get it right?",
            "OK, yes, OK, I OK.",
            "I went to fasten this point, they see.",
            "DC is that the control to regularization is like depending on our big our larger or small is C. You can control how much you tend to overfit your data, so it's I don't know you.",
            "I think you played already with support Vector Machine, maybe this.",
            "Simply sample there is a super meter there as well.",
            "You you play with it and control the regularization.",
            "You just what do you get?",
            "I know, I mean it's a classical model selection problem of many algorithms, there's no.",
            "Good answer to this.",
            "I mean Knights Cross validation if you have support vector machine.",
            "For example if you have a supervised setting, no, there's no rule of thumb, but to set it.",
            "Difficult function would see movie theater slides, yeah?",
            "That's a tradeoff there.",
            "If there is no seat, then you got this.",
            "Office politics normalcy.",
            "When you find your place.",
            "Reality, besides only theories we make really answer.",
            "Yeah it is it right for validation.",
            "After reading glasses, just.",
            "So here I got the King of his PCA.",
            "When Y is 1 dimensional, spaces transition.",
            "No, I want just to to show that the Fisher discriminant analysis and Ridge regression are special cases of regularised Canonical correlation analysis, when the second data set it's one vector.",
            "11 maker yeah, the second data set.",
            "We want to mine to pair data.",
            "Set the second data set is disabled.",
            "The vector of the labels.",
            "My experience is, yeah, this is often used when Y is 1 dimensional.",
            "Recognition.",
            "1st.",
            "I did it.",
            "Invite.",
            "Brenda success he used when the first component, then the signal and it's really strong.",
            "Yeah yeah no no no.",
            "Yeah, I agree.",
            "These are.",
            "Sea sea.",
            "Do you constantly that you should extract syrup or pain with no IIII didn't meant that I just want to to to show the similarities of the problem.",
            "I mean that the Fisher discriminant analysis can be also cost as a negative value problem.",
            "If you want impractical.",
            "I mean I think if you use Fisher discriminant analysis you solve the usual problem.",
            "I mean like.",
            "Linear system if you want, I mean.",
            "No, I just want to show the similarities, not that I recommend to solve Fisher discriminant analysis as a negative value problem.",
            "I mean I have a personal experience with what I will show in the second part of the talk where we developed a kind of extension to Fisher discriminant analysis, official discriminant analysis for structured output learning.",
            "And there we could not all work on eigenvalue problems.",
            "We have to find.",
            "Better way to solve linear systems.",
            "So now I've is just formulas is not.",
            "This will be a.",
            "In the case of the sea, it's not all about just introduction with an important thing afterwards.",
            "Also, importation of the new plug, right?",
            "If you visualize, I can hide a mental data set in lower dimensions you need me to say what those axes are high, yes?",
            "Yeah, that that that's they're not really interpretable, you know, interpretable there is this problem with.",
            "Yeah, go ahead.",
            "I figure that is in this implication easier.",
            "They are right because they are just linear combinations of the originals.",
            "I don't know, I'm not sure to get to what you said.",
            "I think it's just a second.",
            "High dimension data set included in two dimensions.",
            "I want to know what those exit means first in Europe in the input.",
            "Measurements, that's pretty clear then what it means.",
            "But just like to dimension, but afterwards you need to assign names to the excess of busy.",
            "That I'm.",
            "Thing I mean what I wanted to show you here is that what you get the principal component today.",
            "Again vector for this.",
            "For example, for the face that.",
            "So he's not an interpretation behind them.",
            "Interpretation is that in the data set to some pattern are more likely.",
            "Like you know, I don't know the edge of the most I show, I don't know, but.",
            "Not there are other techniques I would say for that imply dimensionality reduction and ever better interpretation of the data, but I'm not sure to get.",
            "Hey.",
            "I mean, I principal component analysis has been the 1st of a very older technique for dimensionality reduction.",
            "They've been banned so afterwards and some are more likely for visualization purposes I think.",
            "I I cannot hear you so I'm sorry.",
            "A Madison Square you haven't heard capacity numbers.",
            "If you look at his lightning Bolt mean if you consider the cost function as axle make faces.",
            "Functions, then you can introduce some sort of I distribution or see next.",
            "I guess I don't know.",
            "I mean, I never thought about the probabilistic interpretation of this year.",
            "Produce.",
            "Distribution check.",
            "I'm at 2.1 or something that makes you maximize the option that I I don't.",
            "I really don't know.",
            "Have in mind some some work, but I mean I don't think honestly that Canonical correlation analysis as a nice probably think I'm not.",
            "I'm not sure.",
            "I don't think so.",
            "I I, I'm not sure really I. I remember some work of people trying to use Canonical correlation analysis and gets some interpretation of the data, but.",
            "Not really, I I would love to.",
            "Just one question.",
            "Who is the?",
            "I.",
            "Uh huh.",
            "No, it was not.",
            "We can resort of Mackennal.",
            "We will discuss later on because all of this technique rely on DOT products between data, But yeah.",
            "You can ask questions also after the camera bag which may be recorded second, partially covered.",
            "So.",
            "I say thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just want to recall a some few concept that never already introduced quite a lot of like what we mean by pattern analysis.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By pattern analysis we mean the automatic detection of patterns into the attack and what is a pattern is a basically an instructor and irregularity.",
                    "label": 0
                },
                {
                    "sent": "Any relations that it's present in the data and we want to be able to discover automatically the concept of pattern analysis is strongly connected with the concept of learning because we want to use learning technique in order to be able to detect the significant factor and to transfer the information that we learn.",
                    "label": 1
                },
                {
                    "sent": "By making prediction on new data that you assume coming from the same source in this talk, I will simply talk about data as are represented as vector.",
                    "label": 1
                },
                {
                    "sent": "The other tools will introduce other type of data like sequences.",
                    "label": 0
                },
                {
                    "sent": "So basically I'm talking about statistical pattern recognition where we where we have data represented as vectors and we made some statistical assumption about their distribution before we.",
                    "label": 1
                },
                {
                    "sent": "Into the talk, let's see.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's assume that we want to find faces in images.",
                    "label": 1
                },
                {
                    "sent": "OK, how do we handle this problem?",
                    "label": 0
                },
                {
                    "sent": "I would say it's a typical classification task because we can learn to discriminate faces Patch containing faces from Patch containing background.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So everybody is familiar already with the classification and we want to find in case of binary classification I function that is able to split up the data set in the circle.",
                    "label": 0
                },
                {
                    "sent": "I will there's many techniques that can be used for that in this circle will concentrate on two of them that are official discriminant analysis answer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vector machine.",
                    "label": 0
                },
                {
                    "sent": "Let's analyze another task involving faces like the task of face recognition.",
                    "label": 0
                },
                {
                    "sent": "We have a data set of phases of different subject and given a face knew phase as a probe, we want to be able to recognize which is the person in the field.",
                    "label": 0
                },
                {
                    "sent": "This is a typically template matching problem and since in the most simple way images are represented as a vector.",
                    "label": 1
                },
                {
                    "sent": "The Pixel at the image are concatenated.",
                    "label": 0
                },
                {
                    "sent": "There's a problem due to the high dimensionality of data, so every faces can be typically in the normal computer vision processing task reconstructed by away this submission of some few basis synthetic faces that are called again faces.",
                    "label": 1
                },
                {
                    "sent": "So Eigenfaces recall that we are used.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Principal component analysis that is a well known dimensionality reduction technique.",
                    "label": 1
                },
                {
                    "sent": "Actually, there are many more dimensionality reduction techniques in this circle will simply introduce a principal component analysis.",
                    "label": 1
                },
                {
                    "sent": "What we want to do is to find low dimensional representation of our data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see another task again involving faces.",
                    "label": 0
                },
                {
                    "sent": "We have faces and the famous people, and we have text associated with the images.",
                    "label": 0
                },
                {
                    "sent": "So we have the data that are present in multiple modalities.",
                    "label": 1
                },
                {
                    "sent": "We have images and captures.",
                    "label": 1
                },
                {
                    "sent": "So we ask ourself is there any latent aspects that relate the two modalities?",
                    "label": 1
                },
                {
                    "sent": "How do we detect this latent aspect?",
                    "label": 0
                },
                {
                    "sent": "We can use algorithms like for example Canonical correlation analysis or partial least squares.",
                    "label": 0
                },
                {
                    "sent": "Laura algorithm that has been developed in order to find relationship between 2 pages.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data set.",
                    "label": 0
                },
                {
                    "sent": "OK, I talk about classification, dimensionality reduction, finding relationship between two datasets, the last.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A task we are going to consider is regression where we want to find the function that interpolates between the data points.",
                    "label": 0
                },
                {
                    "sent": "I talk again about simple technique like least square regression and it's a regularised form that is called the regularising square or.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regulation.",
                    "label": 0
                },
                {
                    "sent": "So in the first part of this talk, I'll simply introduce easy technique linear technique for vectorial data for various task classifications, dimensionality reduction, regression and finding relationship between data sets.",
                    "label": 1
                },
                {
                    "sent": "I talk about linear technique that has been, I think everybody knows already but are very powerful and I believe that for three reasons.",
                    "label": 0
                },
                {
                    "sent": "First of all, they are very intuitive because.",
                    "label": 1
                },
                {
                    "sent": "Sometimes we need to be able to visualize the solution, but geometrically they work well may often because most of the natural function are smooth and we can ignore some loan small noise somehow and the good news is also that they are very faster and easy to solve because simply involves monitoring operation usually.",
                    "label": 0
                },
                {
                    "sent": "So this is the last part of the talk that I'm gonna do before lunch and after lunch I gonna do.",
                    "label": 0
                },
                {
                    "sent": "I would say more interesting things may be born you first I want to I ask.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Myself, but what do we do if we have nonlinear function and I will introduce the notion of kernels, so I will canvas are used when we want to map our data in some high dimensional feature space where we can still use a linear technique after the mapping, and then there's part of the talker, and I think it's maybe the most knew and the most interesting is what what do we do if we don't have?",
                    "label": 1
                },
                {
                    "sent": "Simple data, but we have structured data like sequences, graphs, images.",
                    "label": 1
                },
                {
                    "sent": "How do we encode this into vectors?",
                    "label": 0
                },
                {
                    "sent": "And which algorithm can we use to mine this data?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a brief outline before lunch we we will talk about linear technique of our Potter analysis, some technique for regression, linear regression, least squares regression, Ridge regression introduced Fisher discriminant analysis for classification.",
                    "label": 0
                },
                {
                    "sent": "Then I will talk about some problem that can be solved as again value problem.",
                    "label": 0
                },
                {
                    "sent": "There are principal component analysis, partial least squares and Canonical correlation analysis.",
                    "label": 1
                },
                {
                    "sent": "And then I will talk about.",
                    "label": 0
                },
                {
                    "sent": "I will start to talk about the support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second part of the talk I will introduce Cannon method.",
                    "label": 0
                },
                {
                    "sent": "I wish continued support vector machine but introducer or their Kennametal like Kernel Ridge regression cannot principal component analysis and kind of Canonical correlation analysis.",
                    "label": 1
                },
                {
                    "sent": "The last part of the talk will discuss recent trend in machine learning.",
                    "label": 0
                },
                {
                    "sent": "That is learning instructor at outer space that I like and I also worked a bit on it since I switched my search.",
                    "label": 0
                },
                {
                    "sent": "Field in the last two years I will discuss some application model in computer vision, then in other fields.",
                    "label": 0
                },
                {
                    "sent": "But you already know that the oldest technique are very powerful, widely used, and people from many fields rely on them.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, let's talk about linear technique.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with the least square problems and in case of regression briefer notation, we have data points represented as vector in the dimensional spaces and we have labels.",
                    "label": 0
                },
                {
                    "sent": "Each label is associated to a data point labels come from.",
                    "label": 0
                },
                {
                    "sent": "My lover is a real number, each level is.",
                    "label": 0
                },
                {
                    "sent": "This color is alien near number.",
                    "label": 0
                },
                {
                    "sent": "We want to find the linear decision function that basically what we want to learn.",
                    "label": 0
                },
                {
                    "sent": "He's a parameter vector vector W that parameterized our decision function.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An eye, our goal for regression is to find the linear function that approximate the label.",
                    "label": 1
                },
                {
                    "sent": "Let's assume that the labeler are generated by this process, where we have a Gaussian noise with the diagonal covariance matrix and expected value equal to 0.",
                    "label": 0
                },
                {
                    "sent": "When we look for the best double, the best W is the maximum likelihood estimator for W, and what we want to we want to get W. By solving these problems, once we get W. And we are able to do.",
                    "label": 1
                },
                {
                    "sent": "Give the optimal label WHI too.",
                    "label": 0
                },
                {
                    "sent": "When you test point X in the I want to do some homework, as Nello suggested in the in the in the store.",
                    "label": 0
                },
                {
                    "sent": "This is a pattern function.",
                    "label": 0
                },
                {
                    "sent": "I think you already see.",
                    "label": 0
                },
                {
                    "sent": "I'm not doing any assumption, for example, about the capacity and no regularization, but this is a I'm trying to minimize something on W and this is.",
                    "label": 0
                },
                {
                    "sent": "But the function.",
                    "label": 0
                },
                {
                    "sent": "So just so do I have.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nutritional what we are doing is like with the list with the square regression we have for each data point a label and a prediction, and we measured for each training point there or the residual.",
                    "label": 0
                },
                {
                    "sent": "So what we are summing up all the error and obtaining our cost function.",
                    "label": 0
                },
                {
                    "sent": "So we are minimizing this cost function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Speed.",
                    "label": 0
                },
                {
                    "sent": "We are minimizing discuss function that is the sum of square rather wide is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correspondence with the likelihood it's quite straightforward to see that we can brighten the lively with the in that way as a product of exponential, we can put the we can change the product in this submission if we go inside this potential and you can see that minimizing the likelihood that maximizing the likelihood is equal to minimizing the sum of square error.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we solve the least square regression problem we can?",
                    "label": 0
                },
                {
                    "sent": "Get the discretion and put the gradient with respect to W equal to 0 and get W by inverting the covariance matrix an this is called the least square regression is it often is not a good idea to try to invert matrices, but it's better for numerical issue to solve the associated linear system that is written down.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And this quarter regression in is well known that tend to over to overfit data if we have data that are noisy and high dimensional.",
                    "label": 1
                },
                {
                    "sent": "OK, I brought in a very intuitive way some concept that everybody is familiar with that are over fitting.",
                    "label": 0
                },
                {
                    "sent": "So what is overheating?",
                    "label": 0
                },
                {
                    "sent": "We're doing?",
                    "label": 0
                },
                {
                    "sent": "Well on our training data, but we are doing very poorly on our test data, so we are not able to generalize.",
                    "label": 1
                },
                {
                    "sent": "How do we end up with this problem?",
                    "label": 1
                },
                {
                    "sent": "Since learning can be seen as a search in any part of the space, what we can do to add some control over fitting is to restrict the search of our hypothesis spaces in in order to to look for hypothesis that are not too much complex or flexible.",
                    "label": 0
                },
                {
                    "sent": "So we look for iPod, any party space that has a low capacity.",
                    "label": 0
                },
                {
                    "sent": "And if our training set is large enough, we have theoretical guarantee that we're doing well on similar test data, so this is just intuition not doing any bound.",
                    "label": 1
                },
                {
                    "sent": "Any formula in this story?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we control their capacity for our problem in this query?",
                    "label": 0
                },
                {
                    "sent": "Regression we can minimize the loss that we see before at the same time put a control on the capacity.",
                    "label": 1
                },
                {
                    "sent": "That means that we are restricting the search of our hypothesis spaces by imposing that the normal power vector W square should be less than one.",
                    "label": 1
                },
                {
                    "sent": "This is known as a regularizer, least square or Ridge regression.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, again I'll do.",
                    "label": 0
                },
                {
                    "sent": "We solve this problem and I have to introduce the Lagrangian due to constraints.",
                    "label": 0
                },
                {
                    "sent": "I put the gradient the gradient equal to 0 degraded with yeah.",
                    "label": 1
                },
                {
                    "sent": "Any type of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I had to to report an example.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, it's just that I have so many things to to show in in the talk, but I yeah, I mean, it's better to visualize with the data just talking.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to explain.",
                    "label": 0
                },
                {
                    "sent": "I mean you, you know how are nonlinear function over fields.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah OK OK yeah but it's the same but I unfortunately I don't have.",
                    "label": 0
                },
                {
                    "sent": "High dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I don't have a picture to show this.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry for this.",
                    "label": 0
                },
                {
                    "sent": "I remove all the feature on the 1st part will give more space to the second part because I assume this is I would say.",
                    "label": 0
                },
                {
                    "sent": "Warming up.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can look later if I have some picture for this.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, again we rely on in matrix inversion where we have to.",
                    "label": 0
                },
                {
                    "sent": "We rely on matrix inversion to solve.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our problem equating the gradient with respect to W equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Again, we have a probabilistic interpretation or and like for this square regression and basically what we are maximizes, we are maximizing the posterior that we assume proportional to the joint.",
                    "label": 0
                },
                {
                    "sent": "We also assume that the.",
                    "label": 0
                },
                {
                    "sent": "Prior on the vector W and you can see by simple algebra that minimizing the problem above is equal to maximizing the procedure.",
                    "label": 0
                },
                {
                    "sent": "And let's see some.",
                    "label": 0
                },
                {
                    "sent": "Let's introduce a bit more the notation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fication we have a data points that are in a D dimensional space to each data point.",
                    "label": 0
                },
                {
                    "sent": "A label is associated, but at this time the label can belong only to can be only plus 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "We are looking for a linear decision function.",
                    "label": 1
                },
                {
                    "sent": "I put that in the bias be inside the weight vector and in the following with a slight abuse of notation I just use W transpose X, including the bias inside the the parameter vector.",
                    "label": 0
                },
                {
                    "sent": "So this is a four.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relation for Fisher discriminant analysis or linear discriminant analysis and you can see that simply changing the the set of the label we are solving the same problem and and solving different task.",
                    "label": 1
                },
                {
                    "sent": "So from regression to classification we have we can solve exactly the same problem.",
                    "label": 0
                },
                {
                    "sent": "In practice, what the Fisher discriminant analysis is doing is maximizing the function that represents the difference between the mean of the two classes, normalized by measure that.",
                    "label": 1
                },
                {
                    "sent": "Represent the spread of the two of the data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the typical relation that is minimized.",
                    "label": 0
                },
                {
                    "sent": "Maximizing Fisher discriminant analysis, so we're maximizing the distance between the mean of the two classes normalized by the sum of standard deviation.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this in term of the vector W by considering what are well known as.",
                    "label": 0
                },
                {
                    "sent": "Between between class scatter metrics within class scatter metrics and this is the formulation of the problem of Fisher discriminant analysis.",
                    "label": 0
                },
                {
                    "sent": "And we can admit for this also a regularised form, and the solution is the same up to a custom factor to what we see before.",
                    "label": 0
                },
                {
                    "sent": "Later we will see another interpretation official.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agreement analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, I will talk now about some.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eigenvalues problem.",
                    "label": 0
                },
                {
                    "sent": "The well known principal component analysis can be cast as a negative value problem or other problem that.",
                    "label": 0
                },
                {
                    "sent": "Want to find the relationship between two paid data sets are also eigenvalue problems and, for example, principal component analysis on their data set its amount.",
                    "label": 1
                },
                {
                    "sent": "Is it an invalid problem, partially square written again value problem.",
                    "label": 1
                },
                {
                    "sent": "Canonical correlation analysis can be cast as another invalid problem for all of them we can admit to regularize form and we can stand also this technique to finding relationship between more than two data sources and we will see multiway Canonical correlation analysis.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is a general form of an eigenvalue problems.",
                    "label": 1
                },
                {
                    "sent": "This is pretty nice because we have nice properties on problem like that.",
                    "label": 0
                },
                {
                    "sent": "First of all that for real and symmetric matrix, again value at negative vector.",
                    "label": 0
                },
                {
                    "sent": "Ariel then also that the hidden vector are orthogonal and the good news is also that if we are able to cast our problem as an again value problem, it's fast to compute because there are.",
                    "label": 0
                },
                {
                    "sent": "A lot of studying of iterative technique that can solve very easily this problem.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about the most famous one that is a principal component analysis in principal component analysis, we want to find direction of large variance in the data.",
                    "label": 1
                },
                {
                    "sent": "And we are maximizing.",
                    "label": 0
                },
                {
                    "sent": "These are function that represent the variance of the projection of the data set on the vector W, and again this is a pattern function and we can impose a capacity control as a.",
                    "label": 1
                },
                {
                    "sent": "Just to recall the framework introduced by Nell imposing that the norm of WEC.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But one.",
                    "label": 0
                },
                {
                    "sent": "How do we solve this problem?",
                    "label": 0
                },
                {
                    "sent": "Again, we can compute the Lagrangian and solve the following eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, this is a negative value problem as.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this phone.",
                    "label": 0
                },
                {
                    "sent": "Another way to.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formulate a principal component analysis is by the Russia on the right and the the Matrix X. Transpose X represented the empirical covariance matrix that is a positive semidefinite symmetric matrix and.",
                    "label": 1
                },
                {
                    "sent": "The solution of this problem is given by the largest I get the eigenvector corresponding to the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "The value of the ratio.",
                    "label": 0
                },
                {
                    "sent": "Correspond to the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "If we start to deflate this matrix with respect to W, then we get all the other eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Principal component analysis is usually used for dimensionality reduction, where we have the need to approximate our data in a low dimensional subspace.",
                    "label": 1
                },
                {
                    "sent": "So from K2D where case is much much smaller than the, why do we need dimensionality reduction for many things?",
                    "label": 0
                },
                {
                    "sent": "First of all, for visualization purposes, imagine that we want to be able to project our data into.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional, three dimension.",
                    "label": 1
                },
                {
                    "sent": "We can use the dimensionality reduction for data compression when we have the need to store amount of data like images, we can use a dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "For example for the noise.",
                    "label": 0
                },
                {
                    "sent": "England because.",
                    "label": 0
                },
                {
                    "sent": "We remove the small noise direction somehow.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality reduction very often imply information loss, but we have guaranteed at the PC a press serve as much information as possible.",
                    "label": 1
                },
                {
                    "sent": "I say not always because let's assume that our debt I have for example one feature that is cost and for all the data points that if we project our vector to the dimension D -- 1.",
                    "label": 0
                },
                {
                    "sent": "We don't lose any information, but this is in general the case.",
                    "label": 0
                },
                {
                    "sent": "We are losing information so.",
                    "label": 1
                },
                {
                    "sent": "How do we protect our data to a low dimensional subspace?",
                    "label": 0
                },
                {
                    "sent": "How do we get these compact representation of our data by considering the agent vector corresponding to the K largest eigenvalues?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a question that usually arise.",
                    "label": 0
                },
                {
                    "sent": "How many eigenvectors do we keep?",
                    "label": 0
                },
                {
                    "sent": "How many principal components do we keep?",
                    "label": 1
                },
                {
                    "sent": "Typically we studied eigenvalues spectrum, so we study the variance explained by all the components and this is an example of the eigenvalues spectral.",
                    "label": 0
                },
                {
                    "sent": "We can for example user these.",
                    "label": 0
                },
                {
                    "sent": "This rule to choose K where the ratio is between the sum of the first K eigenvalues divided by the sum of the old eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If we put the threshold T equal to 0.9, we mean that we explain the 90% of the variance of the data by keeping only the first again vectors.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "PCA is being used a lot for dimensionality reduction, but in many problems, maybe it's not a good idea.",
                    "label": 1
                },
                {
                    "sent": "For example, where we want to.",
                    "label": 1
                },
                {
                    "sent": "To reduce the dimensionality of our data for classification purposes, you can see from the picture that if you project our data in the in the direction of the principal component, the data are not overlapped.",
                    "label": 0
                },
                {
                    "sent": "If we find another way to project our data, like for example as a using a linear discriminant analysis, our data are well separated so.",
                    "label": 1
                },
                {
                    "sent": "Also, Fisher discriminant analysis or linear discriminant analysis can be used to as a dimensionality reduction technique where this time and the metrics we contain the first degree in vector of the scatter, the product of the scatter matrix that I introduced before.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is quite a.",
                    "label": 0
                },
                {
                    "sent": "All the style discussion ING in computer vision.",
                    "label": 0
                },
                {
                    "sent": "Alot of people and we use a PCA, LDA for dimensionality reduction as a preprocessing step to do other things and there's a desert seminar favor that.",
                    "label": 0
                },
                {
                    "sent": "Days do result that is better to keep in mind as a practical issue.",
                    "label": 0
                },
                {
                    "sent": "I mean, when the training set is small, typically the scatter matrix, artificial discriminants analysis are not well characterized, so it's better to use PCA instead.",
                    "label": 1
                },
                {
                    "sent": "When the number of sample is enough, we can get a good representation of our data.",
                    "label": 0
                },
                {
                    "sent": "Representative for the class and we can use linear discriminant analysis, for example to perform dimensionality reduction.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what if we have two data set and we want to find related direction on two paid data set?",
                    "label": 1
                },
                {
                    "sent": "And there are many applications where we need to do that, like for example whether we want to do cross language retrieval of documents and we have English and French test.",
                    "label": 1
                },
                {
                    "sent": "We want to discover the relationship between them or we have images an.",
                    "label": 0
                },
                {
                    "sent": "And captions as I show you in the introduction or for example where we want to do speaker recognition.",
                    "label": 0
                },
                {
                    "sent": "And we have some audio data and visual data with the data, for example describing the end it movement.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we can do?",
                    "label": 0
                },
                {
                    "sent": "We can use a serious of eigenvalue problems.",
                    "label": 1
                },
                {
                    "sent": "For example, we can use a principal component analysis on the joint data set.",
                    "label": 0
                },
                {
                    "sent": "The joint data set created by the two pair data set X&Y.",
                    "label": 0
                },
                {
                    "sent": "We can maximize the covariance by partially squared, or we can maximize the correlation between the two data set by Canonical correlation and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a the same problem of principal component analysis that we saw before, but I plug but I put all the data together or X&Y as a whole data set.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We have exactly the same problem as before.",
                    "label": 0
                },
                {
                    "sent": "The metrics in the objective function is made by covariance matrices and the cross covariance matrices on the.",
                    "label": 0
                },
                {
                    "sent": "Opposite diagonal a.",
                    "label": 0
                },
                {
                    "sent": "We want to compute the optimal projection WX&WY in order to maximize the variance on the joint data set.",
                    "label": 0
                },
                {
                    "sent": "And we can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "You know computing the Lagrangian equation gradient to zero and get these again value problem exactly as before and solve this again value problem.",
                    "label": 0
                },
                {
                    "sent": "The good news is that the result is essentially independent.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "End of our data has been split up.",
                    "label": 0
                },
                {
                    "sent": "Another technique could be to maximize the covariance between the projection.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we can solve a decent immigration problem.",
                    "label": 0
                },
                {
                    "sent": "If we fix the scale, we can see that these two optimization problem, up and down are equivalent and this.",
                    "label": 0
                },
                {
                    "sent": "By equivalent we mean that the solution is the same up to a constant factor.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's also equivalent to the following problem that maybe it's more easy to to solve to see how to solve, and we compute the Lagrangian.",
                    "label": 1
                },
                {
                    "sent": "Again, we put the gradient and with respect to WX&WY equal to 0, and what we get is again an order a eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Last second value problem is Canonical correlation analysis.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are maximizing the correlation, assuming that the variance is irrelevant, we will see after what does it mean in terms of result.",
                    "label": 1
                },
                {
                    "sent": "This problem is equivalent.",
                    "label": 1
                },
                {
                    "sent": "Again, fixing the scale to the problem below.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can solve this problem again with the same technique.",
                    "label": 0
                },
                {
                    "sent": "Computing the Lagrangian, putting the gradient equal to 0, and we get another eigenvalue problem.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to summarize.",
                    "label": 0
                },
                {
                    "sent": "I did.",
                    "label": 0
                },
                {
                    "sent": "It is exactly the same procedure for the three problems.",
                    "label": 0
                },
                {
                    "sent": "Different objective function maximize correlation, covariance variance of the joint data set.",
                    "label": 0
                },
                {
                    "sent": "The machine is exactly the same and they changed the.",
                    "label": 0
                },
                {
                    "sent": "What are the value of the metrics A&B?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What happened in practice is that the PCA and partially square look at.",
                    "label": 1
                },
                {
                    "sent": "Look at large variance direction so they ignore a small variance in the data and they are more robust to noise.",
                    "label": 1
                },
                {
                    "sent": "Canonical correlation analysis.",
                    "label": 1
                },
                {
                    "sent": "Assume that the variance of the data is irrelevant, so only looks at correlation in the data, and it's more sensitive to noise.",
                    "label": 1
                },
                {
                    "sent": "That's why it's important in this case to introduce regularization to make CCA robus robust against noise and to introduce regularization.",
                    "label": 1
                },
                {
                    "sent": "As for the other technique that we saw before we try to obtain solution WX&Y of small norm.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the original problem on the top, below regularised problem where we have a tradeoff regulated by the parimeter C between our pattern function and our regularization term.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can solve the regularised problem exactly as before computing the Lagrangian, equating the gradient equal to 0, and again we get another again value problem.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What it's interesting to see here is that indeed and Fisher discriminant analysis and Ridge regression are special cases of regularised Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "How do we see this?",
                    "label": 0
                },
                {
                    "sent": "Let's, for example, substitute the second data set Y by one vector, that in case official discriminant analysis will be the vector of the label price minus one in case of Ridge, regression will be.",
                    "label": 0
                },
                {
                    "sent": "The vector of the labels belonging to the real set of real number.",
                    "label": 0
                },
                {
                    "sent": "We can solve this problem and get the same that the official discriminants solution so Fisher Discriminator is a special case of regularised Canonical correlation analysis and also read progression.",
                    "label": 1
                },
                {
                    "sent": "We can also see that cannot at this point Canonical correlation analysis is a form of multivariate regression.",
                    "label": 1
                },
                {
                    "sent": "So where we have not just one label but we want to regress on a multiple set of label.",
                    "label": 0
                },
                {
                    "sent": "It's a good idea perhaps to use the regularize Canonical Coronation and.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lost things.",
                    "label": 0
                },
                {
                    "sent": "What do we do if we want the identify relationship?",
                    "label": 0
                },
                {
                    "sent": "A common factors between more than two data sets?",
                    "label": 1
                },
                {
                    "sent": "It's we we can develop with some multi way extension of partially square root principal component analysis and Canonical correlation analysis.",
                    "label": 1
                },
                {
                    "sent": "In this case we have a K data set and we want to compute the associated vector W. For example, just for Canonical.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Election analysis this is how can we?",
                    "label": 0
                },
                {
                    "sent": "And this problem solving this resulting session problem.",
                    "label": 0
                },
                {
                    "sent": "But we have again a pattern function and capacity control.",
                    "label": 0
                },
                {
                    "sent": "You can see that the problem up and now are we violent, because if you look at the last expression, the last term is controlled by the regularization function factor and is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So the two problems are equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Then we can see how solving this problem.",
                    "label": 0
                },
                {
                    "sent": "We simply write it and we see that the the solution of the multiway CCA can be obtained again by solving an negative value problem of this.",
                    "label": 0
                },
                {
                    "sent": "Metrics A&B are the metrics in the formula.",
                    "label": 0
                },
                {
                    "sent": "And then this problem and meet regularised.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Went just to conclude.",
                    "label": 0
                },
                {
                    "sent": "And there are many important problems impacting analysis, dimensionality reduction, finding relationship between two sets of data classification, all of them.",
                    "label": 1
                },
                {
                    "sent": "They can be reduced to eigenvalue problems and they can be started by simple linear algebra.",
                    "label": 1
                },
                {
                    "sent": "They can be solved or approximated efficiently with Sander technique.",
                    "label": 0
                },
                {
                    "sent": "But most importantly, all of them involve only the inner product between training data.",
                    "label": 0
                },
                {
                    "sent": "Between the data and so we can easily canalized these approaches.",
                    "label": 0
                },
                {
                    "sent": "We're gonna see this after lunch.",
                    "label": 0
                },
                {
                    "sent": "Want to say just.",
                    "label": 0
                },
                {
                    "sent": "Could have been a bit boring here because I just reported the former.",
                    "label": 0
                },
                {
                    "sent": "I assume everybody is a bit.",
                    "label": 0
                },
                {
                    "sent": "Familiar with these things, but the message is that.",
                    "label": 0
                },
                {
                    "sent": "All of these techniques have a common framework behind.",
                    "label": 0
                },
                {
                    "sent": "Once we learn one of them, we can easily switch from one to another one.",
                    "label": 0
                },
                {
                    "sent": "It requires a bit of practice to see what is the problems related with noise or not.",
                    "label": 0
                },
                {
                    "sent": "Why partial square sometime is better than Canonical correlation analysis, but in general we recommend the use of this technique for many applications.",
                    "label": 0
                },
                {
                    "sent": "I think I'll do this after lunch.",
                    "label": 0
                },
                {
                    "sent": "So what is the robustness of the parameter is important to get it right?",
                    "label": 0
                },
                {
                    "sent": "OK, yes, OK, I OK.",
                    "label": 0
                },
                {
                    "sent": "I went to fasten this point, they see.",
                    "label": 0
                },
                {
                    "sent": "DC is that the control to regularization is like depending on our big our larger or small is C. You can control how much you tend to overfit your data, so it's I don't know you.",
                    "label": 0
                },
                {
                    "sent": "I think you played already with support Vector Machine, maybe this.",
                    "label": 0
                },
                {
                    "sent": "Simply sample there is a super meter there as well.",
                    "label": 0
                },
                {
                    "sent": "You you play with it and control the regularization.",
                    "label": 0
                },
                {
                    "sent": "You just what do you get?",
                    "label": 0
                },
                {
                    "sent": "I know, I mean it's a classical model selection problem of many algorithms, there's no.",
                    "label": 0
                },
                {
                    "sent": "Good answer to this.",
                    "label": 0
                },
                {
                    "sent": "I mean Knights Cross validation if you have support vector machine.",
                    "label": 0
                },
                {
                    "sent": "For example if you have a supervised setting, no, there's no rule of thumb, but to set it.",
                    "label": 0
                },
                {
                    "sent": "Difficult function would see movie theater slides, yeah?",
                    "label": 0
                },
                {
                    "sent": "That's a tradeoff there.",
                    "label": 0
                },
                {
                    "sent": "If there is no seat, then you got this.",
                    "label": 0
                },
                {
                    "sent": "Office politics normalcy.",
                    "label": 0
                },
                {
                    "sent": "When you find your place.",
                    "label": 0
                },
                {
                    "sent": "Reality, besides only theories we make really answer.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is it right for validation.",
                    "label": 0
                },
                {
                    "sent": "After reading glasses, just.",
                    "label": 0
                },
                {
                    "sent": "So here I got the King of his PCA.",
                    "label": 0
                },
                {
                    "sent": "When Y is 1 dimensional, spaces transition.",
                    "label": 0
                },
                {
                    "sent": "No, I want just to to show that the Fisher discriminant analysis and Ridge regression are special cases of regularised Canonical correlation analysis, when the second data set it's one vector.",
                    "label": 1
                },
                {
                    "sent": "11 maker yeah, the second data set.",
                    "label": 0
                },
                {
                    "sent": "We want to mine to pair data.",
                    "label": 0
                },
                {
                    "sent": "Set the second data set is disabled.",
                    "label": 0
                },
                {
                    "sent": "The vector of the labels.",
                    "label": 0
                },
                {
                    "sent": "My experience is, yeah, this is often used when Y is 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Recognition.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "I did it.",
                    "label": 0
                },
                {
                    "sent": "Invite.",
                    "label": 0
                },
                {
                    "sent": "Brenda success he used when the first component, then the signal and it's really strong.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah no no no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "Sea sea.",
                    "label": 0
                },
                {
                    "sent": "Do you constantly that you should extract syrup or pain with no IIII didn't meant that I just want to to to show the similarities of the problem.",
                    "label": 0
                },
                {
                    "sent": "I mean that the Fisher discriminant analysis can be also cost as a negative value problem.",
                    "label": 0
                },
                {
                    "sent": "If you want impractical.",
                    "label": 0
                },
                {
                    "sent": "I mean I think if you use Fisher discriminant analysis you solve the usual problem.",
                    "label": 0
                },
                {
                    "sent": "I mean like.",
                    "label": 0
                },
                {
                    "sent": "Linear system if you want, I mean.",
                    "label": 0
                },
                {
                    "sent": "No, I just want to show the similarities, not that I recommend to solve Fisher discriminant analysis as a negative value problem.",
                    "label": 0
                },
                {
                    "sent": "I mean I have a personal experience with what I will show in the second part of the talk where we developed a kind of extension to Fisher discriminant analysis, official discriminant analysis for structured output learning.",
                    "label": 0
                },
                {
                    "sent": "And there we could not all work on eigenvalue problems.",
                    "label": 0
                },
                {
                    "sent": "We have to find.",
                    "label": 0
                },
                {
                    "sent": "Better way to solve linear systems.",
                    "label": 0
                },
                {
                    "sent": "So now I've is just formulas is not.",
                    "label": 0
                },
                {
                    "sent": "This will be a.",
                    "label": 0
                },
                {
                    "sent": "In the case of the sea, it's not all about just introduction with an important thing afterwards.",
                    "label": 0
                },
                {
                    "sent": "Also, importation of the new plug, right?",
                    "label": 0
                },
                {
                    "sent": "If you visualize, I can hide a mental data set in lower dimensions you need me to say what those axes are high, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that that that's they're not really interpretable, you know, interpretable there is this problem with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "I figure that is in this implication easier.",
                    "label": 0
                },
                {
                    "sent": "They are right because they are just linear combinations of the originals.",
                    "label": 0
                },
                {
                    "sent": "I don't know, I'm not sure to get to what you said.",
                    "label": 0
                },
                {
                    "sent": "I think it's just a second.",
                    "label": 0
                },
                {
                    "sent": "High dimension data set included in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "I want to know what those exit means first in Europe in the input.",
                    "label": 0
                },
                {
                    "sent": "Measurements, that's pretty clear then what it means.",
                    "label": 0
                },
                {
                    "sent": "But just like to dimension, but afterwards you need to assign names to the excess of busy.",
                    "label": 0
                },
                {
                    "sent": "That I'm.",
                    "label": 0
                },
                {
                    "sent": "Thing I mean what I wanted to show you here is that what you get the principal component today.",
                    "label": 0
                },
                {
                    "sent": "Again vector for this.",
                    "label": 0
                },
                {
                    "sent": "For example, for the face that.",
                    "label": 0
                },
                {
                    "sent": "So he's not an interpretation behind them.",
                    "label": 0
                },
                {
                    "sent": "Interpretation is that in the data set to some pattern are more likely.",
                    "label": 0
                },
                {
                    "sent": "Like you know, I don't know the edge of the most I show, I don't know, but.",
                    "label": 0
                },
                {
                    "sent": "Not there are other techniques I would say for that imply dimensionality reduction and ever better interpretation of the data, but I'm not sure to get.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "I mean, I principal component analysis has been the 1st of a very older technique for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "They've been banned so afterwards and some are more likely for visualization purposes I think.",
                    "label": 0
                },
                {
                    "sent": "I I cannot hear you so I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "A Madison Square you haven't heard capacity numbers.",
                    "label": 0
                },
                {
                    "sent": "If you look at his lightning Bolt mean if you consider the cost function as axle make faces.",
                    "label": 0
                },
                {
                    "sent": "Functions, then you can introduce some sort of I distribution or see next.",
                    "label": 0
                },
                {
                    "sent": "I guess I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, I never thought about the probabilistic interpretation of this year.",
                    "label": 0
                },
                {
                    "sent": "Produce.",
                    "label": 0
                },
                {
                    "sent": "Distribution check.",
                    "label": 0
                },
                {
                    "sent": "I'm at 2.1 or something that makes you maximize the option that I I don't.",
                    "label": 0
                },
                {
                    "sent": "I really don't know.",
                    "label": 0
                },
                {
                    "sent": "Have in mind some some work, but I mean I don't think honestly that Canonical correlation analysis as a nice probably think I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "I I, I'm not sure really I. I remember some work of people trying to use Canonical correlation analysis and gets some interpretation of the data, but.",
                    "label": 0
                },
                {
                    "sent": "Not really, I I would love to.",
                    "label": 0
                },
                {
                    "sent": "Just one question.",
                    "label": 0
                },
                {
                    "sent": "Who is the?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "No, it was not.",
                    "label": 0
                },
                {
                    "sent": "We can resort of Mackennal.",
                    "label": 0
                },
                {
                    "sent": "We will discuss later on because all of this technique rely on DOT products between data, But yeah.",
                    "label": 0
                },
                {
                    "sent": "You can ask questions also after the camera bag which may be recorded second, partially covered.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I say thank you again.",
                    "label": 0
                }
            ]
        }
    }
}