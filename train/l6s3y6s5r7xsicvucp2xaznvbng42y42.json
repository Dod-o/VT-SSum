{
    "id": "l6s3y6s5r7xsicvucp2xaznvbng42y42",
    "title": "Modeling real-world networks using Kronecker multiplication",
    "info": {
        "author": [
            "Jure Leskovec, Computer Science Department, Stanford University"
        ],
        "published": "March 20, 2007",
        "recorded": "March 2007",
        "category": [
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/solomon_leskovec_mrw/",
    "segmentation": [
        [
            "I don't know Tori.",
            "Play on it.",
            "Carbon downstairs, Lago yeah?",
            "We we.",
            "Recombination Naprosyn Castle 0 W 20 motor viral part label.",
            "Press tab and tuck model problem you pass low tech technician.",
            "Product key problem in.",
            "But there's a check 2K parazitii Premier.",
            "OK so sale Malaysia answer probably."
        ],
        [
            "Web Core schema Webster any form a hyperlink minibuses, date torch torch capable Java Lacombe Ammonium Hooper Abney can draw Kakashi Mitsuo Asoiaf participate participate.",
            "Patch Torch custodian may so.",
            "Dumb luck, OK?",
            "See more Internet attackers struck him as the root area proposal.",
            "Amid root record album of occasion.",
            "Boss, OK, cool protein mitsuba, so interacting in Karen's kindness days and Imma yeah so action so they lost most effective.",
            "Dollar Store layaway banner ludy.",
            "RIM Cobley Alpha Alpha.",
            "Naravno Asoka Chanel's no sticker, so scoop Nehru symptoms swearing in Carson.",
            "Check cashing so talus mosty allocated.",
            "Quick anchors now model.",
            "Alcatraz.",
            "Stick up Obama.",
            "They.",
            "Super a gram amount as I percocet.",
            "Additionally, so many, she says that she let him break.",
            "Could you share the city flea tick in sociology should be per week or so.",
            "So those are gravel in the next over the same Patch emails IMO.",
            "Couple's argument so they make interaction with me outdoor pozna Koga.",
            "In social products output graph input instances pressball stereo 10 graffle keibu problems.",
            "I want to quarrel slow motion emerges as a major key.",
            "Second console of Connery shall prevail as they took our myeloma to special name.",
            "Kiper middle of the kite Odegaard."
        ],
        [
            "You put the armor Patch more ritual measure on a million to check in.",
            "Marisa tell number picks those man Connor in drug Astoria.",
            "The kitchen Oprah shanya kissable.",
            "Sydney, Australia will say but they got Selena Gomez.",
            "Factor is the central central nautical park.",
            "Isis Goody Vaniki, organization of Mno, tiny sword.",
            "McNamara was Arteaga, NICU, Nicosia, Virte, Organizaci respond lol narrative.",
            "Or become a Patch part million already apart.",
            "Who is the central central?",
            "Which logic will be solely owner?",
            "Uh huh.",
            "So should I switch?",
            "So we have an English speaking person in the audience, so maybe we should.",
            "We should be sure.",
            "Sure, no problem.",
            "Yeah, OK so.",
            "And OK, now.",
            "So these are the questions that.",
            "We used to ask.",
            "We can ask for smaller networks.",
            "What is for example more more interesting to ask for bigger networks is like how many nodes do I need to remove from the network so that the network like can be separated into two pieces or something like that, right?",
            "So this would be like a different type of questions that starts to make make sense when you have a much larger network, right?",
            "And what we want to do now is basically we want to develop some kind of statistical tools and statistical method to study these big networks right?",
            "And the whole problem comes from the fact that.",
            "We cannot prove that we cannot draw this."
        ],
        [
            "Sphinx right and what there are basically 3232 three goals we have right?",
            "First is can we can we find the analyzed quantified this statistical properties of large networks can be modeled them.",
            "Can we like?",
            "Can we find simple generative models that will be able to generate this type of properties and then given the models then the next step is can we predict what will happen and things like that right?",
            "And so here's the outline of the rest of the talk."
        ],
        [
            "So I will spend some time now in today."
        ],
        [
            "Using these properties of real world networks, then I want to talk about Chronicle graphs, show some properties of them, and then present how to feed them real graphs and present experimental results.",
            "OK so."
        ],
        [
            "For example, here is a list of.",
            "Properties that can be found in.",
            "Real world networks, right?",
            "So for example, first one is like small small world effect.",
            "They call it so it means that the diameter of the network is very small even if the network itself is very large.",
            "I'll explain about this more later.",
            "The other the other properties, like they call it transitivity or clustering, which in social networks means that if I have two friends there, probably the friends themselves also writes, and this will like create a triangle, right?",
            "I have a friend here.",
            "I have a friend here and then.",
            "They're also friends, so this this is called transitivity or clustering.",
            "Then we have this skewed degree distributions.",
            "Like then the expression that is used is called scale free networks then then we have other.",
            "I'll explain later.",
            "OK, so this is just just glance and then also some temporal patterns.",
            "So all I want to say here."
        ],
        [
            "There was this, which is actually quite fascinating.",
            "There was this experiment by Milgram in 60s where he asked people in Nebraska to do the following right.",
            "So he asked these people to send letters to some stockbrokers in Boston, right?",
            "And all these people knew was the name of the person and that they were stockbroker, right?",
            "And they were asked to send to send these letters to their like immediate friends and what he was measuring is how many hops does it take for the letter to arrive to the particular stock broker.",
            "Right and it turned out that like 25% of the letters that were initially sent reached the goal, but they but the important part is that they reached it in about 6 steps, right?",
            "So it means that.",
            "A random person in Nebraska can send the letter to random person in Boston in about 6 hops, which has two implications for the distances in the networks are small and the other one is that the humans are able to find them OK. And what do we mean by distances in the networks?",
            "So this is usually captured by the diameter and people, so diameter is defined as like maximum.",
            "So if DJ is the shortest path, the length of the shortest path between nodes I&J, then diameter is just the maximum over them.",
            "Then, since this is like very noisy measure right you what people usually do is they call.",
            "They talk about effective diameter, which is like just the 19th percentile of the diameter.",
            "Or you can like calculate like average shortest path distance, right?",
            "So I just go here over all pairs of shortest paths and I normalize appropriately, right?",
            "Or the other one?",
            "OK, I won't go into details and for example here is."
        ],
        [
            "Here's what you get right.",
            "So here we have a Microsoft Messenger network, right?",
            "So we have 180 million people talking to each other over a period of a month, and we have 1.3 billion edges, right?",
            "So we have pretty much the whole world, and there is an edge.",
            "If two people exchange.",
            "If two people were engaged in a conversation at least once in this one month period, right?",
            "And if I hear plot, the distance and the number of number of pairs of nodes at that distance, you can see that the mode of distribution is at 7.",
            "But this means that most of the network is reachable within seven hops, right?",
            "And this is log scale, right?",
            "So if this would be like 100 million?",
            "Oh sorry, yeah.",
            "Then note here is 10,000,000, right?",
            "So this is like almost 100 million people up there, right?",
            "So more than half of the network can be reached in.",
            "In at least seven hops, right?",
            "And this says that they are not very small.",
            "So the other property."
        ],
        [
            "This degree distributions that people talk a lot about and this is basically so.",
            "This is what we mean, right?",
            "So we take PK to denote like a fraction of nodes that have degree K right?",
            "And then we can just plot the histogram of probability of observing with the degree K, right and?",
            "What would like what one would natively expect this to see something like this right?",
            "If we would expect that there will be some mean to this distribution, meaning that we would say on average a person has ten friends and then some have a bit less and some have a bit more.",
            "But there is a well like there is an average or there is a mode to the distribution.",
            "What happens is that it's more like that, right?",
            "So it's skewed.",
            "It skewed towards towards towards the right.",
            "OK and they call these types of distributions like heavy or long tail distributions and.",
            "There are many quantities in real world that follow these types of distributions, right?",
            "For example, the Amazon sales or the word length distribution follows this OK. And to be even more precise."
        ],
        [
            "So we said it's it's skewed, but what is even more important?",
            "That is that the tail does not decay exponentially.",
            "But in the case of the power rule.",
            "So what I mean is that right probability of observing degree of a node with degree case scales like K to the minus Alpha, where Alpha is some.",
            "Constant right?",
            "And so if I plot, if I put a lock here in the lock here the Alpha like goes down right?",
            "And if I plot the whole thing so I have log K versus LOC PK right?",
            "I will get a line OK and justice next one.",
            "So here I plot the same."
        ],
        [
            "I think, but I plot it on different scales, right?",
            "So here I have linear linear here I have log linear and here I have log log.",
            "OK, so if the whole thing would be exponential then here I should get a line right?",
            "So if there would be like exponential decay in detail I should get a line here since I'm.",
            "And here so if I if I get a line in the log log then I have a power, right?",
            "So it's a very it's in.",
            "Sometimes it's very easy to test whether you have to distinguish between exponential and the power law, right?",
            "And usually instead of instead of working with the.",
            "PDF we work with the CDF, right?",
            "So we the cumulative version of the distribution cause.",
            "The variance right?",
            "The variance in the tail is so high that the like.",
            "Measurements are very noisy OK, but the important thing that is that this degree distributions in graphs are like this, which means that most of the people have just like one friend or two friends, and then there is a few people that have many friends and the other thing is there's a few of these people, but they have very high number of friends and there's still quite a lot of them in some sense.",
            "OK, and for four more into it."
        ],
        [
            "Here is an example right?",
            "So this would be this is an example of a network where that has degree distribution like this, right?",
            "There is like a Bell shaped degree distribution with like exponential decay in the details right?",
            "And this is like a power law or scale free type of network, right?",
            "Where I have a degree distribution that is skewed and that it follows a power law.",
            "And what is the immediate difference you see is that you have like hubs or nodes with very high degree and you have a lot of just like nodes that have just.",
            "A slight degree one.",
            "OK, so this is so real networks are more like this and they're not like that.",
            "OK generated randomly.",
            "Yeah, this is this is called airless rainy random graph.",
            "Which means you have nodes and now you pick two nodes uniformly at random and create create an edge between them right?",
            "And everything is iid.",
            "So this is just pick to connect them pick next to connect while here for example.",
            "The way how you generate this one is that you say probability of picking a node is proportional to the number of edges it already has.",
            "So at 1st at first.",
            "Which will mean that if a node they call this like rich get richer and is the other name they call, they call this or they call it preferential attachment, which means that you will if a node has already a lot of links, it will acquire more if you acquire more faster than somebody that has only one link.",
            "OK, and this mechanism like push the tail of the distribution further and make it fatter.",
            "Graph on the right.",
            "Is it also called small world?",
            "No, that's different.",
            "So what small world does is so small world, so usually right people would go and they would discover something like this and they would say oh what is a simple generative process that gets me something like that, OK and?",
            "For example, a simple generative process for scale free networks.",
            "Is this what I just described called preferential attachment, right?",
            "You pick a node proportional to the degree, which means I know on the other intuition is if you have a web page and you want to create the link to someone you are more likely to create a link to Google then to some other random insignificant webpage for what is a small world model is that.",
            "They have actually I can.",
            "I have slides for this.",
            "Um?",
            "Just a second.",
            "So basically what is the small world model is they wanted they wanted to model the.",
            "Transitivity of network.",
            "So basically they wanted to have a model where you can interpolate between completely random networks and completely like regular networks like lattices or grids, right and?",
            "Oops.",
            "So.",
            "This is this is an example of a small world model right?",
            "You start with some regular network, this you can like, right?",
            "It's a it's a circle where every node has what two edges?",
            "Right now what they do is you can randomly rewire the network, which means just you pick two random nodes two like you pick four random nodes and you like change the edges.",
            "Or you this is one option.",
            "The other option is that you just introduce more random images.",
            "And this is what you can get, right?",
            "And as you as you as you as you keep doing this, you will get from a like a regular network to a completely random type of network.",
            "And what is the interesting is that this is what's going on right?",
            "So here is the rewiring probability.",
            "So probability that I will start with my regular network, let's say like a grid and then I start messing with it, right?",
            "I start picking up nodes or and like swapping edges and things like that, right?",
            "And this is the probability or the proportion of?",
            "I just I swapped or the proportion of random and random edges are introduced and what happens is that.",
            "So this is the diameter.",
            "Another as I do as I introduce more randomness, the diameter will get smaller, right?",
            "'cause the diameter of the grid, right?",
            "If I increase the size, if I increase the size of the grid, what the diameter will increase if I increase the size of the grid by a square, the diameter will grow linearly, right?",
            "So diameter will grow really quickly.",
            "If I if I what is going on here is that I am getting smaller and smaller diameter.",
            "The other thing is they called it.",
            "In coefficient, which means which is basically a proportion of triangles in the network.",
            "And again by doing by introducing this random edges the I'm basically destroying the triangles that are already there in the network.",
            "So this is what they call a small world.",
            "Producing new links or this is the depends on the on the exact definition, but it's the same right?",
            "So for example, here is you can add or remove edges.",
            "Basically you want to create random shortcuts and now.",
            "Either rewiring or introducing new links would decrease the diameter if you.",
            "If you're doing this uniformly.",
            "If you do, if you are selecting this uniformly at random right?",
            "Yeah, then both processes are pretty much the same for the second one is introducing new links would certainly increase.",
            "The first one is less intuitive.",
            "Right, but if you have like short short short short links and now you will on the average right, you'll pick nodes that are like at half distance, so you will definitely bring the diameter down OK. Small World graphs model many real world networks and like sociological technological system.",
            "So is this either not true anymore or so?",
            "Better model or framework network.",
            "So what?",
            "For example what is not good with small world model?",
            "Is is this right?",
            "So here I have degree distribution right?",
            "And I was just showing you that it's a power law, right?",
            "And this is not a power law, right?",
            "Right, So what is so?",
            "The thing is, people usually would just like go for they would discover something and then they would try to find the model that would explain it, right?",
            "So people found degree, power or degrees solutions and they said OK, preferential attachment.",
            "Then they found clustering right?",
            "This triangles and they were saying oh how can we model that?",
            "Oh here's the small world model right?",
            "But small world model won't get you won't get to power low degree distribution.",
            "So if you want one you have one model.",
            "If you want the other you have the other model.",
            "OK, and this is.",
            "And now if you would care about some third property, there's a third model.",
            "That will get you that property, but there is there was very little work that would say.",
            "What if I want all of them?",
            "Or can I find the generator that probably can do all of them and this is what I'll show.",
            "Triangles because we started social network analysis and meet actually does show that based on the analysis of triangles you can say whether some networks are.",
            "I don't know more or less connected or something like that.",
            "So So what is the question?",
            "The question is what is the point about mentioning triangles right now?",
            "Because we're talking about small world model.",
            "Otherwise there is no.",
            "The clustering coefficient is the triangle.",
            "Write.",
            "The clustering coefficient is calculated, so here's here.",
            "Should be the formula.",
            "No, it's not OK.",
            "The clustering coefficient is.",
            "There is the right.",
            "You have your clustering coefficient of a vertex with degree K, right?",
            "Should be one over.",
            "All right, if you work this case degree K then you want to ask how many possible triangles are there which means.",
            "This is the number of possible triangles right here.",
            "I call I say, number of triangles.",
            "OK, So what this says if I have a vertex.",
            "Like this, right?",
            "And I know something like this.",
            "Then I say how many possible triangles are there, right?",
            "It's K choose two.",
            "OK, so I need to select two vertices.",
            "This is the total total number of triangles.",
            "So one triangle could be here.",
            "The other could be here, here, here, here and so on OK. And now I say OK, there are two, two out of whatever.",
            "Five choose two.",
            "OK, so this is clustering coefficient.",
            "And then what you can do?",
            "Actually I should say I should say this way, right?",
            "So this is for a particular vertex.",
            "Now CK is just the average over all vertices where of that particular degree and this is now you can plug this and you call this distribution of the clustering coefficient and then if you want a single single single see, then you can again every jewel or vertices, right?",
            "So there are this is this is what I mean.",
            "If you look from the perspective of a single node, it's only important that he's connected, but when you are looking at the clusters of, obviously you're looking at this one whether it's connected as well, so therefore you get the triangles.",
            "Is that so?",
            "I don't understand.",
            "I mean all this says right?",
            "Which makes sense is if that's me and I have two friends, then there is a higher probability of the two people knowing each other right?",
            "And this is something that's definitely true.",
            "Or for social networks, OK, but let's go back.",
            "Tick tick.",
            "OK, the other thing for which I."
        ],
        [
            "No.",
            "Want to go into too much detail is that there are also you can study spectral properties of graphs, which means that you can take your graph adjacency matrix right where where you have one.",
            "If there is an edge and you have zero if there's no edge right?",
            "And now you can do if the graph is undirected.",
            "Then you can do like principal component analysis.",
            "Or you can do SVD so that you get.",
            "Basically you get Eigen values and eigen vectors and if you plot the distribution of this you also find that it's skewed right and.",
            "One intuition is that the they call it a spectral gap and a spectral gap measures the difference between first and second eigenvalue and then tells.",
            "That tells you how, how, how nicely the graph would cluster, which means how nice communities are there in the cluster.",
            "OK, but that's.",
            "Let's go on right."
        ],
        [
            "So for Temple temple.",
            "So this this was all for static graphs for temporal temporal patterns there are two right.",
            "One is very basic question.",
            "Basically want to ask what is the relation between the number of."
        ],
        [
            "And the number of edges, overtime, right and right.",
            "So if you would say N is the number of nodes at time, T is the number of edges of time T. If I double the number of nodes, will the number of edges also double?",
            "OK, and if it turns out it won't right so it will more than double and how you can reason about this is if you plot it on log.",
            "So here's the number of nodes at time T number of edges at time T and you plot both on log log scales and you will see it follows pretty much align right.",
            "And if the relation would be linear then this line should have slope one.",
            "If the graph is fully connected, or if it's a click, or every node has like a links to the the constant.",
            "Constant part of the network.",
            "Then this should be 2, right?",
            "So the.",
            "The exponent this a."
        ],
        [
            "The slope should be between one and two right?",
            "One means you have the linear growth of the number of edges right here.",
            "Or you can have quadratic growth of the number of failures.",
            "OK, so this is what has been found for graphs that evolve overtime, so they're basically getting denser OK. And, um, so this is what I also explained, right?",
            "If you have, if you have a equals one, then you have linear growth, which means the average degree in the graph is constant.",
            "And if you have a = 2 then then you have quadratic growth, which means that.",
            "You're getting very dense graph, right?",
            "Because everyone is like connected to the constant fraction of the of the network.",
            "Um?"
        ],
        [
            "And this I will skip.",
            "And."
        ],
        [
            "The other thing that is also surprising for evolving networks is if you ask how do the distances increase as my network gets larger, right and?",
            "I know conventional wisdom or results on the existing models tell you this right.",
            "They would say the distance is slowly increased like log in or log log N and things like that, right?",
            "So if as the as the end the number of nodes gets larger, the diameter will slowly increase.",
            "This is what happens if you measure these things overtime, right?",
            "So here for example, this is a citation network right from 1992 to 2003 and diameter decreases.",
            "Right now the question is, so I showed you all this.",
            "Things.",
            "And so, before this is just just."
        ],
        [
            "Short list of where of types of networks where these patterns or these properties can be found.",
            "So what live web you have?",
            "Web pages in hyperlinks, online communities would be.",
            "I know friends there and things like that.",
            "Who calls whom is like from AT&T.",
            "You have phone numbers and who calls whom autonomous systems are are.",
            "You can think of them.",
            "You can think of them as routers or like ibm.com is an autonomous systems and AT&T is an autonomous system.",
            "So now this autonomous systems like like route packages between each other.",
            "You can do the actual routers so this is the second one.",
            "You could like Luke bipartite graphs like movie star actors.",
            "You can look at citation networks.",
            "You can look at the Co authorship networks.",
            "You can look at sexual networks where an edge means there was some sexual interaction or something.",
            "And you can look at also clickstream data.",
            "OK, so."
        ],
        [
            "Text.",
            "How do you create a network out of text?",
            "Movies.",
            "Yes, you can always make it as words which are adjacent people also do that.",
            "Yeah.",
            "Could answer I think so, but it's just."
        ],
        [
            "Like I mean then you can just like you can create a network out of everything and then it's like.",
            "I don't know.",
            "I mean, we picked up just the famous guys and.",
            "So that I could be in a good company or something.",
            "OK so right?",
            "And there is a lot of also graph models that I also write.",
            "I described what is a random graph I described.",
            "What is preferential attachment?",
            "This was developed for power law, degree distributions.",
            "Copying model is something that gives you proper power law degree distributions but also gives you communities.",
            "We had also small world model I described and so on right?",
            "And for example, this one won't give you.",
            "We generated graphs, the densify, but the diameter will be increasing and again you just got one property but not the others and so on right?",
            "So this is usually the problem.",
            "And here's here's one solution, right so?"
        ],
        [
            "You want the model that for which you can prove and then fit it, and it does all this.",
            "The thing here is that this model won't have like a very nice generative semantics.",
            "OK, so it won't be like something that you could say is happening in the nature in some sense.",
            "So let me define this Chronicle model, Chronicle graphs and what we will basically do.",
            "We will.",
            "We will try to generate our graph."
        ],
        [
            "Recursively by yeah, let's say we will do it recursively, right?",
            "So we'll start with some initial graph and then somehow recursively expand nodes of this graph to get larger and larger versions of the graph, right?",
            "And here's an example what we're doing so basic."
        ],
        [
            "We're doing tensor product of graph adjacency matrices, right?",
            "This is the graph adjacency matrix of the following graph.",
            "I go and I expand each node here with a miniature copy of the graph itself, and then I'm introducing some additional edges and this is exactly the process.",
            "If I take this matrix and tensor, multiply it with itself, which means that I will take the constant here and and multiply it with the whole matrix.",
            "So here I get G1 and here I'll get 0 * G One which is zero.",
            "OK, and I'm just doing this product is basically multiplying two graphs exactly.",
            "Nothing else, just this operation.",
            "Would this be?",
            "This is probably not.",
            "Typical metrics multiplication somewhere.",
            "Can you express it to you analytically in some way?",
            "The this is how it's, I mean.",
            "This mathematicians have defined this along time ago.",
            "I mean, this is a chronic product, right?",
            "Usually you have two types of products between graphs.",
            "One is this tensor product or current product, the other the other one is Cartesian product, which doesn't really.",
            "OK, so right.",
            "So here I I called this G1, then I then I multiplied with itself.",
            "I get G2 if I would now multiply G2 with itself.",
            "This is what I would get."
        ],
        [
            "So I will be getting some kind of nice.",
            "I know self similar structures if you like.",
            "And."
        ],
        [
            "And so right, we will start with G1N1 and Y1.",
            "Edges will call this initiator and then we'll get we'll get.",
            "Sequence of growing graphs.",
            "Each one will have N1 to the K nodes right?",
            "'cause the size of the Matrix certificate here is 3."
        ],
        [
            "This is No 9 right cause just how the product works."
        ],
        [
            "And then for.",
            "So this is the model.",
            "And here is here is more precisely what the tensor product."
        ],
        [
            "Chronic product of two matrices is right if I have A&BI just take the elements of A and like sticking the whole B matrix OK and here are right.",
            "If these are the sizes of AMB, this is the size of the of the product, so the matrix is growing.",
            "I."
        ],
        [
            "This right so will get case Chronicle power by just multiplying G1 with itself.",
            "Um?",
            "So the intuition."
        ],
        [
            "You can have is that basically we're having this recursive growth of communities where every where right.",
            "This is like our building block and now every node in the network gets like expanded with this building block right?",
            "And this continuous continuous.",
            "Um, the other thing is what I showed you here.",
            "This is."
        ],
        [
            "Deterministic, right?",
            "So I have this matrix here, right?"
        ],
        [
            "It is just zeros and ones, so it's completely deterministic process.",
            "How I can randomize it?"
        ],
        [
            "Is here so this is the idea how to get now a stochastic version or how to get random version of the graph and the idea is that instead of starting with 01 matrix I will start with some probability matrix where here now every cell is a probability, so it's between zero and one and the cells don't need to sum to one right?",
            "So it's just every number is an arbitrary number between zero and one.",
            "I can now Chronicle multiply this with itself and get some probabilities right?",
            "So for example here I will give get .5 squared.",
            "Here I'll get.",
            "Five times .2 and so on.",
            "Right now I can.",
            "I can go for every cell.",
            "I can flip a coin and this will generate meograph OK and with higher probability.",
            "This is like the edge that will happen with the highest probability and so on, OK?"
        ],
        [
            "So this was quickly on Chronicle graphs and now I want to show I want to skip this, but what you can do you can prove."
        ],
        [
            "That this process will have will be able to generate or there exist parameters for which all this is true, right?",
            "So for this model you can show that the following holds, and so now I'll skip like 10 slides of these proofs.",
            "And I will go into how to feed them, right?",
            "So now the question.",
            "So now the question is we."
        ],
        [
            "So the Chronicle graphs can basically have nice expressive power, right?",
            "They can generate graphs that have all these properties.",
            "The question is, how do I figure out that initial little graph, right?",
            "How do I figure out the one?",
            "So given the real network, I would like to find out G1 so that I can then do do various things.",
            "So what can I?"
        ],
        [
            "Right, I would like to know if I know the parameters how the graph are generated.",
            "Maybe the parameters itself will reveal something about the structure.",
            "What I can also use this for is I can use it for extrapolation, meaning given a graph today I can fit it and then I can.",
            "I know generate 10 times larger graph and hope that this will be similar to what will happen in 10 years or next year.",
            "I can use this for sampling again I can fit and then generate a smaller graph.",
            "Or I can use this for anonymization which means?",
            "I have an email network I don't want to share it, so maybe we should feed the model and then generate a new network using this model and the parameters we learn and then share this with other people and hope that like large scale properties will be the same right?",
            "And since we're not really caring about a particular node, this should work.",
            "OK, and how we do this, right?",
            "So now I'll change the note."
        ],
        [
            "Right, so Theta is now a parameter is I call it a parameter matrix, but this is basically G1 in a probabilistic sense.",
            "OK, so I want to find this little matrix and how do I do this right by maximum likelihood which means I have a true graph and now I would like to find most likely theater that generates the graph, which is my data, right?",
            "And then the whole problem is basically how do I calculate this and then I maximize over Theta and what we will do?",
            "We'll be using gradient descent right?",
            "So instead of re calculating this, I will calculate the gradient.",
            "Of this with respect to Theta or with respect to the elements of Theta.",
            "Yeah, I mean it's it's one thing you need to decide is the size, right?",
            "Whether it's two by two or three by three and then we need to decide on this on these numbers, that's it, I mean.",
            "I'm I yeah, I don't know if so here."
        ],
        [
            "Here is more write more details, so I'll get this real graph which is here.",
            "I have its adjacency matrix.",
            "I have some Theta.",
            "Let's say somebody told it or here is 1 I'll get.",
            "I'll do the Chronicle powering right and now I want to calculate this and it's very easy, right?",
            "So probability of a graph being generated by this set of parameters is.",
            "Basically I go over all edges and then multiply probabilities of edges appearing and then for all edges that are missing in the graph I take 1 minus the probability of an edge, right?",
            "So what this will do is?",
            "I'll take since there is an edge.",
            "I'll take this one and multiply it and then I'll.",
            "This one and I know since here is not an edge this this probability will go into this part, right?",
            "It's like 1 minus OK is this?",
            "Is this about clear?",
            "So basically I'm taking I'm taking whatever is generated by my set of parameters.",
            "I take the graph adjacency matrix, somehow overlap them right?",
            "And whenever there are once I multiply them here and wherever there are zeros I multiply 1 minus that probability OK. Yeah yeah, OK, so this is.",
            "This is the basic so this is all we're doing right?",
            "It's very simple, so somebody tells me the parameters I get this.",
            "I have the graph and now I'm just multiplying these probabilities of individual edges.",
            "If edge occurs, I think the probability of an edge.",
            "If there is no edge, I take 1 minus, right?",
            "The matrix.",
            "Exactly so I know a spot or value here.",
            "It's a probability of an edge from node.",
            "What four to node two?",
            "OK, so here's two.",
            "Here's 4, so this is the probability of an edge of a node #4 linking to node number 2.",
            "Exactly right, so I'm doing a maximum likelihood type of fitting.",
            "So I'm asking how should I select the set of parameters to maximize this probability so to maximize the probability that the parameters generated me the graph I observe right?",
            "And the way I'm doing this is that basically now every cell is like Bernoulli trial, right?",
            "To flip a coin and now I say these are the flips of the coins that succeeded and these are the flips of the coins that did not succeed.",
            "So these are the edges I see and these are the edges.",
            "I didn't, I don't see.",
            "OK, so this is what now we.",
            "Basically we want to do this and that."
        ],
        [
            "2 problems with this.",
            "First one is the node labeling right?",
            "So if I have two graphs right, they are the same.",
            "I just label the nodes differently, right?",
            "So here I get.",
            "This is no number 1234 or I do it this way?",
            "OK, I will get two different adjacency matrices.",
            "OK, and now if I would just go and overlap them.",
            "This would be different in general, right?",
            "So this is one problem, so I don't know.",
            "I don't know the node labeling right.",
            "I don't know how to map nodes of this graph to the node or how to make this adjacency matrix to this adjacency matrix, right?",
            "So what I need to do in principle, I need to consider all all node labeling Sigma.",
            "OK, so Sigma is just the permutation basically tells me how to label how to label these nodes so that I can then take this matrix, they take that one and calculate that equation I showed before OK.",
            "So the problem here is, So what I would really like to do is I would like to average over over all labelings and.",
            "I mean the number of permutations is like N factorial, right?",
            "So this will explode for more than a graph of Idol 10 nodes if you like.",
            "And the other assumption I'm making is that all labelings are apriori, have the same probability, right?",
            "Labeling is just something that comes at the end when the graph is created and what labeling tells me is how to map the rows and columns of here to here right?",
            "So in principle?",
            "This is very bad news.",
            "OK the other."
        ],
        [
            "See now that, right?",
            "So what I'm really calculating is this right probability of a graph given the parameters right?",
            "This should be Theta.",
            "Sorry and this is some permutation somebody told me how to do this mapping, and right now I'm again going over edges, but I'm I know flipping them around so this means I'm taking the youth.",
            "The element at position you of this permutation, this vector, this mapping and this is what I'm doing right and?",
            "The one problem with this equation is try this again, takes an time order N squared right because I have to go into reverse every cell of this of this matrix, right?",
            "So I have to go.",
            "So this is this is the adjacency matrix of a graph right?",
            "And this is adjacency matrix of a graph.",
            "So what I need to do is now traverse every cell of this matrix, which means if I have a graph of final size 100,000, this will never fit into memory or the complexity of justice evaluating evaluating the probability for a single Theta in the single.",
            "Permutation will take order N squared right, which will never.",
            "Will be never able to calculate that and.",
            "So I have solution."
        ],
        [
            "For both OK, this is good news.",
            "So basically the bad news is that if you naively calculate this then you have to average over all permutations and you have to evaluate every every cell of your graph adjacency matrix.",
            "So we get the complexity would be like N factorial and and worse right.",
            "And we can do this in linear time.",
            "So in time order E right time proportional to the number of edges and there are two 2 tricks will be using right?",
            "We won't, we won't consider.",
            "All labelings but we will.",
            "We will use some like statistical simulation techniques like MCMC to sample permutations from this distribution, right?",
            "So now I now I fixed the graph and I fixed the parameters and I want to send sample.",
            "Distribution of the permutations of this.",
            "So how you can think about this is that right?",
            "Once I fix the graph and so let this be like the space of all permutations, right?",
            "So here you can like think of them like.",
            "I know being in some some lexicographical order or something right?",
            "So here is a permutation that is like 1234 and here is like North minus one and so on right?",
            "And there will be.",
            "The distribution will be like this, right?",
            "So there will be some permutations that are good that are true, right?",
            "The ones that are really good and there will be a lot of bad permutations.",
            "So what we want to do is we want to sample permutations from this distribution.",
            "And what we hope is that if when will take a lot of samples.",
            "It won't.",
            "We won't have to consider everyone, but just sample many times and this will work right?",
            "And the other trick that will be using is since real graphs are sparse.",
            "What I mean by that is that right?",
            "So I have is the number of edges and is the number of nodes in worse.",
            "In worst case it would equal go like North squared, right?",
            "So is the number of cells right?",
            "And this grows quadratically then which is the size of the matrix?",
            "But real graphs are sparse, which means that majority of elements in the in the matrix will be 0.",
            "So what I can do is I can quickly calculate probability that the graph is empty, right?",
            "That given the parameters I did not observe, not even a single edge, and now I only go and correct this for the edges that I actually observed, right?",
            "So this is the trick will be using and Now this this will now.",
            "So calculating this and using some approximation will take constant time.",
            "So I will only go and have to correct this probability for the edges that were really there OK, and now I'll go into a bit more."
        ],
        [
            "So.",
            "Um?",
            "What, So what we're really doing, but will be will be doing gradient decent, right?",
            "So I want to have a derivative of the log likelihood with respect to my parameters, right?",
            "So there should be some kind of matrix of derivatives, right for every parameter, so for every element of my little initial matrix, I need to calculate this.",
            "If you are a bit clever, you get something.",
            "You get an expression like that, right?",
            "So I'm again averaging over all the permutations, but it says.",
            "Here's the probability of a permutation, and then I take once I have that.",
            "I take I take the the derivative of the likelihoods, but what is convenient now here is that I can sample this.",
            "I can then evaluate this and just output output the average OK and.",
            "So here."
        ],
        [
            "Here is what we are doing right, so we want to.",
            "We want to sample permutations from this distribution and all we are doing is we start with something with some permutation, right?",
            "Let's say just want to end and then we pick two spots in this permutation we swap the elements OK and Now what I'm doing is I'm just evaluating the ratio right?",
            "So this says if the new.",
            "If the new one.",
            "So this is the new the new permutation I got.",
            "If the new permutation has higher likelihood than the old one, I will always accept this permutation, otherwise I will accept it with this probability.",
            "OK, this is you can think of this as some kind of simulated annealing right?",
            "I tried, I have my current permutation.",
            "I swap two elements.",
            "If this is if this gives me a better permutation, or more likely permutation.",
            "I keep it.",
            "If it doesn't, then with some with some probability right?",
            "I will still keep it.",
            "So this is the intuition of the algorithm.",
            "Right, so I'm just doing this swapping if gets me if it gets me to better permutations, right to more likely permutations.",
            "I keep that if it gets me towards permutations I will keep it with some probability right?",
            "With this you that sampled again from uniform distribution of this shape.",
            "Or this is just no I'm.",
            "Just this is just the intuition.",
            "OK, so the idea is if the if the random change gets me to more likely to better part of the space, I will keep that.",
            "Otherwise I just keep with some probability spaces.",
            "So I will show OK.",
            "So this is, this is what we're doing.",
            "The other thing that's very nice here is right now we have local changes in my distribution, right?",
            "I'm just sorry, my permutation.",
            "I'm just swapping two elements, right?",
            "So if this is my.",
            "The adjacency matrix right that says what I'm basically doing.",
            "I'm swapping the rows and columns of this adjacency matrix, right?",
            "So if I swap elements J&K, this means I'm swapping Jason case row and column right?",
            "So I'm just swapping these two rows and columns OK, but what this means is that the large portion of the adjacency matrix remains constant, right?",
            "So when I will when I will be calculating this ratio, this parts that will cancel out right?",
            "So I don't really need to.",
            "Given the permutation, I don't really need to go and evaluate everything, I just need to evaluate these two rows and columns.",
            "Does it make sense?",
            "Stop what?",
            "What one does let's say within PHP or these kind of problems.",
            "Classical local optimizations now is just noise, but you can lock it changes influence just.",
            "Luckily I'm here.",
            "Sure, sure, I mean there are many ways how you could do this, right?",
            "Nobody says you want to do local local changes on your on your on your.",
            "Permutation 'cause this will be.",
            "This will be bad in a sense that it will take long time to get to get a completely random permutation right.",
            "For example, a better strategy would be to pick pick some element element at random spot and insert it at some other random spot right?",
            "And this would then shift all the other elements by one and this is much better shuffling strategy.",
            "The problem with that stuffing strategies that it's a better strategy so it means it will be more efficient or I'll need less less.",
            "Less samples to converge or to get that sample from the distribution, but the problem is that it's yeah it's not local, so everything will change, so I'll have to evaluate much more.",
            "I think.",
            "I mean it's not clear it's not entirely clear why this would be obvious.",
            "I mean, why is it obvious that this?",
            "Here the elements will cancel?",
            "This other shopping strategy better, I mean, because if I if I now swap this so it means I'm taking this one and inserting it here, then the whole part here moves one to the right.",
            "So it means all.",
            "Compared to this one, it's better for getting it's a better shuffling strategy, right?",
            "Because it will, sorry.",
            "I have a the.",
            "Big cause if you start you can show that I know if so now I'm just talking by memory, but still I think if you start with the order permutation and you ask how long, how many shuffles do I need to get to get to a random permutation?",
            "For this thing you I think you need like order N squared swaps, while if you take a random one and insert it you will need like order and log in or something.",
            "So it's much better in this sense, so we need less samples to get to sample the whole space.",
            "The problem is that you won't have this locality, so we'll have to at every at every iteration.",
            "It will cost you more to evaluate this OK, but.",
            "Yeah.",
            "Um so."
        ],
        [
            "So this was the idea how to get how to sample, how to sample these permutations from the distribution using this local moves.",
            "This works quickly and the other one is this observation that makes everything even faster.",
            "OK, and now the other question is so now I have this.",
            "So I have the parameters I have the I have the permutation.",
            "How do I?",
            "How do I get this?",
            "How do I calculate the probability right and before we were so if you would do it maybe you would have to evaluate every cell.",
            "Of this adjacency matrix right, which takes order N squared.",
            "But what you can notice is that every element here will have this this form.",
            "So if I have my capital state at the parameter matrix, which is, let's say two by two, then I have four parameters.",
            "So this is 4, sorry.",
            "Then the probability here will be like a power, so it will be first parameter to some power second parameter to some power and so on, and the sum of these parameters and this parameter.",
            "This ABCD should sum to some constant like let's say K, right?",
            "So here this one will be Theta, one to the K&NAB&C&D will be 0 and so on right?",
            "So basically I'm having.",
            "If you look at this carefully, you get to you get a multinomial series so you can easily summed it up.",
            "The problem is that if you want to calculate the likelihood of an empty graph, which means none of the edges are there, right?",
            "So we want to calculate this type of expression, which is ugly, but if you if you approximate this one, we it's Taylor approximation.",
            "So log 1 -- X -- X -- 1/2 X squared, right?",
            "So this is a second 2nd order Taylor approximation to this.",
            "Then because you have such a nice structure of your probability matrix.",
            "You can you can like analytically solve this right?",
            "So these are just the so this is now a sum over 4 elements, in our case right?",
            "So I can easily get the likelihood of an empty graph.",
            "So now the likelihood of the graph I get is the likelihood of an empty graph.",
            "Now I only need to go over the edges.",
            "So this is this is I sub basically now I'm now for the edges that I observed.",
            "I remove the likelihood of not observing an edge and add the likelihood for observing an edge.",
            "So it's very simple.",
            "And this again saves me a lot of a lot of time 'cause I don't need to go for every cell, but only for cells that have one.",
            "And this is basically can be calculated like trivially, right?",
            "This is like a sum over 4 elements and some powering, and I'm done.",
            "OK, so Mark was so now the question is."
        ],
        [
            "So I did all this magic.",
            "I used the approximations and everything.",
            "The question is, does it converge?",
            "So if I start with random point, do my gradient descent thing can I can I recover the two parameters?",
            "So first experiment we did was basically the question is right, can I recover the true parameters?",
            "Which again means how nice?",
            "How smooth is my space or how much local minimize that and so on, right?",
            "And this is the experiment we did.",
            "So you pick a set of parameters using the model you generate.",
            "You generate the graph.",
            "Now we forget the parameters and you try to feed this graph, right?",
            "So you know the true parameters and you try, you try to feed the graph and the question is starting on the random point.",
            "Will you recover the parameters that you used to generate your graph or not OK, and basically we can do this always right?",
            "Which means that even though my search space is not convex my search space.",
            "Right, so my search space is not like that, right?",
            "I can explain why it's not my search space is like this.",
            "Or as many calls as you like, but it seems there are a lot of local minimum, but all local minima are global minima.",
            "I don't have any formal.",
            "Proof, but this this should say this this.",
            "This is suggesting that this is the case.",
            "What's going on so we don't have a situation like this, right?",
            "That you would have that you would get easily stuck somewhere far.",
            "But whenever I start I converge to something that is good and the reason so one of the problems with so this is these are now details.",
            "One of the problems with the model is that.",
            "If I have no parameter matrix ABC&D, this is the same as having.",
            "The so here right?",
            "If I just shuffle.",
            "So now I have a here should have been here and see here right?",
            "So these are these two should generate same graphs right?",
            "These are the same parameter matrices right?",
            "Even though they are different?",
            "This is so cause cause these two.",
            "This so so every permutation basically gives you the same likelihood, right?",
            "So if I have if I have a graph, the likelihood of these parameters should be the same as likelihood of these parameters, right?",
            "I just promoted them right in this 98%.",
            "What I did here I would go and try to match the parameters right?",
            "So if I start with this and I converge to that, that is good for me, right?",
            "I recovered the true parameters and this is this is the reason why the from this intuition you can show that your search space is not is not like this, but it's like it's like that.",
            "Or there could be some other ugly things, but it seems they're not there because you can pretty much always find the parameters that used to generate your synthetic graph OK.",
            "Sorry.",
            "If you can only global minimum.",
            "Always accept on the change, for better, but that's what I'm doing.",
            "This is.",
            "This is what I'm doing right?",
            "I'm doing gradient descent.",
            "What else is gradient descent as being here calculating the gradient and making?",
            "Oh no, that's that's that's that's for calculating this thing right.",
            "G of G of Theta, right?",
            "But what I'm really doing?",
            "I'm taking I'm calculating a derivative of that but I just don't want to explain you that because it's even more confusing than what I just explained.",
            "OK, so I'm not at the end.",
            "I'm not really calculating this but I'm calculating this right and then I get the derivative and I can do my.",
            "My gradient descent but to but to calculate this part I need to do the same thing because here is that some over all the permutations OK, But so this is one thing.",
            "So it seems that the search space is nice.",
            "This shows that search space is nice so that I can recover.",
            "The parameters are used to generate the graph and the other question is for example.",
            "Here is I'm doing now the I'm plotting the gradient decent iterations, right?",
            "So I start at random point and now I'm starting to update my Theta.",
            "And this is what happens to be the graph, right?",
            "So I will start with Theta 0, which is just some random thing and now I get Theta one which is Theta 0 plus right?",
            "Some Lambda and this derivative right?",
            "Right, and at every iteration I would take this generator random graph from these parameters.",
            "And now I'm comparing the both graphs.",
            "OK, so for example here is here is I'm comparing the diameter of the of the of the true graph, right?",
            "The true parameters I used to generate the graph and if I start no trend.",
            "This is where I start at random point and as I as I change my parameters as I as I keep on moving down.",
            "In my in my optimization I get a graph that soon gets the diameter of the true graph or here is the.",
            "So if I calculate the first eigenvalue of my graph adjacency matrix again, right?",
            "This is the true one and this is how the how the gradient descent is getting better and better or what I'm showing here is every jab salute error in the parameters right?",
            "So I know what is my my I know Theta star IDs are the parameters are used to generate my graph.",
            "Now given given this one I can just go.",
            "And elementwise subtract, add them together and get the average right?",
            "So this tells me what is my average absolute error in the parameters, right?",
            "At first when I start at random point, it's high as the gradient descent continuous it's getting smaller OK. And this is for likelihood again.",
            "I'm not getting there but but I get close.",
            "OK, so here it was like 16,000 nodes I think.",
            "The number of iterations needed to be large size of the graph.",
            "I know or I didn't really check.",
            "I think that's better.",
            "So for this experiment I would.",
            "I would assume that my model is D model, right?",
            "I find some I find some parameter, I choose some parameters, I generate the graph and Now the question is, can I find the parameters right?",
            "So this is under assumption that all the graphs are Chronicle right?",
            "So now the next question is?",
            "If you take a real graph.",
            "Is there, I know is the is the Chronicle model good enough?",
            "Model right?",
            "So what I showed is that."
        ],
        [
            "If the if the chronic model is a good model, I'm able to fit it right from the previous slide.",
            "Now the question is, if I take a real graph, is chronically model good enough to feed feed them?",
            "Over the drugs can be random graphs.",
            "OK, I mean, yeah really.",
            "Up in this sense would be something which is really measured.",
            "For example, this is an autonomous systems graph.",
            "That has this many nodes in this many edges.",
            "This is the parameter matrix I found.",
            "Now if you take this and generate a Chronicle graph and you take a real graph and I know you plot the degree distribution, this is the real degree distribution and this is the one of the fitted guy.",
            "Or this is so here what I'm plotting here is the.",
            "Here is the number of hops.",
            "This is the number of reachable pairs of nodes and again, right.",
            "This one is the true one, taken from the nature.",
            "If you like, this is what we got from fitting right using these parameters.",
            "If you generate a Chronicle graph, this is what you get.",
            "This is for eigenvalues of a graph adjacency matrix.",
            "It's close and this is for basic network which is just.",
            "Components of the first eigenvector.",
            "Again, are very close, so this is for autonomous systems.",
            "And this is for opinions which is like online community who trusts whom type of network?"
        ],
        [
            "We have like 76,000 people and half a million edges.",
            "This is this is the parameters we recovered if I now.",
            "Take the reality and see what I get again.",
            "It's close.",
            "Solution behind the difference in what does that mean?",
            "So here it says so the difference.",
            "So the results about about these are just they called this a spectral gap, right?",
            "So the difference between the 1st and the 2nd.",
            "So this guy right should tell you how how nicely the graph clusters, how nicer communities.",
            "But I don't know any results for for what, why?",
            "Why is why would you care about the rest?",
            "But on the other hand, if you just care about generating choreographs then you can say.",
            "Given a graph, I want to have some function that we calculate some statistic and I want to match that statistic right.",
            "So this is like.",
            "Normalize that the difference in the first eigenvalue in the second one.",
            "What is your conclusion?",
            "Can you generate?",
            "Is the same conclusion over different real datasets that you said?",
            "So here for that carries lower.",
            "I'm getting lower yeah, and on the previous one I think it was lower also yeah.",
            "Then one clusters better what I'm saying that since the gap right, the gap in the this gap right here here is larger than here.",
            "It means that chronic doesn't mean or under these parameters.",
            "We don't have good, let's say good enough community structure or something, right?",
            "The size of the of the cut here will be smaller than the size of the cut there.",
            "That's what this means, but.",
            "Repeat, yeah, maybe.",
            "But this can be achieved how you can improve your approach because you always have a problem on this and the other ones look.",
            "Should I mean these are not ultimate results?",
            "Um, this is how much I was able to fit into 8 pages.",
            "Sorry, I mean.",
            "Um?",
            "Care for the rest of eigenvalues?",
            "Something so from like a computer scientist, right from physics.",
            "If you're a physicist then you care about this.",
            "If you are computer scientists, all you want to have is like some function that will take a graph and will output you something.",
            "And now if your graph is real, if you take your simulated graph edge, you want to have the same function and Now this is 1 example of such function OK?",
            "For me this is quite informative.",
            "The distribution of eigenvalues as well because it gives you.",
            "Kind of distribution of.",
            "Dense areas in the grass, and now this is this.",
            "These are these are these are the components of the first eigenvector, right?",
            "These are the page rank weights.",
            "These are these are the dense areas.",
            "No, no, this is the eigenvalue and this is the eigenvector.",
            "OK. First, I can so I can.",
            "Sure, OK, so the rest of the eigenvalues would give you important.",
            "Also, the rest of the dense areas in the graph.",
            "OK, so that's why it has sense.",
            "So basically the slope of this curve would be interesting property as well, exactly or what there are theorems for is the difference between this one and this one.",
            "Roll average, let's say.",
            "So.",
            "The slope of, let's say if you would fit into a linear.",
            "Line would be already informative as well.",
            "In my opinion, it would be.",
            "It would have some intuitive sense.",
            "Let's leave it there OK?",
            "Um, so the other thing I want to show."
        ],
        [
            "We just it scales right?",
            "So here I would.",
            "I would be fitting graphs with no.",
            "Here was, I think like 16,000,000 or something.",
            "And this is the time I needed the here's one thing I write.",
            "It's linear.",
            "The problem is that.",
            "The constant is very, very high, and what do I mean is because when I'm doing when I'm doing this sampling of these permutations, right?",
            "I didn't say how much samples is enough, so we have to decide on how many, how many, how many sigmas you want to sample, and that's that's a totally different question, right?",
            "So how long do you need to sample to get to match this distribution?",
            "Well, that's.",
            "There are.",
            "I mean there are like ways how you can examine whether you sample long enough, but this is something of black art.",
            "OK, and the other one that was the message.",
            "So here's time.",
            "Here's the size of the graph, and this is a linear feet.",
            "Just intuitive, I mean why is black heart?",
            "No, no, no.",
            "It scales linearly.",
            "The question is, how big is the constant in front of North right?",
            "And the constant is basically how many samples do I need to draw from here and there is.",
            "I mean there is no.",
            "Nobody will tell you how to send it.",
            "OK, and the last one that I."
        ],
        [
            "So that is also is called model selection, right?",
            "So how many parameters should I take, right?",
            "So how big should this Theta matrix should be 2 by two?",
            "So I was showing you examples for two by two.",
            "So basically given just four parameters I showed you that we got close right, but I wasn't.",
            "So the question is if I would take three by three I should get at least that close or better, right?",
            "Because my model is more expressive.",
            "So the question is.",
            "Should I take them and four by 4:00 or so in one way?",
            "One way you can, basically what you're basically doing then is there are many different different ways how to how to how to basically how to trade off between the fit of the model and the model complexity, right?",
            "So this is my fit of the model and this is this is the model complexity and and this is.",
            "This is called Bayes information criterion, so bye see, right?",
            "I have the feet of the likelihood of the model and this is the.",
            "This is the number of parameters, right?",
            "The size of the matrix.",
            "And this is the size the size of the graph.",
            "OK, the all possible edges I could get.",
            "So this is one way.",
            "So this is one way how you can set up this.",
            "Trading off between the model complexity or model field.",
            "The other way would be to use MDF, right?",
            "So again here we have the likelihood, which is again how many bits.",
            "Do you need to describe the the how well the model fits in here?",
            "I would have how much, how many, how many bits I need to.",
            "I need to decode the model the other so there is also I know so AIC which is again very similar.",
            "You always have a term like this and you have another term right?",
            "First term tells you how well the model fits in the second.",
            "Term tells you how good is, how complex is the model right?",
            "And here I have an example of, uh, so this is a little example here.",
            "So here is the size of my matrix.",
            "So of my Theta matrix, how big it is?",
            "I generated a graph using the three by three matrix, and here is the IC score right?",
            "And since this one is the lowest I was able to recover the true number of parameters right?",
            "So this is another question, how do you select the size of the matrix?",
            "Find the equivalence classes, then in larger metric matrices this can get quite large.",
            "Still is.",
            "Permutation of all the elements or.",
            "If you would take the larger so the larger right if I if I take 2 by two right then 4 by 4 by 4 should be there, right?",
            "So if I take if I take this one and Chronicle power it right?",
            "I should get.",
            "Right here I should get a squared and so on, right?",
            "So so in terms of likelihood, this one and this one should have the same likelihood, right?",
            "So the same fit but but this will get larger, right?",
            "'cause this will be.",
            "60 mil exactly.",
            "You want to have the smallest one that fits well, right?",
            "So we're trading of how well the model fits and how complex is the model.",
            "And this is this is the question what you that you are doing OK?",
            "And if you have this question everywhere right in linear regression you want to ask how many variables do you want to take.",
            "And again, there you can use this so you can use AIC.",
            "Or you can do cross validation, right?",
            "I could do cross validation.",
            "But I cannot do cross validation because I have just one graph, right?",
            "So I cannot really split the graph into a training and test set right?",
            "So this is also one one thing why this is in some sense hard.",
            "I don't write.",
            "I don't have 10 graphs and then let's say I will use 5 to find the parameters and then I'll use the other five to see how well I fit right?",
            "I cannot do that.",
            "I have one graph so I cannot do cross validation.",
            "So this is this is 1 possible way how to do it?",
            "Which is before we were showing this graph search.",
            "It was well.",
            "It looks like it looks like that one set of parameters always generates one set of.",
            "Properties or the map between 7 tropical graph properties matches very nicely set of parameters as well.",
            "So why why one wouldn't say that different sets of parameters would generate same?",
            "Set of rappers.",
            "I mean.",
            "This is somehow too nice somehow what you got in this sense of why so clear mapping one to one from parameters to properties in a way, so I would assume my intuition would tell me that somehow that one set of parameters or different sets of parameters would generate the same set of properties as well.",
            "This would be realistic in general.",
            "I yeah.",
            "I mean I don't.",
            "I mean, I don't really know what you're saying.",
            "So on one side you have set of parameters.",
            "On the other side, you're checking whether the graph.",
            "More or less have the same properties.",
            "No, I'm not doing that right?",
            "What is nice?",
            "I'm not really fitting this right.",
            "I'm not saying you're not fitting right?",
            "I'm comparing this.",
            "This is what I can do at the end, right?",
            "This is."
        ],
        [
            "My way of evaluation, but what I'm really doing is I'm I'm calculating the probability wherever that was right.",
            "The graph was generated by the parameters, so I'm really kept comparing every edge to every edge and this is just some statistics I showed to valuate how good it is, but I'm not really fitting this right.",
            "I'm fitting the probability OK.",
            "I mean, I guess there are some.",
            "There is some other set of parameters that I know would get me this this such such a degree distribution, but I, but what is nice is that I'm not really saying I want to have the degree distribution and I want to have a good ha plot or something, right?",
            "I'm saying I want to have.",
            "I want to have right.",
            "I want to have probability.",
            "I want to maximize this, right?",
            "OK, yeah and then why?",
            "I mean, you can say we are lucky right?",
            "We are solving this problem but.",
            "Then all this is nice, right?",
            "This well matched in the end.",
            "No, these are the all I checked.",
            "I want to check clustering coefficient so that is that may not work.",
            "I don't know so that I didn't check, but for these are the four I checked and this was this was this was good right?",
            "But so?",
            "I mean this is also nice right?",
            "I'm fit we're we're trying to maximize just this probability, but.",
            "But then if you like compare from the other side by by measuring some properties of the graph, it seems it works.",
            "OK, so.",
            "I don't know if this answers your question, but.",
            "Done OK, OK, OK so basically I'm done."
        ],
        [
            "Yeah.",
            "I think I was saying no, but if you think it was yes then and you are happy then I'm also happy so so basically I'm done.",
            "I just wanted to recap or say what I did right.",
            "So basically we saw that this model we can prove that it has properties and then we basically developed a scalable algorithm to fit fit to fit it to real graphs.",
            "And it seems that it fits right?",
            "This is this is all we can say right?",
            "What would be what would be the next step is to say I know to measure some kind of.",
            "Error on this properties or something and try to see and compare it to other models and so on, but not no other models were developed to give you give you more than one of match more than one of these plots.",
            "Right, so this is very, very early work, so I don't know of any other work that would really try to feed to do this type of fitting.",
            "There was only one paper that appeared that in the PNS, so the Proceedings of National Academy of Science and they were trying to feed preferential attachment and at the end the conclusion was the model is not good enough or is not expressive enough to model real networks.",
            "Hike.",
            "Yeah it is not so this is this is what they concluded at the end right?",
            "And they had the same type of problem because in preferential attachment nodes the one after the other.",
            "So again you have to find the ordering of the nodes that tell you how the nodes were coming and dislike appeared a month ago.",
            "So this is, I mean, this is not not yet like a classical machine learning treatment of the problem.",
            "Where you would I know?",
            "Networks will try.",
            "I tried this too and I was able to feed this one.",
            "It took 2 hours and this one it took 20 minutes.",
            "On my laptop.",
            "Why did you only try to?",
            "I mean I yeah I.",
            "Sure, I this is.",
            "I mean, come on, I have two hands still 10 fingers on them and.",
            "I'm in.",
            "All life could be reduced to metrics 2 by 2.",
            "Paul, I will look at this more and run more experiments.",
            "Or how do we approach?",
            "I mean, this is a never ending story, right?",
            "So somebody will come and they will say, oh, let's measure something else that nobody has thought of before, right?",
            "And they will say oh, and now none of the existing models is able to generate me that right?",
            "So in some sense, we're asking, it's like which sequence is more random, right?",
            "You come up with another test for randomness, and now so this whole previous random number generators are bad.",
            "And here's a better random number generator, and this is in a sense of similar type of stories as we're doing right.",
            "So tomorrow somebody can come and say, oh.",
            "But looking in graphs, this is also true.",
            "And you are not taking care of that.",
            "So look, here's my new little model that does it better.",
            "I mean, this is at the end, it's like that, right?",
            "These guys, which we're trying to fit preferential attachment.",
            "They were trying to order.",
            "He said pretty much right.",
            "You need to find the order so we have the same problem.",
            "At the end you need to.",
            "You need to get around this permutation, right?",
            "The permutations defines the order how the nodes were added, like the age of the node, right?",
            "And they were trying to feed that.",
            "And then I think they were using some biological networks and.",
            "But they were physicists, so that's also.",
            "Everybody can prove it.",
            "At least in some respects.",
            "OK, so.",
            "Yeah.",
            "This one is larger, right?",
            "I mean, but this is large, right?",
            "This is this is not so small.",
            "Um, but I mean, there's there's a lot.",
            "There's a lot to be done here.",
            "I mean, this is like the first, the first, the first speaker.",
            "This actually I was I wanted to get a general version done to send it to the machine learning on graphs.",
            "But there was no time.",
            "March 10 and March 20th.",
            "Shop.",
            "Can you move?",
            "Oh, so this graph is undirected.",
            "So my probability matrix is symmetric, right?",
            "Even if I started at random point?"
        ],
        [
            "This right, these are these two elements and they are the same since since the Matrix since the graph is undirected, the matrix should be like.",
            "And this is what we got right?",
            "So this is also nice.",
            "Um?",
            "But yeah, now I mean now the next question is, can you say anything about right parameters are similar?",
            "Can you say anything from the parameters?",
            "What is also interesting is why this went so high right?",
            "So here it went to .99 and I would like artificially."
        ],
        [
            "Keep it below 99, so maybe you should go to one, but then why would you want to have like a very dense core of the network and so on?",
            "I mean there are more questions than answers here.",
            "Or there is a lot of opportunities for future work to be more politically correct.",
            "Did you try to fly?",
            "This forest fire morning.",
            "That would be even more set because it just has just two parameters.",
            "It would be more said right here.",
            "The world reduces to four.",
            "Then it would reduce the two.",
            "But in the previous case, it would be possible.",
            "So to generate the model I mean this Chronicle graph and compare it.",
            "Delete one.",
            "This is what I'm doing here.",
            "Right, OK, you are comparing to the statistical parameters, but I mean the real one.",
            "Preceded by even more freedom or whatever.",
            "More detail you mean you could plot them and try to visually compare them, but then you could go to 100 nodes and.",
            "I mean, I'm not right.",
            "I'm yeah, I know that you are not in there.",
            "That would be one option.",
            "So the idea would be to take a small real graph like I know some social network with 10 people.",
            "Or let's say what 24 or 32 people and then try to try to feed the Chronicle and see whether it works.",
            "In yeah I, I think it wouldn't work.",
            "In case because of another characteristic of a graph.",
            "So how how good can come Bhutanese amorphism?",
            "Can you get on how many notes or something like that?",
            "So that you know this is what that means because I know how correct correct my approach.",
            "No, no, I'm I'm missing a lot of things.",
            "I mean, I just didn't tell you about those.",
            "But can you get?",
            "Can you get like?",
            "He isn't isomorphism.",
            "Just yes, nothing.",
            "Can you say up to you can't write?",
            "It's hot, but still right.",
            "There is no 90% isomorphic.",
            "It's zero or one so.",
            "So sorry.",
            "Number that's there is no really good measure for comparing graphs.",
            "So you can't do like number of changes.",
            "I need to make on this one to get the other one.",
            "Draft rules don't work really in practice, yeah.",
            "I mean, Addison system graphs.",
            "No way you can do edit distances on trees.",
            "That's that's like computable for edge distances on graphs.",
            "Doesn't work right for isomorphism test testing, it's somewhere between P and NP, and nobody really knows, right?",
            "I think it's as hard as factoring so in factoring it's not clear on which side it is, so it's hard for.",
            "Yeah, I mean you could do graph kernels, but graph.",
            "Yeah what you could also do right?",
            "You could go and extract all possible subgraphs and have like a sparse vector, right?",
            "So you have like you go and now every possible sub graph is a word and now you describe your graph with all possible subgraphs.",
            "This.",
            "Could this could be what I mean.",
            "The kernels do random walks on graphs and that's also.",
            "And usually they have labeled graphs, right?",
            "And then they are comparing labels.",
            "I have no labels.",
            "Right, So what?",
            "Maybe Giannis will correctly, but as far as or how some random kernels on random walks work is basically we're doing a random walk on the network and you are like writing down the labels you saw and then you are comparing these labels.",
            "Switch match the distribution of this substance.",
            "That would be an option.",
            "For big graphs this should work, but then yeah.",
            "But then extracting subgraphs and counting them is again a nontrivial problem.",
            "Kompleksov, sorry.",
            "Subgraphs management, let's say for money.",
            "But molecules are labeled right, so you can cut your search space when you are doing the isomorphism testing you can.",
            "You can just additional constraints.",
            "Yeah, and it makes it makes the thing scalable.",
            "OK. Quick question.",
            "Personal phone.",
            "We take both.",
            "OK, match one more nickel."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know Tori.",
                    "label": 0
                },
                {
                    "sent": "Play on it.",
                    "label": 0
                },
                {
                    "sent": "Carbon downstairs, Lago yeah?",
                    "label": 0
                },
                {
                    "sent": "We we.",
                    "label": 0
                },
                {
                    "sent": "Recombination Naprosyn Castle 0 W 20 motor viral part label.",
                    "label": 0
                },
                {
                    "sent": "Press tab and tuck model problem you pass low tech technician.",
                    "label": 0
                },
                {
                    "sent": "Product key problem in.",
                    "label": 0
                },
                {
                    "sent": "But there's a check 2K parazitii Premier.",
                    "label": 0
                },
                {
                    "sent": "OK so sale Malaysia answer probably.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Web Core schema Webster any form a hyperlink minibuses, date torch torch capable Java Lacombe Ammonium Hooper Abney can draw Kakashi Mitsuo Asoiaf participate participate.",
                    "label": 0
                },
                {
                    "sent": "Patch Torch custodian may so.",
                    "label": 0
                },
                {
                    "sent": "Dumb luck, OK?",
                    "label": 0
                },
                {
                    "sent": "See more Internet attackers struck him as the root area proposal.",
                    "label": 0
                },
                {
                    "sent": "Amid root record album of occasion.",
                    "label": 0
                },
                {
                    "sent": "Boss, OK, cool protein mitsuba, so interacting in Karen's kindness days and Imma yeah so action so they lost most effective.",
                    "label": 0
                },
                {
                    "sent": "Dollar Store layaway banner ludy.",
                    "label": 0
                },
                {
                    "sent": "RIM Cobley Alpha Alpha.",
                    "label": 0
                },
                {
                    "sent": "Naravno Asoka Chanel's no sticker, so scoop Nehru symptoms swearing in Carson.",
                    "label": 0
                },
                {
                    "sent": "Check cashing so talus mosty allocated.",
                    "label": 0
                },
                {
                    "sent": "Quick anchors now model.",
                    "label": 0
                },
                {
                    "sent": "Alcatraz.",
                    "label": 0
                },
                {
                    "sent": "Stick up Obama.",
                    "label": 0
                },
                {
                    "sent": "They.",
                    "label": 0
                },
                {
                    "sent": "Super a gram amount as I percocet.",
                    "label": 0
                },
                {
                    "sent": "Additionally, so many, she says that she let him break.",
                    "label": 0
                },
                {
                    "sent": "Could you share the city flea tick in sociology should be per week or so.",
                    "label": 0
                },
                {
                    "sent": "So those are gravel in the next over the same Patch emails IMO.",
                    "label": 0
                },
                {
                    "sent": "Couple's argument so they make interaction with me outdoor pozna Koga.",
                    "label": 0
                },
                {
                    "sent": "In social products output graph input instances pressball stereo 10 graffle keibu problems.",
                    "label": 0
                },
                {
                    "sent": "I want to quarrel slow motion emerges as a major key.",
                    "label": 0
                },
                {
                    "sent": "Second console of Connery shall prevail as they took our myeloma to special name.",
                    "label": 0
                },
                {
                    "sent": "Kiper middle of the kite Odegaard.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You put the armor Patch more ritual measure on a million to check in.",
                    "label": 0
                },
                {
                    "sent": "Marisa tell number picks those man Connor in drug Astoria.",
                    "label": 0
                },
                {
                    "sent": "The kitchen Oprah shanya kissable.",
                    "label": 0
                },
                {
                    "sent": "Sydney, Australia will say but they got Selena Gomez.",
                    "label": 0
                },
                {
                    "sent": "Factor is the central central nautical park.",
                    "label": 0
                },
                {
                    "sent": "Isis Goody Vaniki, organization of Mno, tiny sword.",
                    "label": 0
                },
                {
                    "sent": "McNamara was Arteaga, NICU, Nicosia, Virte, Organizaci respond lol narrative.",
                    "label": 0
                },
                {
                    "sent": "Or become a Patch part million already apart.",
                    "label": 0
                },
                {
                    "sent": "Who is the central central?",
                    "label": 0
                },
                {
                    "sent": "Which logic will be solely owner?",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "So should I switch?",
                    "label": 0
                },
                {
                    "sent": "So we have an English speaking person in the audience, so maybe we should.",
                    "label": 0
                },
                {
                    "sent": "We should be sure.",
                    "label": 0
                },
                {
                    "sent": "Sure, no problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "And OK, now.",
                    "label": 0
                },
                {
                    "sent": "So these are the questions that.",
                    "label": 0
                },
                {
                    "sent": "We used to ask.",
                    "label": 0
                },
                {
                    "sent": "We can ask for smaller networks.",
                    "label": 0
                },
                {
                    "sent": "What is for example more more interesting to ask for bigger networks is like how many nodes do I need to remove from the network so that the network like can be separated into two pieces or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So this would be like a different type of questions that starts to make make sense when you have a much larger network, right?",
                    "label": 0
                },
                {
                    "sent": "And what we want to do now is basically we want to develop some kind of statistical tools and statistical method to study these big networks right?",
                    "label": 0
                },
                {
                    "sent": "And the whole problem comes from the fact that.",
                    "label": 0
                },
                {
                    "sent": "We cannot prove that we cannot draw this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sphinx right and what there are basically 3232 three goals we have right?",
                    "label": 0
                },
                {
                    "sent": "First is can we can we find the analyzed quantified this statistical properties of large networks can be modeled them.",
                    "label": 1
                },
                {
                    "sent": "Can we like?",
                    "label": 0
                },
                {
                    "sent": "Can we find simple generative models that will be able to generate this type of properties and then given the models then the next step is can we predict what will happen and things like that right?",
                    "label": 1
                },
                {
                    "sent": "And so here's the outline of the rest of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will spend some time now in today.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using these properties of real world networks, then I want to talk about Chronicle graphs, show some properties of them, and then present how to feed them real graphs and present experimental results.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, here is a list of.",
                    "label": 0
                },
                {
                    "sent": "Properties that can be found in.",
                    "label": 0
                },
                {
                    "sent": "Real world networks, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, first one is like small small world effect.",
                    "label": 0
                },
                {
                    "sent": "They call it so it means that the diameter of the network is very small even if the network itself is very large.",
                    "label": 0
                },
                {
                    "sent": "I'll explain about this more later.",
                    "label": 0
                },
                {
                    "sent": "The other the other properties, like they call it transitivity or clustering, which in social networks means that if I have two friends there, probably the friends themselves also writes, and this will like create a triangle, right?",
                    "label": 0
                },
                {
                    "sent": "I have a friend here.",
                    "label": 0
                },
                {
                    "sent": "I have a friend here and then.",
                    "label": 0
                },
                {
                    "sent": "They're also friends, so this this is called transitivity or clustering.",
                    "label": 0
                },
                {
                    "sent": "Then we have this skewed degree distributions.",
                    "label": 0
                },
                {
                    "sent": "Like then the expression that is used is called scale free networks then then we have other.",
                    "label": 0
                },
                {
                    "sent": "I'll explain later.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just just glance and then also some temporal patterns.",
                    "label": 0
                },
                {
                    "sent": "So all I want to say here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There was this, which is actually quite fascinating.",
                    "label": 0
                },
                {
                    "sent": "There was this experiment by Milgram in 60s where he asked people in Nebraska to do the following right.",
                    "label": 0
                },
                {
                    "sent": "So he asked these people to send letters to some stockbrokers in Boston, right?",
                    "label": 0
                },
                {
                    "sent": "And all these people knew was the name of the person and that they were stockbroker, right?",
                    "label": 0
                },
                {
                    "sent": "And they were asked to send to send these letters to their like immediate friends and what he was measuring is how many hops does it take for the letter to arrive to the particular stock broker.",
                    "label": 0
                },
                {
                    "sent": "Right and it turned out that like 25% of the letters that were initially sent reached the goal, but they but the important part is that they reached it in about 6 steps, right?",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "A random person in Nebraska can send the letter to random person in Boston in about 6 hops, which has two implications for the distances in the networks are small and the other one is that the humans are able to find them OK. And what do we mean by distances in the networks?",
                    "label": 0
                },
                {
                    "sent": "So this is usually captured by the diameter and people, so diameter is defined as like maximum.",
                    "label": 0
                },
                {
                    "sent": "So if DJ is the shortest path, the length of the shortest path between nodes I&J, then diameter is just the maximum over them.",
                    "label": 0
                },
                {
                    "sent": "Then, since this is like very noisy measure right you what people usually do is they call.",
                    "label": 0
                },
                {
                    "sent": "They talk about effective diameter, which is like just the 19th percentile of the diameter.",
                    "label": 0
                },
                {
                    "sent": "Or you can like calculate like average shortest path distance, right?",
                    "label": 0
                },
                {
                    "sent": "So I just go here over all pairs of shortest paths and I normalize appropriately, right?",
                    "label": 0
                },
                {
                    "sent": "Or the other one?",
                    "label": 0
                },
                {
                    "sent": "OK, I won't go into details and for example here is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's what you get right.",
                    "label": 0
                },
                {
                    "sent": "So here we have a Microsoft Messenger network, right?",
                    "label": 0
                },
                {
                    "sent": "So we have 180 million people talking to each other over a period of a month, and we have 1.3 billion edges, right?",
                    "label": 0
                },
                {
                    "sent": "So we have pretty much the whole world, and there is an edge.",
                    "label": 0
                },
                {
                    "sent": "If two people exchange.",
                    "label": 0
                },
                {
                    "sent": "If two people were engaged in a conversation at least once in this one month period, right?",
                    "label": 0
                },
                {
                    "sent": "And if I hear plot, the distance and the number of number of pairs of nodes at that distance, you can see that the mode of distribution is at 7.",
                    "label": 0
                },
                {
                    "sent": "But this means that most of the network is reachable within seven hops, right?",
                    "label": 0
                },
                {
                    "sent": "And this is log scale, right?",
                    "label": 0
                },
                {
                    "sent": "So if this would be like 100 million?",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, yeah.",
                    "label": 0
                },
                {
                    "sent": "Then note here is 10,000,000, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like almost 100 million people up there, right?",
                    "label": 0
                },
                {
                    "sent": "So more than half of the network can be reached in.",
                    "label": 0
                },
                {
                    "sent": "In at least seven hops, right?",
                    "label": 0
                },
                {
                    "sent": "And this says that they are not very small.",
                    "label": 0
                },
                {
                    "sent": "So the other property.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This degree distributions that people talk a lot about and this is basically so.",
                    "label": 0
                },
                {
                    "sent": "This is what we mean, right?",
                    "label": 0
                },
                {
                    "sent": "So we take PK to denote like a fraction of nodes that have degree K right?",
                    "label": 0
                },
                {
                    "sent": "And then we can just plot the histogram of probability of observing with the degree K, right and?",
                    "label": 0
                },
                {
                    "sent": "What would like what one would natively expect this to see something like this right?",
                    "label": 0
                },
                {
                    "sent": "If we would expect that there will be some mean to this distribution, meaning that we would say on average a person has ten friends and then some have a bit less and some have a bit more.",
                    "label": 0
                },
                {
                    "sent": "But there is a well like there is an average or there is a mode to the distribution.",
                    "label": 0
                },
                {
                    "sent": "What happens is that it's more like that, right?",
                    "label": 0
                },
                {
                    "sent": "So it's skewed.",
                    "label": 0
                },
                {
                    "sent": "It skewed towards towards towards the right.",
                    "label": 0
                },
                {
                    "sent": "OK and they call these types of distributions like heavy or long tail distributions and.",
                    "label": 0
                },
                {
                    "sent": "There are many quantities in real world that follow these types of distributions, right?",
                    "label": 0
                },
                {
                    "sent": "For example, the Amazon sales or the word length distribution follows this OK. And to be even more precise.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we said it's it's skewed, but what is even more important?",
                    "label": 0
                },
                {
                    "sent": "That is that the tail does not decay exponentially.",
                    "label": 0
                },
                {
                    "sent": "But in the case of the power rule.",
                    "label": 0
                },
                {
                    "sent": "So what I mean is that right probability of observing degree of a node with degree case scales like K to the minus Alpha, where Alpha is some.",
                    "label": 0
                },
                {
                    "sent": "Constant right?",
                    "label": 0
                },
                {
                    "sent": "And so if I plot, if I put a lock here in the lock here the Alpha like goes down right?",
                    "label": 0
                },
                {
                    "sent": "And if I plot the whole thing so I have log K versus LOC PK right?",
                    "label": 0
                },
                {
                    "sent": "I will get a line OK and justice next one.",
                    "label": 0
                },
                {
                    "sent": "So here I plot the same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think, but I plot it on different scales, right?",
                    "label": 0
                },
                {
                    "sent": "So here I have linear linear here I have log linear and here I have log log.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the whole thing would be exponential then here I should get a line right?",
                    "label": 0
                },
                {
                    "sent": "So if there would be like exponential decay in detail I should get a line here since I'm.",
                    "label": 0
                },
                {
                    "sent": "And here so if I if I get a line in the log log then I have a power, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a very it's in.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's very easy to test whether you have to distinguish between exponential and the power law, right?",
                    "label": 0
                },
                {
                    "sent": "And usually instead of instead of working with the.",
                    "label": 0
                },
                {
                    "sent": "PDF we work with the CDF, right?",
                    "label": 0
                },
                {
                    "sent": "So we the cumulative version of the distribution cause.",
                    "label": 0
                },
                {
                    "sent": "The variance right?",
                    "label": 0
                },
                {
                    "sent": "The variance in the tail is so high that the like.",
                    "label": 0
                },
                {
                    "sent": "Measurements are very noisy OK, but the important thing that is that this degree distributions in graphs are like this, which means that most of the people have just like one friend or two friends, and then there is a few people that have many friends and the other thing is there's a few of these people, but they have very high number of friends and there's still quite a lot of them in some sense.",
                    "label": 0
                },
                {
                    "sent": "OK, and for four more into it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is an example right?",
                    "label": 0
                },
                {
                    "sent": "So this would be this is an example of a network where that has degree distribution like this, right?",
                    "label": 0
                },
                {
                    "sent": "There is like a Bell shaped degree distribution with like exponential decay in the details right?",
                    "label": 0
                },
                {
                    "sent": "And this is like a power law or scale free type of network, right?",
                    "label": 0
                },
                {
                    "sent": "Where I have a degree distribution that is skewed and that it follows a power law.",
                    "label": 0
                },
                {
                    "sent": "And what is the immediate difference you see is that you have like hubs or nodes with very high degree and you have a lot of just like nodes that have just.",
                    "label": 0
                },
                {
                    "sent": "A slight degree one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so real networks are more like this and they're not like that.",
                    "label": 0
                },
                {
                    "sent": "OK generated randomly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is this is called airless rainy random graph.",
                    "label": 0
                },
                {
                    "sent": "Which means you have nodes and now you pick two nodes uniformly at random and create create an edge between them right?",
                    "label": 0
                },
                {
                    "sent": "And everything is iid.",
                    "label": 0
                },
                {
                    "sent": "So this is just pick to connect them pick next to connect while here for example.",
                    "label": 0
                },
                {
                    "sent": "The way how you generate this one is that you say probability of picking a node is proportional to the number of edges it already has.",
                    "label": 0
                },
                {
                    "sent": "So at 1st at first.",
                    "label": 0
                },
                {
                    "sent": "Which will mean that if a node they call this like rich get richer and is the other name they call, they call this or they call it preferential attachment, which means that you will if a node has already a lot of links, it will acquire more if you acquire more faster than somebody that has only one link.",
                    "label": 0
                },
                {
                    "sent": "OK, and this mechanism like push the tail of the distribution further and make it fatter.",
                    "label": 0
                },
                {
                    "sent": "Graph on the right.",
                    "label": 0
                },
                {
                    "sent": "Is it also called small world?",
                    "label": 0
                },
                {
                    "sent": "No, that's different.",
                    "label": 0
                },
                {
                    "sent": "So what small world does is so small world, so usually right people would go and they would discover something like this and they would say oh what is a simple generative process that gets me something like that, OK and?",
                    "label": 0
                },
                {
                    "sent": "For example, a simple generative process for scale free networks.",
                    "label": 0
                },
                {
                    "sent": "Is this what I just described called preferential attachment, right?",
                    "label": 0
                },
                {
                    "sent": "You pick a node proportional to the degree, which means I know on the other intuition is if you have a web page and you want to create the link to someone you are more likely to create a link to Google then to some other random insignificant webpage for what is a small world model is that.",
                    "label": 0
                },
                {
                    "sent": "They have actually I can.",
                    "label": 0
                },
                {
                    "sent": "I have slides for this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just a second.",
                    "label": 0
                },
                {
                    "sent": "So basically what is the small world model is they wanted they wanted to model the.",
                    "label": 0
                },
                {
                    "sent": "Transitivity of network.",
                    "label": 0
                },
                {
                    "sent": "So basically they wanted to have a model where you can interpolate between completely random networks and completely like regular networks like lattices or grids, right and?",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is this is an example of a small world model right?",
                    "label": 0
                },
                {
                    "sent": "You start with some regular network, this you can like, right?",
                    "label": 0
                },
                {
                    "sent": "It's a it's a circle where every node has what two edges?",
                    "label": 0
                },
                {
                    "sent": "Right now what they do is you can randomly rewire the network, which means just you pick two random nodes two like you pick four random nodes and you like change the edges.",
                    "label": 0
                },
                {
                    "sent": "Or you this is one option.",
                    "label": 0
                },
                {
                    "sent": "The other option is that you just introduce more random images.",
                    "label": 0
                },
                {
                    "sent": "And this is what you can get, right?",
                    "label": 0
                },
                {
                    "sent": "And as you as you as you as you keep doing this, you will get from a like a regular network to a completely random type of network.",
                    "label": 0
                },
                {
                    "sent": "And what is the interesting is that this is what's going on right?",
                    "label": 0
                },
                {
                    "sent": "So here is the rewiring probability.",
                    "label": 0
                },
                {
                    "sent": "So probability that I will start with my regular network, let's say like a grid and then I start messing with it, right?",
                    "label": 0
                },
                {
                    "sent": "I start picking up nodes or and like swapping edges and things like that, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the probability or the proportion of?",
                    "label": 0
                },
                {
                    "sent": "I just I swapped or the proportion of random and random edges are introduced and what happens is that.",
                    "label": 0
                },
                {
                    "sent": "So this is the diameter.",
                    "label": 0
                },
                {
                    "sent": "Another as I do as I introduce more randomness, the diameter will get smaller, right?",
                    "label": 0
                },
                {
                    "sent": "'cause the diameter of the grid, right?",
                    "label": 0
                },
                {
                    "sent": "If I increase the size, if I increase the size of the grid, what the diameter will increase if I increase the size of the grid by a square, the diameter will grow linearly, right?",
                    "label": 0
                },
                {
                    "sent": "So diameter will grow really quickly.",
                    "label": 0
                },
                {
                    "sent": "If I if I what is going on here is that I am getting smaller and smaller diameter.",
                    "label": 0
                },
                {
                    "sent": "The other thing is they called it.",
                    "label": 0
                },
                {
                    "sent": "In coefficient, which means which is basically a proportion of triangles in the network.",
                    "label": 0
                },
                {
                    "sent": "And again by doing by introducing this random edges the I'm basically destroying the triangles that are already there in the network.",
                    "label": 0
                },
                {
                    "sent": "So this is what they call a small world.",
                    "label": 0
                },
                {
                    "sent": "Producing new links or this is the depends on the on the exact definition, but it's the same right?",
                    "label": 0
                },
                {
                    "sent": "So for example, here is you can add or remove edges.",
                    "label": 0
                },
                {
                    "sent": "Basically you want to create random shortcuts and now.",
                    "label": 0
                },
                {
                    "sent": "Either rewiring or introducing new links would decrease the diameter if you.",
                    "label": 0
                },
                {
                    "sent": "If you're doing this uniformly.",
                    "label": 0
                },
                {
                    "sent": "If you do, if you are selecting this uniformly at random right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, then both processes are pretty much the same for the second one is introducing new links would certainly increase.",
                    "label": 0
                },
                {
                    "sent": "The first one is less intuitive.",
                    "label": 0
                },
                {
                    "sent": "Right, but if you have like short short short short links and now you will on the average right, you'll pick nodes that are like at half distance, so you will definitely bring the diameter down OK. Small World graphs model many real world networks and like sociological technological system.",
                    "label": 0
                },
                {
                    "sent": "So is this either not true anymore or so?",
                    "label": 0
                },
                {
                    "sent": "Better model or framework network.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "For example what is not good with small world model?",
                    "label": 0
                },
                {
                    "sent": "Is is this right?",
                    "label": 0
                },
                {
                    "sent": "So here I have degree distribution right?",
                    "label": 0
                },
                {
                    "sent": "And I was just showing you that it's a power law, right?",
                    "label": 0
                },
                {
                    "sent": "And this is not a power law, right?",
                    "label": 0
                },
                {
                    "sent": "Right, So what is so?",
                    "label": 0
                },
                {
                    "sent": "The thing is, people usually would just like go for they would discover something and then they would try to find the model that would explain it, right?",
                    "label": 0
                },
                {
                    "sent": "So people found degree, power or degrees solutions and they said OK, preferential attachment.",
                    "label": 0
                },
                {
                    "sent": "Then they found clustering right?",
                    "label": 0
                },
                {
                    "sent": "This triangles and they were saying oh how can we model that?",
                    "label": 0
                },
                {
                    "sent": "Oh here's the small world model right?",
                    "label": 0
                },
                {
                    "sent": "But small world model won't get you won't get to power low degree distribution.",
                    "label": 1
                },
                {
                    "sent": "So if you want one you have one model.",
                    "label": 0
                },
                {
                    "sent": "If you want the other you have the other model.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is.",
                    "label": 0
                },
                {
                    "sent": "And now if you would care about some third property, there's a third model.",
                    "label": 0
                },
                {
                    "sent": "That will get you that property, but there is there was very little work that would say.",
                    "label": 0
                },
                {
                    "sent": "What if I want all of them?",
                    "label": 0
                },
                {
                    "sent": "Or can I find the generator that probably can do all of them and this is what I'll show.",
                    "label": 0
                },
                {
                    "sent": "Triangles because we started social network analysis and meet actually does show that based on the analysis of triangles you can say whether some networks are.",
                    "label": 0
                },
                {
                    "sent": "I don't know more or less connected or something like that.",
                    "label": 0
                },
                {
                    "sent": "So So what is the question?",
                    "label": 0
                },
                {
                    "sent": "The question is what is the point about mentioning triangles right now?",
                    "label": 0
                },
                {
                    "sent": "Because we're talking about small world model.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there is no.",
                    "label": 0
                },
                {
                    "sent": "The clustering coefficient is the triangle.",
                    "label": 0
                },
                {
                    "sent": "Write.",
                    "label": 0
                },
                {
                    "sent": "The clustering coefficient is calculated, so here's here.",
                    "label": 0
                },
                {
                    "sent": "Should be the formula.",
                    "label": 0
                },
                {
                    "sent": "No, it's not OK.",
                    "label": 0
                },
                {
                    "sent": "The clustering coefficient is.",
                    "label": 0
                },
                {
                    "sent": "There is the right.",
                    "label": 0
                },
                {
                    "sent": "You have your clustering coefficient of a vertex with degree K, right?",
                    "label": 0
                },
                {
                    "sent": "Should be one over.",
                    "label": 0
                },
                {
                    "sent": "All right, if you work this case degree K then you want to ask how many possible triangles are there which means.",
                    "label": 0
                },
                {
                    "sent": "This is the number of possible triangles right here.",
                    "label": 0
                },
                {
                    "sent": "I call I say, number of triangles.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this says if I have a vertex.",
                    "label": 0
                },
                {
                    "sent": "Like this, right?",
                    "label": 0
                },
                {
                    "sent": "And I know something like this.",
                    "label": 0
                },
                {
                    "sent": "Then I say how many possible triangles are there, right?",
                    "label": 0
                },
                {
                    "sent": "It's K choose two.",
                    "label": 0
                },
                {
                    "sent": "OK, so I need to select two vertices.",
                    "label": 0
                },
                {
                    "sent": "This is the total total number of triangles.",
                    "label": 0
                },
                {
                    "sent": "So one triangle could be here.",
                    "label": 0
                },
                {
                    "sent": "The other could be here, here, here, here and so on OK. And now I say OK, there are two, two out of whatever.",
                    "label": 0
                },
                {
                    "sent": "Five choose two.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is clustering coefficient.",
                    "label": 0
                },
                {
                    "sent": "And then what you can do?",
                    "label": 0
                },
                {
                    "sent": "Actually I should say I should say this way, right?",
                    "label": 0
                },
                {
                    "sent": "So this is for a particular vertex.",
                    "label": 0
                },
                {
                    "sent": "Now CK is just the average over all vertices where of that particular degree and this is now you can plug this and you call this distribution of the clustering coefficient and then if you want a single single single see, then you can again every jewel or vertices, right?",
                    "label": 0
                },
                {
                    "sent": "So there are this is this is what I mean.",
                    "label": 0
                },
                {
                    "sent": "If you look from the perspective of a single node, it's only important that he's connected, but when you are looking at the clusters of, obviously you're looking at this one whether it's connected as well, so therefore you get the triangles.",
                    "label": 0
                },
                {
                    "sent": "Is that so?",
                    "label": 0
                },
                {
                    "sent": "I don't understand.",
                    "label": 0
                },
                {
                    "sent": "I mean all this says right?",
                    "label": 0
                },
                {
                    "sent": "Which makes sense is if that's me and I have two friends, then there is a higher probability of the two people knowing each other right?",
                    "label": 0
                },
                {
                    "sent": "And this is something that's definitely true.",
                    "label": 0
                },
                {
                    "sent": "Or for social networks, OK, but let's go back.",
                    "label": 0
                },
                {
                    "sent": "Tick tick.",
                    "label": 0
                },
                {
                    "sent": "OK, the other thing for which I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Want to go into too much detail is that there are also you can study spectral properties of graphs, which means that you can take your graph adjacency matrix right where where you have one.",
                    "label": 0
                },
                {
                    "sent": "If there is an edge and you have zero if there's no edge right?",
                    "label": 0
                },
                {
                    "sent": "And now you can do if the graph is undirected.",
                    "label": 0
                },
                {
                    "sent": "Then you can do like principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "Or you can do SVD so that you get.",
                    "label": 0
                },
                {
                    "sent": "Basically you get Eigen values and eigen vectors and if you plot the distribution of this you also find that it's skewed right and.",
                    "label": 0
                },
                {
                    "sent": "One intuition is that the they call it a spectral gap and a spectral gap measures the difference between first and second eigenvalue and then tells.",
                    "label": 0
                },
                {
                    "sent": "That tells you how, how, how nicely the graph would cluster, which means how nice communities are there in the cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's.",
                    "label": 0
                },
                {
                    "sent": "Let's go on right.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for Temple temple.",
                    "label": 0
                },
                {
                    "sent": "So this this was all for static graphs for temporal temporal patterns there are two right.",
                    "label": 0
                },
                {
                    "sent": "One is very basic question.",
                    "label": 0
                },
                {
                    "sent": "Basically want to ask what is the relation between the number of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the number of edges, overtime, right and right.",
                    "label": 0
                },
                {
                    "sent": "So if you would say N is the number of nodes at time, T is the number of edges of time T. If I double the number of nodes, will the number of edges also double?",
                    "label": 0
                },
                {
                    "sent": "OK, and if it turns out it won't right so it will more than double and how you can reason about this is if you plot it on log.",
                    "label": 0
                },
                {
                    "sent": "So here's the number of nodes at time T number of edges at time T and you plot both on log log scales and you will see it follows pretty much align right.",
                    "label": 0
                },
                {
                    "sent": "And if the relation would be linear then this line should have slope one.",
                    "label": 0
                },
                {
                    "sent": "If the graph is fully connected, or if it's a click, or every node has like a links to the the constant.",
                    "label": 0
                },
                {
                    "sent": "Constant part of the network.",
                    "label": 0
                },
                {
                    "sent": "Then this should be 2, right?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The exponent this a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The slope should be between one and two right?",
                    "label": 0
                },
                {
                    "sent": "One means you have the linear growth of the number of edges right here.",
                    "label": 1
                },
                {
                    "sent": "Or you can have quadratic growth of the number of failures.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what has been found for graphs that evolve overtime, so they're basically getting denser OK. And, um, so this is what I also explained, right?",
                    "label": 0
                },
                {
                    "sent": "If you have, if you have a equals one, then you have linear growth, which means the average degree in the graph is constant.",
                    "label": 0
                },
                {
                    "sent": "And if you have a = 2 then then you have quadratic growth, which means that.",
                    "label": 1
                },
                {
                    "sent": "You're getting very dense graph, right?",
                    "label": 0
                },
                {
                    "sent": "Because everyone is like connected to the constant fraction of the of the network.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this I will skip.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing that is also surprising for evolving networks is if you ask how do the distances increase as my network gets larger, right and?",
                    "label": 0
                },
                {
                    "sent": "I know conventional wisdom or results on the existing models tell you this right.",
                    "label": 0
                },
                {
                    "sent": "They would say the distance is slowly increased like log in or log log N and things like that, right?",
                    "label": 0
                },
                {
                    "sent": "So if as the as the end the number of nodes gets larger, the diameter will slowly increase.",
                    "label": 1
                },
                {
                    "sent": "This is what happens if you measure these things overtime, right?",
                    "label": 0
                },
                {
                    "sent": "So here for example, this is a citation network right from 1992 to 2003 and diameter decreases.",
                    "label": 0
                },
                {
                    "sent": "Right now the question is, so I showed you all this.",
                    "label": 0
                },
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "And so, before this is just just.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Short list of where of types of networks where these patterns or these properties can be found.",
                    "label": 0
                },
                {
                    "sent": "So what live web you have?",
                    "label": 0
                },
                {
                    "sent": "Web pages in hyperlinks, online communities would be.",
                    "label": 0
                },
                {
                    "sent": "I know friends there and things like that.",
                    "label": 0
                },
                {
                    "sent": "Who calls whom is like from AT&T.",
                    "label": 0
                },
                {
                    "sent": "You have phone numbers and who calls whom autonomous systems are are.",
                    "label": 0
                },
                {
                    "sent": "You can think of them.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as routers or like ibm.com is an autonomous systems and AT&T is an autonomous system.",
                    "label": 0
                },
                {
                    "sent": "So now this autonomous systems like like route packages between each other.",
                    "label": 0
                },
                {
                    "sent": "You can do the actual routers so this is the second one.",
                    "label": 0
                },
                {
                    "sent": "You could like Luke bipartite graphs like movie star actors.",
                    "label": 0
                },
                {
                    "sent": "You can look at citation networks.",
                    "label": 0
                },
                {
                    "sent": "You can look at the Co authorship networks.",
                    "label": 0
                },
                {
                    "sent": "You can look at sexual networks where an edge means there was some sexual interaction or something.",
                    "label": 0
                },
                {
                    "sent": "And you can look at also clickstream data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text.",
                    "label": 0
                },
                {
                    "sent": "How do you create a network out of text?",
                    "label": 0
                },
                {
                    "sent": "Movies.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can always make it as words which are adjacent people also do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Could answer I think so, but it's just.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I mean then you can just like you can create a network out of everything and then it's like.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, we picked up just the famous guys and.",
                    "label": 0
                },
                {
                    "sent": "So that I could be in a good company or something.",
                    "label": 0
                },
                {
                    "sent": "OK so right?",
                    "label": 0
                },
                {
                    "sent": "And there is a lot of also graph models that I also write.",
                    "label": 0
                },
                {
                    "sent": "I described what is a random graph I described.",
                    "label": 0
                },
                {
                    "sent": "What is preferential attachment?",
                    "label": 0
                },
                {
                    "sent": "This was developed for power law, degree distributions.",
                    "label": 0
                },
                {
                    "sent": "Copying model is something that gives you proper power law degree distributions but also gives you communities.",
                    "label": 0
                },
                {
                    "sent": "We had also small world model I described and so on right?",
                    "label": 0
                },
                {
                    "sent": "And for example, this one won't give you.",
                    "label": 0
                },
                {
                    "sent": "We generated graphs, the densify, but the diameter will be increasing and again you just got one property but not the others and so on right?",
                    "label": 0
                },
                {
                    "sent": "So this is usually the problem.",
                    "label": 0
                },
                {
                    "sent": "And here's here's one solution, right so?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want the model that for which you can prove and then fit it, and it does all this.",
                    "label": 0
                },
                {
                    "sent": "The thing here is that this model won't have like a very nice generative semantics.",
                    "label": 0
                },
                {
                    "sent": "OK, so it won't be like something that you could say is happening in the nature in some sense.",
                    "label": 0
                },
                {
                    "sent": "So let me define this Chronicle model, Chronicle graphs and what we will basically do.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                },
                {
                    "sent": "We will try to generate our graph.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recursively by yeah, let's say we will do it recursively, right?",
                    "label": 0
                },
                {
                    "sent": "So we'll start with some initial graph and then somehow recursively expand nodes of this graph to get larger and larger versions of the graph, right?",
                    "label": 0
                },
                {
                    "sent": "And here's an example what we're doing so basic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're doing tensor product of graph adjacency matrices, right?",
                    "label": 0
                },
                {
                    "sent": "This is the graph adjacency matrix of the following graph.",
                    "label": 0
                },
                {
                    "sent": "I go and I expand each node here with a miniature copy of the graph itself, and then I'm introducing some additional edges and this is exactly the process.",
                    "label": 0
                },
                {
                    "sent": "If I take this matrix and tensor, multiply it with itself, which means that I will take the constant here and and multiply it with the whole matrix.",
                    "label": 0
                },
                {
                    "sent": "So here I get G1 and here I'll get 0 * G One which is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'm just doing this product is basically multiplying two graphs exactly.",
                    "label": 0
                },
                {
                    "sent": "Nothing else, just this operation.",
                    "label": 0
                },
                {
                    "sent": "Would this be?",
                    "label": 0
                },
                {
                    "sent": "This is probably not.",
                    "label": 0
                },
                {
                    "sent": "Typical metrics multiplication somewhere.",
                    "label": 0
                },
                {
                    "sent": "Can you express it to you analytically in some way?",
                    "label": 0
                },
                {
                    "sent": "The this is how it's, I mean.",
                    "label": 0
                },
                {
                    "sent": "This mathematicians have defined this along time ago.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a chronic product, right?",
                    "label": 0
                },
                {
                    "sent": "Usually you have two types of products between graphs.",
                    "label": 0
                },
                {
                    "sent": "One is this tensor product or current product, the other the other one is Cartesian product, which doesn't really.",
                    "label": 0
                },
                {
                    "sent": "OK, so right.",
                    "label": 0
                },
                {
                    "sent": "So here I I called this G1, then I then I multiplied with itself.",
                    "label": 0
                },
                {
                    "sent": "I get G2 if I would now multiply G2 with itself.",
                    "label": 0
                },
                {
                    "sent": "This is what I would get.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will be getting some kind of nice.",
                    "label": 0
                },
                {
                    "sent": "I know self similar structures if you like.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so right, we will start with G1N1 and Y1.",
                    "label": 0
                },
                {
                    "sent": "Edges will call this initiator and then we'll get we'll get.",
                    "label": 0
                },
                {
                    "sent": "Sequence of growing graphs.",
                    "label": 0
                },
                {
                    "sent": "Each one will have N1 to the K nodes right?",
                    "label": 0
                },
                {
                    "sent": "'cause the size of the Matrix certificate here is 3.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is No 9 right cause just how the product works.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for.",
                    "label": 0
                },
                {
                    "sent": "So this is the model.",
                    "label": 0
                },
                {
                    "sent": "And here is here is more precisely what the tensor product.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chronic product of two matrices is right if I have A&BI just take the elements of A and like sticking the whole B matrix OK and here are right.",
                    "label": 0
                },
                {
                    "sent": "If these are the sizes of AMB, this is the size of the of the product, so the matrix is growing.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This right so will get case Chronicle power by just multiplying G1 with itself.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the intuition.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can have is that basically we're having this recursive growth of communities where every where right.",
                    "label": 0
                },
                {
                    "sent": "This is like our building block and now every node in the network gets like expanded with this building block right?",
                    "label": 0
                },
                {
                    "sent": "And this continuous continuous.",
                    "label": 0
                },
                {
                    "sent": "Um, the other thing is what I showed you here.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deterministic, right?",
                    "label": 0
                },
                {
                    "sent": "So I have this matrix here, right?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is just zeros and ones, so it's completely deterministic process.",
                    "label": 0
                },
                {
                    "sent": "How I can randomize it?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is here so this is the idea how to get now a stochastic version or how to get random version of the graph and the idea is that instead of starting with 01 matrix I will start with some probability matrix where here now every cell is a probability, so it's between zero and one and the cells don't need to sum to one right?",
                    "label": 0
                },
                {
                    "sent": "So it's just every number is an arbitrary number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "I can now Chronicle multiply this with itself and get some probabilities right?",
                    "label": 0
                },
                {
                    "sent": "So for example here I will give get .5 squared.",
                    "label": 0
                },
                {
                    "sent": "Here I'll get.",
                    "label": 0
                },
                {
                    "sent": "Five times .2 and so on.",
                    "label": 0
                },
                {
                    "sent": "Right now I can.",
                    "label": 0
                },
                {
                    "sent": "I can go for every cell.",
                    "label": 0
                },
                {
                    "sent": "I can flip a coin and this will generate meograph OK and with higher probability.",
                    "label": 0
                },
                {
                    "sent": "This is like the edge that will happen with the highest probability and so on, OK?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was quickly on Chronicle graphs and now I want to show I want to skip this, but what you can do you can prove.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this process will have will be able to generate or there exist parameters for which all this is true, right?",
                    "label": 0
                },
                {
                    "sent": "So for this model you can show that the following holds, and so now I'll skip like 10 slides of these proofs.",
                    "label": 0
                },
                {
                    "sent": "And I will go into how to feed them, right?",
                    "label": 0
                },
                {
                    "sent": "So now the question.",
                    "label": 0
                },
                {
                    "sent": "So now the question is we.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Chronicle graphs can basically have nice expressive power, right?",
                    "label": 0
                },
                {
                    "sent": "They can generate graphs that have all these properties.",
                    "label": 0
                },
                {
                    "sent": "The question is, how do I figure out that initial little graph, right?",
                    "label": 0
                },
                {
                    "sent": "How do I figure out the one?",
                    "label": 0
                },
                {
                    "sent": "So given the real network, I would like to find out G1 so that I can then do do various things.",
                    "label": 0
                },
                {
                    "sent": "So what can I?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, I would like to know if I know the parameters how the graph are generated.",
                    "label": 0
                },
                {
                    "sent": "Maybe the parameters itself will reveal something about the structure.",
                    "label": 0
                },
                {
                    "sent": "What I can also use this for is I can use it for extrapolation, meaning given a graph today I can fit it and then I can.",
                    "label": 0
                },
                {
                    "sent": "I know generate 10 times larger graph and hope that this will be similar to what will happen in 10 years or next year.",
                    "label": 0
                },
                {
                    "sent": "I can use this for sampling again I can fit and then generate a smaller graph.",
                    "label": 0
                },
                {
                    "sent": "Or I can use this for anonymization which means?",
                    "label": 0
                },
                {
                    "sent": "I have an email network I don't want to share it, so maybe we should feed the model and then generate a new network using this model and the parameters we learn and then share this with other people and hope that like large scale properties will be the same right?",
                    "label": 0
                },
                {
                    "sent": "And since we're not really caring about a particular node, this should work.",
                    "label": 0
                },
                {
                    "sent": "OK, and how we do this, right?",
                    "label": 0
                },
                {
                    "sent": "So now I'll change the note.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so Theta is now a parameter is I call it a parameter matrix, but this is basically G1 in a probabilistic sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to find this little matrix and how do I do this right by maximum likelihood which means I have a true graph and now I would like to find most likely theater that generates the graph, which is my data, right?",
                    "label": 0
                },
                {
                    "sent": "And then the whole problem is basically how do I calculate this and then I maximize over Theta and what we will do?",
                    "label": 0
                },
                {
                    "sent": "We'll be using gradient descent right?",
                    "label": 0
                },
                {
                    "sent": "So instead of re calculating this, I will calculate the gradient.",
                    "label": 0
                },
                {
                    "sent": "Of this with respect to Theta or with respect to the elements of Theta.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it's it's one thing you need to decide is the size, right?",
                    "label": 0
                },
                {
                    "sent": "Whether it's two by two or three by three and then we need to decide on this on these numbers, that's it, I mean.",
                    "label": 0
                },
                {
                    "sent": "I'm I yeah, I don't know if so here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is more write more details, so I'll get this real graph which is here.",
                    "label": 0
                },
                {
                    "sent": "I have its adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "I have some Theta.",
                    "label": 0
                },
                {
                    "sent": "Let's say somebody told it or here is 1 I'll get.",
                    "label": 0
                },
                {
                    "sent": "I'll do the Chronicle powering right and now I want to calculate this and it's very easy, right?",
                    "label": 0
                },
                {
                    "sent": "So probability of a graph being generated by this set of parameters is.",
                    "label": 0
                },
                {
                    "sent": "Basically I go over all edges and then multiply probabilities of edges appearing and then for all edges that are missing in the graph I take 1 minus the probability of an edge, right?",
                    "label": 0
                },
                {
                    "sent": "So what this will do is?",
                    "label": 0
                },
                {
                    "sent": "I'll take since there is an edge.",
                    "label": 0
                },
                {
                    "sent": "I'll take this one and multiply it and then I'll.",
                    "label": 0
                },
                {
                    "sent": "This one and I know since here is not an edge this this probability will go into this part, right?",
                    "label": 0
                },
                {
                    "sent": "It's like 1 minus OK is this?",
                    "label": 0
                },
                {
                    "sent": "Is this about clear?",
                    "label": 0
                },
                {
                    "sent": "So basically I'm taking I'm taking whatever is generated by my set of parameters.",
                    "label": 0
                },
                {
                    "sent": "I take the graph adjacency matrix, somehow overlap them right?",
                    "label": 0
                },
                {
                    "sent": "And whenever there are once I multiply them here and wherever there are zeros I multiply 1 minus that probability OK. Yeah yeah, OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is the basic so this is all we're doing right?",
                    "label": 0
                },
                {
                    "sent": "It's very simple, so somebody tells me the parameters I get this.",
                    "label": 0
                },
                {
                    "sent": "I have the graph and now I'm just multiplying these probabilities of individual edges.",
                    "label": 0
                },
                {
                    "sent": "If edge occurs, I think the probability of an edge.",
                    "label": 0
                },
                {
                    "sent": "If there is no edge, I take 1 minus, right?",
                    "label": 0
                },
                {
                    "sent": "The matrix.",
                    "label": 0
                },
                {
                    "sent": "Exactly so I know a spot or value here.",
                    "label": 0
                },
                {
                    "sent": "It's a probability of an edge from node.",
                    "label": 0
                },
                {
                    "sent": "What four to node two?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's two.",
                    "label": 0
                },
                {
                    "sent": "Here's 4, so this is the probability of an edge of a node #4 linking to node number 2.",
                    "label": 0
                },
                {
                    "sent": "Exactly right, so I'm doing a maximum likelihood type of fitting.",
                    "label": 0
                },
                {
                    "sent": "So I'm asking how should I select the set of parameters to maximize this probability so to maximize the probability that the parameters generated me the graph I observe right?",
                    "label": 0
                },
                {
                    "sent": "And the way I'm doing this is that basically now every cell is like Bernoulli trial, right?",
                    "label": 0
                },
                {
                    "sent": "To flip a coin and now I say these are the flips of the coins that succeeded and these are the flips of the coins that did not succeed.",
                    "label": 0
                },
                {
                    "sent": "So these are the edges I see and these are the edges.",
                    "label": 0
                },
                {
                    "sent": "I didn't, I don't see.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what now we.",
                    "label": 0
                },
                {
                    "sent": "Basically we want to do this and that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 problems with this.",
                    "label": 0
                },
                {
                    "sent": "First one is the node labeling right?",
                    "label": 0
                },
                {
                    "sent": "So if I have two graphs right, they are the same.",
                    "label": 0
                },
                {
                    "sent": "I just label the nodes differently, right?",
                    "label": 0
                },
                {
                    "sent": "So here I get.",
                    "label": 0
                },
                {
                    "sent": "This is no number 1234 or I do it this way?",
                    "label": 0
                },
                {
                    "sent": "OK, I will get two different adjacency matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, and now if I would just go and overlap them.",
                    "label": 0
                },
                {
                    "sent": "This would be different in general, right?",
                    "label": 0
                },
                {
                    "sent": "So this is one problem, so I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know the node labeling right.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to map nodes of this graph to the node or how to make this adjacency matrix to this adjacency matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So what I need to do in principle, I need to consider all all node labeling Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK, so Sigma is just the permutation basically tells me how to label how to label these nodes so that I can then take this matrix, they take that one and calculate that equation I showed before OK.",
                    "label": 0
                },
                {
                    "sent": "So the problem here is, So what I would really like to do is I would like to average over over all labelings and.",
                    "label": 0
                },
                {
                    "sent": "I mean the number of permutations is like N factorial, right?",
                    "label": 0
                },
                {
                    "sent": "So this will explode for more than a graph of Idol 10 nodes if you like.",
                    "label": 0
                },
                {
                    "sent": "And the other assumption I'm making is that all labelings are apriori, have the same probability, right?",
                    "label": 0
                },
                {
                    "sent": "Labeling is just something that comes at the end when the graph is created and what labeling tells me is how to map the rows and columns of here to here right?",
                    "label": 0
                },
                {
                    "sent": "So in principle?",
                    "label": 0
                },
                {
                    "sent": "This is very bad news.",
                    "label": 0
                },
                {
                    "sent": "OK the other.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See now that, right?",
                    "label": 0
                },
                {
                    "sent": "So what I'm really calculating is this right probability of a graph given the parameters right?",
                    "label": 0
                },
                {
                    "sent": "This should be Theta.",
                    "label": 0
                },
                {
                    "sent": "Sorry and this is some permutation somebody told me how to do this mapping, and right now I'm again going over edges, but I'm I know flipping them around so this means I'm taking the youth.",
                    "label": 0
                },
                {
                    "sent": "The element at position you of this permutation, this vector, this mapping and this is what I'm doing right and?",
                    "label": 0
                },
                {
                    "sent": "The one problem with this equation is try this again, takes an time order N squared right because I have to go into reverse every cell of this of this matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So I have to go.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the adjacency matrix of a graph right?",
                    "label": 0
                },
                {
                    "sent": "And this is adjacency matrix of a graph.",
                    "label": 0
                },
                {
                    "sent": "So what I need to do is now traverse every cell of this matrix, which means if I have a graph of final size 100,000, this will never fit into memory or the complexity of justice evaluating evaluating the probability for a single Theta in the single.",
                    "label": 0
                },
                {
                    "sent": "Permutation will take order N squared right, which will never.",
                    "label": 0
                },
                {
                    "sent": "Will be never able to calculate that and.",
                    "label": 0
                },
                {
                    "sent": "So I have solution.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For both OK, this is good news.",
                    "label": 0
                },
                {
                    "sent": "So basically the bad news is that if you naively calculate this then you have to average over all permutations and you have to evaluate every every cell of your graph adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So we get the complexity would be like N factorial and and worse right.",
                    "label": 0
                },
                {
                    "sent": "And we can do this in linear time.",
                    "label": 0
                },
                {
                    "sent": "So in time order E right time proportional to the number of edges and there are two 2 tricks will be using right?",
                    "label": 0
                },
                {
                    "sent": "We won't, we won't consider.",
                    "label": 0
                },
                {
                    "sent": "All labelings but we will.",
                    "label": 0
                },
                {
                    "sent": "We will use some like statistical simulation techniques like MCMC to sample permutations from this distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So now I now I fixed the graph and I fixed the parameters and I want to send sample.",
                    "label": 0
                },
                {
                    "sent": "Distribution of the permutations of this.",
                    "label": 0
                },
                {
                    "sent": "So how you can think about this is that right?",
                    "label": 0
                },
                {
                    "sent": "Once I fix the graph and so let this be like the space of all permutations, right?",
                    "label": 0
                },
                {
                    "sent": "So here you can like think of them like.",
                    "label": 0
                },
                {
                    "sent": "I know being in some some lexicographical order or something right?",
                    "label": 0
                },
                {
                    "sent": "So here is a permutation that is like 1234 and here is like North minus one and so on right?",
                    "label": 0
                },
                {
                    "sent": "And there will be.",
                    "label": 0
                },
                {
                    "sent": "The distribution will be like this, right?",
                    "label": 0
                },
                {
                    "sent": "So there will be some permutations that are good that are true, right?",
                    "label": 0
                },
                {
                    "sent": "The ones that are really good and there will be a lot of bad permutations.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to sample permutations from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And what we hope is that if when will take a lot of samples.",
                    "label": 0
                },
                {
                    "sent": "It won't.",
                    "label": 0
                },
                {
                    "sent": "We won't have to consider everyone, but just sample many times and this will work right?",
                    "label": 0
                },
                {
                    "sent": "And the other trick that will be using is since real graphs are sparse.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is that right?",
                    "label": 0
                },
                {
                    "sent": "So I have is the number of edges and is the number of nodes in worse.",
                    "label": 0
                },
                {
                    "sent": "In worst case it would equal go like North squared, right?",
                    "label": 0
                },
                {
                    "sent": "So is the number of cells right?",
                    "label": 0
                },
                {
                    "sent": "And this grows quadratically then which is the size of the matrix?",
                    "label": 0
                },
                {
                    "sent": "But real graphs are sparse, which means that majority of elements in the in the matrix will be 0.",
                    "label": 0
                },
                {
                    "sent": "So what I can do is I can quickly calculate probability that the graph is empty, right?",
                    "label": 0
                },
                {
                    "sent": "That given the parameters I did not observe, not even a single edge, and now I only go and correct this for the edges that I actually observed, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the trick will be using and Now this this will now.",
                    "label": 0
                },
                {
                    "sent": "So calculating this and using some approximation will take constant time.",
                    "label": 0
                },
                {
                    "sent": "So I will only go and have to correct this probability for the edges that were really there OK, and now I'll go into a bit more.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What, So what we're really doing, but will be will be doing gradient decent, right?",
                    "label": 0
                },
                {
                    "sent": "So I want to have a derivative of the log likelihood with respect to my parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So there should be some kind of matrix of derivatives, right for every parameter, so for every element of my little initial matrix, I need to calculate this.",
                    "label": 0
                },
                {
                    "sent": "If you are a bit clever, you get something.",
                    "label": 0
                },
                {
                    "sent": "You get an expression like that, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm again averaging over all the permutations, but it says.",
                    "label": 0
                },
                {
                    "sent": "Here's the probability of a permutation, and then I take once I have that.",
                    "label": 0
                },
                {
                    "sent": "I take I take the the derivative of the likelihoods, but what is convenient now here is that I can sample this.",
                    "label": 0
                },
                {
                    "sent": "I can then evaluate this and just output output the average OK and.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is what we are doing right, so we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to sample permutations from this distribution and all we are doing is we start with something with some permutation, right?",
                    "label": 0
                },
                {
                    "sent": "Let's say just want to end and then we pick two spots in this permutation we swap the elements OK and Now what I'm doing is I'm just evaluating the ratio right?",
                    "label": 0
                },
                {
                    "sent": "So this says if the new.",
                    "label": 0
                },
                {
                    "sent": "If the new one.",
                    "label": 0
                },
                {
                    "sent": "So this is the new the new permutation I got.",
                    "label": 0
                },
                {
                    "sent": "If the new permutation has higher likelihood than the old one, I will always accept this permutation, otherwise I will accept it with this probability.",
                    "label": 0
                },
                {
                    "sent": "OK, this is you can think of this as some kind of simulated annealing right?",
                    "label": 0
                },
                {
                    "sent": "I tried, I have my current permutation.",
                    "label": 0
                },
                {
                    "sent": "I swap two elements.",
                    "label": 0
                },
                {
                    "sent": "If this is if this gives me a better permutation, or more likely permutation.",
                    "label": 0
                },
                {
                    "sent": "I keep it.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, then with some with some probability right?",
                    "label": 0
                },
                {
                    "sent": "I will still keep it.",
                    "label": 0
                },
                {
                    "sent": "So this is the intuition of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'm just doing this swapping if gets me if it gets me to better permutations, right to more likely permutations.",
                    "label": 0
                },
                {
                    "sent": "I keep that if it gets me towards permutations I will keep it with some probability right?",
                    "label": 0
                },
                {
                    "sent": "With this you that sampled again from uniform distribution of this shape.",
                    "label": 0
                },
                {
                    "sent": "Or this is just no I'm.",
                    "label": 0
                },
                {
                    "sent": "Just this is just the intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is if the if the random change gets me to more likely to better part of the space, I will keep that.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I just keep with some probability spaces.",
                    "label": 0
                },
                {
                    "sent": "So I will show OK.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what we're doing.",
                    "label": 0
                },
                {
                    "sent": "The other thing that's very nice here is right now we have local changes in my distribution, right?",
                    "label": 0
                },
                {
                    "sent": "I'm just sorry, my permutation.",
                    "label": 0
                },
                {
                    "sent": "I'm just swapping two elements, right?",
                    "label": 0
                },
                {
                    "sent": "So if this is my.",
                    "label": 0
                },
                {
                    "sent": "The adjacency matrix right that says what I'm basically doing.",
                    "label": 0
                },
                {
                    "sent": "I'm swapping the rows and columns of this adjacency matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So if I swap elements J&K, this means I'm swapping Jason case row and column right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just swapping these two rows and columns OK, but what this means is that the large portion of the adjacency matrix remains constant, right?",
                    "label": 0
                },
                {
                    "sent": "So when I will when I will be calculating this ratio, this parts that will cancel out right?",
                    "label": 1
                },
                {
                    "sent": "So I don't really need to.",
                    "label": 0
                },
                {
                    "sent": "Given the permutation, I don't really need to go and evaluate everything, I just need to evaluate these two rows and columns.",
                    "label": 0
                },
                {
                    "sent": "Does it make sense?",
                    "label": 0
                },
                {
                    "sent": "Stop what?",
                    "label": 0
                },
                {
                    "sent": "What one does let's say within PHP or these kind of problems.",
                    "label": 0
                },
                {
                    "sent": "Classical local optimizations now is just noise, but you can lock it changes influence just.",
                    "label": 0
                },
                {
                    "sent": "Luckily I'm here.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure, I mean there are many ways how you could do this, right?",
                    "label": 0
                },
                {
                    "sent": "Nobody says you want to do local local changes on your on your on your.",
                    "label": 0
                },
                {
                    "sent": "Permutation 'cause this will be.",
                    "label": 0
                },
                {
                    "sent": "This will be bad in a sense that it will take long time to get to get a completely random permutation right.",
                    "label": 0
                },
                {
                    "sent": "For example, a better strategy would be to pick pick some element element at random spot and insert it at some other random spot right?",
                    "label": 1
                },
                {
                    "sent": "And this would then shift all the other elements by one and this is much better shuffling strategy.",
                    "label": 0
                },
                {
                    "sent": "The problem with that stuffing strategies that it's a better strategy so it means it will be more efficient or I'll need less less.",
                    "label": 0
                },
                {
                    "sent": "Less samples to converge or to get that sample from the distribution, but the problem is that it's yeah it's not local, so everything will change, so I'll have to evaluate much more.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not clear it's not entirely clear why this would be obvious.",
                    "label": 0
                },
                {
                    "sent": "I mean, why is it obvious that this?",
                    "label": 0
                },
                {
                    "sent": "Here the elements will cancel?",
                    "label": 0
                },
                {
                    "sent": "This other shopping strategy better, I mean, because if I if I now swap this so it means I'm taking this one and inserting it here, then the whole part here moves one to the right.",
                    "label": 0
                },
                {
                    "sent": "So it means all.",
                    "label": 0
                },
                {
                    "sent": "Compared to this one, it's better for getting it's a better shuffling strategy, right?",
                    "label": 0
                },
                {
                    "sent": "Because it will, sorry.",
                    "label": 0
                },
                {
                    "sent": "I have a the.",
                    "label": 0
                },
                {
                    "sent": "Big cause if you start you can show that I know if so now I'm just talking by memory, but still I think if you start with the order permutation and you ask how long, how many shuffles do I need to get to get to a random permutation?",
                    "label": 0
                },
                {
                    "sent": "For this thing you I think you need like order N squared swaps, while if you take a random one and insert it you will need like order and log in or something.",
                    "label": 0
                },
                {
                    "sent": "So it's much better in this sense, so we need less samples to get to sample the whole space.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you won't have this locality, so we'll have to at every at every iteration.",
                    "label": 0
                },
                {
                    "sent": "It will cost you more to evaluate this OK, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was the idea how to get how to sample, how to sample these permutations from the distribution using this local moves.",
                    "label": 0
                },
                {
                    "sent": "This works quickly and the other one is this observation that makes everything even faster.",
                    "label": 0
                },
                {
                    "sent": "OK, and now the other question is so now I have this.",
                    "label": 0
                },
                {
                    "sent": "So I have the parameters I have the I have the permutation.",
                    "label": 0
                },
                {
                    "sent": "How do I?",
                    "label": 0
                },
                {
                    "sent": "How do I get this?",
                    "label": 0
                },
                {
                    "sent": "How do I calculate the probability right and before we were so if you would do it maybe you would have to evaluate every cell.",
                    "label": 0
                },
                {
                    "sent": "Of this adjacency matrix right, which takes order N squared.",
                    "label": 0
                },
                {
                    "sent": "But what you can notice is that every element here will have this this form.",
                    "label": 0
                },
                {
                    "sent": "So if I have my capital state at the parameter matrix, which is, let's say two by two, then I have four parameters.",
                    "label": 1
                },
                {
                    "sent": "So this is 4, sorry.",
                    "label": 0
                },
                {
                    "sent": "Then the probability here will be like a power, so it will be first parameter to some power second parameter to some power and so on, and the sum of these parameters and this parameter.",
                    "label": 0
                },
                {
                    "sent": "This ABCD should sum to some constant like let's say K, right?",
                    "label": 0
                },
                {
                    "sent": "So here this one will be Theta, one to the K&NAB&C&D will be 0 and so on right?",
                    "label": 0
                },
                {
                    "sent": "So basically I'm having.",
                    "label": 0
                },
                {
                    "sent": "If you look at this carefully, you get to you get a multinomial series so you can easily summed it up.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if you want to calculate the likelihood of an empty graph, which means none of the edges are there, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to calculate this type of expression, which is ugly, but if you if you approximate this one, we it's Taylor approximation.",
                    "label": 0
                },
                {
                    "sent": "So log 1 -- X -- X -- 1/2 X squared, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a second 2nd order Taylor approximation to this.",
                    "label": 1
                },
                {
                    "sent": "Then because you have such a nice structure of your probability matrix.",
                    "label": 0
                },
                {
                    "sent": "You can you can like analytically solve this right?",
                    "label": 0
                },
                {
                    "sent": "So these are just the so this is now a sum over 4 elements, in our case right?",
                    "label": 0
                },
                {
                    "sent": "So I can easily get the likelihood of an empty graph.",
                    "label": 0
                },
                {
                    "sent": "So now the likelihood of the graph I get is the likelihood of an empty graph.",
                    "label": 0
                },
                {
                    "sent": "Now I only need to go over the edges.",
                    "label": 0
                },
                {
                    "sent": "So this is this is I sub basically now I'm now for the edges that I observed.",
                    "label": 0
                },
                {
                    "sent": "I remove the likelihood of not observing an edge and add the likelihood for observing an edge.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple.",
                    "label": 0
                },
                {
                    "sent": "And this again saves me a lot of a lot of time 'cause I don't need to go for every cell, but only for cells that have one.",
                    "label": 0
                },
                {
                    "sent": "And this is basically can be calculated like trivially, right?",
                    "label": 0
                },
                {
                    "sent": "This is like a sum over 4 elements and some powering, and I'm done.",
                    "label": 0
                },
                {
                    "sent": "OK, so Mark was so now the question is.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I did all this magic.",
                    "label": 0
                },
                {
                    "sent": "I used the approximations and everything.",
                    "label": 0
                },
                {
                    "sent": "The question is, does it converge?",
                    "label": 0
                },
                {
                    "sent": "So if I start with random point, do my gradient descent thing can I can I recover the two parameters?",
                    "label": 0
                },
                {
                    "sent": "So first experiment we did was basically the question is right, can I recover the true parameters?",
                    "label": 0
                },
                {
                    "sent": "Which again means how nice?",
                    "label": 0
                },
                {
                    "sent": "How smooth is my space or how much local minimize that and so on, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the experiment we did.",
                    "label": 0
                },
                {
                    "sent": "So you pick a set of parameters using the model you generate.",
                    "label": 0
                },
                {
                    "sent": "You generate the graph.",
                    "label": 0
                },
                {
                    "sent": "Now we forget the parameters and you try to feed this graph, right?",
                    "label": 0
                },
                {
                    "sent": "So you know the true parameters and you try, you try to feed the graph and the question is starting on the random point.",
                    "label": 0
                },
                {
                    "sent": "Will you recover the parameters that you used to generate your graph or not OK, and basically we can do this always right?",
                    "label": 0
                },
                {
                    "sent": "Which means that even though my search space is not convex my search space.",
                    "label": 0
                },
                {
                    "sent": "Right, so my search space is not like that, right?",
                    "label": 0
                },
                {
                    "sent": "I can explain why it's not my search space is like this.",
                    "label": 0
                },
                {
                    "sent": "Or as many calls as you like, but it seems there are a lot of local minimum, but all local minima are global minima.",
                    "label": 0
                },
                {
                    "sent": "I don't have any formal.",
                    "label": 0
                },
                {
                    "sent": "Proof, but this this should say this this.",
                    "label": 0
                },
                {
                    "sent": "This is suggesting that this is the case.",
                    "label": 0
                },
                {
                    "sent": "What's going on so we don't have a situation like this, right?",
                    "label": 0
                },
                {
                    "sent": "That you would have that you would get easily stuck somewhere far.",
                    "label": 0
                },
                {
                    "sent": "But whenever I start I converge to something that is good and the reason so one of the problems with so this is these are now details.",
                    "label": 0
                },
                {
                    "sent": "One of the problems with the model is that.",
                    "label": 0
                },
                {
                    "sent": "If I have no parameter matrix ABC&D, this is the same as having.",
                    "label": 0
                },
                {
                    "sent": "The so here right?",
                    "label": 0
                },
                {
                    "sent": "If I just shuffle.",
                    "label": 0
                },
                {
                    "sent": "So now I have a here should have been here and see here right?",
                    "label": 0
                },
                {
                    "sent": "So these are these two should generate same graphs right?",
                    "label": 0
                },
                {
                    "sent": "These are the same parameter matrices right?",
                    "label": 0
                },
                {
                    "sent": "Even though they are different?",
                    "label": 0
                },
                {
                    "sent": "This is so cause cause these two.",
                    "label": 0
                },
                {
                    "sent": "This so so every permutation basically gives you the same likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have if I have a graph, the likelihood of these parameters should be the same as likelihood of these parameters, right?",
                    "label": 0
                },
                {
                    "sent": "I just promoted them right in this 98%.",
                    "label": 0
                },
                {
                    "sent": "What I did here I would go and try to match the parameters right?",
                    "label": 0
                },
                {
                    "sent": "So if I start with this and I converge to that, that is good for me, right?",
                    "label": 0
                },
                {
                    "sent": "I recovered the true parameters and this is this is the reason why the from this intuition you can show that your search space is not is not like this, but it's like it's like that.",
                    "label": 0
                },
                {
                    "sent": "Or there could be some other ugly things, but it seems they're not there because you can pretty much always find the parameters that used to generate your synthetic graph OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "If you can only global minimum.",
                    "label": 0
                },
                {
                    "sent": "Always accept on the change, for better, but that's what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm doing right?",
                    "label": 0
                },
                {
                    "sent": "I'm doing gradient descent.",
                    "label": 0
                },
                {
                    "sent": "What else is gradient descent as being here calculating the gradient and making?",
                    "label": 0
                },
                {
                    "sent": "Oh no, that's that's that's that's for calculating this thing right.",
                    "label": 0
                },
                {
                    "sent": "G of G of Theta, right?",
                    "label": 0
                },
                {
                    "sent": "But what I'm really doing?",
                    "label": 0
                },
                {
                    "sent": "I'm taking I'm calculating a derivative of that but I just don't want to explain you that because it's even more confusing than what I just explained.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm not at the end.",
                    "label": 0
                },
                {
                    "sent": "I'm not really calculating this but I'm calculating this right and then I get the derivative and I can do my.",
                    "label": 0
                },
                {
                    "sent": "My gradient descent but to but to calculate this part I need to do the same thing because here is that some over all the permutations OK, But so this is one thing.",
                    "label": 0
                },
                {
                    "sent": "So it seems that the search space is nice.",
                    "label": 0
                },
                {
                    "sent": "This shows that search space is nice so that I can recover.",
                    "label": 0
                },
                {
                    "sent": "The parameters are used to generate the graph and the other question is for example.",
                    "label": 0
                },
                {
                    "sent": "Here is I'm doing now the I'm plotting the gradient decent iterations, right?",
                    "label": 0
                },
                {
                    "sent": "So I start at random point and now I'm starting to update my Theta.",
                    "label": 0
                },
                {
                    "sent": "And this is what happens to be the graph, right?",
                    "label": 0
                },
                {
                    "sent": "So I will start with Theta 0, which is just some random thing and now I get Theta one which is Theta 0 plus right?",
                    "label": 0
                },
                {
                    "sent": "Some Lambda and this derivative right?",
                    "label": 0
                },
                {
                    "sent": "Right, and at every iteration I would take this generator random graph from these parameters.",
                    "label": 0
                },
                {
                    "sent": "And now I'm comparing the both graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example here is here is I'm comparing the diameter of the of the of the true graph, right?",
                    "label": 0
                },
                {
                    "sent": "The true parameters I used to generate the graph and if I start no trend.",
                    "label": 0
                },
                {
                    "sent": "This is where I start at random point and as I as I change my parameters as I as I keep on moving down.",
                    "label": 0
                },
                {
                    "sent": "In my in my optimization I get a graph that soon gets the diameter of the true graph or here is the.",
                    "label": 0
                },
                {
                    "sent": "So if I calculate the first eigenvalue of my graph adjacency matrix again, right?",
                    "label": 0
                },
                {
                    "sent": "This is the true one and this is how the how the gradient descent is getting better and better or what I'm showing here is every jab salute error in the parameters right?",
                    "label": 0
                },
                {
                    "sent": "So I know what is my my I know Theta star IDs are the parameters are used to generate my graph.",
                    "label": 0
                },
                {
                    "sent": "Now given given this one I can just go.",
                    "label": 0
                },
                {
                    "sent": "And elementwise subtract, add them together and get the average right?",
                    "label": 0
                },
                {
                    "sent": "So this tells me what is my average absolute error in the parameters, right?",
                    "label": 0
                },
                {
                    "sent": "At first when I start at random point, it's high as the gradient descent continuous it's getting smaller OK. And this is for likelihood again.",
                    "label": 0
                },
                {
                    "sent": "I'm not getting there but but I get close.",
                    "label": 0
                },
                {
                    "sent": "OK, so here it was like 16,000 nodes I think.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations needed to be large size of the graph.",
                    "label": 0
                },
                {
                    "sent": "I know or I didn't really check.",
                    "label": 0
                },
                {
                    "sent": "I think that's better.",
                    "label": 0
                },
                {
                    "sent": "So for this experiment I would.",
                    "label": 0
                },
                {
                    "sent": "I would assume that my model is D model, right?",
                    "label": 0
                },
                {
                    "sent": "I find some I find some parameter, I choose some parameters, I generate the graph and Now the question is, can I find the parameters right?",
                    "label": 0
                },
                {
                    "sent": "So this is under assumption that all the graphs are Chronicle right?",
                    "label": 0
                },
                {
                    "sent": "So now the next question is?",
                    "label": 0
                },
                {
                    "sent": "If you take a real graph.",
                    "label": 0
                },
                {
                    "sent": "Is there, I know is the is the Chronicle model good enough?",
                    "label": 0
                },
                {
                    "sent": "Model right?",
                    "label": 0
                },
                {
                    "sent": "So what I showed is that.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the if the chronic model is a good model, I'm able to fit it right from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Now the question is, if I take a real graph, is chronically model good enough to feed feed them?",
                    "label": 0
                },
                {
                    "sent": "Over the drugs can be random graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean, yeah really.",
                    "label": 0
                },
                {
                    "sent": "Up in this sense would be something which is really measured.",
                    "label": 0
                },
                {
                    "sent": "For example, this is an autonomous systems graph.",
                    "label": 0
                },
                {
                    "sent": "That has this many nodes in this many edges.",
                    "label": 0
                },
                {
                    "sent": "This is the parameter matrix I found.",
                    "label": 0
                },
                {
                    "sent": "Now if you take this and generate a Chronicle graph and you take a real graph and I know you plot the degree distribution, this is the real degree distribution and this is the one of the fitted guy.",
                    "label": 0
                },
                {
                    "sent": "Or this is so here what I'm plotting here is the.",
                    "label": 0
                },
                {
                    "sent": "Here is the number of hops.",
                    "label": 0
                },
                {
                    "sent": "This is the number of reachable pairs of nodes and again, right.",
                    "label": 0
                },
                {
                    "sent": "This one is the true one, taken from the nature.",
                    "label": 0
                },
                {
                    "sent": "If you like, this is what we got from fitting right using these parameters.",
                    "label": 0
                },
                {
                    "sent": "If you generate a Chronicle graph, this is what you get.",
                    "label": 0
                },
                {
                    "sent": "This is for eigenvalues of a graph adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "It's close and this is for basic network which is just.",
                    "label": 0
                },
                {
                    "sent": "Components of the first eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Again, are very close, so this is for autonomous systems.",
                    "label": 0
                },
                {
                    "sent": "And this is for opinions which is like online community who trusts whom type of network?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have like 76,000 people and half a million edges.",
                    "label": 0
                },
                {
                    "sent": "This is this is the parameters we recovered if I now.",
                    "label": 0
                },
                {
                    "sent": "Take the reality and see what I get again.",
                    "label": 0
                },
                {
                    "sent": "It's close.",
                    "label": 0
                },
                {
                    "sent": "Solution behind the difference in what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So here it says so the difference.",
                    "label": 0
                },
                {
                    "sent": "So the results about about these are just they called this a spectral gap, right?",
                    "label": 0
                },
                {
                    "sent": "So the difference between the 1st and the 2nd.",
                    "label": 0
                },
                {
                    "sent": "So this guy right should tell you how how nicely the graph clusters, how nicer communities.",
                    "label": 0
                },
                {
                    "sent": "But I don't know any results for for what, why?",
                    "label": 0
                },
                {
                    "sent": "Why is why would you care about the rest?",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if you just care about generating choreographs then you can say.",
                    "label": 0
                },
                {
                    "sent": "Given a graph, I want to have some function that we calculate some statistic and I want to match that statistic right.",
                    "label": 0
                },
                {
                    "sent": "So this is like.",
                    "label": 0
                },
                {
                    "sent": "Normalize that the difference in the first eigenvalue in the second one.",
                    "label": 0
                },
                {
                    "sent": "What is your conclusion?",
                    "label": 0
                },
                {
                    "sent": "Can you generate?",
                    "label": 0
                },
                {
                    "sent": "Is the same conclusion over different real datasets that you said?",
                    "label": 0
                },
                {
                    "sent": "So here for that carries lower.",
                    "label": 0
                },
                {
                    "sent": "I'm getting lower yeah, and on the previous one I think it was lower also yeah.",
                    "label": 0
                },
                {
                    "sent": "Then one clusters better what I'm saying that since the gap right, the gap in the this gap right here here is larger than here.",
                    "label": 0
                },
                {
                    "sent": "It means that chronic doesn't mean or under these parameters.",
                    "label": 0
                },
                {
                    "sent": "We don't have good, let's say good enough community structure or something, right?",
                    "label": 0
                },
                {
                    "sent": "The size of the of the cut here will be smaller than the size of the cut there.",
                    "label": 0
                },
                {
                    "sent": "That's what this means, but.",
                    "label": 0
                },
                {
                    "sent": "Repeat, yeah, maybe.",
                    "label": 0
                },
                {
                    "sent": "But this can be achieved how you can improve your approach because you always have a problem on this and the other ones look.",
                    "label": 0
                },
                {
                    "sent": "Should I mean these are not ultimate results?",
                    "label": 0
                },
                {
                    "sent": "Um, this is how much I was able to fit into 8 pages.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mean.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Care for the rest of eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "Something so from like a computer scientist, right from physics.",
                    "label": 0
                },
                {
                    "sent": "If you're a physicist then you care about this.",
                    "label": 0
                },
                {
                    "sent": "If you are computer scientists, all you want to have is like some function that will take a graph and will output you something.",
                    "label": 0
                },
                {
                    "sent": "And now if your graph is real, if you take your simulated graph edge, you want to have the same function and Now this is 1 example of such function OK?",
                    "label": 0
                },
                {
                    "sent": "For me this is quite informative.",
                    "label": 0
                },
                {
                    "sent": "The distribution of eigenvalues as well because it gives you.",
                    "label": 0
                },
                {
                    "sent": "Kind of distribution of.",
                    "label": 0
                },
                {
                    "sent": "Dense areas in the grass, and now this is this.",
                    "label": 0
                },
                {
                    "sent": "These are these are these are the components of the first eigenvector, right?",
                    "label": 0
                },
                {
                    "sent": "These are the page rank weights.",
                    "label": 0
                },
                {
                    "sent": "These are these are the dense areas.",
                    "label": 0
                },
                {
                    "sent": "No, no, this is the eigenvalue and this is the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "OK. First, I can so I can.",
                    "label": 0
                },
                {
                    "sent": "Sure, OK, so the rest of the eigenvalues would give you important.",
                    "label": 0
                },
                {
                    "sent": "Also, the rest of the dense areas in the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why it has sense.",
                    "label": 0
                },
                {
                    "sent": "So basically the slope of this curve would be interesting property as well, exactly or what there are theorems for is the difference between this one and this one.",
                    "label": 0
                },
                {
                    "sent": "Roll average, let's say.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The slope of, let's say if you would fit into a linear.",
                    "label": 0
                },
                {
                    "sent": "Line would be already informative as well.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, it would be.",
                    "label": 0
                },
                {
                    "sent": "It would have some intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "Let's leave it there OK?",
                    "label": 0
                },
                {
                    "sent": "Um, so the other thing I want to show.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just it scales right?",
                    "label": 0
                },
                {
                    "sent": "So here I would.",
                    "label": 0
                },
                {
                    "sent": "I would be fitting graphs with no.",
                    "label": 0
                },
                {
                    "sent": "Here was, I think like 16,000,000 or something.",
                    "label": 0
                },
                {
                    "sent": "And this is the time I needed the here's one thing I write.",
                    "label": 0
                },
                {
                    "sent": "It's linear.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                },
                {
                    "sent": "The constant is very, very high, and what do I mean is because when I'm doing when I'm doing this sampling of these permutations, right?",
                    "label": 0
                },
                {
                    "sent": "I didn't say how much samples is enough, so we have to decide on how many, how many, how many sigmas you want to sample, and that's that's a totally different question, right?",
                    "label": 0
                },
                {
                    "sent": "So how long do you need to sample to get to match this distribution?",
                    "label": 0
                },
                {
                    "sent": "Well, that's.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "I mean there are like ways how you can examine whether you sample long enough, but this is something of black art.",
                    "label": 0
                },
                {
                    "sent": "OK, and the other one that was the message.",
                    "label": 0
                },
                {
                    "sent": "So here's time.",
                    "label": 0
                },
                {
                    "sent": "Here's the size of the graph, and this is a linear feet.",
                    "label": 0
                },
                {
                    "sent": "Just intuitive, I mean why is black heart?",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It scales linearly.",
                    "label": 0
                },
                {
                    "sent": "The question is, how big is the constant in front of North right?",
                    "label": 0
                },
                {
                    "sent": "And the constant is basically how many samples do I need to draw from here and there is.",
                    "label": 0
                },
                {
                    "sent": "I mean there is no.",
                    "label": 0
                },
                {
                    "sent": "Nobody will tell you how to send it.",
                    "label": 0
                },
                {
                    "sent": "OK, and the last one that I.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that is also is called model selection, right?",
                    "label": 0
                },
                {
                    "sent": "So how many parameters should I take, right?",
                    "label": 0
                },
                {
                    "sent": "So how big should this Theta matrix should be 2 by two?",
                    "label": 0
                },
                {
                    "sent": "So I was showing you examples for two by two.",
                    "label": 0
                },
                {
                    "sent": "So basically given just four parameters I showed you that we got close right, but I wasn't.",
                    "label": 0
                },
                {
                    "sent": "So the question is if I would take three by three I should get at least that close or better, right?",
                    "label": 0
                },
                {
                    "sent": "Because my model is more expressive.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "Should I take them and four by 4:00 or so in one way?",
                    "label": 0
                },
                {
                    "sent": "One way you can, basically what you're basically doing then is there are many different different ways how to how to how to basically how to trade off between the fit of the model and the model complexity, right?",
                    "label": 0
                },
                {
                    "sent": "So this is my fit of the model and this is this is the model complexity and and this is.",
                    "label": 0
                },
                {
                    "sent": "This is called Bayes information criterion, so bye see, right?",
                    "label": 0
                },
                {
                    "sent": "I have the feet of the likelihood of the model and this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the number of parameters, right?",
                    "label": 0
                },
                {
                    "sent": "The size of the matrix.",
                    "label": 1
                },
                {
                    "sent": "And this is the size the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, the all possible edges I could get.",
                    "label": 0
                },
                {
                    "sent": "So this is one way.",
                    "label": 0
                },
                {
                    "sent": "So this is one way how you can set up this.",
                    "label": 0
                },
                {
                    "sent": "Trading off between the model complexity or model field.",
                    "label": 0
                },
                {
                    "sent": "The other way would be to use MDF, right?",
                    "label": 0
                },
                {
                    "sent": "So again here we have the likelihood, which is again how many bits.",
                    "label": 0
                },
                {
                    "sent": "Do you need to describe the the how well the model fits in here?",
                    "label": 0
                },
                {
                    "sent": "I would have how much, how many, how many bits I need to.",
                    "label": 1
                },
                {
                    "sent": "I need to decode the model the other so there is also I know so AIC which is again very similar.",
                    "label": 0
                },
                {
                    "sent": "You always have a term like this and you have another term right?",
                    "label": 0
                },
                {
                    "sent": "First term tells you how well the model fits in the second.",
                    "label": 0
                },
                {
                    "sent": "Term tells you how good is, how complex is the model right?",
                    "label": 0
                },
                {
                    "sent": "And here I have an example of, uh, so this is a little example here.",
                    "label": 0
                },
                {
                    "sent": "So here is the size of my matrix.",
                    "label": 0
                },
                {
                    "sent": "So of my Theta matrix, how big it is?",
                    "label": 0
                },
                {
                    "sent": "I generated a graph using the three by three matrix, and here is the IC score right?",
                    "label": 0
                },
                {
                    "sent": "And since this one is the lowest I was able to recover the true number of parameters right?",
                    "label": 0
                },
                {
                    "sent": "So this is another question, how do you select the size of the matrix?",
                    "label": 0
                },
                {
                    "sent": "Find the equivalence classes, then in larger metric matrices this can get quite large.",
                    "label": 0
                },
                {
                    "sent": "Still is.",
                    "label": 0
                },
                {
                    "sent": "Permutation of all the elements or.",
                    "label": 0
                },
                {
                    "sent": "If you would take the larger so the larger right if I if I take 2 by two right then 4 by 4 by 4 should be there, right?",
                    "label": 0
                },
                {
                    "sent": "So if I take if I take this one and Chronicle power it right?",
                    "label": 0
                },
                {
                    "sent": "I should get.",
                    "label": 0
                },
                {
                    "sent": "Right here I should get a squared and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So so in terms of likelihood, this one and this one should have the same likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So the same fit but but this will get larger, right?",
                    "label": 0
                },
                {
                    "sent": "'cause this will be.",
                    "label": 0
                },
                {
                    "sent": "60 mil exactly.",
                    "label": 0
                },
                {
                    "sent": "You want to have the smallest one that fits well, right?",
                    "label": 0
                },
                {
                    "sent": "So we're trading of how well the model fits and how complex is the model.",
                    "label": 0
                },
                {
                    "sent": "And this is this is the question what you that you are doing OK?",
                    "label": 0
                },
                {
                    "sent": "And if you have this question everywhere right in linear regression you want to ask how many variables do you want to take.",
                    "label": 0
                },
                {
                    "sent": "And again, there you can use this so you can use AIC.",
                    "label": 0
                },
                {
                    "sent": "Or you can do cross validation, right?",
                    "label": 0
                },
                {
                    "sent": "I could do cross validation.",
                    "label": 0
                },
                {
                    "sent": "But I cannot do cross validation because I have just one graph, right?",
                    "label": 0
                },
                {
                    "sent": "So I cannot really split the graph into a training and test set right?",
                    "label": 0
                },
                {
                    "sent": "So this is also one one thing why this is in some sense hard.",
                    "label": 0
                },
                {
                    "sent": "I don't write.",
                    "label": 0
                },
                {
                    "sent": "I don't have 10 graphs and then let's say I will use 5 to find the parameters and then I'll use the other five to see how well I fit right?",
                    "label": 0
                },
                {
                    "sent": "I cannot do that.",
                    "label": 0
                },
                {
                    "sent": "I have one graph so I cannot do cross validation.",
                    "label": 0
                },
                {
                    "sent": "So this is this is 1 possible way how to do it?",
                    "label": 0
                },
                {
                    "sent": "Which is before we were showing this graph search.",
                    "label": 0
                },
                {
                    "sent": "It was well.",
                    "label": 0
                },
                {
                    "sent": "It looks like it looks like that one set of parameters always generates one set of.",
                    "label": 0
                },
                {
                    "sent": "Properties or the map between 7 tropical graph properties matches very nicely set of parameters as well.",
                    "label": 0
                },
                {
                    "sent": "So why why one wouldn't say that different sets of parameters would generate same?",
                    "label": 0
                },
                {
                    "sent": "Set of rappers.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "This is somehow too nice somehow what you got in this sense of why so clear mapping one to one from parameters to properties in a way, so I would assume my intuition would tell me that somehow that one set of parameters or different sets of parameters would generate the same set of properties as well.",
                    "label": 0
                },
                {
                    "sent": "This would be realistic in general.",
                    "label": 0
                },
                {
                    "sent": "I yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't really know what you're saying.",
                    "label": 0
                },
                {
                    "sent": "So on one side you have set of parameters.",
                    "label": 0
                },
                {
                    "sent": "On the other side, you're checking whether the graph.",
                    "label": 0
                },
                {
                    "sent": "More or less have the same properties.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not doing that right?",
                    "label": 0
                },
                {
                    "sent": "What is nice?",
                    "label": 0
                },
                {
                    "sent": "I'm not really fitting this right.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying you're not fitting right?",
                    "label": 0
                },
                {
                    "sent": "I'm comparing this.",
                    "label": 0
                },
                {
                    "sent": "This is what I can do at the end, right?",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My way of evaluation, but what I'm really doing is I'm I'm calculating the probability wherever that was right.",
                    "label": 0
                },
                {
                    "sent": "The graph was generated by the parameters, so I'm really kept comparing every edge to every edge and this is just some statistics I showed to valuate how good it is, but I'm not really fitting this right.",
                    "label": 0
                },
                {
                    "sent": "I'm fitting the probability OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess there are some.",
                    "label": 0
                },
                {
                    "sent": "There is some other set of parameters that I know would get me this this such such a degree distribution, but I, but what is nice is that I'm not really saying I want to have the degree distribution and I want to have a good ha plot or something, right?",
                    "label": 0
                },
                {
                    "sent": "I'm saying I want to have.",
                    "label": 0
                },
                {
                    "sent": "I want to have right.",
                    "label": 0
                },
                {
                    "sent": "I want to have probability.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize this, right?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah and then why?",
                    "label": 0
                },
                {
                    "sent": "I mean, you can say we are lucky right?",
                    "label": 0
                },
                {
                    "sent": "We are solving this problem but.",
                    "label": 0
                },
                {
                    "sent": "Then all this is nice, right?",
                    "label": 0
                },
                {
                    "sent": "This well matched in the end.",
                    "label": 0
                },
                {
                    "sent": "No, these are the all I checked.",
                    "label": 0
                },
                {
                    "sent": "I want to check clustering coefficient so that is that may not work.",
                    "label": 0
                },
                {
                    "sent": "I don't know so that I didn't check, but for these are the four I checked and this was this was this was good right?",
                    "label": 0
                },
                {
                    "sent": "But so?",
                    "label": 0
                },
                {
                    "sent": "I mean this is also nice right?",
                    "label": 0
                },
                {
                    "sent": "I'm fit we're we're trying to maximize just this probability, but.",
                    "label": 0
                },
                {
                    "sent": "But then if you like compare from the other side by by measuring some properties of the graph, it seems it works.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this answers your question, but.",
                    "label": 0
                },
                {
                    "sent": "Done OK, OK, OK so basically I'm done.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I think I was saying no, but if you think it was yes then and you are happy then I'm also happy so so basically I'm done.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to recap or say what I did right.",
                    "label": 0
                },
                {
                    "sent": "So basically we saw that this model we can prove that it has properties and then we basically developed a scalable algorithm to fit fit to fit it to real graphs.",
                    "label": 0
                },
                {
                    "sent": "And it seems that it fits right?",
                    "label": 0
                },
                {
                    "sent": "This is this is all we can say right?",
                    "label": 0
                },
                {
                    "sent": "What would be what would be the next step is to say I know to measure some kind of.",
                    "label": 0
                },
                {
                    "sent": "Error on this properties or something and try to see and compare it to other models and so on, but not no other models were developed to give you give you more than one of match more than one of these plots.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is very, very early work, so I don't know of any other work that would really try to feed to do this type of fitting.",
                    "label": 0
                },
                {
                    "sent": "There was only one paper that appeared that in the PNS, so the Proceedings of National Academy of Science and they were trying to feed preferential attachment and at the end the conclusion was the model is not good enough or is not expressive enough to model real networks.",
                    "label": 0
                },
                {
                    "sent": "Hike.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is not so this is this is what they concluded at the end right?",
                    "label": 0
                },
                {
                    "sent": "And they had the same type of problem because in preferential attachment nodes the one after the other.",
                    "label": 0
                },
                {
                    "sent": "So again you have to find the ordering of the nodes that tell you how the nodes were coming and dislike appeared a month ago.",
                    "label": 0
                },
                {
                    "sent": "So this is, I mean, this is not not yet like a classical machine learning treatment of the problem.",
                    "label": 0
                },
                {
                    "sent": "Where you would I know?",
                    "label": 0
                },
                {
                    "sent": "Networks will try.",
                    "label": 0
                },
                {
                    "sent": "I tried this too and I was able to feed this one.",
                    "label": 0
                },
                {
                    "sent": "It took 2 hours and this one it took 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "On my laptop.",
                    "label": 0
                },
                {
                    "sent": "Why did you only try to?",
                    "label": 0
                },
                {
                    "sent": "I mean I yeah I.",
                    "label": 0
                },
                {
                    "sent": "Sure, I this is.",
                    "label": 0
                },
                {
                    "sent": "I mean, come on, I have two hands still 10 fingers on them and.",
                    "label": 0
                },
                {
                    "sent": "I'm in.",
                    "label": 0
                },
                {
                    "sent": "All life could be reduced to metrics 2 by 2.",
                    "label": 0
                },
                {
                    "sent": "Paul, I will look at this more and run more experiments.",
                    "label": 0
                },
                {
                    "sent": "Or how do we approach?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a never ending story, right?",
                    "label": 0
                },
                {
                    "sent": "So somebody will come and they will say, oh, let's measure something else that nobody has thought of before, right?",
                    "label": 0
                },
                {
                    "sent": "And they will say oh, and now none of the existing models is able to generate me that right?",
                    "label": 0
                },
                {
                    "sent": "So in some sense, we're asking, it's like which sequence is more random, right?",
                    "label": 0
                },
                {
                    "sent": "You come up with another test for randomness, and now so this whole previous random number generators are bad.",
                    "label": 0
                },
                {
                    "sent": "And here's a better random number generator, and this is in a sense of similar type of stories as we're doing right.",
                    "label": 0
                },
                {
                    "sent": "So tomorrow somebody can come and say, oh.",
                    "label": 0
                },
                {
                    "sent": "But looking in graphs, this is also true.",
                    "label": 0
                },
                {
                    "sent": "And you are not taking care of that.",
                    "label": 0
                },
                {
                    "sent": "So look, here's my new little model that does it better.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is at the end, it's like that, right?",
                    "label": 0
                },
                {
                    "sent": "These guys, which we're trying to fit preferential attachment.",
                    "label": 0
                },
                {
                    "sent": "They were trying to order.",
                    "label": 0
                },
                {
                    "sent": "He said pretty much right.",
                    "label": 0
                },
                {
                    "sent": "You need to find the order so we have the same problem.",
                    "label": 0
                },
                {
                    "sent": "At the end you need to.",
                    "label": 0
                },
                {
                    "sent": "You need to get around this permutation, right?",
                    "label": 0
                },
                {
                    "sent": "The permutations defines the order how the nodes were added, like the age of the node, right?",
                    "label": 0
                },
                {
                    "sent": "And they were trying to feed that.",
                    "label": 0
                },
                {
                    "sent": "And then I think they were using some biological networks and.",
                    "label": 0
                },
                {
                    "sent": "But they were physicists, so that's also.",
                    "label": 0
                },
                {
                    "sent": "Everybody can prove it.",
                    "label": 0
                },
                {
                    "sent": "At least in some respects.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This one is larger, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, but this is large, right?",
                    "label": 0
                },
                {
                    "sent": "This is this is not so small.",
                    "label": 0
                },
                {
                    "sent": "Um, but I mean, there's there's a lot.",
                    "label": 0
                },
                {
                    "sent": "There's a lot to be done here.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is like the first, the first, the first speaker.",
                    "label": 0
                },
                {
                    "sent": "This actually I was I wanted to get a general version done to send it to the machine learning on graphs.",
                    "label": 0
                },
                {
                    "sent": "But there was no time.",
                    "label": 0
                },
                {
                    "sent": "March 10 and March 20th.",
                    "label": 0
                },
                {
                    "sent": "Shop.",
                    "label": 0
                },
                {
                    "sent": "Can you move?",
                    "label": 0
                },
                {
                    "sent": "Oh, so this graph is undirected.",
                    "label": 0
                },
                {
                    "sent": "So my probability matrix is symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "Even if I started at random point?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This right, these are these two elements and they are the same since since the Matrix since the graph is undirected, the matrix should be like.",
                    "label": 0
                },
                {
                    "sent": "And this is what we got right?",
                    "label": 0
                },
                {
                    "sent": "So this is also nice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But yeah, now I mean now the next question is, can you say anything about right parameters are similar?",
                    "label": 0
                },
                {
                    "sent": "Can you say anything from the parameters?",
                    "label": 0
                },
                {
                    "sent": "What is also interesting is why this went so high right?",
                    "label": 0
                },
                {
                    "sent": "So here it went to .99 and I would like artificially.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Keep it below 99, so maybe you should go to one, but then why would you want to have like a very dense core of the network and so on?",
                    "label": 0
                },
                {
                    "sent": "I mean there are more questions than answers here.",
                    "label": 0
                },
                {
                    "sent": "Or there is a lot of opportunities for future work to be more politically correct.",
                    "label": 0
                },
                {
                    "sent": "Did you try to fly?",
                    "label": 0
                },
                {
                    "sent": "This forest fire morning.",
                    "label": 0
                },
                {
                    "sent": "That would be even more set because it just has just two parameters.",
                    "label": 0
                },
                {
                    "sent": "It would be more said right here.",
                    "label": 0
                },
                {
                    "sent": "The world reduces to four.",
                    "label": 0
                },
                {
                    "sent": "Then it would reduce the two.",
                    "label": 0
                },
                {
                    "sent": "But in the previous case, it would be possible.",
                    "label": 0
                },
                {
                    "sent": "So to generate the model I mean this Chronicle graph and compare it.",
                    "label": 0
                },
                {
                    "sent": "Delete one.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, you are comparing to the statistical parameters, but I mean the real one.",
                    "label": 0
                },
                {
                    "sent": "Preceded by even more freedom or whatever.",
                    "label": 0
                },
                {
                    "sent": "More detail you mean you could plot them and try to visually compare them, but then you could go to 100 nodes and.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not right.",
                    "label": 0
                },
                {
                    "sent": "I'm yeah, I know that you are not in there.",
                    "label": 0
                },
                {
                    "sent": "That would be one option.",
                    "label": 0
                },
                {
                    "sent": "So the idea would be to take a small real graph like I know some social network with 10 people.",
                    "label": 0
                },
                {
                    "sent": "Or let's say what 24 or 32 people and then try to try to feed the Chronicle and see whether it works.",
                    "label": 0
                },
                {
                    "sent": "In yeah I, I think it wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "In case because of another characteristic of a graph.",
                    "label": 0
                },
                {
                    "sent": "So how how good can come Bhutanese amorphism?",
                    "label": 0
                },
                {
                    "sent": "Can you get on how many notes or something like that?",
                    "label": 0
                },
                {
                    "sent": "So that you know this is what that means because I know how correct correct my approach.",
                    "label": 0
                },
                {
                    "sent": "No, no, I'm I'm missing a lot of things.",
                    "label": 0
                },
                {
                    "sent": "I mean, I just didn't tell you about those.",
                    "label": 0
                },
                {
                    "sent": "But can you get?",
                    "label": 0
                },
                {
                    "sent": "Can you get like?",
                    "label": 0
                },
                {
                    "sent": "He isn't isomorphism.",
                    "label": 0
                },
                {
                    "sent": "Just yes, nothing.",
                    "label": 0
                },
                {
                    "sent": "Can you say up to you can't write?",
                    "label": 0
                },
                {
                    "sent": "It's hot, but still right.",
                    "label": 0
                },
                {
                    "sent": "There is no 90% isomorphic.",
                    "label": 0
                },
                {
                    "sent": "It's zero or one so.",
                    "label": 0
                },
                {
                    "sent": "So sorry.",
                    "label": 0
                },
                {
                    "sent": "Number that's there is no really good measure for comparing graphs.",
                    "label": 0
                },
                {
                    "sent": "So you can't do like number of changes.",
                    "label": 0
                },
                {
                    "sent": "I need to make on this one to get the other one.",
                    "label": 0
                },
                {
                    "sent": "Draft rules don't work really in practice, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, Addison system graphs.",
                    "label": 0
                },
                {
                    "sent": "No way you can do edit distances on trees.",
                    "label": 0
                },
                {
                    "sent": "That's that's like computable for edge distances on graphs.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work right for isomorphism test testing, it's somewhere between P and NP, and nobody really knows, right?",
                    "label": 0
                },
                {
                    "sent": "I think it's as hard as factoring so in factoring it's not clear on which side it is, so it's hard for.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean you could do graph kernels, but graph.",
                    "label": 0
                },
                {
                    "sent": "Yeah what you could also do right?",
                    "label": 0
                },
                {
                    "sent": "You could go and extract all possible subgraphs and have like a sparse vector, right?",
                    "label": 0
                },
                {
                    "sent": "So you have like you go and now every possible sub graph is a word and now you describe your graph with all possible subgraphs.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Could this could be what I mean.",
                    "label": 0
                },
                {
                    "sent": "The kernels do random walks on graphs and that's also.",
                    "label": 0
                },
                {
                    "sent": "And usually they have labeled graphs, right?",
                    "label": 0
                },
                {
                    "sent": "And then they are comparing labels.",
                    "label": 0
                },
                {
                    "sent": "I have no labels.",
                    "label": 0
                },
                {
                    "sent": "Right, So what?",
                    "label": 0
                },
                {
                    "sent": "Maybe Giannis will correctly, but as far as or how some random kernels on random walks work is basically we're doing a random walk on the network and you are like writing down the labels you saw and then you are comparing these labels.",
                    "label": 0
                },
                {
                    "sent": "Switch match the distribution of this substance.",
                    "label": 0
                },
                {
                    "sent": "That would be an option.",
                    "label": 0
                },
                {
                    "sent": "For big graphs this should work, but then yeah.",
                    "label": 0
                },
                {
                    "sent": "But then extracting subgraphs and counting them is again a nontrivial problem.",
                    "label": 0
                },
                {
                    "sent": "Kompleksov, sorry.",
                    "label": 0
                },
                {
                    "sent": "Subgraphs management, let's say for money.",
                    "label": 0
                },
                {
                    "sent": "But molecules are labeled right, so you can cut your search space when you are doing the isomorphism testing you can.",
                    "label": 0
                },
                {
                    "sent": "You can just additional constraints.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and it makes it makes the thing scalable.",
                    "label": 0
                },
                {
                    "sent": "OK. Quick question.",
                    "label": 0
                },
                {
                    "sent": "Personal phone.",
                    "label": 0
                },
                {
                    "sent": "We take both.",
                    "label": 0
                },
                {
                    "sent": "OK, match one more nickel.",
                    "label": 0
                }
            ]
        }
    }
}