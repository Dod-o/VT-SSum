{
    "id": "lbvvvcinplqlacpazbjzxkmmjyera342",
    "title": "Multi-Stage Multi-Task Feature Learning",
    "info": {
        "author": [
            "Changshui Zhang, Tsinghua University"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_zhang_learning/",
    "segmentation": [
        [
            "I in multitask learning we assume for a certain feature is a shared by some task but not necessary by all.",
            "The task in this kind of multitasking feature learning we formulated at optimization problem and here LW is loss function.",
            "An RW is a regularization term and our learning goal is to find the W."
        ],
        [
            "Anna 433 function here they are proposed by others.",
            "They are convex function so.",
            "But the problem is when W is big the these terms are too big.",
            "So they called it over Panelization.",
            "So in this paper we propose to use this function here and here.",
            "D is the number of the task and we draw the inside number.",
            "And below from.",
            "From this we can see near the Central Park central area.",
            "This function is L1 and the other in the other area is LO number.",
            "So this function can be see is between the L1 norm and L0 norm.",
            "So we think we think it's more reasonable.",
            "But the problem is is no convex so it's difficult to optimize."
        ],
        [
            "And so we proposed a multi task multi stage multi task feature learning algorithm.",
            "In our algorithm there three step.",
            "The first step is is initialization and then we repeat Step 2 and step 3IN Step 2 and we can see when I'll hear L is 1.",
            "This is a lost soul and basically you general the general.",
            "This is related less so.",
            "Basically we fixed Lambda and we find the W in Step 3 and we fixed the W and the updated Lambda and.",
            "This this algorithm is easy, it's simple, but."
        ],
        [
            "Formulate, but the performance is good and we calculate the error bound between the WL and best W. The error bound has two parts and we know when L is is 1.",
            "This is lost so so we can see compared to the second part, the first part is big.",
            "If the dimensionality number D is big, but very Interestingly and well iteration number L is big.",
            "The this this item is very goes to a very small number very quickly, so we test our algorithm on the real data set real data set.",
            "The results are good.",
            "So for more detailed information please come to our poster.",
            "TH 29 thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I in multitask learning we assume for a certain feature is a shared by some task but not necessary by all.",
                    "label": 0
                },
                {
                    "sent": "The task in this kind of multitasking feature learning we formulated at optimization problem and here LW is loss function.",
                    "label": 0
                },
                {
                    "sent": "An RW is a regularization term and our learning goal is to find the W.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anna 433 function here they are proposed by others.",
                    "label": 0
                },
                {
                    "sent": "They are convex function so.",
                    "label": 0
                },
                {
                    "sent": "But the problem is when W is big the these terms are too big.",
                    "label": 0
                },
                {
                    "sent": "So they called it over Panelization.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we propose to use this function here and here.",
                    "label": 0
                },
                {
                    "sent": "D is the number of the task and we draw the inside number.",
                    "label": 0
                },
                {
                    "sent": "And below from.",
                    "label": 0
                },
                {
                    "sent": "From this we can see near the Central Park central area.",
                    "label": 0
                },
                {
                    "sent": "This function is L1 and the other in the other area is LO number.",
                    "label": 0
                },
                {
                    "sent": "So this function can be see is between the L1 norm and L0 norm.",
                    "label": 0
                },
                {
                    "sent": "So we think we think it's more reasonable.",
                    "label": 0
                },
                {
                    "sent": "But the problem is is no convex so it's difficult to optimize.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we proposed a multi task multi stage multi task feature learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "In our algorithm there three step.",
                    "label": 0
                },
                {
                    "sent": "The first step is is initialization and then we repeat Step 2 and step 3IN Step 2 and we can see when I'll hear L is 1.",
                    "label": 0
                },
                {
                    "sent": "This is a lost soul and basically you general the general.",
                    "label": 0
                },
                {
                    "sent": "This is related less so.",
                    "label": 0
                },
                {
                    "sent": "Basically we fixed Lambda and we find the W in Step 3 and we fixed the W and the updated Lambda and.",
                    "label": 0
                },
                {
                    "sent": "This this algorithm is easy, it's simple, but.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formulate, but the performance is good and we calculate the error bound between the WL and best W. The error bound has two parts and we know when L is is 1.",
                    "label": 0
                },
                {
                    "sent": "This is lost so so we can see compared to the second part, the first part is big.",
                    "label": 0
                },
                {
                    "sent": "If the dimensionality number D is big, but very Interestingly and well iteration number L is big.",
                    "label": 0
                },
                {
                    "sent": "The this this item is very goes to a very small number very quickly, so we test our algorithm on the real data set real data set.",
                    "label": 0
                },
                {
                    "sent": "The results are good.",
                    "label": 0
                },
                {
                    "sent": "So for more detailed information please come to our poster.",
                    "label": 0
                },
                {
                    "sent": "TH 29 thank you.",
                    "label": 0
                }
            ]
        }
    }
}