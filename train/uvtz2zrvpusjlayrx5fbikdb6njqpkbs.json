{
    "id": "uvtz2zrvpusjlayrx5fbikdb6njqpkbs",
    "title": "Convex structure learning in log-linear models beyond pairwise potentials",
    "info": {
        "author": [
            "Mark Schmidt, Department of Computer Science, University of British Columbia"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/aistats2010_schmidt_cslil/",
    "segmentation": [
        [
            "OK, so I'm going to talk about convex structure learning and log linear models where we want to actually include potentials that aren't just pairwise, so this is fairly complementary to the invited talk this morning where John talked about.",
            "Well, all these methods for convex structure learning and also some interesting results in the continuous case.",
            "So here we're looking at the discrete case an using log linear models which were mentioned this morning as what was called a nonparametric method for the discrete case.",
            "And this is joint work with my advisor, Kevin Murphy."
        ],
        [
            "At UBC.",
            "So I'll start with just a very brief review of the problem, which is short and in light of the talk this morning, and then our contribution and then I'll talk about log linear models with higher order."
        ],
        [
            "Mitchell's, and then our formulation, and how we solve the optimization problem.",
            "And then a few experiments.",
            "So basically we are looking at the problem of fitting a density model to some discrete data where we don't know don't know the structure either.",
            "Don't want to assume this structure apriori.",
            "So a lot of authors have recently looked at using L1 regularization to address this."
        ],
        [
            "Home it's very nice 'cause it combines both regularization and structure.",
            "Learning into a convex framework.",
            "1st works that looked at this.",
            "Looked at the Gaussian case looking at Gaussian graphical models such as the graphical lasso.",
            "Another recent works have considered log linear models with discrete data.",
            "So we can just write.",
            "We can give an example here.",
            "So if we have a pairwise undirected graphical model, we can write it in this form where we put by pairwise.",
            "I mean we're only putting potentials on pairs of variables and single variables, so it's sort of this globalized globally normalized distribution where we have a potential in each pair and on each variable.",
            "And if we assume that we."
        ],
        [
            "Parameterized are potentials such that if you set all of the parameters associated with the potential to zero, it becomes equivalent to removing the edge.",
            "And in."
        ],
        [
            "That case we can use group L1 regularization to do the simultaneous parameter estimation and structure learning.",
            "So some people are kind of confused when they see this because it says L1 regularization.",
            "But there's no absolute values there.",
            "So what it is is actually L1 regularization of the norms of the groups.",
            "The length of the vectors and since the links are not negative, you don't actually need the absolute values there.",
            "So that basically encompasses the graphical lasso and the icing model model and other things that have been proposed."
        ],
        [
            "So there's a lot of work on this topic recently, so here's a definitely incomplete list of recent papers on this topic, and I'm not even including many of the papers that were at the conference here, but a lot of these papers make the pairwise sum."
        ],
        [
            "And they they assume that you only factorize according to pairs of variables.",
            "So like the 1st works, they looked at.",
            "This looked at the Gaussian case.",
            "So there the pairwise assumption is."
        ],
        [
            "Sort of built in and then the 1st works to look at discrete data.",
            "Also made the pairwise assumption they were looking at these."
        ],
        [
            "Pricing models and then these papers also made the pairwise assumption.",
            "So with only one exception, all previous work on this problem has only looked at pairwise."
        ],
        [
            "Models.",
            "So it's built into Gaussian models, so Gaussian models you can't really criticize that you can criticize the Gaussian assumption, but Gaussian models?",
            "That's not really."
        ],
        [
            "An issue but for log linear models.",
            "That's not the pairwise assumption really hasn't been traditionally used, so you can go all the way back to work in the 70s.",
            "This textbook by Bishop from the 70s where they didn't need to assume the pairwise assumption, so it's not really clear why we need to make the pairwise assumption.",
            "You know, after that, you know.",
            "Intel came along and gave us Pentiums and GPU's and all that stuff."
        ],
        [
            "So the pairwise assumption is going to be restrictive if the higher statistics for your data actually matter.",
            "So a case in biology is that in the development of tumors and cancer, usually you have to have multiple breakdowns in the self process before you actually get cancer, so you actually need to knockout both gene A and gene B.",
            "They both have to have a mutation before you actually get cancer, so there you would need like a 3 way factor between gene A, gene B and the cancer variable."
        ],
        [
            "So in this work we want to think about going beyond pairwise potentials.",
            "We want to think about including higher order potentials.",
            "So the challenge there is just."
        ],
        [
            "There's an exponential number of possible higher order potentials to consider.",
            "So that's mainly the reason why the prior work has looked at pairwise potentials, and obviously it's a very hard issue to deal with."
        ],
        [
            "So we're going to consider one special case.",
            "The case of hierarchical log linear models, which are well studied in statistics."
        ],
        [
            "We're going to give a convex formulation formulation of learning hierarchical log linear models using overlapping group Ellen Regularization."
        ],
        [
            "And then we're going to talk about an active set method that's going to rule out many of the higher order potentials to actually let us do."
        ],
        [
            "This tractably and actually solve the optimization problem.",
            "Going to talk about projected gradient method where we use die clicks cyclic projection algorithm."
        ],
        [
            "So I'm just going to review log linear models and hierarchical log linear models and then give our convex formulation."
        ],
        [
            "So here we write instead of having that product over pairs of variables, we use a globally normalized product over all possible subsets of the variables.",
            "So we have this non negative potential function and each to make it log linear model.",
            "Each of those potentials its logarithm is going to be a linear function."
        ],
        [
            "And then we're going to consider basically a full parameterisation of these potential functions, and then also a more parsimonious weighted icing parametrization."
        ],
        [
            "So I'm giving the for.",
            "For three way potentials on binary nodes, the full privatisation looks like this.",
            "We basically have an indicator function on every possible set of states that they can take and then we have a parameter associated with each one."
        ],
        [
            "So each potential is going to have an exponential number of parameters, and if you set all of those parameters to zero, it's equivalent to removing the potential from the model.",
            "Because you're setting the log potential to 0, so it's equivalent to multiplying the distribution by one for any choice of X."
        ],
        [
            "And you get the pairwise models in this special case where you just set WA to zero whenever the cardinality is greater than two.",
            "So this sort."
        ],
        [
            "Straightforward way to extend the work on pairwise models to the general case is you basically solve this optimization problem.",
            "You put the group L1 penalty on the parameters associated with each of the potentials."
        ],
        [
            "So one of the problems with this is that actually setting the parameters to 0 doesn't actually correspond to conditional independence in the model, so it's like if you set a two way factor to zero and you knocked it out of the model that you still have the Thruway factor, then you know you're not actually inducing a conditional independence in the distribution, so the models I'm going to talk about on the next slide are one of the main advantages of them is their interpretability.",
            "In terms of conditional independence and the other problem is just there's an exponential number of variables here, so if you don't put a cardinality restriction like assume.",
            "Sing paralyzer through."
        ],
        [
            "Your models, then you can't really solve the problem.",
            "So instead of using a cardinality restriction, we're going to consider the hierarchal inclusion restriction, which is that if potential is 0, then you're going to enforce that all it's super set."
        ],
        [
            "So we're going to be 0.",
            "So basically that means we can only have a factor on one, two, and three.",
            "If we also have factors on one 213."
        ],
        [
            "23 So this is the well known class of hierarchal log linear models."
        ],
        [
            "Which are well studied.",
            "And it's much larger than the set of pairwise models, and it is in some sense you can still model any distribution with a hierarchal log linear model, so it's not restricted in what you can."
        ],
        [
            "Approximate and if you actually have that heracle assumption, then group sparsity does correspond to conditional independence.",
            "So if you set a variable to zero, it actually does induce a conditional independency in the day."
        ],
        [
            "Tribu Shun, but you can actually.",
            "You can't enforce this constraint with disjoint group Ellen Regularization.",
            "So group L1 doesn't really know about the hierarchy.",
            "It really can only enforce that groups or variables on or off."
        ],
        [
            "Together but there's been recent work.",
            "This looked at overlapping group Ellen Regularization to enforce this sort of hierarchy, so the original works on this where work by Francis Bach and work by jowl.",
            "And there's been a lot of recent work, including several posters at this at this conference.",
            "So."
        ],
        [
            "Give an example of how you use overlapping groups to enforce hierarchy.",
            "So if we want to enforce that BA0 whenever a zero, we just make these two groups B&AB and then we use a regularizer that looks like that, and the basic idea is that if B is non zero then this regularizer is going to be smooth with respect to a, so it acts like an L2 regularizer with respect to a."
        ],
        [
            "So generalizing that basic idea, we can give a convex formulation formulation of structure learning, hierarchical log linear models.",
            "So where we basically put a group L1 penalty on each subset, but for each subset you add to the groups all of its possible supersets.",
            "Anne."
        ],
        [
            "And so under reasonable assumptions, you can show that a minimizer of this convex optimization problem will be a hierarchical log linear model, and the assumptions are basically the same assumptions you would use to show that L2 regularization yields a dense solution, and then, since I don't want to keep writing that whole thing, I'm just going to write it in this way and use WA star, where that includes all the parameters WA and all the parameters associated with all supersets of a."
        ],
        [
            "OK, so basically we've taken a problem with an exponential number of variables and turned it into a harder problem with an exponential number of variables."
        ],
        [
            "So we want to avoid having to consider the exponential number of possible potentials, but we know the solution is going."
        ],
        [
            "The hierarchal so the heuristic we're going to use is we're only going to consider groups that actually satisfy the hierarchy property.",
            "When we do the optimization."
        ],
        [
            "And Despite that, we can still guarantee a weak form of global optimality of the method.",
            "So some notation."
        ],
        [
            "And I'm going to call a Group A an active group if it's if it's non zero or some super set is non zero and I'm going to say."
        ],
        [
            "A group is inactive if basically it's not an active group and it also has some set of 0 so it wouldn't satisfy the hierarchy property."
        ],
        [
            "The other groups are going to be called boundary groups, so the boundary groups are the groups that we can make non zero and."
        ],
        [
            "Still satisfy the hierarchy property.",
            "So if we fix the."
        ],
        [
            "In active groups, then the optimality conditions with respect to the boundary groups reduced to this very simple form.",
            "So it's just that the gradient, the norm of the gradient with respect to the the group is just below a certain value."
        ],
        [
            "And so if the gradient is also zero for active groups, then these end up being necessary and sufficient optimality conditions for the global problem, and they're also necessary conditions of global optimality."
        ],
        [
            "So similar to work by Francis Bach, we're going to use an active set method.",
            "In ours.",
            "We're going to find the set of active groups and all the boundary groups that violate the necessary conditions, and then we're just going to solve the problem with respect to those variables and then just repeat this procedure.",
            "And it's not exactly the same 'cause they also consider finding sufficient variables, but we don't look at that in this work."
        ],
        [
            "So basically, the variables that end up getting added to the model are the ones that satisfy hierarchical inclusion an by that optimality condition which just falls from the subdifferential it adds groups that also are poorly estimated by the current model, because the gradient is the difference between the empirical and the model marginals.",
            "So if the model is not modeling a higher moment, well then it will add that."
        ],
        [
            "And this is analogous to methods that have been proposed.",
            "Greedy methods in AI in the 80s in the context of fitting maximum entropy distributions with marginal constraints."
        ],
        [
            "So I'm just going to sort of cartoon if the algorithm running.",
            "So I'm using the shaded nodes to denote inactive sets and then the full nodes denote boundary and then I'll active groups will be denoted by black.",
            "So initially we just have all the unary things and then we optimize."
        ],
        [
            "Is the initial boundary."
        ],
        [
            "Groups then we have some variables that are non Z."
        ],
        [
            "And those are the active groups.",
            "And then we find our new boundaries.",
            "So now we find all the variables that satisfy the groups that satisfy the hierarchy proper."
        ],
        [
            "So then we find the active groups and the suboptimal boundary groups the ones that our model is not modeling well and we."
        ],
        [
            "Otherwise, with respect to those."
        ],
        [
            "We get a new set of active groups."
        ],
        [
            "Add a new boundary."
        ],
        [
            "And we basically."
        ],
        [
            "Continue this process."
        ],
        [
            "And we."
        ],
        [
            "We add higher order."
        ],
        [
            "Doctors."
        ],
        [
            "And we keep going."
        ],
        [
            "Until basically we don't expand the boundary anymore, and at the point where we don't find any new variables to optimize, we've by construction satisfied the necessary."
        ],
        [
            "Anality conditions.",
            "So in that example, we only consider four out of the 10 possible through interactions and only one out of the 5 four way interactions, and we never needed to consider the five way interaction, and in general."
        ],
        [
            "The active set might actually save us from looking at an exponential number of higher order factors, which is what we want."
        ],
        [
            "So that's the active set method, but I haven't really talked about how we solve with respect to the variables, so that's what I'm going to talk about now."
        ],
        [
            "So it's basically a group L and regularization problem with overlapping groups."
        ],
        [
            "And we're going to write that sort of nonsmooth problem.",
            "It's non smooth, because when you set the all the variables in a group to 0, then the norm is non differentiable, so we're going to write this as a smooth problem with constraints called Euclidean norm constraints, which are part of a class that's called a simple constraints.",
            "And basically you just introduced these new variables.",
            "Jie, that bound the norms, and then you can clearly see that this is.",
            "This is an upper bound on that, and it's going to be tight at the minimizer."
        ],
        [
            "So we can we can draw the Euclidean norm cone quite easily in when we have W has two elements.",
            "So here I'm drawing the plane and then."
        ],
        [
            "It is the third axis, and then we put a cone at the origin there."
        ],
        [
            "So basically it doesn't really have texture or anything, but it's basically you put the point at the origin and then you just go off in every direction and then the feasible set is where the ice cream goes."
        ],
        [
            "So one way to optimize over simple sets is projected gradient methods.",
            "They're fairly widely used, and they have a very simple iteration.",
            "It's basically gradient descent, but you take the projection after you take the step."
        ],
        [
            "And that projection is just finding the closest point inside the feasible set.",
            "So in the case of the."
        ],
        [
            "The norm: It's very easy to compute that it's at the level of a textbook exercise, so it's very simple to compute that projection for a single group."
        ],
        [
            "So this is just a cartoon illustrating the gradient projection algorithm, so we have some feasible set which I'm denoting with this blue region and level sets of some function F of W and the current iterate WK.",
            "So we take."
        ],
        [
            "Our gradient step which is going to be orthogonal to the level curves, and then we compute the projection, which is the close."
        ],
        [
            "This point inside the set to our gradient step, and then we're going to search along the direction to that projection."
        ],
        [
            "So the basic projected gradient method converges very, very slow."
        ],
        [
            "Early, but there's lots of recent work on enhancements to it that actually make it converge very fast, so there's spectral projected gradient methods where they use the bars, Libor went step by step length, and then there's accelerated projected gradient methods where they add an extra extrapolation step and it proves nice theoretical results for strongly convex functions.",
            "And last year I talked about and it's sort of an LBFGS extension of this algorithm for certain functions which are approximately which can also be applied in this case.",
            "But"
        ],
        [
            "Not really going to talk about those today.",
            "The problem that arises in applying the methods is that although we can easily compute the projection onto each norm."
        ],
        [
            "Own the groups actually overlap, so we can't compute those projections independently.",
            "So what we?"
        ],
        [
            "She wanted to projection onto the intersection of simple sets, and this is actually a fairly."
        ],
        [
            "Well studied problem in various fields.",
            "So one of the."
        ],
        [
            "1st results on this problem was due to Von Neumann who showed that if you have two subspaces and you cyclically converge onto the two subspaces, that that the limit of that is the projection onto the intersection.",
            "So here's."
        ],
        [
            "The lecture notes from 1933 I've circled the relevant theorem, so you see on mathoverflow, there's people asking, should I learn latech?",
            "Yes, you should learn latech."
        ],
        [
            "So von Neumann's book doesn't have any pictures, so I'm just I've made a simple cartoon to demonstrate this theorem.",
            "So we have two subspaces.",
            "So in our two subspaces are obviously not interesting, you've just got lines going through the origin or R2 or the origin."
        ],
        [
            "And then we want to find the projection of a point onto their intersection, which is trivially solved in closed form in R2.",
            "But this is."
        ],
        [
            "Trace the algorithm nevertheless, so we find the closest point."
        ],
        [
            "In subspace one."
        ],
        [
            "And then we find the closest point in."
        ],
        [
            "Space 2."
        ],
        [
            "And then we sort of."
        ],
        [
            "Continue that and."
        ],
        [
            "You see what's happening?"
        ],
        [
            "Slowly converging."
        ],
        [
            "To the intersection.",
            "So the theorem says that the limit of that process is going to be the projection onto the Inter."
        ],
        [
            "Section.",
            "So that's fine, but what I'm actually interested is projecting onto the Euclidean norm cone and not subspaces.",
            "And we don't have just two, so Bregman."
        ],
        [
            "And along with many other people proposed, an algorithm to solve the convex feasibility problem where you have some number of convex sets and you want to compute a point in their intersection so we can illustrate that with."
        ],
        [
            "Nice cartoon, so I've got two convex sets here.",
            "A circle Anna rectange."
        ],
        [
            "And then we start with some point we produce."
        ],
        [
            "Get onto the circle."
        ],
        [
            "And then we project onto the rectange."
        ],
        [
            "And if you keep so you won't keep going back and forth 'cause we're already in the intersection, but the limit has."
        ],
        [
            "To be a point in the intersection, but in this case, and in general, that limit is not going to be the projection.",
            "The projection is really what we want, because that's the point we can prove that will always give us."
        ],
        [
            "Descent direction.",
            "So that does."
        ],
        [
            "Really give us the projection so the contribution of Dijkstra was to show that you can add an extra step to this algorithm and that'll make it converge to the projection."
        ],
        [
            "For general convex sets.",
            "So basically the same thing we want to project a point onto the."
        ],
        [
            "This section of convex sets, so we projected onto the first set and we also store the difference we used in making that."
        ],
        [
            "Projection, then we project onto the second set and store that difference.",
            "Now before we consider projection onto the."
        ],
        [
            "First set again.",
            "We actually removed the difference we had."
        ],
        [
            "Projection on the first step and then we do the projection and then we go back."
        ],
        [
            "To the second step."
        ],
        [
            "You can remove the difference and then."
        ],
        [
            "Do the projection and the theorem is that the limit of that process is going to be the projection onto the Intersect?"
        ],
        [
            "And more recently Deutschen handle showed that Dexter's algorithm is going to converge at a geometric rate for polyhedral sets.",
            "So it actually has we actually know how fast this will converge.",
            "Now.",
            "The set I'm optimizing is not actually polyhedral.",
            "If you use the Infinity norm of the groups, it will be a polyhedral, and they wrote when they wrote this paper they wrote it as part one and Part 2 is supposed to be general convex sets, but it never came out, but it's still reassuring to know that in some cases this algorithm.",
            "Will converge quickly."
        ],
        [
            "So I'm just going to detail a few experiments we did.",
            "You can see more in the paper and more in my thesis too.",
            "Um?"
        ],
        [
            "So does it empirically help to have higher potentials so related to the invited talk on the first day the best experiment is one you don't have to do, so obviously not all distributions were ever going to see in practice are going to be pairwise, so in some cases it will help to have higher potentials.",
            "It's a matter of can we actually estimate the parameters efficiently enough to give it a performance improvement on a real."
        ],
        [
            "Data set, so I'm just going to talk about this multivariate flow cytometry data.",
            "That's going to be that's analyzed quite a bit lately.",
            "It's just 11 variables in three states, so as long as the log linear model has some level sparsity, we can actually brute force compute the partition function."
        ],
        [
            "And on this data set, we compared using pairwise log linear models with either L2 regularization or group L1 regularization.",
            "And we also compared 3 way models with both those choices.",
            "And then we compared the new formulation for hierarchical models with overlapping group L1.",
            "And I'm sort of missing the hierarchal with L2, but you just there's too many variables you can't really do that fees abli.",
            "Well, at least not with the resource is available to me."
        ],
        [
            "So we trained on a third of the tests, a third of the data used 1/3.",
            "Just select the hyperparameter and then use 1/3.",
            "Is the test set for 10 splits."
        ],
        [
            "And then I'm displaying the results in this colored box plot.",
            "So I'm plotting the negative log likelihood.",
            "So you want to be lower and I'm plotting the relative score on each trial.",
            "So the worst method will get a score of 1 on each trial, and the best method will get a score of 0.",
            "And the trends are fairly clear here.",
            "3 way models give better test set performance than pairwise models.",
            "And then the hierarchical model seems to do better than the three way, and L1 seems to do a little bit better than our group.",
            "L1 rather seems to do a bit better than L2."
        ],
        [
            "So we also considered some slightly larger datasets.",
            "I'm just going to detail to here, so we looked at the traffic flow level, which was analyzed in AI stats last year and when I looked at the central 16 pixels in the USPS digits discretized into four States and here we use weighted icing potentials to not have an exponential number of parameters for each factor.",
            "And then I used a pseudo likelihood for training and testing so the traffic flow data is."
        ],
        [
            "Here, and it basically roughly shows the same trends and the USPS data also showed the same trend."
        ],
        [
            "And we've applied this to a lot more datasets and basically we found that the hierarchical model never hurts.",
            "In some cases it does about the same as pair.",
            "Why?",
            "But it never did worse, and in contrast the three models sometimes actually do do worse.",
            "Just because you have to actually estimate all those parameters at least three way with L2 sometimes does worse."
        ],
        [
            "So we also did a simple experiment on structural discovery.",
            "We wanted to know whether this model could conceivably recover an actual structure.",
            "So I generated a simple."
        ],
        [
            "Dataset where I'd potentials on 2, three and then a three way potential on 456 and a four way potential on 7, eight 910."
        ],
        [
            "And then, since it's actually a little bit hard to choose the regularization parameter for the structure estimation task I just recorded the number of false positives of different orders for the first model that actually includes the true model.",
            "So as an example of that."
        ],
        [
            "This is the order of variable additions.",
            "The model used with 20,000 samples, so first it adds 810 which is a subset of seven, eight, 910 and then it adds a bunch of other pairwise factors and the 1st through three way factor.",
            "There is eight 910 and then it adds another pair wise in a few 3 way and then it has a bunch of pairwise false positives, so it adds some things that aren't in it and then it eventually adds the other Thruway factors and then finally finds the four way factor."
        ],
        [
            "So I'm plotting here the number of false positives of different orders for the hierarchical model against the number of training examples and the point I want to.",
            "The point I thought it was kind of interesting here was that although it does have false positives, even with a very large number of samples, they tend to be lower order false positives, so it never actually introduces A5 way factor in any of our experiments.",
            "And it really once you get to maybe 10,000 samples, it never introduces any four way factors besides the truth.",
            "Norway factor and then the three way factors start disappearing, disappearing at around 25,000 samples too.",
            "So even though this is a model that actually has a four way factor, and it's finding the four way factor is not introducing any like Thruway factors, if you have enough samples or any false."
        ],
        [
            "Positive through factors.",
            "OK, so just to conclude."
        ],
        [
            "So the optimization algorithm here really doesn't make a whole lot of assumptions about the problem, so Dijkstra's algorithm might be useful for other overlapping group Eleanor regularization problems.",
            "And there was also work in the conference on another optimization or closely related method.",
            "You can also use.",
            "You can add covariates to do the model and learn like a conditional random field if you want to with higher order models and then of course the remaining issue with related to this work is trying to find inactive groups that don't satisfy sufficient optimality conditions.",
            "So one trick you can do is just try and look at an extended boundary and test the optimality conditions there."
        ],
        [
            "So before I end, I'll just summarize our contributions, so we gave a convex formalization formulation of structure learning and hierarchical."
        ],
        [
            "Log linear models.",
            "We gave a few methods to deal with the exponential number of variables."
        ],
        [
            "And we found that the new model that goes beyond pairwise potentials gives similar or better results on every data set.",
            "We've actually applied it to.",
            "So I'd like to thank the Program Committee for letting me speak and to the reviewers who gave us."
        ],
        [
            "A lot of very helpful comments, and as with all of my other papers, when I have time, the code that we used will be on my web page."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about convex structure learning and log linear models where we want to actually include potentials that aren't just pairwise, so this is fairly complementary to the invited talk this morning where John talked about.",
                    "label": 0
                },
                {
                    "sent": "Well, all these methods for convex structure learning and also some interesting results in the continuous case.",
                    "label": 1
                },
                {
                    "sent": "So here we're looking at the discrete case an using log linear models which were mentioned this morning as what was called a nonparametric method for the discrete case.",
                    "label": 1
                },
                {
                    "sent": "And this is joint work with my advisor, Kevin Murphy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At UBC.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with just a very brief review of the problem, which is short and in light of the talk this morning, and then our contribution and then I'll talk about log linear models with higher order.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mitchell's, and then our formulation, and how we solve the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And then a few experiments.",
                    "label": 0
                },
                {
                    "sent": "So basically we are looking at the problem of fitting a density model to some discrete data where we don't know don't know the structure either.",
                    "label": 1
                },
                {
                    "sent": "Don't want to assume this structure apriori.",
                    "label": 0
                },
                {
                    "sent": "So a lot of authors have recently looked at using L1 regularization to address this.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Home it's very nice 'cause it combines both regularization and structure.",
                    "label": 1
                },
                {
                    "sent": "Learning into a convex framework.",
                    "label": 0
                },
                {
                    "sent": "1st works that looked at this.",
                    "label": 0
                },
                {
                    "sent": "Looked at the Gaussian case looking at Gaussian graphical models such as the graphical lasso.",
                    "label": 0
                },
                {
                    "sent": "Another recent works have considered log linear models with discrete data.",
                    "label": 0
                },
                {
                    "sent": "So we can just write.",
                    "label": 0
                },
                {
                    "sent": "We can give an example here.",
                    "label": 0
                },
                {
                    "sent": "So if we have a pairwise undirected graphical model, we can write it in this form where we put by pairwise.",
                    "label": 1
                },
                {
                    "sent": "I mean we're only putting potentials on pairs of variables and single variables, so it's sort of this globalized globally normalized distribution where we have a potential in each pair and on each variable.",
                    "label": 1
                },
                {
                    "sent": "And if we assume that we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameterized are potentials such that if you set all of the parameters associated with the potential to zero, it becomes equivalent to removing the edge.",
                    "label": 0
                },
                {
                    "sent": "And in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That case we can use group L1 regularization to do the simultaneous parameter estimation and structure learning.",
                    "label": 1
                },
                {
                    "sent": "So some people are kind of confused when they see this because it says L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "But there's no absolute values there.",
                    "label": 0
                },
                {
                    "sent": "So what it is is actually L1 regularization of the norms of the groups.",
                    "label": 0
                },
                {
                    "sent": "The length of the vectors and since the links are not negative, you don't actually need the absolute values there.",
                    "label": 0
                },
                {
                    "sent": "So that basically encompasses the graphical lasso and the icing model model and other things that have been proposed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a lot of work on this topic recently, so here's a definitely incomplete list of recent papers on this topic, and I'm not even including many of the papers that were at the conference here, but a lot of these papers make the pairwise sum.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they they assume that you only factorize according to pairs of variables.",
                    "label": 0
                },
                {
                    "sent": "So like the 1st works, they looked at.",
                    "label": 0
                },
                {
                    "sent": "This looked at the Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "So there the pairwise assumption is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of built in and then the 1st works to look at discrete data.",
                    "label": 0
                },
                {
                    "sent": "Also made the pairwise assumption they were looking at these.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pricing models and then these papers also made the pairwise assumption.",
                    "label": 0
                },
                {
                    "sent": "So with only one exception, all previous work on this problem has only looked at pairwise.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "So it's built into Gaussian models, so Gaussian models you can't really criticize that you can criticize the Gaussian assumption, but Gaussian models?",
                    "label": 0
                },
                {
                    "sent": "That's not really.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An issue but for log linear models.",
                    "label": 0
                },
                {
                    "sent": "That's not the pairwise assumption really hasn't been traditionally used, so you can go all the way back to work in the 70s.",
                    "label": 0
                },
                {
                    "sent": "This textbook by Bishop from the 70s where they didn't need to assume the pairwise assumption, so it's not really clear why we need to make the pairwise assumption.",
                    "label": 1
                },
                {
                    "sent": "You know, after that, you know.",
                    "label": 0
                },
                {
                    "sent": "Intel came along and gave us Pentiums and GPU's and all that stuff.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the pairwise assumption is going to be restrictive if the higher statistics for your data actually matter.",
                    "label": 1
                },
                {
                    "sent": "So a case in biology is that in the development of tumors and cancer, usually you have to have multiple breakdowns in the self process before you actually get cancer, so you actually need to knockout both gene A and gene B.",
                    "label": 1
                },
                {
                    "sent": "They both have to have a mutation before you actually get cancer, so there you would need like a 3 way factor between gene A, gene B and the cancer variable.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we want to think about going beyond pairwise potentials.",
                    "label": 1
                },
                {
                    "sent": "We want to think about including higher order potentials.",
                    "label": 0
                },
                {
                    "sent": "So the challenge there is just.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's an exponential number of possible higher order potentials to consider.",
                    "label": 0
                },
                {
                    "sent": "So that's mainly the reason why the prior work has looked at pairwise potentials, and obviously it's a very hard issue to deal with.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to consider one special case.",
                    "label": 0
                },
                {
                    "sent": "The case of hierarchical log linear models, which are well studied in statistics.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to give a convex formulation formulation of learning hierarchical log linear models using overlapping group Ellen Regularization.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we're going to talk about an active set method that's going to rule out many of the higher order potentials to actually let us do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This tractably and actually solve the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Going to talk about projected gradient method where we use die clicks cyclic projection algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to review log linear models and hierarchical log linear models and then give our convex formulation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we write instead of having that product over pairs of variables, we use a globally normalized product over all possible subsets of the variables.",
                    "label": 0
                },
                {
                    "sent": "So we have this non negative potential function and each to make it log linear model.",
                    "label": 0
                },
                {
                    "sent": "Each of those potentials its logarithm is going to be a linear function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we're going to consider basically a full parameterisation of these potential functions, and then also a more parsimonious weighted icing parametrization.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm giving the for.",
                    "label": 0
                },
                {
                    "sent": "For three way potentials on binary nodes, the full privatisation looks like this.",
                    "label": 0
                },
                {
                    "sent": "We basically have an indicator function on every possible set of states that they can take and then we have a parameter associated with each one.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each potential is going to have an exponential number of parameters, and if you set all of those parameters to zero, it's equivalent to removing the potential from the model.",
                    "label": 0
                },
                {
                    "sent": "Because you're setting the log potential to 0, so it's equivalent to multiplying the distribution by one for any choice of X.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get the pairwise models in this special case where you just set WA to zero whenever the cardinality is greater than two.",
                    "label": 0
                },
                {
                    "sent": "So this sort.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straightforward way to extend the work on pairwise models to the general case is you basically solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You put the group L1 penalty on the parameters associated with each of the potentials.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the problems with this is that actually setting the parameters to 0 doesn't actually correspond to conditional independence in the model, so it's like if you set a two way factor to zero and you knocked it out of the model that you still have the Thruway factor, then you know you're not actually inducing a conditional independence in the distribution, so the models I'm going to talk about on the next slide are one of the main advantages of them is their interpretability.",
                    "label": 0
                },
                {
                    "sent": "In terms of conditional independence and the other problem is just there's an exponential number of variables here, so if you don't put a cardinality restriction like assume.",
                    "label": 1
                },
                {
                    "sent": "Sing paralyzer through.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your models, then you can't really solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of using a cardinality restriction, we're going to consider the hierarchal inclusion restriction, which is that if potential is 0, then you're going to enforce that all it's super set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to be 0.",
                    "label": 0
                },
                {
                    "sent": "So basically that means we can only have a factor on one, two, and three.",
                    "label": 1
                },
                {
                    "sent": "If we also have factors on one 213.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "23 So this is the well known class of hierarchal log linear models.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which are well studied.",
                    "label": 0
                },
                {
                    "sent": "And it's much larger than the set of pairwise models, and it is in some sense you can still model any distribution with a hierarchal log linear model, so it's not restricted in what you can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate and if you actually have that heracle assumption, then group sparsity does correspond to conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So if you set a variable to zero, it actually does induce a conditional independency in the day.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tribu Shun, but you can actually.",
                    "label": 0
                },
                {
                    "sent": "You can't enforce this constraint with disjoint group Ellen Regularization.",
                    "label": 1
                },
                {
                    "sent": "So group L1 doesn't really know about the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "It really can only enforce that groups or variables on or off.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together but there's been recent work.",
                    "label": 0
                },
                {
                    "sent": "This looked at overlapping group Ellen Regularization to enforce this sort of hierarchy, so the original works on this where work by Francis Bach and work by jowl.",
                    "label": 0
                },
                {
                    "sent": "And there's been a lot of recent work, including several posters at this at this conference.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give an example of how you use overlapping groups to enforce hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So if we want to enforce that BA0 whenever a zero, we just make these two groups B&AB and then we use a regularizer that looks like that, and the basic idea is that if B is non zero then this regularizer is going to be smooth with respect to a, so it acts like an L2 regularizer with respect to a.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So generalizing that basic idea, we can give a convex formulation formulation of structure learning, hierarchical log linear models.",
                    "label": 0
                },
                {
                    "sent": "So where we basically put a group L1 penalty on each subset, but for each subset you add to the groups all of its possible supersets.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so under reasonable assumptions, you can show that a minimizer of this convex optimization problem will be a hierarchical log linear model, and the assumptions are basically the same assumptions you would use to show that L2 regularization yields a dense solution, and then, since I don't want to keep writing that whole thing, I'm just going to write it in this way and use WA star, where that includes all the parameters WA and all the parameters associated with all supersets of a.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically we've taken a problem with an exponential number of variables and turned it into a harder problem with an exponential number of variables.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to avoid having to consider the exponential number of possible potentials, but we know the solution is going.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hierarchal so the heuristic we're going to use is we're only going to consider groups that actually satisfy the hierarchy property.",
                    "label": 0
                },
                {
                    "sent": "When we do the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Despite that, we can still guarantee a weak form of global optimality of the method.",
                    "label": 0
                },
                {
                    "sent": "So some notation.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to call a Group A an active group if it's if it's non zero or some super set is non zero and I'm going to say.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A group is inactive if basically it's not an active group and it also has some set of 0 so it wouldn't satisfy the hierarchy property.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other groups are going to be called boundary groups, so the boundary groups are the groups that we can make non zero and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still satisfy the hierarchy property.",
                    "label": 0
                },
                {
                    "sent": "So if we fix the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In active groups, then the optimality conditions with respect to the boundary groups reduced to this very simple form.",
                    "label": 0
                },
                {
                    "sent": "So it's just that the gradient, the norm of the gradient with respect to the the group is just below a certain value.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if the gradient is also zero for active groups, then these end up being necessary and sufficient optimality conditions for the global problem, and they're also necessary conditions of global optimality.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So similar to work by Francis Bach, we're going to use an active set method.",
                    "label": 0
                },
                {
                    "sent": "In ours.",
                    "label": 0
                },
                {
                    "sent": "We're going to find the set of active groups and all the boundary groups that violate the necessary conditions, and then we're just going to solve the problem with respect to those variables and then just repeat this procedure.",
                    "label": 1
                },
                {
                    "sent": "And it's not exactly the same 'cause they also consider finding sufficient variables, but we don't look at that in this work.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, the variables that end up getting added to the model are the ones that satisfy hierarchical inclusion an by that optimality condition which just falls from the subdifferential it adds groups that also are poorly estimated by the current model, because the gradient is the difference between the empirical and the model marginals.",
                    "label": 0
                },
                {
                    "sent": "So if the model is not modeling a higher moment, well then it will add that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is analogous to methods that have been proposed.",
                    "label": 0
                },
                {
                    "sent": "Greedy methods in AI in the 80s in the context of fitting maximum entropy distributions with marginal constraints.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to sort of cartoon if the algorithm running.",
                    "label": 0
                },
                {
                    "sent": "So I'm using the shaded nodes to denote inactive sets and then the full nodes denote boundary and then I'll active groups will be denoted by black.",
                    "label": 0
                },
                {
                    "sent": "So initially we just have all the unary things and then we optimize.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the initial boundary.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Groups then we have some variables that are non Z.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And those are the active groups.",
                    "label": 1
                },
                {
                    "sent": "And then we find our new boundaries.",
                    "label": 0
                },
                {
                    "sent": "So now we find all the variables that satisfy the groups that satisfy the hierarchy proper.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we find the active groups and the suboptimal boundary groups the ones that our model is not modeling well and we.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise, with respect to those.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get a new set of active groups.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add a new boundary.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we basically.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue this process.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We add higher order.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctors.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we keep going.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until basically we don't expand the boundary anymore, and at the point where we don't find any new variables to optimize, we've by construction satisfied the necessary.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anality conditions.",
                    "label": 0
                },
                {
                    "sent": "So in that example, we only consider four out of the 10 possible through interactions and only one out of the 5 four way interactions, and we never needed to consider the five way interaction, and in general.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The active set might actually save us from looking at an exponential number of higher order factors, which is what we want.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the active set method, but I haven't really talked about how we solve with respect to the variables, so that's what I'm going to talk about now.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's basically a group L and regularization problem with overlapping groups.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to write that sort of nonsmooth problem.",
                    "label": 0
                },
                {
                    "sent": "It's non smooth, because when you set the all the variables in a group to 0, then the norm is non differentiable, so we're going to write this as a smooth problem with constraints called Euclidean norm constraints, which are part of a class that's called a simple constraints.",
                    "label": 1
                },
                {
                    "sent": "And basically you just introduced these new variables.",
                    "label": 0
                },
                {
                    "sent": "Jie, that bound the norms, and then you can clearly see that this is.",
                    "label": 0
                },
                {
                    "sent": "This is an upper bound on that, and it's going to be tight at the minimizer.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can we can draw the Euclidean norm cone quite easily in when we have W has two elements.",
                    "label": 0
                },
                {
                    "sent": "So here I'm drawing the plane and then.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is the third axis, and then we put a cone at the origin there.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically it doesn't really have texture or anything, but it's basically you put the point at the origin and then you just go off in every direction and then the feasible set is where the ice cream goes.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way to optimize over simple sets is projected gradient methods.",
                    "label": 1
                },
                {
                    "sent": "They're fairly widely used, and they have a very simple iteration.",
                    "label": 0
                },
                {
                    "sent": "It's basically gradient descent, but you take the projection after you take the step.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that projection is just finding the closest point inside the feasible set.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The norm: It's very easy to compute that it's at the level of a textbook exercise, so it's very simple to compute that projection for a single group.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a cartoon illustrating the gradient projection algorithm, so we have some feasible set which I'm denoting with this blue region and level sets of some function F of W and the current iterate WK.",
                    "label": 0
                },
                {
                    "sent": "So we take.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our gradient step which is going to be orthogonal to the level curves, and then we compute the projection, which is the close.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This point inside the set to our gradient step, and then we're going to search along the direction to that projection.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the basic projected gradient method converges very, very slow.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Early, but there's lots of recent work on enhancements to it that actually make it converge very fast, so there's spectral projected gradient methods where they use the bars, Libor went step by step length, and then there's accelerated projected gradient methods where they add an extra extrapolation step and it proves nice theoretical results for strongly convex functions.",
                    "label": 1
                },
                {
                    "sent": "And last year I talked about and it's sort of an LBFGS extension of this algorithm for certain functions which are approximately which can also be applied in this case.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not really going to talk about those today.",
                    "label": 0
                },
                {
                    "sent": "The problem that arises in applying the methods is that although we can easily compute the projection onto each norm.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Own the groups actually overlap, so we can't compute those projections independently.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She wanted to projection onto the intersection of simple sets, and this is actually a fairly.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well studied problem in various fields.",
                    "label": 0
                },
                {
                    "sent": "So one of the.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st results on this problem was due to Von Neumann who showed that if you have two subspaces and you cyclically converge onto the two subspaces, that that the limit of that is the projection onto the intersection.",
                    "label": 0
                },
                {
                    "sent": "So here's.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The lecture notes from 1933 I've circled the relevant theorem, so you see on mathoverflow, there's people asking, should I learn latech?",
                    "label": 0
                },
                {
                    "sent": "Yes, you should learn latech.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So von Neumann's book doesn't have any pictures, so I'm just I've made a simple cartoon to demonstrate this theorem.",
                    "label": 0
                },
                {
                    "sent": "So we have two subspaces.",
                    "label": 0
                },
                {
                    "sent": "So in our two subspaces are obviously not interesting, you've just got lines going through the origin or R2 or the origin.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we want to find the projection of a point onto their intersection, which is trivially solved in closed form in R2.",
                    "label": 0
                },
                {
                    "sent": "But this is.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trace the algorithm nevertheless, so we find the closest point.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In subspace one.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we find the closest point in.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space 2.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we sort of.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue that and.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see what's happening?",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slowly converging.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the intersection.",
                    "label": 0
                },
                {
                    "sent": "So the theorem says that the limit of that process is going to be the projection onto the Inter.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Section.",
                    "label": 0
                },
                {
                    "sent": "So that's fine, but what I'm actually interested is projecting onto the Euclidean norm cone and not subspaces.",
                    "label": 0
                },
                {
                    "sent": "And we don't have just two, so Bregman.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And along with many other people proposed, an algorithm to solve the convex feasibility problem where you have some number of convex sets and you want to compute a point in their intersection so we can illustrate that with.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice cartoon, so I've got two convex sets here.",
                    "label": 0
                },
                {
                    "sent": "A circle Anna rectange.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we start with some point we produce.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get onto the circle.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we project onto the rectange.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you keep so you won't keep going back and forth 'cause we're already in the intersection, but the limit has.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be a point in the intersection, but in this case, and in general, that limit is not going to be the projection.",
                    "label": 0
                },
                {
                    "sent": "The projection is really what we want, because that's the point we can prove that will always give us.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Descent direction.",
                    "label": 0
                },
                {
                    "sent": "So that does.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really give us the projection so the contribution of Dijkstra was to show that you can add an extra step to this algorithm and that'll make it converge to the projection.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For general convex sets.",
                    "label": 0
                },
                {
                    "sent": "So basically the same thing we want to project a point onto the.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This section of convex sets, so we projected onto the first set and we also store the difference we used in making that.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection, then we project onto the second set and store that difference.",
                    "label": 0
                },
                {
                    "sent": "Now before we consider projection onto the.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First set again.",
                    "label": 0
                },
                {
                    "sent": "We actually removed the difference we had.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection on the first step and then we do the projection and then we go back.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the second step.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can remove the difference and then.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do the projection and the theorem is that the limit of that process is going to be the projection onto the Intersect?",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_118": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more recently Deutschen handle showed that Dexter's algorithm is going to converge at a geometric rate for polyhedral sets.",
                    "label": 1
                },
                {
                    "sent": "So it actually has we actually know how fast this will converge.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The set I'm optimizing is not actually polyhedral.",
                    "label": 0
                },
                {
                    "sent": "If you use the Infinity norm of the groups, it will be a polyhedral, and they wrote when they wrote this paper they wrote it as part one and Part 2 is supposed to be general convex sets, but it never came out, but it's still reassuring to know that in some cases this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Will converge quickly.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to detail a few experiments we did.",
                    "label": 0
                },
                {
                    "sent": "You can see more in the paper and more in my thesis too.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So does it empirically help to have higher potentials so related to the invited talk on the first day the best experiment is one you don't have to do, so obviously not all distributions were ever going to see in practice are going to be pairwise, so in some cases it will help to have higher potentials.",
                    "label": 0
                },
                {
                    "sent": "It's a matter of can we actually estimate the parameters efficiently enough to give it a performance improvement on a real.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data set, so I'm just going to talk about this multivariate flow cytometry data.",
                    "label": 1
                },
                {
                    "sent": "That's going to be that's analyzed quite a bit lately.",
                    "label": 1
                },
                {
                    "sent": "It's just 11 variables in three states, so as long as the log linear model has some level sparsity, we can actually brute force compute the partition function.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And on this data set, we compared using pairwise log linear models with either L2 regularization or group L1 regularization.",
                    "label": 1
                },
                {
                    "sent": "And we also compared 3 way models with both those choices.",
                    "label": 0
                },
                {
                    "sent": "And then we compared the new formulation for hierarchical models with overlapping group L1.",
                    "label": 1
                },
                {
                    "sent": "And I'm sort of missing the hierarchal with L2, but you just there's too many variables you can't really do that fees abli.",
                    "label": 0
                },
                {
                    "sent": "Well, at least not with the resource is available to me.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we trained on a third of the tests, a third of the data used 1/3.",
                    "label": 1
                },
                {
                    "sent": "Just select the hyperparameter and then use 1/3.",
                    "label": 0
                },
                {
                    "sent": "Is the test set for 10 splits.",
                    "label": 1
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'm displaying the results in this colored box plot.",
                    "label": 0
                },
                {
                    "sent": "So I'm plotting the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So you want to be lower and I'm plotting the relative score on each trial.",
                    "label": 0
                },
                {
                    "sent": "So the worst method will get a score of 1 on each trial, and the best method will get a score of 0.",
                    "label": 0
                },
                {
                    "sent": "And the trends are fairly clear here.",
                    "label": 0
                },
                {
                    "sent": "3 way models give better test set performance than pairwise models.",
                    "label": 0
                },
                {
                    "sent": "And then the hierarchical model seems to do better than the three way, and L1 seems to do a little bit better than our group.",
                    "label": 0
                },
                {
                    "sent": "L1 rather seems to do a bit better than L2.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also considered some slightly larger datasets.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to detail to here, so we looked at the traffic flow level, which was analyzed in AI stats last year and when I looked at the central 16 pixels in the USPS digits discretized into four States and here we use weighted icing potentials to not have an exponential number of parameters for each factor.",
                    "label": 0
                },
                {
                    "sent": "And then I used a pseudo likelihood for training and testing so the traffic flow data is.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, and it basically roughly shows the same trends and the USPS data also showed the same trend.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we've applied this to a lot more datasets and basically we found that the hierarchical model never hurts.",
                    "label": 0
                },
                {
                    "sent": "In some cases it does about the same as pair.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "But it never did worse, and in contrast the three models sometimes actually do do worse.",
                    "label": 0
                },
                {
                    "sent": "Just because you have to actually estimate all those parameters at least three way with L2 sometimes does worse.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also did a simple experiment on structural discovery.",
                    "label": 0
                },
                {
                    "sent": "We wanted to know whether this model could conceivably recover an actual structure.",
                    "label": 0
                },
                {
                    "sent": "So I generated a simple.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dataset where I'd potentials on 2, three and then a three way potential on 456 and a four way potential on 7, eight 910.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then, since it's actually a little bit hard to choose the regularization parameter for the structure estimation task I just recorded the number of false positives of different orders for the first model that actually includes the true model.",
                    "label": 0
                },
                {
                    "sent": "So as an example of that.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the order of variable additions.",
                    "label": 0
                },
                {
                    "sent": "The model used with 20,000 samples, so first it adds 810 which is a subset of seven, eight, 910 and then it adds a bunch of other pairwise factors and the 1st through three way factor.",
                    "label": 0
                },
                {
                    "sent": "There is eight 910 and then it adds another pair wise in a few 3 way and then it has a bunch of pairwise false positives, so it adds some things that aren't in it and then it eventually adds the other Thruway factors and then finally finds the four way factor.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm plotting here the number of false positives of different orders for the hierarchical model against the number of training examples and the point I want to.",
                    "label": 1
                },
                {
                    "sent": "The point I thought it was kind of interesting here was that although it does have false positives, even with a very large number of samples, they tend to be lower order false positives, so it never actually introduces A5 way factor in any of our experiments.",
                    "label": 0
                },
                {
                    "sent": "And it really once you get to maybe 10,000 samples, it never introduces any four way factors besides the truth.",
                    "label": 0
                },
                {
                    "sent": "Norway factor and then the three way factors start disappearing, disappearing at around 25,000 samples too.",
                    "label": 0
                },
                {
                    "sent": "So even though this is a model that actually has a four way factor, and it's finding the four way factor is not introducing any like Thruway factors, if you have enough samples or any false.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positive through factors.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the optimization algorithm here really doesn't make a whole lot of assumptions about the problem, so Dijkstra's algorithm might be useful for other overlapping group Eleanor regularization problems.",
                    "label": 0
                },
                {
                    "sent": "And there was also work in the conference on another optimization or closely related method.",
                    "label": 0
                },
                {
                    "sent": "You can also use.",
                    "label": 0
                },
                {
                    "sent": "You can add covariates to do the model and learn like a conditional random field if you want to with higher order models and then of course the remaining issue with related to this work is trying to find inactive groups that don't satisfy sufficient optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "So one trick you can do is just try and look at an extended boundary and test the optimality conditions there.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I end, I'll just summarize our contributions, so we gave a convex formalization formulation of structure learning and hierarchical.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Log linear models.",
                    "label": 0
                },
                {
                    "sent": "We gave a few methods to deal with the exponential number of variables.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we found that the new model that goes beyond pairwise potentials gives similar or better results on every data set.",
                    "label": 1
                },
                {
                    "sent": "We've actually applied it to.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to thank the Program Committee for letting me speak and to the reviewers who gave us.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of very helpful comments, and as with all of my other papers, when I have time, the code that we used will be on my web page.",
                    "label": 0
                }
            ]
        }
    }
}