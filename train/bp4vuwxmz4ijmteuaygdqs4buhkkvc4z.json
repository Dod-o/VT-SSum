{
    "id": "bp4vuwxmz4ijmteuaygdqs4buhkkvc4z",
    "title": "Using upper confidence bounds to control exploration and exploitation",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning->Exploration vs. Exploitation"
        ]
    },
    "url": "http://videolectures.net/otee06_szepesvari_uucbc/",
    "segmentation": [
        [
            "Expectations.",
            "Thank you.",
            "Set."
        ],
        [
            "I have 40 minutes or something, right?",
            "Yeah.",
            "Just tell me if I'm ready.",
            "Stretching time or something please?",
            "So that is the contents.",
            "So first I'm going to introduce bandit problems and shortly talk about previous work and upper confidence based algorithms and.",
            "Then we are going to to consider special keys where actions take some random time and that introduces some additional complications and look at their approaches.",
            "Studio is those difficulties and we'll have a nice result I guess.",
            "And then I'm going to talk about what you could do if you have larger action spaces there.",
            "I'm still talking about discrete action spaces, but.",
            "We're talking about really, really large action action spaces, which are maybe exponentially in the size of some nature quantity.",
            "And conclusions."
        ],
        [
            "So maybe you are all familiar with exploration and exploitation tradeoff, so I can just split this, but I have this very nice.",
            "Picture showing two herbs.",
            "And yeah, the idea is that if he estimated quantity or estimates an uncertain an, you might underestimate something an underestimate the optimal.",
            "The value of the optimal choice and cares you're going to choose too many times the suboptimal one, or overestimate the suboptimal choice.",
            "And then you are going to suffer loss because of that, and so you can't trust your estimates and such a situation much D. So that's really the question that we're looking at."
        ],
        [
            "Yeah, and of course, if you just trust your estimates then you are exploiting an.",
            "If you are not.",
            "If you make that you don't trust your estimates for whatever reason that you exploit.",
            "Save exploration exploitation.",
            "While I guess has quite a few nice applications and.",
            "Of course, if you look at more complicated processes that add memory and state information than the range of applications explodes, and so we made some or freshly made some.",
            "Tremendous progress in the past, I guess.",
            "Exploration versus exploitation.",
            "But there's but still there are many, many challenges."
        ],
        [
            "Say the the very basic underlying principle that many of these algorithms using is called the optimism in the face of uncertainty principle, and probably it was introduced by Lauren Robinson.",
            "Yeah, Peters agrees.",
            "So I I pretty much sure then that this is a true thing.",
            "So the idea is is can be presented in very very simple case value.",
            "Just have two options which give you pay offs and these are the payouts is a sequence for option one and this sequence for option two and similarly to one of few previous talks.",
            "We assume that if you are basically consuming these sequences.",
            "So when you select an option T's time step, then you get that reward.",
            "And the principle says that.",
            "You should.",
            "You should not pretty ever hear the options by just taking maybe sample mean, but you should rather look at an optimistic way about what is the possible value that that option can verse.",
            "Given your past experience.",
            "Maybe you use a mother or maybe you don't need a model in order to have such an estimate, but this is basically the principle behind.",
            "Many of these arguments that we're talking about.",
            "And so why does this principle work?",
            "I have slide yeah, and so the idea is that we should select option with the best inflated value.",
            "So the idea of the core underlying idea is that if he.",
            "If you choose.",
            "The optimal von because still that has the best inflated value.",
            "Then that's good, but if you choose a suboptimal one that has a big influence, bigger inflated value than the other options, then you will automatically learn by this principle about the sub optimal suboptimal choice and the process will regulate itself.",
            "If you inflate the values and principles way.",
            "I think it's pretty clear."
        ],
        [
            "So.",
            "Lion Rob is looked at the cases where there are parametric distributions.",
            "We we have certain parametric forms for the pay offs and you don't know the values of the parameters.",
            "And then.",
            "Basically, the way they approach this problem is just to define an uncertain is set, so that's the set of parameters that look quite possible given the observation that you had before.",
            "Actually in algorithm key, usually you compute Kia Divergance or something like that.",
            "So you take the maximum likelihood estimator parameter.",
            "That gives you a probability distribution and then you say that every probability distribution which is which is within a certain every parameter value which gives you a probability distribution that is missing a certain distance to the estimated probability distribution is acceptable and you have to control.",
            "Of course the size of these distances.",
            "So.",
            "And you control the size by just defining large in terms of the number of trials that elapsed from the beginning and the number of choices that you made for an option.",
            "And then.",
            "Given given such an answer to the set, so let's eat eat, eat to denote the expected value.",
            "AXA, given a certain value of data under probability distribution, and then you just take the maximum of this.",
            "So you look at the future in optimistic variances."
        ],
        [
            "Pretty nice.",
            "And then you choose basically the rule which gives you the biggest inflated way.",
            "And what's even nicer is that they have nice upper bounds.",
            "Like TMJ FN here denotes the number of times an suboptimal arm was chosen.",
            "So J here is the index of sub optimal arm.",
            "And you have these lower ethnic dependence on the number of trials which is which is really good.",
            "That's very slow growth.",
            "So whenever the growth rate of choosing the optimal arms is less than linear, then you're basically converging and and logarithmic growth rate is really good.",
            "And what's even nicer is that they have matching lower bound, and so we know that the target and that I'm just sketched.",
            "It's just giving you the best possible way to control exploration and exploitation at least in asymptotic sense.",
            "So that's really nice."
        ],
        [
            "The problem is both cores that during the transient we don't know how these algorithm behaves and the other problem is that if you don't have parametric distributions than.",
            "The last thing you did.",
            "So.",
            "Just very recently by Peter and number of codes or save him introduced and studied this UCB one algorithm.",
            "Which takes this nonparametric.",
            "Here you assume that the rewards are bounded between zero and one, but you don't make any parametric assumptions whatsoever about the distribution of the rewards.",
            "Instead, you just compute the sample means and visited the compute an upper estimate on the variance, which is scared in a in a principled way.",
            "So the scaling is really interesting here, so if you so and GFT, here is a number of times are chavers chosen up to time time step T. And so if you don't select an arm, that is, bond grows and so you are going to select this arm ultimately.",
            "And the other hand, you can see that.",
            "The only way to to make this disappear is to select an army infinitely often so you will end up with selecting order arms in field often, but.",
            "It turns out that the number of crimes that you select as suboptimal arm is still arhythmic, and actually the bond holds for each end.",
            "So we have a regret bound here, so the regret is just the expected loss due to not selecting best option, time step and.",
            "And they are talking here, but the expected regret.",
            "And you can have a bond like this.",
            "And here that of I is the expected loss if you select arm I instead of the optimal.",
            "Or an optimal line?",
            "And so this is really nice design."
        ],
        [
            "And so in the first part I'm going to talk about how to use the same principle and build on this previous result.",
            "And used it in for bandits where actions take continuous time."
        ],
        [
            "So what is this problem that we're looking at?",
            "So the motivation is to look at maybe task scheduling so we have a finite number of types of tasks which are arriving randomly, and so this information about the type of the task is revealed to us.",
            "And then we have to make a decision.",
            "About how to actually process the task, maybe we have several ways to process a task, and depending on the type, some of the ways are more rewarding.",
            "Some of the ways are less rewarding.",
            "We can actually, maybe you can add an action that just drops a test.",
            "That's a certain way of processing and task is fun.",
            "And The thing is that once you decided how to process the tasks, so this is single machine problem, you have a single machine that's going to process attacks, so it will take some time until you finish processing the task and these times random random themselves as far.",
            "And we're looking at the non permit version of this problem where you don't know anything about this version of the test times tasks that are arriving and you don't know anything about the rewarded solutions or distribution of the delays.",
            "So I call the time to process a task.",
            "The execution delay of the corresponding action actually selecting they process the task.",
            "But we assume that times I hear, yeah, so we are under the simple framework then then we make this idea assumption.",
            "So you can wonder if you can extend this, of course by showing state things like that and then you are in a semi MDP favor.",
            "OK, so."
        ],
        [
            "Normally this is it, so we assume that we observe this ID Korea, so these are not the words.",
            "So these are the side information that we have.",
            "And there are a finite number of possible values that this side information can take.",
            "And we also assume that.",
            "In each time step, there is an IID sequence of rewards and an ID sequence of delays.",
            "And actually we are changing here.",
            "Seems a little bit.",
            "It doesn't really matter.",
            "So here RIT means that in discrete time step T in T's choice.",
            "What was the random reward at that?",
            "You're going to experience if the side information is acts, so that's the function.",
            "So we don't know these quantities, but we assume some boundedness, and so we're going to use.",
            "Are eye of X to denote the expected reward given that signed information is X and we choose option I and that of data.",
            "I ask students aim for the delays."
        ],
        [
            "OK.",
            "So.",
            "An allocation rule is basically a way to choose what to do next time step, and the choice of surgery will be denoted by capital IFT.",
            "And if you choose a particle way of what this is not not actually correct, I guess.",
            "Save.",
            "So that should be so if you choose a particle way to to process a pass, then you then you incur Allevard suffered late and that should be RIFTT.",
            "Of course at evaluated X of T, which is the side information that you have at time T and you have the same thing, this delay.",
            "And so if you have a policy you that certain way of selecting the arms or options, then you can think about that.",
            "Maybe the natural way to avoid such a policy is not just signing the rewards, because you can have an option that takes a long time to execute or via processing task and then therefore it doesn't really make sense to choose an option because.",
            "Maybe during that time you are going to miss some good opportunities to to process other tasks that are coming along.",
            "And so maybe a better way to measure performance in this case is just business ratio.",
            "There are many alternative ways, and you can play around with that by rather not go into that right now.",
            "So basically this ratio measures the delivered per timestamp, the expected departure time step if you want.",
            "So you want to maximize that annual Greg, that is, will be computed compare."
        ],
        [
            "To that.",
            "Select the model is that you get a new task as soon as the machines finish with no actually in.",
            "In every time step you get a new task.",
            "It doesn't really matter.",
            "Because everything is IID so.",
            "But yet so in that sense, you are right, so there is no additional delay.",
            "So once you're finished, there is a test there and you get.",
            "You can process any task.",
            "And so we're looking for a policy that maximizes the gain so that we are going to denote by Lambda Stern.",
            "And so it turns out that this just special case of Sammy and EPS and so you can compute so called action values.",
            "So let's call options right now actions and then you can given a side information X, you can just every it the option but just looking at the expected reward, but you subtract how much you lose during the time that you're going to execute option.",
            "Uh.",
            "Because.",
            "During that time you cannot choose any other option and you are going to lose an average Lambda star value and therefore given this amount of time your losses this so this is the true value of the tree value of the.",
            "Actions, and this is a true value in the sense that as well, that if you have access to this values then you can just compute an optimal policy by just OK. You have side information X, you just look at the option that maximize this action values and that gives you optimal policy.",
            "So that's a nice and easy way to compute an optimal policy.",
            "And the regret is redefined also to reflect the case that.",
            "Actions take some time, so one way to define the regret and there are other ways as well and.",
            "Yeah, I'm not sure that all of them are excellent.",
            "We can actually prove things about this regret, but some of them are excellent, but not all after.",
            "And so these regret just says so.",
            "This is kind of pseudo regret, so this thing that says that if I have a policy that is going to suffer these delays, I just some delays and compute how much on the average the optimal policy would give me during that time.",
            "And I subtract the total amount of rewarded.",
            "He said during that end time steps as far or an choices.",
            "So is that clear?"
        ],
        [
            "So one option would be that you are probably thinking of is to just estimate the underlying probability distribution for sign information and.",
            "The rewards, the expected rewards and the expected delays.",
            "And.",
            "And in line with the optimism in the face of uncertainty principle, you could say that so I have an estimate for these quantities.",
            "And so again, I can think about the models that are quite likely given the information that I have.",
            "Maybe you can define distances and change the distances with time and number of time steps you have chosen different things.",
            "And so you define this set MFT, which is the set of possible models given your past information that they sync a lightly and then you can just say that OK if I have such a model then I can avoid any policy here and let's find jointly the model and the policy that gives me the highest average reward.",
            "So that's fine.",
            "That would probably work.",
            "I guess.",
            "But maybe the problem is that.",
            "Regret will depend if you try to analyze such an algorithm then the regret is going to depend.",
            "I believe on the recipe rock of the smallest, why you have seeing some side information.",
            "So basically, if you want to estimate P and.",
            "And foot bones and P then.",
            "Then this is the quantity that governs how much you have to wait.",
            "Basically, in order to see sufficiently many examples for the particle state, and this quantity is going to dominate.",
            "This maximization program.",
            "And we want to avoid that.",
            "So is there any other way?",
            "To come up with with the principle way of selecting actions, trading off exploration, exploitation video estimating, the probability distribution with this question would be even more interesting if we work this continues site information.",
            "Because then we could argue that OK, we want to argue density estimation.",
            "Maybe density estimation is more difficult than the problem that we are trying to solve."
        ],
        [
            "Here.",
            "And yes, there is, so there's a little algorithm.",
            "It's not difficult as well.",
            "It's not difficult at all, so.",
            "Basically what we do here is that still we are using the same ideas.",
            "Maybe it's best to start breeding here, so we are estimating rewards delayes average reward and we're putting confidence intervals on top of them.",
            "And.",
            "The way it works is that.",
            "We are estimating the rewatchers usual way, so just taking averages.",
            "You can do that.",
            "We're in finite case.",
            "We're estimating delays in the same way as well, so trick is to estimate the average gain slightly differently.",
            "So we had several ideas about how to do that, and actually we're quite confident about that.",
            "If you just take maybe the average of.",
            "Given all past trials, then that that would work, but we actually could not prove that.",
            "So the trick here that makes the proof work, and I'm not sure if this actually needed for the algorithm is to compute the average dividing different way the way you compute this average reward is to look at to look at every policy, and for any policy you you compute, you can just compute the average reward by just looking at those time steps.",
            "Value are sort of following that policy.",
            "And.",
            "And you put the confidence interval on that.",
            "So this is the number of times you have followed that policy.",
            "TUFT, so you is a different index right now and then I saw that in this is police indexes policies.",
            "OK, and we take the maximum of this so.",
            "This is a cautious estimate.",
            "That So what we are trying to to avoid is to overestimate the average gain.",
            "If you overestimate the average game then you are substracting here value that's too big and then you are not going to work with upper confidence estimates.",
            "You are going to work with lower confidence or something as basically.",
            "And so the idea is to avoid that, but just substracting this confidence values and that that."
        ],
        [
            "Works.",
            "So we can get a regret bound that looks like this and this is an awful formula.",
            "I have to admit.",
            "But it sort of shows you the right scaling I believe, so K is the number of options that you have to process a task, and so we see that the Luckily the regret is still lowering a number of trials.",
            "And we're not scaling exponentially with the number of policies.",
            "So you oh, by the way, there is a very, very simple way to solve this problem.",
            "You would just at least all the policies and you would view the policies as arms themselves and just use UCB maybe.",
            "And you could get a bond from that as well, but there the constant would scale exponentially in the natural parameters, so it would scale.",
            "Linearly with the number of policies, so we want to avoid that as well.",
            "And the.",
            "So capital you here is the set of policies, so this is nice small number.",
            "Yeah, I should have suppressed that and a something some complicated constant that I was too afraid to put on slides otherwise.",
            "So this K I'm not.",
            "Having said all this, I'm not absolutely happy with this bond, so I I, I believe that there must be a way to improve it.",
            "So we have this strange scaling with the square.",
            "Inverse Square of the expected losses for different choices.",
            "Which you usually don't have.",
            "So usually the regrets we're not talking about the number of choices of suboptimal arms were talking about regret.",
            "So regret usually just scales inversely with this quantities.",
            "OK. Yeah, and maybe it's interesting that.",
            "Yeah, the way this quality is defined.",
            "Yes.",
            "Little weird that the you there isn't a denominators.",
            "Yeah.",
            "Yeah yeah yeah.",
            "So it basically means that this constant is not important.",
            "You have two here.",
            "Something like that, maybe I should just say 3 and then you would not worry about that.",
            "It's a VA speculative proof.",
            "Previous slide why do you get tickled estimate for the bar?",
            "Yeah, it's coming.",
            "Because that's the only question right, I guess.",
            "Unbiased estimate?",
            "This doesn't look like an unbiased estimate.",
            "Because you are yeah yeah, because you could.",
            "You could buy as your estimates.",
            "Because.",
            "The number of times where you select a certain action or act according to your certain policies?",
            "Not so I'm not covering that actually, but in the talk, so I was not prepared to cover that, but.",
            "But it's.",
            "But you came, you can do that.",
            "Yeah, yeah."
        ],
        [
            "So I saw that you were asking why it's a lower bound and.",
            "But not OK. Say this is nicely the statement that states that.",
            "The estimate if you have these confidence intervals satisfied and dyrestam it is really a good estimate of Lambda star.",
            "And.",
            "And the proof basically goes by maybe I shouldn't cover the proof.",
            "Just.",
            "2 lines you just.",
            "Think about both separately and pic pic.",
            "The Maximizers the maximiser policy that gives you actually disregard and just consider it."
        ],
        [
            "Say.",
            "Open problems, so I'm as I said, I'm not particularly happy about the form of the proof for the form of the bond and.",
            "Maybe you could use various estimates.",
            "I've seen the constants are way, way too big.",
            "And.",
            "And maybe you could actually avoid this complicated wave of computing this estimate for the average keyboard and just work with the average reward per step so far.",
            "And.",
            "What's really interesting that I found out just recently were reading this paper by Barnabas and cut a Hawk is is that?",
            "They are looking at Target and that I just inflating.",
            "The action value without looking in the future, so not taking into account the whole uncertainty that you have just the local uncertainty and I don't know if that works with a finite time analysis as well, but that's much simpler to compute and I actually got optimal asymptotic constant suspense.",
            "Or I was wondering about if.",
            "It is possible to do something."
        ],
        [
            "Similar here.",
            "OK, so that was the first part.",
            "So what's the time?",
            "10 minutes.",
            "I have 10 minutes.",
            "OK.",
            "So when the number of actions is large."
        ],
        [
            "So we have.",
            "We can go fast then so we have this bond.",
            "That's chaos, really badly.",
            "If the number of actions is is large, so you have to select basically or alarm service once.",
            "That's the way the right mattress starts and you can wonder what you can do if if the number of ours is just huge and you run out of time before trying all day."
        ],
        [
            "Actions.",
            "So we want sort of anytime property mixed with bandit R Gittens.",
            "And of course, without making any prior assumptions, that could be really hard to do, so we're going to make a stronger assumption.",
            "But we are quite happy to have situations where in practice you can meet these assumptions.",
            "So the idea is that what if actions have our trip together actions that give you similar proofs are grouped together.",
            "So if you have two groups up actions, these are Group One, Group 2.",
            "All actions here give you a payoff of one, and all actions give you a payoff of 0 here.",
            "Then it doesn't really matter how many actions you have here, right?",
            "And how many actions you have here.",
            "So you have basically a torrent bandit problem, so if you're lucky.",
            "To have these groupings.",
            "Then you can organize everything in a tree and use it restructure."
        ],
        [
            "Should come up with some.",
            "It's an algorithm.",
            "So we call.",
            "These are getting the upper confidence based research target and and so it's based on two very very simple rules through one is keep a counter and average in each node.",
            "So in each node we've problem as bandit problem itself and that's at."
        ],
        [
            "Tree root 2.",
            "So this is how it works.",
            "So these the previous tree we are time step T = 1 and this is that the total reward received so far for this choice is zero and the number of times we have chosen this option is zero and this is the confidence interval.",
            "And we do the same for the for the others, and in each node.",
            "So they are getting traverses a tree from the root and it goes down and in each trial it goes down through leaves so eventually selects a primitive action.",
            "And.",
            "Anet basis it's decisions based on on these statistics.",
            "ANAN uses the UC One rule in all the notes or something similar, so this is how it computes things.",
            "So we are in red go down because we always take right first and then we update the update again and then that."
        ],
        [
            "I'm increases and then we have a new confidence interval here, and this is a bigger value of course than this value, and therefore we are going to select the second option, this time go down.",
            "And if you go down here, no matter where you want, you get a 0.",
            "So I'm not showing that, but so the estimated reward here is still there."
        ],
        [
            "Movie update times three ban we have.",
            "We are using the UCB formula to actually compute this confidence interval, as in this case we have to compare 1.5 and 2.0 so we're going to choose the first option again."
        ],
        [
            "Windows Phone updates.",
            "The contract is the cumulative sum of the rewards and then then it goes on an on.",
            "And you can.",
            "You can sort of four."
        ],
        [
            "I see that this argument is going to select very frequently.",
            "The good options, no matter how many of them there are.",
            "So if you assume that the estimated value of a good action is 1 and Ferb action is 0, then the bed actually selected.",
            "When you have this inequality after, so we have already selected but action once, and so next time when we select bad action is equals zero.",
            "The next time we select bad action we have this one confidence interval is the other, and if you look at the actual values you get.",
            "An increasing sequence, so you exponentially rarely slow."
        ],
        [
            "The bad actions.",
            "This is the way.",
            "Sharks.",
            "And you can have many variants of this argument.",
            "You can adapt it to minimax research.",
            "So sometimes the grouping is given by.",
            "Looking sequences of actions as primitive actions and then there are natural organizing trees, and then you can actually think about that.",
            "Maybe this is a good organization as well, because if you follow the same trajectory for long long time then the these primitive actions or action sequences should give you similar results.",
            "And so if you.",
            "Work with this very very large dies.",
            "Then you have to do something to the obvious memory problems so you can introduce transposition tables.",
            "You can change the by sequence if you have a deterministic problem then we are such as you use a different bias sequence that not very greedily exploring close to the leaves.",
            "This Costa believes your information is more precise.",
            "So the the thing here to note is that although I said that you view each node as bandit problem itself, this is non stationary bandit problem because your choices down in the tree are influencing the."
        ],
        [
            "You are that you're experiencing.",
            "At the node up in the tree.",
            "And.",
            "And you can do lots of other things like stopping episodes earlier, using elevation function functions, iterative deepening, clever move selection.",
            "Mixing conceptual different action groupings we ready?"
        ],
        [
            "Creative about this and so we have some trickle results and the first is just basic result that makes people in the game community happy.",
            "Just finish.",
            "And it just says that at the root of the tree, we're not taking in turn into account the bias then ultimately are going to select the best choice there.",
            "And what's more interesting is that the rate of convergence is not dependent on the if you organize actions in tree on the full size of a tree, it really depends on the affect what we call the effective size of tree.",
            "In the previous example.",
            "No matter how many leaf nodes we had, we had the same convergence rate.",
            "It was basically because the structure of the tree was so nice.",
            "And so in this way this argument is nice because if you have some prior information about how to group actions together, then you will have a chance to solve problems with really, really large action spaces, and targeting just adapts to the actual difficulty of the problem.",
            "So this is an illustration, so if you have a tree, you can cut those parts of the tree that give you identical reward."
        ],
        [
            "Some fine and very, very similar words.",
            "So since I already have one minute left, I just mentioned that we try."
        ],
        [
            "These are getting sailing problem and that's an MDP, and so there's an action sequence for you."
        ],
        [
            "And maybe I don't go into this, we just I just mentioned that the scaling behavior in this particle example a few City made us really happy.",
            "So it scared much better than alternative organs.",
            "Still in his MVP, if you just kill it up then then the number of samples needed to reach a certain position blows up exponentially.",
            "But but the rate of growth is much much less than for any argument that you could find out there so.",
            "And we could actually scale up up to problems of state space of size of.",
            "So that's the grid size and state space is constant number of that so."
        ],
        [
            "We're happy about that.",
            "And there is going to be at talk about using you sitting in goal.",
            "That has been very, very successful.",
            "So I'm not going to talk about that.",
            "We have tried use it in other games with moderate success.",
            "I would say maybe I should say it's nice access because we haven't actually tried to put in any prior knowledge into those games and we got our gardens that were competitive with with arguments that were changed for several months or maybe years."
        ],
        [
            "And that's it.",
            "Question.",
            "Kiss we ended talking talking about this problem.",
            "Bandit problems with side information and it seems like UCT is not in that framework or not using some information.",
            "Is that correct?",
            "Yeah, you city, we didn't consider using site information.",
            "That that would add to the difficulties of usage, so use it usually faces memory problems.",
            "So it depends on the problem that you're looking at.",
            "But there is an obvious way to combine it with side information I guess, but we haven't looked at that.",
            "Expect some difficulties."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expectations.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have 40 minutes or something, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Just tell me if I'm ready.",
                    "label": 0
                },
                {
                    "sent": "Stretching time or something please?",
                    "label": 0
                },
                {
                    "sent": "So that is the contents.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to introduce bandit problems and shortly talk about previous work and upper confidence based algorithms and.",
                    "label": 1
                },
                {
                    "sent": "Then we are going to to consider special keys where actions take some random time and that introduces some additional complications and look at their approaches.",
                    "label": 0
                },
                {
                    "sent": "Studio is those difficulties and we'll have a nice result I guess.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk about what you could do if you have larger action spaces there.",
                    "label": 1
                },
                {
                    "sent": "I'm still talking about discrete action spaces, but.",
                    "label": 0
                },
                {
                    "sent": "We're talking about really, really large action action spaces, which are maybe exponentially in the size of some nature quantity.",
                    "label": 0
                },
                {
                    "sent": "And conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe you are all familiar with exploration and exploitation tradeoff, so I can just split this, but I have this very nice.",
                    "label": 0
                },
                {
                    "sent": "Picture showing two herbs.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the idea is that if he estimated quantity or estimates an uncertain an, you might underestimate something an underestimate the optimal.",
                    "label": 0
                },
                {
                    "sent": "The value of the optimal choice and cares you're going to choose too many times the suboptimal one, or overestimate the suboptimal choice.",
                    "label": 0
                },
                {
                    "sent": "And then you are going to suffer loss because of that, and so you can't trust your estimates and such a situation much D. So that's really the question that we're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and of course, if you just trust your estimates then you are exploiting an.",
                    "label": 0
                },
                {
                    "sent": "If you are not.",
                    "label": 0
                },
                {
                    "sent": "If you make that you don't trust your estimates for whatever reason that you exploit.",
                    "label": 0
                },
                {
                    "sent": "Save exploration exploitation.",
                    "label": 0
                },
                {
                    "sent": "While I guess has quite a few nice applications and.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you look at more complicated processes that add memory and state information than the range of applications explodes, and so we made some or freshly made some.",
                    "label": 0
                },
                {
                    "sent": "Tremendous progress in the past, I guess.",
                    "label": 0
                },
                {
                    "sent": "Exploration versus exploitation.",
                    "label": 0
                },
                {
                    "sent": "But there's but still there are many, many challenges.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say the the very basic underlying principle that many of these algorithms using is called the optimism in the face of uncertainty principle, and probably it was introduced by Lauren Robinson.",
                    "label": 1
                },
                {
                    "sent": "Yeah, Peters agrees.",
                    "label": 0
                },
                {
                    "sent": "So I I pretty much sure then that this is a true thing.",
                    "label": 0
                },
                {
                    "sent": "So the idea is is can be presented in very very simple case value.",
                    "label": 0
                },
                {
                    "sent": "Just have two options which give you pay offs and these are the payouts is a sequence for option one and this sequence for option two and similarly to one of few previous talks.",
                    "label": 0
                },
                {
                    "sent": "We assume that if you are basically consuming these sequences.",
                    "label": 0
                },
                {
                    "sent": "So when you select an option T's time step, then you get that reward.",
                    "label": 0
                },
                {
                    "sent": "And the principle says that.",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "You should not pretty ever hear the options by just taking maybe sample mean, but you should rather look at an optimistic way about what is the possible value that that option can verse.",
                    "label": 0
                },
                {
                    "sent": "Given your past experience.",
                    "label": 0
                },
                {
                    "sent": "Maybe you use a mother or maybe you don't need a model in order to have such an estimate, but this is basically the principle behind.",
                    "label": 0
                },
                {
                    "sent": "Many of these arguments that we're talking about.",
                    "label": 0
                },
                {
                    "sent": "And so why does this principle work?",
                    "label": 1
                },
                {
                    "sent": "I have slide yeah, and so the idea is that we should select option with the best inflated value.",
                    "label": 0
                },
                {
                    "sent": "So the idea of the core underlying idea is that if he.",
                    "label": 1
                },
                {
                    "sent": "If you choose.",
                    "label": 0
                },
                {
                    "sent": "The optimal von because still that has the best inflated value.",
                    "label": 0
                },
                {
                    "sent": "Then that's good, but if you choose a suboptimal one that has a big influence, bigger inflated value than the other options, then you will automatically learn by this principle about the sub optimal suboptimal choice and the process will regulate itself.",
                    "label": 0
                },
                {
                    "sent": "If you inflate the values and principles way.",
                    "label": 0
                },
                {
                    "sent": "I think it's pretty clear.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Lion Rob is looked at the cases where there are parametric distributions.",
                    "label": 0
                },
                {
                    "sent": "We we have certain parametric forms for the pay offs and you don't know the values of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Basically, the way they approach this problem is just to define an uncertain is set, so that's the set of parameters that look quite possible given the observation that you had before.",
                    "label": 0
                },
                {
                    "sent": "Actually in algorithm key, usually you compute Kia Divergance or something like that.",
                    "label": 0
                },
                {
                    "sent": "So you take the maximum likelihood estimator parameter.",
                    "label": 0
                },
                {
                    "sent": "That gives you a probability distribution and then you say that every probability distribution which is which is within a certain every parameter value which gives you a probability distribution that is missing a certain distance to the estimated probability distribution is acceptable and you have to control.",
                    "label": 0
                },
                {
                    "sent": "Of course the size of these distances.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And you control the size by just defining large in terms of the number of trials that elapsed from the beginning and the number of choices that you made for an option.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Given given such an answer to the set, so let's eat eat, eat to denote the expected value.",
                    "label": 0
                },
                {
                    "sent": "AXA, given a certain value of data under probability distribution, and then you just take the maximum of this.",
                    "label": 0
                },
                {
                    "sent": "So you look at the future in optimistic variances.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty nice.",
                    "label": 0
                },
                {
                    "sent": "And then you choose basically the rule which gives you the biggest inflated way.",
                    "label": 0
                },
                {
                    "sent": "And what's even nicer is that they have nice upper bounds.",
                    "label": 0
                },
                {
                    "sent": "Like TMJ FN here denotes the number of times an suboptimal arm was chosen.",
                    "label": 0
                },
                {
                    "sent": "So J here is the index of sub optimal arm.",
                    "label": 0
                },
                {
                    "sent": "And you have these lower ethnic dependence on the number of trials which is which is really good.",
                    "label": 0
                },
                {
                    "sent": "That's very slow growth.",
                    "label": 0
                },
                {
                    "sent": "So whenever the growth rate of choosing the optimal arms is less than linear, then you're basically converging and and logarithmic growth rate is really good.",
                    "label": 0
                },
                {
                    "sent": "And what's even nicer is that they have matching lower bound, and so we know that the target and that I'm just sketched.",
                    "label": 0
                },
                {
                    "sent": "It's just giving you the best possible way to control exploration and exploitation at least in asymptotic sense.",
                    "label": 0
                },
                {
                    "sent": "So that's really nice.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is both cores that during the transient we don't know how these algorithm behaves and the other problem is that if you don't have parametric distributions than.",
                    "label": 0
                },
                {
                    "sent": "The last thing you did.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just very recently by Peter and number of codes or save him introduced and studied this UCB one algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which takes this nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Here you assume that the rewards are bounded between zero and one, but you don't make any parametric assumptions whatsoever about the distribution of the rewards.",
                    "label": 0
                },
                {
                    "sent": "Instead, you just compute the sample means and visited the compute an upper estimate on the variance, which is scared in a in a principled way.",
                    "label": 0
                },
                {
                    "sent": "So the scaling is really interesting here, so if you so and GFT, here is a number of times are chavers chosen up to time time step T. And so if you don't select an arm, that is, bond grows and so you are going to select this arm ultimately.",
                    "label": 0
                },
                {
                    "sent": "And the other hand, you can see that.",
                    "label": 0
                },
                {
                    "sent": "The only way to to make this disappear is to select an army infinitely often so you will end up with selecting order arms in field often, but.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the number of crimes that you select as suboptimal arm is still arhythmic, and actually the bond holds for each end.",
                    "label": 0
                },
                {
                    "sent": "So we have a regret bound here, so the regret is just the expected loss due to not selecting best option, time step and.",
                    "label": 1
                },
                {
                    "sent": "And they are talking here, but the expected regret.",
                    "label": 0
                },
                {
                    "sent": "And you can have a bond like this.",
                    "label": 0
                },
                {
                    "sent": "And here that of I is the expected loss if you select arm I instead of the optimal.",
                    "label": 0
                },
                {
                    "sent": "Or an optimal line?",
                    "label": 0
                },
                {
                    "sent": "And so this is really nice design.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in the first part I'm going to talk about how to use the same principle and build on this previous result.",
                    "label": 0
                },
                {
                    "sent": "And used it in for bandits where actions take continuous time.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is this problem that we're looking at?",
                    "label": 0
                },
                {
                    "sent": "So the motivation is to look at maybe task scheduling so we have a finite number of types of tasks which are arriving randomly, and so this information about the type of the task is revealed to us.",
                    "label": 0
                },
                {
                    "sent": "And then we have to make a decision.",
                    "label": 0
                },
                {
                    "sent": "About how to actually process the task, maybe we have several ways to process a task, and depending on the type, some of the ways are more rewarding.",
                    "label": 0
                },
                {
                    "sent": "Some of the ways are less rewarding.",
                    "label": 0
                },
                {
                    "sent": "We can actually, maybe you can add an action that just drops a test.",
                    "label": 0
                },
                {
                    "sent": "That's a certain way of processing and task is fun.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that once you decided how to process the tasks, so this is single machine problem, you have a single machine that's going to process attacks, so it will take some time until you finish processing the task and these times random random themselves as far.",
                    "label": 0
                },
                {
                    "sent": "And we're looking at the non permit version of this problem where you don't know anything about this version of the test times tasks that are arriving and you don't know anything about the rewarded solutions or distribution of the delays.",
                    "label": 0
                },
                {
                    "sent": "So I call the time to process a task.",
                    "label": 0
                },
                {
                    "sent": "The execution delay of the corresponding action actually selecting they process the task.",
                    "label": 0
                },
                {
                    "sent": "But we assume that times I hear, yeah, so we are under the simple framework then then we make this idea assumption.",
                    "label": 0
                },
                {
                    "sent": "So you can wonder if you can extend this, of course by showing state things like that and then you are in a semi MDP favor.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Normally this is it, so we assume that we observe this ID Korea, so these are not the words.",
                    "label": 0
                },
                {
                    "sent": "So these are the side information that we have.",
                    "label": 0
                },
                {
                    "sent": "And there are a finite number of possible values that this side information can take.",
                    "label": 0
                },
                {
                    "sent": "And we also assume that.",
                    "label": 0
                },
                {
                    "sent": "In each time step, there is an IID sequence of rewards and an ID sequence of delays.",
                    "label": 1
                },
                {
                    "sent": "And actually we are changing here.",
                    "label": 0
                },
                {
                    "sent": "Seems a little bit.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "So here RIT means that in discrete time step T in T's choice.",
                    "label": 0
                },
                {
                    "sent": "What was the random reward at that?",
                    "label": 0
                },
                {
                    "sent": "You're going to experience if the side information is acts, so that's the function.",
                    "label": 0
                },
                {
                    "sent": "So we don't know these quantities, but we assume some boundedness, and so we're going to use.",
                    "label": 0
                },
                {
                    "sent": "Are eye of X to denote the expected reward given that signed information is X and we choose option I and that of data.",
                    "label": 0
                },
                {
                    "sent": "I ask students aim for the delays.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An allocation rule is basically a way to choose what to do next time step, and the choice of surgery will be denoted by capital IFT.",
                    "label": 0
                },
                {
                    "sent": "And if you choose a particle way of what this is not not actually correct, I guess.",
                    "label": 0
                },
                {
                    "sent": "Save.",
                    "label": 0
                },
                {
                    "sent": "So that should be so if you choose a particle way to to process a pass, then you then you incur Allevard suffered late and that should be RIFTT.",
                    "label": 0
                },
                {
                    "sent": "Of course at evaluated X of T, which is the side information that you have at time T and you have the same thing, this delay.",
                    "label": 1
                },
                {
                    "sent": "And so if you have a policy you that certain way of selecting the arms or options, then you can think about that.",
                    "label": 0
                },
                {
                    "sent": "Maybe the natural way to avoid such a policy is not just signing the rewards, because you can have an option that takes a long time to execute or via processing task and then therefore it doesn't really make sense to choose an option because.",
                    "label": 0
                },
                {
                    "sent": "Maybe during that time you are going to miss some good opportunities to to process other tasks that are coming along.",
                    "label": 0
                },
                {
                    "sent": "And so maybe a better way to measure performance in this case is just business ratio.",
                    "label": 0
                },
                {
                    "sent": "There are many alternative ways, and you can play around with that by rather not go into that right now.",
                    "label": 0
                },
                {
                    "sent": "So basically this ratio measures the delivered per timestamp, the expected departure time step if you want.",
                    "label": 0
                },
                {
                    "sent": "So you want to maximize that annual Greg, that is, will be computed compare.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To that.",
                    "label": 0
                },
                {
                    "sent": "Select the model is that you get a new task as soon as the machines finish with no actually in.",
                    "label": 0
                },
                {
                    "sent": "In every time step you get a new task.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "Because everything is IID so.",
                    "label": 0
                },
                {
                    "sent": "But yet so in that sense, you are right, so there is no additional delay.",
                    "label": 0
                },
                {
                    "sent": "So once you're finished, there is a test there and you get.",
                    "label": 0
                },
                {
                    "sent": "You can process any task.",
                    "label": 0
                },
                {
                    "sent": "And so we're looking for a policy that maximizes the gain so that we are going to denote by Lambda Stern.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out that this just special case of Sammy and EPS and so you can compute so called action values.",
                    "label": 1
                },
                {
                    "sent": "So let's call options right now actions and then you can given a side information X, you can just every it the option but just looking at the expected reward, but you subtract how much you lose during the time that you're going to execute option.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "During that time you cannot choose any other option and you are going to lose an average Lambda star value and therefore given this amount of time your losses this so this is the true value of the tree value of the.",
                    "label": 0
                },
                {
                    "sent": "Actions, and this is a true value in the sense that as well, that if you have access to this values then you can just compute an optimal policy by just OK. You have side information X, you just look at the option that maximize this action values and that gives you optimal policy.",
                    "label": 1
                },
                {
                    "sent": "So that's a nice and easy way to compute an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "And the regret is redefined also to reflect the case that.",
                    "label": 0
                },
                {
                    "sent": "Actions take some time, so one way to define the regret and there are other ways as well and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure that all of them are excellent.",
                    "label": 0
                },
                {
                    "sent": "We can actually prove things about this regret, but some of them are excellent, but not all after.",
                    "label": 0
                },
                {
                    "sent": "And so these regret just says so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of pseudo regret, so this thing that says that if I have a policy that is going to suffer these delays, I just some delays and compute how much on the average the optimal policy would give me during that time.",
                    "label": 0
                },
                {
                    "sent": "And I subtract the total amount of rewarded.",
                    "label": 0
                },
                {
                    "sent": "He said during that end time steps as far or an choices.",
                    "label": 0
                },
                {
                    "sent": "So is that clear?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one option would be that you are probably thinking of is to just estimate the underlying probability distribution for sign information and.",
                    "label": 0
                },
                {
                    "sent": "The rewards, the expected rewards and the expected delays.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And in line with the optimism in the face of uncertainty principle, you could say that so I have an estimate for these quantities.",
                    "label": 0
                },
                {
                    "sent": "And so again, I can think about the models that are quite likely given the information that I have.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can define distances and change the distances with time and number of time steps you have chosen different things.",
                    "label": 0
                },
                {
                    "sent": "And so you define this set MFT, which is the set of possible models given your past information that they sync a lightly and then you can just say that OK if I have such a model then I can avoid any policy here and let's find jointly the model and the policy that gives me the highest average reward.",
                    "label": 0
                },
                {
                    "sent": "So that's fine.",
                    "label": 0
                },
                {
                    "sent": "That would probably work.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "But maybe the problem is that.",
                    "label": 0
                },
                {
                    "sent": "Regret will depend if you try to analyze such an algorithm then the regret is going to depend.",
                    "label": 1
                },
                {
                    "sent": "I believe on the recipe rock of the smallest, why you have seeing some side information.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you want to estimate P and.",
                    "label": 0
                },
                {
                    "sent": "And foot bones and P then.",
                    "label": 0
                },
                {
                    "sent": "Then this is the quantity that governs how much you have to wait.",
                    "label": 0
                },
                {
                    "sent": "Basically, in order to see sufficiently many examples for the particle state, and this quantity is going to dominate.",
                    "label": 0
                },
                {
                    "sent": "This maximization program.",
                    "label": 0
                },
                {
                    "sent": "And we want to avoid that.",
                    "label": 0
                },
                {
                    "sent": "So is there any other way?",
                    "label": 0
                },
                {
                    "sent": "To come up with with the principle way of selecting actions, trading off exploration, exploitation video estimating, the probability distribution with this question would be even more interesting if we work this continues site information.",
                    "label": 0
                },
                {
                    "sent": "Because then we could argue that OK, we want to argue density estimation.",
                    "label": 0
                },
                {
                    "sent": "Maybe density estimation is more difficult than the problem that we are trying to solve.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And yes, there is, so there's a little algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult as well.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult at all, so.",
                    "label": 0
                },
                {
                    "sent": "Basically what we do here is that still we are using the same ideas.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's best to start breeding here, so we are estimating rewards delayes average reward and we're putting confidence intervals on top of them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The way it works is that.",
                    "label": 0
                },
                {
                    "sent": "We are estimating the rewatchers usual way, so just taking averages.",
                    "label": 0
                },
                {
                    "sent": "You can do that.",
                    "label": 0
                },
                {
                    "sent": "We're in finite case.",
                    "label": 0
                },
                {
                    "sent": "We're estimating delays in the same way as well, so trick is to estimate the average gain slightly differently.",
                    "label": 0
                },
                {
                    "sent": "So we had several ideas about how to do that, and actually we're quite confident about that.",
                    "label": 0
                },
                {
                    "sent": "If you just take maybe the average of.",
                    "label": 0
                },
                {
                    "sent": "Given all past trials, then that that would work, but we actually could not prove that.",
                    "label": 0
                },
                {
                    "sent": "So the trick here that makes the proof work, and I'm not sure if this actually needed for the algorithm is to compute the average dividing different way the way you compute this average reward is to look at to look at every policy, and for any policy you you compute, you can just compute the average reward by just looking at those time steps.",
                    "label": 0
                },
                {
                    "sent": "Value are sort of following that policy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And you put the confidence interval on that.",
                    "label": 0
                },
                {
                    "sent": "So this is the number of times you have followed that policy.",
                    "label": 0
                },
                {
                    "sent": "TUFT, so you is a different index right now and then I saw that in this is police indexes policies.",
                    "label": 0
                },
                {
                    "sent": "OK, and we take the maximum of this so.",
                    "label": 0
                },
                {
                    "sent": "This is a cautious estimate.",
                    "label": 0
                },
                {
                    "sent": "That So what we are trying to to avoid is to overestimate the average gain.",
                    "label": 0
                },
                {
                    "sent": "If you overestimate the average game then you are substracting here value that's too big and then you are not going to work with upper confidence estimates.",
                    "label": 0
                },
                {
                    "sent": "You are going to work with lower confidence or something as basically.",
                    "label": 0
                },
                {
                    "sent": "And so the idea is to avoid that, but just substracting this confidence values and that that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "So we can get a regret bound that looks like this and this is an awful formula.",
                    "label": 0
                },
                {
                    "sent": "I have to admit.",
                    "label": 0
                },
                {
                    "sent": "But it sort of shows you the right scaling I believe, so K is the number of options that you have to process a task, and so we see that the Luckily the regret is still lowering a number of trials.",
                    "label": 0
                },
                {
                    "sent": "And we're not scaling exponentially with the number of policies.",
                    "label": 0
                },
                {
                    "sent": "So you oh, by the way, there is a very, very simple way to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "You would just at least all the policies and you would view the policies as arms themselves and just use UCB maybe.",
                    "label": 0
                },
                {
                    "sent": "And you could get a bond from that as well, but there the constant would scale exponentially in the natural parameters, so it would scale.",
                    "label": 0
                },
                {
                    "sent": "Linearly with the number of policies, so we want to avoid that as well.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "So capital you here is the set of policies, so this is nice small number.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I should have suppressed that and a something some complicated constant that I was too afraid to put on slides otherwise.",
                    "label": 0
                },
                {
                    "sent": "So this K I'm not.",
                    "label": 0
                },
                {
                    "sent": "Having said all this, I'm not absolutely happy with this bond, so I I, I believe that there must be a way to improve it.",
                    "label": 0
                },
                {
                    "sent": "So we have this strange scaling with the square.",
                    "label": 0
                },
                {
                    "sent": "Inverse Square of the expected losses for different choices.",
                    "label": 0
                },
                {
                    "sent": "Which you usually don't have.",
                    "label": 0
                },
                {
                    "sent": "So usually the regrets we're not talking about the number of choices of suboptimal arms were talking about regret.",
                    "label": 0
                },
                {
                    "sent": "So regret usually just scales inversely with this quantities.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, and maybe it's interesting that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the way this quality is defined.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Little weird that the you there isn't a denominators.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So it basically means that this constant is not important.",
                    "label": 0
                },
                {
                    "sent": "You have two here.",
                    "label": 0
                },
                {
                    "sent": "Something like that, maybe I should just say 3 and then you would not worry about that.",
                    "label": 0
                },
                {
                    "sent": "It's a VA speculative proof.",
                    "label": 0
                },
                {
                    "sent": "Previous slide why do you get tickled estimate for the bar?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's coming.",
                    "label": 0
                },
                {
                    "sent": "Because that's the only question right, I guess.",
                    "label": 0
                },
                {
                    "sent": "Unbiased estimate?",
                    "label": 0
                },
                {
                    "sent": "This doesn't look like an unbiased estimate.",
                    "label": 0
                },
                {
                    "sent": "Because you are yeah yeah, because you could.",
                    "label": 0
                },
                {
                    "sent": "You could buy as your estimates.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "The number of times where you select a certain action or act according to your certain policies?",
                    "label": 0
                },
                {
                    "sent": "Not so I'm not covering that actually, but in the talk, so I was not prepared to cover that, but.",
                    "label": 0
                },
                {
                    "sent": "But it's.",
                    "label": 0
                },
                {
                    "sent": "But you came, you can do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I saw that you were asking why it's a lower bound and.",
                    "label": 0
                },
                {
                    "sent": "But not OK. Say this is nicely the statement that states that.",
                    "label": 0
                },
                {
                    "sent": "The estimate if you have these confidence intervals satisfied and dyrestam it is really a good estimate of Lambda star.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the proof basically goes by maybe I shouldn't cover the proof.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "2 lines you just.",
                    "label": 0
                },
                {
                    "sent": "Think about both separately and pic pic.",
                    "label": 0
                },
                {
                    "sent": "The Maximizers the maximiser policy that gives you actually disregard and just consider it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "Open problems, so I'm as I said, I'm not particularly happy about the form of the proof for the form of the bond and.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could use various estimates.",
                    "label": 0
                },
                {
                    "sent": "I've seen the constants are way, way too big.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And maybe you could actually avoid this complicated wave of computing this estimate for the average keyboard and just work with the average reward per step so far.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What's really interesting that I found out just recently were reading this paper by Barnabas and cut a Hawk is is that?",
                    "label": 0
                },
                {
                    "sent": "They are looking at Target and that I just inflating.",
                    "label": 0
                },
                {
                    "sent": "The action value without looking in the future, so not taking into account the whole uncertainty that you have just the local uncertainty and I don't know if that works with a finite time analysis as well, but that's much simpler to compute and I actually got optimal asymptotic constant suspense.",
                    "label": 0
                },
                {
                    "sent": "Or I was wondering about if.",
                    "label": 0
                },
                {
                    "sent": "It is possible to do something.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the first part.",
                    "label": 0
                },
                {
                    "sent": "So what's the time?",
                    "label": 0
                },
                {
                    "sent": "10 minutes.",
                    "label": 0
                },
                {
                    "sent": "I have 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So when the number of actions is large.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We can go fast then so we have this bond.",
                    "label": 0
                },
                {
                    "sent": "That's chaos, really badly.",
                    "label": 0
                },
                {
                    "sent": "If the number of actions is is large, so you have to select basically or alarm service once.",
                    "label": 1
                },
                {
                    "sent": "That's the way the right mattress starts and you can wonder what you can do if if the number of ours is just huge and you run out of time before trying all day.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actions.",
                    "label": 0
                },
                {
                    "sent": "So we want sort of anytime property mixed with bandit R Gittens.",
                    "label": 0
                },
                {
                    "sent": "And of course, without making any prior assumptions, that could be really hard to do, so we're going to make a stronger assumption.",
                    "label": 0
                },
                {
                    "sent": "But we are quite happy to have situations where in practice you can meet these assumptions.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that what if actions have our trip together actions that give you similar proofs are grouped together.",
                    "label": 1
                },
                {
                    "sent": "So if you have two groups up actions, these are Group One, Group 2.",
                    "label": 0
                },
                {
                    "sent": "All actions here give you a payoff of one, and all actions give you a payoff of 0 here.",
                    "label": 0
                },
                {
                    "sent": "Then it doesn't really matter how many actions you have here, right?",
                    "label": 0
                },
                {
                    "sent": "And how many actions you have here.",
                    "label": 0
                },
                {
                    "sent": "So you have basically a torrent bandit problem, so if you're lucky.",
                    "label": 0
                },
                {
                    "sent": "To have these groupings.",
                    "label": 0
                },
                {
                    "sent": "Then you can organize everything in a tree and use it restructure.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Should come up with some.",
                    "label": 0
                },
                {
                    "sent": "It's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we call.",
                    "label": 0
                },
                {
                    "sent": "These are getting the upper confidence based research target and and so it's based on two very very simple rules through one is keep a counter and average in each node.",
                    "label": 1
                },
                {
                    "sent": "So in each node we've problem as bandit problem itself and that's at.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree root 2.",
                    "label": 0
                },
                {
                    "sent": "So this is how it works.",
                    "label": 0
                },
                {
                    "sent": "So these the previous tree we are time step T = 1 and this is that the total reward received so far for this choice is zero and the number of times we have chosen this option is zero and this is the confidence interval.",
                    "label": 0
                },
                {
                    "sent": "And we do the same for the for the others, and in each node.",
                    "label": 0
                },
                {
                    "sent": "So they are getting traverses a tree from the root and it goes down and in each trial it goes down through leaves so eventually selects a primitive action.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Anet basis it's decisions based on on these statistics.",
                    "label": 0
                },
                {
                    "sent": "ANAN uses the UC One rule in all the notes or something similar, so this is how it computes things.",
                    "label": 0
                },
                {
                    "sent": "So we are in red go down because we always take right first and then we update the update again and then that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm increases and then we have a new confidence interval here, and this is a bigger value of course than this value, and therefore we are going to select the second option, this time go down.",
                    "label": 0
                },
                {
                    "sent": "And if you go down here, no matter where you want, you get a 0.",
                    "label": 0
                },
                {
                    "sent": "So I'm not showing that, but so the estimated reward here is still there.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Movie update times three ban we have.",
                    "label": 0
                },
                {
                    "sent": "We are using the UCB formula to actually compute this confidence interval, as in this case we have to compare 1.5 and 2.0 so we're going to choose the first option again.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Windows Phone updates.",
                    "label": 0
                },
                {
                    "sent": "The contract is the cumulative sum of the rewards and then then it goes on an on.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "You can sort of four.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I see that this argument is going to select very frequently.",
                    "label": 0
                },
                {
                    "sent": "The good options, no matter how many of them there are.",
                    "label": 0
                },
                {
                    "sent": "So if you assume that the estimated value of a good action is 1 and Ferb action is 0, then the bed actually selected.",
                    "label": 1
                },
                {
                    "sent": "When you have this inequality after, so we have already selected but action once, and so next time when we select bad action is equals zero.",
                    "label": 1
                },
                {
                    "sent": "The next time we select bad action we have this one confidence interval is the other, and if you look at the actual values you get.",
                    "label": 0
                },
                {
                    "sent": "An increasing sequence, so you exponentially rarely slow.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bad actions.",
                    "label": 0
                },
                {
                    "sent": "This is the way.",
                    "label": 0
                },
                {
                    "sent": "Sharks.",
                    "label": 0
                },
                {
                    "sent": "And you can have many variants of this argument.",
                    "label": 0
                },
                {
                    "sent": "You can adapt it to minimax research.",
                    "label": 0
                },
                {
                    "sent": "So sometimes the grouping is given by.",
                    "label": 0
                },
                {
                    "sent": "Looking sequences of actions as primitive actions and then there are natural organizing trees, and then you can actually think about that.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is a good organization as well, because if you follow the same trajectory for long long time then the these primitive actions or action sequences should give you similar results.",
                    "label": 0
                },
                {
                    "sent": "And so if you.",
                    "label": 0
                },
                {
                    "sent": "Work with this very very large dies.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do something to the obvious memory problems so you can introduce transposition tables.",
                    "label": 0
                },
                {
                    "sent": "You can change the by sequence if you have a deterministic problem then we are such as you use a different bias sequence that not very greedily exploring close to the leaves.",
                    "label": 0
                },
                {
                    "sent": "This Costa believes your information is more precise.",
                    "label": 0
                },
                {
                    "sent": "So the the thing here to note is that although I said that you view each node as bandit problem itself, this is non stationary bandit problem because your choices down in the tree are influencing the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You are that you're experiencing.",
                    "label": 0
                },
                {
                    "sent": "At the node up in the tree.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And you can do lots of other things like stopping episodes earlier, using elevation function functions, iterative deepening, clever move selection.",
                    "label": 0
                },
                {
                    "sent": "Mixing conceptual different action groupings we ready?",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Creative about this and so we have some trickle results and the first is just basic result that makes people in the game community happy.",
                    "label": 0
                },
                {
                    "sent": "Just finish.",
                    "label": 0
                },
                {
                    "sent": "And it just says that at the root of the tree, we're not taking in turn into account the bias then ultimately are going to select the best choice there.",
                    "label": 0
                },
                {
                    "sent": "And what's more interesting is that the rate of convergence is not dependent on the if you organize actions in tree on the full size of a tree, it really depends on the affect what we call the effective size of tree.",
                    "label": 1
                },
                {
                    "sent": "In the previous example.",
                    "label": 0
                },
                {
                    "sent": "No matter how many leaf nodes we had, we had the same convergence rate.",
                    "label": 1
                },
                {
                    "sent": "It was basically because the structure of the tree was so nice.",
                    "label": 0
                },
                {
                    "sent": "And so in this way this argument is nice because if you have some prior information about how to group actions together, then you will have a chance to solve problems with really, really large action spaces, and targeting just adapts to the actual difficulty of the problem.",
                    "label": 0
                },
                {
                    "sent": "So this is an illustration, so if you have a tree, you can cut those parts of the tree that give you identical reward.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some fine and very, very similar words.",
                    "label": 0
                },
                {
                    "sent": "So since I already have one minute left, I just mentioned that we try.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are getting sailing problem and that's an MDP, and so there's an action sequence for you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And maybe I don't go into this, we just I just mentioned that the scaling behavior in this particle example a few City made us really happy.",
                    "label": 0
                },
                {
                    "sent": "So it scared much better than alternative organs.",
                    "label": 0
                },
                {
                    "sent": "Still in his MVP, if you just kill it up then then the number of samples needed to reach a certain position blows up exponentially.",
                    "label": 0
                },
                {
                    "sent": "But but the rate of growth is much much less than for any argument that you could find out there so.",
                    "label": 0
                },
                {
                    "sent": "And we could actually scale up up to problems of state space of size of.",
                    "label": 0
                },
                {
                    "sent": "So that's the grid size and state space is constant number of that so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're happy about that.",
                    "label": 0
                },
                {
                    "sent": "And there is going to be at talk about using you sitting in goal.",
                    "label": 0
                },
                {
                    "sent": "That has been very, very successful.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "We have tried use it in other games with moderate success.",
                    "label": 0
                },
                {
                    "sent": "I would say maybe I should say it's nice access because we haven't actually tried to put in any prior knowledge into those games and we got our gardens that were competitive with with arguments that were changed for several months or maybe years.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Kiss we ended talking talking about this problem.",
                    "label": 0
                },
                {
                    "sent": "Bandit problems with side information and it seems like UCT is not in that framework or not using some information.",
                    "label": 0
                },
                {
                    "sent": "Is that correct?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you city, we didn't consider using site information.",
                    "label": 0
                },
                {
                    "sent": "That that would add to the difficulties of usage, so use it usually faces memory problems.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the problem that you're looking at.",
                    "label": 0
                },
                {
                    "sent": "But there is an obvious way to combine it with side information I guess, but we haven't looked at that.",
                    "label": 0
                },
                {
                    "sent": "Expect some difficulties.",
                    "label": 0
                }
            ]
        }
    }
}