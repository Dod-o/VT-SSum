{
    "id": "rbpizsmxlzwddtwpe5cwt3p2jlufdre3",
    "title": "Introduction to the Workshop",
    "info": {
        "author": [
            "Matthias W. Seeger, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Dec. 31, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Events"
        ]
    },
    "url": "http://videolectures.net/abi07_seeger_int/",
    "segmentation": [
        [
            "I'm just going to give you a very brief intro, 15 minutes and then we have a set up of some really cool talks.",
            "Let me just acknowledge our sponsors.",
            "This is Pascal European Union program and Microsoft Research."
        ],
        [
            "What are we going to talk about?",
            "Your variational methods?",
            "There's this one thing you really need to be able to do.",
            "If you want to do based on inference and that's marginalization, and that basically simply means that you sum over everything that you don't know, and this is also the thing that makes Bayesian inference really hard, or in some cases, even if."
        ],
        [
            "Possible intractable, so we have to do something about it now.",
            "There are several things you can do about it and in this workshop we mainly are concerned about variational methods and the idea of a variational method very broadly is.",
            "I mean, this doesn't come from statistics or inference that originally comes from physics, is that you actually rewrite a problem that deals in our case about marginalization into an optimization problem, so you actually rewrite it in the sense that if you solve the one thing you've solved, the other thing, so it's a.",
            "It's a reduction in some sense.",
            "Now then the optimization problem usually is just as hard as the as the thing you cannot do in the 1st place, but the optimization problem, you can actually tap into ideas such as relaxations, people have thought about an optimization so you can relax the optimization problem.",
            "Then you can go back to your original problem and you can hope that you get a good approximation to your inference problem."
        ],
        [
            "So this is the famous.",
            "This is the famous variational equation that people mostly use in variational inference.",
            "On the left hand side you have the lock partition function which you know for the purpose of this.",
            "Now you should simply note that it's an integral that we can do because X might be very high dimensional.",
            "So it might either be a some if you have discrete variables over exponentially many things or in our case even you might have a continuous integral and you have no clue how to do it.",
            "And then on the right hand side I just write something that's completely equivalent, and it's actually a maximization over all possible distributions, so it's probably not not much easier in the thing on the left, and also you can rewrite that by noting that this is actually linear.",
            "You can rewrite that as an optimization over the moments of these distributions only.",
            "This is this is often called the marginal polytope here.",
            "And then you can start.",
            "I mean, this is all intractable just as well, but then you can start to relax the right hand side and then you can go back and see whether you get something useful.",
            "So this is actually entropy here.",
            "I mean, I'm not going to talk about any details here.",
            "This is maybe just too.",
            "I mean, this is probably going to turn up in later."
        ],
        [
            "Talks we have two.",
            "I guess if you if you want to do this in a kind of a more general setting then only discrete binary variables you probably in the moment have two main players here on the left.",
            "This is variational mean field based method."
        ],
        [
            "The idea is really that you just go on the right hand side and you relax the supremum over distributions by just constraining the distribute."
        ],
        [
            "Things to factorize in some way.",
            "So then you actually have to optimize over fewer distribution and this makes it feasible.",
            "And on the right hand side we have expectation propagation, which you could.",
            "Actually view as a generalization of loopy BP.",
            "Um?"
        ],
        [
            "And there the idea is really that you are relaxing these these moments.",
            "This marginal polytope is a very complicated thing.",
            "You would need a lot of inequality's to describe that now you're just going to drop many of them and then you're left with a tractable optimization problem.",
            "That's not all you have to do.",
            "You also have to approximate."
        ],
        [
            "Entropy if you have done these two things, you get to a tractable problem, which then would approximate your original one.",
            "So in that case you actually optimizing over more feasable pseudo moments because they might actually not be moments."
        ],
        [
            "So now both of these ideas lead you to propagation schemes, which means that you have some very efficient means of at least getting a local optimum or saddle point.",
            "In these formulations, they're very efficient because they are usually just scaling linearly with the number of nodes, because you have to pass the messages back and forth, and if the graph that you're talking about your probabilistic model is very sparse, that buys you a lot."
        ],
        [
            "And very roughly speaking, these propagation schemes are just iterative in the sense that they always do local updates of some product type.",
            "And the computation you have to do locally is a local marginalization, so it's again and marginalization and then you need to do something global, which means you have to pass the messages that pass the information along the graph, and that is usually done in terms of messages.",
            "And the later thing actually needs a feasable core representation that you actually using as a backbone for passing this."
        ],
        [
            "Messages around, so you might think, well, this works great already.",
            "People use this a lot in practice.",
            "So so why are we actually here?",
            "I mean, we could just just go skiing immediately."
        ],
        [
            "To disappoint you, because there are some problems if you want to take this from the purely discrete binary variable domain to the things that people have actually taken them to.",
            "Hybrid models to models with continuous variables.",
            "I mean, I've just written down 2 problems.",
            "I mean there are many more and it would be great to discuss about this during the workshop.",
            "One problem is there's actually no if you are going away from this cozy completely discrete field into the field where you have some continuous variables.",
            "There's actually no tractable core representation anymore that would allow you to pass messages for any given model.",
            "For example, I mean, just take this.",
            "The state space model here, where the cats really is that these latent variables, so the blue variables observed and stuff on top is latent.",
            "Now the latent variables are not purely discrete, and they're not purely Gaussian, but they have variables of both types in there.",
            "And then, if you do that and you actually looking at marginals locali along the chain as you go propagating along the chain on messages as you propagate along the chain, they tend to be more and more become more and more complicated as."
        ],
        [
            "Go on so you might have a you."
        ],
        [
            "Model thing then you have a mixture mixture."
        ],
        [
            "Three things.",
            "So the catch here is really that completely against intuition.",
            "Local things can actually become almost exponentially more complex than global things, and it's actually not that they are less complex.",
            "So we have to deal with that, and one of the one of the general things you can do is that you're actually restricting the core representation to be representable in some terms of finite moments, and then you have to somehow devise a way of always projecting back into this family.",
            "This is just one general idea that."
        ],
        [
            "We have used and that means that if you're doing things like loopy belief propagation or EP, you actually have to step away from strong consistency on overlaps.",
            "Which gives you this marginal polytope, and you have to go towards weak consistency between moments.",
            "So you don't make sure that these distributions are actually the same on the overlaps, but just that they agree on some."
        ],
        [
            "Moments.",
            "The second thing that's really hot here is that even the local computations of the sum product type can be just intractable, because these local computations always require you to do some sort of moments where you have to call representation Q and you have to local potential fire.",
            "And this is just the sufficient statistics of the family.",
            "Are you working in?",
            "Or you have to do this in PPP, but in variational mean field base you have to do the same thing weather where you have to integrate over the lock potentials.",
            "And although these are usually thought to be local computations, they could still."
        ],
        [
            "We not tractable, so again you need approximations here.",
            "So one thing people have done that to recognize that often these potentials only depend on very few linear degrees of freedom of the variables.",
            "Then you can use.",
            "Then you end up with very low dimensional integrals and you can hope to do them ecurity."
        ],
        [
            "Accurately with quadrature in mean field based variation.",
            "Mean field base.",
            "You might need additional bounding steps, usually by you."
        ],
        [
            "Doing some convex duality tricks.",
            "Or then recently people have started to do.",
            "In such situations they started to do projections which are different from moment matching, so you might have heard about."
        ],
        [
            "OEP or fractional DP.",
            "So there are really a lot of problems so so we cannot just go skiing yet.",
            "And the problems really in this domain.",
            "Are that even the elementary steps that are completely thought to be very tractable and no problem at all in the discrete case we have to approximate them here.",
            "So this is happens at both phases of the propagation.",
            "It happens for the message propagation, the global thing, and it also can happen for the local some product updates.",
            "And how do we actually?",
            "I mean we have to deal with these errors now they're not there in the discrete case."
        ],
        [
            "So how do how do we analyze them?",
            "The second thing that people have figured out becomes really important here is that we have numerical stability problems.",
            "Again, they're not there in the discrete case.",
            "Becausw these some product formulas actually tend to be very sensitive computations.",
            "They tend to involve matrix inversions, which people in American mathematics fear, and they tend to involve things where ratios where both things go to zero at a different rate.",
            "So you really have to deal with this, and the problem, really, that we should really figure out is what of this stuff is inherent.",
            "Inherently a problem and what is just you know that we don't in the moment don't know how to do this better."
        ],
        [
            "And then OK, so there's also some good news.",
            "I mean, some things are simpler.",
            "For example, if you work with a Gaussian core representation, you can usually represent couplings of higher order almost for free.",
            "I mean this is just polynomial to do Gaussian computations."
        ],
        [
            "Then we also have that many important models are actually log concave, which basically means that the posterior you get from them is unimodal.",
            "For example, GP classification.",
            "I mean things that people really use in practice.",
            "Parts linear models, generally in models, they usually all tend to give you a unimodal posterior, so we should be able to use that simplicity somehow and we really should ask the question which which are actually the properties for continuous models that make it hard or easy to do inference.",
            "And are these properties actually tide to the method that you're using, or are they universal?"
        ],
        [
            "So really, to just give you an outlook to some things that we might address here and we really should address in the future is one thing that would be important is to take all this discrete model knowledge and there's a lot of it available now and test it against our common continuous models that we really use.",
            "I mean, what is still what can we transfer?",
            "What do we need to modify here?",
            "Because we can do the log computations exactly.",
            "Which methods can we actually transfer?",
            "We have many more methods than these two that I showed in the discrete case.",
            "Which of them can we transfer?",
            "What are these simple models?",
            "This load tree with models?",
            "If you go to a continuous domain."
        ],
        [
            "We we really probably need input from other fields.",
            "In statistical physics.",
            "We need maybe input from dynamical systems, people they talk about projection filters we have.",
            "We have a talk about one of the new cool things there.",
            "Later on these people have studied things like Gaussian propagation and obtained very numerically robust ways of doing them.",
            "We should really use them.",
            "Then we can go to optimization.",
            "We can look at convergence theory of projection methods.",
            "We can look at things like self concordant.",
            "I mean we can look at.",
            "Ways that people have tried to classify the hardness of a convex problem which is supposed to be simple, but it's actually a lot."
        ],
        [
            "Degrees of simplicity.",
            "Here we can go to MCMC.",
            "We can.",
            "We can look at stochastic approximation.",
            "We can look at the role of lock concavity.",
            "Cheap place for sampling and we can try to transfer this to variational methods and we also really have to look at an American mathematics so that we can get when we have a cool idea.",
            "We make sure that we can do it robustly and also quadrature.",
            "So if we have small integrals these guys can actually do them very well.",
            "So we have to import that some."
        ],
        [
            "Into into the field, and I also think we have to look at the fact that the main issues in applications which are not statistical physics up might actually be different from well, I just want to have accurate marginals.",
            "I just want to have a accurate approximation of the free energy because in these fields people probably care more about downstream things like learning.",
            "So can I learn with this inference?",
            "Can I do experimental design or planning with this inference?",
            "I don't care about the inference as such.",
            "These guys also care about scalability and about issues.",
            "Does this always converge?",
            "How fast does it converge?",
            "I mean, we also care about this and also these guys also care a lot about robustness and about, you know, is this method easy to use?",
            "And I probably just end here and then give over to our speakers because I hope they're going to address some of these things."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to give you a very brief intro, 15 minutes and then we have a set up of some really cool talks.",
                    "label": 0
                },
                {
                    "sent": "Let me just acknowledge our sponsors.",
                    "label": 0
                },
                {
                    "sent": "This is Pascal European Union program and Microsoft Research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are we going to talk about?",
                    "label": 0
                },
                {
                    "sent": "Your variational methods?",
                    "label": 0
                },
                {
                    "sent": "There's this one thing you really need to be able to do.",
                    "label": 0
                },
                {
                    "sent": "If you want to do based on inference and that's marginalization, and that basically simply means that you sum over everything that you don't know, and this is also the thing that makes Bayesian inference really hard, or in some cases, even if.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possible intractable, so we have to do something about it now.",
                    "label": 0
                },
                {
                    "sent": "There are several things you can do about it and in this workshop we mainly are concerned about variational methods and the idea of a variational method very broadly is.",
                    "label": 1
                },
                {
                    "sent": "I mean, this doesn't come from statistics or inference that originally comes from physics, is that you actually rewrite a problem that deals in our case about marginalization into an optimization problem, so you actually rewrite it in the sense that if you solve the one thing you've solved, the other thing, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a reduction in some sense.",
                    "label": 1
                },
                {
                    "sent": "Now then the optimization problem usually is just as hard as the as the thing you cannot do in the 1st place, but the optimization problem, you can actually tap into ideas such as relaxations, people have thought about an optimization so you can relax the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Then you can go back to your original problem and you can hope that you get a good approximation to your inference problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the famous.",
                    "label": 0
                },
                {
                    "sent": "This is the famous variational equation that people mostly use in variational inference.",
                    "label": 0
                },
                {
                    "sent": "On the left hand side you have the lock partition function which you know for the purpose of this.",
                    "label": 0
                },
                {
                    "sent": "Now you should simply note that it's an integral that we can do because X might be very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So it might either be a some if you have discrete variables over exponentially many things or in our case even you might have a continuous integral and you have no clue how to do it.",
                    "label": 0
                },
                {
                    "sent": "And then on the right hand side I just write something that's completely equivalent, and it's actually a maximization over all possible distributions, so it's probably not not much easier in the thing on the left, and also you can rewrite that by noting that this is actually linear.",
                    "label": 0
                },
                {
                    "sent": "You can rewrite that as an optimization over the moments of these distributions only.",
                    "label": 0
                },
                {
                    "sent": "This is this is often called the marginal polytope here.",
                    "label": 0
                },
                {
                    "sent": "And then you can start.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is all intractable just as well, but then you can start to relax the right hand side and then you can go back and see whether you get something useful.",
                    "label": 0
                },
                {
                    "sent": "So this is actually entropy here.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not going to talk about any details here.",
                    "label": 0
                },
                {
                    "sent": "This is maybe just too.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is probably going to turn up in later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talks we have two.",
                    "label": 0
                },
                {
                    "sent": "I guess if you if you want to do this in a kind of a more general setting then only discrete binary variables you probably in the moment have two main players here on the left.",
                    "label": 0
                },
                {
                    "sent": "This is variational mean field based method.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is really that you just go on the right hand side and you relax the supremum over distributions by just constraining the distribute.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things to factorize in some way.",
                    "label": 0
                },
                {
                    "sent": "So then you actually have to optimize over fewer distribution and this makes it feasible.",
                    "label": 0
                },
                {
                    "sent": "And on the right hand side we have expectation propagation, which you could.",
                    "label": 0
                },
                {
                    "sent": "Actually view as a generalization of loopy BP.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there the idea is really that you are relaxing these these moments.",
                    "label": 0
                },
                {
                    "sent": "This marginal polytope is a very complicated thing.",
                    "label": 0
                },
                {
                    "sent": "You would need a lot of inequality's to describe that now you're just going to drop many of them and then you're left with a tractable optimization problem.",
                    "label": 0
                },
                {
                    "sent": "That's not all you have to do.",
                    "label": 0
                },
                {
                    "sent": "You also have to approximate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entropy if you have done these two things, you get to a tractable problem, which then would approximate your original one.",
                    "label": 0
                },
                {
                    "sent": "So in that case you actually optimizing over more feasable pseudo moments because they might actually not be moments.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now both of these ideas lead you to propagation schemes, which means that you have some very efficient means of at least getting a local optimum or saddle point.",
                    "label": 0
                },
                {
                    "sent": "In these formulations, they're very efficient because they are usually just scaling linearly with the number of nodes, because you have to pass the messages back and forth, and if the graph that you're talking about your probabilistic model is very sparse, that buys you a lot.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And very roughly speaking, these propagation schemes are just iterative in the sense that they always do local updates of some product type.",
                    "label": 0
                },
                {
                    "sent": "And the computation you have to do locally is a local marginalization, so it's again and marginalization and then you need to do something global, which means you have to pass the messages that pass the information along the graph, and that is usually done in terms of messages.",
                    "label": 0
                },
                {
                    "sent": "And the later thing actually needs a feasable core representation that you actually using as a backbone for passing this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Messages around, so you might think, well, this works great already.",
                    "label": 1
                },
                {
                    "sent": "People use this a lot in practice.",
                    "label": 0
                },
                {
                    "sent": "So so why are we actually here?",
                    "label": 1
                },
                {
                    "sent": "I mean, we could just just go skiing immediately.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To disappoint you, because there are some problems if you want to take this from the purely discrete binary variable domain to the things that people have actually taken them to.",
                    "label": 0
                },
                {
                    "sent": "Hybrid models to models with continuous variables.",
                    "label": 1
                },
                {
                    "sent": "I mean, I've just written down 2 problems.",
                    "label": 0
                },
                {
                    "sent": "I mean there are many more and it would be great to discuss about this during the workshop.",
                    "label": 0
                },
                {
                    "sent": "One problem is there's actually no if you are going away from this cozy completely discrete field into the field where you have some continuous variables.",
                    "label": 1
                },
                {
                    "sent": "There's actually no tractable core representation anymore that would allow you to pass messages for any given model.",
                    "label": 0
                },
                {
                    "sent": "For example, I mean, just take this.",
                    "label": 0
                },
                {
                    "sent": "The state space model here, where the cats really is that these latent variables, so the blue variables observed and stuff on top is latent.",
                    "label": 0
                },
                {
                    "sent": "Now the latent variables are not purely discrete, and they're not purely Gaussian, but they have variables of both types in there.",
                    "label": 0
                },
                {
                    "sent": "And then, if you do that and you actually looking at marginals locali along the chain as you go propagating along the chain on messages as you propagate along the chain, they tend to be more and more become more and more complicated as.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go on so you might have a you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model thing then you have a mixture mixture.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three things.",
                    "label": 0
                },
                {
                    "sent": "So the catch here is really that completely against intuition.",
                    "label": 0
                },
                {
                    "sent": "Local things can actually become almost exponentially more complex than global things, and it's actually not that they are less complex.",
                    "label": 1
                },
                {
                    "sent": "So we have to deal with that, and one of the one of the general things you can do is that you're actually restricting the core representation to be representable in some terms of finite moments, and then you have to somehow devise a way of always projecting back into this family.",
                    "label": 0
                },
                {
                    "sent": "This is just one general idea that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have used and that means that if you're doing things like loopy belief propagation or EP, you actually have to step away from strong consistency on overlaps.",
                    "label": 1
                },
                {
                    "sent": "Which gives you this marginal polytope, and you have to go towards weak consistency between moments.",
                    "label": 1
                },
                {
                    "sent": "So you don't make sure that these distributions are actually the same on the overlaps, but just that they agree on some.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moments.",
                    "label": 0
                },
                {
                    "sent": "The second thing that's really hot here is that even the local computations of the sum product type can be just intractable, because these local computations always require you to do some sort of moments where you have to call representation Q and you have to local potential fire.",
                    "label": 0
                },
                {
                    "sent": "And this is just the sufficient statistics of the family.",
                    "label": 0
                },
                {
                    "sent": "Are you working in?",
                    "label": 0
                },
                {
                    "sent": "Or you have to do this in PPP, but in variational mean field base you have to do the same thing weather where you have to integrate over the lock potentials.",
                    "label": 0
                },
                {
                    "sent": "And although these are usually thought to be local computations, they could still.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We not tractable, so again you need approximations here.",
                    "label": 0
                },
                {
                    "sent": "So one thing people have done that to recognize that often these potentials only depend on very few linear degrees of freedom of the variables.",
                    "label": 0
                },
                {
                    "sent": "Then you can use.",
                    "label": 0
                },
                {
                    "sent": "Then you end up with very low dimensional integrals and you can hope to do them ecurity.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accurately with quadrature in mean field based variation.",
                    "label": 0
                },
                {
                    "sent": "Mean field base.",
                    "label": 0
                },
                {
                    "sent": "You might need additional bounding steps, usually by you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing some convex duality tricks.",
                    "label": 0
                },
                {
                    "sent": "Or then recently people have started to do.",
                    "label": 0
                },
                {
                    "sent": "In such situations they started to do projections which are different from moment matching, so you might have heard about.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OEP or fractional DP.",
                    "label": 0
                },
                {
                    "sent": "So there are really a lot of problems so so we cannot just go skiing yet.",
                    "label": 0
                },
                {
                    "sent": "And the problems really in this domain.",
                    "label": 0
                },
                {
                    "sent": "Are that even the elementary steps that are completely thought to be very tractable and no problem at all in the discrete case we have to approximate them here.",
                    "label": 0
                },
                {
                    "sent": "So this is happens at both phases of the propagation.",
                    "label": 0
                },
                {
                    "sent": "It happens for the message propagation, the global thing, and it also can happen for the local some product updates.",
                    "label": 0
                },
                {
                    "sent": "And how do we actually?",
                    "label": 0
                },
                {
                    "sent": "I mean we have to deal with these errors now they're not there in the discrete case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do how do we analyze them?",
                    "label": 1
                },
                {
                    "sent": "The second thing that people have figured out becomes really important here is that we have numerical stability problems.",
                    "label": 0
                },
                {
                    "sent": "Again, they're not there in the discrete case.",
                    "label": 0
                },
                {
                    "sent": "Becausw these some product formulas actually tend to be very sensitive computations.",
                    "label": 1
                },
                {
                    "sent": "They tend to involve matrix inversions, which people in American mathematics fear, and they tend to involve things where ratios where both things go to zero at a different rate.",
                    "label": 0
                },
                {
                    "sent": "So you really have to deal with this, and the problem, really, that we should really figure out is what of this stuff is inherent.",
                    "label": 1
                },
                {
                    "sent": "Inherently a problem and what is just you know that we don't in the moment don't know how to do this better.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then OK, so there's also some good news.",
                    "label": 0
                },
                {
                    "sent": "I mean, some things are simpler.",
                    "label": 1
                },
                {
                    "sent": "For example, if you work with a Gaussian core representation, you can usually represent couplings of higher order almost for free.",
                    "label": 1
                },
                {
                    "sent": "I mean this is just polynomial to do Gaussian computations.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we also have that many important models are actually log concave, which basically means that the posterior you get from them is unimodal.",
                    "label": 1
                },
                {
                    "sent": "For example, GP classification.",
                    "label": 0
                },
                {
                    "sent": "I mean things that people really use in practice.",
                    "label": 0
                },
                {
                    "sent": "Parts linear models, generally in models, they usually all tend to give you a unimodal posterior, so we should be able to use that simplicity somehow and we really should ask the question which which are actually the properties for continuous models that make it hard or easy to do inference.",
                    "label": 0
                },
                {
                    "sent": "And are these properties actually tide to the method that you're using, or are they universal?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So really, to just give you an outlook to some things that we might address here and we really should address in the future is one thing that would be important is to take all this discrete model knowledge and there's a lot of it available now and test it against our common continuous models that we really use.",
                    "label": 0
                },
                {
                    "sent": "I mean, what is still what can we transfer?",
                    "label": 0
                },
                {
                    "sent": "What do we need to modify here?",
                    "label": 0
                },
                {
                    "sent": "Because we can do the log computations exactly.",
                    "label": 0
                },
                {
                    "sent": "Which methods can we actually transfer?",
                    "label": 0
                },
                {
                    "sent": "We have many more methods than these two that I showed in the discrete case.",
                    "label": 0
                },
                {
                    "sent": "Which of them can we transfer?",
                    "label": 0
                },
                {
                    "sent": "What are these simple models?",
                    "label": 0
                },
                {
                    "sent": "This load tree with models?",
                    "label": 0
                },
                {
                    "sent": "If you go to a continuous domain.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We we really probably need input from other fields.",
                    "label": 1
                },
                {
                    "sent": "In statistical physics.",
                    "label": 1
                },
                {
                    "sent": "We need maybe input from dynamical systems, people they talk about projection filters we have.",
                    "label": 1
                },
                {
                    "sent": "We have a talk about one of the new cool things there.",
                    "label": 0
                },
                {
                    "sent": "Later on these people have studied things like Gaussian propagation and obtained very numerically robust ways of doing them.",
                    "label": 0
                },
                {
                    "sent": "We should really use them.",
                    "label": 0
                },
                {
                    "sent": "Then we can go to optimization.",
                    "label": 1
                },
                {
                    "sent": "We can look at convergence theory of projection methods.",
                    "label": 0
                },
                {
                    "sent": "We can look at things like self concordant.",
                    "label": 0
                },
                {
                    "sent": "I mean we can look at.",
                    "label": 0
                },
                {
                    "sent": "Ways that people have tried to classify the hardness of a convex problem which is supposed to be simple, but it's actually a lot.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Degrees of simplicity.",
                    "label": 0
                },
                {
                    "sent": "Here we can go to MCMC.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can look at stochastic approximation.",
                    "label": 0
                },
                {
                    "sent": "We can look at the role of lock concavity.",
                    "label": 0
                },
                {
                    "sent": "Cheap place for sampling and we can try to transfer this to variational methods and we also really have to look at an American mathematics so that we can get when we have a cool idea.",
                    "label": 0
                },
                {
                    "sent": "We make sure that we can do it robustly and also quadrature.",
                    "label": 0
                },
                {
                    "sent": "So if we have small integrals these guys can actually do them very well.",
                    "label": 0
                },
                {
                    "sent": "So we have to import that some.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into into the field, and I also think we have to look at the fact that the main issues in applications which are not statistical physics up might actually be different from well, I just want to have accurate marginals.",
                    "label": 1
                },
                {
                    "sent": "I just want to have a accurate approximation of the free energy because in these fields people probably care more about downstream things like learning.",
                    "label": 0
                },
                {
                    "sent": "So can I learn with this inference?",
                    "label": 1
                },
                {
                    "sent": "Can I do experimental design or planning with this inference?",
                    "label": 0
                },
                {
                    "sent": "I don't care about the inference as such.",
                    "label": 0
                },
                {
                    "sent": "These guys also care about scalability and about issues.",
                    "label": 0
                },
                {
                    "sent": "Does this always converge?",
                    "label": 0
                },
                {
                    "sent": "How fast does it converge?",
                    "label": 0
                },
                {
                    "sent": "I mean, we also care about this and also these guys also care a lot about robustness and about, you know, is this method easy to use?",
                    "label": 0
                },
                {
                    "sent": "And I probably just end here and then give over to our speakers because I hope they're going to address some of these things.",
                    "label": 0
                }
            ]
        }
    }
}