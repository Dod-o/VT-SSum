{
    "id": "bnrblpx4wpcnterk2pw6nfqqeik423yi",
    "title": "HEADY: News Headline Abstraction Through Event Pattern Clustering",
    "info": {
        "author": [
            "Daniele Pighin, Google, Inc."
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_pighin_clustering/",
    "segmentation": [
        [
            "Thanks for coming to this talk.",
            "In this talk, I will basically introduce you to a kind of extreme summarization in the sense that we take a collection of news that can have like even hundreds of thousands of news and try to reduce it to just one sentence that captures the gist of the of the news collection.",
            "So the motivation of this work is as follows.",
            "There are many domains and these are especially domains in which that are very popular.",
            "So there is evil.",
            "There is a lot of clicks on the web, for example like celebrity news and sports, in which it's very difficult to find objective and straight to the point headlines that actually convey in a clear way the main event expressed by this collection.",
            "And here we."
        ],
        [
            "Give an example so that a lot of headlines describing the wedding between these two guys, Alaska, Alaska, San Carmelo Anthony.",
            "I don't know who they are and as you can see, they're talking about a lot of collateral events, but it's very difficult to find a headline like Laura Vasquez metric, Carmelo Anthony, and this is the kind of information that we would like to be able to access both for human and for machine consumption."
        ],
        [
            "So what I'm going to present is an approach in which first of all we try to learn from data in a completely unsupervised and open domain.",
            "Way how the same event can be expressed in text, so we have no data whatsoever and we do not focus on specific set of topics.",
            "And then once we have a new stream, we want to be able to understand what event is is described by the is happening in the news and then we want to be able to generate a short objective and straight to the point headline that captures the gist of the news collection.",
            "OK, so basically it's.",
            "Event learning, event, understanding and headline generation based on the learning events."
        ],
        [
            "So first I will describe the technique that we use to learn the event model.",
            "You will see that it's a very simple approach and then I will show how the event model that we learn can be used to generate headlines and then I will wait and finally conclude."
        ],
        [
            "So we start from Google News, and as you may already know, Google News collections are like collections of related news which span a few days or in some cases just a few hours.",
            "In this example you see for example collection of news related to the riots in Brazil.",
            "Just last month.",
            "These collections can be very valuable in size.",
            "You have collections in which there are just like three or four articles, but there are collections in which you can have hundreds of thousands of articles, like for example.",
            "I don't know when Michael Jackson passed away, you had tons of news published in just a few hours about the same event."
        ],
        [
            "So our approach is based on two basic assumptions.",
            "The first one is that the collections are about closely related news and this we can assume.",
            "By construction I mean the news collections are supposed to be related news.",
            "There is some noise of course, but the assumption generally holds.",
            "And the second assumption is that if the same set of entities appears in the titles or in the first sentence in the news, in some news collection, then we can conclude that the event in which these entities are involved is actually the same.",
            "And why do we limit ourselves to the title center sentences?",
            "Well, the title is obvious, it's the headline and the first sentence, especially in English Press.",
            "It's like a summary of the of the news, so it's like a short description of the news that is supposed to drive the reader inside the news, and it actually contains something which generally is a much more objective and understandable description of the real event.",
            "So the headline describes the same event, like in a fancy way in the first sentence described describes it in a more.",
            "Mystic way if you want."
        ],
        [
            "So how do we use these two assumptions to learn event representations?",
            "So assume that we have this news collection, which is about the wedding between Anna Faris and Chris Pratt.",
            "An from these news collection, we select titles and 1st sentence is and we extract some patterns from business collection.",
            "I will come shortly to how the patterns are extracted.",
            "If we observe patterns in only one use collection, we cannot conclude very much about the usefulness of this patterns.",
            "But if we observe another news collection which is always about a wedding.",
            "Then we can see that some of the patterns that we extract are the same, so there will be some patterns that reinforce each other.",
            "They're very frequently appear in many movies collections dealing with the same event.",
            "Where are some other patterns?",
            "They're either ungrammatical or they use very fancy wording so they are less frequent and therefore they are less good candidates to represent the same semantics.",
            "So basic."
        ],
        [
            "The only piece that is missing here is a way to understand which news collections we should consider to be the same, and we will do that with a simple clustering algorithm."
        ],
        [
            "So this is basically the approach that we used to learn our event model.",
            "So we start with the corpus of dependency parsing news collections, which is basically subset of Google News data.",
            "And these corpuses parts with a standard NLP pipeline.",
            "So there is no special processing.",
            "There is dependency parsing, there is coreference resolution.",
            "There are entity annotations.",
            "An entities are tagged with Freebase entity types.",
            "So for each entity mention in a document, we know if that entity is an actor is a, is a merchant or whatever.",
            "So for each news collection in this corpus we select the most important entities in the news collection.",
            "We have a measure of topicality that measure basically measures how relevant dimensions of a specific entity are within each news collection, with respect to the background distribution of entities in the whole corpus.",
            "And then for each document initially in each, in each in each collection.",
            "Sorry, we look at different combinations of these topical entities that we found.",
            "We look at combinations of different size and we extract from the first sentence and the title the syntactic pattern that encode the relation between these subset of entities.",
            "So you know what experiments we look at, subset of entities of size 2 and three.",
            "So we build description of events for binary and ternary events.",
            "But in theory the approach is not limited to that, and it can scale down to unary patterns.",
            "It can scale up to patterns involving four or more entities.",
            "As you can imagine, there is a data sparsity problem that arises, so we need to address it, but the model in theory can scale to the size."
        ],
        [
            "So once we have extracted these patterns and I will describe shortly how these are extracted, we just group the patterns on a per news clusterin fair entities at basis, and then we use these patterns to train a noise.",
            "Your network that basically is the event model.",
            "So in annoys your network you have observed variables which are patterns in this case and then you have a layer of hidden variables which are the events that we're trying to learn.",
            "Plus there is another term which is called the noise which accounts for the fact that whenever there is a news collection, we might observe some pattern, but Piper.",
            "Might not be generated by some of the events that we know about, so there could be noise that generate some patterns.",
            "And basically what we do is that we use these groups of pattern at .2, so these patterns grouped on their news collection pair, entity set basis to bootstrap the process.",
            "So we're saying all the patterns extracted from the same set of entities for the sameness collection.",
            "It's including the same event, so use it as a starting point for the optimization."
        ],
        [
            "And this is in more detail the way that we extract patterns.",
            "It's actually a very simple approach.",
            "We start from dependency parsed.",
            "Sentences.",
            "So we have that dependency parts.",
            "We have entities which have disambiguated.",
            "Then we have Freebase types associated to each entity.",
            "And we select different subsets of entities.",
            "So let's assume that we want to select entities E1 and E2.",
            "First of all, we extract the minimum spanning tree from these from the graph, and in this case as you can see, the MST is not very expressive because it's just like 2 nodes connected by conjunction.",
            "And then we apply upset over Distichs, which are actually very simple and they just try to enforce the fact that the patterns that we extract should be possibly well formed.",
            "Do these really sticks?",
            "Are things like there should be a verb in the pattern, and if there is a verb, there should be a subject.",
            "If the verb is transitive, there should be an object.",
            "This kind of thing.",
            "So if there are two entities which are connected by conjunction relation, there should be a conjunction in between.",
            "So very simplistic sits like a dozen rules.",
            "And finally, so we go from three to three.",
            "We have these patterns to which we applied this holistic's, and then in four we have the actual patterns that we use for the clustering, which is the same pattern history, but in which we have recombined all the entity type assignments so that each entity is instantiated with with one particular type.",
            "So for example, in this case we would generate the patterns person, an actress to marry and celebrity an actress.",
            "Harry, OK. And as far as the noise your model goes, these patterns are just atomic units, so there is no sophisticated.",
            "Structural matching going on.",
            "We're not looking at subtrees overlap or whatever, we're just treating them as strings and we're comparing strings.",
            "So two patterns.",
            "Either they're the same or they're different, so the relevant information here for the clustering algorithm is the way that these clusters.",
            "Sorry, these patterns are grouped in the previous stage."
        ],
        [
            "One thing that you may notice that using this very simple approach.",
            "If we in the same patterns we have the same entity assignments, the same type assignments for two entities, it's not really possible to understand what is the direction of the relation, so this is not a problem for symmetric relations like weddings, becausw.",
            "If Tom Crew is mariscotti ormes, it's the same as if.",
            "Because we have celebrity marriage that this fine.",
            "But if Tom Crew is killed Katie Holmes, then there is a difference because we want to understand who is killing whom.",
            "So in this case the relation is no longer symmetric.",
            "And actually we solved this problem with a very simple approach again.",
            "We just sort the entities that appear in a pattern alphabetically and we assign an alphabetical offset to all the entities that appear in a pattern.",
            "So in the first case we would have.",
            "These two variants of the pattern Celebrity, Two marriage celebrity, one and Celebrity one marriage celebrity tool.",
            "Now this is a symmetric relation, so it's very likely that we would observe both these patterns within the same news collection, becauses in this collection you will see that Tom Cruise married Kathy Ormson categories maritime queries.",
            "So we can learn that these two events are actually the same.",
            "These two event representations are the same and therefore they would end up in the same cluster, whereas in this other case, it's very unlikely that in the same news collection we would observe that Tom Cruise is killed.",
            "Katie Holmes and Catacombs Skelton crew is.",
            "Therefore what would happen is that we would learn two different clusters, one in which the first entity is alphabetically coming after the other one and the other one in which the other one is sorted before the other.",
            "So you."
        ],
        [
            "In this simple approach, we can actually model a lot of interesting linguistic phenomena.",
            "As I already mentioned, we can capture symmetry and symmetry becausw as in metric relations would end up like in different clusters.",
            "We can model arguments rotations so we can learn that a team acquiring an athlete from a second team is the same thing as the second team selling an athlete to the first team.",
            "And we can also learn synthetic alternations like we can learn passive forms and for example different forms of the different synthetic teams will develop give these are toy examples, But actually I'm going to show you Now some real examples coming from the classes that we have learned."
        ],
        [
            "So these are like extrapolations of some of the clusters, because the clusters can have several hundreds of patterns inside.",
            "So the first one is very interesting.",
            "This is a ternary relation an we can see that we can learn.",
            "That's a professional sports team replacing a coach with another coach is equivalent to the professional sport team hiring the second coach.",
            "We see that the offset of the entity is 0 after firing the first one.",
            "As another."
        ],
        [
            "Sample we can also learn like figurative ways of convey some information so we can learn that the team dismissing a player is the same thing as the acts of the team following on the player, for example, or that it's the same as a soccer player being sacked by the team.",
            "And this is an example of a passive form that we can learn."
        ],
        [
            "Another interesting thing is that we can also learn to disambiguate the meaning of verbs given the context provided by the entity types that participate in an event.",
            "So for example, we can learn that's armed, force a warning, sorry, awarding a contract to an aerospace business is different from a person being awarded some price."
        ],
        [
            "So how do we?"
        ],
        [
            "User information for headline generation.",
            "It's pretty straightforward once we have our model, we just extract patterns from a stream of news.",
            "We use a two step random walk in the event model.",
            "So we first identify what are the events that are most likely given the patterns that we observe in a new collection, and then we go back to the patterns in the model and we find the patterns which are most likely given the activated clusters.",
            "So at the end of this block here.",
            "We select the patterns which are most likely given the news collection, but these patterns are coming from the model.",
            "They are no longer coming from the new collection.",
            "There are different.",
            "They might be similar or the same to some pattern observed, but in principle they are different.",
            "And once we have the patterns, we can just generate the headlines very simply, because we've seen the patterns are completely instantiated.",
            "We just need to replace the entities with the real evidence coming from the new collection."
        ],
        [
            "This is an example of headlines.",
            "This is a subset of unused collection about the engagement of Alyssa Milano.",
            "With this David Bujari, even though in the titles the IT never appears, we can generate titles like Alyssa Milano is engaged to David will Gary at least someone is set to marry David will Gary and several other equivalent forms of the same predicate."
        ],
        [
            "So could you valuation?",
            "The objective of."
        ],
        [
            "Evolution is to demonstrate that we can generate headlines that are compatible in quality with extractive approaches.",
            "Extractive approaches are those that basically select a headline from a news collection.",
            "OK, as you can imagine, these are human generated headlines because they have been written by someone.",
            "So comparing against extractive approaches for an abstractive approaches is challenging, so we compare against five baselines.",
            "One is topics on which is a state of the art summarizer.",
            "This basically selects from a news collection the sentence that minimizes the KL divergent's with respect to the distribution of the news collection.",
            "Then we select the most frequent headline in the news collection, we select the latest headline in the news collection.",
            "And then we also have this baseline which we call most frequent pattern which does the pattern extraction process.",
            "But it doesn't do the inference in the event model.",
            "So we extract patterns from the news collection.",
            "We find the most frequent one, and we just replace the entity placeholders with the evidence from this collection.",
            "So this one is selecting a pattern which is observed in the news collection.",
            "And then we're comparing against a set of the art abstractive approach which is multi sentence compression by Philip over.",
            "She's also working at Google and this is basically a system that is building a lattice of all the titles and 1st sentence is in a news collection and then it's finding the shortest path between a beginning of sentence tag and end of sentence tag that maximizes some weights that depend on the frequency of words and frequency of dependence relations."
        ],
        [
            "So we carry out both an automatic evaluation using Rouge scores and I crowdsourcing like evaluation to measure readability and informativeness of deadlines that we generate.",
            "I do not have time to go into details, but we can come during the Question Time."
        ],
        [
            "So these are the results and I would like to point out some.",
            "Some items here, so the first thing to notice is that according to the automatic evaluation and the human evaluation, there are different rankings of the systems, but this is not surprising because also as the previous speaker was mentioning, it's known that Rouge is not very good for differentiating between machine and human generated deadlines, especially when the summaries are very compressed.",
            "When information is very compressed.",
            "And the other thing that we should mention that for us is the positive result is that heavy is statistically significantly better than MSE in all dimensions.",
            "And most importantly, it feels half of the gap that separates abstractive approaches from extractive approaches.",
            "So it's getting very close to the performance of topics on, which is a very high performance."
        ],
        [
            "Another thing to mention is that, not surprisingly, extractive methods are evaluated very positively.",
            "This is not surprising because the human raters.",
            "Are shown real headlines that have been observed in the news, and it makes perfectly sense that they like them.",
            "We should also mention, and this is maybe a problem in the way that we set up the evaluation that we didn't ask a question.",
            "Trying the Raiders to judge explicitly the objectivity of the headlines, which is a parameter in said that we should have measured directly."
        ],
        [
            "And top exam is significantly better than all the other methods.",
            "The only exception is the informativeness dimension when compared to the most frequent headline.",
            "Topic time is so good that we are thinking that it would be possible to use it to generate references for automatic evaluation because the performance of the exam on this task is really very very good."
        ],
        [
            "And finally, the most frequent pattern and heady are compatible across all metrics.",
            "Head does a little bit better than MFP concerning readability, even though the difference is not significant and this might suggest that the abstraction is kind of compensating for the grammatical errors introduced by the pattern extraction stage.",
            "Again, the main difference between these two approaches is that heavy is abstractive, whereas the most frequent pattern is extracted.",
            "So in the most frequent pattern we are showing a pattern coming from the new collection.",
            "Eddie is showing a pattern coming from the noise your model, so it's a pattern that has not been observed in the collection."
        ],
        [
            "So to conclude I have presented."
        ],
        [
            "An unsupervised open domain and what I like most very simple framework for event learning.",
            "It's based on very simple assumptions about the way that news collections are built and the fact that this information can be exploited to learn how the same event can be represented in text.",
            "It captures relevant linguistic phenomena that are encoded in our event model like synthetic alternations, argument movement and it can model symmetric versus symmetric relations by distributing patterns in different clusters.",
            "We use it for obstructive weather in generation and we significantly improved over a set of the arts and we also have the gap with respect to extractive methods."
        ],
        [
            "There are many directions for improvement, the first of which is to try to improve the way that these patterns are extracted because their risztics, as I was mentioning are very simple.",
            "In many cases they fail introducing noise in the event model and producing headlines, which sometimes are ungrammatical.",
            "So what we would like to do is to automatically learn the rules that we should encode to build correct patterns directly from data.",
            "And then we would like to explore the possibility of applying the event model to populating knowledge basis.",
            "Because we can filter incoming news and we can observe events that are happening in the news and we can populate world models with this information.",
            "And finally, we would like to look into header and personalization for example.",
            "Or it would be possible once we have these abstract model and we have all the ways in which the same event can be expressed to select which pattern to use to generate the headline based on the reading level of the user or for example or there still.",
            "And that would be all.",
            "And thank you all for you.",
            "I'm really happy to see more work on abstractive summarization and that goes for this talk in the talk which came before.",
            "I'm wondering on your results where you presented where you talked about the fact that topic sum is so good some somehow.",
            "Something seems a little bit wrong there an.",
            "I'm wondering if rather than using the automatic metrics if and.",
            "I know you, you are using some the responsiveness and informativeness.",
            "Yeah, readability and informative for madness.",
            "I wonder if you might do an error analysis to look in at the cases where yours fails and topic some.",
            "Wins or the cases where topics some fails an yours does better.",
            "I mean, I somehow my guess would be that the metrics are part of what's causing problem, but this is a setting which is very very good setting for topic sound 'cause we're we're feeding topics.",
            "Um, with all the headlines from the news collection and we're asking it to select the best one.",
            "So it's basically calculating statistics about the word distributions and it's finding the headline that.",
            "Maximizes the coverage of the news collection, so it's not so surprising that topic Sam does very well on the task.",
            "And if it were working on full document bodies then you then yeah, I would agree with you.",
            "But OK, thank you.",
            "You are making an assumption that your set of documents from Google News is on contains only documents that are on very closely closely related topic is that not such a strong assumption is assumption doesn't hold all the time.",
            "It generally holds that the fact is that we are using a lot of data, so the numbers are actually in the paper.",
            "I don't remember, but we're talking about millions of news collections.",
            "So even though the assumptions is not completely true, it's very easy to isolate noise.",
            "There are some problems which are more difficult to solve, like for example, one problem of the clustering is that the clustering of the news is that whenever there is an event like for example a sports match.",
            "We tend to have in the same clusters news about the event itself and then like the pre match talk and the post match talk and in this case is the clusters are very noisy and the events that we learn are also quite noisy.",
            "So in that case it's more difficult to pick up a headline and be sure that you are generating a rights title so there is noise to the news collections.",
            "Of course to some extent it's corrected by the fact that we are leveraging a lot of data.",
            "To some extent it's still there and we need to work to improve it.",
            "Of course OK, great thanks.",
            "I have only one small question.",
            "Are you doing or are you planning on doing that for other languages as well?",
            "I'm doing it for you.",
            "Planning to do that for other languages as well.",
            "Definitely we are planning to do that.",
            "The problem is always that the NLP pipeline for other languages is not as good As for English, so we rely a lot on the fact that, for example, entities are disambiguated within certain bounds of accuracy.",
            "But yeah, we're looking into that.",
            "Of course, OK, thank you.",
            "And you had a great talk.",
            "Thank you.",
            "So probably one last small.",
            "OK, thanks for the talk.",
            "I have a question.",
            "So you evaluate the quality of your headlines across many dimensions.",
            "Informatives readability.",
            "What about like the novelty, the impact of the headline?",
            "So of course when you analyze menus in the same cluster, there will be some redundant information.",
            "Like for example you know the wife of Tom Cruise this Katie Campbell and they're divorcing.",
            "So of course putting this information that everybody already knows that there are married.",
            "But what what is interesting?",
            "Is there actually divorcing?",
            "So I was wondering how, how would you address this issue?",
            "So if you are using any kind of existing relationship from the Freebase as a good prior to actually look for the novel relations, where to put in?",
            "The fact is that we saw the data set that we OPS.",
            "The data set that we are using for the test set is related to all the news.",
            "It's between 2008 and 2012, so it's very difficult to discover new relations that, for example, are not already in Freebase.",
            "But that's definitely a direction in which we are working.",
            "That is, using the event model to understand which events are being reported in the news, and to understand how much we can, for example, update knowledge basis like 3 based on the Knowledge Graph an we have some measure of that, but it's not final.",
            "And by the way, the evaluation is limited in the sense that.",
            "Already asking for.",
            "So what we should have done and we didn't do innovation is to ask for the.",
            "Objectivity of the headline because the motivation behind this work was to try to understand if we can actually generate headlines which are more objective than those that you find in the news collections.",
            "And I mean, we can see that's the case.",
            "I mean, we have a strong feeling that we're going in the right direction, but unlikely with gentle with dimension.",
            "OK thanks anyway, OK so thank you very much.",
            "One more thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for coming to this talk.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I will basically introduce you to a kind of extreme summarization in the sense that we take a collection of news that can have like even hundreds of thousands of news and try to reduce it to just one sentence that captures the gist of the of the news collection.",
                    "label": 0
                },
                {
                    "sent": "So the motivation of this work is as follows.",
                    "label": 0
                },
                {
                    "sent": "There are many domains and these are especially domains in which that are very popular.",
                    "label": 0
                },
                {
                    "sent": "So there is evil.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of clicks on the web, for example like celebrity news and sports, in which it's very difficult to find objective and straight to the point headlines that actually convey in a clear way the main event expressed by this collection.",
                    "label": 0
                },
                {
                    "sent": "And here we.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give an example so that a lot of headlines describing the wedding between these two guys, Alaska, Alaska, San Carmelo Anthony.",
                    "label": 0
                },
                {
                    "sent": "I don't know who they are and as you can see, they're talking about a lot of collateral events, but it's very difficult to find a headline like Laura Vasquez metric, Carmelo Anthony, and this is the kind of information that we would like to be able to access both for human and for machine consumption.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to present is an approach in which first of all we try to learn from data in a completely unsupervised and open domain.",
                    "label": 0
                },
                {
                    "sent": "Way how the same event can be expressed in text, so we have no data whatsoever and we do not focus on specific set of topics.",
                    "label": 1
                },
                {
                    "sent": "And then once we have a new stream, we want to be able to understand what event is is described by the is happening in the news and then we want to be able to generate a short objective and straight to the point headline that captures the gist of the news collection.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically it's.",
                    "label": 0
                },
                {
                    "sent": "Event learning, event, understanding and headline generation based on the learning events.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I will describe the technique that we use to learn the event model.",
                    "label": 0
                },
                {
                    "sent": "You will see that it's a very simple approach and then I will show how the event model that we learn can be used to generate headlines and then I will wait and finally conclude.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we start from Google News, and as you may already know, Google News collections are like collections of related news which span a few days or in some cases just a few hours.",
                    "label": 1
                },
                {
                    "sent": "In this example you see for example collection of news related to the riots in Brazil.",
                    "label": 0
                },
                {
                    "sent": "Just last month.",
                    "label": 0
                },
                {
                    "sent": "These collections can be very valuable in size.",
                    "label": 0
                },
                {
                    "sent": "You have collections in which there are just like three or four articles, but there are collections in which you can have hundreds of thousands of articles, like for example.",
                    "label": 0
                },
                {
                    "sent": "I don't know when Michael Jackson passed away, you had tons of news published in just a few hours about the same event.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is based on two basic assumptions.",
                    "label": 0
                },
                {
                    "sent": "The first one is that the collections are about closely related news and this we can assume.",
                    "label": 0
                },
                {
                    "sent": "By construction I mean the news collections are supposed to be related news.",
                    "label": 1
                },
                {
                    "sent": "There is some noise of course, but the assumption generally holds.",
                    "label": 0
                },
                {
                    "sent": "And the second assumption is that if the same set of entities appears in the titles or in the first sentence in the news, in some news collection, then we can conclude that the event in which these entities are involved is actually the same.",
                    "label": 1
                },
                {
                    "sent": "And why do we limit ourselves to the title center sentences?",
                    "label": 0
                },
                {
                    "sent": "Well, the title is obvious, it's the headline and the first sentence, especially in English Press.",
                    "label": 0
                },
                {
                    "sent": "It's like a summary of the of the news, so it's like a short description of the news that is supposed to drive the reader inside the news, and it actually contains something which generally is a much more objective and understandable description of the real event.",
                    "label": 0
                },
                {
                    "sent": "So the headline describes the same event, like in a fancy way in the first sentence described describes it in a more.",
                    "label": 0
                },
                {
                    "sent": "Mystic way if you want.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we use these two assumptions to learn event representations?",
                    "label": 0
                },
                {
                    "sent": "So assume that we have this news collection, which is about the wedding between Anna Faris and Chris Pratt.",
                    "label": 0
                },
                {
                    "sent": "An from these news collection, we select titles and 1st sentence is and we extract some patterns from business collection.",
                    "label": 0
                },
                {
                    "sent": "I will come shortly to how the patterns are extracted.",
                    "label": 0
                },
                {
                    "sent": "If we observe patterns in only one use collection, we cannot conclude very much about the usefulness of this patterns.",
                    "label": 0
                },
                {
                    "sent": "But if we observe another news collection which is always about a wedding.",
                    "label": 0
                },
                {
                    "sent": "Then we can see that some of the patterns that we extract are the same, so there will be some patterns that reinforce each other.",
                    "label": 0
                },
                {
                    "sent": "They're very frequently appear in many movies collections dealing with the same event.",
                    "label": 0
                },
                {
                    "sent": "Where are some other patterns?",
                    "label": 0
                },
                {
                    "sent": "They're either ungrammatical or they use very fancy wording so they are less frequent and therefore they are less good candidates to represent the same semantics.",
                    "label": 0
                },
                {
                    "sent": "So basic.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only piece that is missing here is a way to understand which news collections we should consider to be the same, and we will do that with a simple clustering algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is basically the approach that we used to learn our event model.",
                    "label": 0
                },
                {
                    "sent": "So we start with the corpus of dependency parsing news collections, which is basically subset of Google News data.",
                    "label": 0
                },
                {
                    "sent": "And these corpuses parts with a standard NLP pipeline.",
                    "label": 0
                },
                {
                    "sent": "So there is no special processing.",
                    "label": 0
                },
                {
                    "sent": "There is dependency parsing, there is coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "There are entity annotations.",
                    "label": 0
                },
                {
                    "sent": "An entities are tagged with Freebase entity types.",
                    "label": 0
                },
                {
                    "sent": "So for each entity mention in a document, we know if that entity is an actor is a, is a merchant or whatever.",
                    "label": 0
                },
                {
                    "sent": "So for each news collection in this corpus we select the most important entities in the news collection.",
                    "label": 1
                },
                {
                    "sent": "We have a measure of topicality that measure basically measures how relevant dimensions of a specific entity are within each news collection, with respect to the background distribution of entities in the whole corpus.",
                    "label": 1
                },
                {
                    "sent": "And then for each document initially in each, in each in each collection.",
                    "label": 0
                },
                {
                    "sent": "Sorry, we look at different combinations of these topical entities that we found.",
                    "label": 1
                },
                {
                    "sent": "We look at combinations of different size and we extract from the first sentence and the title the syntactic pattern that encode the relation between these subset of entities.",
                    "label": 0
                },
                {
                    "sent": "So you know what experiments we look at, subset of entities of size 2 and three.",
                    "label": 0
                },
                {
                    "sent": "So we build description of events for binary and ternary events.",
                    "label": 0
                },
                {
                    "sent": "But in theory the approach is not limited to that, and it can scale down to unary patterns.",
                    "label": 0
                },
                {
                    "sent": "It can scale up to patterns involving four or more entities.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, there is a data sparsity problem that arises, so we need to address it, but the model in theory can scale to the size.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have extracted these patterns and I will describe shortly how these are extracted, we just group the patterns on a per news clusterin fair entities at basis, and then we use these patterns to train a noise.",
                    "label": 1
                },
                {
                    "sent": "Your network that basically is the event model.",
                    "label": 0
                },
                {
                    "sent": "So in annoys your network you have observed variables which are patterns in this case and then you have a layer of hidden variables which are the events that we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Plus there is another term which is called the noise which accounts for the fact that whenever there is a news collection, we might observe some pattern, but Piper.",
                    "label": 0
                },
                {
                    "sent": "Might not be generated by some of the events that we know about, so there could be noise that generate some patterns.",
                    "label": 0
                },
                {
                    "sent": "And basically what we do is that we use these groups of pattern at .2, so these patterns grouped on their news collection pair, entity set basis to bootstrap the process.",
                    "label": 0
                },
                {
                    "sent": "So we're saying all the patterns extracted from the same set of entities for the sameness collection.",
                    "label": 0
                },
                {
                    "sent": "It's including the same event, so use it as a starting point for the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is in more detail the way that we extract patterns.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very simple approach.",
                    "label": 0
                },
                {
                    "sent": "We start from dependency parsed.",
                    "label": 0
                },
                {
                    "sent": "Sentences.",
                    "label": 0
                },
                {
                    "sent": "So we have that dependency parts.",
                    "label": 0
                },
                {
                    "sent": "We have entities which have disambiguated.",
                    "label": 0
                },
                {
                    "sent": "Then we have Freebase types associated to each entity.",
                    "label": 0
                },
                {
                    "sent": "And we select different subsets of entities.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that we want to select entities E1 and E2.",
                    "label": 0
                },
                {
                    "sent": "First of all, we extract the minimum spanning tree from these from the graph, and in this case as you can see, the MST is not very expressive because it's just like 2 nodes connected by conjunction.",
                    "label": 1
                },
                {
                    "sent": "And then we apply upset over Distichs, which are actually very simple and they just try to enforce the fact that the patterns that we extract should be possibly well formed.",
                    "label": 0
                },
                {
                    "sent": "Do these really sticks?",
                    "label": 0
                },
                {
                    "sent": "Are things like there should be a verb in the pattern, and if there is a verb, there should be a subject.",
                    "label": 0
                },
                {
                    "sent": "If the verb is transitive, there should be an object.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So if there are two entities which are connected by conjunction relation, there should be a conjunction in between.",
                    "label": 0
                },
                {
                    "sent": "So very simplistic sits like a dozen rules.",
                    "label": 0
                },
                {
                    "sent": "And finally, so we go from three to three.",
                    "label": 0
                },
                {
                    "sent": "We have these patterns to which we applied this holistic's, and then in four we have the actual patterns that we use for the clustering, which is the same pattern history, but in which we have recombined all the entity type assignments so that each entity is instantiated with with one particular type.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this case we would generate the patterns person, an actress to marry and celebrity an actress.",
                    "label": 0
                },
                {
                    "sent": "Harry, OK. And as far as the noise your model goes, these patterns are just atomic units, so there is no sophisticated.",
                    "label": 0
                },
                {
                    "sent": "Structural matching going on.",
                    "label": 0
                },
                {
                    "sent": "We're not looking at subtrees overlap or whatever, we're just treating them as strings and we're comparing strings.",
                    "label": 0
                },
                {
                    "sent": "So two patterns.",
                    "label": 0
                },
                {
                    "sent": "Either they're the same or they're different, so the relevant information here for the clustering algorithm is the way that these clusters.",
                    "label": 0
                },
                {
                    "sent": "Sorry, these patterns are grouped in the previous stage.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing that you may notice that using this very simple approach.",
                    "label": 0
                },
                {
                    "sent": "If we in the same patterns we have the same entity assignments, the same type assignments for two entities, it's not really possible to understand what is the direction of the relation, so this is not a problem for symmetric relations like weddings, becausw.",
                    "label": 1
                },
                {
                    "sent": "If Tom Crew is mariscotti ormes, it's the same as if.",
                    "label": 0
                },
                {
                    "sent": "Because we have celebrity marriage that this fine.",
                    "label": 1
                },
                {
                    "sent": "But if Tom Crew is killed Katie Holmes, then there is a difference because we want to understand who is killing whom.",
                    "label": 0
                },
                {
                    "sent": "So in this case the relation is no longer symmetric.",
                    "label": 0
                },
                {
                    "sent": "And actually we solved this problem with a very simple approach again.",
                    "label": 0
                },
                {
                    "sent": "We just sort the entities that appear in a pattern alphabetically and we assign an alphabetical offset to all the entities that appear in a pattern.",
                    "label": 0
                },
                {
                    "sent": "So in the first case we would have.",
                    "label": 0
                },
                {
                    "sent": "These two variants of the pattern Celebrity, Two marriage celebrity, one and Celebrity one marriage celebrity tool.",
                    "label": 0
                },
                {
                    "sent": "Now this is a symmetric relation, so it's very likely that we would observe both these patterns within the same news collection, becauses in this collection you will see that Tom Cruise married Kathy Ormson categories maritime queries.",
                    "label": 0
                },
                {
                    "sent": "So we can learn that these two events are actually the same.",
                    "label": 0
                },
                {
                    "sent": "These two event representations are the same and therefore they would end up in the same cluster, whereas in this other case, it's very unlikely that in the same news collection we would observe that Tom Cruise is killed.",
                    "label": 0
                },
                {
                    "sent": "Katie Holmes and Catacombs Skelton crew is.",
                    "label": 0
                },
                {
                    "sent": "Therefore what would happen is that we would learn two different clusters, one in which the first entity is alphabetically coming after the other one and the other one in which the other one is sorted before the other.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this simple approach, we can actually model a lot of interesting linguistic phenomena.",
                    "label": 0
                },
                {
                    "sent": "As I already mentioned, we can capture symmetry and symmetry becausw as in metric relations would end up like in different clusters.",
                    "label": 0
                },
                {
                    "sent": "We can model arguments rotations so we can learn that a team acquiring an athlete from a second team is the same thing as the second team selling an athlete to the first team.",
                    "label": 0
                },
                {
                    "sent": "And we can also learn synthetic alternations like we can learn passive forms and for example different forms of the different synthetic teams will develop give these are toy examples, But actually I'm going to show you Now some real examples coming from the classes that we have learned.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are like extrapolations of some of the clusters, because the clusters can have several hundreds of patterns inside.",
                    "label": 0
                },
                {
                    "sent": "So the first one is very interesting.",
                    "label": 0
                },
                {
                    "sent": "This is a ternary relation an we can see that we can learn.",
                    "label": 0
                },
                {
                    "sent": "That's a professional sports team replacing a coach with another coach is equivalent to the professional sport team hiring the second coach.",
                    "label": 0
                },
                {
                    "sent": "We see that the offset of the entity is 0 after firing the first one.",
                    "label": 0
                },
                {
                    "sent": "As another.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample we can also learn like figurative ways of convey some information so we can learn that the team dismissing a player is the same thing as the acts of the team following on the player, for example, or that it's the same as a soccer player being sacked by the team.",
                    "label": 0
                },
                {
                    "sent": "And this is an example of a passive form that we can learn.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another interesting thing is that we can also learn to disambiguate the meaning of verbs given the context provided by the entity types that participate in an event.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can learn that's armed, force a warning, sorry, awarding a contract to an aerospace business is different from a person being awarded some price.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "User information for headline generation.",
                    "label": 1
                },
                {
                    "sent": "It's pretty straightforward once we have our model, we just extract patterns from a stream of news.",
                    "label": 1
                },
                {
                    "sent": "We use a two step random walk in the event model.",
                    "label": 0
                },
                {
                    "sent": "So we first identify what are the events that are most likely given the patterns that we observe in a new collection, and then we go back to the patterns in the model and we find the patterns which are most likely given the activated clusters.",
                    "label": 0
                },
                {
                    "sent": "So at the end of this block here.",
                    "label": 0
                },
                {
                    "sent": "We select the patterns which are most likely given the news collection, but these patterns are coming from the model.",
                    "label": 0
                },
                {
                    "sent": "They are no longer coming from the new collection.",
                    "label": 0
                },
                {
                    "sent": "There are different.",
                    "label": 0
                },
                {
                    "sent": "They might be similar or the same to some pattern observed, but in principle they are different.",
                    "label": 0
                },
                {
                    "sent": "And once we have the patterns, we can just generate the headlines very simply, because we've seen the patterns are completely instantiated.",
                    "label": 0
                },
                {
                    "sent": "We just need to replace the entities with the real evidence coming from the new collection.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example of headlines.",
                    "label": 0
                },
                {
                    "sent": "This is a subset of unused collection about the engagement of Alyssa Milano.",
                    "label": 0
                },
                {
                    "sent": "With this David Bujari, even though in the titles the IT never appears, we can generate titles like Alyssa Milano is engaged to David will Gary at least someone is set to marry David will Gary and several other equivalent forms of the same predicate.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So could you valuation?",
                    "label": 0
                },
                {
                    "sent": "The objective of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evolution is to demonstrate that we can generate headlines that are compatible in quality with extractive approaches.",
                    "label": 0
                },
                {
                    "sent": "Extractive approaches are those that basically select a headline from a news collection.",
                    "label": 0
                },
                {
                    "sent": "OK, as you can imagine, these are human generated headlines because they have been written by someone.",
                    "label": 0
                },
                {
                    "sent": "So comparing against extractive approaches for an abstractive approaches is challenging, so we compare against five baselines.",
                    "label": 0
                },
                {
                    "sent": "One is topics on which is a state of the art summarizer.",
                    "label": 0
                },
                {
                    "sent": "This basically selects from a news collection the sentence that minimizes the KL divergent's with respect to the distribution of the news collection.",
                    "label": 0
                },
                {
                    "sent": "Then we select the most frequent headline in the news collection, we select the latest headline in the news collection.",
                    "label": 1
                },
                {
                    "sent": "And then we also have this baseline which we call most frequent pattern which does the pattern extraction process.",
                    "label": 1
                },
                {
                    "sent": "But it doesn't do the inference in the event model.",
                    "label": 0
                },
                {
                    "sent": "So we extract patterns from the news collection.",
                    "label": 0
                },
                {
                    "sent": "We find the most frequent one, and we just replace the entity placeholders with the evidence from this collection.",
                    "label": 0
                },
                {
                    "sent": "So this one is selecting a pattern which is observed in the news collection.",
                    "label": 0
                },
                {
                    "sent": "And then we're comparing against a set of the art abstractive approach which is multi sentence compression by Philip over.",
                    "label": 0
                },
                {
                    "sent": "She's also working at Google and this is basically a system that is building a lattice of all the titles and 1st sentence is in a news collection and then it's finding the shortest path between a beginning of sentence tag and end of sentence tag that maximizes some weights that depend on the frequency of words and frequency of dependence relations.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we carry out both an automatic evaluation using Rouge scores and I crowdsourcing like evaluation to measure readability and informativeness of deadlines that we generate.",
                    "label": 0
                },
                {
                    "sent": "I do not have time to go into details, but we can come during the Question Time.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the results and I would like to point out some.",
                    "label": 0
                },
                {
                    "sent": "Some items here, so the first thing to notice is that according to the automatic evaluation and the human evaluation, there are different rankings of the systems, but this is not surprising because also as the previous speaker was mentioning, it's known that Rouge is not very good for differentiating between machine and human generated deadlines, especially when the summaries are very compressed.",
                    "label": 1
                },
                {
                    "sent": "When information is very compressed.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that we should mention that for us is the positive result is that heavy is statistically significantly better than MSE in all dimensions.",
                    "label": 0
                },
                {
                    "sent": "And most importantly, it feels half of the gap that separates abstractive approaches from extractive approaches.",
                    "label": 1
                },
                {
                    "sent": "So it's getting very close to the performance of topics on, which is a very high performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing to mention is that, not surprisingly, extractive methods are evaluated very positively.",
                    "label": 1
                },
                {
                    "sent": "This is not surprising because the human raters.",
                    "label": 0
                },
                {
                    "sent": "Are shown real headlines that have been observed in the news, and it makes perfectly sense that they like them.",
                    "label": 0
                },
                {
                    "sent": "We should also mention, and this is maybe a problem in the way that we set up the evaluation that we didn't ask a question.",
                    "label": 0
                },
                {
                    "sent": "Trying the Raiders to judge explicitly the objectivity of the headlines, which is a parameter in said that we should have measured directly.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And top exam is significantly better than all the other methods.",
                    "label": 1
                },
                {
                    "sent": "The only exception is the informativeness dimension when compared to the most frequent headline.",
                    "label": 0
                },
                {
                    "sent": "Topic time is so good that we are thinking that it would be possible to use it to generate references for automatic evaluation because the performance of the exam on this task is really very very good.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, the most frequent pattern and heady are compatible across all metrics.",
                    "label": 1
                },
                {
                    "sent": "Head does a little bit better than MFP concerning readability, even though the difference is not significant and this might suggest that the abstraction is kind of compensating for the grammatical errors introduced by the pattern extraction stage.",
                    "label": 0
                },
                {
                    "sent": "Again, the main difference between these two approaches is that heavy is abstractive, whereas the most frequent pattern is extracted.",
                    "label": 1
                },
                {
                    "sent": "So in the most frequent pattern we are showing a pattern coming from the new collection.",
                    "label": 0
                },
                {
                    "sent": "Eddie is showing a pattern coming from the noise your model, so it's a pattern that has not been observed in the collection.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude I have presented.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An unsupervised open domain and what I like most very simple framework for event learning.",
                    "label": 1
                },
                {
                    "sent": "It's based on very simple assumptions about the way that news collections are built and the fact that this information can be exploited to learn how the same event can be represented in text.",
                    "label": 1
                },
                {
                    "sent": "It captures relevant linguistic phenomena that are encoded in our event model like synthetic alternations, argument movement and it can model symmetric versus symmetric relations by distributing patterns in different clusters.",
                    "label": 0
                },
                {
                    "sent": "We use it for obstructive weather in generation and we significantly improved over a set of the arts and we also have the gap with respect to extractive methods.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many directions for improvement, the first of which is to try to improve the way that these patterns are extracted because their risztics, as I was mentioning are very simple.",
                    "label": 0
                },
                {
                    "sent": "In many cases they fail introducing noise in the event model and producing headlines, which sometimes are ungrammatical.",
                    "label": 1
                },
                {
                    "sent": "So what we would like to do is to automatically learn the rules that we should encode to build correct patterns directly from data.",
                    "label": 0
                },
                {
                    "sent": "And then we would like to explore the possibility of applying the event model to populating knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "Because we can filter incoming news and we can observe events that are happening in the news and we can populate world models with this information.",
                    "label": 0
                },
                {
                    "sent": "And finally, we would like to look into header and personalization for example.",
                    "label": 0
                },
                {
                    "sent": "Or it would be possible once we have these abstract model and we have all the ways in which the same event can be expressed to select which pattern to use to generate the headline based on the reading level of the user or for example or there still.",
                    "label": 0
                },
                {
                    "sent": "And that would be all.",
                    "label": 0
                },
                {
                    "sent": "And thank you all for you.",
                    "label": 0
                },
                {
                    "sent": "I'm really happy to see more work on abstractive summarization and that goes for this talk in the talk which came before.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering on your results where you presented where you talked about the fact that topic sum is so good some somehow.",
                    "label": 0
                },
                {
                    "sent": "Something seems a little bit wrong there an.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering if rather than using the automatic metrics if and.",
                    "label": 0
                },
                {
                    "sent": "I know you, you are using some the responsiveness and informativeness.",
                    "label": 0
                },
                {
                    "sent": "Yeah, readability and informative for madness.",
                    "label": 0
                },
                {
                    "sent": "I wonder if you might do an error analysis to look in at the cases where yours fails and topic some.",
                    "label": 0
                },
                {
                    "sent": "Wins or the cases where topics some fails an yours does better.",
                    "label": 0
                },
                {
                    "sent": "I mean, I somehow my guess would be that the metrics are part of what's causing problem, but this is a setting which is very very good setting for topic sound 'cause we're we're feeding topics.",
                    "label": 0
                },
                {
                    "sent": "Um, with all the headlines from the news collection and we're asking it to select the best one.",
                    "label": 0
                },
                {
                    "sent": "So it's basically calculating statistics about the word distributions and it's finding the headline that.",
                    "label": 0
                },
                {
                    "sent": "Maximizes the coverage of the news collection, so it's not so surprising that topic Sam does very well on the task.",
                    "label": 0
                },
                {
                    "sent": "And if it were working on full document bodies then you then yeah, I would agree with you.",
                    "label": 0
                },
                {
                    "sent": "But OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "You are making an assumption that your set of documents from Google News is on contains only documents that are on very closely closely related topic is that not such a strong assumption is assumption doesn't hold all the time.",
                    "label": 0
                },
                {
                    "sent": "It generally holds that the fact is that we are using a lot of data, so the numbers are actually in the paper.",
                    "label": 0
                },
                {
                    "sent": "I don't remember, but we're talking about millions of news collections.",
                    "label": 0
                },
                {
                    "sent": "So even though the assumptions is not completely true, it's very easy to isolate noise.",
                    "label": 0
                },
                {
                    "sent": "There are some problems which are more difficult to solve, like for example, one problem of the clustering is that the clustering of the news is that whenever there is an event like for example a sports match.",
                    "label": 0
                },
                {
                    "sent": "We tend to have in the same clusters news about the event itself and then like the pre match talk and the post match talk and in this case is the clusters are very noisy and the events that we learn are also quite noisy.",
                    "label": 0
                },
                {
                    "sent": "So in that case it's more difficult to pick up a headline and be sure that you are generating a rights title so there is noise to the news collections.",
                    "label": 0
                },
                {
                    "sent": "Of course to some extent it's corrected by the fact that we are leveraging a lot of data.",
                    "label": 0
                },
                {
                    "sent": "To some extent it's still there and we need to work to improve it.",
                    "label": 0
                },
                {
                    "sent": "Of course OK, great thanks.",
                    "label": 0
                },
                {
                    "sent": "I have only one small question.",
                    "label": 0
                },
                {
                    "sent": "Are you doing or are you planning on doing that for other languages as well?",
                    "label": 0
                },
                {
                    "sent": "I'm doing it for you.",
                    "label": 0
                },
                {
                    "sent": "Planning to do that for other languages as well.",
                    "label": 0
                },
                {
                    "sent": "Definitely we are planning to do that.",
                    "label": 0
                },
                {
                    "sent": "The problem is always that the NLP pipeline for other languages is not as good As for English, so we rely a lot on the fact that, for example, entities are disambiguated within certain bounds of accuracy.",
                    "label": 0
                },
                {
                    "sent": "But yeah, we're looking into that.",
                    "label": 0
                },
                {
                    "sent": "Of course, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "And you had a great talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So probably one last small.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "So you evaluate the quality of your headlines across many dimensions.",
                    "label": 0
                },
                {
                    "sent": "Informatives readability.",
                    "label": 0
                },
                {
                    "sent": "What about like the novelty, the impact of the headline?",
                    "label": 0
                },
                {
                    "sent": "So of course when you analyze menus in the same cluster, there will be some redundant information.",
                    "label": 0
                },
                {
                    "sent": "Like for example you know the wife of Tom Cruise this Katie Campbell and they're divorcing.",
                    "label": 0
                },
                {
                    "sent": "So of course putting this information that everybody already knows that there are married.",
                    "label": 0
                },
                {
                    "sent": "But what what is interesting?",
                    "label": 0
                },
                {
                    "sent": "Is there actually divorcing?",
                    "label": 0
                },
                {
                    "sent": "So I was wondering how, how would you address this issue?",
                    "label": 0
                },
                {
                    "sent": "So if you are using any kind of existing relationship from the Freebase as a good prior to actually look for the novel relations, where to put in?",
                    "label": 0
                },
                {
                    "sent": "The fact is that we saw the data set that we OPS.",
                    "label": 0
                },
                {
                    "sent": "The data set that we are using for the test set is related to all the news.",
                    "label": 0
                },
                {
                    "sent": "It's between 2008 and 2012, so it's very difficult to discover new relations that, for example, are not already in Freebase.",
                    "label": 0
                },
                {
                    "sent": "But that's definitely a direction in which we are working.",
                    "label": 0
                },
                {
                    "sent": "That is, using the event model to understand which events are being reported in the news, and to understand how much we can, for example, update knowledge basis like 3 based on the Knowledge Graph an we have some measure of that, but it's not final.",
                    "label": 0
                },
                {
                    "sent": "And by the way, the evaluation is limited in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Already asking for.",
                    "label": 0
                },
                {
                    "sent": "So what we should have done and we didn't do innovation is to ask for the.",
                    "label": 0
                },
                {
                    "sent": "Objectivity of the headline because the motivation behind this work was to try to understand if we can actually generate headlines which are more objective than those that you find in the news collections.",
                    "label": 0
                },
                {
                    "sent": "And I mean, we can see that's the case.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have a strong feeling that we're going in the right direction, but unlikely with gentle with dimension.",
                    "label": 0
                },
                {
                    "sent": "OK thanks anyway, OK so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "One more thank you.",
                    "label": 0
                }
            ]
        }
    }
}