{
    "id": "wkrd7stmoozm7btjhdo7fgw26r25jcap",
    "title": "The Dynamics of AdaBoost",
    "info": {
        "author": [
            "Cynthia Rudin, Sloan School of Management, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/mlss05us_rudin_da/",
    "segmentation": [
        [
            "I get sick of talking about it because it's just so interesting and this is joint work with in Kenosha and Russia, Perry.",
            "So I'm going to start with a little story about Adaboost I'm going to."
        ],
        [
            "Start with the fact that Oedipus was introduced in 1997 by your friend in Russia.",
            "And it was introduced as an algorithm for solving classification problems.",
            "No other types of problems, just classification problems.",
            "Keep that in mind for later.",
            "Now also Adaboost was given a prescribed number of iterations, after which the user was supposed to stop running the algorithm because without it would overfit.",
            "If they ran it past that prescribed number of iterations.",
            "But in fact, thank goodness the experimentalist paid absolutely no attention to this fact and ran out of boost well past its prescribed number of iterations, and they found that in fact Adaboost often tends not to overfit.",
            "So even after they ran it well well past the prescribed number of iterations, they say Adaboost performance continues to increase, so the designers of Adaboost invented this margin theory to try and explain these observations and the margin theory.",
            "For boosting is similar to the margin theory for support vector machines, in that it's based on loose generalization bounds in terms of the margin which you heard about yesterday morning, but I want to tell you two major differences in the theory between support vector machines.",
            "Actually, the history between support vector machines and boosting.",
            "So for support vector machines the theory came first, so you have this generalization bound in terms of the margin.",
            "So you design the support vector machine to maximize the margin, but you have to remember that for Adaboost.",
            "The algorithm, oh boy.",
            "Yeah, the algorithm was invented before the margin theory so that the algorithm was not specifically designed to maximize the margin.",
            "So this is, you know this is a huge difference, so it kind of begs this.",
            "And does Adaboost maximize the margin?",
            "OK, so you think this is a yes or no answer.",
            "We should be able to answer this thing, you know, just couple of examples.",
            "We can tell whether Adaboost is maximizing the margin or not."
        ],
        [
            "But it turned out that this question is quite a bit harder than than we originally thought, so there are a number of scientists who attempted to, you know, figure out the answer to the question so they you know they ran out of boost a whole bunch of times on different datasets and what they found is that Adaboost seems to maximize the margin in the limit, so they think the answer is yes, every time they try it, Adaboost maximizes the margin.",
            "OK, so there are some theoretical results that helped."
        ],
        [
            "This claim up.",
            "And the theoretical results say that Adaboost gets at least halfway there it generates a margin that's at least 1/2 rho where rho is the maximum margin.",
            "So it seems like the answer is going to be yes, you know we're getting halfway there and actually Russian Bournemouth have tightened this bound slightly.",
            "They tightened it to this function Y of row, which is slightly bigger than 1/2 rho and it actually has a very strange.",
            "It's a very strange formula that gets you this."
        ],
        [
            "They could see this function and so that gets us even closer to guests, but I'd like you to notice that there's still this very large gap between theory and practice.",
            "So what this is saying is that if the if the maximum margin is .6, Adaboost gets at least to about .35.",
            "Now, that's not, you know that's not necessarily going to be good enough.",
            "OK, so I would before I go on like to explain a little bit more about the results in this paper, which is a beautiful paper they."
        ],
        [
            "Have studied Adaboost in two cases, so they say, well, there's two cases in which we can study Adaboost.",
            "The optimal case in the non optimal case and in the optimal case the week learning algorithm has to choose the best week classifier at each iteration.",
            "So for example, if you're running an algorithm called Boost texture, you're going to be running at a boost in the optimal case, But there's this other case in which people often use Adaboost, which is the non optimal case where the week learning algorithm is only required to choose a sufficiently good week classifier and not necessarily the best one for the job.",
            "So for example, if you're putting out a boost on top of a decision tree or neural network, you're probably running in the non optimal case.",
            "So we have these two different ways you can use Adaboost and what their paper actually says is that they think they're bound.",
            "This wiro is tight for the non optimal case."
        ],
        [
            "So let me just, I'll just give you the bottom line here.",
            "They think the answer is yes for the optimal case, but they say, well, you know we did some studies and we think maybe the answer is no for the non optimal case.",
            "So in other words they think the answer is yes, but they think it's kind of sensitive to the type of weak learning algorithm using and sort of whether it's being run to its best capacity.",
            "OK, so there were hundreds of paper."
        ],
        [
            "Is that Rob listed a whole bunch of papers at one point?",
            "Hundreds of papers were published using Adaboost between these years, even though these fundamental convergence properties were not understood, they did not know whether Adaboost maximize the margin, and so even after seven years this problem was still open and I have to say, though, just to qualify this that most of the people who are running out of boost.",
            "They don't care about convergence of Adaboost.",
            "In fact, they don't care bout convergence of anything, they just want an algorithm to run.",
            "They wanted to do well, but I'm sure there are some people out there who actually care whether the algorithm is achieving a margin of .35, when in fact the true marginis.",
            "6.",
            "OK."
        ],
        [
            "So the problem is still open even after this time, and the reason is because Adaboost is difficult to analyze, and it's because usually what you do well, the margin doesn't always increase it every iteration.",
            "It kind of bubbles up and down, so the usual tricks don't work.",
            "'cause usually what you do is you add up the increases in the margin at at each iteration you see whether it got to the maximum, but in this case, since it goes up and down, you can't do that, and so we really needed some kind of new approach to understand the convergence of this algorithm.",
            "And so I won't keep you in suspense any longer.",
            "The answer is actually no.",
            "It's exactly the opposite of what everyone thought.",
            "In fact, Adaboost may converge to a margin that significantly below the maximum value, and what's even more interesting is that Russian bormuth actually were were they were even more right than they thought they were.",
            "Bound is actually tight, so in other words, non optimal Adaboost.",
            "You can actually get it to converge to exactly there bound, so I can get Adaboost to converge to exactly a margin of livro in a very specific limiting condition, which I'll describe a little bit later.",
            "And this theorem down here is a specific case of a more general theorem, which I hope to be able to state later, but what I'm going to do for the rest of my talk is I'm going to prove this theorem question."
        ],
        [
            "Or other.",
            "OK so I will introduce notation in a minute so I'll be able to answer your question in a minute.",
            "OK, so just this is an overview of my talk.",
            "The first thing I wanted to do is give you this little history about the margin theory for boosting, and I finished that and now what I'm going to do is give you an introduction to Adaboost.",
            "I know you've already seen it, but I know that a lot of you hadn't seen it before yesterday, so I'm going to go over it again, and then I'm going to spend the rest of my talk giving the proof of the theorem that Adaboost doesn't always converge to a maximum margin solution, and the proof of the theorem consists of reducing Adaboost to a dynamical system in order to understand its convergence properties.",
            "So in other words, we have an analysis problem.",
            "We're trying to understand convergence of an algorithm.",
            "What we did was we converted it into a physics problem in order to solve it.",
            "OK, so here's a sample problem you can keep in mind just to have something in my."
        ],
        [
            "OK, so let's say that you have a database of new."
        ],
        [
            "Articles and the articles are labeled plus one.",
            "If the category of the article is entertainment and negative, 1 otherwise, and we're in a standard classification setting, our goal is given a new article which is not in the database.",
            "We want the computer to automatically be able to find its label.",
            "So to tell us whether it's entertainment or not entertainment, so just you know, straightforward classification.",
            "We have a whole bunch of algorithms for first, you know, solving these kinds of problems and we have boosting and boosting can be used.",
            "As I mentioned in two ways, the optimal case in the non optimal case.",
            "So you can use it.",
            "You can use the boosting algorithm to do most of the work for you.",
            "For example with boost texture, so you can boost sort of very basic features and come up with some combination of some very basic features.",
            "Or you can use it as a wrapper for another algorithm.",
            "You can plop it on top of a decision tree or neural network, which would mean that you were probably using it in the non optimal case and see here.",
            "So just to formalize the problem for you."
        ],
        [
            "As usual, we have this training data, which is in the form of M ordered pairs, where each ordered pair is chosen randomly from an unknown probability distribution on the SpaceX cross negative 11 and here remember access the space of all possible news articles and negative one to the labels.",
            "And so you know, given a new article, what's the label?",
            "And how do we construct a classifier of course."
        ],
        [
            "To do the usual thing, which is, we're going to divide our SpaceX into two sections based on the sign of a function and the decision boundary is going to be the zero level set of that function.",
            "So what we're trying to do is we're trying to construct the function in a nice way so that it's zero level set is in the right place to be a nice decision boundary.",
            "OK, so now we're going to go into boosting.",
            "Let's say that we have a we."
        ],
        [
            "Learning algorithm and remember weak learning algorithm produces weak classifiers.",
            "Then you should think of a weak classifier as a rule of thumb.",
            "So for example, if we're sorting these articles into entertainment and not entertainment, weak classifier might be to take a look at the article."
        ],
        [
            "Whether it contains the term movie and classified based on that, so take a look at the article if it contains the term movies say Yep, Entertainment.",
            "Does it contain the term movie?",
            "Nope, not entertainment.",
            "And of course the."
        ],
        [
            "As you know, not a particularly good way to classify things, but it gives us some information, at least.",
            "So what we're going to do is introduce a couple more weak classifiers so you could check for the term actor.",
            "You could check for the term drama, but instead of using any of these weak classifiers individually, you want to combine them so that you can get something stronger, and that's."
        ],
        [
            "But that's why we're going to use Adaboost, because it's going to combine the classifiers in a meaningful way.",
            "The weak classifiers in a meaningful way.",
            "So for example, the function F is going to be the sign of .4 times H 1 + .3 times H 2 + .3 * H Three, so that if the article contains the term movie, which means each one is 1 and it contains the term drama, which means H3 is 1 but it doesn't contain the term actor, which means H2 is minus one, then the value of F is sign of .4 -- .3 + .3, so it's.",
            "Positive one so we say Yep this article is entertainment.",
            "You know it contains 2 out of these three terms and even though it didn't contain the last term, we say Yep, this you know it contains 2 out of these three terms.",
            "It's an article about entertainment.",
            "OK, So what I'm really trying to say here is that you can think of a boosting algorithm as doing the following.",
            "You can think of it as taking input, which is the weak learning algorithm which produces these weak classifiers H1H2 and H3, and it takes a large training database of labeled examples.",
            "And it outputs the coefficients of the weak classifiers to make the combined classifier.",
            "So it outputs point 4.3 and .3.",
            "OK, and so."
        ],
        [
            "Just to go over what Adaboost does again, so without a boost, what we do is we have these labeled training examples and we're going to assign an importance to each training example, and at the very beginning.",
            "Oh, and the weights the weights on the training examples are going to tell the weak learning algorithm which examples are going to be important, and at the very beginning we don't know which examples are more important than which other examples.",
            "So we give all the training instances equal weights.",
            "OK, so then we take the weighted training examples and feed them into the week learning algorithm and it gives us a weak classifier and then we cluster is just a rule of thumb.",
            "It's weak classifier here says OK this should be positive and there should be negative so.",
            "So it gets a few wrong, right?",
            "'cause it's only a rule of thumb, in fact, it gets those ones wrong, and So what we're going to do is say OK at the next round, we got these wrong, so we're going to make them more important and everybody else gets their weights reduced because we got them correct.",
            "OK, so we repeat this over and over again.",
            "We take the weighted training examples, stick them into the week learning algorithm, receive a weak classifier, and it gets most of the examples right.",
            "But it misses a few, and so we're going to increase the weights on those examples and decrease the weights and all the others.",
            "And then we repeat this over and over again until we're completely blue in the face and then at the end we carefully make a linear combination of the weak classifiers obtained at all the iterations.",
            "So our final combined classifier looks something like this, and in this case it gets all the training examples correct, OK, and so the final combined classifiers of this form.",
            "It's a sign of a linear combination of the weak classifiers.",
            "OK so I need 3 pieces of notation."
        ],
        [
            "So you'll have to remember what these three things are.",
            "Number one is the matrix M and the matrix, and is a binary matrix.",
            "All of its entries are either plus one or minus one, and the matrix is going to contain all the information we need about the week learning algorithm and about the training data.",
            "So in our case, the roll axis is going to be the training example axis, so we get sort of 1 news article for each row.",
            "Now.",
            "On the column axis we get the weak classifiers.",
            "So let me just tell you again what I'm doing.",
            "I'm taking the week learning algorithm and ripping its insides out, and I'm enumerating every possible weak classifier which the week learning algorithm can produce.",
            "So there could be a huge number of these weak classifiers, and in fact too many to actually be enumerated, so I'm never going to actually enumerate this matrix in practice, but since I'm doing some theoretical work, I can consider the matrix.",
            "Just I'm going to use it as a sort of a tool.",
            "OK, and because this matrix X is the only input data boost, it contains all the information we need about the week learning algorithm in about the training data and the entries of this matrix are plus one or minus one and the entries plus one.",
            "If we classify RJ classifies training example I correctly and negative 1 otherwise.",
            "So for example, if we have this news article right here and let's say it contains the term actor but it's not an article about entertainment, then we missed it.",
            "So MI days minus one.",
            "So if the article does contain the term actor.",
            "And the article really is about entertainment than MI Days plus one.",
            "And there are two more.",
            "There are two more things that two more examples I could state.",
            "OK.",
            "So I have the matrix M and now I'm going to introduce the notation D."
        ],
        [
            "So the DTR, the distribution, the weights over the training examples at time T. So remember at each iteration we have a set of weights, one weight for each training example.",
            "And so I'm just going to write those weights out.",
            "The weights are always non negative and they add up to one.",
            "So there are discrete probability distribution over the training examples.",
            "OK, so the last thing I want to introduce is."
        ],
        [
            "This vector Lambda and the lambdas are the coefficients of the weak classifiers for this linear combination.",
            "Now remember my goal here is to construct this function F so that its decision boundary is a good.",
            "You know, a good so that its decision boundary is a good decision boundary.",
            "So what I'm trying to do really is I'm trying to find these lambdas so that this function F has a nice decision boundary.",
            "OK, so just to recap all the notes."
        ],
        [
            "And so the matrix M is this very large binary matrix which.",
            "Which has all the information we need about the weak classifiers and about the training examples and then at each iteration we adjust two things.",
            "In Adaboost we adjust the weights on the training instances and we adjust the coefficients on the weak classifiers to form the combined classifier.",
            "OK, so we iterate until we're blue in the face.",
            "Adjusting these two quantities at every iteration and then at the end we spit out the coefficients for the final combined classifier.",
            "OK, so.",
            "I'm trying to find this final combined classifier.",
            "So because you know that's my goal, I want to have a good final combined classifier, so you'd think I'd be interested in these Lambda tease because I want to know how the Lambda teams converge.",
            "I want to know what they converge to.",
            "I want to know whether the decision whether the function made from Lambda final has a maximum margin solution.",
            "I want to know whether whether it Lambda finals going to give me a maximum margin solution, but the problem is that studying the Lambda teas are really really difficult.",
            "Trying to understand the convergence properties of those Lambda tease is not something I want to do because they spiral out to Infinity and they just there.",
            "There are a big mess, So what I found was that studying the DTS is actually much easier and much more much, much more elegant because they do some very nice things.",
            "So what you should expect is that I'm going to be studying the evolution of the DTS to try and understand what these Lambda teas do, and in fact the DTS.",
            "Are going to do things like cycle around and in the case cases where the DT cycle around the lambdas, the normalized versions of the lambdas are going to converge.",
            "So I'm going to be able to understand completely how my Lambda teaser converging by understanding the convergence of the DTS.",
            "So you should expect the details to cycle and the lambdas normalized lambdas to converge."
        ],
        [
            "OK, so there's one more bit of notation I wanted to tell you, which is the edge, and I'm not going to find it for you, but the edges can be made from DT an from M and the edge is a number between zero and one.",
            "And.",
            "OK, so the edge the edge can be made from DTN from M and it tells you it tells us how well the weak classifiers doing at each iteration.",
            "So if the weak classifier did particularly well at iteration T, then with respect to the weighted training examples then the edge is going to be large.",
            "Otherwise the edge is going to be small so the edge tells us how well are weak classifiers doing at each iteration OK?",
            "So, um, so remember."
        ],
        [
            "What we're trying to do, we're trying to construct.",
            "We're trying to find trying to understand these Lambda finals because our decision function is made from a linear combination.",
            "You know, the vector Lambda final, that's those are the coefficients for linear combination.",
            "We want to understand what they converge to.",
            "In fact, what we want to find is whether or not Adaboost produces a maximum margin solution.",
            "So in other words, what we want to do is we want to find Lambda final an.",
            "We want to understand whether or not it maximizes the margin and the margin is defined this way.",
            "So I just want to tell you a little bit about this notation.",
            "This is a matrix.",
            "This is a vector, so when you multiply them together you get a vector.",
            "This is the ice component of that vector, so this is just a number, so I want to find out whether or not Adaboost produces the Max over Lambda of this quantity.",
            "I want to know whether it maximizes the margin."
        ],
        [
            "OK, So what I'm going to do now is I'm going to change topics and I'm going to prove this theorem for you, or at least give you some intuition about about the proof.",
            "So.",
            "As I mentioned, Adaboost is difficult to analyze because the margins don't always increase.",
            "So what we're going to do is we're going to take this dynamical systems approach and reduce Adaboost to a dynamical system in order to understand its convergence properties and when we analyze the dynamical system in simple cases were going to find these remarkable stable cycles, and Luckily for us, when we find the stable cycles, the convergence properties can be completely understood.",
            "OK, so the key to answering the open question was really a set of examples in which Atta boost convergence properties could be completely understood, and that's something that we just didn't have before.",
            "Adaboost was kind of like a black box, you know.",
            "You just send stuff into it, and then I mean you have all these nice guarantees that Rob told you about.",
            "But as far as the margin goes, all bets are off.",
            "OK, so here is the dynamical system that I promised you.",
            "It looks like."
        ],
        [
            "This does not look anything like the original Adaboost algorithm.",
            "Well, not really.",
            "I mean, the original Adaboost algorithm looks like this, and Adaboost has there's an exponential there zalog there's a renormalization.",
            "Well, this looks completely different, but The funny thing is that this thing takes everything in."
        ],
        [
            "The account to get from here to here is just an analytical squish.",
            "There's no approximation, and the reduction from here to here uses the fact that M is a binary matrix, and So what this is in fact is an iterated map because it's an equation for the weight vector at time T + 1 in terms of the weight vector at time T. So it gives us a way to directly update the weight vector.",
            "And the existence of this map really enables the study of low dimensional cases.",
            "So that's exactly what I'm going to do now.",
            "So I say, well, I have this tool.",
            "I can analyze Adaboost in low dimensional cases, so here I go.",
            "So this is the simplest case that I could come up with.",
            "This is the case where the matrix and looks like this.",
            "We have three weak classifiers and three training in."
        ],
        [
            "Senses each weak classifier misclassified one training instance.",
            "OK, now the weight vector.",
            "Remember since we have three training examples, the weight vector has three components.",
            "And they are a probability they form a discrete probability distribution on the on the instances.",
            "So what I'm talking about is the fact that the weight vectors always lie somewhere on this nice complex.",
            "But I didn't feel like plotting things on the simplex.",
            "So what I'm going to do is I'm going to be putting a lot of triangles.",
            "What I'm going to do is I'm going to take the simplex and just squash it on to that purple triangle so when I'm plotting triangles in the future, what I'm going to be plotting is the simplex projected onto the first 2 dimensions, DT1 and DT2.",
            "And what I'm going to do is I'm going to start somewhere on the simplex, actually right in the middle of the simplex where the weight vectors just one third, one third, one third, and then I'm going to send that into this dynamical system, and it's going to tell me where to go at the next iteration.",
            "So I'm going to be jumping all around this simplex and all what you'll see is I'm going to be jumping around on the triangle in this purple triangle.",
            "According to the dynamics, and that's exactly what what Adaboost is doing.",
            "OK so I'm gonna put that in the corner so you remember what I'm doing."
        ],
        [
            "And at each iteration when I just wanted to do this so you could follow the dynamics.",
            "So earlier iterations I'm going to plot small blue circles and it leader iterations.",
            "I'm going to plot large red circles, and as time increases the circles get larger and larger and they happen to get more and more red, although the color is not important that it's really the size that's important.",
            "OK, so I'm going to start here at T = 1 with the weight vectors being all equal, so DT one is 132 is 1/3 and DT three is just one minus two 2 -- 3 two one which is 1/3.",
            "OK so I start here.",
            "I take this and I send this into the dynamical system and it tells me where to go next and in fact it tells me to go over here.",
            "OK, so any guesses as to where it's going to go next?",
            "It's impossible to figure it out.",
            "I'm going to put all 50 circles up so you can see where it goes in fact.",
            "So hope you can see what's going on here at equals three.",
            "We end up actually over here at equals four.",
            "We end up here equals 5 here to equal 6 here equals 7 right over here to equals 8 = 90 = 10 eleven 1213 converging to a cycle the weight vector is converging to a cycle.",
            "OK, so what's really going on here?",
            "So let me let me analyze this for you.",
            "So what happens is that this first step."
        ],
        [
            "This JT and argmax step.",
            "What that does is it divides the simplex into three regions.",
            "There's one region where you choose.",
            "We classify 11 region where you choose we classifier two in a region where you choose we classifier 3 and then the next step.",
            "What it does is IT projects each triangle onto a line, so the date equals one triangle gets projected onto this line over here."
        ],
        [
            "That equals 2 triangle ends up here and that equals 3 triangle ends up over here."
        ],
        [
            "Sure, so that within one iteration the."
        ],
        [
            "The dynamics on the dynamical."
        ],
        [
            "System on this 2 dimensional simplex becomes a 1 dimensional iterative map.",
            "On this on the edges of this inner triangle and then from there.",
            "What happens is that different chunks of the tree."
        ],
        [
            "Google map to other chunks by a monotonic contraction, in fact, so this chunk gets mapped over here, which gets mapped over here, which gets mapped to a subset of this.",
            "So what happens is that within three iterations, each chunk of the triangle gets mapped to a subset of itself by a monotonic contraction.",
            "So you get these nice fixed points 'cause."
        ],
        [
            "Things.",
            "So there are in fact 6 fixed points.",
            "There are three fixed points for each of two possible cycles.",
            "So what in the world is she talking about?",
            "Well, The thing is what I'm."
        ],
        [
            "And here is that Oedipus converge to a cycle over here.",
            "But it could have equally converge to another cycle over here.",
            "And I hope one of you is going to ask me.",
            "Well, how does that abuse know which cycle to converge to?",
            "Well it knows how to it knows which cycle to converge to because of this step.",
            "This is this is JT is in argmax blah blah blah blah blah arc.",
            "Max is a set.",
            "So there could be more than one element in that set, and in fact at the very first iteration of Adaboost, there's three elements in the set.",
            "It doesn't know which of the weak classifiers to pick, because all of them, all of them, are the best weak classifier for the job, so it makes an arbitrary decision, and then it iteration two.",
            "It also makes an arbitrary decision between two of the weak classifiers and those two arbitrary decisions determine which of the two possible cycles that added boost converges to.",
            "And the nice thing is that I can actually calculate the coordinates of these of these fixed points.",
            "By using this equation it just comes right out.",
            "OK, now the nice thing about this.",
            "This example is that a maximum margin solution is actually attained.",
            "The maximum margin in this case is 1/3 and Adaboost actually choose that maximum marking solution.",
            "So when I was studying this, this was obviously the first case that I was looking at 'cause you know this is the case where I can really analyze it and here you know the maximum margin solution is obtained and no one had ever proved before that Adaboost achieved a maximum margin solution ever.",
            "So I looked at this and I thought, Oh my goodness, this conjecture must be true.",
            "Adaboost must always achieve a maximum margin solution.",
            "And I was studying statistical learning theory at the time and I should have known better than to judge anything by one training example.",
            "OK, so the conjecture the conjecture was true in at least one case, but you know, just wait.",
            "And also I would like to mention one more thing, which is that the edge this mysterious quantity between zero and one that I mentioned before is actually actually the Golden ratio minus one in the cycle.",
            "So whenever you're doing this kind of calculations, you see the Golden ratio absolutely everywhere.",
            "OK, so then, after I analyzed that case, that simple case what I did was I looked at the case where you have negative ones on the diagonal.",
            "So this is a case where we have Emily classifiers, each one misclassifying one training example and we find the exist."
        ],
        [
            "We can prove the existence of at least N -- 1 factorial stable cycles, each one yielding a maximum margin solution, and I would like to say though in this case we can't solve for the cycle coordinates exactly like we could for the other case because, well, we have an equation and we can't solve it, but we can prove that the equation has a unique solution for each cycle, so we can prove the solution exists.",
            "We can prove it's unique, but we can't solve for it.",
            "OK, and I also analyzed this other case where we have a number of training examples that are classified the same way by all the weak classifiers.",
            "So these these training samples are classified the same way by the weak classifiers.",
            "Same with these.",
            "Same with those.",
            "And same with these an in that."
        ],
        [
            "Case where you have these identically classified training training examples, what happens is that, well, OK, what happens is that you have stable manifolds of cycles, so before I had sort of individual cycles I had, you know, iterate 3 * 1 two, three.",
            "You end up back restarted.",
            "Now what I have here are manifolds of cycles.",
            "So what I'm saying is that there's a sort of a direction I can move in the DT space where if I iterate 123I end up back where I started from.",
            "So I have a whole continuum of these.",
            "Stable cycles, so it's a manifold of stable cycles.",
            "OK, so I also should mention that you know you're saying, hey Cynthia, you know?",
            "Your examples are contrived.",
            "Why do I care that you know you're looking at stuff worthy of negative ones in the diagonal of these very special cases?",
            "But this cycling actually occurs pretty much almost always when when you choose a fairly small matrix, like when you have a small number of weak classifiers in a small number of training instances.",
            "So every time I tried it, I actually got cycles.",
            "So let me show you some examples."
        ],
        [
            "So this is that this is a matrix."
        ],
        [
            "Randomly generated matrix that was reduced a little bit where M is this is M sorry, red is plus one Blues minus one.",
            "I sent this thing into Adaboost and then as usual the smaller circles or earlier time steps in the larger circles or later time steps.",
            "Now the circles are all inside each other, which means I'm getting cycles so cycles and I'm just going to show you a few more examples where I plugged in random matrices into Adaboost and also got cycles, yeah?",
            "That means that you you keep picking the same after three iterations.",
            "You keep picking the same influence.",
            "So what this means?",
            "What this means is that so I started sort of here.",
            "And then I went to here, here, here and here.",
            "And then after a while I was going to hear to hear, to hear, to hear, to hear.",
            "Well, I don't know what order it was.",
            "It could have been all over the place, but I'm alternating between these.",
            "You know these ones and then going back and doing it again.",
            "And it does mean that I'm picking the same set of weak weak classifiers over and over again.",
            "On that email.",
            "Well, the thing about this is OK, so the question is, does it help tell you where to stop?",
            "Well, The thing is that when Adaboost converges to a cycle here, there are some very nice things we can learn about the convergence properties, which which I haven't really told you yet, but in fact it tells you whenever you have cycling behavior.",
            "It tells you exactly what Adaboost is converging to.",
            "So.",
            "I don't know if it can tell you where to stop, but you know where to stop is kind of arbitrary because it's it's.",
            "Yeah, that's a difficult question that because 'cause I'm studying convergence properties of Adaboost, so figuring out where to stop is not exactly part of convergence, so that's kind of that's kind of a more empirical question, and I'm sort of studying throughout theoretical questions, kind of a different different style of question I think.",
            "Where to stop really depends on the application and how your experiments are going, and you should do cross validation and all that kind of stuff.",
            "And here I'm just really talking about convergence properties of the algorithm.",
            "OK, so in any case I'm."
        ],
        [
            "I'll show you some more random matrices and hopefully you'll see some more cycles.",
            "Ah, what happened, OK?",
            "OK, so this one is really interesting because it took sort of 3000 iterations and it converge to the simplest of all possible cycles, the three cycle."
        ],
        [
            "And we plotted every 20th iteration here.",
            "I mean this complete."
        ],
        [
            "Be full of circles."
        ],
        [
            "OK, so here's here's."
        ],
        [
            "Only the key."
        ],
        [
            "If Adam is cycles, we can actually calculate the margin.",
            "It's going to asymptotically converge to in terms of the cycle parameters.",
            "So if I know what the cycle is, I can compute the margin.",
            "So if I know what Adaboost is like, if I know the cycle that it's converging to, I can maybe find that out in the first few iterations if I know what cycle Adaboost is converging to, I can calculate the margin because I can plug it into this formula and it will tell me the margin that Adaboost is converging to so I can check and see whether it's a maximum margin solution, and so in fact, that really proves the theorem because."
        ],
        [
            "Because, well, because there's an example where it doesn't converge to a maximum margin solution.",
            "There is an 8 by 8 matrix where Adam was probably converges to a non maximal margin solution.",
            "Here's the matrix.",
            "Why does plus one black is minus one?",
            "And in fact, what happens here is that Adaboost converges to a manifold of strongly attracting stable three cycles."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I get sick of talking about it because it's just so interesting and this is joint work with in Kenosha and Russia, Perry.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start with a little story about Adaboost I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with the fact that Oedipus was introduced in 1997 by your friend in Russia.",
                    "label": 1
                },
                {
                    "sent": "And it was introduced as an algorithm for solving classification problems.",
                    "label": 0
                },
                {
                    "sent": "No other types of problems, just classification problems.",
                    "label": 0
                },
                {
                    "sent": "Keep that in mind for later.",
                    "label": 0
                },
                {
                    "sent": "Now also Adaboost was given a prescribed number of iterations, after which the user was supposed to stop running the algorithm because without it would overfit.",
                    "label": 0
                },
                {
                    "sent": "If they ran it past that prescribed number of iterations.",
                    "label": 0
                },
                {
                    "sent": "But in fact, thank goodness the experimentalist paid absolutely no attention to this fact and ran out of boost well past its prescribed number of iterations, and they found that in fact Adaboost often tends not to overfit.",
                    "label": 0
                },
                {
                    "sent": "So even after they ran it well well past the prescribed number of iterations, they say Adaboost performance continues to increase, so the designers of Adaboost invented this margin theory to try and explain these observations and the margin theory.",
                    "label": 0
                },
                {
                    "sent": "For boosting is similar to the margin theory for support vector machines, in that it's based on loose generalization bounds in terms of the margin which you heard about yesterday morning, but I want to tell you two major differences in the theory between support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Actually, the history between support vector machines and boosting.",
                    "label": 0
                },
                {
                    "sent": "So for support vector machines the theory came first, so you have this generalization bound in terms of the margin.",
                    "label": 0
                },
                {
                    "sent": "So you design the support vector machine to maximize the margin, but you have to remember that for Adaboost.",
                    "label": 0
                },
                {
                    "sent": "The algorithm, oh boy.",
                    "label": 1
                },
                {
                    "sent": "Yeah, the algorithm was invented before the margin theory so that the algorithm was not specifically designed to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know this is a huge difference, so it kind of begs this.",
                    "label": 0
                },
                {
                    "sent": "And does Adaboost maximize the margin?",
                    "label": 0
                },
                {
                    "sent": "OK, so you think this is a yes or no answer.",
                    "label": 0
                },
                {
                    "sent": "We should be able to answer this thing, you know, just couple of examples.",
                    "label": 0
                },
                {
                    "sent": "We can tell whether Adaboost is maximizing the margin or not.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it turned out that this question is quite a bit harder than than we originally thought, so there are a number of scientists who attempted to, you know, figure out the answer to the question so they you know they ran out of boost a whole bunch of times on different datasets and what they found is that Adaboost seems to maximize the margin in the limit, so they think the answer is yes, every time they try it, Adaboost maximizes the margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are some theoretical results that helped.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This claim up.",
                    "label": 0
                },
                {
                    "sent": "And the theoretical results say that Adaboost gets at least halfway there it generates a margin that's at least 1/2 rho where rho is the maximum margin.",
                    "label": 0
                },
                {
                    "sent": "So it seems like the answer is going to be yes, you know we're getting halfway there and actually Russian Bournemouth have tightened this bound slightly.",
                    "label": 0
                },
                {
                    "sent": "They tightened it to this function Y of row, which is slightly bigger than 1/2 rho and it actually has a very strange.",
                    "label": 0
                },
                {
                    "sent": "It's a very strange formula that gets you this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They could see this function and so that gets us even closer to guests, but I'd like you to notice that there's still this very large gap between theory and practice.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that if the if the maximum margin is .6, Adaboost gets at least to about .35.",
                    "label": 0
                },
                {
                    "sent": "Now, that's not, you know that's not necessarily going to be good enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so I would before I go on like to explain a little bit more about the results in this paper, which is a beautiful paper they.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have studied Adaboost in two cases, so they say, well, there's two cases in which we can study Adaboost.",
                    "label": 0
                },
                {
                    "sent": "The optimal case in the non optimal case and in the optimal case the week learning algorithm has to choose the best week classifier at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're running an algorithm called Boost texture, you're going to be running at a boost in the optimal case, But there's this other case in which people often use Adaboost, which is the non optimal case where the week learning algorithm is only required to choose a sufficiently good week classifier and not necessarily the best one for the job.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're putting out a boost on top of a decision tree or neural network, you're probably running in the non optimal case.",
                    "label": 0
                },
                {
                    "sent": "So we have these two different ways you can use Adaboost and what their paper actually says is that they think they're bound.",
                    "label": 0
                },
                {
                    "sent": "This wiro is tight for the non optimal case.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me just, I'll just give you the bottom line here.",
                    "label": 0
                },
                {
                    "sent": "They think the answer is yes for the optimal case, but they say, well, you know we did some studies and we think maybe the answer is no for the non optimal case.",
                    "label": 0
                },
                {
                    "sent": "So in other words they think the answer is yes, but they think it's kind of sensitive to the type of weak learning algorithm using and sort of whether it's being run to its best capacity.",
                    "label": 0
                },
                {
                    "sent": "OK, so there were hundreds of paper.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that Rob listed a whole bunch of papers at one point?",
                    "label": 0
                },
                {
                    "sent": "Hundreds of papers were published using Adaboost between these years, even though these fundamental convergence properties were not understood, they did not know whether Adaboost maximize the margin, and so even after seven years this problem was still open and I have to say, though, just to qualify this that most of the people who are running out of boost.",
                    "label": 0
                },
                {
                    "sent": "They don't care about convergence of Adaboost.",
                    "label": 1
                },
                {
                    "sent": "In fact, they don't care bout convergence of anything, they just want an algorithm to run.",
                    "label": 0
                },
                {
                    "sent": "They wanted to do well, but I'm sure there are some people out there who actually care whether the algorithm is achieving a margin of .35, when in fact the true marginis.",
                    "label": 0
                },
                {
                    "sent": "6.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is still open even after this time, and the reason is because Adaboost is difficult to analyze, and it's because usually what you do well, the margin doesn't always increase it every iteration.",
                    "label": 0
                },
                {
                    "sent": "It kind of bubbles up and down, so the usual tricks don't work.",
                    "label": 0
                },
                {
                    "sent": "'cause usually what you do is you add up the increases in the margin at at each iteration you see whether it got to the maximum, but in this case, since it goes up and down, you can't do that, and so we really needed some kind of new approach to understand the convergence of this algorithm.",
                    "label": 1
                },
                {
                    "sent": "And so I won't keep you in suspense any longer.",
                    "label": 0
                },
                {
                    "sent": "The answer is actually no.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the opposite of what everyone thought.",
                    "label": 0
                },
                {
                    "sent": "In fact, Adaboost may converge to a margin that significantly below the maximum value, and what's even more interesting is that Russian bormuth actually were were they were even more right than they thought they were.",
                    "label": 0
                },
                {
                    "sent": "Bound is actually tight, so in other words, non optimal Adaboost.",
                    "label": 0
                },
                {
                    "sent": "You can actually get it to converge to exactly there bound, so I can get Adaboost to converge to exactly a margin of livro in a very specific limiting condition, which I'll describe a little bit later.",
                    "label": 0
                },
                {
                    "sent": "And this theorem down here is a specific case of a more general theorem, which I hope to be able to state later, but what I'm going to do for the rest of my talk is I'm going to prove this theorem question.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or other.",
                    "label": 0
                },
                {
                    "sent": "OK so I will introduce notation in a minute so I'll be able to answer your question in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, so just this is an overview of my talk.",
                    "label": 0
                },
                {
                    "sent": "The first thing I wanted to do is give you this little history about the margin theory for boosting, and I finished that and now what I'm going to do is give you an introduction to Adaboost.",
                    "label": 0
                },
                {
                    "sent": "I know you've already seen it, but I know that a lot of you hadn't seen it before yesterday, so I'm going to go over it again, and then I'm going to spend the rest of my talk giving the proof of the theorem that Adaboost doesn't always converge to a maximum margin solution, and the proof of the theorem consists of reducing Adaboost to a dynamical system in order to understand its convergence properties.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we have an analysis problem.",
                    "label": 0
                },
                {
                    "sent": "We're trying to understand convergence of an algorithm.",
                    "label": 0
                },
                {
                    "sent": "What we did was we converted it into a physics problem in order to solve it.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a sample problem you can keep in mind just to have something in my.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's say that you have a database of new.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Articles and the articles are labeled plus one.",
                    "label": 0
                },
                {
                    "sent": "If the category of the article is entertainment and negative, 1 otherwise, and we're in a standard classification setting, our goal is given a new article which is not in the database.",
                    "label": 0
                },
                {
                    "sent": "We want the computer to automatically be able to find its label.",
                    "label": 0
                },
                {
                    "sent": "So to tell us whether it's entertainment or not entertainment, so just you know, straightforward classification.",
                    "label": 0
                },
                {
                    "sent": "We have a whole bunch of algorithms for first, you know, solving these kinds of problems and we have boosting and boosting can be used.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned in two ways, the optimal case in the non optimal case.",
                    "label": 0
                },
                {
                    "sent": "So you can use it.",
                    "label": 0
                },
                {
                    "sent": "You can use the boosting algorithm to do most of the work for you.",
                    "label": 0
                },
                {
                    "sent": "For example with boost texture, so you can boost sort of very basic features and come up with some combination of some very basic features.",
                    "label": 0
                },
                {
                    "sent": "Or you can use it as a wrapper for another algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can plop it on top of a decision tree or neural network, which would mean that you were probably using it in the non optimal case and see here.",
                    "label": 1
                },
                {
                    "sent": "So just to formalize the problem for you.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As usual, we have this training data, which is in the form of M ordered pairs, where each ordered pair is chosen randomly from an unknown probability distribution on the SpaceX cross negative 11 and here remember access the space of all possible news articles and negative one to the labels.",
                    "label": 0
                },
                {
                    "sent": "And so you know, given a new article, what's the label?",
                    "label": 0
                },
                {
                    "sent": "And how do we construct a classifier of course.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do the usual thing, which is, we're going to divide our SpaceX into two sections based on the sign of a function and the decision boundary is going to be the zero level set of that function.",
                    "label": 0
                },
                {
                    "sent": "So what we're trying to do is we're trying to construct the function in a nice way so that it's zero level set is in the right place to be a nice decision boundary.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to go into boosting.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have a we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning algorithm and remember weak learning algorithm produces weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "Then you should think of a weak classifier as a rule of thumb.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we're sorting these articles into entertainment and not entertainment, weak classifier might be to take a look at the article.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whether it contains the term movie and classified based on that, so take a look at the article if it contains the term movies say Yep, Entertainment.",
                    "label": 0
                },
                {
                    "sent": "Does it contain the term movie?",
                    "label": 0
                },
                {
                    "sent": "Nope, not entertainment.",
                    "label": 0
                },
                {
                    "sent": "And of course the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you know, not a particularly good way to classify things, but it gives us some information, at least.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is introduce a couple more weak classifiers so you could check for the term actor.",
                    "label": 0
                },
                {
                    "sent": "You could check for the term drama, but instead of using any of these weak classifiers individually, you want to combine them so that you can get something stronger, and that's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But that's why we're going to use Adaboost, because it's going to combine the classifiers in a meaningful way.",
                    "label": 0
                },
                {
                    "sent": "The weak classifiers in a meaningful way.",
                    "label": 0
                },
                {
                    "sent": "So for example, the function F is going to be the sign of .4 times H 1 + .3 times H 2 + .3 * H Three, so that if the article contains the term movie, which means each one is 1 and it contains the term drama, which means H3 is 1 but it doesn't contain the term actor, which means H2 is minus one, then the value of F is sign of .4 -- .3 + .3, so it's.",
                    "label": 0
                },
                {
                    "sent": "Positive one so we say Yep this article is entertainment.",
                    "label": 1
                },
                {
                    "sent": "You know it contains 2 out of these three terms and even though it didn't contain the last term, we say Yep, this you know it contains 2 out of these three terms.",
                    "label": 0
                },
                {
                    "sent": "It's an article about entertainment.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm really trying to say here is that you can think of a boosting algorithm as doing the following.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as taking input, which is the weak learning algorithm which produces these weak classifiers H1H2 and H3, and it takes a large training database of labeled examples.",
                    "label": 1
                },
                {
                    "sent": "And it outputs the coefficients of the weak classifiers to make the combined classifier.",
                    "label": 0
                },
                {
                    "sent": "So it outputs point 4.3 and .3.",
                    "label": 0
                },
                {
                    "sent": "OK, and so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to go over what Adaboost does again, so without a boost, what we do is we have these labeled training examples and we're going to assign an importance to each training example, and at the very beginning.",
                    "label": 0
                },
                {
                    "sent": "Oh, and the weights the weights on the training examples are going to tell the weak learning algorithm which examples are going to be important, and at the very beginning we don't know which examples are more important than which other examples.",
                    "label": 0
                },
                {
                    "sent": "So we give all the training instances equal weights.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we take the weighted training examples and feed them into the week learning algorithm and it gives us a weak classifier and then we cluster is just a rule of thumb.",
                    "label": 0
                },
                {
                    "sent": "It's weak classifier here says OK this should be positive and there should be negative so.",
                    "label": 0
                },
                {
                    "sent": "So it gets a few wrong, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it's only a rule of thumb, in fact, it gets those ones wrong, and So what we're going to do is say OK at the next round, we got these wrong, so we're going to make them more important and everybody else gets their weights reduced because we got them correct.",
                    "label": 0
                },
                {
                    "sent": "OK, so we repeat this over and over again.",
                    "label": 0
                },
                {
                    "sent": "We take the weighted training examples, stick them into the week learning algorithm, receive a weak classifier, and it gets most of the examples right.",
                    "label": 0
                },
                {
                    "sent": "But it misses a few, and so we're going to increase the weights on those examples and decrease the weights and all the others.",
                    "label": 0
                },
                {
                    "sent": "And then we repeat this over and over again until we're completely blue in the face and then at the end we carefully make a linear combination of the weak classifiers obtained at all the iterations.",
                    "label": 0
                },
                {
                    "sent": "So our final combined classifier looks something like this, and in this case it gets all the training examples correct, OK, and so the final combined classifiers of this form.",
                    "label": 0
                },
                {
                    "sent": "It's a sign of a linear combination of the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK so I need 3 pieces of notation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you'll have to remember what these three things are.",
                    "label": 0
                },
                {
                    "sent": "Number one is the matrix M and the matrix, and is a binary matrix.",
                    "label": 0
                },
                {
                    "sent": "All of its entries are either plus one or minus one, and the matrix is going to contain all the information we need about the week learning algorithm and about the training data.",
                    "label": 0
                },
                {
                    "sent": "So in our case, the roll axis is going to be the training example axis, so we get sort of 1 news article for each row.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "On the column axis we get the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So let me just tell you again what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "I'm taking the week learning algorithm and ripping its insides out, and I'm enumerating every possible weak classifier which the week learning algorithm can produce.",
                    "label": 0
                },
                {
                    "sent": "So there could be a huge number of these weak classifiers, and in fact too many to actually be enumerated, so I'm never going to actually enumerate this matrix in practice, but since I'm doing some theoretical work, I can consider the matrix.",
                    "label": 0
                },
                {
                    "sent": "Just I'm going to use it as a sort of a tool.",
                    "label": 0
                },
                {
                    "sent": "OK, and because this matrix X is the only input data boost, it contains all the information we need about the week learning algorithm in about the training data and the entries of this matrix are plus one or minus one and the entries plus one.",
                    "label": 0
                },
                {
                    "sent": "If we classify RJ classifies training example I correctly and negative 1 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have this news article right here and let's say it contains the term actor but it's not an article about entertainment, then we missed it.",
                    "label": 0
                },
                {
                    "sent": "So MI days minus one.",
                    "label": 0
                },
                {
                    "sent": "So if the article does contain the term actor.",
                    "label": 0
                },
                {
                    "sent": "And the article really is about entertainment than MI Days plus one.",
                    "label": 0
                },
                {
                    "sent": "And there are two more.",
                    "label": 0
                },
                {
                    "sent": "There are two more things that two more examples I could state.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I have the matrix M and now I'm going to introduce the notation D.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the DTR, the distribution, the weights over the training examples at time T. So remember at each iteration we have a set of weights, one weight for each training example.",
                    "label": 0
                },
                {
                    "sent": "And so I'm just going to write those weights out.",
                    "label": 0
                },
                {
                    "sent": "The weights are always non negative and they add up to one.",
                    "label": 0
                },
                {
                    "sent": "So there are discrete probability distribution over the training examples.",
                    "label": 1
                },
                {
                    "sent": "OK, so the last thing I want to introduce is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This vector Lambda and the lambdas are the coefficients of the weak classifiers for this linear combination.",
                    "label": 0
                },
                {
                    "sent": "Now remember my goal here is to construct this function F so that its decision boundary is a good.",
                    "label": 0
                },
                {
                    "sent": "You know, a good so that its decision boundary is a good decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to do really is I'm trying to find these lambdas so that this function F has a nice decision boundary.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to recap all the notes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the matrix M is this very large binary matrix which.",
                    "label": 0
                },
                {
                    "sent": "Which has all the information we need about the weak classifiers and about the training examples and then at each iteration we adjust two things.",
                    "label": 0
                },
                {
                    "sent": "In Adaboost we adjust the weights on the training instances and we adjust the coefficients on the weak classifiers to form the combined classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so we iterate until we're blue in the face.",
                    "label": 0
                },
                {
                    "sent": "Adjusting these two quantities at every iteration and then at the end we spit out the coefficients for the final combined classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to find this final combined classifier.",
                    "label": 0
                },
                {
                    "sent": "So because you know that's my goal, I want to have a good final combined classifier, so you'd think I'd be interested in these Lambda tease because I want to know how the Lambda teams converge.",
                    "label": 0
                },
                {
                    "sent": "I want to know what they converge to.",
                    "label": 0
                },
                {
                    "sent": "I want to know whether the decision whether the function made from Lambda final has a maximum margin solution.",
                    "label": 0
                },
                {
                    "sent": "I want to know whether whether it Lambda finals going to give me a maximum margin solution, but the problem is that studying the Lambda teas are really really difficult.",
                    "label": 0
                },
                {
                    "sent": "Trying to understand the convergence properties of those Lambda tease is not something I want to do because they spiral out to Infinity and they just there.",
                    "label": 0
                },
                {
                    "sent": "There are a big mess, So what I found was that studying the DTS is actually much easier and much more much, much more elegant because they do some very nice things.",
                    "label": 0
                },
                {
                    "sent": "So what you should expect is that I'm going to be studying the evolution of the DTS to try and understand what these Lambda teas do, and in fact the DTS.",
                    "label": 0
                },
                {
                    "sent": "Are going to do things like cycle around and in the case cases where the DT cycle around the lambdas, the normalized versions of the lambdas are going to converge.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be able to understand completely how my Lambda teaser converging by understanding the convergence of the DTS.",
                    "label": 0
                },
                {
                    "sent": "So you should expect the details to cycle and the lambdas normalized lambdas to converge.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's one more bit of notation I wanted to tell you, which is the edge, and I'm not going to find it for you, but the edges can be made from DT an from M and the edge is a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so the edge the edge can be made from DTN from M and it tells you it tells us how well the weak classifiers doing at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So if the weak classifier did particularly well at iteration T, then with respect to the weighted training examples then the edge is going to be large.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the edge is going to be small so the edge tells us how well are weak classifiers doing at each iteration OK?",
                    "label": 0
                },
                {
                    "sent": "So, um, so remember.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're trying to do, we're trying to construct.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find trying to understand these Lambda finals because our decision function is made from a linear combination.",
                    "label": 0
                },
                {
                    "sent": "You know, the vector Lambda final, that's those are the coefficients for linear combination.",
                    "label": 0
                },
                {
                    "sent": "We want to understand what they converge to.",
                    "label": 0
                },
                {
                    "sent": "In fact, what we want to find is whether or not Adaboost produces a maximum margin solution.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what we want to do is we want to find Lambda final an.",
                    "label": 0
                },
                {
                    "sent": "We want to understand whether or not it maximizes the margin and the margin is defined this way.",
                    "label": 0
                },
                {
                    "sent": "So I just want to tell you a little bit about this notation.",
                    "label": 0
                },
                {
                    "sent": "This is a matrix.",
                    "label": 0
                },
                {
                    "sent": "This is a vector, so when you multiply them together you get a vector.",
                    "label": 0
                },
                {
                    "sent": "This is the ice component of that vector, so this is just a number, so I want to find out whether or not Adaboost produces the Max over Lambda of this quantity.",
                    "label": 0
                },
                {
                    "sent": "I want to know whether it maximizes the margin.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I'm going to do now is I'm going to change topics and I'm going to prove this theorem for you, or at least give you some intuition about about the proof.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, Adaboost is difficult to analyze because the margins don't always increase.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to take this dynamical systems approach and reduce Adaboost to a dynamical system in order to understand its convergence properties and when we analyze the dynamical system in simple cases were going to find these remarkable stable cycles, and Luckily for us, when we find the stable cycles, the convergence properties can be completely understood.",
                    "label": 0
                },
                {
                    "sent": "OK, so the key to answering the open question was really a set of examples in which Atta boost convergence properties could be completely understood, and that's something that we just didn't have before.",
                    "label": 0
                },
                {
                    "sent": "Adaboost was kind of like a black box, you know.",
                    "label": 0
                },
                {
                    "sent": "You just send stuff into it, and then I mean you have all these nice guarantees that Rob told you about.",
                    "label": 0
                },
                {
                    "sent": "But as far as the margin goes, all bets are off.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the dynamical system that I promised you.",
                    "label": 0
                },
                {
                    "sent": "It looks like.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This does not look anything like the original Adaboost algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, not really.",
                    "label": 0
                },
                {
                    "sent": "I mean, the original Adaboost algorithm looks like this, and Adaboost has there's an exponential there zalog there's a renormalization.",
                    "label": 0
                },
                {
                    "sent": "Well, this looks completely different, but The funny thing is that this thing takes everything in.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The account to get from here to here is just an analytical squish.",
                    "label": 0
                },
                {
                    "sent": "There's no approximation, and the reduction from here to here uses the fact that M is a binary matrix, and So what this is in fact is an iterated map because it's an equation for the weight vector at time T + 1 in terms of the weight vector at time T. So it gives us a way to directly update the weight vector.",
                    "label": 0
                },
                {
                    "sent": "And the existence of this map really enables the study of low dimensional cases.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly what I'm going to do now.",
                    "label": 0
                },
                {
                    "sent": "So I say, well, I have this tool.",
                    "label": 0
                },
                {
                    "sent": "I can analyze Adaboost in low dimensional cases, so here I go.",
                    "label": 0
                },
                {
                    "sent": "So this is the simplest case that I could come up with.",
                    "label": 0
                },
                {
                    "sent": "This is the case where the matrix and looks like this.",
                    "label": 0
                },
                {
                    "sent": "We have three weak classifiers and three training in.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Senses each weak classifier misclassified one training instance.",
                    "label": 1
                },
                {
                    "sent": "OK, now the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Remember since we have three training examples, the weight vector has three components.",
                    "label": 1
                },
                {
                    "sent": "And they are a probability they form a discrete probability distribution on the on the instances.",
                    "label": 0
                },
                {
                    "sent": "So what I'm talking about is the fact that the weight vectors always lie somewhere on this nice complex.",
                    "label": 0
                },
                {
                    "sent": "But I didn't feel like plotting things on the simplex.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is I'm going to be putting a lot of triangles.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to take the simplex and just squash it on to that purple triangle so when I'm plotting triangles in the future, what I'm going to be plotting is the simplex projected onto the first 2 dimensions, DT1 and DT2.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to do is I'm going to start somewhere on the simplex, actually right in the middle of the simplex where the weight vectors just one third, one third, one third, and then I'm going to send that into this dynamical system, and it's going to tell me where to go at the next iteration.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be jumping all around this simplex and all what you'll see is I'm going to be jumping around on the triangle in this purple triangle.",
                    "label": 0
                },
                {
                    "sent": "According to the dynamics, and that's exactly what what Adaboost is doing.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm gonna put that in the corner so you remember what I'm doing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at each iteration when I just wanted to do this so you could follow the dynamics.",
                    "label": 0
                },
                {
                    "sent": "So earlier iterations I'm going to plot small blue circles and it leader iterations.",
                    "label": 0
                },
                {
                    "sent": "I'm going to plot large red circles, and as time increases the circles get larger and larger and they happen to get more and more red, although the color is not important that it's really the size that's important.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to start here at T = 1 with the weight vectors being all equal, so DT one is 132 is 1/3 and DT three is just one minus two 2 -- 3 two one which is 1/3.",
                    "label": 0
                },
                {
                    "sent": "OK so I start here.",
                    "label": 0
                },
                {
                    "sent": "I take this and I send this into the dynamical system and it tells me where to go next and in fact it tells me to go over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so any guesses as to where it's going to go next?",
                    "label": 0
                },
                {
                    "sent": "It's impossible to figure it out.",
                    "label": 0
                },
                {
                    "sent": "I'm going to put all 50 circles up so you can see where it goes in fact.",
                    "label": 0
                },
                {
                    "sent": "So hope you can see what's going on here at equals three.",
                    "label": 0
                },
                {
                    "sent": "We end up actually over here at equals four.",
                    "label": 0
                },
                {
                    "sent": "We end up here equals 5 here to equal 6 here equals 7 right over here to equals 8 = 90 = 10 eleven 1213 converging to a cycle the weight vector is converging to a cycle.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's really going on here?",
                    "label": 0
                },
                {
                    "sent": "So let me let me analyze this for you.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that this first step.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This JT and argmax step.",
                    "label": 0
                },
                {
                    "sent": "What that does is it divides the simplex into three regions.",
                    "label": 0
                },
                {
                    "sent": "There's one region where you choose.",
                    "label": 0
                },
                {
                    "sent": "We classify 11 region where you choose we classifier two in a region where you choose we classifier 3 and then the next step.",
                    "label": 0
                },
                {
                    "sent": "What it does is IT projects each triangle onto a line, so the date equals one triangle gets projected onto this line over here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That equals 2 triangle ends up here and that equals 3 triangle ends up over here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, so that within one iteration the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dynamics on the dynamical.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System on this 2 dimensional simplex becomes a 1 dimensional iterative map.",
                    "label": 0
                },
                {
                    "sent": "On this on the edges of this inner triangle and then from there.",
                    "label": 0
                },
                {
                    "sent": "What happens is that different chunks of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Google map to other chunks by a monotonic contraction, in fact, so this chunk gets mapped over here, which gets mapped over here, which gets mapped to a subset of this.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that within three iterations, each chunk of the triangle gets mapped to a subset of itself by a monotonic contraction.",
                    "label": 0
                },
                {
                    "sent": "So you get these nice fixed points 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "So there are in fact 6 fixed points.",
                    "label": 0
                },
                {
                    "sent": "There are three fixed points for each of two possible cycles.",
                    "label": 0
                },
                {
                    "sent": "So what in the world is she talking about?",
                    "label": 0
                },
                {
                    "sent": "Well, The thing is what I'm.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is that Oedipus converge to a cycle over here.",
                    "label": 0
                },
                {
                    "sent": "But it could have equally converge to another cycle over here.",
                    "label": 0
                },
                {
                    "sent": "And I hope one of you is going to ask me.",
                    "label": 0
                },
                {
                    "sent": "Well, how does that abuse know which cycle to converge to?",
                    "label": 0
                },
                {
                    "sent": "Well it knows how to it knows which cycle to converge to because of this step.",
                    "label": 0
                },
                {
                    "sent": "This is this is JT is in argmax blah blah blah blah blah arc.",
                    "label": 0
                },
                {
                    "sent": "Max is a set.",
                    "label": 0
                },
                {
                    "sent": "So there could be more than one element in that set, and in fact at the very first iteration of Adaboost, there's three elements in the set.",
                    "label": 0
                },
                {
                    "sent": "It doesn't know which of the weak classifiers to pick, because all of them, all of them, are the best weak classifier for the job, so it makes an arbitrary decision, and then it iteration two.",
                    "label": 0
                },
                {
                    "sent": "It also makes an arbitrary decision between two of the weak classifiers and those two arbitrary decisions determine which of the two possible cycles that added boost converges to.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that I can actually calculate the coordinates of these of these fixed points.",
                    "label": 0
                },
                {
                    "sent": "By using this equation it just comes right out.",
                    "label": 0
                },
                {
                    "sent": "OK, now the nice thing about this.",
                    "label": 0
                },
                {
                    "sent": "This example is that a maximum margin solution is actually attained.",
                    "label": 0
                },
                {
                    "sent": "The maximum margin in this case is 1/3 and Adaboost actually choose that maximum marking solution.",
                    "label": 0
                },
                {
                    "sent": "So when I was studying this, this was obviously the first case that I was looking at 'cause you know this is the case where I can really analyze it and here you know the maximum margin solution is obtained and no one had ever proved before that Adaboost achieved a maximum margin solution ever.",
                    "label": 0
                },
                {
                    "sent": "So I looked at this and I thought, Oh my goodness, this conjecture must be true.",
                    "label": 0
                },
                {
                    "sent": "Adaboost must always achieve a maximum margin solution.",
                    "label": 0
                },
                {
                    "sent": "And I was studying statistical learning theory at the time and I should have known better than to judge anything by one training example.",
                    "label": 0
                },
                {
                    "sent": "OK, so the conjecture the conjecture was true in at least one case, but you know, just wait.",
                    "label": 0
                },
                {
                    "sent": "And also I would like to mention one more thing, which is that the edge this mysterious quantity between zero and one that I mentioned before is actually actually the Golden ratio minus one in the cycle.",
                    "label": 0
                },
                {
                    "sent": "So whenever you're doing this kind of calculations, you see the Golden ratio absolutely everywhere.",
                    "label": 0
                },
                {
                    "sent": "OK, so then, after I analyzed that case, that simple case what I did was I looked at the case where you have negative ones on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So this is a case where we have Emily classifiers, each one misclassifying one training example and we find the exist.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can prove the existence of at least N -- 1 factorial stable cycles, each one yielding a maximum margin solution, and I would like to say though in this case we can't solve for the cycle coordinates exactly like we could for the other case because, well, we have an equation and we can't solve it, but we can prove that the equation has a unique solution for each cycle, so we can prove the solution exists.",
                    "label": 0
                },
                {
                    "sent": "We can prove it's unique, but we can't solve for it.",
                    "label": 0
                },
                {
                    "sent": "OK, and I also analyzed this other case where we have a number of training examples that are classified the same way by all the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "So these these training samples are classified the same way by the weak classifiers.",
                    "label": 1
                },
                {
                    "sent": "Same with these.",
                    "label": 0
                },
                {
                    "sent": "Same with those.",
                    "label": 0
                },
                {
                    "sent": "And same with these an in that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case where you have these identically classified training training examples, what happens is that, well, OK, what happens is that you have stable manifolds of cycles, so before I had sort of individual cycles I had, you know, iterate 3 * 1 two, three.",
                    "label": 0
                },
                {
                    "sent": "You end up back restarted.",
                    "label": 0
                },
                {
                    "sent": "Now what I have here are manifolds of cycles.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is that there's a sort of a direction I can move in the DT space where if I iterate 123I end up back where I started from.",
                    "label": 0
                },
                {
                    "sent": "So I have a whole continuum of these.",
                    "label": 0
                },
                {
                    "sent": "Stable cycles, so it's a manifold of stable cycles.",
                    "label": 0
                },
                {
                    "sent": "OK, so I also should mention that you know you're saying, hey Cynthia, you know?",
                    "label": 0
                },
                {
                    "sent": "Your examples are contrived.",
                    "label": 0
                },
                {
                    "sent": "Why do I care that you know you're looking at stuff worthy of negative ones in the diagonal of these very special cases?",
                    "label": 0
                },
                {
                    "sent": "But this cycling actually occurs pretty much almost always when when you choose a fairly small matrix, like when you have a small number of weak classifiers in a small number of training instances.",
                    "label": 1
                },
                {
                    "sent": "So every time I tried it, I actually got cycles.",
                    "label": 0
                },
                {
                    "sent": "So let me show you some examples.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is that this is a matrix.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Randomly generated matrix that was reduced a little bit where M is this is M sorry, red is plus one Blues minus one.",
                    "label": 0
                },
                {
                    "sent": "I sent this thing into Adaboost and then as usual the smaller circles or earlier time steps in the larger circles or later time steps.",
                    "label": 0
                },
                {
                    "sent": "Now the circles are all inside each other, which means I'm getting cycles so cycles and I'm just going to show you a few more examples where I plugged in random matrices into Adaboost and also got cycles, yeah?",
                    "label": 0
                },
                {
                    "sent": "That means that you you keep picking the same after three iterations.",
                    "label": 0
                },
                {
                    "sent": "You keep picking the same influence.",
                    "label": 0
                },
                {
                    "sent": "So what this means?",
                    "label": 0
                },
                {
                    "sent": "What this means is that so I started sort of here.",
                    "label": 0
                },
                {
                    "sent": "And then I went to here, here, here and here.",
                    "label": 0
                },
                {
                    "sent": "And then after a while I was going to hear to hear, to hear, to hear, to hear.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know what order it was.",
                    "label": 0
                },
                {
                    "sent": "It could have been all over the place, but I'm alternating between these.",
                    "label": 0
                },
                {
                    "sent": "You know these ones and then going back and doing it again.",
                    "label": 0
                },
                {
                    "sent": "And it does mean that I'm picking the same set of weak weak classifiers over and over again.",
                    "label": 1
                },
                {
                    "sent": "On that email.",
                    "label": 0
                },
                {
                    "sent": "Well, the thing about this is OK, so the question is, does it help tell you where to stop?",
                    "label": 0
                },
                {
                    "sent": "Well, The thing is that when Adaboost converges to a cycle here, there are some very nice things we can learn about the convergence properties, which which I haven't really told you yet, but in fact it tells you whenever you have cycling behavior.",
                    "label": 0
                },
                {
                    "sent": "It tells you exactly what Adaboost is converging to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it can tell you where to stop, but you know where to stop is kind of arbitrary because it's it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a difficult question that because 'cause I'm studying convergence properties of Adaboost, so figuring out where to stop is not exactly part of convergence, so that's kind of that's kind of a more empirical question, and I'm sort of studying throughout theoretical questions, kind of a different different style of question I think.",
                    "label": 0
                },
                {
                    "sent": "Where to stop really depends on the application and how your experiments are going, and you should do cross validation and all that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "And here I'm just really talking about convergence properties of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so in any case I'm.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you some more random matrices and hopefully you'll see some more cycles.",
                    "label": 0
                },
                {
                    "sent": "Ah, what happened, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so this one is really interesting because it took sort of 3000 iterations and it converge to the simplest of all possible cycles, the three cycle.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we plotted every 20th iteration here.",
                    "label": 0
                },
                {
                    "sent": "I mean this complete.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be full of circles.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's here's.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only the key.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If Adam is cycles, we can actually calculate the margin.",
                    "label": 0
                },
                {
                    "sent": "It's going to asymptotically converge to in terms of the cycle parameters.",
                    "label": 0
                },
                {
                    "sent": "So if I know what the cycle is, I can compute the margin.",
                    "label": 0
                },
                {
                    "sent": "So if I know what Adaboost is like, if I know the cycle that it's converging to, I can maybe find that out in the first few iterations if I know what cycle Adaboost is converging to, I can calculate the margin because I can plug it into this formula and it will tell me the margin that Adaboost is converging to so I can check and see whether it's a maximum margin solution, and so in fact, that really proves the theorem because.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because, well, because there's an example where it doesn't converge to a maximum margin solution.",
                    "label": 0
                },
                {
                    "sent": "There is an 8 by 8 matrix where Adam was probably converges to a non maximal margin solution.",
                    "label": 0
                },
                {
                    "sent": "Here's the matrix.",
                    "label": 0
                },
                {
                    "sent": "Why does plus one black is minus one?",
                    "label": 0
                },
                {
                    "sent": "And in fact, what happens here is that Adaboost converges to a manifold of strongly attracting stable three cycles.",
                    "label": 0
                }
            ]
        }
    }
}