{
    "id": "kquoxvgxgcwz7xt6afngrem2mpshnurg",
    "title": "Optimal Probability Estimation with Applications to Prediction and Classification",
    "info": {
        "author": [
            "Ananda Theertha Suresh, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_theertha_suresh_optimal/",
    "segmentation": [
        [
            "Hi everyone I'm on and I'll be talking about optimal probability estimation with application skill classification and prediction.",
            "This is joint work with JJ watch, Aria Ashkan, Jeff Operand along Outlook Ski."
        ],
        [
            "So this is our plan of the talk will first start with the problem of combined probability estimation, which we motivate fast, can give an optimal estimator for the same.",
            "Then we move on to the problem of classification of discrete distributions where surprising example, we show that one can do something back again, just empirical techniques.",
            "Again, we provide a uniformly diminishing additional error classifier for the same, and if time permits, we'll go to brief proof sketch for your problem."
        ],
        [
            "So let's start with your motivation.",
            "Ark."
        ],
        [
            "Very very simple example.",
            "You have a calling and you want to find X by us and you talk a bunch of times you get few hex few tiles and probably in this case if you ask what so probably affects, your guess probably would be 4 by 10.",
            "So this is well known empirical ischemic, which is just the number of occurrences of a particular symbol divided by the total number of samples.",
            "We all know it works because my laugh large numbers as the number of samples keeping keeps increasing.",
            "Empirical estimate will actually converge contractual probability.",
            "So the question is, what is convergence rate for this problem?",
            "So."
        ],
        [
            "So we measure conversion tracking comes of sample complexity.",
            "So in general you have a distribution P over with probabilities P1 to PK.",
            "And your observer in IID samples from it.",
            "Let's call it X.",
            "And you want to ischemic be using empirical frequencies.",
            "We can use L1 discounts as a measure of closeness between two distributions, but in general always whatever I'm going to say is going to work for all other discounts measures also.",
            "So one natural question is ask how many samples unique for this L1 distance to be small with high probability.",
            "Amtrak surprising can be easily shown that order K samples are sufficient.",
            "Slightly more with some more work you can show that other care of samples are necessary for any estimator and any discounts measure you choose.",
            "One easy way to see this is if you have a distribution with really long tail out heavy tail gang, unique to see more symbols at least once to complete.",
            "Scan pointer so we're contrasting opposites.",
            "Hedging wearing is really small compared to K. In other words, if the number of samples is very small compared to alphabet size."
        ],
        [
            "So this way we use gun notion of natural estimators.",
            "Let's again.",
            "An example, so you have a distribution, say from a graph and you observe filing sequence ABC.",
            "And if we ask, you compute washer properties of PA&PB.",
            "Not just with both of them have appeared exactly twice, so if there is no prior information, you can assign it the same probability.",
            "In other words, for hanging natural estimator, QQ equals QB.",
            "And similarly, all Lansing symbols will get the same probability."
        ],
        [
            "So I guess we can have your following definition.",
            "Let's define immune to bigger sum of probabilities of symbols copium you camps and female bigger actual number of symbols that appear in your camps.",
            "I can get an example, will make it very clear.",
            "So again you have a distribution from a Cody and your sequence is a DCD.",
            "No, just like symbol be hasn't appeared at all.",
            "So Mo is PB.",
            "Symbols, ANSI, have appeared once, so M1 is PA plus PC.",
            "And M2 is pretty because G has appeared twice.",
            "Straightforward, similarly feasible is 1 because one simple hasn't appeared.",
            "Two symbols have appeared once and one symbol has appeared twice.",
            "So we have definitions, so if you know M and if you know free, you can assign probability N one by two.",
            "In this case Q equals Q, C = M, one by two, or in general it could be M1 by three one.",
            "So if you have a symbol like appeared mute camps, the public you are sending to it would be MU by female and things in general you don't know how many symbols you're going you having seen, so you can just come young sing mass which is Mo.",
            "So guess we call combined probability estimation, which is to find this combined probability mass Mo M1 up there man.",
            "And at some motivation we look at this problem."
        ],
        [
            "And in fact, it has been scheduled before."
        ],
        [
            "So the good during classical butchering Escuchar falls into the same category where you want to find what's your problem.",
            "Fun seeing mass and so on and has some applications and it has been scheduled practically by McAllister.",
            "Ensure Perian, who can monster before.",
            "Why's it interesting so far you collect that I said if you want to find the actual properties unique order K samples, But once you jump from the actual properties, could just finding combined probability mass your number of samples you need is actually independent of K which is alphabet size.",
            "And what do you mean by ischemic?",
            "Well, we can look at L1 distance or care divergent.",
            "So L1 gas cans has been motion of indistinguishability and hence it can be used for classification problems.",
            "And if you have caregiver, Jinx has applications in universal compression information here.",
            "So."
        ],
        [
            "Offline so so far we have only seen empirical schematica, so let's see how it works in this new notion of combined probability mass.",
            "Again, an example.",
            "You have a sequence of length 9.",
            "Ann.",
            "You observe both symbols A&B twice, so if you want to compute M2, let's call the empirical estimate E2E scans for empirical notice guidebook symbols.",
            "A&B will get empirical estimate 2 by 9, so overall estimate for Rico is cookcamp screw by 9 which is 4 by 9.",
            "So in general, if female symbols have appeared New Times, guess connect you gave is female camps nearby and.",
            "So we can ask the same question again how many samples you need for this new L1 distance between me could be small with high probability.",
            "So observing easier is always 0 because you're multiplying feasible with zero by North, so it's always 0.",
            "So if a distribution is again almost uniform, unless your number of samples is much larger than KMO would be some positive number.",
            "So you need again order K samples."
        ],
        [
            "So Google, including in particular, they wanted to estimate the probability of unseen mass which is Mo.",
            "So they came up with this estimator in which to find the probability of things which appeared mu times you look at number of things which appeared one higher gang back which is female plus one.",
            "It has several nice properties Fastclick sung by ask you know what's expected.",
            "Value of GMU is same as expected value of MU for every mu.",
            "And Furthermore, even use a non zero public announcing mass which is fee one.",
            "By then we call the empirical always give zero probability.",
            "So give for several variations of it, especially the smoothing version suffixes using natural language processing and also in population estimation in biology.",
            "So what kind of performance we're increasing going for this problem?",
            "True, yeah.",
            "Yeah, that's a very valid point, so depends on how you define it.",
            "So for this talk I'm assuming stuff getting any samples you get poison and samples.",
            "So number of samples which you get with traffic being exactly, and you assume it to be poison random variable with mean and standard technique.",
            "And as you said your bias would be 1 / N I think otherwise.",
            "So what kind of performance guarantees I'm going for this problem?"
        ],
        [
            "So gorgeous cock.",
            "I'm going to use with high probability whenever I mean something probably 1 -- 1 / 10 or so, and I'm going to use OK. Got to neglect all log factors.",
            "But I should mention the previous workers have found the exact constants and so on.",
            "So McAllister interfering shortcut am human is GMU is bounded by mu by square root can.",
            "Which is nice because if mu is small gang, just bond is really small.",
            "But does it actually satisfy what we wanted before?",
            "In other words, gallon gas cans go cozero.",
            "Gangster is because if you is bigger than square root, N double bond blows up.",
            "So let's go fix so we call it.",
            "I call you empirical estimates work if the number of samples is a lot.",
            "So in other words, if mu is locked in particular schema, perform well, so looking so combined.",
            "I'm desk again results in terms of adding finishing on I'm scanning.",
            "It comes of L1 norm and caregiver agents.",
            "Get showed that the L1 just gave results can be suggested.",
            "Actually 16 The nice thing is independent of K. Distributions you have.",
            "And we are interested in knowing whether this is best.",
            "One can hope for."
        ],
        [
            "So first we try to combine the same scheme occurs in different ways and also tracking program analysis.",
            "But later we realized that no matter how you combine these two gal always be a distribution.",
            "Such a gallon gas cans is one over in Q16.",
            "You know it's a previous bond, so very cake.",
            "So we come up against Jamaica such that it imposed exponent from 161 fourth.",
            "And also we show that it's optimal in the sense that there is a distribution such that whatever schema can you come up with has an L1 distance of one hour in 31 folk.",
            "Angle same set of results.",
            "Hold for care versions with different exponents.",
            "Collected by Pinsker inequality in One Direction and other direction needs some work.",
            "So."
        ],
        [
            "Before I actually go onto the rough idea of the proof, I'm going to give a brief outline of classifica."
        ],
        [
            "So we look at classifications of discrete distributions.",
            "Sapien Q An you get inclining samples from both of them.",
            "Say offline thanks X&Y and you get symbol which is I got from PR key where is tangled 2nd.",
            "So let's take an example.",
            "Can be training can be a BC BA an your task is, say, A.",
            "Probably you're going to see Exhibit X because it appeared higher number of times in the first sequence.",
            "So we define labeling variants classifieds as follows.",
            "So if you re label symbols in a one to one fashion, as you say A2 UB to B&CQW golf book of your classifier should not change.",
            "In other words, if you say in the first case.",
            "Z&X are according to the same distribution.",
            "You should say the same thing for the second one.",
            "So this week all labeling variant.",
            "It's also nice Canonical classifiers, computer science literature.",
            "So another question is works for optimal classifier.",
            "And similar to pack learning, we want the optimal classifier with respect to the best labeling wearing classifier which may actually no longer lying discriminations."
        ],
        [
            "So let's look at the previous example.",
            "Again.",
            "As I said before, 1 probably says Z&X are from the same distribution because the task has appeared more number of times in the first sequence.",
            "So it gives us the empirical classifier where you want us any ticker, symbol, test coming string sequence where it has occurred higher number of times.",
            "Huawei ideally would want to send you to training sequence which has higher probability but you don't know that.",
            "So you are using the higher multiplicity as a proxy.",
            "The first question we had was can someone go anything apart from this so fast with sugar is not optimal.",
            "In fact it incurs a constant conditional error.",
            "So for example, if you have, you take a simple example.",
            "You have two distributions, U1 playing U1, correct and you get any samples exactly notice yet.",
            "Apple has appeared both after training sequences gang piacente probability one normal thank you, a sense.",
            "It probably won't work then and now the optimal ones causing it to the first distribution.",
            "P because you guys are things like higher probability.",
            "But with some constant publicly, you can show that this multiplicity switch never occurs more number of times in weigang index.",
            "Therefore you make an additional error of .01.",
            "In other words, if you just want to do the empirical classifier get exist compare of distribution such that gather is .01 more gangga.",
            "Optimal classifier."
        ],
        [
            "So what we do is we first collect the classification to your problem of estimation, or pairs of sequences, and then we give up good touring type of estimators for pairs of sequences.",
            "So previously whatever we had good during Oreskovich purpose was for a single sequence.",
            "Now we related to pairs of sequences.",
            "Angry proposal classifier.",
            "Such vector error is at most of God of war, not working today 150 compared to the optimal one.",
            "Even this is independent of K. Also, we show a lower bounds.",
            "Classifier there will be some pair of distribution such that the error is at least one over into 1/3 higher than optimal.",
            "And also I should mention that proposed estimator and this classifier runs in linear time."
        ],
        [
            "So I'll give you a brief proof sketch."
        ],
        [
            "Arm.",
            "Recall that empirical estimator was just female times new buying where females can number of symbols that occurred New Times.",
            "So we just post marked by some kind of correction term to make it better.",
            "In general this correction term could be some function of the absurd sequence.",
            "And for this talk, I'm going to assume that if you have bones on bias and variance gang in place, a good classifier estimator, and also I'm going to ignore Constans and not use over notation for the rest of the talk.",
            "So further empirical classifier connection comes one and you can show it has a certain bias and variance actual numbers going back on that match.",
            "So for good curing the correction term is new plus one by mu times female plus one by females, so the females cancel and you just get Freeview plus 1 * 1 + 1 by.",
            "It's good in the sense that you have zero by us, but it has a large variance in about 6 variances.",
            "Mu, higher gangl empirical one.",
            "So what we're proposing things have multiplying by female plus one by Phi mu, you multiply by the ratio of female plus one by expected values of Premier plus one by expected value from you.",
            "This is a nice proper kick.",
            "Knock only has zero bias, but even the variance is same as empirical.",
            "Now about six really good.",
            "But there's a catch.",
            "We don't know this expected values.",
            "We don't know what ratio of expected value of female plus one an expected value of female so good touring estimator approximated by just female plus one by female.",
            "And hangs in Gaza, large bears."
        ],
        [
            "Large variance so.",
            "So what we do is, so let's look at this problem carefully.",
            "So you just want to find this ratio exactly.",
            "So instead of having a ratio, it, let's just look at one time.",
            "How do we estimate the expected value of female?",
            "I just clear expected value of female.",
            "I mean how many symbols you expect to see mu times.",
            "Group touring just makes expected value female female and it had a variance.",
            "So what kind of vector Husky mix we can think of?",
            "Let's just look at linear estimators for now because it's more natural and easy to analyze of some linear ask Micah, and I'm just going to try to convince you why we actually work instead of giving actual estimator."
        ],
        [
            "So in a rescue maker for expected value of female was just female.",
            "As I said before, it has zero by us, just some standard deviation, so error is roughly order of the standard deviation.",
            "Let's make two assumptions for now.",
            "Let's say if expected value of famous female and female plus one are very close.",
            "And also, let's assume that female minus one female plus one and female are independent.",
            "Second one is not true, but let's assume for now.",
            "So your new ischemic I would just bigotry, and obviously by us, which is less than two epsilon by three because epsilon was biased for each one and things variance of sum is sum of variances, your standard deviation decreases and your error would be one over square root, 3 times previous error plus some small by us.",
            "So if epsilon is 1 exciting, comment."
        ],
        [
            "So yeah, some technical details which I could not work, which makes them act a bit messy.",
            "So firstly female minus one female female plus one are not independent and we still have to show that expected value of Phi MU and expected value female plus one or close.",
            "And we also have to show that these are well behaved distributions.",
            "Innocence track bias and variance are sufficient for concentration.",
            "Gas pipe, all of this skill taking average of three of them won't work.",
            "In fact, taking average of any number of them won't work.",
            "So here's where we use some tools from linear filters and some properties of poison functions and some tools from approximation theory to come up with an explicit desk maker and bulk after work is to come up with this actual coefficients, and so on.",
            "So."
        ],
        [
            "So to summarize, we looked at the problem of estimating this combined quality, mass M and previous best known result was one six.",
            "We improved it to include one 4th.",
            "We also showed its optimal and it has linear time complexity and the park, which I did not talk about.",
            "It has some implications in universal compression in information theory, and there you can show your personal redundancy will be one over square root can.",
            "I'm finally we looked at classification of discrete distributions where we show came up against America with classifier with conditional error one over in 5th 5th and also showed that any classifier.",
            "They would be pairs of distribution such that there will be will be."
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone I'm on and I'll be talking about optimal probability estimation with application skill classification and prediction.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with JJ watch, Aria Ashkan, Jeff Operand along Outlook Ski.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our plan of the talk will first start with the problem of combined probability estimation, which we motivate fast, can give an optimal estimator for the same.",
                    "label": 0
                },
                {
                    "sent": "Then we move on to the problem of classification of discrete distributions where surprising example, we show that one can do something back again, just empirical techniques.",
                    "label": 0
                },
                {
                    "sent": "Again, we provide a uniformly diminishing additional error classifier for the same, and if time permits, we'll go to brief proof sketch for your problem.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with your motivation.",
                    "label": 0
                },
                {
                    "sent": "Ark.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very very simple example.",
                    "label": 0
                },
                {
                    "sent": "You have a calling and you want to find X by us and you talk a bunch of times you get few hex few tiles and probably in this case if you ask what so probably affects, your guess probably would be 4 by 10.",
                    "label": 0
                },
                {
                    "sent": "So this is well known empirical ischemic, which is just the number of occurrences of a particular symbol divided by the total number of samples.",
                    "label": 0
                },
                {
                    "sent": "We all know it works because my laugh large numbers as the number of samples keeping keeps increasing.",
                    "label": 0
                },
                {
                    "sent": "Empirical estimate will actually converge contractual probability.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what is convergence rate for this problem?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we measure conversion tracking comes of sample complexity.",
                    "label": 1
                },
                {
                    "sent": "So in general you have a distribution P over with probabilities P1 to PK.",
                    "label": 0
                },
                {
                    "sent": "And your observer in IID samples from it.",
                    "label": 0
                },
                {
                    "sent": "Let's call it X.",
                    "label": 1
                },
                {
                    "sent": "And you want to ischemic be using empirical frequencies.",
                    "label": 0
                },
                {
                    "sent": "We can use L1 discounts as a measure of closeness between two distributions, but in general always whatever I'm going to say is going to work for all other discounts measures also.",
                    "label": 0
                },
                {
                    "sent": "So one natural question is ask how many samples unique for this L1 distance to be small with high probability.",
                    "label": 0
                },
                {
                    "sent": "Amtrak surprising can be easily shown that order K samples are sufficient.",
                    "label": 1
                },
                {
                    "sent": "Slightly more with some more work you can show that other care of samples are necessary for any estimator and any discounts measure you choose.",
                    "label": 1
                },
                {
                    "sent": "One easy way to see this is if you have a distribution with really long tail out heavy tail gang, unique to see more symbols at least once to complete.",
                    "label": 0
                },
                {
                    "sent": "Scan pointer so we're contrasting opposites.",
                    "label": 0
                },
                {
                    "sent": "Hedging wearing is really small compared to K. In other words, if the number of samples is very small compared to alphabet size.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this way we use gun notion of natural estimators.",
                    "label": 1
                },
                {
                    "sent": "Let's again.",
                    "label": 0
                },
                {
                    "sent": "An example, so you have a distribution, say from a graph and you observe filing sequence ABC.",
                    "label": 0
                },
                {
                    "sent": "And if we ask, you compute washer properties of PA&PB.",
                    "label": 1
                },
                {
                    "sent": "Not just with both of them have appeared exactly twice, so if there is no prior information, you can assign it the same probability.",
                    "label": 1
                },
                {
                    "sent": "In other words, for hanging natural estimator, QQ equals QB.",
                    "label": 0
                },
                {
                    "sent": "And similarly, all Lansing symbols will get the same probability.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess we can have your following definition.",
                    "label": 0
                },
                {
                    "sent": "Let's define immune to bigger sum of probabilities of symbols copium you camps and female bigger actual number of symbols that appear in your camps.",
                    "label": 1
                },
                {
                    "sent": "I can get an example, will make it very clear.",
                    "label": 0
                },
                {
                    "sent": "So again you have a distribution from a Cody and your sequence is a DCD.",
                    "label": 0
                },
                {
                    "sent": "No, just like symbol be hasn't appeared at all.",
                    "label": 0
                },
                {
                    "sent": "So Mo is PB.",
                    "label": 0
                },
                {
                    "sent": "Symbols, ANSI, have appeared once, so M1 is PA plus PC.",
                    "label": 0
                },
                {
                    "sent": "And M2 is pretty because G has appeared twice.",
                    "label": 0
                },
                {
                    "sent": "Straightforward, similarly feasible is 1 because one simple hasn't appeared.",
                    "label": 0
                },
                {
                    "sent": "Two symbols have appeared once and one symbol has appeared twice.",
                    "label": 0
                },
                {
                    "sent": "So we have definitions, so if you know M and if you know free, you can assign probability N one by two.",
                    "label": 0
                },
                {
                    "sent": "In this case Q equals Q, C = M, one by two, or in general it could be M1 by three one.",
                    "label": 0
                },
                {
                    "sent": "So if you have a symbol like appeared mute camps, the public you are sending to it would be MU by female and things in general you don't know how many symbols you're going you having seen, so you can just come young sing mass which is Mo.",
                    "label": 0
                },
                {
                    "sent": "So guess we call combined probability estimation, which is to find this combined probability mass Mo M1 up there man.",
                    "label": 0
                },
                {
                    "sent": "And at some motivation we look at this problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, it has been scheduled before.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the good during classical butchering Escuchar falls into the same category where you want to find what's your problem.",
                    "label": 0
                },
                {
                    "sent": "Fun seeing mass and so on and has some applications and it has been scheduled practically by McAllister.",
                    "label": 0
                },
                {
                    "sent": "Ensure Perian, who can monster before.",
                    "label": 0
                },
                {
                    "sent": "Why's it interesting so far you collect that I said if you want to find the actual properties unique order K samples, But once you jump from the actual properties, could just finding combined probability mass your number of samples you need is actually independent of K which is alphabet size.",
                    "label": 0
                },
                {
                    "sent": "And what do you mean by ischemic?",
                    "label": 0
                },
                {
                    "sent": "Well, we can look at L1 distance or care divergent.",
                    "label": 0
                },
                {
                    "sent": "So L1 gas cans has been motion of indistinguishability and hence it can be used for classification problems.",
                    "label": 0
                },
                {
                    "sent": "And if you have caregiver, Jinx has applications in universal compression information here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offline so so far we have only seen empirical schematica, so let's see how it works in this new notion of combined probability mass.",
                    "label": 0
                },
                {
                    "sent": "Again, an example.",
                    "label": 0
                },
                {
                    "sent": "You have a sequence of length 9.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "You observe both symbols A&B twice, so if you want to compute M2, let's call the empirical estimate E2E scans for empirical notice guidebook symbols.",
                    "label": 0
                },
                {
                    "sent": "A&B will get empirical estimate 2 by 9, so overall estimate for Rico is cookcamp screw by 9 which is 4 by 9.",
                    "label": 0
                },
                {
                    "sent": "So in general, if female symbols have appeared New Times, guess connect you gave is female camps nearby and.",
                    "label": 0
                },
                {
                    "sent": "So we can ask the same question again how many samples you need for this new L1 distance between me could be small with high probability.",
                    "label": 0
                },
                {
                    "sent": "So observing easier is always 0 because you're multiplying feasible with zero by North, so it's always 0.",
                    "label": 0
                },
                {
                    "sent": "So if a distribution is again almost uniform, unless your number of samples is much larger than KMO would be some positive number.",
                    "label": 0
                },
                {
                    "sent": "So you need again order K samples.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Google, including in particular, they wanted to estimate the probability of unseen mass which is Mo.",
                    "label": 1
                },
                {
                    "sent": "So they came up with this estimator in which to find the probability of things which appeared mu times you look at number of things which appeared one higher gang back which is female plus one.",
                    "label": 0
                },
                {
                    "sent": "It has several nice properties Fastclick sung by ask you know what's expected.",
                    "label": 0
                },
                {
                    "sent": "Value of GMU is same as expected value of MU for every mu.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, even use a non zero public announcing mass which is fee one.",
                    "label": 0
                },
                {
                    "sent": "By then we call the empirical always give zero probability.",
                    "label": 0
                },
                {
                    "sent": "So give for several variations of it, especially the smoothing version suffixes using natural language processing and also in population estimation in biology.",
                    "label": 0
                },
                {
                    "sent": "So what kind of performance we're increasing going for this problem?",
                    "label": 0
                },
                {
                    "sent": "True, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a very valid point, so depends on how you define it.",
                    "label": 0
                },
                {
                    "sent": "So for this talk I'm assuming stuff getting any samples you get poison and samples.",
                    "label": 0
                },
                {
                    "sent": "So number of samples which you get with traffic being exactly, and you assume it to be poison random variable with mean and standard technique.",
                    "label": 1
                },
                {
                    "sent": "And as you said your bias would be 1 / N I think otherwise.",
                    "label": 0
                },
                {
                    "sent": "So what kind of performance guarantees I'm going for this problem?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So gorgeous cock.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use with high probability whenever I mean something probably 1 -- 1 / 10 or so, and I'm going to use OK. Got to neglect all log factors.",
                    "label": 0
                },
                {
                    "sent": "But I should mention the previous workers have found the exact constants and so on.",
                    "label": 0
                },
                {
                    "sent": "So McAllister interfering shortcut am human is GMU is bounded by mu by square root can.",
                    "label": 0
                },
                {
                    "sent": "Which is nice because if mu is small gang, just bond is really small.",
                    "label": 0
                },
                {
                    "sent": "But does it actually satisfy what we wanted before?",
                    "label": 0
                },
                {
                    "sent": "In other words, gallon gas cans go cozero.",
                    "label": 0
                },
                {
                    "sent": "Gangster is because if you is bigger than square root, N double bond blows up.",
                    "label": 0
                },
                {
                    "sent": "So let's go fix so we call it.",
                    "label": 0
                },
                {
                    "sent": "I call you empirical estimates work if the number of samples is a lot.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if mu is locked in particular schema, perform well, so looking so combined.",
                    "label": 0
                },
                {
                    "sent": "I'm desk again results in terms of adding finishing on I'm scanning.",
                    "label": 0
                },
                {
                    "sent": "It comes of L1 norm and caregiver agents.",
                    "label": 0
                },
                {
                    "sent": "Get showed that the L1 just gave results can be suggested.",
                    "label": 0
                },
                {
                    "sent": "Actually 16 The nice thing is independent of K. Distributions you have.",
                    "label": 0
                },
                {
                    "sent": "And we are interested in knowing whether this is best.",
                    "label": 0
                },
                {
                    "sent": "One can hope for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we try to combine the same scheme occurs in different ways and also tracking program analysis.",
                    "label": 0
                },
                {
                    "sent": "But later we realized that no matter how you combine these two gal always be a distribution.",
                    "label": 0
                },
                {
                    "sent": "Such a gallon gas cans is one over in Q16.",
                    "label": 0
                },
                {
                    "sent": "You know it's a previous bond, so very cake.",
                    "label": 0
                },
                {
                    "sent": "So we come up against Jamaica such that it imposed exponent from 161 fourth.",
                    "label": 0
                },
                {
                    "sent": "And also we show that it's optimal in the sense that there is a distribution such that whatever schema can you come up with has an L1 distance of one hour in 31 folk.",
                    "label": 0
                },
                {
                    "sent": "Angle same set of results.",
                    "label": 0
                },
                {
                    "sent": "Hold for care versions with different exponents.",
                    "label": 0
                },
                {
                    "sent": "Collected by Pinsker inequality in One Direction and other direction needs some work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I actually go onto the rough idea of the proof, I'm going to give a brief outline of classifica.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we look at classifications of discrete distributions.",
                    "label": 1
                },
                {
                    "sent": "Sapien Q An you get inclining samples from both of them.",
                    "label": 0
                },
                {
                    "sent": "Say offline thanks X&Y and you get symbol which is I got from PR key where is tangled 2nd.",
                    "label": 0
                },
                {
                    "sent": "So let's take an example.",
                    "label": 0
                },
                {
                    "sent": "Can be training can be a BC BA an your task is, say, A.",
                    "label": 0
                },
                {
                    "sent": "Probably you're going to see Exhibit X because it appeared higher number of times in the first sequence.",
                    "label": 0
                },
                {
                    "sent": "So we define labeling variants classifieds as follows.",
                    "label": 0
                },
                {
                    "sent": "So if you re label symbols in a one to one fashion, as you say A2 UB to B&CQW golf book of your classifier should not change.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you say in the first case.",
                    "label": 0
                },
                {
                    "sent": "Z&X are according to the same distribution.",
                    "label": 0
                },
                {
                    "sent": "You should say the same thing for the second one.",
                    "label": 0
                },
                {
                    "sent": "So this week all labeling variant.",
                    "label": 1
                },
                {
                    "sent": "It's also nice Canonical classifiers, computer science literature.",
                    "label": 0
                },
                {
                    "sent": "So another question is works for optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "And similar to pack learning, we want the optimal classifier with respect to the best labeling wearing classifier which may actually no longer lying discriminations.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the previous example.",
                    "label": 1
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "As I said before, 1 probably says Z&X are from the same distribution because the task has appeared more number of times in the first sequence.",
                    "label": 0
                },
                {
                    "sent": "So it gives us the empirical classifier where you want us any ticker, symbol, test coming string sequence where it has occurred higher number of times.",
                    "label": 0
                },
                {
                    "sent": "Huawei ideally would want to send you to training sequence which has higher probability but you don't know that.",
                    "label": 0
                },
                {
                    "sent": "So you are using the higher multiplicity as a proxy.",
                    "label": 0
                },
                {
                    "sent": "The first question we had was can someone go anything apart from this so fast with sugar is not optimal.",
                    "label": 0
                },
                {
                    "sent": "In fact it incurs a constant conditional error.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have, you take a simple example.",
                    "label": 0
                },
                {
                    "sent": "You have two distributions, U1 playing U1, correct and you get any samples exactly notice yet.",
                    "label": 0
                },
                {
                    "sent": "Apple has appeared both after training sequences gang piacente probability one normal thank you, a sense.",
                    "label": 1
                },
                {
                    "sent": "It probably won't work then and now the optimal ones causing it to the first distribution.",
                    "label": 1
                },
                {
                    "sent": "P because you guys are things like higher probability.",
                    "label": 0
                },
                {
                    "sent": "But with some constant publicly, you can show that this multiplicity switch never occurs more number of times in weigang index.",
                    "label": 1
                },
                {
                    "sent": "Therefore you make an additional error of .01.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you just want to do the empirical classifier get exist compare of distribution such that gather is .01 more gangga.",
                    "label": 1
                },
                {
                    "sent": "Optimal classifier.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is we first collect the classification to your problem of estimation, or pairs of sequences, and then we give up good touring type of estimators for pairs of sequences.",
                    "label": 0
                },
                {
                    "sent": "So previously whatever we had good during Oreskovich purpose was for a single sequence.",
                    "label": 0
                },
                {
                    "sent": "Now we related to pairs of sequences.",
                    "label": 0
                },
                {
                    "sent": "Angry proposal classifier.",
                    "label": 0
                },
                {
                    "sent": "Such vector error is at most of God of war, not working today 150 compared to the optimal one.",
                    "label": 0
                },
                {
                    "sent": "Even this is independent of K. Also, we show a lower bounds.",
                    "label": 1
                },
                {
                    "sent": "Classifier there will be some pair of distribution such that the error is at least one over into 1/3 higher than optimal.",
                    "label": 1
                },
                {
                    "sent": "And also I should mention that proposed estimator and this classifier runs in linear time.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll give you a brief proof sketch.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arm.",
                    "label": 0
                },
                {
                    "sent": "Recall that empirical estimator was just female times new buying where females can number of symbols that occurred New Times.",
                    "label": 1
                },
                {
                    "sent": "So we just post marked by some kind of correction term to make it better.",
                    "label": 0
                },
                {
                    "sent": "In general this correction term could be some function of the absurd sequence.",
                    "label": 1
                },
                {
                    "sent": "And for this talk, I'm going to assume that if you have bones on bias and variance gang in place, a good classifier estimator, and also I'm going to ignore Constans and not use over notation for the rest of the talk.",
                    "label": 1
                },
                {
                    "sent": "So further empirical classifier connection comes one and you can show it has a certain bias and variance actual numbers going back on that match.",
                    "label": 0
                },
                {
                    "sent": "So for good curing the correction term is new plus one by mu times female plus one by females, so the females cancel and you just get Freeview plus 1 * 1 + 1 by.",
                    "label": 0
                },
                {
                    "sent": "It's good in the sense that you have zero by us, but it has a large variance in about 6 variances.",
                    "label": 0
                },
                {
                    "sent": "Mu, higher gangl empirical one.",
                    "label": 0
                },
                {
                    "sent": "So what we're proposing things have multiplying by female plus one by Phi mu, you multiply by the ratio of female plus one by expected values of Premier plus one by expected value from you.",
                    "label": 0
                },
                {
                    "sent": "This is a nice proper kick.",
                    "label": 0
                },
                {
                    "sent": "Knock only has zero bias, but even the variance is same as empirical.",
                    "label": 0
                },
                {
                    "sent": "Now about six really good.",
                    "label": 0
                },
                {
                    "sent": "But there's a catch.",
                    "label": 0
                },
                {
                    "sent": "We don't know this expected values.",
                    "label": 0
                },
                {
                    "sent": "We don't know what ratio of expected value of female plus one an expected value of female so good touring estimator approximated by just female plus one by female.",
                    "label": 0
                },
                {
                    "sent": "And hangs in Gaza, large bears.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large variance so.",
                    "label": 0
                },
                {
                    "sent": "So what we do is, so let's look at this problem carefully.",
                    "label": 0
                },
                {
                    "sent": "So you just want to find this ratio exactly.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a ratio, it, let's just look at one time.",
                    "label": 0
                },
                {
                    "sent": "How do we estimate the expected value of female?",
                    "label": 0
                },
                {
                    "sent": "I just clear expected value of female.",
                    "label": 0
                },
                {
                    "sent": "I mean how many symbols you expect to see mu times.",
                    "label": 0
                },
                {
                    "sent": "Group touring just makes expected value female female and it had a variance.",
                    "label": 0
                },
                {
                    "sent": "So what kind of vector Husky mix we can think of?",
                    "label": 0
                },
                {
                    "sent": "Let's just look at linear estimators for now because it's more natural and easy to analyze of some linear ask Micah, and I'm just going to try to convince you why we actually work instead of giving actual estimator.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a rescue maker for expected value of female was just female.",
                    "label": 0
                },
                {
                    "sent": "As I said before, it has zero by us, just some standard deviation, so error is roughly order of the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "Let's make two assumptions for now.",
                    "label": 0
                },
                {
                    "sent": "Let's say if expected value of famous female and female plus one are very close.",
                    "label": 0
                },
                {
                    "sent": "And also, let's assume that female minus one female plus one and female are independent.",
                    "label": 1
                },
                {
                    "sent": "Second one is not true, but let's assume for now.",
                    "label": 0
                },
                {
                    "sent": "So your new ischemic I would just bigotry, and obviously by us, which is less than two epsilon by three because epsilon was biased for each one and things variance of sum is sum of variances, your standard deviation decreases and your error would be one over square root, 3 times previous error plus some small by us.",
                    "label": 1
                },
                {
                    "sent": "So if epsilon is 1 exciting, comment.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, some technical details which I could not work, which makes them act a bit messy.",
                    "label": 0
                },
                {
                    "sent": "So firstly female minus one female female plus one are not independent and we still have to show that expected value of Phi MU and expected value female plus one or close.",
                    "label": 1
                },
                {
                    "sent": "And we also have to show that these are well behaved distributions.",
                    "label": 0
                },
                {
                    "sent": "Innocence track bias and variance are sufficient for concentration.",
                    "label": 1
                },
                {
                    "sent": "Gas pipe, all of this skill taking average of three of them won't work.",
                    "label": 0
                },
                {
                    "sent": "In fact, taking average of any number of them won't work.",
                    "label": 1
                },
                {
                    "sent": "So here's where we use some tools from linear filters and some properties of poison functions and some tools from approximation theory to come up with an explicit desk maker and bulk after work is to come up with this actual coefficients, and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we looked at the problem of estimating this combined quality, mass M and previous best known result was one six.",
                    "label": 0
                },
                {
                    "sent": "We improved it to include one 4th.",
                    "label": 0
                },
                {
                    "sent": "We also showed its optimal and it has linear time complexity and the park, which I did not talk about.",
                    "label": 1
                },
                {
                    "sent": "It has some implications in universal compression in information theory, and there you can show your personal redundancy will be one over square root can.",
                    "label": 0
                },
                {
                    "sent": "I'm finally we looked at classification of discrete distributions where we show came up against America with classifier with conditional error one over in 5th 5th and also showed that any classifier.",
                    "label": 0
                },
                {
                    "sent": "They would be pairs of distribution such that there will be will be.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}