{
    "id": "cwbtmlulz4s7bgyvx3paore42us5kt6e",
    "title": "Bias/variance analysis of relational domains",
    "info": {
        "author": [
            "Jennifer Neville, Purdue University"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_neville_bvar/",
    "segmentation": [
        [
            "OK, thanks so you can hear me out back if I talk like this OK?",
            "OK, so I'm going to talk about framework that David and I developed to analyze the sort of error characteristics of relational learning techniques that use collective inference.",
            "Can you still hear me because all of a sudden I can't?",
            "Not very well.",
            "OK, maybe I'll just switch to this other.",
            "Microphone.",
            "OK, so if I use this is that better, yeah?",
            "OK, so let me start OFM."
        ],
        [
            "By showing you some motivation as to why we developed these, this framework, here are some results that we have that we had a couple of years ago when we were developing these latent group models and what we're doing is we're evaluating these two relational models.",
            "We're evaluating area into the RC curve here and what we're doing on the X axis is we're varying the proportion of labeled instances in the test set.",
            "So we're using a collective inference process that I'll explain in more detail later.",
            "But we're trying to see how the collective inference process works.",
            "As we know more and more of the true class labels of the instances in the test set, and we got these results.",
            "And what I was concerned about was this really poor performance that relational dependency networks had here when there were fewer than 50% labeled instances in the test set.",
            "And these are these are the web data, so this is real data and.",
            "My hypothesis as to why the Guardians were performing so poorly in this situation was really that the collective inference process that it was using was increasing the variance of the predictions when there were sparse seed information in the data set, and then the question became how are we going to actually evaluate this hypothesis?",
            "How we're going to figure out if this is true about these algorithms and the key thing here to realize is that our hypothesis is involved with.",
            "Measuring the amount of variance of the model so it was natural to think about using bias variance analysis to decompose the errors of the model and see if were in.",
            "If there was more variance associated with Guardians compared to the latent group models.",
            "So before I get into why we can't do this with conventional bias variance analysis, let me just tell it, remind you of what the traditional framework is and I'll show you procedure."
        ],
        [
            "See how you go about measuring bias and variance in standard IID data.",
            "So here we have a training set which is a table set of IID data.",
            "And what we do is sample subsamples from that table, learn models on each of those sub samples and then apply all of those models to a single test set instance and then after we apply all those models will have a distribution of model predictions for that instance and so we can see how the predictions for that instance very.",
            "As a function of the training data that we're going to use that we use to learn the models.",
            "And."
        ],
        [
            "So once we have that distribution, we can compare it to the optimal prediction that you would make for that for that test instance.",
            "So the Bayes optimal prediction is typically what we use, and if you look at the distance between the mean of the distribution, the mean of the distribution of the predictions that you had for that instance, and the optimal prediction that gives you the bias of the model, and if you look at the spread of the distribution, then that gives you the variance.",
            "OK, so here is the actual math decomposition.",
            "Here we have the.",
            "Squared loss for an instance are predicted value of Y and the true value of T. And here the expectation is over the datasets that you use for learning the data.",
            "Learning the model and so this decomposes into three factors.",
            "Here we have the noise here, which is the natural variability of the true distribution you're trying to model.",
            "We have the bias here, which is the 6th systematic difference between your learned function and the true function that you're trying to model.",
            "And then we have the variance here, which is the variability of your learn function.",
            "Given the training data used to learn it, and So what you might notice as I was describing this is that we're just talking about the learning process alone here.",
            "So how does the learn function vary as a function of the data that you used to learn it?",
            "What this assumes is that there's no additional variation or error due to the inference process process that you're using.",
            "So when you apply the model to do inference for single instance, every time you apply that model, you're going to get the same answer, the same prediction.",
            "So now let me show you.",
            "Why that doesn't often hold?",
            "That assumption is often violated in relational learning cases."
        ],
        [
            "And so in a lot of relational learning settings, what we're trying to do is infer a set of attribute values over an interconnected network of data.",
            "So here we might have instances.",
            "Say we have people that are brand nodes.",
            "They're working at organizations that are green nodes, and they have some events that are the white nodes hanging off, and what we'd like to do is infer the class labels, say whether or not a person is involved in fraud based on the attributes in its relational network.",
            "So there's two different types of.",
            "Inference that we generally use in relational learning.",
            "The first one I'm going to call individual inference, and that's where we're going to classify each instance in this network independently and will use the attributes of related instances, but will only use the observed attributes of those related instances, so there's not going to be any variability in the inference process, because generally we can do exact inference in that case, and the second type of inference is generally called collective inference, where we try to jointly infer the values of.",
            "All the objects in the data graph together, and so the inferred value for one instance is actually going to be used to infer the value for a neighboring instance.",
            "And the reason that we do this generally is because there's autocorrelation in our relational datasets.",
            "So if you know, say that somebody is involved in fraud, then the people that that person interacts with have a higher likelihood of being involved in fraud as well, and so typically when we use collective inference models, we get a significant increase in accuracy.",
            "In our models.",
            "OK, OK and I should say that they are a wide number of relational models that use collective inference techniques, so ranging from relational Markov networks, relational Bayes Nets, Markov logic networks, Bayesian logic programs.",
            "I guess I should just put up Lisa's slide of all of the all of the different names, and also there's the ad hoc collective models like the one that earlier in the LP conference that Golston and Cohen paper where they were talking about soft label and hard label propagation.",
            "OK, so once you're in a situation where you're using collective inference to make predictions in your network view."
        ],
        [
            "You actually have potentially additional sources of error, so the error can come from either the use of approximate inference techniques where you can't do or exact inference is intractable, and also we have situations where generally we do inference with a partially labeled data set, so you may have some instances that you initially know as seed labels, and then that information propagates through the rest of the network to improve the inference, and so the both the location of.",
            "Those labels and the number of labels that you have in your test set can introduce error or variation into the collective inference process, and So what we wanted was a framework that now instead of analyzing just the learning algorithm that we use in our models, we really want to move to analyzing model systems.",
            "So we want to analyze both the learning algorithm and the inference algorithm that is chosen to be coupled with the with the learning algorithm.",
            "OK, so now I'll."
        ],
        [
            "So you got the framework that we developed is, and again I'll start off with the procedural view of things and how you actually calculate the bias and variance and the so the relational version of this has two components to it.",
            "The first component is.",
            "Aimed to mimic their conventional bias variance analysis, so we have a training set that's now a graph instead of a table, and we're going to sample a number of subgraphs from that larger graph, learn models on each of those subgraphs, and then apply those models to infer the value of a single instance here in the test set.",
            "And what we're going to do in this case is use individual inference, so there's no variation due to the inference process and what we're going to do.",
            "In the case where you'd normally apply collective inferences instead of using the inferred class labels of our neighboring knows, what will use is the optimal probabilities for the class labels of those nodes in the model models.",
            "OK, so this gets you a distribution of model predictions and we're going to call that the learning distribution."
        ],
        [
            "And from the learning distribution we can measure the learning bias in the learning variance.",
            "And so in this case the we're measuring the variability of the model just based on the learned function alone.",
            "So the learn function is going to change based on the training data that you use to train it.",
            "But inference is not going to introduce any more variation into your predictions.",
            "OK, so that was the first part, the 2nd."
        ],
        [
            "Art looks very much like the 1st at the beginning here, so again we take a training set, used the same subsamples, use the same models, but now when we're going to apply those models, we apply each of those models multiple times to infer the value of a single test instance.",
            "So we do collective inference and we run that collective inference process multiple times for the same to get the prediction for the same instance.",
            "So if you had.",
            "If you were using approximate inference and you ran the inference algorithm many times, you may get different answers on each inference run, or if you have some number of labeled instances in the test set.",
            "If those instances changed at Foreach inference run, then you're going to get a different, potentially different answer for your prediction for that instance.",
            "So that gives you a distribution of model predictions as well, and we're going to call that the total distribution and now."
        ],
        [
            "From the total distribution again, we can measure the total bias and the total variance, and now this is going to be an expectation over learning and inference.",
            "So the learn function is going to again change to the training data.",
            "But inference is also going to vary based on the collective inference process.",
            "And once we have these two distributions, the learning distribution and the total."
        ],
        [
            "Solution we can calculate from those the inference bias and the inference variance.",
            "So the inference bias is the difference between the mean of the total distribution and the mean of the learn distribution and the inference variances that learning the total variance.",
            "Subtract out the learning variance.",
            "OK, and now let me show you the mathematical decomposition here."
        ],
        [
            "So.",
            "The first thing to note here is now we have an expectation over learning and inference and then now let me break up the decomposition here for you.",
            "The first three terms are the same terms from the conventional decomposition, so we have noise in the same way bias and variance which are associated with learning.",
            "So we have the expectation over learning alone and now the next two components are the inference bias.",
            "So you can see here is the difference between the mean of the total distribution in the mean of the learning distribution and we have inference variance here.",
            "Which this actually is equal to the total variation minus the learning variation, and then we have the final component here, which is an interaction term which is the learning bias here times the.",
            "Inference bias here.",
            "OK, so that was the decomposition.",
            "Now let's go back to."
        ],
        [
            "Our hypothesis about why Guardians do badly in this situation.",
            "We applied this framework to three different kinds of SRL models, which I'll discuss on the next slide, and we ran synthetic data experiments where we varied the group size in the data or the clustering of the data.",
            "The density of the linkage, the level of autocorrelation, and the proportion of labeled instances in the test set, and we compared these three models measuring learning bias and variance and inference bias and variance to see.",
            "How the models compared?"
        ],
        [
            "OK, so here are the three models we compared.",
            "We compared a latent group models that use EM for learning and belief propagation for inference.",
            "We looked at relational dependency networks that use pseudo likelihood estimation for learning and Gibbs sampling for inference.",
            "And we looked at relational Markov networks using map estimation, an loopy belief propagation for inference."
        ],
        [
            "And here I'll just tell you the high level findings now and I'll go over the results in detail for the ardens.",
            "But you can come see me at the poster to see all the full range of results that we have.",
            "So the high level results are that latent group models demonstrate high learning bias when the algorithm can't accurately identify the underlying group structure.",
            "So when the density of the linkage is too high to identify accurate groups, the latent group models do poorly.",
            "Relational Markov networks, on the other hand, have high inference bias when the data graph network is tightly connected or has very dense linkage, and the ardens do indeed have high inference variance when there's little information.",
            "Cheating the inference process, or a few labeled instances in the test set.",
            "OK."
        ],
        [
            "So here's the PDN analysis here, so the RDN line is in blue, solid blue here, so we're graphing squared loss.",
            "So lowering the graph is better, and so you can see that the PDN does really super bad when there's zero percent labeled data.",
            "In this in the test instances.",
            "But as soon as there's 30% labeled, it does much better, and in fact better than the arm ends.",
            "And so now if I show you the bias of these models, the solid lines are the inference bias, bias ease, and the dashed lines are the learning biases.",
            "So you can see here, you're just quickly that's the high learning bias that the RNS high inference bias that the arm ends are experiencing, and so there's there's higher inference bias of the audience compared to when there's few labeled instances here.",
            "But if you look at the variance, actually you can see here that it has much higher variance, and in fact this the shape of this curve here of the inference variance for the RDN looks very similar to the loss function that over the last performance that you see.",
            "OK, OK so this so I'll just point that out there, so let."
        ],
        [
            "Is just explain why this is happening with the PDN.",
            "So if you know about the Guardian learning algorithm, you know that it's a selective model.",
            "So when there is high auto correlation in the data, what the PDN model does is it's a selects.",
            "It uses the class labels of neighbors in the model in lieu of any of the observed attributes of the neighbors, and because it's not considering the inference scenario that you're going to be in at all.",
            "So when it goes to do inference, it doesn't have any known labels to anchor the Gibbs sampling inference process.",
            "As it would if it was using the observed attributes of the neighbors, and So what happens is the because you're using Gibbs sampling, the initial random labeling that you use for Gibbs sampling can skew the inference results that you end up having after that round of Gibbs sampling, and so the Gibbs sampling is causing a huge variation in the predictions that you get for certain instances, and so the initial modification that we thought of to fix this is to actually see the Gibbs sampling process instead of with a random labeling actually.",
            "Use a model to give initial labels to the nodes based on the observed attributes of themselves and their neighbors, and so just with this very simple modification we get this."
        ],
        [
            "Improvement in loss here on those same data.",
            "So we get a 40% reduction in error just by not changing the inference process or the learning process.",
            "Just changing how we seed the initial labels for the inference process and that of course comes out if you look at the variance of these models that you also see a 50% reduction in the inference variance, just including that in the algorithm.",
            "So this is just an example, let me show you that this is in fact carries out in the real data.",
            "The web KB data though."
        ],
        [
            "We initially were looking at and wondering about why we saw that poor performance.",
            "If we do the same modification, we get a 12% reduction in error on the real data as well.",
            "So this is just to show you that the framework allows us to start it, really examining the kinds of errors that are.",
            "Algorithms are experiencing both in the learning side of things and the infant side of things, and it suggests what you find in that exploration suggests algorithmic improvements that you can make.",
            "To possibly widen the space of the domains that you're going to the algorithm is going to do well in, so let me."
        ],
        [
            "Conclude that the over the set of synthetic data experiments we've done so far, what has borne out is that the collective inference process indeed introduces a significant source of error into our model, so it's not necessarily that the error is coming from the learning algorithms themselves, and more importantly, each of the relational models exhibits different types of errors, so looking at had different relative performance of these models and understanding when.",
            "They have high bias inference bias or learning bias really starts to show us the situations where we'd expect these different algorithms to perform well, and in fact it points exactly to a point that Pedro I think mentioned in the panel earlier this morning.",
            "It suggests that really developing these learning algorithms and inference algorithms independently is not necessarily the best thing to do, and this analysis may be going to show us ways that we can incorporate our knowledge of how the inference procedures.",
            "Work into our learning algorithms.",
            "So for example with our DNN we may use test set characteristics to bias the learning that we do so that we won't only select the class labels in cases where we're not going to have any labeled instances in the test set and current the current things that we're working on with this framework is to extend it to work with real datasets and analyze the algorithms on real datasets in investigate the weather.",
            "Well, the type of interaction effects that are there between learning and inference and also evaluating or investigating these potential algorithmica modifications.",
            "So I will stop there and."
        ],
        [
            "Take any questions that you have now, thanks.",
            "Differences?",
            "Yeah, that's that's a good point that that I think that's a good point when we're doing approximate inference during learning, where generally the the only error that's going to be there is due to the approximate inference technique, and that actually may be a reason that the RMN is seen.",
            "Such high inference bias because you've really skewed the model weights so far by using loopy belief propagation during the inference process.",
            "But what we're not seeing in learning phase is the variation due to the number of labeled instances, or where they are in the graph during inference.",
            "But it's true it's very difficult to D couple that inside the learning process.",
            "It would be good to think about.",
            "Did you have a?",
            "Square.",
            "OK, I think there's two issues there.",
            "First, if you're trying to actually infer the links, you can in some way, think of inferring links as inferring an attribute.",
            "If you consider all possible pairs of objects and you're just inferring some binary attribute about weather and link is there or not, and I think that would this.",
            "You could still analyze that in this case, but.",
            "And I just forgot what the other point was that I thought I was going to answer about that.",
            "I've just totally drawing a blank.",
            "did I answer your question then?",
            "Is square, yeah?",
            "Problems that may affect.",
            "Relations.",
            "OK, so now I remember what I was going to say.",
            "Actually, the thing that I sort of glossed over was really the issue of taking a larger.",
            "So instead of talking about splitting up the model, if you talk about splitting up your data set into independent samples that you can learn a model on each of the smaller subgraphs that I talked about.",
            "If your data are actually very densely linked, we don't really know how to sample something in a way that when you cut the links.",
            "You know you're not fundamentally changing the data set, so is that what you're is that where you're talking about?",
            "Yeah, I think that that's one of the reasons that we haven't that you're not seeing real data experiments yet, because we're still thinking about how to do that sampling in a way that you can accurately approximate the bias and variance.",
            "So that's a very good point.",
            "Representation.",
            "Yes, everything.",
            "Is actually.",
            "Well, with a pseudo likelihood estimation that that should come out the the variability that's due to that should come out in the learning variance.",
            "An If I go back to the slide.",
            "So you can you can see here that actually Ardian."
        ],
        [
            "I have very high learning variance as well and I think really that's what the pseudo likelihood is showing, because if if you use the optimal probabilities of your neighbors and you still get high variance, that's really showing the variance of the learning algorithm.",
            "Yes.",
            "Yes.",
            "But the other models assume that you know all of your class labels of your neighbors during learning as well.",
            "That's not something that's.",
            "Yes.",
            "1.",
            "Yes.",
            "So.",
            "Yeah, I'd still think that that would come out in the learning variance, but probably we should talk about that offline.",
            "I think that it's going to end up.",
            "There's going to be.",
            "There's an interaction between the learning and the inference variance, But what you're talking about with pseudo likelihood I think will show up in the in the learning variance and not the inference variants.",
            "But we can talk more about that.",
            "Are you next?",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thanks so you can hear me out back if I talk like this OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about framework that David and I developed to analyze the sort of error characteristics of relational learning techniques that use collective inference.",
                    "label": 0
                },
                {
                    "sent": "Can you still hear me because all of a sudden I can't?",
                    "label": 0
                },
                {
                    "sent": "Not very well.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'll just switch to this other.",
                    "label": 0
                },
                {
                    "sent": "Microphone.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I use this is that better, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start OFM.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By showing you some motivation as to why we developed these, this framework, here are some results that we have that we had a couple of years ago when we were developing these latent group models and what we're doing is we're evaluating these two relational models.",
                    "label": 0
                },
                {
                    "sent": "We're evaluating area into the RC curve here and what we're doing on the X axis is we're varying the proportion of labeled instances in the test set.",
                    "label": 0
                },
                {
                    "sent": "So we're using a collective inference process that I'll explain in more detail later.",
                    "label": 0
                },
                {
                    "sent": "But we're trying to see how the collective inference process works.",
                    "label": 1
                },
                {
                    "sent": "As we know more and more of the true class labels of the instances in the test set, and we got these results.",
                    "label": 0
                },
                {
                    "sent": "And what I was concerned about was this really poor performance that relational dependency networks had here when there were fewer than 50% labeled instances in the test set.",
                    "label": 1
                },
                {
                    "sent": "And these are these are the web data, so this is real data and.",
                    "label": 0
                },
                {
                    "sent": "My hypothesis as to why the Guardians were performing so poorly in this situation was really that the collective inference process that it was using was increasing the variance of the predictions when there were sparse seed information in the data set, and then the question became how are we going to actually evaluate this hypothesis?",
                    "label": 1
                },
                {
                    "sent": "How we're going to figure out if this is true about these algorithms and the key thing here to realize is that our hypothesis is involved with.",
                    "label": 0
                },
                {
                    "sent": "Measuring the amount of variance of the model so it was natural to think about using bias variance analysis to decompose the errors of the model and see if were in.",
                    "label": 0
                },
                {
                    "sent": "If there was more variance associated with Guardians compared to the latent group models.",
                    "label": 0
                },
                {
                    "sent": "So before I get into why we can't do this with conventional bias variance analysis, let me just tell it, remind you of what the traditional framework is and I'll show you procedure.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See how you go about measuring bias and variance in standard IID data.",
                    "label": 0
                },
                {
                    "sent": "So here we have a training set which is a table set of IID data.",
                    "label": 1
                },
                {
                    "sent": "And what we do is sample subsamples from that table, learn models on each of those sub samples and then apply all of those models to a single test set instance and then after we apply all those models will have a distribution of model predictions for that instance and so we can see how the predictions for that instance very.",
                    "label": 0
                },
                {
                    "sent": "As a function of the training data that we're going to use that we use to learn the models.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we have that distribution, we can compare it to the optimal prediction that you would make for that for that test instance.",
                    "label": 0
                },
                {
                    "sent": "So the Bayes optimal prediction is typically what we use, and if you look at the distance between the mean of the distribution, the mean of the distribution of the predictions that you had for that instance, and the optimal prediction that gives you the bias of the model, and if you look at the spread of the distribution, then that gives you the variance.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the actual math decomposition.",
                    "label": 0
                },
                {
                    "sent": "Here we have the.",
                    "label": 0
                },
                {
                    "sent": "Squared loss for an instance are predicted value of Y and the true value of T. And here the expectation is over the datasets that you use for learning the data.",
                    "label": 0
                },
                {
                    "sent": "Learning the model and so this decomposes into three factors.",
                    "label": 0
                },
                {
                    "sent": "Here we have the noise here, which is the natural variability of the true distribution you're trying to model.",
                    "label": 0
                },
                {
                    "sent": "We have the bias here, which is the 6th systematic difference between your learned function and the true function that you're trying to model.",
                    "label": 0
                },
                {
                    "sent": "And then we have the variance here, which is the variability of your learn function.",
                    "label": 0
                },
                {
                    "sent": "Given the training data used to learn it, and So what you might notice as I was describing this is that we're just talking about the learning process alone here.",
                    "label": 0
                },
                {
                    "sent": "So how does the learn function vary as a function of the data that you used to learn it?",
                    "label": 0
                },
                {
                    "sent": "What this assumes is that there's no additional variation or error due to the inference process process that you're using.",
                    "label": 0
                },
                {
                    "sent": "So when you apply the model to do inference for single instance, every time you apply that model, you're going to get the same answer, the same prediction.",
                    "label": 0
                },
                {
                    "sent": "So now let me show you.",
                    "label": 0
                },
                {
                    "sent": "Why that doesn't often hold?",
                    "label": 0
                },
                {
                    "sent": "That assumption is often violated in relational learning cases.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so in a lot of relational learning settings, what we're trying to do is infer a set of attribute values over an interconnected network of data.",
                    "label": 0
                },
                {
                    "sent": "So here we might have instances.",
                    "label": 0
                },
                {
                    "sent": "Say we have people that are brand nodes.",
                    "label": 0
                },
                {
                    "sent": "They're working at organizations that are green nodes, and they have some events that are the white nodes hanging off, and what we'd like to do is infer the class labels, say whether or not a person is involved in fraud based on the attributes in its relational network.",
                    "label": 0
                },
                {
                    "sent": "So there's two different types of.",
                    "label": 1
                },
                {
                    "sent": "Inference that we generally use in relational learning.",
                    "label": 0
                },
                {
                    "sent": "The first one I'm going to call individual inference, and that's where we're going to classify each instance in this network independently and will use the attributes of related instances, but will only use the observed attributes of those related instances, so there's not going to be any variability in the inference process, because generally we can do exact inference in that case, and the second type of inference is generally called collective inference, where we try to jointly infer the values of.",
                    "label": 1
                },
                {
                    "sent": "All the objects in the data graph together, and so the inferred value for one instance is actually going to be used to infer the value for a neighboring instance.",
                    "label": 0
                },
                {
                    "sent": "And the reason that we do this generally is because there's autocorrelation in our relational datasets.",
                    "label": 0
                },
                {
                    "sent": "So if you know, say that somebody is involved in fraud, then the people that that person interacts with have a higher likelihood of being involved in fraud as well, and so typically when we use collective inference models, we get a significant increase in accuracy.",
                    "label": 0
                },
                {
                    "sent": "In our models.",
                    "label": 0
                },
                {
                    "sent": "OK, OK and I should say that they are a wide number of relational models that use collective inference techniques, so ranging from relational Markov networks, relational Bayes Nets, Markov logic networks, Bayesian logic programs.",
                    "label": 1
                },
                {
                    "sent": "I guess I should just put up Lisa's slide of all of the all of the different names, and also there's the ad hoc collective models like the one that earlier in the LP conference that Golston and Cohen paper where they were talking about soft label and hard label propagation.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you're in a situation where you're using collective inference to make predictions in your network view.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You actually have potentially additional sources of error, so the error can come from either the use of approximate inference techniques where you can't do or exact inference is intractable, and also we have situations where generally we do inference with a partially labeled data set, so you may have some instances that you initially know as seed labels, and then that information propagates through the rest of the network to improve the inference, and so the both the location of.",
                    "label": 1
                },
                {
                    "sent": "Those labels and the number of labels that you have in your test set can introduce error or variation into the collective inference process, and So what we wanted was a framework that now instead of analyzing just the learning algorithm that we use in our models, we really want to move to analyzing model systems.",
                    "label": 1
                },
                {
                    "sent": "So we want to analyze both the learning algorithm and the inference algorithm that is chosen to be coupled with the with the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'll.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you got the framework that we developed is, and again I'll start off with the procedural view of things and how you actually calculate the bias and variance and the so the relational version of this has two components to it.",
                    "label": 0
                },
                {
                    "sent": "The first component is.",
                    "label": 0
                },
                {
                    "sent": "Aimed to mimic their conventional bias variance analysis, so we have a training set that's now a graph instead of a table, and we're going to sample a number of subgraphs from that larger graph, learn models on each of those subgraphs, and then apply those models to infer the value of a single instance here in the test set.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do in this case is use individual inference, so there's no variation due to the inference process and what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "In the case where you'd normally apply collective inferences instead of using the inferred class labels of our neighboring knows, what will use is the optimal probabilities for the class labels of those nodes in the model models.",
                    "label": 1
                },
                {
                    "sent": "OK, so this gets you a distribution of model predictions and we're going to call that the learning distribution.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from the learning distribution we can measure the learning bias in the learning variance.",
                    "label": 0
                },
                {
                    "sent": "And so in this case the we're measuring the variability of the model just based on the learned function alone.",
                    "label": 1
                },
                {
                    "sent": "So the learn function is going to change based on the training data that you use to train it.",
                    "label": 1
                },
                {
                    "sent": "But inference is not going to introduce any more variation into your predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the first part, the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Art looks very much like the 1st at the beginning here, so again we take a training set, used the same subsamples, use the same models, but now when we're going to apply those models, we apply each of those models multiple times to infer the value of a single test instance.",
                    "label": 0
                },
                {
                    "sent": "So we do collective inference and we run that collective inference process multiple times for the same to get the prediction for the same instance.",
                    "label": 0
                },
                {
                    "sent": "So if you had.",
                    "label": 0
                },
                {
                    "sent": "If you were using approximate inference and you ran the inference algorithm many times, you may get different answers on each inference run, or if you have some number of labeled instances in the test set.",
                    "label": 0
                },
                {
                    "sent": "If those instances changed at Foreach inference run, then you're going to get a different, potentially different answer for your prediction for that instance.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a distribution of model predictions as well, and we're going to call that the total distribution and now.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the total distribution again, we can measure the total bias and the total variance, and now this is going to be an expectation over learning and inference.",
                    "label": 1
                },
                {
                    "sent": "So the learn function is going to again change to the training data.",
                    "label": 1
                },
                {
                    "sent": "But inference is also going to vary based on the collective inference process.",
                    "label": 0
                },
                {
                    "sent": "And once we have these two distributions, the learning distribution and the total.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution we can calculate from those the inference bias and the inference variance.",
                    "label": 1
                },
                {
                    "sent": "So the inference bias is the difference between the mean of the total distribution and the mean of the learn distribution and the inference variances that learning the total variance.",
                    "label": 0
                },
                {
                    "sent": "Subtract out the learning variance.",
                    "label": 0
                },
                {
                    "sent": "OK, and now let me show you the mathematical decomposition here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first thing to note here is now we have an expectation over learning and inference and then now let me break up the decomposition here for you.",
                    "label": 1
                },
                {
                    "sent": "The first three terms are the same terms from the conventional decomposition, so we have noise in the same way bias and variance which are associated with learning.",
                    "label": 0
                },
                {
                    "sent": "So we have the expectation over learning alone and now the next two components are the inference bias.",
                    "label": 1
                },
                {
                    "sent": "So you can see here is the difference between the mean of the total distribution in the mean of the learning distribution and we have inference variance here.",
                    "label": 0
                },
                {
                    "sent": "Which this actually is equal to the total variation minus the learning variation, and then we have the final component here, which is an interaction term which is the learning bias here times the.",
                    "label": 0
                },
                {
                    "sent": "Inference bias here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the decomposition.",
                    "label": 0
                },
                {
                    "sent": "Now let's go back to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our hypothesis about why Guardians do badly in this situation.",
                    "label": 0
                },
                {
                    "sent": "We applied this framework to three different kinds of SRL models, which I'll discuss on the next slide, and we ran synthetic data experiments where we varied the group size in the data or the clustering of the data.",
                    "label": 0
                },
                {
                    "sent": "The density of the linkage, the level of autocorrelation, and the proportion of labeled instances in the test set, and we compared these three models measuring learning bias and variance and inference bias and variance to see.",
                    "label": 0
                },
                {
                    "sent": "How the models compared?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here are the three models we compared.",
                    "label": 0
                },
                {
                    "sent": "We compared a latent group models that use EM for learning and belief propagation for inference.",
                    "label": 0
                },
                {
                    "sent": "We looked at relational dependency networks that use pseudo likelihood estimation for learning and Gibbs sampling for inference.",
                    "label": 0
                },
                {
                    "sent": "And we looked at relational Markov networks using map estimation, an loopy belief propagation for inference.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here I'll just tell you the high level findings now and I'll go over the results in detail for the ardens.",
                    "label": 0
                },
                {
                    "sent": "But you can come see me at the poster to see all the full range of results that we have.",
                    "label": 0
                },
                {
                    "sent": "So the high level results are that latent group models demonstrate high learning bias when the algorithm can't accurately identify the underlying group structure.",
                    "label": 1
                },
                {
                    "sent": "So when the density of the linkage is too high to identify accurate groups, the latent group models do poorly.",
                    "label": 0
                },
                {
                    "sent": "Relational Markov networks, on the other hand, have high inference bias when the data graph network is tightly connected or has very dense linkage, and the ardens do indeed have high inference variance when there's little information.",
                    "label": 1
                },
                {
                    "sent": "Cheating the inference process, or a few labeled instances in the test set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the PDN analysis here, so the RDN line is in blue, solid blue here, so we're graphing squared loss.",
                    "label": 0
                },
                {
                    "sent": "So lowering the graph is better, and so you can see that the PDN does really super bad when there's zero percent labeled data.",
                    "label": 0
                },
                {
                    "sent": "In this in the test instances.",
                    "label": 0
                },
                {
                    "sent": "But as soon as there's 30% labeled, it does much better, and in fact better than the arm ends.",
                    "label": 0
                },
                {
                    "sent": "And so now if I show you the bias of these models, the solid lines are the inference bias, bias ease, and the dashed lines are the learning biases.",
                    "label": 0
                },
                {
                    "sent": "So you can see here, you're just quickly that's the high learning bias that the RNS high inference bias that the arm ends are experiencing, and so there's there's higher inference bias of the audience compared to when there's few labeled instances here.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the variance, actually you can see here that it has much higher variance, and in fact this the shape of this curve here of the inference variance for the RDN looks very similar to the loss function that over the last performance that you see.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so this so I'll just point that out there, so let.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is just explain why this is happening with the PDN.",
                    "label": 0
                },
                {
                    "sent": "So if you know about the Guardian learning algorithm, you know that it's a selective model.",
                    "label": 0
                },
                {
                    "sent": "So when there is high auto correlation in the data, what the PDN model does is it's a selects.",
                    "label": 0
                },
                {
                    "sent": "It uses the class labels of neighbors in the model in lieu of any of the observed attributes of the neighbors, and because it's not considering the inference scenario that you're going to be in at all.",
                    "label": 0
                },
                {
                    "sent": "So when it goes to do inference, it doesn't have any known labels to anchor the Gibbs sampling inference process.",
                    "label": 0
                },
                {
                    "sent": "As it would if it was using the observed attributes of the neighbors, and So what happens is the because you're using Gibbs sampling, the initial random labeling that you use for Gibbs sampling can skew the inference results that you end up having after that round of Gibbs sampling, and so the Gibbs sampling is causing a huge variation in the predictions that you get for certain instances, and so the initial modification that we thought of to fix this is to actually see the Gibbs sampling process instead of with a random labeling actually.",
                    "label": 0
                },
                {
                    "sent": "Use a model to give initial labels to the nodes based on the observed attributes of themselves and their neighbors, and so just with this very simple modification we get this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Improvement in loss here on those same data.",
                    "label": 0
                },
                {
                    "sent": "So we get a 40% reduction in error just by not changing the inference process or the learning process.",
                    "label": 1
                },
                {
                    "sent": "Just changing how we seed the initial labels for the inference process and that of course comes out if you look at the variance of these models that you also see a 50% reduction in the inference variance, just including that in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is just an example, let me show you that this is in fact carries out in the real data.",
                    "label": 0
                },
                {
                    "sent": "The web KB data though.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We initially were looking at and wondering about why we saw that poor performance.",
                    "label": 0
                },
                {
                    "sent": "If we do the same modification, we get a 12% reduction in error on the real data as well.",
                    "label": 1
                },
                {
                    "sent": "So this is just to show you that the framework allows us to start it, really examining the kinds of errors that are.",
                    "label": 0
                },
                {
                    "sent": "Algorithms are experiencing both in the learning side of things and the infant side of things, and it suggests what you find in that exploration suggests algorithmic improvements that you can make.",
                    "label": 0
                },
                {
                    "sent": "To possibly widen the space of the domains that you're going to the algorithm is going to do well in, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclude that the over the set of synthetic data experiments we've done so far, what has borne out is that the collective inference process indeed introduces a significant source of error into our model, so it's not necessarily that the error is coming from the learning algorithms themselves, and more importantly, each of the relational models exhibits different types of errors, so looking at had different relative performance of these models and understanding when.",
                    "label": 1
                },
                {
                    "sent": "They have high bias inference bias or learning bias really starts to show us the situations where we'd expect these different algorithms to perform well, and in fact it points exactly to a point that Pedro I think mentioned in the panel earlier this morning.",
                    "label": 0
                },
                {
                    "sent": "It suggests that really developing these learning algorithms and inference algorithms independently is not necessarily the best thing to do, and this analysis may be going to show us ways that we can incorporate our knowledge of how the inference procedures.",
                    "label": 0
                },
                {
                    "sent": "Work into our learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for example with our DNN we may use test set characteristics to bias the learning that we do so that we won't only select the class labels in cases where we're not going to have any labeled instances in the test set and current the current things that we're working on with this framework is to extend it to work with real datasets and analyze the algorithms on real datasets in investigate the weather.",
                    "label": 1
                },
                {
                    "sent": "Well, the type of interaction effects that are there between learning and inference and also evaluating or investigating these potential algorithmica modifications.",
                    "label": 0
                },
                {
                    "sent": "So I will stop there and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take any questions that you have now, thanks.",
                    "label": 0
                },
                {
                    "sent": "Differences?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good point that that I think that's a good point when we're doing approximate inference during learning, where generally the the only error that's going to be there is due to the approximate inference technique, and that actually may be a reason that the RMN is seen.",
                    "label": 0
                },
                {
                    "sent": "Such high inference bias because you've really skewed the model weights so far by using loopy belief propagation during the inference process.",
                    "label": 0
                },
                {
                    "sent": "But what we're not seeing in learning phase is the variation due to the number of labeled instances, or where they are in the graph during inference.",
                    "label": 0
                },
                {
                    "sent": "But it's true it's very difficult to D couple that inside the learning process.",
                    "label": 0
                },
                {
                    "sent": "It would be good to think about.",
                    "label": 0
                },
                {
                    "sent": "Did you have a?",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 0
                },
                {
                    "sent": "OK, I think there's two issues there.",
                    "label": 0
                },
                {
                    "sent": "First, if you're trying to actually infer the links, you can in some way, think of inferring links as inferring an attribute.",
                    "label": 0
                },
                {
                    "sent": "If you consider all possible pairs of objects and you're just inferring some binary attribute about weather and link is there or not, and I think that would this.",
                    "label": 0
                },
                {
                    "sent": "You could still analyze that in this case, but.",
                    "label": 0
                },
                {
                    "sent": "And I just forgot what the other point was that I thought I was going to answer about that.",
                    "label": 0
                },
                {
                    "sent": "I've just totally drawing a blank.",
                    "label": 0
                },
                {
                    "sent": "did I answer your question then?",
                    "label": 0
                },
                {
                    "sent": "Is square, yeah?",
                    "label": 0
                },
                {
                    "sent": "Problems that may affect.",
                    "label": 0
                },
                {
                    "sent": "Relations.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I remember what I was going to say.",
                    "label": 0
                },
                {
                    "sent": "Actually, the thing that I sort of glossed over was really the issue of taking a larger.",
                    "label": 0
                },
                {
                    "sent": "So instead of talking about splitting up the model, if you talk about splitting up your data set into independent samples that you can learn a model on each of the smaller subgraphs that I talked about.",
                    "label": 0
                },
                {
                    "sent": "If your data are actually very densely linked, we don't really know how to sample something in a way that when you cut the links.",
                    "label": 0
                },
                {
                    "sent": "You know you're not fundamentally changing the data set, so is that what you're is that where you're talking about?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that that's one of the reasons that we haven't that you're not seeing real data experiments yet, because we're still thinking about how to do that sampling in a way that you can accurately approximate the bias and variance.",
                    "label": 0
                },
                {
                    "sent": "So that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "Yes, everything.",
                    "label": 0
                },
                {
                    "sent": "Is actually.",
                    "label": 0
                },
                {
                    "sent": "Well, with a pseudo likelihood estimation that that should come out the the variability that's due to that should come out in the learning variance.",
                    "label": 0
                },
                {
                    "sent": "An If I go back to the slide.",
                    "label": 0
                },
                {
                    "sent": "So you can you can see here that actually Ardian.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have very high learning variance as well and I think really that's what the pseudo likelihood is showing, because if if you use the optimal probabilities of your neighbors and you still get high variance, that's really showing the variance of the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "But the other models assume that you know all of your class labels of your neighbors during learning as well.",
                    "label": 0
                },
                {
                    "sent": "That's not something that's.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'd still think that that would come out in the learning variance, but probably we should talk about that offline.",
                    "label": 0
                },
                {
                    "sent": "I think that it's going to end up.",
                    "label": 0
                },
                {
                    "sent": "There's going to be.",
                    "label": 0
                },
                {
                    "sent": "There's an interaction between the learning and the inference variance, But what you're talking about with pseudo likelihood I think will show up in the in the learning variance and not the inference variants.",
                    "label": 0
                },
                {
                    "sent": "But we can talk more about that.",
                    "label": 0
                },
                {
                    "sent": "Are you next?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}