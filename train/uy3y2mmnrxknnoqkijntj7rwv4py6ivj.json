{
    "id": "uy3y2mmnrxknnoqkijntj7rwv4py6ivj",
    "title": "Multi-task Learning with Gaussian Processes",
    "info": {
        "author": [
            "Chris Williams, University of Edinburgh"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/bark08_williams_mtlwgp/",
    "segmentation": [
        [
            "OK, so I'll try to work this time.",
            "I want to talk about multitask Gaussian process prediction and this is joint work with a bunch of different people.",
            "Edwin Bonier coming Chai Steven Clanking that you Vijay Kumar all in Edinburgh.",
            "Some of the."
        ],
        [
            "This probably you will have seen before, but there is some new stuff here about the robotic task that I want to get to.",
            "So in general, and this certainly is the case, for example in what we was talking about yesterday is that we did.",
            "We shouldn't really come to new tasks.",
            "It is so tabular rasa, function and say oh I forgot everything I ever learned about all those other tasks.",
            "I was I was trying to solve before and how that might relate to my new task.",
            "It seems kind of a bad idea, so it does make sense to say we share information across tasks.",
            "And of course typically we probably do that, but we maybe do that in various kinds of informal ways, which are to remember that we found that this method or this kind of thing worked quite well for this kind of problem.",
            "That seems quite like this other problem.",
            "We're working on, but we could try and be.",
            "We could actually try and make use of the data from these other tasks, and that's something I want to talk about here.",
            "So there are a bunch of examples where they could actually be multiple related problems or problem."
        ],
        [
            "So is potentially related.",
            "And of course, if we just assume that tasks are related when they're not, that could be detrimental.",
            "We could also try to really think about task relatedness in a couple of ways.",
            "One way is that we might just say, OK, I can try and describe my task in certain ways.",
            "Maybe I can come up with some task descriptors which actually.",
            "Swimming express what these tasks are and then I can kind of look at the similarity of tasks by considering those task descriptors and.",
            "Sort of makes certain can make some sense, although.",
            "There it is."
        ],
        [
            "Can be difficult to find task descriptors.",
            "Certainly in some of the compiler problems that we looked at.",
            "It seems very hard to gain even from compiler experts.",
            "Seems very hard to extract features that really characterize the these different problems.",
            "So one thing you could do is to sort of directly have some notion of Inter task similarity and actually trying to learn those things so we actually don't need to go through the notion of task descriptors.",
            "But we can just actually talk about correlations between tasks and then within a GP framework that so."
        ],
        [
            "Quite an easy thing to do so.",
            "In outline, I want to talk about basically the model.",
            "We might make predictions with it, a sort of curious result about cancellation of transfer.",
            "And I really want to get down to this stuff here when we actually talk about multitask learning in robot inverse dynamics, because in fact, in that case we have some.",
            "Basically, it turns out that the kind of factorization that we actually would have been assuming in some other work.",
            "Actually we can show actually applies precisely in the in the right form for the robot inverse dynamics problem, and that means we can hopefully we can actually get some nice results in that problem."
        ],
        [
            "So the multi task setting is that we've got.",
            "We've got em different tasks and say we've got 10 different inputs.",
            "We of course don't need to see all this data we don't expect expect normally that will have a kind of complete grid design where we see all the observations on all tasks.",
            "But conceptually we can have this design.",
            "So we've got Y one up to Y when on the 1st task and so on up to the task.",
            "And given our observations WHIO, which is subset of these complete.",
            "Design then we'd like."
        ],
        [
            "To make predictions.",
            "One fairly simple model for a multitask problem, so we've got some some functions FL and FM, and we've got locations X&X prime and then we can actually just factorize this into some Inter task similarity matrix, KF and some sort of covariance function KX.",
            "This is clearly quite a strong assumption.",
            "But it's actually probably one of the simplest assumptions we could actually make about multitask learning."
        ],
        [
            "It's worth saying that.",
            "Often people think about multi task learning, perhaps in hierarchical modeling framework we have some say some theater and we maybe draw parameters for regression, say from some distribution over theaters.",
            "And if that if that.",
            "Distribution of a theater or something like a mixture distribution we might actually get a bunch of kind of related tasks where we drawing from the same mixture component and then a bunch of different tasks.",
            "Of a different task cluster, so we can.",
            "We can imagine that kind of stuff."
        ],
        [
            "In a sense, what the?",
            "The kind of frame what we do in the GP is kind of integrating out this data and saying we really just going to induce correlations between all these different apps."
        ],
        [
            "Always different underlying function.",
            "And of course, when we do that, observations on one."
        ],
        [
            "Task will effect predict."
        ],
        [
            "Another it's of course in this kind of thing it's quite easy to talk about multitask clustering.",
            "Basically, if we believe that there are groups of tasks that should be clustered together, but they have no relation to other tasks, then of course that KF that Inter task similarity matrix will just have a block diagonal structure representing those cluster stuff.",
            "It's easy to see we can do."
        ],
        [
            "Really pops is a picture and Maps.",
            "Is it happening?",
            "Little dot OK, let's just think about this in terms of 1 dimensional space.",
            "So if we have X is so some some individual problem might be if we drew from our Gaussian process would have this kind of form.",
            "The key thing is what we've got as we actually as we consider different tasks.",
            "The whole point is not independent draws from some Gaussian process with covariance KX, but there's somehow correlated.",
            "So we actually get a bunch of correlated tasks that look like this.",
            "That's what sort of.",
            "That's a useful picture.",
            "That's a good question in the.",
            "In the robot case, we certainly can actually think about having a continuous space for those tasks to live in.",
            "Perhaps not, but we don't have to.",
            "I mean, it's yeah.",
            "Well, you probably can imagine how I actually generated this figure.",
            "In that case, it was convenient to do it in a continuous."
        ],
        [
            "OK, so making predictions?",
            "If we've got, we've got complete data, then in fact, there's a Chronicle product structure of the covariance matrix between at all.",
            "X is all tasks, and we can exploit that.",
            "But in general, of course, if we don't have complete data, then this complete grid design, then we won't be able to have that structure, but it's just a big GP.",
            "We can just condition on what we see and make predictions."
        ],
        [
            "Slide.",
            "Um, it's also true that we can.",
            "Then the hyperparameters in the usual way if we're doing M L2, we can maximize the marginal likelihood respect these parameters.",
            "The only slight trick is that if we want to parameterise KF, we have to be a little bit careful about that when we do this, we typically use a cheskey decomposition form for that, but there actually is, for example, another way of doing this with them, where you actually just get guaranteed positive symmetric definite updates.",
            "Just buy it.",
            "It just falls out of the game so that all works."
        ],
        [
            "Nicely.",
            "One kind of curious thing about this model.",
            "Is.",
            "That if we have no noise and we have grid observations, then something rather weird happens, which is that say I consider this task F2 and I'm actually got observations at all these black dots.",
            "And so I've got this kind of grid.",
            "I've got the tasks and they've got the X is and then I say OK, I want to make this prediction for F2 on X star.",
            "If I workout actually what those?",
            "Predictions actually depend on it actually only depends on the observations.",
            "I got F2, so even though I've got this big joint Gaussian.",
            "Are all these axes and I actually know about what's going on these other places?",
            "Then in fact those."
        ],
        [
            "Things actually aren't informative.",
            "About my predictions for F2.",
            "There are of course 2 caveats we need this grid design an we need to have no observation noise.",
            "If we if neither of those things happen.",
            "And this isn't true, and knowing about the processes elsewhere will actually help.",
            "So for example, if I didn't have an observation here, but I did have one there, then that would also violate things and those other observations would be helpful.",
            "So it turns out actually thanks to Dan for telling me about this.",
            "Actually talked about this.",
            "Asked you Acms and then said well, there's something called Auto Krige ability in geostatistics, and what's that about?",
            "And it turns out that actually, this is this result, so we worked it out and then it turned out it was known that before.",
            "Is.",
            "I, I'm not sure I it falls out very easily from the comic Chronicle.",
            "Product forms of the of the.",
            "There probably is, and I've thought about that before.",
            "I'm not sure if I have stored the answer.",
            "Kernelization.",
            "Well.",
            "Should be careful with the user, usually coined by French people these terms and."
        ],
        [
            "Maybe they work better in French.",
            "OK um.",
            "OK, so clearly there's a lot of work on multi task learning.",
            "Certainly in machine learning in the mid 90s Sebastian true and Rich Caruana talked about this in terms of Gaussian process.",
            "Some work done, certainly well.",
            "The other stuff I know about this is by Tom Minka, but there's certainly other people in this audience who also use this.",
            "If we, if we assume you've got multiple tasks, they shame this share the same hyperparameters, but they're not really authorized correlated, then certainly learning those those hyperparameters together.",
            "That makes sense, and people have done that.",
            "This really is that, of course, exactly what the cokriging setup in your statistics is about.",
            "And in fact they do also use this kind of covariance function, and they also use like more complicated ones.",
            "For example sums of this kind of thing.",
            "I guess let me.",
            "That's one thing is worth saying about focusing on it is for example the work of UIT and others on the three semiparametric latent factor model so that that is actually So what they actually have is a bunch of GPS which are linearly mixed to produce the observed functions and will see.",
            "Basically this is very similar to what we do.",
            "The main difference and actually works exactly like that for the robot arm problem.",
            "One of the main difference is that they actually considered.",
            "That each of these latent these P latent process had its own covariance function and kind of made things a bit more complicated.",
            "If you actually assume they all have the same covariance function, and basically you actually get this structure like we have here."
        ],
        [
            "So let me skip over so we had a NIPS paper last year where we talked about some experiments and compiler."
        ],
        [
            "Performance prediction and also on exam score prediction that the latter one is the data set that backer in his skis had used actually.",
            "In in, in previous works we use that for compare."
        ],
        [
            "But and we could show that this is."
        ],
        [
            "This was affective there, but I really want to talk about was the multitask learning and robot inverse dynamics.",
            "So.",
            "OK, here's the setup we have.",
            "Some more, but we actually be considering a more complex one, but let's just for now just consider this simple picture with two 2 links and we can parameterise that by these queues these joint angles.",
            "What we're interested in really doing, and so saying, we've decided that we want to take this arm and follow a trajectory, and so we have specified essentially positions last season.",
            "Accelerations along this trajectory and what we need to do is to figure out what talks to apply at these joints so as to achieve this trajectory.",
            "That's the that's the inverse dynamics problem.",
            "OK, that's what we're going to be talking."
        ],
        [
            "App.",
            "And.",
            "We would like to know we can collect some data is but we drive this robot arm we we find what talks when we apply talks, how those affect the queues and so on.",
            "So we can actually get some data about this about this function and you might think, well, yeah you could do it in a data driven way, but should you really be doing that, there's a lot of physics in this right?",
            "You know, there's a we can write down Newton's laws.",
            "We can do all that.",
            "Do we really need to actually sort of Dua?",
            "Kind of black box machine learning approach to this, and that's perhaps certainly true.",
            "It certainly makes a lot of sense to think about the dynamics problem, and so one can derive various.",
            "Essentially, yeah, Newton's equations for this system, and certainly this isn't our work.",
            "There's a lot of stuff in robotics, and about about these equations and sort of analyzing them.",
            "Come, however, although we can actually write down the physics for this there there there certainly there are things we have to do, like parameter estimation for various parameters in that we also have to worry about some things like friction and contact forces, joint elasticity and so on.",
            "Other things that certainly for real sort of compliant lightweight robots like you.",
            "If you have some sort of lightweight human humanoid arm style thing, which certainly my colleague said to Vijay Kumar is interested in.",
            "Then you might worry about some of these problems that arise when actually trying to model this in a physics way so."
        ],
        [
            "Our strategy.",
            "And this is something else, because we have to worry about, which is that typically.",
            "The things we actually probably care about most is actually being able to control this under different load, so I can pick up various different loads and actually then control it in that you follow trajectories and the question is really important.",
            "Most interesting thing that this gives rise to multitask problem.",
            "Basically, given these different loads that we can have on the end effector and we still want to control these different things, how can we?",
            "How can we actually learn about that?",
            "So that's the thing we want to do, and the bad news is of course the talk functions change as a function of the load on the end effector.",
            "And it's also true that.",
            "Maybe I should explore different spaces with different loads.",
            "Certainly if some loads are heavier than others then.",
            "I would probably not move them so fast, so it might be more difficult to move them fast.",
            "So there are perhaps if we think of the X SpaceX is QQQ double deck.",
            "It's the positions, velocities and acceleration's.",
            "Then we can actually.",
            "We might explore different."
        ],
        [
            "That space so perhaps the saving grace.",
            "The thing that someone will make this work is really that if we consider that our IX of that Ally X is that talk for the ice joint.",
            "And this is the NTH context.",
            "So here think of context is different loads that I've got.",
            "Then it turns out the.",
            "Basically this is kind of linear in some.",
            "In this these pies is parameters that cycle dynamic parameters which actually physically characterize, so the mass the moment of inertia of the central gravity in the moment of inertia of the of the load on the end effector.",
            "And we can also, perhaps including here some coefficients of friction and so on.",
            "Anyway, the key thing is that essentially because of the way that the physics works, one thing we can take away from the physics.",
            "Perhaps abstracting away some of the details of the.",
            "Equations is basically the way that these different parameters enter these equations, so this is basically there are to get the talk for the I TH context.",
            "There are basically sort of 11 functions that we should know about.",
            "These wise whyyy is a vector of 11 functions corresponding to the I TH joint and what we do is drop them with Pi M. So I think that you can then see that.",
            "Basically, if we have different.",
            "Iams for different masses, then we can actually exploit this.",
            "So what's going on?",
            "We can just just think a little bit about some reparameterization.",
            "There's actually nothing to stop us adding in a non singular matrix.",
            "In her reading, we can kind of convert these things, so think about.",
            "Just call this some functions, Ed and some parameters row.",
            "These are basically the wise and apply."
        ],
        [
            "Then let's think about having this is same talk, so it's the same joint.",
            "On the elf context and on the M context, and really, let's put Gaussian process priors over the Zeds.",
            "OK, we just got the talk is just a linear combination of these is weighted by this thing bro.",
            "Basically what will happen is that if we go deep fryers on the.",
            "On the zed functions and we basically we just got different weights.",
            "Just like this, and really what's happening is that this the correlation between the MLF task on the end task depends on the dot product between the row parameters for the NTH task and the Alpha task.",
            "So think about it like this.",
            "We've got these latent functions Ed.",
            "And what we're doing is taking some linear combination of those with this row vector to get Tau I and basically different.",
            "Tasks different EMS give rise to different linear combination.",
            "So thinking about the semiparametric latent factor model, this is really exactly the set up right?",
            "So really the point is what we've got we're assuming.",
            "The simplicity that all these ads have the same covariance function and also essentially what we've done is discover a problem where the set up was actually the solution, right?",
            "So given this, what we really of course want to do is to try and exploit.",
            "Cross task learning or sharing of information across tasks in order to improve."
        ],
        [
            "Control.",
            "So is that I think of this as K. Anyway, the covariance function, what we actually using is just.",
            "Perhaps something is generally fairly.",
            "Simple bias, linear term, square, exponential term, but there's actually a special bit of physics that comes in here, which is that the Coulomb friction term depends on the sign.",
            "Signal of Q DOT is basically the way that they could on friction term works, so it's definitely very important in terms of predicted performance to actually include that."
        ],
        [
            "So OK. We want to do some experiments to try and verify that this is actually going to work.",
            "Here's a picture of the Puma 560 robot arm, which is a 6 degree of freedom arm, and we're doing all this in the simulation.",
            "I have to admit.",
            "We're gonna consider.",
            "A bunch of different loads, so 15 different loads.",
            "And they have various shapes and sizes and some different masses were also to explore this idea about that we might be interested in.",
            "Going in different parts of different parts of X space, we actually also going to drawing to consider.",
            "We're going to figure of eight trajectories that we move load around.",
            "We consider four different directories and also at four different speeds, so we can do a number of different trajectory and speed we consider.",
            "And these are the sort of different parts of X space, so we're actually going to be particularly interested in the case where we maybe pick up one load and do one kind of trajectory with it.",
            "Then we actually want to maybe follow another kind of trajectory with that same load.",
            "We actually haven't seen any data in that context before.",
            "We actually want to sort of generalize to that."
        ],
        [
            "OK, so where we actually done the experiments is that we're actually going to use one reference directory, which is actually 14 unique trajectory's, one for each of the other context, one for each context.",
            "And then basically we consider two kinds of test data.",
            "Either interpolation were essentially, you know, we've taken this load and we've seen it in this context, and following its particular trajectory, we're going to try and then test on this trajectory, or we consider extrapolation, where we go to a new trajectory that we haven't seen before.",
            "We've seen data on other trajectories.",
            "I'm going to seeing everything on the reference trajectory, and we're going to try."
        ],
        [
            "Crossover.",
            "The.",
            "I'll tell you about some experiments with three different methods.",
            "The independent Gaussian process.",
            "So basically, here this is exactly what we were talking about before.",
            "We basically only take data from each context.",
            "But we assume the save the hyperparameters that I'd across logs, and actually we've found the experiments.",
            "This works better than just doing the single basically in the single thing where we learn hyper parameters for each load separately.",
            "Um?",
            "Pooled Gaussian process.",
            "Well, the other the other way to deal with multi task learning is actually, so it's all one problem.",
            "And actually let's just consider that altogether.",
            "And that actually sometimes for things that people have talked about as multitask learning in the literature and actually doesn't turn to be such a bad thing to do.",
            "However, it's certainly not going to be highly effective in this context, and also multi task Gaussian process where we're doing what we're talking about.",
            "Here we have this KF this thing, which is actually the Inter task similarity, and we want to learn that matrix.",
            "Which is fundamentally about those.",
            "Those premises, those dynamic parameters.",
            "The DOT product those dynamic parameters.",
            "There's also a little bit more detail, which is in fact we're going to control the rank of KF, and we're actually going to do that with BICS.",
            "Like like an indicator function for that context.",
            "To say I mean it.",
            "Having waiter.",
            "So do you wanna do you wanna draw what?",
            "What I shall we so we talk about it afterwards, OK?",
            "That perhaps relates to this notion of task descriptors, but ask me that at the end.",
            "Think about this.",
            "Basically, becausw.",
            "You know you're giving yourself more free parameters in KF, right?",
            "As you increase the rank of it, and you're not gonna, actually.",
            "You know there there is no.",
            "Peak you gonna find it, yet we are optimizing.",
            "We're not integrating.",
            "We're being incompletely Bayesian, definitely.",
            "As is very common."
        ],
        [
            "Definitely, that's true.",
            "OK, so this might be a little bit of complicated graph to look at.",
            "First of all, there's two sets of experiments.",
            "This is interpolation and this is extrapolation.",
            "We've got six joints.",
            "In general, this is joint one joint, 4 joint, six.",
            "There are three curves which actually I can't even see the difference at this here.",
            "OK IGP which I think is in red, although hard to see PGP the pooled which is in blue an MVP which is the.",
            "The multiple GP which is in black and I'll sort of talk you through the results.",
            "So basically what OK there's one other thing.",
            "There's one other thing you should know about the plotting, which is that if a method is actually doing very poorly, we just stick it at the top of the of the Y scale here.",
            "So basically these other things could be way off the scale, but we just conventionally putting them here.",
            "Basically what we can see.",
            "Is for example on this method that the pool Gaussian process.",
            "Actually this is the interpolation pool.",
            "Gaussian process is doing very poorly, and in fact the independent Gaussian process and the multiple Gaussian processes are doing about the same here.",
            "Actually, for joint one, the pool Gaussian process isn't doing so badly.",
            "This is on the interpolation.",
            "Perhaps the more interesting cases, the extrapolation where here the independent Gaussian process is doing kind of badly, and in fact the pooled and multiple GP are doing or other similar.",
            "This is the normalized mean squared error.",
            "And notice these facts about here.",
            "This is the amount of data that we have.",
            "So basically in this case the MVP is doing better as well as the pool Gaussian process.",
            "This is a joint one where it actually is pretty sure it's one that's right.",
            "It's near down near the base of the robot, so in many ways perhaps that the tasks maybe all data can be pulled because it's all.",
            "Mirror in space, whereas if we consider say join 406 which are further out along the the arm.",
            "Here we see for example and that the multi task.",
            "GP is actually performing better than the than the independent GP and the pull Jeep pull Gaussian process is off the scale here.",
            "So so basically we can demonstrate and these are these are catch.",
            "These are actually averages over the 15 different context we've got.",
            "OK, 'cause what we're considering is for given joint.",
            "We're extrapolating the other contexts and we're actually looking at these averages over these contexts.",
            "So basically, at least in this kind of setup, we can demonstrate that this is actually working reason."
        ],
        [
            "Effectively.",
            "Maybe just to conclude.",
            "Um?",
            "We've talked about this factorization between KX&KF and that's actually we've talked about that before and where KF encodes task similarity.",
            "One while the nice thing about the multi task multi context inverse robot inverse dynamics problem is in fact this actually fits this structure very well, so it's not in other work.",
            "Typically this is an assumption about this decomposition.",
            "Here we can actually show that this decomposition is actually appropriate for this kind of this kind of data.",
            "I guess it's perhaps fairly obvious to say, is that clearly this kind of internally for general problems?",
            "This kind of decomposition between KX&KF is a fairly strong assumption, and it's actually probably more interesting to ask.",
            "About what kinds of more general structures could allow for the full multi task learning when we don't have such simple.",
            "Factorization assumption, so you stop.",
            "Did you want to start off by asking you?",
            "OK.",
            "So we were sort of following up some of your work.",
            "Coffee break.",
            "But one thing that I thought hitting with.",
            "It's it's this multi output GP at the end of the day I mean.",
            "In what way does multitask?",
            "Task rather than multi output.",
            "OK, so.",
            "I guess the.",
            "Point is that.",
            "And certainly in this standard multi output setting right?",
            "You might consider the case that essentially you have a bunch of functions and they have some correlated noise right between these different things.",
            "That's a standard multi output setting, whereas in certainly in this case the multitask thing you actually consider there are sort of.",
            "You know the different F that we actually have.",
            "Oh, actually.",
            "Actually all correlated right?",
            "So there could be suddenly the tweet is conceptually between say multi output as you probably talked about it in section 1.1 and multi task.",
            "I think there is a difference there, at least in the way that the models are set up.",
            "It's certainly true that given the.",
            "Given correlated noise in the multi output case you will get.",
            "You will take into account the other outputs right when you do inference.",
            "This seems to me to be slightly different things.",
            "I don't know.",
            "Maybe other people disagree.",
            "Whether you put the correlation.",
            "Tell the noise.",
            "Yeah, I think that's what I was saying.",
            "Whether they actually believe that we'll have to think about, but that's certainly what I was like.",
            "Yeah, and that's certainly the setup that we talked about.",
            "For example, we talked about multi output.",
            "GP's in the in Williams book.",
            "Think in terms of setting in spatial sense.",
            "If you think about Co creating which is multi output.",
            "Assumption is the underlying processes that correlate.",
            "Yes, yes yes.",
            "Yes, I think.",
            "If you want to talk to Tony O'hagan or somebody in the stats community that do emulation, they characterize this.",
            "Actually is multivariate emulation with separable.",
            "No.",
            "Work on because the structures right?",
            "Problem gives you this separable structure, which is unusual.",
            "Yeah.",
            "Nization or some of these other structures near from your foot bilateral.",
            "OK, so yeah, so certainly in one of the references that pops like I skipped over saying, but certainly the Kantian O'hagan paper from 2007 is about exactly using this in the emulator case, although there's something that worries me about this paper, I actually been meaning to Mail Tony about this, which is that they do it in the noise free case and they do it with the grid design.",
            "Yeah, so so.",
            "Should worry.",
            "OK, OK.",
            "Testing for a very specific reason, but I'll probably talk about this tomorrow morning.",
            "OK, no, it just.",
            "I mean, it's fine to have a noise free case and everything is just that.",
            "If they have the good design or the block design.",
            "In that case then auto create ability to tell you if she shouldn't help, right so.",
            "That's the worrying thing.",
            "Right, sure.",
            "Yes, yes.",
            "Peter question.",
            "Indicated.",
            "Jake that would give you something that's much more general than this packed right assumption is, it is difficult to come up with a very functions that are visible in that context, or it is computationally.",
            "So let me let me just think about what this is actually means.",
            "So we sort of take.",
            "So next Tilda, which is your original X and then we append some.",
            "Some OK, so the one out of an encoding right OK.",
            "So.",
            "Well, actually, if you do this, and certainly so I have to admit that we actually actually sort of stumbled into this area by having exactly this notion of a task descriptor function.",
            "If you use this in the squared exponential kernel.",
            "Of course, if you think about the feature vector there, basically you get the KX bid multipliers.",
            "But here all this is saying this.",
            "If you just had indicators, this is just saying basically that.",
            "It either you're the same as something else, you're different, so in fact, that's exactly the kind of thing that."
        ],
        [
            "Again, you ET al used actually.",
            "Basically it's it's it's diagonal plus rank one.",
            "Yeah, which is restricted.",
            "OK, but you could if you come up in general with some task descriptive features then and in fact what of course is going on the.",
            "The multi task.",
            "The robot case is we're actually coming up with the dynamic parameters and we actually using the linear kernel in dynamic parameter space, right?",
            "That's actually what's going on there.",
            "Multiply those together.",
            "Hello.",
            "You said that was the exploration.",
            "Huh?",
            "Do you support which is the mass?",
            "Built-in already.",
            "You have taken some knowledge from you.",
            "And guess that's actually sort of what we're doing, but we.",
            "That's that decomposition into the wise and the pie, right?",
            "And the pie is got the masses.",
            "It's got the got the mass and the centers of gravity in the moments of inertia.",
            "We sort of doing that, but it comes in in this other way.",
            "Without taking.",
            "K with index.",
            "Crime.",
            "Yes, yes.",
            "Say I generate maybe small number of vectors in poison.",
            "I build up the commands matrix as a sort of combination valid product of those.",
            "Right?",
            "Person you talked about.",
            "Um, I guess that.",
            "What?",
            "Let me get this right.",
            "Say again what you, or your construction is.",
            "That generates.",
            "Damage number of cars.",
            "At every point, right?",
            "I bet you filled out.",
            "Is this aimed at?",
            "Allowing that like this KF this thing to actually vary spatially as well.",
            "OK right right OK?",
            "I think probably the easiest thing for me to say here is let's talk about it over coffee.",
            "Timing is actually not too bad, is it?",
            "So.",
            "Chris again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll try to work this time.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about multitask Gaussian process prediction and this is joint work with a bunch of different people.",
                    "label": 1
                },
                {
                    "sent": "Edwin Bonier coming Chai Steven Clanking that you Vijay Kumar all in Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "Some of the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This probably you will have seen before, but there is some new stuff here about the robotic task that I want to get to.",
                    "label": 0
                },
                {
                    "sent": "So in general, and this certainly is the case, for example in what we was talking about yesterday is that we did.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't really come to new tasks.",
                    "label": 0
                },
                {
                    "sent": "It is so tabular rasa, function and say oh I forgot everything I ever learned about all those other tasks.",
                    "label": 0
                },
                {
                    "sent": "I was I was trying to solve before and how that might relate to my new task.",
                    "label": 0
                },
                {
                    "sent": "It seems kind of a bad idea, so it does make sense to say we share information across tasks.",
                    "label": 1
                },
                {
                    "sent": "And of course typically we probably do that, but we maybe do that in various kinds of informal ways, which are to remember that we found that this method or this kind of thing worked quite well for this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "That seems quite like this other problem.",
                    "label": 0
                },
                {
                    "sent": "We're working on, but we could try and be.",
                    "label": 0
                },
                {
                    "sent": "We could actually try and make use of the data from these other tasks, and that's something I want to talk about here.",
                    "label": 0
                },
                {
                    "sent": "So there are a bunch of examples where they could actually be multiple related problems or problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is potentially related.",
                    "label": 0
                },
                {
                    "sent": "And of course, if we just assume that tasks are related when they're not, that could be detrimental.",
                    "label": 1
                },
                {
                    "sent": "We could also try to really think about task relatedness in a couple of ways.",
                    "label": 1
                },
                {
                    "sent": "One way is that we might just say, OK, I can try and describe my task in certain ways.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can come up with some task descriptors which actually.",
                    "label": 1
                },
                {
                    "sent": "Swimming express what these tasks are and then I can kind of look at the similarity of tasks by considering those task descriptors and.",
                    "label": 0
                },
                {
                    "sent": "Sort of makes certain can make some sense, although.",
                    "label": 0
                },
                {
                    "sent": "There it is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can be difficult to find task descriptors.",
                    "label": 1
                },
                {
                    "sent": "Certainly in some of the compiler problems that we looked at.",
                    "label": 0
                },
                {
                    "sent": "It seems very hard to gain even from compiler experts.",
                    "label": 0
                },
                {
                    "sent": "Seems very hard to extract features that really characterize the these different problems.",
                    "label": 0
                },
                {
                    "sent": "So one thing you could do is to sort of directly have some notion of Inter task similarity and actually trying to learn those things so we actually don't need to go through the notion of task descriptors.",
                    "label": 1
                },
                {
                    "sent": "But we can just actually talk about correlations between tasks and then within a GP framework that so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite an easy thing to do so.",
                    "label": 0
                },
                {
                    "sent": "In outline, I want to talk about basically the model.",
                    "label": 1
                },
                {
                    "sent": "We might make predictions with it, a sort of curious result about cancellation of transfer.",
                    "label": 1
                },
                {
                    "sent": "And I really want to get down to this stuff here when we actually talk about multitask learning in robot inverse dynamics, because in fact, in that case we have some.",
                    "label": 1
                },
                {
                    "sent": "Basically, it turns out that the kind of factorization that we actually would have been assuming in some other work.",
                    "label": 0
                },
                {
                    "sent": "Actually we can show actually applies precisely in the in the right form for the robot inverse dynamics problem, and that means we can hopefully we can actually get some nice results in that problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the multi task setting is that we've got.",
                    "label": 0
                },
                {
                    "sent": "We've got em different tasks and say we've got 10 different inputs.",
                    "label": 0
                },
                {
                    "sent": "We of course don't need to see all this data we don't expect expect normally that will have a kind of complete grid design where we see all the observations on all tasks.",
                    "label": 0
                },
                {
                    "sent": "But conceptually we can have this design.",
                    "label": 0
                },
                {
                    "sent": "So we've got Y one up to Y when on the 1st task and so on up to the task.",
                    "label": 0
                },
                {
                    "sent": "And given our observations WHIO, which is subset of these complete.",
                    "label": 0
                },
                {
                    "sent": "Design then we'd like.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To make predictions.",
                    "label": 0
                },
                {
                    "sent": "One fairly simple model for a multitask problem, so we've got some some functions FL and FM, and we've got locations X&X prime and then we can actually just factorize this into some Inter task similarity matrix, KF and some sort of covariance function KX.",
                    "label": 0
                },
                {
                    "sent": "This is clearly quite a strong assumption.",
                    "label": 0
                },
                {
                    "sent": "But it's actually probably one of the simplest assumptions we could actually make about multitask learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's worth saying that.",
                    "label": 0
                },
                {
                    "sent": "Often people think about multi task learning, perhaps in hierarchical modeling framework we have some say some theater and we maybe draw parameters for regression, say from some distribution over theaters.",
                    "label": 0
                },
                {
                    "sent": "And if that if that.",
                    "label": 0
                },
                {
                    "sent": "Distribution of a theater or something like a mixture distribution we might actually get a bunch of kind of related tasks where we drawing from the same mixture component and then a bunch of different tasks.",
                    "label": 0
                },
                {
                    "sent": "Of a different task cluster, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can imagine that kind of stuff.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a sense, what the?",
                    "label": 0
                },
                {
                    "sent": "The kind of frame what we do in the GP is kind of integrating out this data and saying we really just going to induce correlations between all these different apps.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Always different underlying function.",
                    "label": 0
                },
                {
                    "sent": "And of course, when we do that, observations on one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task will effect predict.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another it's of course in this kind of thing it's quite easy to talk about multitask clustering.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we believe that there are groups of tasks that should be clustered together, but they have no relation to other tasks, then of course that KF that Inter task similarity matrix will just have a block diagonal structure representing those cluster stuff.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see we can do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really pops is a picture and Maps.",
                    "label": 0
                },
                {
                    "sent": "Is it happening?",
                    "label": 0
                },
                {
                    "sent": "Little dot OK, let's just think about this in terms of 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So if we have X is so some some individual problem might be if we drew from our Gaussian process would have this kind of form.",
                    "label": 0
                },
                {
                    "sent": "The key thing is what we've got as we actually as we consider different tasks.",
                    "label": 0
                },
                {
                    "sent": "The whole point is not independent draws from some Gaussian process with covariance KX, but there's somehow correlated.",
                    "label": 0
                },
                {
                    "sent": "So we actually get a bunch of correlated tasks that look like this.",
                    "label": 0
                },
                {
                    "sent": "That's what sort of.",
                    "label": 0
                },
                {
                    "sent": "That's a useful picture.",
                    "label": 0
                },
                {
                    "sent": "That's a good question in the.",
                    "label": 0
                },
                {
                    "sent": "In the robot case, we certainly can actually think about having a continuous space for those tasks to live in.",
                    "label": 0
                },
                {
                    "sent": "Perhaps not, but we don't have to.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, you probably can imagine how I actually generated this figure.",
                    "label": 0
                },
                {
                    "sent": "In that case, it was convenient to do it in a continuous.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so making predictions?",
                    "label": 0
                },
                {
                    "sent": "If we've got, we've got complete data, then in fact, there's a Chronicle product structure of the covariance matrix between at all.",
                    "label": 0
                },
                {
                    "sent": "X is all tasks, and we can exploit that.",
                    "label": 0
                },
                {
                    "sent": "But in general, of course, if we don't have complete data, then this complete grid design, then we won't be able to have that structure, but it's just a big GP.",
                    "label": 0
                },
                {
                    "sent": "We can just condition on what we see and make predictions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide.",
                    "label": 0
                },
                {
                    "sent": "Um, it's also true that we can.",
                    "label": 0
                },
                {
                    "sent": "Then the hyperparameters in the usual way if we're doing M L2, we can maximize the marginal likelihood respect these parameters.",
                    "label": 0
                },
                {
                    "sent": "The only slight trick is that if we want to parameterise KF, we have to be a little bit careful about that when we do this, we typically use a cheskey decomposition form for that, but there actually is, for example, another way of doing this with them, where you actually just get guaranteed positive symmetric definite updates.",
                    "label": 0
                },
                {
                    "sent": "Just buy it.",
                    "label": 0
                },
                {
                    "sent": "It just falls out of the game so that all works.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicely.",
                    "label": 0
                },
                {
                    "sent": "One kind of curious thing about this model.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "That if we have no noise and we have grid observations, then something rather weird happens, which is that say I consider this task F2 and I'm actually got observations at all these black dots.",
                    "label": 0
                },
                {
                    "sent": "And so I've got this kind of grid.",
                    "label": 0
                },
                {
                    "sent": "I've got the tasks and they've got the X is and then I say OK, I want to make this prediction for F2 on X star.",
                    "label": 0
                },
                {
                    "sent": "If I workout actually what those?",
                    "label": 0
                },
                {
                    "sent": "Predictions actually depend on it actually only depends on the observations.",
                    "label": 0
                },
                {
                    "sent": "I got F2, so even though I've got this big joint Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Are all these axes and I actually know about what's going on these other places?",
                    "label": 0
                },
                {
                    "sent": "Then in fact those.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things actually aren't informative.",
                    "label": 0
                },
                {
                    "sent": "About my predictions for F2.",
                    "label": 0
                },
                {
                    "sent": "There are of course 2 caveats we need this grid design an we need to have no observation noise.",
                    "label": 1
                },
                {
                    "sent": "If we if neither of those things happen.",
                    "label": 0
                },
                {
                    "sent": "And this isn't true, and knowing about the processes elsewhere will actually help.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I didn't have an observation here, but I did have one there, then that would also violate things and those other observations would be helpful.",
                    "label": 0
                },
                {
                    "sent": "So it turns out actually thanks to Dan for telling me about this.",
                    "label": 0
                },
                {
                    "sent": "Actually talked about this.",
                    "label": 0
                },
                {
                    "sent": "Asked you Acms and then said well, there's something called Auto Krige ability in geostatistics, and what's that about?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that actually, this is this result, so we worked it out and then it turned out it was known that before.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "I, I'm not sure I it falls out very easily from the comic Chronicle.",
                    "label": 0
                },
                {
                    "sent": "Product forms of the of the.",
                    "label": 0
                },
                {
                    "sent": "There probably is, and I've thought about that before.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if I have stored the answer.",
                    "label": 0
                },
                {
                    "sent": "Kernelization.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Should be careful with the user, usually coined by French people these terms and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe they work better in French.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "OK, so clearly there's a lot of work on multi task learning.",
                    "label": 0
                },
                {
                    "sent": "Certainly in machine learning in the mid 90s Sebastian true and Rich Caruana talked about this in terms of Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "Some work done, certainly well.",
                    "label": 0
                },
                {
                    "sent": "The other stuff I know about this is by Tom Minka, but there's certainly other people in this audience who also use this.",
                    "label": 0
                },
                {
                    "sent": "If we, if we assume you've got multiple tasks, they shame this share the same hyperparameters, but they're not really authorized correlated, then certainly learning those those hyperparameters together.",
                    "label": 0
                },
                {
                    "sent": "That makes sense, and people have done that.",
                    "label": 0
                },
                {
                    "sent": "This really is that, of course, exactly what the cokriging setup in your statistics is about.",
                    "label": 0
                },
                {
                    "sent": "And in fact they do also use this kind of covariance function, and they also use like more complicated ones.",
                    "label": 0
                },
                {
                    "sent": "For example sums of this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "I guess let me.",
                    "label": 0
                },
                {
                    "sent": "That's one thing is worth saying about focusing on it is for example the work of UIT and others on the three semiparametric latent factor model so that that is actually So what they actually have is a bunch of GPS which are linearly mixed to produce the observed functions and will see.",
                    "label": 1
                },
                {
                    "sent": "Basically this is very similar to what we do.",
                    "label": 0
                },
                {
                    "sent": "The main difference and actually works exactly like that for the robot arm problem.",
                    "label": 0
                },
                {
                    "sent": "One of the main difference is that they actually considered.",
                    "label": 0
                },
                {
                    "sent": "That each of these latent these P latent process had its own covariance function and kind of made things a bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "If you actually assume they all have the same covariance function, and basically you actually get this structure like we have here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me skip over so we had a NIPS paper last year where we talked about some experiments and compiler.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance prediction and also on exam score prediction that the latter one is the data set that backer in his skis had used actually.",
                    "label": 0
                },
                {
                    "sent": "In in, in previous works we use that for compare.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But and we could show that this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was affective there, but I really want to talk about was the multitask learning and robot inverse dynamics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, here's the setup we have.",
                    "label": 0
                },
                {
                    "sent": "Some more, but we actually be considering a more complex one, but let's just for now just consider this simple picture with two 2 links and we can parameterise that by these queues these joint angles.",
                    "label": 0
                },
                {
                    "sent": "What we're interested in really doing, and so saying, we've decided that we want to take this arm and follow a trajectory, and so we have specified essentially positions last season.",
                    "label": 0
                },
                {
                    "sent": "Accelerations along this trajectory and what we need to do is to figure out what talks to apply at these joints so as to achieve this trajectory.",
                    "label": 0
                },
                {
                    "sent": "That's the that's the inverse dynamics problem.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what we're going to be talking.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We would like to know we can collect some data is but we drive this robot arm we we find what talks when we apply talks, how those affect the queues and so on.",
                    "label": 0
                },
                {
                    "sent": "So we can actually get some data about this about this function and you might think, well, yeah you could do it in a data driven way, but should you really be doing that, there's a lot of physics in this right?",
                    "label": 0
                },
                {
                    "sent": "You know, there's a we can write down Newton's laws.",
                    "label": 0
                },
                {
                    "sent": "We can do all that.",
                    "label": 0
                },
                {
                    "sent": "Do we really need to actually sort of Dua?",
                    "label": 0
                },
                {
                    "sent": "Kind of black box machine learning approach to this, and that's perhaps certainly true.",
                    "label": 0
                },
                {
                    "sent": "It certainly makes a lot of sense to think about the dynamics problem, and so one can derive various.",
                    "label": 0
                },
                {
                    "sent": "Essentially, yeah, Newton's equations for this system, and certainly this isn't our work.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of stuff in robotics, and about about these equations and sort of analyzing them.",
                    "label": 0
                },
                {
                    "sent": "Come, however, although we can actually write down the physics for this there there there certainly there are things we have to do, like parameter estimation for various parameters in that we also have to worry about some things like friction and contact forces, joint elasticity and so on.",
                    "label": 0
                },
                {
                    "sent": "Other things that certainly for real sort of compliant lightweight robots like you.",
                    "label": 0
                },
                {
                    "sent": "If you have some sort of lightweight human humanoid arm style thing, which certainly my colleague said to Vijay Kumar is interested in.",
                    "label": 0
                },
                {
                    "sent": "Then you might worry about some of these problems that arise when actually trying to model this in a physics way so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our strategy.",
                    "label": 0
                },
                {
                    "sent": "And this is something else, because we have to worry about, which is that typically.",
                    "label": 0
                },
                {
                    "sent": "The things we actually probably care about most is actually being able to control this under different load, so I can pick up various different loads and actually then control it in that you follow trajectories and the question is really important.",
                    "label": 0
                },
                {
                    "sent": "Most interesting thing that this gives rise to multitask problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, given these different loads that we can have on the end effector and we still want to control these different things, how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we actually learn about that?",
                    "label": 0
                },
                {
                    "sent": "So that's the thing we want to do, and the bad news is of course the talk functions change as a function of the load on the end effector.",
                    "label": 0
                },
                {
                    "sent": "And it's also true that.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should explore different spaces with different loads.",
                    "label": 0
                },
                {
                    "sent": "Certainly if some loads are heavier than others then.",
                    "label": 0
                },
                {
                    "sent": "I would probably not move them so fast, so it might be more difficult to move them fast.",
                    "label": 0
                },
                {
                    "sent": "So there are perhaps if we think of the X SpaceX is QQQ double deck.",
                    "label": 0
                },
                {
                    "sent": "It's the positions, velocities and acceleration's.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually.",
                    "label": 0
                },
                {
                    "sent": "We might explore different.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That space so perhaps the saving grace.",
                    "label": 0
                },
                {
                    "sent": "The thing that someone will make this work is really that if we consider that our IX of that Ally X is that talk for the ice joint.",
                    "label": 0
                },
                {
                    "sent": "And this is the NTH context.",
                    "label": 0
                },
                {
                    "sent": "So here think of context is different loads that I've got.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out the.",
                    "label": 0
                },
                {
                    "sent": "Basically this is kind of linear in some.",
                    "label": 0
                },
                {
                    "sent": "In this these pies is parameters that cycle dynamic parameters which actually physically characterize, so the mass the moment of inertia of the central gravity in the moment of inertia of the of the load on the end effector.",
                    "label": 0
                },
                {
                    "sent": "And we can also, perhaps including here some coefficients of friction and so on.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the key thing is that essentially because of the way that the physics works, one thing we can take away from the physics.",
                    "label": 0
                },
                {
                    "sent": "Perhaps abstracting away some of the details of the.",
                    "label": 0
                },
                {
                    "sent": "Equations is basically the way that these different parameters enter these equations, so this is basically there are to get the talk for the I TH context.",
                    "label": 0
                },
                {
                    "sent": "There are basically sort of 11 functions that we should know about.",
                    "label": 0
                },
                {
                    "sent": "These wise whyyy is a vector of 11 functions corresponding to the I TH joint and what we do is drop them with Pi M. So I think that you can then see that.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we have different.",
                    "label": 0
                },
                {
                    "sent": "Iams for different masses, then we can actually exploit this.",
                    "label": 0
                },
                {
                    "sent": "So what's going on?",
                    "label": 0
                },
                {
                    "sent": "We can just just think a little bit about some reparameterization.",
                    "label": 0
                },
                {
                    "sent": "There's actually nothing to stop us adding in a non singular matrix.",
                    "label": 0
                },
                {
                    "sent": "In her reading, we can kind of convert these things, so think about.",
                    "label": 0
                },
                {
                    "sent": "Just call this some functions, Ed and some parameters row.",
                    "label": 0
                },
                {
                    "sent": "These are basically the wise and apply.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then let's think about having this is same talk, so it's the same joint.",
                    "label": 0
                },
                {
                    "sent": "On the elf context and on the M context, and really, let's put Gaussian process priors over the Zeds.",
                    "label": 0
                },
                {
                    "sent": "OK, we just got the talk is just a linear combination of these is weighted by this thing bro.",
                    "label": 0
                },
                {
                    "sent": "Basically what will happen is that if we go deep fryers on the.",
                    "label": 0
                },
                {
                    "sent": "On the zed functions and we basically we just got different weights.",
                    "label": 0
                },
                {
                    "sent": "Just like this, and really what's happening is that this the correlation between the MLF task on the end task depends on the dot product between the row parameters for the NTH task and the Alpha task.",
                    "label": 0
                },
                {
                    "sent": "So think about it like this.",
                    "label": 0
                },
                {
                    "sent": "We've got these latent functions Ed.",
                    "label": 0
                },
                {
                    "sent": "And what we're doing is taking some linear combination of those with this row vector to get Tau I and basically different.",
                    "label": 0
                },
                {
                    "sent": "Tasks different EMS give rise to different linear combination.",
                    "label": 0
                },
                {
                    "sent": "So thinking about the semiparametric latent factor model, this is really exactly the set up right?",
                    "label": 0
                },
                {
                    "sent": "So really the point is what we've got we're assuming.",
                    "label": 0
                },
                {
                    "sent": "The simplicity that all these ads have the same covariance function and also essentially what we've done is discover a problem where the set up was actually the solution, right?",
                    "label": 0
                },
                {
                    "sent": "So given this, what we really of course want to do is to try and exploit.",
                    "label": 0
                },
                {
                    "sent": "Cross task learning or sharing of information across tasks in order to improve.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Control.",
                    "label": 0
                },
                {
                    "sent": "So is that I think of this as K. Anyway, the covariance function, what we actually using is just.",
                    "label": 0
                },
                {
                    "sent": "Perhaps something is generally fairly.",
                    "label": 0
                },
                {
                    "sent": "Simple bias, linear term, square, exponential term, but there's actually a special bit of physics that comes in here, which is that the Coulomb friction term depends on the sign.",
                    "label": 0
                },
                {
                    "sent": "Signal of Q DOT is basically the way that they could on friction term works, so it's definitely very important in terms of predicted performance to actually include that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK. We want to do some experiments to try and verify that this is actually going to work.",
                    "label": 0
                },
                {
                    "sent": "Here's a picture of the Puma 560 robot arm, which is a 6 degree of freedom arm, and we're doing all this in the simulation.",
                    "label": 0
                },
                {
                    "sent": "I have to admit.",
                    "label": 0
                },
                {
                    "sent": "We're gonna consider.",
                    "label": 0
                },
                {
                    "sent": "A bunch of different loads, so 15 different loads.",
                    "label": 0
                },
                {
                    "sent": "And they have various shapes and sizes and some different masses were also to explore this idea about that we might be interested in.",
                    "label": 0
                },
                {
                    "sent": "Going in different parts of different parts of X space, we actually also going to drawing to consider.",
                    "label": 0
                },
                {
                    "sent": "We're going to figure of eight trajectories that we move load around.",
                    "label": 0
                },
                {
                    "sent": "We consider four different directories and also at four different speeds, so we can do a number of different trajectory and speed we consider.",
                    "label": 0
                },
                {
                    "sent": "And these are the sort of different parts of X space, so we're actually going to be particularly interested in the case where we maybe pick up one load and do one kind of trajectory with it.",
                    "label": 0
                },
                {
                    "sent": "Then we actually want to maybe follow another kind of trajectory with that same load.",
                    "label": 0
                },
                {
                    "sent": "We actually haven't seen any data in that context before.",
                    "label": 0
                },
                {
                    "sent": "We actually want to sort of generalize to that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so where we actually done the experiments is that we're actually going to use one reference directory, which is actually 14 unique trajectory's, one for each of the other context, one for each context.",
                    "label": 0
                },
                {
                    "sent": "And then basically we consider two kinds of test data.",
                    "label": 0
                },
                {
                    "sent": "Either interpolation were essentially, you know, we've taken this load and we've seen it in this context, and following its particular trajectory, we're going to try and then test on this trajectory, or we consider extrapolation, where we go to a new trajectory that we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "We've seen data on other trajectories.",
                    "label": 0
                },
                {
                    "sent": "I'm going to seeing everything on the reference trajectory, and we're going to try.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crossover.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about some experiments with three different methods.",
                    "label": 0
                },
                {
                    "sent": "The independent Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So basically, here this is exactly what we were talking about before.",
                    "label": 0
                },
                {
                    "sent": "We basically only take data from each context.",
                    "label": 0
                },
                {
                    "sent": "But we assume the save the hyperparameters that I'd across logs, and actually we've found the experiments.",
                    "label": 0
                },
                {
                    "sent": "This works better than just doing the single basically in the single thing where we learn hyper parameters for each load separately.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Pooled Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Well, the other the other way to deal with multi task learning is actually, so it's all one problem.",
                    "label": 0
                },
                {
                    "sent": "And actually let's just consider that altogether.",
                    "label": 0
                },
                {
                    "sent": "And that actually sometimes for things that people have talked about as multitask learning in the literature and actually doesn't turn to be such a bad thing to do.",
                    "label": 0
                },
                {
                    "sent": "However, it's certainly not going to be highly effective in this context, and also multi task Gaussian process where we're doing what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "Here we have this KF this thing, which is actually the Inter task similarity, and we want to learn that matrix.",
                    "label": 0
                },
                {
                    "sent": "Which is fundamentally about those.",
                    "label": 0
                },
                {
                    "sent": "Those premises, those dynamic parameters.",
                    "label": 0
                },
                {
                    "sent": "The DOT product those dynamic parameters.",
                    "label": 0
                },
                {
                    "sent": "There's also a little bit more detail, which is in fact we're going to control the rank of KF, and we're actually going to do that with BICS.",
                    "label": 0
                },
                {
                    "sent": "Like like an indicator function for that context.",
                    "label": 0
                },
                {
                    "sent": "To say I mean it.",
                    "label": 0
                },
                {
                    "sent": "Having waiter.",
                    "label": 0
                },
                {
                    "sent": "So do you wanna do you wanna draw what?",
                    "label": 0
                },
                {
                    "sent": "What I shall we so we talk about it afterwards, OK?",
                    "label": 0
                },
                {
                    "sent": "That perhaps relates to this notion of task descriptors, but ask me that at the end.",
                    "label": 0
                },
                {
                    "sent": "Think about this.",
                    "label": 0
                },
                {
                    "sent": "Basically, becausw.",
                    "label": 0
                },
                {
                    "sent": "You know you're giving yourself more free parameters in KF, right?",
                    "label": 0
                },
                {
                    "sent": "As you increase the rank of it, and you're not gonna, actually.",
                    "label": 0
                },
                {
                    "sent": "You know there there is no.",
                    "label": 0
                },
                {
                    "sent": "Peak you gonna find it, yet we are optimizing.",
                    "label": 0
                },
                {
                    "sent": "We're not integrating.",
                    "label": 0
                },
                {
                    "sent": "We're being incompletely Bayesian, definitely.",
                    "label": 0
                },
                {
                    "sent": "As is very common.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definitely, that's true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this might be a little bit of complicated graph to look at.",
                    "label": 0
                },
                {
                    "sent": "First of all, there's two sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "This is interpolation and this is extrapolation.",
                    "label": 0
                },
                {
                    "sent": "We've got six joints.",
                    "label": 0
                },
                {
                    "sent": "In general, this is joint one joint, 4 joint, six.",
                    "label": 0
                },
                {
                    "sent": "There are three curves which actually I can't even see the difference at this here.",
                    "label": 0
                },
                {
                    "sent": "OK IGP which I think is in red, although hard to see PGP the pooled which is in blue an MVP which is the.",
                    "label": 0
                },
                {
                    "sent": "The multiple GP which is in black and I'll sort of talk you through the results.",
                    "label": 0
                },
                {
                    "sent": "So basically what OK there's one other thing.",
                    "label": 0
                },
                {
                    "sent": "There's one other thing you should know about the plotting, which is that if a method is actually doing very poorly, we just stick it at the top of the of the Y scale here.",
                    "label": 0
                },
                {
                    "sent": "So basically these other things could be way off the scale, but we just conventionally putting them here.",
                    "label": 0
                },
                {
                    "sent": "Basically what we can see.",
                    "label": 0
                },
                {
                    "sent": "Is for example on this method that the pool Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Actually this is the interpolation pool.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process is doing very poorly, and in fact the independent Gaussian process and the multiple Gaussian processes are doing about the same here.",
                    "label": 0
                },
                {
                    "sent": "Actually, for joint one, the pool Gaussian process isn't doing so badly.",
                    "label": 0
                },
                {
                    "sent": "This is on the interpolation.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the more interesting cases, the extrapolation where here the independent Gaussian process is doing kind of badly, and in fact the pooled and multiple GP are doing or other similar.",
                    "label": 0
                },
                {
                    "sent": "This is the normalized mean squared error.",
                    "label": 0
                },
                {
                    "sent": "And notice these facts about here.",
                    "label": 0
                },
                {
                    "sent": "This is the amount of data that we have.",
                    "label": 0
                },
                {
                    "sent": "So basically in this case the MVP is doing better as well as the pool Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "This is a joint one where it actually is pretty sure it's one that's right.",
                    "label": 0
                },
                {
                    "sent": "It's near down near the base of the robot, so in many ways perhaps that the tasks maybe all data can be pulled because it's all.",
                    "label": 0
                },
                {
                    "sent": "Mirror in space, whereas if we consider say join 406 which are further out along the the arm.",
                    "label": 0
                },
                {
                    "sent": "Here we see for example and that the multi task.",
                    "label": 0
                },
                {
                    "sent": "GP is actually performing better than the than the independent GP and the pull Jeep pull Gaussian process is off the scale here.",
                    "label": 0
                },
                {
                    "sent": "So so basically we can demonstrate and these are these are catch.",
                    "label": 0
                },
                {
                    "sent": "These are actually averages over the 15 different context we've got.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause what we're considering is for given joint.",
                    "label": 0
                },
                {
                    "sent": "We're extrapolating the other contexts and we're actually looking at these averages over these contexts.",
                    "label": 0
                },
                {
                    "sent": "So basically, at least in this kind of setup, we can demonstrate that this is actually working reason.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Effectively.",
                    "label": 0
                },
                {
                    "sent": "Maybe just to conclude.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We've talked about this factorization between KX&KF and that's actually we've talked about that before and where KF encodes task similarity.",
                    "label": 0
                },
                {
                    "sent": "One while the nice thing about the multi task multi context inverse robot inverse dynamics problem is in fact this actually fits this structure very well, so it's not in other work.",
                    "label": 0
                },
                {
                    "sent": "Typically this is an assumption about this decomposition.",
                    "label": 0
                },
                {
                    "sent": "Here we can actually show that this decomposition is actually appropriate for this kind of this kind of data.",
                    "label": 0
                },
                {
                    "sent": "I guess it's perhaps fairly obvious to say, is that clearly this kind of internally for general problems?",
                    "label": 0
                },
                {
                    "sent": "This kind of decomposition between KX&KF is a fairly strong assumption, and it's actually probably more interesting to ask.",
                    "label": 0
                },
                {
                    "sent": "About what kinds of more general structures could allow for the full multi task learning when we don't have such simple.",
                    "label": 0
                },
                {
                    "sent": "Factorization assumption, so you stop.",
                    "label": 0
                },
                {
                    "sent": "Did you want to start off by asking you?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we were sort of following up some of your work.",
                    "label": 0
                },
                {
                    "sent": "Coffee break.",
                    "label": 0
                },
                {
                    "sent": "But one thing that I thought hitting with.",
                    "label": 0
                },
                {
                    "sent": "It's it's this multi output GP at the end of the day I mean.",
                    "label": 0
                },
                {
                    "sent": "In what way does multitask?",
                    "label": 0
                },
                {
                    "sent": "Task rather than multi output.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I guess the.",
                    "label": 0
                },
                {
                    "sent": "Point is that.",
                    "label": 0
                },
                {
                    "sent": "And certainly in this standard multi output setting right?",
                    "label": 0
                },
                {
                    "sent": "You might consider the case that essentially you have a bunch of functions and they have some correlated noise right between these different things.",
                    "label": 0
                },
                {
                    "sent": "That's a standard multi output setting, whereas in certainly in this case the multitask thing you actually consider there are sort of.",
                    "label": 0
                },
                {
                    "sent": "You know the different F that we actually have.",
                    "label": 0
                },
                {
                    "sent": "Oh, actually.",
                    "label": 0
                },
                {
                    "sent": "Actually all correlated right?",
                    "label": 0
                },
                {
                    "sent": "So there could be suddenly the tweet is conceptually between say multi output as you probably talked about it in section 1.1 and multi task.",
                    "label": 0
                },
                {
                    "sent": "I think there is a difference there, at least in the way that the models are set up.",
                    "label": 0
                },
                {
                    "sent": "It's certainly true that given the.",
                    "label": 0
                },
                {
                    "sent": "Given correlated noise in the multi output case you will get.",
                    "label": 0
                },
                {
                    "sent": "You will take into account the other outputs right when you do inference.",
                    "label": 0
                },
                {
                    "sent": "This seems to me to be slightly different things.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe other people disagree.",
                    "label": 0
                },
                {
                    "sent": "Whether you put the correlation.",
                    "label": 0
                },
                {
                    "sent": "Tell the noise.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's what I was saying.",
                    "label": 0
                },
                {
                    "sent": "Whether they actually believe that we'll have to think about, but that's certainly what I was like.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and that's certainly the setup that we talked about.",
                    "label": 0
                },
                {
                    "sent": "For example, we talked about multi output.",
                    "label": 0
                },
                {
                    "sent": "GP's in the in Williams book.",
                    "label": 0
                },
                {
                    "sent": "Think in terms of setting in spatial sense.",
                    "label": 0
                },
                {
                    "sent": "If you think about Co creating which is multi output.",
                    "label": 0
                },
                {
                    "sent": "Assumption is the underlying processes that correlate.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think.",
                    "label": 0
                },
                {
                    "sent": "If you want to talk to Tony O'hagan or somebody in the stats community that do emulation, they characterize this.",
                    "label": 0
                },
                {
                    "sent": "Actually is multivariate emulation with separable.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Work on because the structures right?",
                    "label": 0
                },
                {
                    "sent": "Problem gives you this separable structure, which is unusual.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Nization or some of these other structures near from your foot bilateral.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, so certainly in one of the references that pops like I skipped over saying, but certainly the Kantian O'hagan paper from 2007 is about exactly using this in the emulator case, although there's something that worries me about this paper, I actually been meaning to Mail Tony about this, which is that they do it in the noise free case and they do it with the grid design.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "Should worry.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "Testing for a very specific reason, but I'll probably talk about this tomorrow morning.",
                    "label": 0
                },
                {
                    "sent": "OK, no, it just.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's fine to have a noise free case and everything is just that.",
                    "label": 0
                },
                {
                    "sent": "If they have the good design or the block design.",
                    "label": 0
                },
                {
                    "sent": "In that case then auto create ability to tell you if she shouldn't help, right so.",
                    "label": 0
                },
                {
                    "sent": "That's the worrying thing.",
                    "label": 0
                },
                {
                    "sent": "Right, sure.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Peter question.",
                    "label": 0
                },
                {
                    "sent": "Indicated.",
                    "label": 0
                },
                {
                    "sent": "Jake that would give you something that's much more general than this packed right assumption is, it is difficult to come up with a very functions that are visible in that context, or it is computationally.",
                    "label": 0
                },
                {
                    "sent": "So let me let me just think about what this is actually means.",
                    "label": 0
                },
                {
                    "sent": "So we sort of take.",
                    "label": 0
                },
                {
                    "sent": "So next Tilda, which is your original X and then we append some.",
                    "label": 0
                },
                {
                    "sent": "Some OK, so the one out of an encoding right OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, if you do this, and certainly so I have to admit that we actually actually sort of stumbled into this area by having exactly this notion of a task descriptor function.",
                    "label": 0
                },
                {
                    "sent": "If you use this in the squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you think about the feature vector there, basically you get the KX bid multipliers.",
                    "label": 0
                },
                {
                    "sent": "But here all this is saying this.",
                    "label": 0
                },
                {
                    "sent": "If you just had indicators, this is just saying basically that.",
                    "label": 0
                },
                {
                    "sent": "It either you're the same as something else, you're different, so in fact, that's exactly the kind of thing that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, you ET al used actually.",
                    "label": 0
                },
                {
                    "sent": "Basically it's it's it's diagonal plus rank one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, which is restricted.",
                    "label": 0
                },
                {
                    "sent": "OK, but you could if you come up in general with some task descriptive features then and in fact what of course is going on the.",
                    "label": 0
                },
                {
                    "sent": "The multi task.",
                    "label": 0
                },
                {
                    "sent": "The robot case is we're actually coming up with the dynamic parameters and we actually using the linear kernel in dynamic parameter space, right?",
                    "label": 0
                },
                {
                    "sent": "That's actually what's going on there.",
                    "label": 0
                },
                {
                    "sent": "Multiply those together.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "You said that was the exploration.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "Do you support which is the mass?",
                    "label": 0
                },
                {
                    "sent": "Built-in already.",
                    "label": 0
                },
                {
                    "sent": "You have taken some knowledge from you.",
                    "label": 0
                },
                {
                    "sent": "And guess that's actually sort of what we're doing, but we.",
                    "label": 0
                },
                {
                    "sent": "That's that decomposition into the wise and the pie, right?",
                    "label": 0
                },
                {
                    "sent": "And the pie is got the masses.",
                    "label": 0
                },
                {
                    "sent": "It's got the got the mass and the centers of gravity in the moments of inertia.",
                    "label": 0
                },
                {
                    "sent": "We sort of doing that, but it comes in in this other way.",
                    "label": 0
                },
                {
                    "sent": "Without taking.",
                    "label": 0
                },
                {
                    "sent": "K with index.",
                    "label": 0
                },
                {
                    "sent": "Crime.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Say I generate maybe small number of vectors in poison.",
                    "label": 0
                },
                {
                    "sent": "I build up the commands matrix as a sort of combination valid product of those.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Person you talked about.",
                    "label": 0
                },
                {
                    "sent": "Um, I guess that.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Let me get this right.",
                    "label": 0
                },
                {
                    "sent": "Say again what you, or your construction is.",
                    "label": 0
                },
                {
                    "sent": "That generates.",
                    "label": 0
                },
                {
                    "sent": "Damage number of cars.",
                    "label": 0
                },
                {
                    "sent": "At every point, right?",
                    "label": 0
                },
                {
                    "sent": "I bet you filled out.",
                    "label": 0
                },
                {
                    "sent": "Is this aimed at?",
                    "label": 0
                },
                {
                    "sent": "Allowing that like this KF this thing to actually vary spatially as well.",
                    "label": 0
                },
                {
                    "sent": "OK right right OK?",
                    "label": 0
                },
                {
                    "sent": "I think probably the easiest thing for me to say here is let's talk about it over coffee.",
                    "label": 0
                },
                {
                    "sent": "Timing is actually not too bad, is it?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Chris again.",
                    "label": 0
                }
            ]
        }
    }
}