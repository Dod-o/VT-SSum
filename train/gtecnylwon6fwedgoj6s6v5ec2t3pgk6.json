{
    "id": "gtecnylwon6fwedgoj6s6v5ec2t3pgk6",
    "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations",
    "info": {
        "author": [
            "Francesco Orabona, Toyota Technological Institute at Chicago"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_orabona_learning/",
    "segmentation": [
        [
            "Good morning everyone.",
            "This is a brief outline of the talk.",
            "I will first interest."
        ],
        [
            "Miss you very briefly, what is the problem of online learning?",
            "They would then I will tell you something about our results for minimax analysis.",
            "For unconstrained games, and then I will tell you something about a couple of applications of these results online."
        ],
        [
            "Learning is it's a framework for sequential prediction.",
            "Basically, each time step the learner we receive question and he has to answer to this question just after each prediction the learner will see what is the correct answer and it will suffer loss that basically come measure how different is the predicted answer from the correct ones and an important characteristic of this scenario is that the answer.",
            "And and the and the questions are generated by an adversary, so we don't assume anything about on on how this data is generated.",
            "This doesn't work."
        ],
        [
            "The learner.",
            "Basically, we could say that the learner is to minimize the cumulative loss over the over the samples.",
            "But we say that there is no assumption about the data generated, so this loss can be arbitrary high, so instead."
        ],
        [
            "What we want to do is to minimize the difference between the cumulative loss of the learner and the cumulative loss of a fixed competitor in a certain class.",
            "And this day."
        ],
        [
            "Strength is called the regret of the algorithm.",
            "What we will."
        ],
        [
            "Want is this regret to grow sublinearly in time?",
            "So something like a sqrt D for example."
        ],
        [
            "So in our case, we will focus on online learning with the linear losses in Hilbert spaces.",
            "So we consider any other spaces, for example, reproducing kernel Hilbert spaces or the spaces of matrices and its associated inner product, and we will denote the associate norm just with the symbol of the norm, and basically each round the learner will pick vector in the Hilbert space.",
            "The adversary will choose vector G of T and the learner will gain dinner product between WT and G of T and the regret will be just the sum of the inner product between Jyoti and the difference between WT and fixed competitor you."
        ],
        [
            "So our results are the following.",
            "We will give general conditions under which it is possible to calculate in a closed formula the optimal strategy for the learner and the optimal optimal is not just up to a constant, but it's the true minimax strategy and we will use the results to recover and extends primary results on minimax online learning and we will close the gap between upper and lower bound for unconstrained online learning.",
            "We have also other results in the paper."
        ],
        [
            "But in this talk I will focus just."
        ],
        [
            "These first three points.",
            "So let's move to the second part."
        ],
        [
            "So this is basically the definition of the regret of the algorithm and we just take the supremum over of the regret over the choice of the computer in this silver space.",
            "Now if we want to look at Amy Maxwell online learning what we do."
        ],
        [
            "Who is that?",
            "We basically take the minimum and the maximum over all the rounds of the choice of the learner and the choice of the other side.",
            "No, this quantity."
        ],
        [
            "It's called the value of the game.",
            "And we."
        ],
        [
            "You can just equivalently rewrite it in this way, where 30 is just the sum of all the vector G of T chosen by the adversary."
        ],
        [
            "Now, given that we want to consider the case of unconstrained online learning, instead of taking this supremium over set W in the Hilbert space.",
            "We will change."
        ],
        [
            "This formulation in this way, so we will consider just the function F applied over this norm of deity that is the sum of this vector G of T. And we will call it this.",
            "The benchman function and we just require it to be an increasing the convex function.",
            "And the reason to do it?"
        ],
        [
            "Is that with this very simple theorem that appeared also in the literature in different forms, we can prove that the regret of the algorithm will be always be the Frankel dual of this benchmark function evaluated over the norm of the competitor plus the value of the game.",
            "So basically, every time you choose this function F, you're implicitly choosing what is in in which way you want to measure the complexity of your competitor in regret bound."
        ],
        [
            "Another important definition is the one of the conditional value of the game that is defined through that recursive formula, and basically it's summarizing what happened until time T and tells what is the potential of the algorithm in the next rounds."
        ],
        [
            "And given the definition we have that the conditional value of the game at the end at time Capital T is exactly exactly equal to the benchmark function.",
            "And the potential the conditional value of the game at the beginning at Time Zero is exactly equal to the value of the game, and so we can rewrite the previous theorem just as the regret is less than the fencl dual plus the conditional value of the game at times 0."
        ],
        [
            "And we also have that the minimax optimal strategy will will be given by this argument.",
            "Here this expression here.",
            "So basically, once we have chosen, what is our function F our benchmark function and we're able to calculate in some way the conditional value of the game.",
            "Then we're done cause we will know automatically what will be the minimax optimal strategy at each time step.",
            "And we will also know what is the regret.",
            "Through that theorem.",
            "So the only thing that we need is a way to calculate the conditional value of the game for any benchmark function."
        ],
        [
            "Now it turns out that this is actually.",
            "A difficult problem in general.",
            "But maybe there are conditions under which this is an easy problem.",
            "So for consider for example.",
            "A vector GT that is orthogonal to the sum of the previous place.",
            "So in the first round."
        ],
        [
            "Will play just a vector the 2nd."
        ],
        [
            "Round it will play a vector that is orthogonal to this one, then something that."
        ],
        [
            "Is orthogonal to the sum of the two."
        ],
        [
            "And so on."
        ],
        [
            "So in this way I know that for example at time 5.",
            "I don't know what will be the play of the adversary.",
            "But I know that the norm of 30, that is the sum of the place as only two values.",
            "And this will simplify a lot of the analysis.",
            "Fuck."
        ],
        [
            "We can show that.",
            "Every time the benchmark function F satisfied that condition on the 1st and the 2nd derivative."
        ],
        [
            "We have that the optimal play audible adversary.",
            "It's always orthogonal to the previous ones."
        ],
        [
            "And we can also prove that the conditional value of the game, as always that expression there that depends on the bench or function F."
        ],
        [
            "And the value of the game, by definition, will be just the conditional value of the game and time zero, and so will be F of square root of T. No, given these three properties."
        ],
        [
            "If we have that, the minimax optimal strategy will be always the gradient of the conditional value of the game."
        ],
        [
            "And regret will be always of this form."
        ],
        [
            "So in a nice thing to notice is that here in this case every time we have that the benchmark function is satisfying that conditions we have that the optimal strategy is something like the usual potential function approach to online learning where the potential function is changing overtime and is exactly the conditional value of the game.",
            "So each time we predict with the gradient of this conditional value of the game.",
            "Another thing interesting thing to note is that WT will be always in the same direction of 30, so we're never leaving the span and this is the optimal strategy."
        ],
        [
            "Now let's go through another case.",
            "Let's consider a case in which the adversary is constrained in different way.",
            "It's constrained to play always in the same direction of the previous place.",
            "So we have a first vector G1."
        ],
        [
            "We have another one that has to be in the same day."
        ],
        [
            "And same direction again we can go back."
        ],
        [
            "But all the place will be on the line."
        ],
        [
            "And again we have that at each time step we know that there are only two possible choices that the adversary can pick.",
            "Now it turns out that."
        ],
        [
            "If we define F of T as the expectation with respect to R of T of debt expression where our OT is the sum of capital T -- T IAD, Rademacher random variables and actually still the benchmark function and now F of T satisfy the opposite of the previous condition on the 1st and the 2nd derivative, we have that in this."
        ],
        [
            "Case.",
            "The adversary it's always parallel.",
            "So it's the optimal play.",
            "It's always the parallel one.",
            "And."
        ],
        [
            "We have that the conditional value of the game as a simple expression is exactly that F of the defined above."
        ],
        [
            "And yet we have that the value of the game again as a closer to expression, that is that expectation there.",
            "And so."
        ],
        [
            "Again for free we get."
        ],
        [
            "That the minimax strategy has exactly this expression here.",
            "And direct."
        ],
        [
            "That is, again the fencl dwell plus the value of the game.",
            "So now we have the."
        ],
        [
            "In this case, the minimax optimal strategy is not a gradient anymore.",
            "This cannot be interpreted really a gradient of some potential function.",
            "It looks like more.",
            "Discrete derivative rather than a continuous derivative, but it's also interesting to note, but that is still in the direction of TT, so even in this case we're not leaving the span."
        ],
        [
            "So one problem of these these optimal strategy here is that we have to calculate this F of T. That depends on this, that expectation that is depends on the sum of this idea.",
            "The marker random variables and this can be annoying because this doesn't most of the time this will not have a closer expression.",
            "So what we can do is to approximate that some of ID rather macareno variables with the Gauss."
        ],
        [
            "And we can define this F hot tea in this way and we have that if half activty satisfy the same condition on the 1st and the 2nd derivative.",
            "We have that."
        ],
        [
            "In this case we don't have anymore the exact conditional value of the game, but we have a relaxation to it."
        ],
        [
            "And so we can calculate in upper bound to the value of the game and we."
        ],
        [
            "Will have a strategy and regret guarantee that will depend on this modified formulation.",
            "That in this case, but the advantage here is that most of the time this hefty will be will have a nice closed formula because it will depend on the Gaussian and not on ID Rademacher random variables."
        ],
        [
            "So let's see what is the application of all this theoretical things."
        ],
        [
            "Let's consider this benchmark function.",
            "This family of benchmark function where P is between one and two dimension is greater than one and W is just parameter."
        ],
        [
            "In this case, it's very easy to show that this satisfy the condition in the first case, so the optimal player diversity.",
            "It's always orthogonal to this previous one."
        ],
        [
            "And so the conditional value of the game.",
            "It's always update form."
        ],
        [
            "And the regret by the theorem is the Frankel Dwell Palace, the value of the game.",
            "So I can just calculate the Frankel dual F. And calculates the value of the game blob then."
        ],
        [
            "To the formula and I get the regret for free.",
            "And for P equal to 1, we basically recover the results of Abernathy Burnett.",
            "In the end Intel about online learning in a constrained setting, but this result extends the results also to the case where D is equal to two while their results were holding only four equal to three and bigger."
        ],
        [
            "Let's see another application.",
            "In this case we choose the benchmark function as that exponential form there.",
            "And we choose the easy variation.",
            "That I showed you, in which we we we pick the random variables as a Gaussian."
        ],
        [
            "And in this case, as I told you before, the F out of T will have just a closer to formal.",
            "No."
        ],
        [
            "In this case it's easy to show that we are in the case of the parallel adversary, so the optimal player adversary.",
            "It's always parallel to his previous play."
        ],
        [
            "And so we have that the optimal prediction strategy is that ugly formula there, and the regret is the faculty well plus the value of the game, the value of the game is upper bounded by that expectation.",
            "We can plug every."
        ],
        [
            "Going into the regret and we obtain these regret that is.",
            "Optimal up to constants and improves our my results at NIPS last year, closing the gap between upper bound and lower bound, and this is basically the optimal regret bound for online learning in unconstrained static."
        ],
        [
            "So that's all.",
            "Basically I presented general condition to calculate the minimax optimal strategy for unconstrained games.",
            "And his future work.",
            "There are some interesting connection between.",
            "Optimal regretting and constraint setting and optimal rate in a statistical setting.",
            "So basically if you if you have an algorithm that achieved the first one without parameter, you can have another algorithm very close to 1st one that will achieve optimal racing statistical setting again without parameters.",
            "And also we have the first expert algorithms that does not require communication between the experts and also in the future.",
            "We want to generalize this approach to general norms rather just that normal available space."
        ],
        [
            "OK, that's all.",
            "Thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "This is a brief outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will first interest.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miss you very briefly, what is the problem of online learning?",
                    "label": 0
                },
                {
                    "sent": "They would then I will tell you something about our results for minimax analysis.",
                    "label": 0
                },
                {
                    "sent": "For unconstrained games, and then I will tell you something about a couple of applications of these results online.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning is it's a framework for sequential prediction.",
                    "label": 1
                },
                {
                    "sent": "Basically, each time step the learner we receive question and he has to answer to this question just after each prediction the learner will see what is the correct answer and it will suffer loss that basically come measure how different is the predicted answer from the correct ones and an important characteristic of this scenario is that the answer.",
                    "label": 0
                },
                {
                    "sent": "And and the and the questions are generated by an adversary, so we don't assume anything about on on how this data is generated.",
                    "label": 1
                },
                {
                    "sent": "This doesn't work.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The learner.",
                    "label": 0
                },
                {
                    "sent": "Basically, we could say that the learner is to minimize the cumulative loss over the over the samples.",
                    "label": 0
                },
                {
                    "sent": "But we say that there is no assumption about the data generated, so this loss can be arbitrary high, so instead.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want to do is to minimize the difference between the cumulative loss of the learner and the cumulative loss of a fixed competitor in a certain class.",
                    "label": 0
                },
                {
                    "sent": "And this day.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strength is called the regret of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "What we will.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want is this regret to grow sublinearly in time?",
                    "label": 0
                },
                {
                    "sent": "So something like a sqrt D for example.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our case, we will focus on online learning with the linear losses in Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "So we consider any other spaces, for example, reproducing kernel Hilbert spaces or the spaces of matrices and its associated inner product, and we will denote the associate norm just with the symbol of the norm, and basically each round the learner will pick vector in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "The adversary will choose vector G of T and the learner will gain dinner product between WT and G of T and the regret will be just the sum of the inner product between Jyoti and the difference between WT and fixed competitor you.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our results are the following.",
                    "label": 1
                },
                {
                    "sent": "We will give general conditions under which it is possible to calculate in a closed formula the optimal strategy for the learner and the optimal optimal is not just up to a constant, but it's the true minimax strategy and we will use the results to recover and extends primary results on minimax online learning and we will close the gap between upper and lower bound for unconstrained online learning.",
                    "label": 1
                },
                {
                    "sent": "We have also other results in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in this talk I will focus just.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These first three points.",
                    "label": 0
                },
                {
                    "sent": "So let's move to the second part.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is basically the definition of the regret of the algorithm and we just take the supremum over of the regret over the choice of the computer in this silver space.",
                    "label": 0
                },
                {
                    "sent": "Now if we want to look at Amy Maxwell online learning what we do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is that?",
                    "label": 0
                },
                {
                    "sent": "We basically take the minimum and the maximum over all the rounds of the choice of the learner and the choice of the other side.",
                    "label": 0
                },
                {
                    "sent": "No, this quantity.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's called the value of the game.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can just equivalently rewrite it in this way, where 30 is just the sum of all the vector G of T chosen by the adversary.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, given that we want to consider the case of unconstrained online learning, instead of taking this supremium over set W in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "We will change.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This formulation in this way, so we will consider just the function F applied over this norm of deity that is the sum of this vector G of T. And we will call it this.",
                    "label": 0
                },
                {
                    "sent": "The benchman function and we just require it to be an increasing the convex function.",
                    "label": 0
                },
                {
                    "sent": "And the reason to do it?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that with this very simple theorem that appeared also in the literature in different forms, we can prove that the regret of the algorithm will be always be the Frankel dual of this benchmark function evaluated over the norm of the competitor plus the value of the game.",
                    "label": 0
                },
                {
                    "sent": "So basically, every time you choose this function F, you're implicitly choosing what is in in which way you want to measure the complexity of your competitor in regret bound.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important definition is the one of the conditional value of the game that is defined through that recursive formula, and basically it's summarizing what happened until time T and tells what is the potential of the algorithm in the next rounds.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And given the definition we have that the conditional value of the game at the end at time Capital T is exactly exactly equal to the benchmark function.",
                    "label": 0
                },
                {
                    "sent": "And the potential the conditional value of the game at the beginning at Time Zero is exactly equal to the value of the game, and so we can rewrite the previous theorem just as the regret is less than the fencl dual plus the conditional value of the game at times 0.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also have that the minimax optimal strategy will will be given by this argument.",
                    "label": 0
                },
                {
                    "sent": "Here this expression here.",
                    "label": 0
                },
                {
                    "sent": "So basically, once we have chosen, what is our function F our benchmark function and we're able to calculate in some way the conditional value of the game.",
                    "label": 1
                },
                {
                    "sent": "Then we're done cause we will know automatically what will be the minimax optimal strategy at each time step.",
                    "label": 0
                },
                {
                    "sent": "And we will also know what is the regret.",
                    "label": 0
                },
                {
                    "sent": "Through that theorem.",
                    "label": 0
                },
                {
                    "sent": "So the only thing that we need is a way to calculate the conditional value of the game for any benchmark function.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it turns out that this is actually.",
                    "label": 0
                },
                {
                    "sent": "A difficult problem in general.",
                    "label": 0
                },
                {
                    "sent": "But maybe there are conditions under which this is an easy problem.",
                    "label": 0
                },
                {
                    "sent": "So for consider for example.",
                    "label": 0
                },
                {
                    "sent": "A vector GT that is orthogonal to the sum of the previous place.",
                    "label": 0
                },
                {
                    "sent": "So in the first round.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will play just a vector the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Round it will play a vector that is orthogonal to this one, then something that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is orthogonal to the sum of the two.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this way I know that for example at time 5.",
                    "label": 0
                },
                {
                    "sent": "I don't know what will be the play of the adversary.",
                    "label": 0
                },
                {
                    "sent": "But I know that the norm of 30, that is the sum of the place as only two values.",
                    "label": 0
                },
                {
                    "sent": "And this will simplify a lot of the analysis.",
                    "label": 0
                },
                {
                    "sent": "Fuck.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that.",
                    "label": 0
                },
                {
                    "sent": "Every time the benchmark function F satisfied that condition on the 1st and the 2nd derivative.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have that the optimal play audible adversary.",
                    "label": 0
                },
                {
                    "sent": "It's always orthogonal to the previous ones.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also prove that the conditional value of the game, as always that expression there that depends on the bench or function F.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the value of the game, by definition, will be just the conditional value of the game and time zero, and so will be F of square root of T. No, given these three properties.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have that, the minimax optimal strategy will be always the gradient of the conditional value of the game.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And regret will be always of this form.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a nice thing to notice is that here in this case every time we have that the benchmark function is satisfying that conditions we have that the optimal strategy is something like the usual potential function approach to online learning where the potential function is changing overtime and is exactly the conditional value of the game.",
                    "label": 1
                },
                {
                    "sent": "So each time we predict with the gradient of this conditional value of the game.",
                    "label": 0
                },
                {
                    "sent": "Another thing interesting thing to note is that WT will be always in the same direction of 30, so we're never leaving the span and this is the optimal strategy.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's go through another case.",
                    "label": 0
                },
                {
                    "sent": "Let's consider a case in which the adversary is constrained in different way.",
                    "label": 0
                },
                {
                    "sent": "It's constrained to play always in the same direction of the previous place.",
                    "label": 0
                },
                {
                    "sent": "So we have a first vector G1.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have another one that has to be in the same day.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And same direction again we can go back.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But all the place will be on the line.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again we have that at each time step we know that there are only two possible choices that the adversary can pick.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we define F of T as the expectation with respect to R of T of debt expression where our OT is the sum of capital T -- T IAD, Rademacher random variables and actually still the benchmark function and now F of T satisfy the opposite of the previous condition on the 1st and the 2nd derivative, we have that in this.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "The adversary it's always parallel.",
                    "label": 0
                },
                {
                    "sent": "So it's the optimal play.",
                    "label": 0
                },
                {
                    "sent": "It's always the parallel one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have that the conditional value of the game as a simple expression is exactly that F of the defined above.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yet we have that the value of the game again as a closer to expression, that is that expectation there.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again for free we get.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the minimax strategy has exactly this expression here.",
                    "label": 0
                },
                {
                    "sent": "And direct.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, again the fencl dwell plus the value of the game.",
                    "label": 0
                },
                {
                    "sent": "So now we have the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, the minimax optimal strategy is not a gradient anymore.",
                    "label": 0
                },
                {
                    "sent": "This cannot be interpreted really a gradient of some potential function.",
                    "label": 0
                },
                {
                    "sent": "It looks like more.",
                    "label": 0
                },
                {
                    "sent": "Discrete derivative rather than a continuous derivative, but it's also interesting to note, but that is still in the direction of TT, so even in this case we're not leaving the span.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one problem of these these optimal strategy here is that we have to calculate this F of T. That depends on this, that expectation that is depends on the sum of this idea.",
                    "label": 1
                },
                {
                    "sent": "The marker random variables and this can be annoying because this doesn't most of the time this will not have a closer expression.",
                    "label": 1
                },
                {
                    "sent": "So what we can do is to approximate that some of ID rather macareno variables with the Gauss.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can define this F hot tea in this way and we have that if half activty satisfy the same condition on the 1st and the 2nd derivative.",
                    "label": 0
                },
                {
                    "sent": "We have that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case we don't have anymore the exact conditional value of the game, but we have a relaxation to it.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can calculate in upper bound to the value of the game and we.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will have a strategy and regret guarantee that will depend on this modified formulation.",
                    "label": 0
                },
                {
                    "sent": "That in this case, but the advantage here is that most of the time this hefty will be will have a nice closed formula because it will depend on the Gaussian and not on ID Rademacher random variables.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what is the application of all this theoretical things.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's consider this benchmark function.",
                    "label": 0
                },
                {
                    "sent": "This family of benchmark function where P is between one and two dimension is greater than one and W is just parameter.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, it's very easy to show that this satisfy the condition in the first case, so the optimal player diversity.",
                    "label": 0
                },
                {
                    "sent": "It's always orthogonal to this previous one.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the conditional value of the game.",
                    "label": 0
                },
                {
                    "sent": "It's always update form.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the regret by the theorem is the Frankel Dwell Palace, the value of the game.",
                    "label": 0
                },
                {
                    "sent": "So I can just calculate the Frankel dual F. And calculates the value of the game blob then.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the formula and I get the regret for free.",
                    "label": 0
                },
                {
                    "sent": "And for P equal to 1, we basically recover the results of Abernathy Burnett.",
                    "label": 1
                },
                {
                    "sent": "In the end Intel about online learning in a constrained setting, but this result extends the results also to the case where D is equal to two while their results were holding only four equal to three and bigger.",
                    "label": 1
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see another application.",
                    "label": 0
                },
                {
                    "sent": "In this case we choose the benchmark function as that exponential form there.",
                    "label": 0
                },
                {
                    "sent": "And we choose the easy variation.",
                    "label": 0
                },
                {
                    "sent": "That I showed you, in which we we we pick the random variables as a Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this case, as I told you before, the F out of T will have just a closer to formal.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case it's easy to show that we are in the case of the parallel adversary, so the optimal player adversary.",
                    "label": 0
                },
                {
                    "sent": "It's always parallel to his previous play.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we have that the optimal prediction strategy is that ugly formula there, and the regret is the faculty well plus the value of the game, the value of the game is upper bounded by that expectation.",
                    "label": 0
                },
                {
                    "sent": "We can plug every.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going into the regret and we obtain these regret that is.",
                    "label": 0
                },
                {
                    "sent": "Optimal up to constants and improves our my results at NIPS last year, closing the gap between upper bound and lower bound, and this is basically the optimal regret bound for online learning in unconstrained static.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's all.",
                    "label": 0
                },
                {
                    "sent": "Basically I presented general condition to calculate the minimax optimal strategy for unconstrained games.",
                    "label": 1
                },
                {
                    "sent": "And his future work.",
                    "label": 1
                },
                {
                    "sent": "There are some interesting connection between.",
                    "label": 1
                },
                {
                    "sent": "Optimal regretting and constraint setting and optimal rate in a statistical setting.",
                    "label": 0
                },
                {
                    "sent": "So basically if you if you have an algorithm that achieved the first one without parameter, you can have another algorithm very close to 1st one that will achieve optimal racing statistical setting again without parameters.",
                    "label": 1
                },
                {
                    "sent": "And also we have the first expert algorithms that does not require communication between the experts and also in the future.",
                    "label": 0
                },
                {
                    "sent": "We want to generalize this approach to general norms rather just that normal available space.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's all.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}