{
    "id": "t56tljsqfb3hfwomyj52e3daaai5a25p",
    "title": "Scalable Training of Mixture Models via Coresets",
    "info": {
        "author": [
            "Matthew Faulkner, CalTech"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_faulkner_coresets/",
    "segmentation": [
        [
            "Well my name is Matt Faulkner and today I'll be talking about scalable training of mixture models via core sets and this is joint work with Danny Feldman at MIT and Andreas Krause at ETH.",
            "The problem that we're interested in is learning mixture models from massive datasets.",
            "So imagine that you have a data set D that's spread across a collection of hard drives, and we're interested in fitting a mixture model with some parameters Theta."
        ],
        [
            "That minimizes the negative log likelihood across this data set.",
            "Now, if we're just talking about a moderately sized data set, then this is pretty standard and we could just solve it using the algorithm.",
            "However, if we're looking at mixture models like Gaussian mixtures, they don't have compact sufficient statistics.",
            "So if we have a big data set across a bunch of drives, it's not really clear how we could take that and put it into main memory on one computer and do them.",
            "If we just did a standard DM, we'd end up doing multiple passes over the data, and in particular, if we wanted to do multiple restarts with them, we're going to do a lot of passes over this big data set, and so that it gets quite expensive.",
            "So one way that we could go around this problem is we could imagine taking our big data set and then just choosing a small sample that does fit in main memory.",
            "And in general we might want to assign some weights to this set gamma.",
            "Now if we have this small weighted data set, we could imagine finding mixture parameters Theta hat by minimizing some weighted version of the negative log likelihood and the chief advantage of this approach is that since this small this weighted set C is very small, we could just do some weighted version of them to fit our parameters and it's really fast.",
            "Everything is in main memory, so we can do a bunch of multiple restarts if we want for them, and it's all going to be very quick.",
            "So at this point the main question here is how do we choose a weighted set?",
            "See, that's a good representation of our full data set, and so the main thing will be talking about in this talk is how to construct such a set.",
            "That's going to be a good approximation to the full data set in the sense that log likelihoods computed with respect to see will be an epsilon approximation to log likelihoods computed on the full data set.",
            "So in order to do this, I'm going to have to borrow the concept of a core set from comp."
        ],
        [
            "Additional geometry awaited set C is said to be an epsilon core set for a cost function.",
            "If the cost of a solution Theta with respect to this small set C is A1 plus or minus epsilon approximation of the cost on the full data set, and so in computational geometry, they've come up with some solutions for core sets for different geometric problems like K medians and K means, and here the solution Theta is just the set of the case centers and the cost is just the sum of distances from each point to its nearest center.",
            "So those are solved problems and in this talk we're going to be looking at mixture models like Gaussian mixtures where Theta is just the parameters of our mixture model and the cost here is the negative log likelihood of those parameters over the data set.",
            "Further, we're going to introduce the notation of a K epsilon course set for Gaussian mixture models, and we're going to say that a set C is a K epsilon core set if the log likelihood of the model with respect to that set C is A1 plus or minus epsilon approximation to the log likelihood on the entire data set, and this has to hold for any K mixture model parameters Theta.",
            "Now you should point out or should put out at this point that this notation here is hiding a nontrivial issue of scale, and so you'll want to read the paper for details on that.",
            "But this notation is enough to cover the intuition will need for this talk.",
            "So at this point, the big question here is how do we go about constructing one of these K epsilon core sets?",
            "How do we choose this small set C?"
        ],
        [
            "Something you might want to try, just as a first guess is some kind of uniform sampling, so we can imagine sampling a set you uniformly at random and then computing the weighted negative log likelihood on this set.",
            "But the problem here is, if we choose a very small set, we're likely to only choose a few points in the densest parts of."
        ],
        [
            "Data and we're likely to miss smaller clusters in our data set, and what this means is if we look at our our log likelihood on this small set, it's going to have a lot of variance if we happen to sample a couple points in this, our model would change drastically by fitting a mixture component over there.",
            "So the problem is this is not guaranteed to give us a very tight approximation to the log likelihood on the full data set.",
            "Now importance sampling gives us some techniques for computing estimates that have lower variance."
        ],
        [
            "And So what we'd really like to be able to do is come up with some kind of sampling distribution that biases sampling towards points that might significantly change the results of our computation.",
            "So in particular, interested in biasing the sampling towards these kinds of smaller clusters and sampling fewer points on average from denser regions of the data."
        ],
        [
            "Of course, if we do something with this kind of sampling distribution, we're also going to have to assign some importance weights to account for this sampling bias.",
            "Now, if you've been watching these two pictures, one of the things you might be thinking about is in order to assign all of these weights, it looks like we'd already need to know the distribution of the full data set, and that's exactly what we're setting out to find in the 1st place.",
            "So we have a bit of a chicken and egg problem.",
            "Which do we do first?",
            "And it turns out that we can break that kind of problem because we don't need to know the global full distribution of the data set, and we can make do with a very coarse kind of representation of that distribution.",
            "So how do we go about doing?"
        ],
        [
            "One way that we could build sort of a coarse grained representation of our data is to choose a set of representative points that are well spread out through the data and we can do that quite simply."
        ],
        [
            "So imagine that we just choose a few representative points and we assume that they're going to represent many of the points near to them."
        ],
        [
            "We take some of the points near to them and we just set those aside.",
            "We repeat this."
        ],
        [
            "For a few iterations."
        ],
        [
            "Randomly sampling."
        ],
        [
            "Small representative."
        ],
        [
            "And remove."
        ],
        [
            "Bring some of the other points near them, and by the time we've done this for a few iterations, we get this small set that's well spread out across."
        ],
        [
            "Data and I guess we've lost the blue points completely, but tends to well represent even these smaller clusters in the data set.",
            "So we can formalize the notion that these points here sort of represent their neighborhoods and why."
        ],
        [
            "So you could visualize that as you could imagine, placing a Voronoi tessellation over the data set centered at these Orange representative points.",
            "And now the reason we're doing all this is because we want a good sampling distribution, and So what we'd really like is to be able to sample in expectation about the same number of points from each one of these cells.",
            "So we get a good coverage from the data and."
        ],
        [
            "We can look at that as saying we'd like to place some amount of maybe mass on each one of these cells and distribute it uniformly across the point in that cell.",
            "So what this means is that points in sparsely populated cells should be more likely to be chosen points in densely populated cells.",
            "Also, it turns out we want to bias sampling towards points that are poorly represented by their cluster center, and so we can easily take advantage of that.",
            "Incorporate that here.",
            "So once we have this kind of sampling distribution, it's pretty straightforward just to sample a small set from it and to assign importance weights that account for that sampling bias.",
            "If we do that across the entire data set."
        ],
        [
            "Um?"
        ],
        [
            "We end up with this kind of a weighted set.",
            "See that's pretty small and it's well spread out across the data set and you can see that it's covering even some of the smaller clusters in less densely populated regions of the data set.",
            "So given this weighted set, we could formulate our weighted negative log likelihood, and because we did this via important sampling, it's going to be an unbiased estimate, but we're really interested in what's the variance of this estimate, and to tackle that we're going to formalize this a little bit more."
        ],
        [
            "So in particular, we can just take the ideas we've gone through already and make them concrete by specifying the constants and set sizes we need.",
            "So in particular we need to specify how many of these representative samples we choose at each iteration.",
            "We need to specify the way that we construct this important sampling, sampling, distribution and we need to specify the size of the final set.",
            "See that we take and.",
            "So if you look at this, you can see that it's polynomial in all of the interesting quantities like D&K and the accuracy epsilon and only has log arhythmic dependence on the input size N. It turns out that if you look at the class of non degenerate Gaussian mixture models, the semi spherical Gaussians that don't have that have covariance matrices with eigenvalues bounded away from zero.",
            "It turns out that this set C is a K epsilon core set.",
            "For these non degenerate Gaussian mixtures.",
            "Now the proof of this result and some of the more general results in the paper is a little bit technical, but I'm going to try and sketch the intuition for how we got there, and it involves a pretty neat step of taking course that algorithms from computational geometry and leveraging those to solve this statistical problem."
        ],
        [
            "The starting point of our analysis is some recent work by Feldman Langberg about constructing core sets for general cost functions that decompose additively over the data.",
            "So their framework provides us with two things.",
            "First of all, it gives us a theoretical foundation for.",
            "Finding the size of a core set necessary to obtain an epsilon approximation, and this depends on the quantity called the pseudo dimensionality of the cost functions, which is a lot like their VC dimension.",
            "It's sort of a measure of the statistical complexity of the problem.",
            "Now this isn't necessarily constructive, this just gives you a bound on the size of the course that you need.",
            "However, they also provide efficient algorithms for several specific cost functions, so of course that algorithms for several projective clustering problems.",
            "Now this is a great starting point, and in order to leverage these results for mixture models, what we had to come up with was a new theoretical result about the complexity of Gaussian mixture models, and this tells us what size of set we need to obtain an epsilon core set.",
            "But it doesn't give us a constructive algorithm, and so it turns out that with some clear careful approximations we can reduce the Gaussian mixture problem to a problem in computational geometry in some kind of transform space.",
            "So I'm going to go into that next."
        ],
        [
            "The starting point that we need is a geometric perspective for Gaussian distributions, and so if we look at the level sets, these... of the Gaussian distribution, if we look at them just in Rd, we can think of them as being embedded in some higher dimensional space are to the 2D and what these extra dimensions essentially let us do is to talk about distances between points and affine subspaces.",
            "So here we're interested in and affine subspace S, which is a function of the mean and covariance of our Gaussian distribution.",
            "And so it turns out that the level set... of the Gaussian distribution can be exactly represented as the level sets of distances between points and subspaces.",
            "And what this buys us is a purely geometric perspective for Gaussian distributions."
        ],
        [
            "The picture is pretty similar for Gaussian mixture models where now if we have.",
            "If you're interested in the probability of a single data point, we're going to have to include contributions from distances to two different affine subspaces, and so if we look at the log likelihood for a point, what we find is that this expression is a softman over distances between points and affine subspaces.",
            "Now, instead of doing a soft men, you might imagine that we could just do a hard assignment.",
            "We could just assign each point to its nearest affine subspace, and if we were to do that, we'd find that the softman is an upper bound for this simple min function of distances from points to affine subspaces.",
            "And this turns out to be exactly a projective clustering objective, and so this suggests that maybe we could just use a corset algorithm for projective clustering and solve our core set for Gaussian mixture models.",
            "But the key point to notice is that this is.",
            "This is just an upper bound, and so it's not necessarily the case that a core set for them in would be a core set for the softman, but using a new generalized triangle inequality, it turns out that these are close for mixture models and as a result we can leverage geometric tools for mixtures.",
            "OK, so that's pretty good, but one of the things that is really tricky is that core sets for these projective clustering objectives in general can be hard to compute, But if we look at the case for these non degenerate Gaussian mixture models, it turns out that we don't have to be talking about full affine subspaces in this transform space.",
            "We can just talk about choosing points in this transform space, and So what we end up with is a K means problem in this transform space.",
            "Specifically."
        ],
        [
            "If we look at computing and epsilon core set for the K means problem in this higher dimensional space, we obtain a core set for Gaussian mixture models.",
            "But now the approximation error is just scaled up by some function of the eigenvalues of the covariance matrices.",
            "Now I won't have time to go over sort of all of the generalizations and extensions that we have in this paper, but I want to sketch out a few of the most important ones.",
            "So the main thing is that the results here for Semi Sphere."
        ],
        [
            "Gaussians do generalize to calcium mixture models in general, and the key concept here is that rather than doing a reduction to the K means course at algorithm, you have to do a reduction to a different clustering or a different course algorithm for projective clustering.",
            "And there's been some recent work by Feldman Langberg about that.",
            "Also you can use.",
            "You can extend these results to mixture models based off of other norms, or LQ distances such as a mixture of Laplace distributions.",
            "If you remember it, a couple slides back when we were choosing the set C in our pseudocode, it had a log arhythmic dependence on N. The size of this set depended on login, and it turns out that you can also remove that dependence and you can obtain core sets that are completely independent of them.",
            "And finally, perhaps most importantly, for the practitioner, the algorithms for computing these courses can be done efficiently in both the parallel and streaming setting."
        ],
        [
            "Those involve some pretty cool properties.",
            "So it was observed early by harp, leaden Mazumdar that certain core sets have interesting compositional properties.",
            "So in particular, for certain core sets, you can imagine that if you have two epsilon core sets."
        ],
        [
            "You can just merge them together and the result is an epsilon core set on the total data.",
            "Also, if you have an epsilon core set and you want to further compress it by some factor Delta you."
        ],
        [
            "Do that and the resulting core set is now an epsilon plus Delta plus epsilon Delta core set.",
            "So this means we can merge in compress core sites.",
            "Now now this has some implications for computing core sets in the streaming setting."
        ],
        [
            "So if we had our data and as it's coming in, we're just filling up some finite buffer and whenever it gets full we compress it into a core set and we repeat this process, collecting data and compressing it.",
            "Now if we just were to compress together our data, as soon as we get it, we would."
        ],
        [
            "Find that."
        ],
        [
            "Our error is going to grow linearly in the number of merging, compression operations, and so we're going to get in trouble if we do this.",
            "But we can actually do a lot better if we're more careful with arms."
        ],
        [
            "And compress operations.",
            "So in particular, if as our data is coming in, we've been compressing it into core sets.",
            "But now we do the merging compressions according to a binary tree.",
            "What we find here in this situation is that the error grows logarithmically with the number of kind of leaves of this tree and the main idea is that no set is compressed more than the others.",
            "They are.",
            "Each one is only compressed up the height of this tree.",
            "And so specifically what we find is that you can compute KF Salon core sets for mixture models on streams with space and update time.",
            "That's polynomial in the parameters.",
            "DK one over Epsilon, and again only has log arhythmic dependence on end and one over Delta."
        ],
        [
            "This framework also gives us a way to compute core sets in parallel.",
            "So it's pretty pretty much the same thing, but now if we look at all of our data as being partitioned across machines in the cluster, for example, we could do some kind of map reduce type computation where we compute core sets and then we compute in parallel up the levels of this tree and what we find is that if we do this on M machines we get a nearly linear speedup in the time necessary to compute the core set.",
            "So at this point, we've talked about the theoretical guarantees for computing course.",
            "It's on mixture models, and we've talked about some efficient algorithms for computing them both in parallel and streaming settings.",
            "In this work, we also empirically evaluated core sets on some big data structures and found that they perform quite well in practice.",
            "The first data."
        ],
        [
            "That we tried out should be familiar to a lot of you.",
            "We took a look at the M NIST handwritten data set and we were interested in fitting a mixture of 10 Gaussians to 100 dimensional feature vectors computed from these images.",
            "And So what we find is that if we were to fit a mixture model to the full data set of 60,000 points, and then evaluate its log likelihood on a testing data set of 10,000 points, we get this type of performance.",
            "Now, if we're interested in computing mixture model from a smaller subset of the data, here we have logarithmically the size of the data.",
            "We could imagine just sampling random subsets of the data of smaller and smaller sizes uniformly at random, and what we see is if we fit mixture models to these uniformly chosen subsets, we get drastically worse performance when we get down to small set sizes.",
            "However, the core sets are still performing quite strongly, and even for very small corset sizes are competitive with a model fit to the full data set."
        ],
        [
            "We also looked at some data from neural tetrode recordings from Tennessee Office lab at Caltech, and these are 152 dimensional vectors from neural tetrode recordings, and the picture is pretty much the same.",
            "If we were to fit a model to about 300,000 data points, we see that the uniform sampling does again quite poorly, but corsets are nearly competitive for very small.",
            "Sizes."
        ],
        [
            "A different application that we're interested in is about learning mixture models on mobile phones, and so we took some data from the Cal Tech Community Seismic Network.",
            "Maybe some of you had a chance to see our demo yesterday and in this in this network we're interested in using low-cost devices like cell phones with accelerometers to measure earthquakes.",
            "And so, in order to do that, we need."
        ],
        [
            "To be able to somehow model the acceleration patterns that people acceleration patterns from normal user behavior when they're carrying these phones.",
            "And so the first step of the process is just to model their behavior.",
            "And so again, the results are pretty much the same for a data set of about 7 gigabytes of accelerometer data.",
            "We have the full data set, and again, core sets are way outperforming uniform sampling.",
            "But one of the things that's really interesting about this is we're not just interested in modeling the users acceleration.",
            "We're interested in using that to do anomaly detection and detect possible seismic."
        ],
        [
            "So if we take a look at the anomaly detection performance from these mixture models fit to the different sets, what we find is that here we have the area under an RSC curve, which is a measure for detection performance.",
            "We see that if we were to learn anomaly detection model from the full data set, we could essentially do the same detection performance using these core sets, whereas random sets do abysmally poor.",
            "So in conclusion."
        ],
        [
            "What we've shown is that Gaussian mixture models admit core sets of size independent of N, and that these results generalize to a variety of other mixture models.",
            "We've shown that we can do this by lifting geometric courcelette tools to the realm of statistical mixture models and along the way we came up with a new complexity result, sort of statistical complexity result for Gaussian mixture models, sort of related to the VC dimension of their level sets.",
            "We've also shown that these algorithms can be computed efficiently in both apparel and streaming setting an.",
            "Finally, we've tried them out on several large datasets and we found that they empirically perform quite well.",
            "Thank you have some time for questions.",
            "Question regarding the dominant dimensionality do you run in similar problems like EM algorithm?",
            "Well, when the dimension goes up, do you have any any statements about that local Optima?",
            "So I think the right way to look at this is.",
            "This will obtain a representative sample of the data set.",
            "That's a good representation for the full data, and so if you're interested in running them on this data set, you're going to have the same problems you get if you round it.",
            "M on the full data set.",
            "I guess the main advantage here is that since it's a small set that you could have in main memory, you could do a lot more iterations, alot more random restarts, so if you're concerned about local minima this could potentially help you.",
            "One more quick question, so it's known that compression is related to prediction.",
            "If you can compress your data sometime, you can get generalization bounds and do you know any of any connection between core sets and.",
            "So what I sketched over rather hastily was that in order to choose the size of a course that we had to use a result which is comparable to the VC dimension of the level sets here.",
            "And so I think that that result would allow you to talk about these kind of generalizations.",
            "Well, let's thank the speaker again.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well my name is Matt Faulkner and today I'll be talking about scalable training of mixture models via core sets and this is joint work with Danny Feldman at MIT and Andreas Krause at ETH.",
                    "label": 1
                },
                {
                    "sent": "The problem that we're interested in is learning mixture models from massive datasets.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have a data set D that's spread across a collection of hard drives, and we're interested in fitting a mixture model with some parameters Theta.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That minimizes the negative log likelihood across this data set.",
                    "label": 0
                },
                {
                    "sent": "Now, if we're just talking about a moderately sized data set, then this is pretty standard and we could just solve it using the algorithm.",
                    "label": 0
                },
                {
                    "sent": "However, if we're looking at mixture models like Gaussian mixtures, they don't have compact sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "So if we have a big data set across a bunch of drives, it's not really clear how we could take that and put it into main memory on one computer and do them.",
                    "label": 0
                },
                {
                    "sent": "If we just did a standard DM, we'd end up doing multiple passes over the data, and in particular, if we wanted to do multiple restarts with them, we're going to do a lot of passes over this big data set, and so that it gets quite expensive.",
                    "label": 0
                },
                {
                    "sent": "So one way that we could go around this problem is we could imagine taking our big data set and then just choosing a small sample that does fit in main memory.",
                    "label": 0
                },
                {
                    "sent": "And in general we might want to assign some weights to this set gamma.",
                    "label": 0
                },
                {
                    "sent": "Now if we have this small weighted data set, we could imagine finding mixture parameters Theta hat by minimizing some weighted version of the negative log likelihood and the chief advantage of this approach is that since this small this weighted set C is very small, we could just do some weighted version of them to fit our parameters and it's really fast.",
                    "label": 0
                },
                {
                    "sent": "Everything is in main memory, so we can do a bunch of multiple restarts if we want for them, and it's all going to be very quick.",
                    "label": 0
                },
                {
                    "sent": "So at this point the main question here is how do we choose a weighted set?",
                    "label": 0
                },
                {
                    "sent": "See, that's a good representation of our full data set, and so the main thing will be talking about in this talk is how to construct such a set.",
                    "label": 0
                },
                {
                    "sent": "That's going to be a good approximation to the full data set in the sense that log likelihoods computed with respect to see will be an epsilon approximation to log likelihoods computed on the full data set.",
                    "label": 0
                },
                {
                    "sent": "So in order to do this, I'm going to have to borrow the concept of a core set from comp.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Additional geometry awaited set C is said to be an epsilon core set for a cost function.",
                    "label": 0
                },
                {
                    "sent": "If the cost of a solution Theta with respect to this small set C is A1 plus or minus epsilon approximation of the cost on the full data set, and so in computational geometry, they've come up with some solutions for core sets for different geometric problems like K medians and K means, and here the solution Theta is just the set of the case centers and the cost is just the sum of distances from each point to its nearest center.",
                    "label": 0
                },
                {
                    "sent": "So those are solved problems and in this talk we're going to be looking at mixture models like Gaussian mixtures where Theta is just the parameters of our mixture model and the cost here is the negative log likelihood of those parameters over the data set.",
                    "label": 0
                },
                {
                    "sent": "Further, we're going to introduce the notation of a K epsilon course set for Gaussian mixture models, and we're going to say that a set C is a K epsilon core set if the log likelihood of the model with respect to that set C is A1 plus or minus epsilon approximation to the log likelihood on the entire data set, and this has to hold for any K mixture model parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "Now you should point out or should put out at this point that this notation here is hiding a nontrivial issue of scale, and so you'll want to read the paper for details on that.",
                    "label": 0
                },
                {
                    "sent": "But this notation is enough to cover the intuition will need for this talk.",
                    "label": 0
                },
                {
                    "sent": "So at this point, the big question here is how do we go about constructing one of these K epsilon core sets?",
                    "label": 0
                },
                {
                    "sent": "How do we choose this small set C?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something you might want to try, just as a first guess is some kind of uniform sampling, so we can imagine sampling a set you uniformly at random and then computing the weighted negative log likelihood on this set.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is, if we choose a very small set, we're likely to only choose a few points in the densest parts of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data and we're likely to miss smaller clusters in our data set, and what this means is if we look at our our log likelihood on this small set, it's going to have a lot of variance if we happen to sample a couple points in this, our model would change drastically by fitting a mixture component over there.",
                    "label": 0
                },
                {
                    "sent": "So the problem is this is not guaranteed to give us a very tight approximation to the log likelihood on the full data set.",
                    "label": 0
                },
                {
                    "sent": "Now importance sampling gives us some techniques for computing estimates that have lower variance.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we'd really like to be able to do is come up with some kind of sampling distribution that biases sampling towards points that might significantly change the results of our computation.",
                    "label": 0
                },
                {
                    "sent": "So in particular, interested in biasing the sampling towards these kinds of smaller clusters and sampling fewer points on average from denser regions of the data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, if we do something with this kind of sampling distribution, we're also going to have to assign some importance weights to account for this sampling bias.",
                    "label": 1
                },
                {
                    "sent": "Now, if you've been watching these two pictures, one of the things you might be thinking about is in order to assign all of these weights, it looks like we'd already need to know the distribution of the full data set, and that's exactly what we're setting out to find in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So we have a bit of a chicken and egg problem.",
                    "label": 0
                },
                {
                    "sent": "Which do we do first?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that we can break that kind of problem because we don't need to know the global full distribution of the data set, and we can make do with a very coarse kind of representation of that distribution.",
                    "label": 0
                },
                {
                    "sent": "So how do we go about doing?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way that we could build sort of a coarse grained representation of our data is to choose a set of representative points that are well spread out through the data and we can do that quite simply.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So imagine that we just choose a few representative points and we assume that they're going to represent many of the points near to them.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take some of the points near to them and we just set those aside.",
                    "label": 0
                },
                {
                    "sent": "We repeat this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a few iterations.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Randomly sampling.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small representative.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And remove.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring some of the other points near them, and by the time we've done this for a few iterations, we get this small set that's well spread out across.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data and I guess we've lost the blue points completely, but tends to well represent even these smaller clusters in the data set.",
                    "label": 0
                },
                {
                    "sent": "So we can formalize the notion that these points here sort of represent their neighborhoods and why.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you could visualize that as you could imagine, placing a Voronoi tessellation over the data set centered at these Orange representative points.",
                    "label": 1
                },
                {
                    "sent": "And now the reason we're doing all this is because we want a good sampling distribution, and So what we'd really like is to be able to sample in expectation about the same number of points from each one of these cells.",
                    "label": 0
                },
                {
                    "sent": "So we get a good coverage from the data and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can look at that as saying we'd like to place some amount of maybe mass on each one of these cells and distribute it uniformly across the point in that cell.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that points in sparsely populated cells should be more likely to be chosen points in densely populated cells.",
                    "label": 1
                },
                {
                    "sent": "Also, it turns out we want to bias sampling towards points that are poorly represented by their cluster center, and so we can easily take advantage of that.",
                    "label": 0
                },
                {
                    "sent": "Incorporate that here.",
                    "label": 1
                },
                {
                    "sent": "So once we have this kind of sampling distribution, it's pretty straightforward just to sample a small set from it and to assign importance weights that account for that sampling bias.",
                    "label": 0
                },
                {
                    "sent": "If we do that across the entire data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We end up with this kind of a weighted set.",
                    "label": 0
                },
                {
                    "sent": "See that's pretty small and it's well spread out across the data set and you can see that it's covering even some of the smaller clusters in less densely populated regions of the data set.",
                    "label": 0
                },
                {
                    "sent": "So given this weighted set, we could formulate our weighted negative log likelihood, and because we did this via important sampling, it's going to be an unbiased estimate, but we're really interested in what's the variance of this estimate, and to tackle that we're going to formalize this a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, we can just take the ideas we've gone through already and make them concrete by specifying the constants and set sizes we need.",
                    "label": 0
                },
                {
                    "sent": "So in particular we need to specify how many of these representative samples we choose at each iteration.",
                    "label": 0
                },
                {
                    "sent": "We need to specify the way that we construct this important sampling, sampling, distribution and we need to specify the size of the final set.",
                    "label": 0
                },
                {
                    "sent": "See that we take and.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this, you can see that it's polynomial in all of the interesting quantities like D&K and the accuracy epsilon and only has log arhythmic dependence on the input size N. It turns out that if you look at the class of non degenerate Gaussian mixture models, the semi spherical Gaussians that don't have that have covariance matrices with eigenvalues bounded away from zero.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this set C is a K epsilon core set.",
                    "label": 0
                },
                {
                    "sent": "For these non degenerate Gaussian mixtures.",
                    "label": 0
                },
                {
                    "sent": "Now the proof of this result and some of the more general results in the paper is a little bit technical, but I'm going to try and sketch the intuition for how we got there, and it involves a pretty neat step of taking course that algorithms from computational geometry and leveraging those to solve this statistical problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The starting point of our analysis is some recent work by Feldman Langberg about constructing core sets for general cost functions that decompose additively over the data.",
                    "label": 0
                },
                {
                    "sent": "So their framework provides us with two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, it gives us a theoretical foundation for.",
                    "label": 0
                },
                {
                    "sent": "Finding the size of a core set necessary to obtain an epsilon approximation, and this depends on the quantity called the pseudo dimensionality of the cost functions, which is a lot like their VC dimension.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a measure of the statistical complexity of the problem.",
                    "label": 0
                },
                {
                    "sent": "Now this isn't necessarily constructive, this just gives you a bound on the size of the course that you need.",
                    "label": 0
                },
                {
                    "sent": "However, they also provide efficient algorithms for several specific cost functions, so of course that algorithms for several projective clustering problems.",
                    "label": 0
                },
                {
                    "sent": "Now this is a great starting point, and in order to leverage these results for mixture models, what we had to come up with was a new theoretical result about the complexity of Gaussian mixture models, and this tells us what size of set we need to obtain an epsilon core set.",
                    "label": 1
                },
                {
                    "sent": "But it doesn't give us a constructive algorithm, and so it turns out that with some clear careful approximations we can reduce the Gaussian mixture problem to a problem in computational geometry in some kind of transform space.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go into that next.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The starting point that we need is a geometric perspective for Gaussian distributions, and so if we look at the level sets, these... of the Gaussian distribution, if we look at them just in Rd, we can think of them as being embedded in some higher dimensional space are to the 2D and what these extra dimensions essentially let us do is to talk about distances between points and affine subspaces.",
                    "label": 0
                },
                {
                    "sent": "So here we're interested in and affine subspace S, which is a function of the mean and covariance of our Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out that the level set... of the Gaussian distribution can be exactly represented as the level sets of distances between points and subspaces.",
                    "label": 1
                },
                {
                    "sent": "And what this buys us is a purely geometric perspective for Gaussian distributions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The picture is pretty similar for Gaussian mixture models where now if we have.",
                    "label": 1
                },
                {
                    "sent": "If you're interested in the probability of a single data point, we're going to have to include contributions from distances to two different affine subspaces, and so if we look at the log likelihood for a point, what we find is that this expression is a softman over distances between points and affine subspaces.",
                    "label": 0
                },
                {
                    "sent": "Now, instead of doing a soft men, you might imagine that we could just do a hard assignment.",
                    "label": 0
                },
                {
                    "sent": "We could just assign each point to its nearest affine subspace, and if we were to do that, we'd find that the softman is an upper bound for this simple min function of distances from points to affine subspaces.",
                    "label": 0
                },
                {
                    "sent": "And this turns out to be exactly a projective clustering objective, and so this suggests that maybe we could just use a corset algorithm for projective clustering and solve our core set for Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "But the key point to notice is that this is.",
                    "label": 0
                },
                {
                    "sent": "This is just an upper bound, and so it's not necessarily the case that a core set for them in would be a core set for the softman, but using a new generalized triangle inequality, it turns out that these are close for mixture models and as a result we can leverage geometric tools for mixtures.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's pretty good, but one of the things that is really tricky is that core sets for these projective clustering objectives in general can be hard to compute, But if we look at the case for these non degenerate Gaussian mixture models, it turns out that we don't have to be talking about full affine subspaces in this transform space.",
                    "label": 0
                },
                {
                    "sent": "We can just talk about choosing points in this transform space, and So what we end up with is a K means problem in this transform space.",
                    "label": 0
                },
                {
                    "sent": "Specifically.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at computing and epsilon core set for the K means problem in this higher dimensional space, we obtain a core set for Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "But now the approximation error is just scaled up by some function of the eigenvalues of the covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "Now I won't have time to go over sort of all of the generalizations and extensions that we have in this paper, but I want to sketch out a few of the most important ones.",
                    "label": 0
                },
                {
                    "sent": "So the main thing is that the results here for Semi Sphere.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussians do generalize to calcium mixture models in general, and the key concept here is that rather than doing a reduction to the K means course at algorithm, you have to do a reduction to a different clustering or a different course algorithm for projective clustering.",
                    "label": 0
                },
                {
                    "sent": "And there's been some recent work by Feldman Langberg about that.",
                    "label": 0
                },
                {
                    "sent": "Also you can use.",
                    "label": 0
                },
                {
                    "sent": "You can extend these results to mixture models based off of other norms, or LQ distances such as a mixture of Laplace distributions.",
                    "label": 0
                },
                {
                    "sent": "If you remember it, a couple slides back when we were choosing the set C in our pseudocode, it had a log arhythmic dependence on N. The size of this set depended on login, and it turns out that you can also remove that dependence and you can obtain core sets that are completely independent of them.",
                    "label": 0
                },
                {
                    "sent": "And finally, perhaps most importantly, for the practitioner, the algorithms for computing these courses can be done efficiently in both the parallel and streaming setting.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those involve some pretty cool properties.",
                    "label": 0
                },
                {
                    "sent": "So it was observed early by harp, leaden Mazumdar that certain core sets have interesting compositional properties.",
                    "label": 0
                },
                {
                    "sent": "So in particular, for certain core sets, you can imagine that if you have two epsilon core sets.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can just merge them together and the result is an epsilon core set on the total data.",
                    "label": 0
                },
                {
                    "sent": "Also, if you have an epsilon core set and you want to further compress it by some factor Delta you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that and the resulting core set is now an epsilon plus Delta plus epsilon Delta core set.",
                    "label": 0
                },
                {
                    "sent": "So this means we can merge in compress core sites.",
                    "label": 0
                },
                {
                    "sent": "Now now this has some implications for computing core sets in the streaming setting.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we had our data and as it's coming in, we're just filling up some finite buffer and whenever it gets full we compress it into a core set and we repeat this process, collecting data and compressing it.",
                    "label": 0
                },
                {
                    "sent": "Now if we just were to compress together our data, as soon as we get it, we would.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our error is going to grow linearly in the number of merging, compression operations, and so we're going to get in trouble if we do this.",
                    "label": 0
                },
                {
                    "sent": "But we can actually do a lot better if we're more careful with arms.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And compress operations.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if as our data is coming in, we've been compressing it into core sets.",
                    "label": 0
                },
                {
                    "sent": "But now we do the merging compressions according to a binary tree.",
                    "label": 0
                },
                {
                    "sent": "What we find here in this situation is that the error grows logarithmically with the number of kind of leaves of this tree and the main idea is that no set is compressed more than the others.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "Each one is only compressed up the height of this tree.",
                    "label": 1
                },
                {
                    "sent": "And so specifically what we find is that you can compute KF Salon core sets for mixture models on streams with space and update time.",
                    "label": 0
                },
                {
                    "sent": "That's polynomial in the parameters.",
                    "label": 0
                },
                {
                    "sent": "DK one over Epsilon, and again only has log arhythmic dependence on end and one over Delta.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This framework also gives us a way to compute core sets in parallel.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty pretty much the same thing, but now if we look at all of our data as being partitioned across machines in the cluster, for example, we could do some kind of map reduce type computation where we compute core sets and then we compute in parallel up the levels of this tree and what we find is that if we do this on M machines we get a nearly linear speedup in the time necessary to compute the core set.",
                    "label": 0
                },
                {
                    "sent": "So at this point, we've talked about the theoretical guarantees for computing course.",
                    "label": 0
                },
                {
                    "sent": "It's on mixture models, and we've talked about some efficient algorithms for computing them both in parallel and streaming settings.",
                    "label": 0
                },
                {
                    "sent": "In this work, we also empirically evaluated core sets on some big data structures and found that they perform quite well in practice.",
                    "label": 0
                },
                {
                    "sent": "The first data.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we tried out should be familiar to a lot of you.",
                    "label": 0
                },
                {
                    "sent": "We took a look at the M NIST handwritten data set and we were interested in fitting a mixture of 10 Gaussians to 100 dimensional feature vectors computed from these images.",
                    "label": 0
                },
                {
                    "sent": "And So what we find is that if we were to fit a mixture model to the full data set of 60,000 points, and then evaluate its log likelihood on a testing data set of 10,000 points, we get this type of performance.",
                    "label": 0
                },
                {
                    "sent": "Now, if we're interested in computing mixture model from a smaller subset of the data, here we have logarithmically the size of the data.",
                    "label": 0
                },
                {
                    "sent": "We could imagine just sampling random subsets of the data of smaller and smaller sizes uniformly at random, and what we see is if we fit mixture models to these uniformly chosen subsets, we get drastically worse performance when we get down to small set sizes.",
                    "label": 0
                },
                {
                    "sent": "However, the core sets are still performing quite strongly, and even for very small corset sizes are competitive with a model fit to the full data set.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also looked at some data from neural tetrode recordings from Tennessee Office lab at Caltech, and these are 152 dimensional vectors from neural tetrode recordings, and the picture is pretty much the same.",
                    "label": 1
                },
                {
                    "sent": "If we were to fit a model to about 300,000 data points, we see that the uniform sampling does again quite poorly, but corsets are nearly competitive for very small.",
                    "label": 0
                },
                {
                    "sent": "Sizes.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A different application that we're interested in is about learning mixture models on mobile phones, and so we took some data from the Cal Tech Community Seismic Network.",
                    "label": 1
                },
                {
                    "sent": "Maybe some of you had a chance to see our demo yesterday and in this in this network we're interested in using low-cost devices like cell phones with accelerometers to measure earthquakes.",
                    "label": 0
                },
                {
                    "sent": "And so, in order to do that, we need.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be able to somehow model the acceleration patterns that people acceleration patterns from normal user behavior when they're carrying these phones.",
                    "label": 0
                },
                {
                    "sent": "And so the first step of the process is just to model their behavior.",
                    "label": 0
                },
                {
                    "sent": "And so again, the results are pretty much the same for a data set of about 7 gigabytes of accelerometer data.",
                    "label": 0
                },
                {
                    "sent": "We have the full data set, and again, core sets are way outperforming uniform sampling.",
                    "label": 0
                },
                {
                    "sent": "But one of the things that's really interesting about this is we're not just interested in modeling the users acceleration.",
                    "label": 0
                },
                {
                    "sent": "We're interested in using that to do anomaly detection and detect possible seismic.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take a look at the anomaly detection performance from these mixture models fit to the different sets, what we find is that here we have the area under an RSC curve, which is a measure for detection performance.",
                    "label": 0
                },
                {
                    "sent": "We see that if we were to learn anomaly detection model from the full data set, we could essentially do the same detection performance using these core sets, whereas random sets do abysmally poor.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we've shown is that Gaussian mixture models admit core sets of size independent of N, and that these results generalize to a variety of other mixture models.",
                    "label": 1
                },
                {
                    "sent": "We've shown that we can do this by lifting geometric courcelette tools to the realm of statistical mixture models and along the way we came up with a new complexity result, sort of statistical complexity result for Gaussian mixture models, sort of related to the VC dimension of their level sets.",
                    "label": 0
                },
                {
                    "sent": "We've also shown that these algorithms can be computed efficiently in both apparel and streaming setting an.",
                    "label": 0
                },
                {
                    "sent": "Finally, we've tried them out on several large datasets and we found that they empirically perform quite well.",
                    "label": 0
                },
                {
                    "sent": "Thank you have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "Question regarding the dominant dimensionality do you run in similar problems like EM algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, when the dimension goes up, do you have any any statements about that local Optima?",
                    "label": 0
                },
                {
                    "sent": "So I think the right way to look at this is.",
                    "label": 0
                },
                {
                    "sent": "This will obtain a representative sample of the data set.",
                    "label": 0
                },
                {
                    "sent": "That's a good representation for the full data, and so if you're interested in running them on this data set, you're going to have the same problems you get if you round it.",
                    "label": 0
                },
                {
                    "sent": "M on the full data set.",
                    "label": 0
                },
                {
                    "sent": "I guess the main advantage here is that since it's a small set that you could have in main memory, you could do a lot more iterations, alot more random restarts, so if you're concerned about local minima this could potentially help you.",
                    "label": 0
                },
                {
                    "sent": "One more quick question, so it's known that compression is related to prediction.",
                    "label": 0
                },
                {
                    "sent": "If you can compress your data sometime, you can get generalization bounds and do you know any of any connection between core sets and.",
                    "label": 0
                },
                {
                    "sent": "So what I sketched over rather hastily was that in order to choose the size of a course that we had to use a result which is comparable to the VC dimension of the level sets here.",
                    "label": 0
                },
                {
                    "sent": "And so I think that that result would allow you to talk about these kind of generalizations.",
                    "label": 0
                },
                {
                    "sent": "Well, let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}