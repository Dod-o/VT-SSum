{
    "id": "lqol7cvmm5wir6aigkzauciqmkcmzgzo",
    "title": "Latent Semantic Grammar Induction",
    "info": {
        "author": [
            "Andrew Olney, Department of Psychology, University of Memphis"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/mlcs07_olney_lsg/",
    "segmentation": [
        [
            "OK.",
            "So heard a lot of talks today already and perhaps some of my first slides will be slightly repetitive so."
        ],
        [
            "Oh, I hope not to to bore anyone.",
            "So this is kind of been mentioned, but perhaps not set explicitly so supervised grammar induction works great.",
            "But it can be.",
            "Expensive and time consuming, and it's not a general solution.",
            "It's a very focused solution for particular."
        ],
        [
            "Dataset.",
            "So unsupervised gram reduction is a little bit more general problem, and I think that's why we're all interested in it, and particularly I'm interested in this work and the kinds of knowledge that are required.",
            "So as part of speech necessary to build into your model is hidden syntax, which I just think of this higher order constituents like NP and VP or those necessary is projectivity unnecessary assumption for what would be generally considered a projective language like English.",
            "Prior distributions which have been mentioned before, you know how far away are your dependencies or whatnot and then semantics?",
            "Does it do the words actually have to mean anything in some sense for you to do this successfully?"
        ],
        [
            "Although most of the work that we've seen today seems to suggest that they're not.",
            "So the basic model that I'm going to talk about today is very very simple model, so that's probably good for a talk that's a little bit later on in the day, has no parts of speech, and I would argue it has some little bit of semantics which I think is kind of interesting and maybe a little bit different.",
            "One big component of this model is minimum spanning tree parsing, which I think was introduced by Ryan MacDonald, and that's kind of nice for this because it finds an optimal.",
            "Parse solution.",
            "It's non projective and it has no hidden syntax, so that's what I'm going to do.",
            "I'll be talking."
        ],
        [
            "A fair amount about that at some point.",
            "OK, so the key idea inspiration for this and it's been mentioned before I think I think increases talk is distributional analysis.",
            "So elements distribution is the sum of its environments and if you consider that environment as an array of Co occurrence, then you can use that kind of distributional concepts to build higher levels of structure.",
            "So the way that I consider this is in terms of sub."
        ],
        [
            "Suitability so this is this is pretty old.",
            "This goes back at least two.",
            "At least to bro.",
            "So the idea here is that if you have something like a pronoun and it can substitute for a determiner noun, then you can say oh that's a block.",
            "I can substitute this one element for this bigram, and therefore this bigram is some sort of constituent.",
            "In order to do that though, you need some."
        ],
        [
            "Notion of context.",
            "So I'm going to talk.",
            "Consider two different kinds of contexts.",
            "When is a global context with no word order and the other is a local context that does have word order.",
            "So this notion of global context would be familiar to anyone who has looked into information retrieval models.",
            "This is more or less identical, So what you do to establish this kind of context is you look at every word and you just count how many times that word appears in.",
            "Documents, so here I'm taking here in the next slide I'm taking document just to be a sentence which is perhaps a slightly unusual kind of document.",
            "So the word boy occurs in Document 1.",
            "One time that occurs in Document 2 one time.",
            "You get the idea, you're just."
        ],
        [
            "Counting the frequency.",
            "The local context which is been used quite a bit in this kind of work.",
            "You just look at the words of the right into the left of the target work, and you do a similar thing.",
            "You just count frequencies so the target words are in black, hear the words to the left of that or in green so the occurs before boy one time boy occurs before likes.",
            "Sorry, that's not even in there, but you get the idea.",
            "So for likes occurs after like one time and different people have used different words here across across the top.",
            "Usually the most frequent words actually in this.",
            "I don't do that at all.",
            "I just use all of the words so I end up with extremely large matrices.",
            "But I don't have to do any sort of opery fiddling with."
        ],
        [
            "Words to include.",
            "OK, so one step that I use because I have these ridiculously large matrices is singular value decomposition and this is.",
            "Used fairly often to compress a matrix into a smaller form.",
            "The idea is that you get a kind of Co occurrence analysis through this compression, and most importantly is that terms in this compressed space can become similar have similar representations even though they didn't necessarily Co occur.",
            "So you know theoretically and ideally, so.",
            "The idea here is that you have a.",
            "This actually shouldn't be equals.",
            "This should be approximates, so you have this original matrix A, which is either a local or global kind of matrix that I showed before and you end up splitting it into these three matrices, where in fact these are a little bit a little bit smaller.",
            "I won't go into that too much, but just enough to kind of get you into the idea that you're compressing this, and this has been used by schutze to do part of speech induction.",
            "And it's also been used in something called latent semantic analysis, which is a little bit of my inspiration for this, and LSA has been used for essay grading.",
            "Comparing the meanings of various documents, entailment topic segmentation.",
            "Do you want to end up with a question or?",
            "Exactly."
        ],
        [
            "Well, actually."
        ],
        [
            "Actually, the global is exactly LSA except.",
            "Haven't gotten to this yet, but except that instead of just doing unigrams here, I'm actually doing unigrams and bigrams so now the matrix gets even."
        ],
        [
            "So."
        ],
        [
            "It's kind of that can be kind of a problem.",
            "OK, so the other components of this search because you can do these substitutions, right?",
            "That's a really simple idea, but given a sentence, there are a lot of substitutions that you can do.",
            "I mean you've got 5 words you could do a substitution with these or these, or these, or these and so on and so forth.",
            "So you need a way to find the best substitutions.",
            "And I forgot to mention that the way I'm doing these substitutions is with cosines.",
            "So each word has a vector representation and then to compare 2 words I just compute the cosine between those two vector representations.",
            "An for people who aren't familiar with that.",
            "Basically you've got these two vectors and the cosine is just the angle between them, so it's a similarity measure that's independent of the length of the vector.",
            "OK, so to do this search I use this graph algorithm.",
            "This is what rank Donald used and it finds the minimum directed spanning tree of a graph and it just turns out that if you construct your graph in the right way, that is essentially a dependency parse tree.",
            "So that's kind of nice.",
            "It's order N squared, which is also nice, but it requires a graph to operate on.",
            "So I'm gonna spend a little bit of time.",
            "I'm talking about the graph.",
            "And part of that is because I think if I didn't talk about how to populate the graph, then that part of the model would be completely opaque and I don't think it would be very useful and I'm going to spend a few slides talking about how the algorithm actually operates over the graph to create a minimum spanning tree."
        ],
        [
            "OK, so.",
            "If you had a sentence like the boy sleeps, you would ideally like to get that kind of representation out of it.",
            "I've got a dummy root node here.",
            "Just to make things a little bit simpler in several respects, and you know, if you look at the graph representation for this, it would be great to have ones here indicating a link and perhaps zeros everywhere else to say that there was no link at all."
        ],
        [
            "Um?",
            "So in my model, to actually create a graph like this in the first graph I just showed is what you're aiming for, not necessarily what you get.",
            "You start off and you disallow all self heads.",
            "So that's a little bit I guess, of built-in knowledge.",
            "Perhaps I don't know that."
        ],
        [
            "Kind of debatable.",
            "So you look at the boy.",
            "And the boy is.",
            "Probably something that's a curd in the corpus somewhere, so you've got a vector representation for the diagram.",
            "The boy and I can explain what to do in data sparsity if for some reason you don't have vector representation.",
            "Maybe in the questions of somebody is interested or after.",
            "And you do a search.",
            "You do a search through all possible for through all unigrams.",
            "In order to find the unigram that is closest to the boy.",
            "So this is kind of an EXO centric kind of search."
        ],
        [
            "That you're doing through the space.",
            "OK, and then this part here, which I probably should put on a separate slide.",
            "This uses the kind of linguistic idea of a head test.",
            "So Zwicky and Hudson and a few others have talked about tests that you can do for linguistic heads that are based on the kind of semantic similarity of the words in question.",
            "So this procedure uses that intuition.",
            "So you've got a cosine here between this nearest unigram neighbor that you found.",
            "And one of the elements here, which is boy.",
            "Let's say that's .34.",
            "And then you do the same thing between the nearest unigram neighbor and the other element of the bigram.",
            "And let's say that that's.",
            ".0 two will then, based on those, those two cosines you say?",
            "Well, John is more like the boy.",
            "Sorry John is more like boy than it is like the so boy must be the head of this constituent.",
            "That's the basic idea.",
            "Anyway, you use that information here to populate the graph, and basically that lets you put put these numbers in the right boxes so that.",
            "This graph is just the words that are that one word dominates the other word.",
            "So it's like the degree of confidence exposed and how likely one word dominates the other work.",
            "But they're not strictly probabilities."
        ],
        [
            "OK, so then you have the sleep switch boys broken up so."
        ],
        [
            "This is this is a little bit different.",
            "And quite often in cases like this, because the semantically kind of weak in this representation sleeps would be the nearest neighbor.",
            "So you go through the same price."
        ],
        [
            "Major and you find."
        ],
        [
            "Sleeps is the head and then."
        ],
        [
            "Boy sleeps OK. And at this point you have a little bit of a problem 'cause you have this dummy root, which is nice for other reasons, but you don't really have a meaningful way to compute these boxes in the graph representation.",
            "So I do something very simple.",
            "I basically make them all equally likely, so each word is equally likely to be the root and this is just this.",
            "Value here is just an average of all the other values in it."
        ],
        [
            "So you know, there might be a smarter way of doing that.",
            "OK, so this this graph here, which is fully connected graph with directed arrows.",
            "This was the box that we just saw and this is more familiar graph like representation of the Senate.",
            "So now I'm going to talk a little bit about the search procedure that you would use to solve this."
        ],
        [
            "OK, so.",
            "The basic idea of this minimum spanning tree algorithm.",
            "Is that each each directed edge has a weight and each node in the graph greedily selects the incoming edge that has the highest weight.",
            "So I've I've indicated these here in red, so if you look at this node sleeps, you can see that's got this inbound edge.",
            "That's .3 one that's .1 and this one.",
            "That's .306.",
            "So this is the highest inbound edge.",
            "And likewise you can do that procedure with the with the other node."
        ],
        [
            "So in the case that you do this and there are no cycles which would.",
            "Which would be that?",
            "Then you've essentially just solved solve the graph and you're done.",
            "Which is nice, but it doesn't always happen that way.",
            "So what if you get?"
        ],
        [
            "A cycle.",
            "So the idea for dealing with cycles is that.",
            "You can actually recursively call this algorithm.",
            "On a graph that combines the nodes.",
            "That make up the cycle.",
            "So if you've got two nodes that make up a cycle, you collapse them into a single node.",
            "You rewire the graph in a way that I'll talk through in just a second, and then you run the algorithm on it again and then eventually you're going to find a point where you've got a graph with no cycles, and then you just pop back up each time you rewire the graph.",
            "According to the the rewiring decisions that you may."
        ],
        [
            "Right?",
            "OK, so.",
            "How we've rewired the graph?",
            "OK, so the idea here is that route to the sleeps is .906 because the maximum path from root to sleeps to the OR route to the sleeps.",
            "Was point 906, so I think I could back up a few slides and show that exactly, but I don't think it's entirely necessary.",
            "And also with boy.",
            "Boy to the sleeps.",
            "The path from boy.",
            "To the sleeps or boyda sleeps to the that's maximum.",
            "Is .9, so that's the weight of the edge that you give that.",
            "And then the the boy exists and sleeps.",
            "The boy exists 'cause this is a fully connected graph.",
            "So in order to determine this rewired edge, you just take one of those.",
            "That's the highest."
        ],
        [
            "So if you run the algorithm on this again, you find that.",
            "That you solved it, just that greedy, highest incoming edge.",
            "Procedure you solved it?"
        ],
        [
            "And then you can rewire the graph using this same kinds of ideas.",
            "So before we rewired route to the sleeps, that path actually went from route to sleeps to the.",
            "So you just add that edge back.",
            "And likewise when we had the single edge from the sleeps to boy, it was actually from sleeps, the boy, not from the boy.",
            "So you just have to keep track of where all the ledges were, that that you had before.",
            "So funnily enough I don't know if you remember, but this is the same graph from the beginning when it didn't have a cycle.",
            "And I actually created a cycle and related one of the edges.",
            "So this shows a little bit that this is reasonably sensitive, not sensitive to being perturbed.",
            "Which is kind of nice property, and my thought in using this was originally that if you could get some sort of distinction between.",
            "You know proper constituents and improper constituents, noticed issuance that this would be able to use that information to create a proper proper parts.",
            "So that was the basic idea.",
            "But in my model I don't explicitly model constituents versus Dist issuance."
        ],
        [
            "I think that's perhaps a weakness.",
            "I don't use a filter or anything like that.",
            "OK, so.",
            "67 sweet take a long drink water.",
            "OK, so.",
            "I think some of these datasets have been mentioned multiple times, so I won't talk about it too much.",
            "I use the Pin tree bank.",
            "Wall Street Journal 10, which I think that Dan Klein and Chris Manning kind of named it's the part of the treebank where there's 10 words or less.",
            "No punctuation except for I think$ something like that.",
            "Anyway, or it's 4% anyway, and then use the columns head rules to transform it into the bracketed treebank into dependencies, which is what I'm I'm using, and that's just just for testing.",
            "But to do the actual training to acquire the amount of information.",
            "Our frequency counts if you will.",
            "In order to do this I use a much bigger corpus, which is the Wall Street Journal and the North American News Corpus Year 1994, and that was about as much as my computer could handle, so that's about 10,000,000 words in 460,000 sentences, so some of the matrix matrices were very large.",
            "You know, I think one was like 'cause you know, grams and diagrams like 1.2 million by.",
            "800,000 or something like that."
        ],
        [
            "So yeah, it's quite painful.",
            "You have to use a special kind of SVD to do that.",
            "That writes things out to disk and a big block.",
            "Otherwise, unless you have a super computer which I don't have.",
            "OK, so two baselines, I guess, actually three 'cause there's the random baseline.",
            "There's the right branching baseline in the left branching baseline.",
            "So those will appear on the next slide.",
            "I use the sign test to determine statistical significance.",
            "All the values I'm going to show you were all statistically significant and one thing I will also say before I share the next slide is that all of my numbers are for directed dependencies, so they notice that if you remember the numbers before, you'll notice that there are a lot lower, and in fact the whole kind of range has kind of been squashed a little bit, so even the random baseline is quite a lot lower."
        ],
        [
            "Oh, and of course there's the prior knowledge that was very so.",
            "You know, the context.",
            "Is it global or local?",
            "Essentially?",
            "Do we even care about word order?",
            "Projectivity.",
            "Which I didn't actually introduce today.",
            "Maybe I should have done that.",
            "How much time do I have?",
            "Maybe that can be a question and and so like English is a projective language and non projective just means that you're not making any sort of assumptions about the way that dependencies.",
            "The dependencies between words relate to dependencies between other words.",
            "And then prior distributions.",
            "So really good guess to put into your model.",
            "I think that that Chris mentioned in his talk is to say Oh well, most attachments are local induction or anything like that.",
            "I'm using this LSA like semantics is was noted."
        ],
        [
            "So you can see for that condition the baseline is actually quite low, it's 14.2%.",
            "You can see here that.",
            "Local and global with no additional prior information are not better than the right branching baseline.",
            "But you can see that when you add either productivity or some knowledge of prior distributions, they both beat the right branching baseline and nothing beats the left branching baseline, so this is clearly not as high performing as I think.",
            "All of the models in this that we talked about today, although it's significantly more simple.",
            "One thing that I think is really interesting about this, maybe have this on the next slide.",
            "I'll talk about it now.",
            "Is that the global context did so well, so just ignoring word order, it still be the right branching baseline, which is kind of interesting.",
            "The differences are very small.",
            "There's really little additive effects between.",
            "Objectivity in the prior distribution, which may not be that surprising.",
            "'cause perhaps they bring to bear the same kinds of information, but I think it's kind of nice to see that drawn out."
        ],
        [
            "Is multiple conditions.",
            "OK, so.",
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So heard a lot of talks today already and perhaps some of my first slides will be slightly repetitive so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, I hope not to to bore anyone.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of been mentioned, but perhaps not set explicitly so supervised grammar induction works great.",
                    "label": 1
                },
                {
                    "sent": "But it can be.",
                    "label": 0
                },
                {
                    "sent": "Expensive and time consuming, and it's not a general solution.",
                    "label": 0
                },
                {
                    "sent": "It's a very focused solution for particular.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dataset.",
                    "label": 0
                },
                {
                    "sent": "So unsupervised gram reduction is a little bit more general problem, and I think that's why we're all interested in it, and particularly I'm interested in this work and the kinds of knowledge that are required.",
                    "label": 0
                },
                {
                    "sent": "So as part of speech necessary to build into your model is hidden syntax, which I just think of this higher order constituents like NP and VP or those necessary is projectivity unnecessary assumption for what would be generally considered a projective language like English.",
                    "label": 0
                },
                {
                    "sent": "Prior distributions which have been mentioned before, you know how far away are your dependencies or whatnot and then semantics?",
                    "label": 1
                },
                {
                    "sent": "Does it do the words actually have to mean anything in some sense for you to do this successfully?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Although most of the work that we've seen today seems to suggest that they're not.",
                    "label": 0
                },
                {
                    "sent": "So the basic model that I'm going to talk about today is very very simple model, so that's probably good for a talk that's a little bit later on in the day, has no parts of speech, and I would argue it has some little bit of semantics which I think is kind of interesting and maybe a little bit different.",
                    "label": 0
                },
                {
                    "sent": "One big component of this model is minimum spanning tree parsing, which I think was introduced by Ryan MacDonald, and that's kind of nice for this because it finds an optimal.",
                    "label": 1
                },
                {
                    "sent": "Parse solution.",
                    "label": 1
                },
                {
                    "sent": "It's non projective and it has no hidden syntax, so that's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A fair amount about that at some point.",
                    "label": 0
                },
                {
                    "sent": "OK, so the key idea inspiration for this and it's been mentioned before I think I think increases talk is distributional analysis.",
                    "label": 0
                },
                {
                    "sent": "So elements distribution is the sum of its environments and if you consider that environment as an array of Co occurrence, then you can use that kind of distributional concepts to build higher levels of structure.",
                    "label": 1
                },
                {
                    "sent": "So the way that I consider this is in terms of sub.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suitability so this is this is pretty old.",
                    "label": 0
                },
                {
                    "sent": "This goes back at least two.",
                    "label": 0
                },
                {
                    "sent": "At least to bro.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that if you have something like a pronoun and it can substitute for a determiner noun, then you can say oh that's a block.",
                    "label": 0
                },
                {
                    "sent": "I can substitute this one element for this bigram, and therefore this bigram is some sort of constituent.",
                    "label": 0
                },
                {
                    "sent": "In order to do that though, you need some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notion of context.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk.",
                    "label": 0
                },
                {
                    "sent": "Consider two different kinds of contexts.",
                    "label": 0
                },
                {
                    "sent": "When is a global context with no word order and the other is a local context that does have word order.",
                    "label": 0
                },
                {
                    "sent": "So this notion of global context would be familiar to anyone who has looked into information retrieval models.",
                    "label": 0
                },
                {
                    "sent": "This is more or less identical, So what you do to establish this kind of context is you look at every word and you just count how many times that word appears in.",
                    "label": 0
                },
                {
                    "sent": "Documents, so here I'm taking here in the next slide I'm taking document just to be a sentence which is perhaps a slightly unusual kind of document.",
                    "label": 0
                },
                {
                    "sent": "So the word boy occurs in Document 1.",
                    "label": 0
                },
                {
                    "sent": "One time that occurs in Document 2 one time.",
                    "label": 0
                },
                {
                    "sent": "You get the idea, you're just.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Counting the frequency.",
                    "label": 0
                },
                {
                    "sent": "The local context which is been used quite a bit in this kind of work.",
                    "label": 0
                },
                {
                    "sent": "You just look at the words of the right into the left of the target work, and you do a similar thing.",
                    "label": 0
                },
                {
                    "sent": "You just count frequencies so the target words are in black, hear the words to the left of that or in green so the occurs before boy one time boy occurs before likes.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that's not even in there, but you get the idea.",
                    "label": 0
                },
                {
                    "sent": "So for likes occurs after like one time and different people have used different words here across across the top.",
                    "label": 0
                },
                {
                    "sent": "Usually the most frequent words actually in this.",
                    "label": 0
                },
                {
                    "sent": "I don't do that at all.",
                    "label": 0
                },
                {
                    "sent": "I just use all of the words so I end up with extremely large matrices.",
                    "label": 0
                },
                {
                    "sent": "But I don't have to do any sort of opery fiddling with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Words to include.",
                    "label": 0
                },
                {
                    "sent": "OK, so one step that I use because I have these ridiculously large matrices is singular value decomposition and this is.",
                    "label": 0
                },
                {
                    "sent": "Used fairly often to compress a matrix into a smaller form.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you get a kind of Co occurrence analysis through this compression, and most importantly is that terms in this compressed space can become similar have similar representations even though they didn't necessarily Co occur.",
                    "label": 1
                },
                {
                    "sent": "So you know theoretically and ideally, so.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that you have a.",
                    "label": 0
                },
                {
                    "sent": "This actually shouldn't be equals.",
                    "label": 0
                },
                {
                    "sent": "This should be approximates, so you have this original matrix A, which is either a local or global kind of matrix that I showed before and you end up splitting it into these three matrices, where in fact these are a little bit a little bit smaller.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that too much, but just enough to kind of get you into the idea that you're compressing this, and this has been used by schutze to do part of speech induction.",
                    "label": 0
                },
                {
                    "sent": "And it's also been used in something called latent semantic analysis, which is a little bit of my inspiration for this, and LSA has been used for essay grading.",
                    "label": 0
                },
                {
                    "sent": "Comparing the meanings of various documents, entailment topic segmentation.",
                    "label": 0
                },
                {
                    "sent": "Do you want to end up with a question or?",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, actually.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the global is exactly LSA except.",
                    "label": 0
                },
                {
                    "sent": "Haven't gotten to this yet, but except that instead of just doing unigrams here, I'm actually doing unigrams and bigrams so now the matrix gets even.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's kind of that can be kind of a problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other components of this search because you can do these substitutions, right?",
                    "label": 0
                },
                {
                    "sent": "That's a really simple idea, but given a sentence, there are a lot of substitutions that you can do.",
                    "label": 0
                },
                {
                    "sent": "I mean you've got 5 words you could do a substitution with these or these, or these, or these and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you need a way to find the best substitutions.",
                    "label": 0
                },
                {
                    "sent": "And I forgot to mention that the way I'm doing these substitutions is with cosines.",
                    "label": 0
                },
                {
                    "sent": "So each word has a vector representation and then to compare 2 words I just compute the cosine between those two vector representations.",
                    "label": 0
                },
                {
                    "sent": "An for people who aren't familiar with that.",
                    "label": 0
                },
                {
                    "sent": "Basically you've got these two vectors and the cosine is just the angle between them, so it's a similarity measure that's independent of the length of the vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so to do this search I use this graph algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is what rank Donald used and it finds the minimum directed spanning tree of a graph and it just turns out that if you construct your graph in the right way, that is essentially a dependency parse tree.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of nice.",
                    "label": 0
                },
                {
                    "sent": "It's order N squared, which is also nice, but it requires a graph to operate on.",
                    "label": 1
                },
                {
                    "sent": "So I'm gonna spend a little bit of time.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about the graph.",
                    "label": 0
                },
                {
                    "sent": "And part of that is because I think if I didn't talk about how to populate the graph, then that part of the model would be completely opaque and I don't think it would be very useful and I'm going to spend a few slides talking about how the algorithm actually operates over the graph to create a minimum spanning tree.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you had a sentence like the boy sleeps, you would ideally like to get that kind of representation out of it.",
                    "label": 1
                },
                {
                    "sent": "I've got a dummy root node here.",
                    "label": 0
                },
                {
                    "sent": "Just to make things a little bit simpler in several respects, and you know, if you look at the graph representation for this, it would be great to have ones here indicating a link and perhaps zeros everywhere else to say that there was no link at all.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in my model, to actually create a graph like this in the first graph I just showed is what you're aiming for, not necessarily what you get.",
                    "label": 0
                },
                {
                    "sent": "You start off and you disallow all self heads.",
                    "label": 0
                },
                {
                    "sent": "So that's a little bit I guess, of built-in knowledge.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I don't know that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of debatable.",
                    "label": 0
                },
                {
                    "sent": "So you look at the boy.",
                    "label": 1
                },
                {
                    "sent": "And the boy is.",
                    "label": 0
                },
                {
                    "sent": "Probably something that's a curd in the corpus somewhere, so you've got a vector representation for the diagram.",
                    "label": 0
                },
                {
                    "sent": "The boy and I can explain what to do in data sparsity if for some reason you don't have vector representation.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the questions of somebody is interested or after.",
                    "label": 0
                },
                {
                    "sent": "And you do a search.",
                    "label": 0
                },
                {
                    "sent": "You do a search through all possible for through all unigrams.",
                    "label": 1
                },
                {
                    "sent": "In order to find the unigram that is closest to the boy.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of an EXO centric kind of search.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you're doing through the space.",
                    "label": 0
                },
                {
                    "sent": "OK, and then this part here, which I probably should put on a separate slide.",
                    "label": 0
                },
                {
                    "sent": "This uses the kind of linguistic idea of a head test.",
                    "label": 0
                },
                {
                    "sent": "So Zwicky and Hudson and a few others have talked about tests that you can do for linguistic heads that are based on the kind of semantic similarity of the words in question.",
                    "label": 0
                },
                {
                    "sent": "So this procedure uses that intuition.",
                    "label": 0
                },
                {
                    "sent": "So you've got a cosine here between this nearest unigram neighbor that you found.",
                    "label": 0
                },
                {
                    "sent": "And one of the elements here, which is boy.",
                    "label": 0
                },
                {
                    "sent": "Let's say that's .34.",
                    "label": 0
                },
                {
                    "sent": "And then you do the same thing between the nearest unigram neighbor and the other element of the bigram.",
                    "label": 0
                },
                {
                    "sent": "And let's say that that's.",
                    "label": 0
                },
                {
                    "sent": ".0 two will then, based on those, those two cosines you say?",
                    "label": 0
                },
                {
                    "sent": "Well, John is more like the boy.",
                    "label": 1
                },
                {
                    "sent": "Sorry John is more like boy than it is like the so boy must be the head of this constituent.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea.",
                    "label": 1
                },
                {
                    "sent": "Anyway, you use that information here to populate the graph, and basically that lets you put put these numbers in the right boxes so that.",
                    "label": 0
                },
                {
                    "sent": "This graph is just the words that are that one word dominates the other word.",
                    "label": 0
                },
                {
                    "sent": "So it's like the degree of confidence exposed and how likely one word dominates the other work.",
                    "label": 0
                },
                {
                    "sent": "But they're not strictly probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then you have the sleep switch boys broken up so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is this is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "And quite often in cases like this, because the semantically kind of weak in this representation sleeps would be the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So you go through the same price.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Major and you find.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sleeps is the head and then.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Boy sleeps OK. And at this point you have a little bit of a problem 'cause you have this dummy root, which is nice for other reasons, but you don't really have a meaningful way to compute these boxes in the graph representation.",
                    "label": 1
                },
                {
                    "sent": "So I do something very simple.",
                    "label": 0
                },
                {
                    "sent": "I basically make them all equally likely, so each word is equally likely to be the root and this is just this.",
                    "label": 0
                },
                {
                    "sent": "Value here is just an average of all the other values in it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know, there might be a smarter way of doing that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this graph here, which is fully connected graph with directed arrows.",
                    "label": 0
                },
                {
                    "sent": "This was the box that we just saw and this is more familiar graph like representation of the Senate.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to talk a little bit about the search procedure that you would use to solve this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The basic idea of this minimum spanning tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is that each each directed edge has a weight and each node in the graph greedily selects the incoming edge that has the highest weight.",
                    "label": 1
                },
                {
                    "sent": "So I've I've indicated these here in red, so if you look at this node sleeps, you can see that's got this inbound edge.",
                    "label": 0
                },
                {
                    "sent": "That's .3 one that's .1 and this one.",
                    "label": 0
                },
                {
                    "sent": "That's .306.",
                    "label": 0
                },
                {
                    "sent": "So this is the highest inbound edge.",
                    "label": 0
                },
                {
                    "sent": "And likewise you can do that procedure with the with the other node.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the case that you do this and there are no cycles which would.",
                    "label": 0
                },
                {
                    "sent": "Which would be that?",
                    "label": 0
                },
                {
                    "sent": "Then you've essentially just solved solve the graph and you're done.",
                    "label": 1
                },
                {
                    "sent": "Which is nice, but it doesn't always happen that way.",
                    "label": 0
                },
                {
                    "sent": "So what if you get?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A cycle.",
                    "label": 0
                },
                {
                    "sent": "So the idea for dealing with cycles is that.",
                    "label": 0
                },
                {
                    "sent": "You can actually recursively call this algorithm.",
                    "label": 0
                },
                {
                    "sent": "On a graph that combines the nodes.",
                    "label": 0
                },
                {
                    "sent": "That make up the cycle.",
                    "label": 0
                },
                {
                    "sent": "So if you've got two nodes that make up a cycle, you collapse them into a single node.",
                    "label": 0
                },
                {
                    "sent": "You rewire the graph in a way that I'll talk through in just a second, and then you run the algorithm on it again and then eventually you're going to find a point where you've got a graph with no cycles, and then you just pop back up each time you rewire the graph.",
                    "label": 0
                },
                {
                    "sent": "According to the the rewiring decisions that you may.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "How we've rewired the graph?",
                    "label": 1
                },
                {
                    "sent": "OK, so the idea here is that route to the sleeps is .906 because the maximum path from root to sleeps to the OR route to the sleeps.",
                    "label": 0
                },
                {
                    "sent": "Was point 906, so I think I could back up a few slides and show that exactly, but I don't think it's entirely necessary.",
                    "label": 0
                },
                {
                    "sent": "And also with boy.",
                    "label": 0
                },
                {
                    "sent": "Boy to the sleeps.",
                    "label": 0
                },
                {
                    "sent": "The path from boy.",
                    "label": 1
                },
                {
                    "sent": "To the sleeps or boyda sleeps to the that's maximum.",
                    "label": 0
                },
                {
                    "sent": "Is .9, so that's the weight of the edge that you give that.",
                    "label": 0
                },
                {
                    "sent": "And then the the boy exists and sleeps.",
                    "label": 0
                },
                {
                    "sent": "The boy exists 'cause this is a fully connected graph.",
                    "label": 0
                },
                {
                    "sent": "So in order to determine this rewired edge, you just take one of those.",
                    "label": 0
                },
                {
                    "sent": "That's the highest.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you run the algorithm on this again, you find that.",
                    "label": 0
                },
                {
                    "sent": "That you solved it, just that greedy, highest incoming edge.",
                    "label": 0
                },
                {
                    "sent": "Procedure you solved it?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you can rewire the graph using this same kinds of ideas.",
                    "label": 1
                },
                {
                    "sent": "So before we rewired route to the sleeps, that path actually went from route to sleeps to the.",
                    "label": 0
                },
                {
                    "sent": "So you just add that edge back.",
                    "label": 0
                },
                {
                    "sent": "And likewise when we had the single edge from the sleeps to boy, it was actually from sleeps, the boy, not from the boy.",
                    "label": 0
                },
                {
                    "sent": "So you just have to keep track of where all the ledges were, that that you had before.",
                    "label": 0
                },
                {
                    "sent": "So funnily enough I don't know if you remember, but this is the same graph from the beginning when it didn't have a cycle.",
                    "label": 0
                },
                {
                    "sent": "And I actually created a cycle and related one of the edges.",
                    "label": 0
                },
                {
                    "sent": "So this shows a little bit that this is reasonably sensitive, not sensitive to being perturbed.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of nice property, and my thought in using this was originally that if you could get some sort of distinction between.",
                    "label": 0
                },
                {
                    "sent": "You know proper constituents and improper constituents, noticed issuance that this would be able to use that information to create a proper proper parts.",
                    "label": 0
                },
                {
                    "sent": "So that was the basic idea.",
                    "label": 0
                },
                {
                    "sent": "But in my model I don't explicitly model constituents versus Dist issuance.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that's perhaps a weakness.",
                    "label": 0
                },
                {
                    "sent": "I don't use a filter or anything like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "67 sweet take a long drink water.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I think some of these datasets have been mentioned multiple times, so I won't talk about it too much.",
                    "label": 0
                },
                {
                    "sent": "I use the Pin tree bank.",
                    "label": 0
                },
                {
                    "sent": "Wall Street Journal 10, which I think that Dan Klein and Chris Manning kind of named it's the part of the treebank where there's 10 words or less.",
                    "label": 0
                },
                {
                    "sent": "No punctuation except for I think$ something like that.",
                    "label": 0
                },
                {
                    "sent": "Anyway, or it's 4% anyway, and then use the columns head rules to transform it into the bracketed treebank into dependencies, which is what I'm I'm using, and that's just just for testing.",
                    "label": 0
                },
                {
                    "sent": "But to do the actual training to acquire the amount of information.",
                    "label": 0
                },
                {
                    "sent": "Our frequency counts if you will.",
                    "label": 0
                },
                {
                    "sent": "In order to do this I use a much bigger corpus, which is the Wall Street Journal and the North American News Corpus Year 1994, and that was about as much as my computer could handle, so that's about 10,000,000 words in 460,000 sentences, so some of the matrix matrices were very large.",
                    "label": 0
                },
                {
                    "sent": "You know, I think one was like 'cause you know, grams and diagrams like 1.2 million by.",
                    "label": 0
                },
                {
                    "sent": "800,000 or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, it's quite painful.",
                    "label": 0
                },
                {
                    "sent": "You have to use a special kind of SVD to do that.",
                    "label": 0
                },
                {
                    "sent": "That writes things out to disk and a big block.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, unless you have a super computer which I don't have.",
                    "label": 0
                },
                {
                    "sent": "OK, so two baselines, I guess, actually three 'cause there's the random baseline.",
                    "label": 0
                },
                {
                    "sent": "There's the right branching baseline in the left branching baseline.",
                    "label": 1
                },
                {
                    "sent": "So those will appear on the next slide.",
                    "label": 0
                },
                {
                    "sent": "I use the sign test to determine statistical significance.",
                    "label": 0
                },
                {
                    "sent": "All the values I'm going to show you were all statistically significant and one thing I will also say before I share the next slide is that all of my numbers are for directed dependencies, so they notice that if you remember the numbers before, you'll notice that there are a lot lower, and in fact the whole kind of range has kind of been squashed a little bit, so even the random baseline is quite a lot lower.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, and of course there's the prior knowledge that was very so.",
                    "label": 1
                },
                {
                    "sent": "You know, the context.",
                    "label": 0
                },
                {
                    "sent": "Is it global or local?",
                    "label": 0
                },
                {
                    "sent": "Essentially?",
                    "label": 0
                },
                {
                    "sent": "Do we even care about word order?",
                    "label": 0
                },
                {
                    "sent": "Projectivity.",
                    "label": 0
                },
                {
                    "sent": "Which I didn't actually introduce today.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should have done that.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Maybe that can be a question and and so like English is a projective language and non projective just means that you're not making any sort of assumptions about the way that dependencies.",
                    "label": 0
                },
                {
                    "sent": "The dependencies between words relate to dependencies between other words.",
                    "label": 0
                },
                {
                    "sent": "And then prior distributions.",
                    "label": 0
                },
                {
                    "sent": "So really good guess to put into your model.",
                    "label": 0
                },
                {
                    "sent": "I think that that Chris mentioned in his talk is to say Oh well, most attachments are local induction or anything like that.",
                    "label": 0
                },
                {
                    "sent": "I'm using this LSA like semantics is was noted.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can see for that condition the baseline is actually quite low, it's 14.2%.",
                    "label": 0
                },
                {
                    "sent": "You can see here that.",
                    "label": 0
                },
                {
                    "sent": "Local and global with no additional prior information are not better than the right branching baseline.",
                    "label": 0
                },
                {
                    "sent": "But you can see that when you add either productivity or some knowledge of prior distributions, they both beat the right branching baseline and nothing beats the left branching baseline, so this is clearly not as high performing as I think.",
                    "label": 1
                },
                {
                    "sent": "All of the models in this that we talked about today, although it's significantly more simple.",
                    "label": 0
                },
                {
                    "sent": "One thing that I think is really interesting about this, maybe have this on the next slide.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about it now.",
                    "label": 0
                },
                {
                    "sent": "Is that the global context did so well, so just ignoring word order, it still be the right branching baseline, which is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "The differences are very small.",
                    "label": 0
                },
                {
                    "sent": "There's really little additive effects between.",
                    "label": 0
                },
                {
                    "sent": "Objectivity in the prior distribution, which may not be that surprising.",
                    "label": 0
                },
                {
                    "sent": "'cause perhaps they bring to bear the same kinds of information, but I think it's kind of nice to see that drawn out.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is multiple conditions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}