{
    "id": "7py3jsjmvygb4n35pf36rafdhdlmopct",
    "title": "Real-time RDF extraction from unstructured data streams",
    "info": {
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Databases",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_ngonga_ngomo_data_streams/",
    "segmentation": [
        [
            "So hi everybody, my name is Axel from the University of Leipzig in Germany and I'm going to talk about RDF life news, real time RDF extraction from unstructured data, just to warn you I'm not going to talk about C sparkle or sequels or anything like that.",
            "Here we're going to deal with unstructured data.",
            "So."
        ],
        [
            "The motivation behind our work was actually quite simple and the idea was that.",
            "A large fraction of the cloud, as has been shown in previous work, is actually static in nature and most of the content that you will find in the cloud is encircled, encyclopaedic, in nature.",
            "That is, if you look at the largest datasets such as linked ECG or link Geo data which have billions of triples, the content that you find there are statements that basically descry bicycle product knowledge that things that are just factual but not but not necessarily timely or current, so they do not necessarily reflect what's happening.",
            "In the world right now, now there are also live endpoints, such as to BPD, alive or link Judah life and so on.",
            "But most of these data sets are actually extracted from semi structured sources.",
            "So what?"
        ],
        [
            "We wanted to have is to provide timely data in RDF that allows to answer questions such as give me all news of the last week from the New York Times pertaining to the Director of Nokia.",
            "So those are the kind of questions we were interested in and so far there was no data set that actually allowed answering such questions."
        ],
        [
            "So I gave the whole thing some thought and we asked ourselves what do we need to do to be actually able to create such a data set.",
            "Now obviously we need to be able to run 24/7, so we need to be able to get unstructured data stream, process them, throw them into RDF, and do that continuously.",
            "And we need to scale to large amounts of data.",
            "Obviously we want to be able to process a lot of unstructured data streams in parallel.",
            "One obviously 200 unstructured data, because that's the data source that we instructed interested in.",
            "Obviously between H and 85% of the data on the web is unstructured, so that's quite an important source of data.",
            "And we wanted extraction to be precise.",
            "So basically we had precision over recall in our experiments.",
            "And finally we wanted to deal or to look at the whole thing from the perspective of open information extraction.",
            "What we realize why we were doing this work is that the kind of relations that expressed especially in news differ from what you would find in knowledge basis such as DPJ in order more encyclopedic knowledge bases.",
            "So we actually didn't want to give the system perceived kind of predicates I was supposed to extract.",
            "Rather, we wanted to learn the predicates directly from the.",
            "Data regarded and what I'm going to do is I'm going to give you a short overview of the different steps.",
            "I'm not going to go into too much too much detail if you have questions, I'll be happy to answer."
        ],
        [
            "Leader, so this basically gives you an overview of the architecture of the system.",
            "We basically start with web pages or with streams that actually come from the web, and the first step is obviously data acquisition, so we need to gather the data streams and throw them somewhere.",
            "When you do that with unstructured data streams, especially from the web, you realize that you have plenty of duplicates.",
            "You have plenty of things that occur in different data streams at the same time, and that I basically just repetitions.",
            "So we want to get rid of that.",
            "That's the reason why we needed application step and then we have some clean data out of that data, then renewed to search for patterns patterns actually stand for certain predicates which search for patterns, refine the patterns that we need to group them to actually say these.",
            "Patterns these natural language patterns actually expressed the same relation, and once we found the relation, we can then actually generate RDF.",
            "One thing that we also wanted to do is to be able to track provenance.",
            "I'll show you how we do that using the NLP interchange format and basically once you have all that data, that is the patterns and the sources for the triples as well as triples, we can actually even reuse that data to either tag information that is on the web or basically answer questions.",
            "Lack of pointed out.",
            "Before, so I'm going to describe each of these steps now.",
            "All."
        ],
        [
            "Our basic assumption of former model was that we have a bunch of unstructured data sources or sources that generate unstructured information that emit continuous data streams.",
            "And that these data streams consist of atomic elements with respect to an LP.",
            "What it means is, we assume that we are given some paragraphs or some text, and we have atomic elements that are sentences, and we want to work at atomic level.",
            "And a good example for good sources of data streams are RSS feeds on the web."
        ],
        [
            "Officially.",
            "And the first step of the data acquisition was obviously to gather the information we used time slices.",
            "So we basically we started at a time T and got it got everything that was emitted by our sources up to time T + D une recall.",
            "This basically or time slice or a time window.",
            "And this is basically where you can tune the system to be either real time or to gather information for a longer time.",
            "So if you wondered system to behave in a real time manner, you would set this window to be really small so that it gathers information, say second.",
            "And then processes it and generates the RDF.",
            "If you're happy with the longer delay, you can basically set the value of D to be larger.",
            "In our experiments we had D4 to be approximately 2 hours.",
            "Once we've got all the information, we then have obviously to find the atomic elements that sentence splitting from the NLP POV, and then we have the sentences as atoms and we then perform a string based string similarity based application of the sentences to have the unique sentences in our data set."
        ],
        [
            "OK, so with that we've covered the first 2 steps.",
            "How do we then search for patterns and how do we refine patterns?"
        ],
        [
            "Now here we applied part of speech tagging as well as named entity recognition to basically find the entities that were mentioned within the text and we looked for sections of text or fragments of text that way between these named entities and assume these things models must actually expressed some form of relations.",
            "Obviously we make the implicit.",
            "Assumption here that we have an SPO language, but it is quite easy to turn this to other types of languages such as Japanese where the verb occurs at the end of the sentence.",
            "So out of that particular piece of text, we then get some form of a pattern that is something like, is the manager of the notebook that most expressed some form of relation and we know it is between something that is called Smith and some other thing that is called ABC.",
            "We obviously filtered out a bunch of such patterns by saying that they must contain at least a verb or noun that they should not just be made up of stop words and so on and so forth there is.",
            "Plenty of work on this topic.",
            "I'll suggest that you look at the reverb patterns if you're really interested on the exact details of why patterns should have a certain shape and we only took the top 10% most frequent patterns in our experiments with."
        ],
        [
            "The top 1%.",
            "Now in the next step, now that we have these strings and all these patterns, we now need to disambiguate the support set and the support that is basically the set of strings of pairs of strings that are related to a certain pattern.",
            "And the advantage here is that we can actually make the entity names complete once we have the ur eyes for the strings, we can actually take the ADFS labels of this this ur eyes and say these are actually the full names of the entities that we're interested in.",
            "But on top of it.",
            "We can actually also generate range and domain expressions for the properties, because out of the right we can see the types that we have and obviously then generate domains and range and arrange information using simple forward inference.",
            "Just to give you an idea of how we do the disambiguation, we combine four different measures, that is, local and global context.",
            "Their previous core of their eyes, as well as the similarity between the string that we found and the label of the URS you were interested in."
        ],
        [
            "OK, so now we have patterns.",
            "Question is how do we refine them?",
            "How do we find bunch of patterns that actually express the same?"
        ],
        [
            "Relation.",
            "So for that purpose we use clustering and the idea here was that we can merge patterns to group of patterns that express a certain relations and these group of patterns stand false or predicate that for which we still need to find a label."
        ],
        [
            "Now, one could assume that one could simply use string similarity to do this work.",
            "Basically, to find similar patterns, but that fails very often in the news data or in actually stream data that is on structured for this purpose with those used a combination of word net and string similarity.",
            "One more say that we preprocessed the patterns that we gathered from the unstructured data by using lemmatization and stop word removal.",
            "So something like is.",
            "The manager of became B manager.",
            "And then we apply clustering to the similarity graph that we could build by simply computing the similarity of the different patterns that occurred in the text.",
            "The reason why we use that particular rhythm is that it is parameter free, so we could just take the data, dump it in there, and get clusters of patterns.",
            "Now, once we had the clusters of patterns, what we're interested in is actually was actually finding a label for these clusters, and then we simply used a majority vote within the portion of the graph that was contained in the cluster, another that we generated label."
        ],
        [
            "So now we have refined the whole thing.",
            "We have clusters, we have labels or we need to do is actually generate."
        ],
        [
            "RDF and for that through our right is ambiguation.",
            "We're ready, have the your eyes for the resources, we have a predicate, and we also have a label for it.",
            "That is, we can give it a UI so we can actually generate triples.",
            "We actually also wanted to have the meta data around the extraction.",
            "That is basically where did we get the data from, so the sources.",
            "For this we use new.",
            "For sure it looks like in the 2nd and also linked the data that we generated two different data sources.",
            "Here we use DB pedia."
        ],
        [
            "And this is basically the kind of data that you get out of texts.",
            "What you see up there is basically the description of the property that was extracted automatically.",
            "We have their director Ralph and we see that the preferred label is director off, because that's basically the label that won the majority vote, but will also generate alternative labels which are all the different strings that we found in the unstructured data.",
            "And we were also able to see that is sexual into DP director.",
            "We also encode the facts that we found, for example, that Rolf Heuer is the director of CERN, and we have provenance tracking information where we say that we found something that is a person and basically that relates to a person and relates to a certain entity, and that entity is the entity of higher, and so on and so forth.",
            "We basically give out all the provenance information so that other systems can actually reuse the information that we generated.",
            "For further purposes."
        ],
        [
            "3 minutes left to tell you about the experiments that we carried up, so we wanted to know how good is on your right is ambiguation.",
            "Whether the clustering actually works, the kind of quality that we achieve with the RDF that we generated, and whether we scale so well that near real time audio generation."
        ],
        [
            "Is possible what we did is we crawled up 1457 RSS feeds for 76 hours.",
            "We had 30 time slices of two hours.",
            "Each and we had an average of 26.7 sentences, but article and 3000 articles per timeslice we generated, we had three sets of data.",
            "The one represents that doubles everything.",
            "The 10% said I was just 10% of the data that was generated in each slice and one person."
        ],
        [
            "And we respected you.",
            "Alright, disambiguation were able to achieve an F measure of 0.665 which is building up 65.5%.",
            "We compared our results with the results of Ida, which is standard.",
            "You are righteous ambiguation tool and there we were able to outperform it by 6.5% basically."
        ],
        [
            "We also measure the quality of the pattern clustering against manually clustered data.",
            "For this we use measures that are used commonly in clustering, that is sensitivity with something like recall and positive predictive value, which is akin to precision as well as accuracy.",
            "And here we achieved an accuracy of 82.5%.",
            "We also compared it with what happened if we use only the string similarity or only the word net similarity to measure the similarity of patterns and we could see that word net similarity actually contributed.",
            "Most to a score, but still able to outperform it by combining it with this string similarity."
        ],
        [
            "No, but we are also interested in was the accuracy of the quality of the RDF that we generated and what the results show is that we are somewhere between 86 and 90% accuracy which was quite pleasing."
        ],
        [
            "And we also wanted to figure out whether we scaled so we basically run our experiments on the different data set sizes and we were able to process the two hour time slices in 20 minutes.",
            "So we still had plenty of buffer there to wait for the next data slice."
        ],
        [
            "Good conclusions.",
            "Presented RDF like news IF framework for the extraction of idea from unstructured data that basically sums up the evaluation results and the interesting thing here is that the different processing steps that we have can be easily run in parallel and scaled out.",
            "Which basically means that this system given enough resources can scale to very very large amounts of unstructured data.",
            "What we don't have yet and what we want to deal with soon is dealing with data type properties as well as we verification that is basically want to be.",
            "To model things such as Google said that Motorola contributed to blah blah blah and we also want to integrate a support for temporal logics I have."
        ],
        [
            "6 seconds left to say thank you very much and if you have any questions just let me know."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hi everybody, my name is Axel from the University of Leipzig in Germany and I'm going to talk about RDF life news, real time RDF extraction from unstructured data, just to warn you I'm not going to talk about C sparkle or sequels or anything like that.",
                    "label": 1
                },
                {
                    "sent": "Here we're going to deal with unstructured data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The motivation behind our work was actually quite simple and the idea was that.",
                    "label": 0
                },
                {
                    "sent": "A large fraction of the cloud, as has been shown in previous work, is actually static in nature and most of the content that you will find in the cloud is encircled, encyclopaedic, in nature.",
                    "label": 0
                },
                {
                    "sent": "That is, if you look at the largest datasets such as linked ECG or link Geo data which have billions of triples, the content that you find there are statements that basically descry bicycle product knowledge that things that are just factual but not but not necessarily timely or current, so they do not necessarily reflect what's happening.",
                    "label": 0
                },
                {
                    "sent": "In the world right now, now there are also live endpoints, such as to BPD, alive or link Judah life and so on.",
                    "label": 0
                },
                {
                    "sent": "But most of these data sets are actually extracted from semi structured sources.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We wanted to have is to provide timely data in RDF that allows to answer questions such as give me all news of the last week from the New York Times pertaining to the Director of Nokia.",
                    "label": 0
                },
                {
                    "sent": "So those are the kind of questions we were interested in and so far there was no data set that actually allowed answering such questions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I gave the whole thing some thought and we asked ourselves what do we need to do to be actually able to create such a data set.",
                    "label": 0
                },
                {
                    "sent": "Now obviously we need to be able to run 24/7, so we need to be able to get unstructured data stream, process them, throw them into RDF, and do that continuously.",
                    "label": 1
                },
                {
                    "sent": "And we need to scale to large amounts of data.",
                    "label": 0
                },
                {
                    "sent": "Obviously we want to be able to process a lot of unstructured data streams in parallel.",
                    "label": 0
                },
                {
                    "sent": "One obviously 200 unstructured data, because that's the data source that we instructed interested in.",
                    "label": 1
                },
                {
                    "sent": "Obviously between H and 85% of the data on the web is unstructured, so that's quite an important source of data.",
                    "label": 1
                },
                {
                    "sent": "And we wanted extraction to be precise.",
                    "label": 0
                },
                {
                    "sent": "So basically we had precision over recall in our experiments.",
                    "label": 0
                },
                {
                    "sent": "And finally we wanted to deal or to look at the whole thing from the perspective of open information extraction.",
                    "label": 0
                },
                {
                    "sent": "What we realize why we were doing this work is that the kind of relations that expressed especially in news differ from what you would find in knowledge basis such as DPJ in order more encyclopedic knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "So we actually didn't want to give the system perceived kind of predicates I was supposed to extract.",
                    "label": 0
                },
                {
                    "sent": "Rather, we wanted to learn the predicates directly from the.",
                    "label": 0
                },
                {
                    "sent": "Data regarded and what I'm going to do is I'm going to give you a short overview of the different steps.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into too much too much detail if you have questions, I'll be happy to answer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leader, so this basically gives you an overview of the architecture of the system.",
                    "label": 0
                },
                {
                    "sent": "We basically start with web pages or with streams that actually come from the web, and the first step is obviously data acquisition, so we need to gather the data streams and throw them somewhere.",
                    "label": 0
                },
                {
                    "sent": "When you do that with unstructured data streams, especially from the web, you realize that you have plenty of duplicates.",
                    "label": 0
                },
                {
                    "sent": "You have plenty of things that occur in different data streams at the same time, and that I basically just repetitions.",
                    "label": 0
                },
                {
                    "sent": "So we want to get rid of that.",
                    "label": 0
                },
                {
                    "sent": "That's the reason why we needed application step and then we have some clean data out of that data, then renewed to search for patterns patterns actually stand for certain predicates which search for patterns, refine the patterns that we need to group them to actually say these.",
                    "label": 0
                },
                {
                    "sent": "Patterns these natural language patterns actually expressed the same relation, and once we found the relation, we can then actually generate RDF.",
                    "label": 0
                },
                {
                    "sent": "One thing that we also wanted to do is to be able to track provenance.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how we do that using the NLP interchange format and basically once you have all that data, that is the patterns and the sources for the triples as well as triples, we can actually even reuse that data to either tag information that is on the web or basically answer questions.",
                    "label": 0
                },
                {
                    "sent": "Lack of pointed out.",
                    "label": 0
                },
                {
                    "sent": "Before, so I'm going to describe each of these steps now.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our basic assumption of former model was that we have a bunch of unstructured data sources or sources that generate unstructured information that emit continuous data streams.",
                    "label": 1
                },
                {
                    "sent": "And that these data streams consist of atomic elements with respect to an LP.",
                    "label": 1
                },
                {
                    "sent": "What it means is, we assume that we are given some paragraphs or some text, and we have atomic elements that are sentences, and we want to work at atomic level.",
                    "label": 0
                },
                {
                    "sent": "And a good example for good sources of data streams are RSS feeds on the web.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Officially.",
                    "label": 0
                },
                {
                    "sent": "And the first step of the data acquisition was obviously to gather the information we used time slices.",
                    "label": 0
                },
                {
                    "sent": "So we basically we started at a time T and got it got everything that was emitted by our sources up to time T + D une recall.",
                    "label": 1
                },
                {
                    "sent": "This basically or time slice or a time window.",
                    "label": 0
                },
                {
                    "sent": "And this is basically where you can tune the system to be either real time or to gather information for a longer time.",
                    "label": 0
                },
                {
                    "sent": "So if you wondered system to behave in a real time manner, you would set this window to be really small so that it gathers information, say second.",
                    "label": 0
                },
                {
                    "sent": "And then processes it and generates the RDF.",
                    "label": 0
                },
                {
                    "sent": "If you're happy with the longer delay, you can basically set the value of D to be larger.",
                    "label": 0
                },
                {
                    "sent": "In our experiments we had D4 to be approximately 2 hours.",
                    "label": 0
                },
                {
                    "sent": "Once we've got all the information, we then have obviously to find the atomic elements that sentence splitting from the NLP POV, and then we have the sentences as atoms and we then perform a string based string similarity based application of the sentences to have the unique sentences in our data set.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so with that we've covered the first 2 steps.",
                    "label": 0
                },
                {
                    "sent": "How do we then search for patterns and how do we refine patterns?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here we applied part of speech tagging as well as named entity recognition to basically find the entities that were mentioned within the text and we looked for sections of text or fragments of text that way between these named entities and assume these things models must actually expressed some form of relations.",
                    "label": 1
                },
                {
                    "sent": "Obviously we make the implicit.",
                    "label": 0
                },
                {
                    "sent": "Assumption here that we have an SPO language, but it is quite easy to turn this to other types of languages such as Japanese where the verb occurs at the end of the sentence.",
                    "label": 1
                },
                {
                    "sent": "So out of that particular piece of text, we then get some form of a pattern that is something like, is the manager of the notebook that most expressed some form of relation and we know it is between something that is called Smith and some other thing that is called ABC.",
                    "label": 0
                },
                {
                    "sent": "We obviously filtered out a bunch of such patterns by saying that they must contain at least a verb or noun that they should not just be made up of stop words and so on and so forth there is.",
                    "label": 1
                },
                {
                    "sent": "Plenty of work on this topic.",
                    "label": 0
                },
                {
                    "sent": "I'll suggest that you look at the reverb patterns if you're really interested on the exact details of why patterns should have a certain shape and we only took the top 10% most frequent patterns in our experiments with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The top 1%.",
                    "label": 0
                },
                {
                    "sent": "Now in the next step, now that we have these strings and all these patterns, we now need to disambiguate the support set and the support that is basically the set of strings of pairs of strings that are related to a certain pattern.",
                    "label": 0
                },
                {
                    "sent": "And the advantage here is that we can actually make the entity names complete once we have the ur eyes for the strings, we can actually take the ADFS labels of this this ur eyes and say these are actually the full names of the entities that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "But on top of it.",
                    "label": 0
                },
                {
                    "sent": "We can actually also generate range and domain expressions for the properties, because out of the right we can see the types that we have and obviously then generate domains and range and arrange information using simple forward inference.",
                    "label": 0
                },
                {
                    "sent": "Just to give you an idea of how we do the disambiguation, we combine four different measures, that is, local and global context.",
                    "label": 0
                },
                {
                    "sent": "Their previous core of their eyes, as well as the similarity between the string that we found and the label of the URS you were interested in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have patterns.",
                    "label": 0
                },
                {
                    "sent": "Question is how do we refine them?",
                    "label": 0
                },
                {
                    "sent": "How do we find bunch of patterns that actually express the same?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relation.",
                    "label": 0
                },
                {
                    "sent": "So for that purpose we use clustering and the idea here was that we can merge patterns to group of patterns that express a certain relations and these group of patterns stand false or predicate that for which we still need to find a label.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, one could assume that one could simply use string similarity to do this work.",
                    "label": 0
                },
                {
                    "sent": "Basically, to find similar patterns, but that fails very often in the news data or in actually stream data that is on structured for this purpose with those used a combination of word net and string similarity.",
                    "label": 0
                },
                {
                    "sent": "One more say that we preprocessed the patterns that we gathered from the unstructured data by using lemmatization and stop word removal.",
                    "label": 1
                },
                {
                    "sent": "So something like is.",
                    "label": 1
                },
                {
                    "sent": "The manager of became B manager.",
                    "label": 0
                },
                {
                    "sent": "And then we apply clustering to the similarity graph that we could build by simply computing the similarity of the different patterns that occurred in the text.",
                    "label": 0
                },
                {
                    "sent": "The reason why we use that particular rhythm is that it is parameter free, so we could just take the data, dump it in there, and get clusters of patterns.",
                    "label": 0
                },
                {
                    "sent": "Now, once we had the clusters of patterns, what we're interested in is actually was actually finding a label for these clusters, and then we simply used a majority vote within the portion of the graph that was contained in the cluster, another that we generated label.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have refined the whole thing.",
                    "label": 0
                },
                {
                    "sent": "We have clusters, we have labels or we need to do is actually generate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RDF and for that through our right is ambiguation.",
                    "label": 0
                },
                {
                    "sent": "We're ready, have the your eyes for the resources, we have a predicate, and we also have a label for it.",
                    "label": 0
                },
                {
                    "sent": "That is, we can give it a UI so we can actually generate triples.",
                    "label": 0
                },
                {
                    "sent": "We actually also wanted to have the meta data around the extraction.",
                    "label": 0
                },
                {
                    "sent": "That is basically where did we get the data from, so the sources.",
                    "label": 0
                },
                {
                    "sent": "For this we use new.",
                    "label": 0
                },
                {
                    "sent": "For sure it looks like in the 2nd and also linked the data that we generated two different data sources.",
                    "label": 0
                },
                {
                    "sent": "Here we use DB pedia.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is basically the kind of data that you get out of texts.",
                    "label": 0
                },
                {
                    "sent": "What you see up there is basically the description of the property that was extracted automatically.",
                    "label": 0
                },
                {
                    "sent": "We have their director Ralph and we see that the preferred label is director off, because that's basically the label that won the majority vote, but will also generate alternative labels which are all the different strings that we found in the unstructured data.",
                    "label": 0
                },
                {
                    "sent": "And we were also able to see that is sexual into DP director.",
                    "label": 0
                },
                {
                    "sent": "We also encode the facts that we found, for example, that Rolf Heuer is the director of CERN, and we have provenance tracking information where we say that we found something that is a person and basically that relates to a person and relates to a certain entity, and that entity is the entity of higher, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "We basically give out all the provenance information so that other systems can actually reuse the information that we generated.",
                    "label": 0
                },
                {
                    "sent": "For further purposes.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3 minutes left to tell you about the experiments that we carried up, so we wanted to know how good is on your right is ambiguation.",
                    "label": 0
                },
                {
                    "sent": "Whether the clustering actually works, the kind of quality that we achieve with the RDF that we generated, and whether we scale so well that near real time audio generation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is possible what we did is we crawled up 1457 RSS feeds for 76 hours.",
                    "label": 1
                },
                {
                    "sent": "We had 30 time slices of two hours.",
                    "label": 0
                },
                {
                    "sent": "Each and we had an average of 26.7 sentences, but article and 3000 articles per timeslice we generated, we had three sets of data.",
                    "label": 0
                },
                {
                    "sent": "The one represents that doubles everything.",
                    "label": 0
                },
                {
                    "sent": "The 10% said I was just 10% of the data that was generated in each slice and one person.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we respected you.",
                    "label": 0
                },
                {
                    "sent": "Alright, disambiguation were able to achieve an F measure of 0.665 which is building up 65.5%.",
                    "label": 0
                },
                {
                    "sent": "We compared our results with the results of Ida, which is standard.",
                    "label": 0
                },
                {
                    "sent": "You are righteous ambiguation tool and there we were able to outperform it by 6.5% basically.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also measure the quality of the pattern clustering against manually clustered data.",
                    "label": 1
                },
                {
                    "sent": "For this we use measures that are used commonly in clustering, that is sensitivity with something like recall and positive predictive value, which is akin to precision as well as accuracy.",
                    "label": 1
                },
                {
                    "sent": "And here we achieved an accuracy of 82.5%.",
                    "label": 0
                },
                {
                    "sent": "We also compared it with what happened if we use only the string similarity or only the word net similarity to measure the similarity of patterns and we could see that word net similarity actually contributed.",
                    "label": 0
                },
                {
                    "sent": "Most to a score, but still able to outperform it by combining it with this string similarity.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, but we are also interested in was the accuracy of the quality of the RDF that we generated and what the results show is that we are somewhere between 86 and 90% accuracy which was quite pleasing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also wanted to figure out whether we scaled so we basically run our experiments on the different data set sizes and we were able to process the two hour time slices in 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "So we still had plenty of buffer there to wait for the next data slice.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good conclusions.",
                    "label": 0
                },
                {
                    "sent": "Presented RDF like news IF framework for the extraction of idea from unstructured data that basically sums up the evaluation results and the interesting thing here is that the different processing steps that we have can be easily run in parallel and scaled out.",
                    "label": 1
                },
                {
                    "sent": "Which basically means that this system given enough resources can scale to very very large amounts of unstructured data.",
                    "label": 0
                },
                {
                    "sent": "What we don't have yet and what we want to deal with soon is dealing with data type properties as well as we verification that is basically want to be.",
                    "label": 1
                },
                {
                    "sent": "To model things such as Google said that Motorola contributed to blah blah blah and we also want to integrate a support for temporal logics I have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "6 seconds left to say thank you very much and if you have any questions just let me know.",
                    "label": 0
                }
            ]
        }
    }
}