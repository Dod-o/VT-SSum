{
    "id": "c7pmbi3v46yf5kz47vhel7x3ftaoyvho",
    "title": "Scalable Text and Link Analysis with Mixed-Topic Link Models",
    "info": {
        "author": [
            "Yaojia Zhu, Department of Computer Science, University of New Mexico"
        ],
        "published": "Sept. 27, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2013_zhu_link_models/",
    "segmentation": [
        [
            "Yeah, true.",
            "I'm contraption tree student at University of New Mexico so today I will talk about the mix topic link models that can be used to analyze checks and links are very efficiently."
        ],
        [
            "So this is general form of the problem.",
            "We have some nerves, and for each node we have the rich information.",
            "These are node attributes and also we have the pairwise relationship between the nodes.",
            "These other links.",
            "So basically we have a network and for each node we also have the attributes.",
            "So the learning task can be detecting hidden commit structures in the network or predicting node attributes and links."
        ],
        [
            "So in our work we are focusing on a more specific problem, just like in this picture we have some document citation networks.",
            "These networks the node corresponds to document and attributes are the text, the pairwise relationship, the citation ratio, or the hyperlinks.",
            "So the learning tasks include document topic detection and link prediction.",
            "I'll go into develop models and algorithms which use both text and link information to classified documents by topic and predict missing links with higher accuracy and better scalability."
        ],
        [
            "So why we are interested in this problem?",
            "So previously we are working on models to detect community structures in network by only using the topical information.",
            "Actually this is true story.",
            "So two years ago we are working on patent Citation network.",
            "All the nodes in the network are the patterns that they are related to, the city technology and there is one pattern which is the original pattern for citian.",
            "It's really important in this area, so the later patterns that are related to CG website this order one and actually this network is very sparse, so most of the lot of the letter patterns actually only have one link that connecting through this originel patterns.",
            "So if you only use the link information then you the model will think all these letter patterns are the same other with the same topic, but actually they have different quite different subtopic in this area.",
            "Some are related to software, some are related to hardware.",
            "So in this case if we only use the link information then we just have no way to detect those subtopics.",
            "That's why we think the text information is really important in this case."
        ],
        [
            "And actually, it's this is the very hot research topic.",
            "There are a lot of models developers in this area and.",
            "We found that.",
            "It's really hard to achieve both high accuracy and high scalability."
        ],
        [
            "Like the previous model PHS PSA, it's very efficient but prone to overfitting and the rational Top Model.",
            "We treat this model at the state of art model.",
            "It's not quite scalable.",
            "And why?"
        ],
        [
            "I think we can achieve both, so we are in the network community we.",
            "We able to take advantage of some new developments in that area.",
            "So specifically we are using network model called Balk at a newer model recently introduced in the physics and network community.",
            "Each other has and very simple and efficient algorithm and then we combine the classic text based PSA model with this link.",
            "Basketball Cannon, Newman model and this combination is very perfect so.",
            "Basically the two models are natural compatible with each other and this gives us a very fast and simple EM algorithm to analyze both text and links."
        ],
        [
            "OK, so let's look at the model.",
            "So for text we use the probabilistic latent semantic analysis analysis.",
            "So PRSA basically we have the setup parameters for each document D which are the topic proportions for Document D and also we have the beta parameter which define the topics.",
            "So beta Z for each top topic Z is the distribution over the words.",
            "So you can imagine for like the sports topic, then sunspots, richer words like soccer.",
            "Football will have pretty high probability and for different topics distribution will be pretty different then the generative process for the text in the following.",
            "So for document D. We generate your words independently.",
            "The answer word in document D generated in two steps.",
            "So first step we will choose the top because the according to the theater parameter and then we given the topic Z, we choose word according to the beta parameters.",
            "This gave us total probability that answered in Document D is a given word W is this equation.",
            "So we sum over all the topics.",
            "And for each topic the it's a multiplication of the corresponding state and beta parameters."
        ],
        [
            "And this year the Balkana Newman model for the for the links so.",
            "For each pair of document D&D Prime, we generate the links we first choose the topic for DN D, Prime separately according to the city of parameters, and then if the two topics are the same, we choose the number of edges between them.",
            "It's a DD prime according to percent distribution.",
            "An ETA is the percent mean, so it does either link density for topic Z.",
            "So for different topics.",
            "Data value will be different.",
            "And then the total number of the links between documentary and prime is percent distribution.",
            "With this mean it's a summation over all the topics.",
            "And."
        ],
        [
            "A more complete model where take into account the node popularity, so this is also very important for some networks, so here we introduce an extra parameter for each document D. It's a SD which represents overall popularity for Document D. So imagine like in our patent network, the original pattern is very important that area, so it's very popular.",
            "It's more likely to be connected, so we have these.",
            "As parameters, then the programming is proportional to their popularity."
        ],
        [
            "There is a very similar model.",
            "It's called the mixed membership stochastic block model, so this model usably distribution instead of person.",
            "And there are some other difference, like like MSB have had the digital prior over the seta parameters, MSB doesn't take into account the node popularities, but most important thing that MSB downscale where.",
            "So that's why we choose the ball Karen Newman model and the person link.",
            "Probability makes it possible to develop a very efficient EM algorithm.",
            "So this is."
        ],
        [
            "Typical model for this combined model.",
            "As we already see, for PSA and Broken Newman model, both of them has it set up, amateurs, so the combined model simply assumed that we use the same setup parameters to generate both words and links.",
            "As the Z parameters are the hidden variables.",
            "The grey nose, like the WMA and observe the data.",
            "They are the tags and links."
        ],
        [
            "Do we call our model the person makes topic, link, model and more complicated one with.",
            "With this parameters we call it the person next topic link model with degree correction.",
            "So degree correction is the term comes from the network community basically for networks.",
            "Because the popularity is actually represented by the tree nodes.",
            "So if a node has a very high degree then it's more popular.",
            "So that's why we call this kind of model that we collected model."
        ],
        [
            "And then we have the like little functions for the content.",
            "And here CDW is the number of times over W appeared in Document D, so PSA actually where aggregate the duplicate word in the document.",
            "So this will reduce time complexity.",
            "We only care about the number of distinct words in that document."
        ],
        [
            "This is likelihood for the links.",
            "It's basically the log of the percent distribution."
        ],
        [
            "And a simple way to get the total electrical.",
            "It just add them together.",
            "So we will see that this is not a good way to get the total echoed in some cases, but so far we just leave it in this way.",
            "Then the."
        ],
        [
            "The good thing is that for this combined model, we have a very simple and efficient EM algorithm to estimate those parameters.",
            "So in the E step we calculate the edge and Q.",
            "These are the probability distributions defined which defines the expected value for the hidden variables.",
            "Then, based on the expected value of the hidden variables in the M step, we can calculate the most likely values of these parameters.",
            "So for beta it's only related to the content for ETA, it's only related to the links and for C4C to actually had two terms in person, numerator and denominator.",
            "So the first one is related to content, the second one is related to links.",
            "So it's quite simple and intuitive.",
            "It's easy to understand and this is for the model without degree collection.",
            "For the crashing it will be a little more complicated, but you can get the equations in the same way."
        ],
        [
            "So the algorithm is also very efficient if we think the number of topics is a constant, then basically it's linear inside of the data set.",
            "So we have some size statistiques like any other number of documents, M given number of links and R is the summation.",
            "Overall, the document D anaphoric document D, we only care about the number of distinct words, and then the time complexity is proportional to the number of topics and also the summation of the size statistiques.",
            "So this is for only one M iteration, so the problem."
        ],
        [
            "The comment iterations we need to run to achieve the convergence.",
            "So we have some."
        ],
        [
            "Empirical results.",
            "We run our models.",
            "I like them to these real words.",
            "Document citation networks.",
            "So for color and size are they contain papers in machine learning with seven topics for Cora and six topics for size here we don't count the number of occurrences of words in each document.",
            "So CDW is just binary value for power made.",
            "It consists of medical papers on three topics.",
            "And we do count the number of occurrences of each word.",
            "So these tests have different size for Queen size are they have around 3000 documents for pyramid?",
            "It's larger tires, around 20,000 documents and also it has more number of links and the total number of words in the documents."
        ],
        [
            "So then we test the convergence curve.",
            "We run 5000M iterations.",
            "And the likelihood are scared.",
            "So after at the end of the 5000 iterations, all the codes were true one.",
            "And we can see after running without interruptions or the curves already already converted very well.",
            "And although the deserts are very different, the converse curves are very similar, so they have very similar convergence rates.",
            "So we think maybe for even larger than, say, 5000 demonstrations is quite enough.",
            "Here is the running time."
        ],
        [
            "We run our items and the PSA PSA, which only you process the text information and the PHITS PSA.",
            "We run 5000M iterations and for rational topic model we implement the variational method.",
            "We run up to 500 M iterations and most efficient one is the PSA because it only process the text information and for us.",
            "It's a little slower, so our models are those model.",
            "The two models below the middle line, so it's a little slower because we also process the link information.",
            "But actually most of the time our model our spending on processing the text information.",
            "This is basically because on average each document had more had more words than the links.",
            "PHITS PSA is also very efficient, but its performance in no good.",
            "So Rachel Top Model doesn't scale where?"
        ],
        [
            "Then I want talk about discrete labels and local search, so mixtures of topics are more important, more informative of course.",
            "But sometimes if you really want to hard category for each topic, then you can just assign the most likely topic for each document and then you can further run some local search optimization algorithms to search for some single topic assignment which has higher likelihood.",
            "So we have some results on the local search in our paper, but I'm not going to talk about this here.",
            "So basically these algorithm will improve the performance.",
            "Sometimes it's quite significant for some datasets, but they are.",
            "They don't scale where.",
            "It's pretty slow, like the MSMC."
        ],
        [
            "So how to measure the performance?",
            "Actually, we know the ground truth in the deserts, so the data set will give us a clustering for each document.",
            "We know it's true topic.",
            "And also the model will give us another clustering.",
            "So basically we measure the similarity of these two clusterings or the distance of the two clusterings.",
            "The basic idea is we want the two clustering to be.",
            "To be similar if they are similar, then that means our model works pretty well.",
            "So for normal normal language information it's similarity measurement.",
            "The higher the better.",
            "VI is a distance measurement, the lower the better.",
            "Pairwise F score it's AF score focusing on document pairs predicted in the same cluster.",
            "It's also the higher the better."
        ],
        [
            "And this is for current sizer.",
            "The red ones are the best ones among these algorithms, so our model works very well, except that for color sepia DC have higher normal military information value than ours."
        ],
        [
            "And this is for power made, so it's quite interesting actually, no one works significantly better than than others.",
            "So basically it's because."
        ],
        [
            "The likelihood for the content is much lower because we count number of currencies in the world in the document.",
            "So each document is much longer than all algorithm.",
            "Just focus only on the content so that."
        ],
        [
            "So we need to balance content links so there are two ways to do that in.",
            "These are presented in previous literatures, we have the.",
            "Content normalization which will increase the likelihood for the content.",
            "The linear interpolation we have a tuneable parameter, Alpha.",
            "If it's there, we only use a link if it's one, we only use the content.",
            "So this very nicely let us go from only caring about links to only care about the contract, and we will see that the optimal value of Alpha depends depends on the death date and also the learning task."
        ],
        [
            "So we changed our, forget the values we.",
            "Only recording normalization mutual information for current size.",
            "They are actually the test.",
            "Essays are already well balanced, so that's why we achieve particular adults even without balancing.",
            "But we can do even better if we choose some optimal values like .3 and .4.",
            "For Cortana size there and we compare our results with the baseline.",
            "It's appear tracts psar model works much better.",
            "For permit, it's pretty hard to get the.",
            "Get the window that we can use both text information to achieve high performance.",
            "So we first do the counter normalization and then at some particular values like equal 2.8 the model with degree correction can achieve very good results.",
            "So this is."
        ],
        [
            "Or the link prediction we calculate area under curve values for different Alpha.",
            "We compared our model with the rational talk model.",
            "Here we can see the degree collection works much better than the model without degree correction, and it's pretty interesting that the optimal fire value is around .1 and 24: size.",
            "Here it's a little smaller than the optimal."
        ],
        [
            "Value for the document classification, so this is."
        ],
        [
            "Actually pretty intuitive because for link prediction we want the model actually want to use more links and.",
            "Also, here we see that if we only use the link information, the performance is pretty good.",
            "But for document classification, if we only use the link information like when F = 0, then the performance is pretty poor.",
            "So."
        ],
        [
            "Here the conclusion.",
            "So we combine the PSA model and broken human model.",
            "There are natural compatible with each other.",
            "That gives us a very efficient inference algorithm with high accuracy, and we balance information from text links and the optimal value of Alpha depends on both the state and learning task, and balancing Texan link is really important, like in the pyrmaid.",
            "Otherwise we may only use the link information or the text information.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, true.",
                    "label": 0
                },
                {
                    "sent": "I'm contraption tree student at University of New Mexico so today I will talk about the mix topic link models that can be used to analyze checks and links are very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is general form of the problem.",
                    "label": 1
                },
                {
                    "sent": "We have some nerves, and for each node we have the rich information.",
                    "label": 1
                },
                {
                    "sent": "These are node attributes and also we have the pairwise relationship between the nodes.",
                    "label": 0
                },
                {
                    "sent": "These other links.",
                    "label": 0
                },
                {
                    "sent": "So basically we have a network and for each node we also have the attributes.",
                    "label": 0
                },
                {
                    "sent": "So the learning task can be detecting hidden commit structures in the network or predicting node attributes and links.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our work we are focusing on a more specific problem, just like in this picture we have some document citation networks.",
                    "label": 0
                },
                {
                    "sent": "These networks the node corresponds to document and attributes are the text, the pairwise relationship, the citation ratio, or the hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "So the learning tasks include document topic detection and link prediction.",
                    "label": 1
                },
                {
                    "sent": "I'll go into develop models and algorithms which use both text and link information to classified documents by topic and predict missing links with higher accuracy and better scalability.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why we are interested in this problem?",
                    "label": 1
                },
                {
                    "sent": "So previously we are working on models to detect community structures in network by only using the topical information.",
                    "label": 0
                },
                {
                    "sent": "Actually this is true story.",
                    "label": 0
                },
                {
                    "sent": "So two years ago we are working on patent Citation network.",
                    "label": 1
                },
                {
                    "sent": "All the nodes in the network are the patterns that they are related to, the city technology and there is one pattern which is the original pattern for citian.",
                    "label": 0
                },
                {
                    "sent": "It's really important in this area, so the later patterns that are related to CG website this order one and actually this network is very sparse, so most of the lot of the letter patterns actually only have one link that connecting through this originel patterns.",
                    "label": 1
                },
                {
                    "sent": "So if you only use the link information then you the model will think all these letter patterns are the same other with the same topic, but actually they have different quite different subtopic in this area.",
                    "label": 1
                },
                {
                    "sent": "Some are related to software, some are related to hardware.",
                    "label": 0
                },
                {
                    "sent": "So in this case if we only use the link information then we just have no way to detect those subtopics.",
                    "label": 0
                },
                {
                    "sent": "That's why we think the text information is really important in this case.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually, it's this is the very hot research topic.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of models developers in this area and.",
                    "label": 0
                },
                {
                    "sent": "We found that.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to achieve both high accuracy and high scalability.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like the previous model PHS PSA, it's very efficient but prone to overfitting and the rational Top Model.",
                    "label": 1
                },
                {
                    "sent": "We treat this model at the state of art model.",
                    "label": 0
                },
                {
                    "sent": "It's not quite scalable.",
                    "label": 0
                },
                {
                    "sent": "And why?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think we can achieve both, so we are in the network community we.",
                    "label": 1
                },
                {
                    "sent": "We able to take advantage of some new developments in that area.",
                    "label": 0
                },
                {
                    "sent": "So specifically we are using network model called Balk at a newer model recently introduced in the physics and network community.",
                    "label": 1
                },
                {
                    "sent": "Each other has and very simple and efficient algorithm and then we combine the classic text based PSA model with this link.",
                    "label": 0
                },
                {
                    "sent": "Basketball Cannon, Newman model and this combination is very perfect so.",
                    "label": 1
                },
                {
                    "sent": "Basically the two models are natural compatible with each other and this gives us a very fast and simple EM algorithm to analyze both text and links.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the model.",
                    "label": 0
                },
                {
                    "sent": "So for text we use the probabilistic latent semantic analysis analysis.",
                    "label": 1
                },
                {
                    "sent": "So PRSA basically we have the setup parameters for each document D which are the topic proportions for Document D and also we have the beta parameter which define the topics.",
                    "label": 1
                },
                {
                    "sent": "So beta Z for each top topic Z is the distribution over the words.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine for like the sports topic, then sunspots, richer words like soccer.",
                    "label": 1
                },
                {
                    "sent": "Football will have pretty high probability and for different topics distribution will be pretty different then the generative process for the text in the following.",
                    "label": 0
                },
                {
                    "sent": "So for document D. We generate your words independently.",
                    "label": 0
                },
                {
                    "sent": "The answer word in document D generated in two steps.",
                    "label": 0
                },
                {
                    "sent": "So first step we will choose the top because the according to the theater parameter and then we given the topic Z, we choose word according to the beta parameters.",
                    "label": 0
                },
                {
                    "sent": "This gave us total probability that answered in Document D is a given word W is this equation.",
                    "label": 1
                },
                {
                    "sent": "So we sum over all the topics.",
                    "label": 0
                },
                {
                    "sent": "And for each topic the it's a multiplication of the corresponding state and beta parameters.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this year the Balkana Newman model for the for the links so.",
                    "label": 1
                },
                {
                    "sent": "For each pair of document D&D Prime, we generate the links we first choose the topic for DN D, Prime separately according to the city of parameters, and then if the two topics are the same, we choose the number of edges between them.",
                    "label": 0
                },
                {
                    "sent": "It's a DD prime according to percent distribution.",
                    "label": 0
                },
                {
                    "sent": "An ETA is the percent mean, so it does either link density for topic Z.",
                    "label": 1
                },
                {
                    "sent": "So for different topics.",
                    "label": 0
                },
                {
                    "sent": "Data value will be different.",
                    "label": 1
                },
                {
                    "sent": "And then the total number of the links between documentary and prime is percent distribution.",
                    "label": 0
                },
                {
                    "sent": "With this mean it's a summation over all the topics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A more complete model where take into account the node popularity, so this is also very important for some networks, so here we introduce an extra parameter for each document D. It's a SD which represents overall popularity for Document D. So imagine like in our patent network, the original pattern is very important that area, so it's very popular.",
                    "label": 0
                },
                {
                    "sent": "It's more likely to be connected, so we have these.",
                    "label": 0
                },
                {
                    "sent": "As parameters, then the programming is proportional to their popularity.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a very similar model.",
                    "label": 0
                },
                {
                    "sent": "It's called the mixed membership stochastic block model, so this model usably distribution instead of person.",
                    "label": 0
                },
                {
                    "sent": "And there are some other difference, like like MSB have had the digital prior over the seta parameters, MSB doesn't take into account the node popularities, but most important thing that MSB downscale where.",
                    "label": 0
                },
                {
                    "sent": "So that's why we choose the ball Karen Newman model and the person link.",
                    "label": 0
                },
                {
                    "sent": "Probability makes it possible to develop a very efficient EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typical model for this combined model.",
                    "label": 0
                },
                {
                    "sent": "As we already see, for PSA and Broken Newman model, both of them has it set up, amateurs, so the combined model simply assumed that we use the same setup parameters to generate both words and links.",
                    "label": 0
                },
                {
                    "sent": "As the Z parameters are the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "The grey nose, like the WMA and observe the data.",
                    "label": 0
                },
                {
                    "sent": "They are the tags and links.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do we call our model the person makes topic, link, model and more complicated one with.",
                    "label": 0
                },
                {
                    "sent": "With this parameters we call it the person next topic link model with degree correction.",
                    "label": 1
                },
                {
                    "sent": "So degree correction is the term comes from the network community basically for networks.",
                    "label": 0
                },
                {
                    "sent": "Because the popularity is actually represented by the tree nodes.",
                    "label": 0
                },
                {
                    "sent": "So if a node has a very high degree then it's more popular.",
                    "label": 0
                },
                {
                    "sent": "So that's why we call this kind of model that we collected model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we have the like little functions for the content.",
                    "label": 0
                },
                {
                    "sent": "And here CDW is the number of times over W appeared in Document D, so PSA actually where aggregate the duplicate word in the document.",
                    "label": 1
                },
                {
                    "sent": "So this will reduce time complexity.",
                    "label": 0
                },
                {
                    "sent": "We only care about the number of distinct words in that document.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is likelihood for the links.",
                    "label": 0
                },
                {
                    "sent": "It's basically the log of the percent distribution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a simple way to get the total electrical.",
                    "label": 0
                },
                {
                    "sent": "It just add them together.",
                    "label": 0
                },
                {
                    "sent": "So we will see that this is not a good way to get the total echoed in some cases, but so far we just leave it in this way.",
                    "label": 0
                },
                {
                    "sent": "Then the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The good thing is that for this combined model, we have a very simple and efficient EM algorithm to estimate those parameters.",
                    "label": 0
                },
                {
                    "sent": "So in the E step we calculate the edge and Q.",
                    "label": 0
                },
                {
                    "sent": "These are the probability distributions defined which defines the expected value for the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Then, based on the expected value of the hidden variables in the M step, we can calculate the most likely values of these parameters.",
                    "label": 0
                },
                {
                    "sent": "So for beta it's only related to the content for ETA, it's only related to the links and for C4C to actually had two terms in person, numerator and denominator.",
                    "label": 0
                },
                {
                    "sent": "So the first one is related to content, the second one is related to links.",
                    "label": 0
                },
                {
                    "sent": "So it's quite simple and intuitive.",
                    "label": 0
                },
                {
                    "sent": "It's easy to understand and this is for the model without degree collection.",
                    "label": 0
                },
                {
                    "sent": "For the crashing it will be a little more complicated, but you can get the equations in the same way.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the algorithm is also very efficient if we think the number of topics is a constant, then basically it's linear inside of the data set.",
                    "label": 1
                },
                {
                    "sent": "So we have some size statistiques like any other number of documents, M given number of links and R is the summation.",
                    "label": 0
                },
                {
                    "sent": "Overall, the document D anaphoric document D, we only care about the number of distinct words, and then the time complexity is proportional to the number of topics and also the summation of the size statistiques.",
                    "label": 1
                },
                {
                    "sent": "So this is for only one M iteration, so the problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The comment iterations we need to run to achieve the convergence.",
                    "label": 0
                },
                {
                    "sent": "So we have some.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Empirical results.",
                    "label": 0
                },
                {
                    "sent": "We run our models.",
                    "label": 0
                },
                {
                    "sent": "I like them to these real words.",
                    "label": 0
                },
                {
                    "sent": "Document citation networks.",
                    "label": 0
                },
                {
                    "sent": "So for color and size are they contain papers in machine learning with seven topics for Cora and six topics for size here we don't count the number of occurrences of words in each document.",
                    "label": 1
                },
                {
                    "sent": "So CDW is just binary value for power made.",
                    "label": 1
                },
                {
                    "sent": "It consists of medical papers on three topics.",
                    "label": 1
                },
                {
                    "sent": "And we do count the number of occurrences of each word.",
                    "label": 0
                },
                {
                    "sent": "So these tests have different size for Queen size are they have around 3000 documents for pyramid?",
                    "label": 0
                },
                {
                    "sent": "It's larger tires, around 20,000 documents and also it has more number of links and the total number of words in the documents.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we test the convergence curve.",
                    "label": 0
                },
                {
                    "sent": "We run 5000M iterations.",
                    "label": 0
                },
                {
                    "sent": "And the likelihood are scared.",
                    "label": 0
                },
                {
                    "sent": "So after at the end of the 5000 iterations, all the codes were true one.",
                    "label": 1
                },
                {
                    "sent": "And we can see after running without interruptions or the curves already already converted very well.",
                    "label": 0
                },
                {
                    "sent": "And although the deserts are very different, the converse curves are very similar, so they have very similar convergence rates.",
                    "label": 0
                },
                {
                    "sent": "So we think maybe for even larger than, say, 5000 demonstrations is quite enough.",
                    "label": 0
                },
                {
                    "sent": "Here is the running time.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We run our items and the PSA PSA, which only you process the text information and the PHITS PSA.",
                    "label": 0
                },
                {
                    "sent": "We run 5000M iterations and for rational topic model we implement the variational method.",
                    "label": 0
                },
                {
                    "sent": "We run up to 500 M iterations and most efficient one is the PSA because it only process the text information and for us.",
                    "label": 0
                },
                {
                    "sent": "It's a little slower, so our models are those model.",
                    "label": 0
                },
                {
                    "sent": "The two models below the middle line, so it's a little slower because we also process the link information.",
                    "label": 0
                },
                {
                    "sent": "But actually most of the time our model our spending on processing the text information.",
                    "label": 0
                },
                {
                    "sent": "This is basically because on average each document had more had more words than the links.",
                    "label": 0
                },
                {
                    "sent": "PHITS PSA is also very efficient, but its performance in no good.",
                    "label": 0
                },
                {
                    "sent": "So Rachel Top Model doesn't scale where?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then I want talk about discrete labels and local search, so mixtures of topics are more important, more informative of course.",
                    "label": 1
                },
                {
                    "sent": "But sometimes if you really want to hard category for each topic, then you can just assign the most likely topic for each document and then you can further run some local search optimization algorithms to search for some single topic assignment which has higher likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we have some results on the local search in our paper, but I'm not going to talk about this here.",
                    "label": 0
                },
                {
                    "sent": "So basically these algorithm will improve the performance.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's quite significant for some datasets, but they are.",
                    "label": 0
                },
                {
                    "sent": "They don't scale where.",
                    "label": 0
                },
                {
                    "sent": "It's pretty slow, like the MSMC.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to measure the performance?",
                    "label": 0
                },
                {
                    "sent": "Actually, we know the ground truth in the deserts, so the data set will give us a clustering for each document.",
                    "label": 0
                },
                {
                    "sent": "We know it's true topic.",
                    "label": 0
                },
                {
                    "sent": "And also the model will give us another clustering.",
                    "label": 0
                },
                {
                    "sent": "So basically we measure the similarity of these two clusterings or the distance of the two clusterings.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is we want the two clustering to be.",
                    "label": 0
                },
                {
                    "sent": "To be similar if they are similar, then that means our model works pretty well.",
                    "label": 0
                },
                {
                    "sent": "So for normal normal language information it's similarity measurement.",
                    "label": 0
                },
                {
                    "sent": "The higher the better.",
                    "label": 0
                },
                {
                    "sent": "VI is a distance measurement, the lower the better.",
                    "label": 0
                },
                {
                    "sent": "Pairwise F score it's AF score focusing on document pairs predicted in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "It's also the higher the better.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is for current sizer.",
                    "label": 0
                },
                {
                    "sent": "The red ones are the best ones among these algorithms, so our model works very well, except that for color sepia DC have higher normal military information value than ours.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is for power made, so it's quite interesting actually, no one works significantly better than than others.",
                    "label": 0
                },
                {
                    "sent": "So basically it's because.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The likelihood for the content is much lower because we count number of currencies in the world in the document.",
                    "label": 1
                },
                {
                    "sent": "So each document is much longer than all algorithm.",
                    "label": 1
                },
                {
                    "sent": "Just focus only on the content so that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need to balance content links so there are two ways to do that in.",
                    "label": 0
                },
                {
                    "sent": "These are presented in previous literatures, we have the.",
                    "label": 0
                },
                {
                    "sent": "Content normalization which will increase the likelihood for the content.",
                    "label": 1
                },
                {
                    "sent": "The linear interpolation we have a tuneable parameter, Alpha.",
                    "label": 0
                },
                {
                    "sent": "If it's there, we only use a link if it's one, we only use the content.",
                    "label": 0
                },
                {
                    "sent": "So this very nicely let us go from only caring about links to only care about the contract, and we will see that the optimal value of Alpha depends depends on the death date and also the learning task.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we changed our, forget the values we.",
                    "label": 0
                },
                {
                    "sent": "Only recording normalization mutual information for current size.",
                    "label": 0
                },
                {
                    "sent": "They are actually the test.",
                    "label": 0
                },
                {
                    "sent": "Essays are already well balanced, so that's why we achieve particular adults even without balancing.",
                    "label": 0
                },
                {
                    "sent": "But we can do even better if we choose some optimal values like .3 and .4.",
                    "label": 0
                },
                {
                    "sent": "For Cortana size there and we compare our results with the baseline.",
                    "label": 0
                },
                {
                    "sent": "It's appear tracts psar model works much better.",
                    "label": 0
                },
                {
                    "sent": "For permit, it's pretty hard to get the.",
                    "label": 0
                },
                {
                    "sent": "Get the window that we can use both text information to achieve high performance.",
                    "label": 0
                },
                {
                    "sent": "So we first do the counter normalization and then at some particular values like equal 2.8 the model with degree correction can achieve very good results.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the link prediction we calculate area under curve values for different Alpha.",
                    "label": 0
                },
                {
                    "sent": "We compared our model with the rational talk model.",
                    "label": 0
                },
                {
                    "sent": "Here we can see the degree collection works much better than the model without degree correction, and it's pretty interesting that the optimal fire value is around .1 and 24: size.",
                    "label": 0
                },
                {
                    "sent": "Here it's a little smaller than the optimal.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value for the document classification, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually pretty intuitive because for link prediction we want the model actually want to use more links and.",
                    "label": 0
                },
                {
                    "sent": "Also, here we see that if we only use the link information, the performance is pretty good.",
                    "label": 0
                },
                {
                    "sent": "But for document classification, if we only use the link information like when F = 0, then the performance is pretty poor.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So we combine the PSA model and broken human model.",
                    "label": 1
                },
                {
                    "sent": "There are natural compatible with each other.",
                    "label": 1
                },
                {
                    "sent": "That gives us a very efficient inference algorithm with high accuracy, and we balance information from text links and the optimal value of Alpha depends on both the state and learning task, and balancing Texan link is really important, like in the pyrmaid.",
                    "label": 1
                },
                {
                    "sent": "Otherwise we may only use the link information or the text information.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}