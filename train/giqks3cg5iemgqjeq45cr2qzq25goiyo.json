{
    "id": "giqks3cg5iemgqjeq45cr2qzq25goiyo",
    "title": "Competitive Closeness Testing",
    "info": {
        "author": [
            "Hirakendu Das, UC San Diego"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/colt2011_das_testing/",
    "segmentation": [
        [
            "So we say."
        ],
        [
            "Start with the problem definition.",
            "Given two sequences, we want to find whether they come from similar or different sources.",
            "That is, let there be 2 unknown distributions P1 and P2, discrete distributions, an alphabet of size K, and we are given to lengthen sequences X1 and X2 generated according to P1 and P2 respectively, and we want to find whether P1P2 are similar or very different.",
            "So, for example, we could be given two sequences of coin tosses and from 2 coins, and.",
            "Could be asked that they are similar or differ."
        ],
        [
            "It's related to classification, where we have two sequences X one X2 from 2 unknown distributions, one and P2 and 3rd sequences generated from one of these, and we want to find which one of them.",
            "Which one of them generates it?",
            "It's equal and so if we could, if we had a good test for testing closeness between that is finding weather wise close to X1 or X2 then we also have a good test for classification.",
            "So it's also related to other problems like clustering testing properties like independence applications include that of text categorization, testing, authorship of documents."
        ],
        [
            "Anyway, so back to the problem.",
            "So test it levels each pair of sequences as either same or different to indicate whether the distributions are same or different, and for simplicity in this talk, when we say similar, we're only going to consider when the distributions are the same and for different.",
            "By different we mean that the distributions are separated in some distance, will make this clear as we go along.",
            "There are property of the test for a particular distribution.",
            "Pair is the probability that it pacifies probability that it labels the sequence pairs incorrectly.",
            "So if the distributions are similar, it's the probability that it labels them as different and that is incorrectly labels them as different.",
            "Similarly for different."
        ],
        [
            "To motivate one such closeness test, we can relate it to composite hypothesis testing.",
            "So think of two classes PSM&P different, one consisting of distribution pairs which are identical and the other consisting of separated distributions and given a sequence where we want to find which of these.",
            "The distribution pair belongs to which of these two classes so?",
            "A common technique for this, for composite hypothesis testing is generalized likelihood ratio test, where you are one looks at the ratio of the maximum likelihood of the observation under the two hypothesis and compares it to a threshold.",
            "So in this case 1 looks at the maximum likelihood of X1 and X2 under two different distributions versus under the same distribution.",
            "And if this ratio is too large then.",
            "Of one says that it's different or if it's not too large.",
            "We say that it seems.",
            "So, so it's a test clear.",
            "So in the denominator we have we have the same distribution, maximizing X1 and X2, whereas in the numerator we don't have that restriction.",
            "So actually this ratio is always going to be bigger than one because the numerator is less restrictive.",
            "I."
        ],
        [
            "But OK so.",
            "Alright, so for sequences we know that the empirical distribution maximizes the sequence likelihood.",
            "So if one and two are the number of references of the symbols A1A two and so on.",
            "In the sequence of these, the maximum likelihood expression.",
            "So this test is going to be of this form.",
            "So and one end into higher the number of appearances of the symbol AI in the first sequence and 2nd sequence.",
            "So I hope this this part is clear.",
            "So indeed one can show that when the two distributions are the same, this ratio is going to be small.",
            "That is, it's going to be polynomial with high probability, by the way, so it still grows in the alphabet size K. On the other hand, when the distributions are different, let's say the L1 distance between the distributions is bigger than epsilon, then this quantity is going to be exponentially large with high probability.",
            "So if you compare this ratio to a threshold which is some polynomial bigger than.",
            "This quantity, then you have a test whose error probability is small."
        ],
        [
            "I'm skipping this example.",
            "Suppose you took hundred coin tosses from 2 similar coins.",
            "This ratio is going to be small.",
            "On the other hand, from very different points then this ratio is going to be really low."
        ],
        [
            "On the other hand, so let's see what happens when the alphabet size is large.",
            "So suppose think of an alphabet size of roughly NQ.",
            "Considered these two distributions, one of them is a Singleton and the other one it has half probability half on that same symbol A and the remaining half is uniformly distributed over a long tail.",
            "So let's consider typical sequence pairs from first from P1 and P2, and then from both from."
        ],
        [
            "Happy too.",
            "So when one of them is from P1, another from P2.",
            "So the first sequence is all is and the second one is one such typical sequences half of them arrays and the remaining symbols just appeared once.",
            "So we calculate this ratio.",
            "It comes to be exponentially large, which is good because the distributions are different.",
            "On the other hand, when both these when both of them are generated from P2.",
            "We do the same calculation.",
            "It turns out that this ratio is even.",
            "Sorry this this quantity is even larger than the first case, so no matter what we choose the threshold, this test is never going to work.",
            "I mean it's not going to classify both these pairs correctly.",
            "So so one thing we observe is that I mean in this case one intuition for the maximum likelihood ratio test to work is that.",
            "The maximizing distribution was the empirical distribution.",
            "The If you look at the empirical distributions in the numerator and denominator, they are very different.",
            "So in the first case it is 1/2 and 1 / N / N by two symbols.",
            "But the next case it's it's much different.",
            "But we still see that I mean these two sequences have structurally there.",
            "There is something common in them in terms of how many symbols appeared once, and so on.",
            "So we try to exploit the structure.",
            "In a different test."
        ],
        [
            "So before that, some of the known results, so bottle and others had considered this problem of testing closeness in L1 distance and they have shown an algorithm whose error probability is less than Delta and takes K to the 2/3 samples case.",
            "The alphabet size and has and they also show a matching lower bound that there is an example where you cannot distinguish between.",
            "You cannot distinguish between the distributions using just came to the 2/3 samples.",
            "So their test it's not exactly a likelihood ratio test, so for the high for the high probability symbols they use the empirical distribution.",
            "But for the infrequent symbols, L1 distance is estimated using collisions or the repetitions.",
            "There are also similar results by Valiant, although they have a slightly stronger, so the upper bound on the L1 distance for the similar distributions, that is, that's a constant strictly bounded away from zero, so that's why they require more samples.",
            "There also a similar results by Kelly and others from last year, so they had shown that but.",
            "Latest it, it's a number one.",
            "It's meant for classification and Secondly it's they assume that all the symbols have probabilities in the same order and that is think of these as nearly uniform distributions, but they can go up to alphabet size up to N squared.",
            "That is only taking square root K samples.",
            "They are able to do this closeness testing regardless all of these tests depend on some prior knowledge on the.",
            "An upper bound on the alphabet size, so will show tests which do not require any knowledge on the."
        ],
        [
            "On this alphabet size.",
            "So we started to motivate artists.",
            "We start with a basic observation that in this problem we only the test should depend only on the structure of the sequences.",
            "By that we mean that.",
            "So suppose we have these two.",
            "So consider this pair of sequences and this where they are structurally same in the sense that so wherever you see in the symbol C in this first sequence where you see the symbol B in the second sequence for these two, these two are the same up to a permutation after the labeling of this.",
            "Of these symbols so.",
            "So we are.",
            "So we expect tests to have the same decision for for such.",
            "If they are structurally the same.",
            "In fact, indeed we can show that.",
            "If the test is not symmetric, it's probability of error can be matched by another symmetric test, which which has this which has this property so.",
            "So we incurred this structure by what we called pattern.",
            "So for example, the pattern of the sequence this sequence BC is 1232.",
            "The way we are encoding is that the first symbol gets the level one, the next different symbol gets the Level 2 and the next different symbol gets the Level 3 and so on a.",
            "Similarly we can we can with the.",
            "Structure of two sequences using what we call joint pattern so it so these are the pattern of the first sequence and then.",
            "For the second sequence, when we convert the pattern, we use the labels from the from the first sequence.",
            "For example C in the symbols.",
            "He still gets the level to which wich Western level in the first sequence."
        ],
        [
            "Invite right.",
            "So then we define pattern probabilities.",
            "The probability of a pattern is the probability of observing a sequence with that pattern.",
            "For what we mean is so for example, for the pattern 1213, it's the probability of observing any of these sequences ABC or ABCD which have this same pattern.",
            "So the expression.",
            "We define maximum likelihood of a pattern as the as its maximum likelihood under all possible distributions."
        ],
        [
            "Similarly, we can define joint pattern probabilities.",
            "So for example, if you have two sequences, it's the probability of all probably observing any sequence pair with.",
            "With this joint pattern.",
            "The first sequence being generated by the 1st distribution and the second sequence being generated by the 2nd distribution.",
            "So we can likewise and then we define the pattern maximum likelihood of joint patterns to be the maximum likelihood under all possible distributions."
        ],
        [
            "So then.",
            "As expected, so we look at this ratio test where you look at the maximum look at the maximum.",
            "Look at the ratio of the maximum likelihood of the joint pattern under two different distributions versus that under the same distribution, and then you compare to this threshold D1 over square root Delta.",
            "Think of Delta is something between zero and one.",
            "So to prove to prove its performance bounds, we define a different type of distance measure.",
            "So we say that distributions P1 and P2R N Delta different if for all equal pairs P3 we can find a test such that the error probability of the test for both P1P2 and P3P3 is less than Delta that is weak, and it is possible to distinguish between P1P2 and any P3P3.",
            "There are probably less than 10 to these tests.",
            "Could depend on Harry, could depend on P1P2P3, so of course I mean if they're not in Delta separable then we cannot hope to find a test which which has their probability less than Delta for both different distributions and same distributions.",
            "So here's our first main result that.",
            "And this test it has low error probability whenever the distributions are same or under different.",
            "In other words, if you can distinguish P1 and P2 with other property less than Delta, using N samples, then this test can give you error probability square root Delta times E to the end, and the suffix potential factor.",
            "So this is the error probability of the test when using end length sequences.",
            "The definition doesn't depend on if you could take it.",
            "Swimming test always gives the same answer.",
            "Then this information will always be met.",
            "This no because I so P3 is the set of.",
            "So let's say that you have a trivial test which always outputs different.",
            "Then there are probably for the same pair will be.",
            "We set that.",
            "So P3 is a similar field so.",
            "That's right, yeah.",
            "So so it should.",
            "It should have lower probability for both these cases."
        ],
        [
            "So we can go back to the large alphabet example."
        ],
        [
            "So, at least as a result here.",
            "Any questions there?"
        ],
        [
            "OK, so in that large alphabet example we can again see that if instead instead of the usual maximum likelihood if we use the pattern maximum likelihood."
        ],
        [
            "Then indeed."
        ],
        [
            "So then indeed, this even when the in the second case where we have the ratio, is large.",
            "In this case it's it's going to be small, so one thing is that calculation of this pattern maximum likelihood is difficult in general.",
            "I mean, it's not as straightforward as the as the usual empirical distribution for sequences."
        ],
        [
            "So tomorrow will also provide a computationally efficient test, so for that we look at profiles of patterns, so profile simply conveys how many symbols appeared a given number of times.",
            "So for example.",
            "So profile of a pattern is Sophie.",
            "One is the number of symbols which appeared once we do is the number of samples which appear twice and so on.",
            "So for example, in this pattern Two symbols appeared once no symbol appear twice there is one symbol which appeared three times, which is the symbol one, and so on.",
            "So one can.",
            "There is a one to one correspondence between profile between profiles.",
            "An integer partitions think of even as the number of ones in the partition feature as a number of tools in the partition.",
            "So it's well known that the number of profile the partition number is of the order of each of these square root N."
        ],
        [
            "Similarly for joint profile, so we can define joint profile similarly that how many symbols appeared is specific number of times in sequence one and MU two times in sequence 2.",
            "Exactly so yeah, some people call it fingerprints or histogram of histograms.",
            "So so this week and related to something like a joint joint partition, it's an analog of integer partitions where the first components of the parts add up to anyone and the second components of."
        ],
        [
            "Part side of two and two, and one can similarly upper bound them as single.",
            "Single partition says this subexponential quantity, although the this time it is end to the 2/3 instead of into the end to the 1/2.",
            "So I'm.",
            "So I'm not I'm not giving the detailed proof for the.",
            "For the but they see to the entry 2 third, this number comes from primarily comes from this."
        ],
        [
            "And dismount."
        ],
        [
            "So for the skipping."
        ],
        [
            "Spoof."
        ],
        [
            "So for the for the computationally efficient test, the motivation is that one can consider an estimated 4 joint patterns which assigns equal probability estimate to all the profiles and equal probability estimate to all the patterns within a profile.",
            "So, and this is the this quantity in fees, the number of patterns which have the same."
        ],
        [
            "File.",
            "It's similar to the number of sequences which have the same empirical distribution, except that there is a factor which for discounting the."
        ],
        [
            "Patients.",
            "So.",
            "This test, which counts so.",
            "So in this test we look at the ratio of the number of patterns which have the same total profile, same profile as XX1X2 versus the number of joint patterns which have the same joint profile as X one X2 and we compare this to a threshold and this test also has a has a similar property as the test on the maximum likelihood of as a maximum like maximum likelihood test so."
        ],
        [
            "Yeah, that's."
        ],
        [
            "We can even rephrase the results in terms of sample complexity."
        ],
        [
            "Yeah, so as to conclude we had.",
            "We looked at contests for closing closing stress which are which have lower probability whenever it's possible to do so and we saw two tests, one based on pattern maximum likelihood and the other on number of patterns with the same profile.",
            "And some of these results are useful for maximum likelihood or patterns in."
        ],
        [
            "And I'll skip the ongoing work, which is to consider so this test was primarily useful for low error probabilities, but we are still working on a separate proof for any cleaner proof for larger Delta.",
            "And we also want to directly use this for there is there is a more direct approach for classification rather than using closeness tests for them.",
            "We are also looking at experimental results and among other things."
        ],
        [
            "Thank you.",
            "So I mean I have one question, is like technical and the other general.",
            "The technical question.",
            "I don't see why you look at patterns.",
            "Patterns depend on the ordering, yes, but here it is distributed symmetrically texture.",
            "That's a great question, so indeed.",
            "I mean if you look at the test, I could have actually mentioned that in terms of profile probabilities also so."
        ],
        [
            "What happened was this test.",
            "It might eventually come to be coming out of nowhere, but somehow it was motivated by actually estimating the pattern probabilities rather than the profile probabilities.",
            "Yeah, but in practice, yes, because it's IID and independent.",
            "It just depends on profile and the other person is.",
            "So how do you result compared to the results of poor variant?",
            "So yeah, they also have similar results.",
            "So one thing is, as I mentioned, they depend on the alphabet size.",
            "Secondly, in this case we are saying that we could do as long as you don't have to know the exactly, but you can estimate the alphabet size using something like the good chewing test or something like that.",
            "That would be like an indirect.",
            "I mean it's it's like adding one more step.",
            "I mean we're doing it directly without even doing it directly or estimating.",
            "It's alright.",
            "So there are many cases where the alphabet size is much larger and yet you can separate them as in the example we saw it, so it would be useful for such cases.",
            "Anymore questions.",
            "I'm sorry this may be a very naive and stupid question, but how does this compare to the sort of two using chi squared tests for comparing these things?",
            "I mean, it seems I already just general statistics, classic statistical approach or you have samples from two different distributions and you do chi squared.",
            "No, not necessarily.",
            "You can use a chi squared test to compare.",
            "Yes, you could have.",
            "Potentially, you could use a chi square test, in fact, so the results from Benjamin Kelly last year they had.",
            "They had tried using something similar in.",
            "Potentially we could.",
            "We could use.",
            "Eventually it comes to, but I think even the CHI squared test fails for this large alphabet case.",
            "We can.",
            "We can provide examples where it doesn't work for large alphabets.",
            "Probably have time for one more.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we say.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with the problem definition.",
                    "label": 0
                },
                {
                    "sent": "Given two sequences, we want to find whether they come from similar or different sources.",
                    "label": 1
                },
                {
                    "sent": "That is, let there be 2 unknown distributions P1 and P2, discrete distributions, an alphabet of size K, and we are given to lengthen sequences X1 and X2 generated according to P1 and P2 respectively, and we want to find whether P1P2 are similar or very different.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we could be given two sequences of coin tosses and from 2 coins, and.",
                    "label": 0
                },
                {
                    "sent": "Could be asked that they are similar or differ.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's related to classification, where we have two sequences X one X2 from 2 unknown distributions, one and P2 and 3rd sequences generated from one of these, and we want to find which one of them.",
                    "label": 0
                },
                {
                    "sent": "Which one of them generates it?",
                    "label": 0
                },
                {
                    "sent": "It's equal and so if we could, if we had a good test for testing closeness between that is finding weather wise close to X1 or X2 then we also have a good test for classification.",
                    "label": 0
                },
                {
                    "sent": "So it's also related to other problems like clustering testing properties like independence applications include that of text categorization, testing, authorship of documents.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, so back to the problem.",
                    "label": 0
                },
                {
                    "sent": "So test it levels each pair of sequences as either same or different to indicate whether the distributions are same or different, and for simplicity in this talk, when we say similar, we're only going to consider when the distributions are the same and for different.",
                    "label": 1
                },
                {
                    "sent": "By different we mean that the distributions are separated in some distance, will make this clear as we go along.",
                    "label": 1
                },
                {
                    "sent": "There are property of the test for a particular distribution.",
                    "label": 0
                },
                {
                    "sent": "Pair is the probability that it pacifies probability that it labels the sequence pairs incorrectly.",
                    "label": 0
                },
                {
                    "sent": "So if the distributions are similar, it's the probability that it labels them as different and that is incorrectly labels them as different.",
                    "label": 0
                },
                {
                    "sent": "Similarly for different.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To motivate one such closeness test, we can relate it to composite hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "So think of two classes PSM&P different, one consisting of distribution pairs which are identical and the other consisting of separated distributions and given a sequence where we want to find which of these.",
                    "label": 0
                },
                {
                    "sent": "The distribution pair belongs to which of these two classes so?",
                    "label": 0
                },
                {
                    "sent": "A common technique for this, for composite hypothesis testing is generalized likelihood ratio test, where you are one looks at the ratio of the maximum likelihood of the observation under the two hypothesis and compares it to a threshold.",
                    "label": 1
                },
                {
                    "sent": "So in this case 1 looks at the maximum likelihood of X1 and X2 under two different distributions versus under the same distribution.",
                    "label": 0
                },
                {
                    "sent": "And if this ratio is too large then.",
                    "label": 0
                },
                {
                    "sent": "Of one says that it's different or if it's not too large.",
                    "label": 0
                },
                {
                    "sent": "We say that it seems.",
                    "label": 0
                },
                {
                    "sent": "So, so it's a test clear.",
                    "label": 0
                },
                {
                    "sent": "So in the denominator we have we have the same distribution, maximizing X1 and X2, whereas in the numerator we don't have that restriction.",
                    "label": 0
                },
                {
                    "sent": "So actually this ratio is always going to be bigger than one because the numerator is less restrictive.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But OK so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so for sequences we know that the empirical distribution maximizes the sequence likelihood.",
                    "label": 1
                },
                {
                    "sent": "So if one and two are the number of references of the symbols A1A two and so on.",
                    "label": 0
                },
                {
                    "sent": "In the sequence of these, the maximum likelihood expression.",
                    "label": 0
                },
                {
                    "sent": "So this test is going to be of this form.",
                    "label": 0
                },
                {
                    "sent": "So and one end into higher the number of appearances of the symbol AI in the first sequence and 2nd sequence.",
                    "label": 0
                },
                {
                    "sent": "So I hope this this part is clear.",
                    "label": 0
                },
                {
                    "sent": "So indeed one can show that when the two distributions are the same, this ratio is going to be small.",
                    "label": 0
                },
                {
                    "sent": "That is, it's going to be polynomial with high probability, by the way, so it still grows in the alphabet size K. On the other hand, when the distributions are different, let's say the L1 distance between the distributions is bigger than epsilon, then this quantity is going to be exponentially large with high probability.",
                    "label": 0
                },
                {
                    "sent": "So if you compare this ratio to a threshold which is some polynomial bigger than.",
                    "label": 0
                },
                {
                    "sent": "This quantity, then you have a test whose error probability is small.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm skipping this example.",
                    "label": 0
                },
                {
                    "sent": "Suppose you took hundred coin tosses from 2 similar coins.",
                    "label": 0
                },
                {
                    "sent": "This ratio is going to be small.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, from very different points then this ratio is going to be really low.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, so let's see what happens when the alphabet size is large.",
                    "label": 0
                },
                {
                    "sent": "So suppose think of an alphabet size of roughly NQ.",
                    "label": 0
                },
                {
                    "sent": "Considered these two distributions, one of them is a Singleton and the other one it has half probability half on that same symbol A and the remaining half is uniformly distributed over a long tail.",
                    "label": 0
                },
                {
                    "sent": "So let's consider typical sequence pairs from first from P1 and P2, and then from both from.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happy too.",
                    "label": 0
                },
                {
                    "sent": "So when one of them is from P1, another from P2.",
                    "label": 0
                },
                {
                    "sent": "So the first sequence is all is and the second one is one such typical sequences half of them arrays and the remaining symbols just appeared once.",
                    "label": 0
                },
                {
                    "sent": "So we calculate this ratio.",
                    "label": 0
                },
                {
                    "sent": "It comes to be exponentially large, which is good because the distributions are different.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when both these when both of them are generated from P2.",
                    "label": 0
                },
                {
                    "sent": "We do the same calculation.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this ratio is even.",
                    "label": 0
                },
                {
                    "sent": "Sorry this this quantity is even larger than the first case, so no matter what we choose the threshold, this test is never going to work.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not going to classify both these pairs correctly.",
                    "label": 0
                },
                {
                    "sent": "So so one thing we observe is that I mean in this case one intuition for the maximum likelihood ratio test to work is that.",
                    "label": 0
                },
                {
                    "sent": "The maximizing distribution was the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "The If you look at the empirical distributions in the numerator and denominator, they are very different.",
                    "label": 0
                },
                {
                    "sent": "So in the first case it is 1/2 and 1 / N / N by two symbols.",
                    "label": 0
                },
                {
                    "sent": "But the next case it's it's much different.",
                    "label": 0
                },
                {
                    "sent": "But we still see that I mean these two sequences have structurally there.",
                    "label": 0
                },
                {
                    "sent": "There is something common in them in terms of how many symbols appeared once, and so on.",
                    "label": 0
                },
                {
                    "sent": "So we try to exploit the structure.",
                    "label": 0
                },
                {
                    "sent": "In a different test.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before that, some of the known results, so bottle and others had considered this problem of testing closeness in L1 distance and they have shown an algorithm whose error probability is less than Delta and takes K to the 2/3 samples case.",
                    "label": 1
                },
                {
                    "sent": "The alphabet size and has and they also show a matching lower bound that there is an example where you cannot distinguish between.",
                    "label": 0
                },
                {
                    "sent": "You cannot distinguish between the distributions using just came to the 2/3 samples.",
                    "label": 0
                },
                {
                    "sent": "So their test it's not exactly a likelihood ratio test, so for the high for the high probability symbols they use the empirical distribution.",
                    "label": 1
                },
                {
                    "sent": "But for the infrequent symbols, L1 distance is estimated using collisions or the repetitions.",
                    "label": 0
                },
                {
                    "sent": "There are also similar results by Valiant, although they have a slightly stronger, so the upper bound on the L1 distance for the similar distributions, that is, that's a constant strictly bounded away from zero, so that's why they require more samples.",
                    "label": 0
                },
                {
                    "sent": "There also a similar results by Kelly and others from last year, so they had shown that but.",
                    "label": 0
                },
                {
                    "sent": "Latest it, it's a number one.",
                    "label": 0
                },
                {
                    "sent": "It's meant for classification and Secondly it's they assume that all the symbols have probabilities in the same order and that is think of these as nearly uniform distributions, but they can go up to alphabet size up to N squared.",
                    "label": 0
                },
                {
                    "sent": "That is only taking square root K samples.",
                    "label": 1
                },
                {
                    "sent": "They are able to do this closeness testing regardless all of these tests depend on some prior knowledge on the.",
                    "label": 0
                },
                {
                    "sent": "An upper bound on the alphabet size, so will show tests which do not require any knowledge on the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this alphabet size.",
                    "label": 0
                },
                {
                    "sent": "So we started to motivate artists.",
                    "label": 0
                },
                {
                    "sent": "We start with a basic observation that in this problem we only the test should depend only on the structure of the sequences.",
                    "label": 1
                },
                {
                    "sent": "By that we mean that.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have these two.",
                    "label": 0
                },
                {
                    "sent": "So consider this pair of sequences and this where they are structurally same in the sense that so wherever you see in the symbol C in this first sequence where you see the symbol B in the second sequence for these two, these two are the same up to a permutation after the labeling of this.",
                    "label": 0
                },
                {
                    "sent": "Of these symbols so.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 1
                },
                {
                    "sent": "So we expect tests to have the same decision for for such.",
                    "label": 0
                },
                {
                    "sent": "If they are structurally the same.",
                    "label": 0
                },
                {
                    "sent": "In fact, indeed we can show that.",
                    "label": 0
                },
                {
                    "sent": "If the test is not symmetric, it's probability of error can be matched by another symmetric test, which which has this which has this property so.",
                    "label": 0
                },
                {
                    "sent": "So we incurred this structure by what we called pattern.",
                    "label": 0
                },
                {
                    "sent": "So for example, the pattern of the sequence this sequence BC is 1232.",
                    "label": 0
                },
                {
                    "sent": "The way we are encoding is that the first symbol gets the level one, the next different symbol gets the Level 2 and the next different symbol gets the Level 3 and so on a.",
                    "label": 0
                },
                {
                    "sent": "Similarly we can we can with the.",
                    "label": 0
                },
                {
                    "sent": "Structure of two sequences using what we call joint pattern so it so these are the pattern of the first sequence and then.",
                    "label": 0
                },
                {
                    "sent": "For the second sequence, when we convert the pattern, we use the labels from the from the first sequence.",
                    "label": 0
                },
                {
                    "sent": "For example C in the symbols.",
                    "label": 0
                },
                {
                    "sent": "He still gets the level to which wich Western level in the first sequence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Invite right.",
                    "label": 0
                },
                {
                    "sent": "So then we define pattern probabilities.",
                    "label": 1
                },
                {
                    "sent": "The probability of a pattern is the probability of observing a sequence with that pattern.",
                    "label": 1
                },
                {
                    "sent": "For what we mean is so for example, for the pattern 1213, it's the probability of observing any of these sequences ABC or ABCD which have this same pattern.",
                    "label": 0
                },
                {
                    "sent": "So the expression.",
                    "label": 0
                },
                {
                    "sent": "We define maximum likelihood of a pattern as the as its maximum likelihood under all possible distributions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarly, we can define joint pattern probabilities.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you have two sequences, it's the probability of all probably observing any sequence pair with.",
                    "label": 0
                },
                {
                    "sent": "With this joint pattern.",
                    "label": 0
                },
                {
                    "sent": "The first sequence being generated by the 1st distribution and the second sequence being generated by the 2nd distribution.",
                    "label": 0
                },
                {
                    "sent": "So we can likewise and then we define the pattern maximum likelihood of joint patterns to be the maximum likelihood under all possible distributions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "As expected, so we look at this ratio test where you look at the maximum look at the maximum.",
                    "label": 0
                },
                {
                    "sent": "Look at the ratio of the maximum likelihood of the joint pattern under two different distributions versus that under the same distribution, and then you compare to this threshold D1 over square root Delta.",
                    "label": 0
                },
                {
                    "sent": "Think of Delta is something between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So to prove to prove its performance bounds, we define a different type of distance measure.",
                    "label": 0
                },
                {
                    "sent": "So we say that distributions P1 and P2R N Delta different if for all equal pairs P3 we can find a test such that the error probability of the test for both P1P2 and P3P3 is less than Delta that is weak, and it is possible to distinguish between P1P2 and any P3P3.",
                    "label": 1
                },
                {
                    "sent": "There are probably less than 10 to these tests.",
                    "label": 0
                },
                {
                    "sent": "Could depend on Harry, could depend on P1P2P3, so of course I mean if they're not in Delta separable then we cannot hope to find a test which which has their probability less than Delta for both different distributions and same distributions.",
                    "label": 0
                },
                {
                    "sent": "So here's our first main result that.",
                    "label": 1
                },
                {
                    "sent": "And this test it has low error probability whenever the distributions are same or under different.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you can distinguish P1 and P2 with other property less than Delta, using N samples, then this test can give you error probability square root Delta times E to the end, and the suffix potential factor.",
                    "label": 0
                },
                {
                    "sent": "So this is the error probability of the test when using end length sequences.",
                    "label": 0
                },
                {
                    "sent": "The definition doesn't depend on if you could take it.",
                    "label": 0
                },
                {
                    "sent": "Swimming test always gives the same answer.",
                    "label": 0
                },
                {
                    "sent": "Then this information will always be met.",
                    "label": 0
                },
                {
                    "sent": "This no because I so P3 is the set of.",
                    "label": 0
                },
                {
                    "sent": "So let's say that you have a trivial test which always outputs different.",
                    "label": 0
                },
                {
                    "sent": "Then there are probably for the same pair will be.",
                    "label": 0
                },
                {
                    "sent": "We set that.",
                    "label": 0
                },
                {
                    "sent": "So P3 is a similar field so.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so it should.",
                    "label": 0
                },
                {
                    "sent": "It should have lower probability for both these cases.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can go back to the large alphabet example.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, at least as a result here.",
                    "label": 0
                },
                {
                    "sent": "Any questions there?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in that large alphabet example we can again see that if instead instead of the usual maximum likelihood if we use the pattern maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then indeed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then indeed, this even when the in the second case where we have the ratio, is large.",
                    "label": 0
                },
                {
                    "sent": "In this case it's it's going to be small, so one thing is that calculation of this pattern maximum likelihood is difficult in general.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not as straightforward as the as the usual empirical distribution for sequences.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So tomorrow will also provide a computationally efficient test, so for that we look at profiles of patterns, so profile simply conveys how many symbols appeared a given number of times.",
                    "label": 1
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "So profile of a pattern is Sophie.",
                    "label": 1
                },
                {
                    "sent": "One is the number of symbols which appeared once we do is the number of samples which appear twice and so on.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this pattern Two symbols appeared once no symbol appear twice there is one symbol which appeared three times, which is the symbol one, and so on.",
                    "label": 0
                },
                {
                    "sent": "So one can.",
                    "label": 0
                },
                {
                    "sent": "There is a one to one correspondence between profile between profiles.",
                    "label": 0
                },
                {
                    "sent": "An integer partitions think of even as the number of ones in the partition feature as a number of tools in the partition.",
                    "label": 0
                },
                {
                    "sent": "So it's well known that the number of profile the partition number is of the order of each of these square root N.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly for joint profile, so we can define joint profile similarly that how many symbols appeared is specific number of times in sequence one and MU two times in sequence 2.",
                    "label": 0
                },
                {
                    "sent": "Exactly so yeah, some people call it fingerprints or histogram of histograms.",
                    "label": 0
                },
                {
                    "sent": "So so this week and related to something like a joint joint partition, it's an analog of integer partitions where the first components of the parts add up to anyone and the second components of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part side of two and two, and one can similarly upper bound them as single.",
                    "label": 0
                },
                {
                    "sent": "Single partition says this subexponential quantity, although the this time it is end to the 2/3 instead of into the end to the 1/2.",
                    "label": 0
                },
                {
                    "sent": "So I'm.",
                    "label": 0
                },
                {
                    "sent": "So I'm not I'm not giving the detailed proof for the.",
                    "label": 0
                },
                {
                    "sent": "For the but they see to the entry 2 third, this number comes from primarily comes from this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And dismount.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the skipping.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spoof.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the for the computationally efficient test, the motivation is that one can consider an estimated 4 joint patterns which assigns equal probability estimate to all the profiles and equal probability estimate to all the patterns within a profile.",
                    "label": 0
                },
                {
                    "sent": "So, and this is the this quantity in fees, the number of patterns which have the same.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "File.",
                    "label": 0
                },
                {
                    "sent": "It's similar to the number of sequences which have the same empirical distribution, except that there is a factor which for discounting the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patients.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This test, which counts so.",
                    "label": 0
                },
                {
                    "sent": "So in this test we look at the ratio of the number of patterns which have the same total profile, same profile as XX1X2 versus the number of joint patterns which have the same joint profile as X one X2 and we compare this to a threshold and this test also has a has a similar property as the test on the maximum likelihood of as a maximum like maximum likelihood test so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, that's.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can even rephrase the results in terms of sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so as to conclude we had.",
                    "label": 0
                },
                {
                    "sent": "We looked at contests for closing closing stress which are which have lower probability whenever it's possible to do so and we saw two tests, one based on pattern maximum likelihood and the other on number of patterns with the same profile.",
                    "label": 1
                },
                {
                    "sent": "And some of these results are useful for maximum likelihood or patterns in.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll skip the ongoing work, which is to consider so this test was primarily useful for low error probabilities, but we are still working on a separate proof for any cleaner proof for larger Delta.",
                    "label": 0
                },
                {
                    "sent": "And we also want to directly use this for there is there is a more direct approach for classification rather than using closeness tests for them.",
                    "label": 0
                },
                {
                    "sent": "We are also looking at experimental results and among other things.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So I mean I have one question, is like technical and the other general.",
                    "label": 0
                },
                {
                    "sent": "The technical question.",
                    "label": 0
                },
                {
                    "sent": "I don't see why you look at patterns.",
                    "label": 0
                },
                {
                    "sent": "Patterns depend on the ordering, yes, but here it is distributed symmetrically texture.",
                    "label": 0
                },
                {
                    "sent": "That's a great question, so indeed.",
                    "label": 0
                },
                {
                    "sent": "I mean if you look at the test, I could have actually mentioned that in terms of profile probabilities also so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happened was this test.",
                    "label": 0
                },
                {
                    "sent": "It might eventually come to be coming out of nowhere, but somehow it was motivated by actually estimating the pattern probabilities rather than the profile probabilities.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but in practice, yes, because it's IID and independent.",
                    "label": 0
                },
                {
                    "sent": "It just depends on profile and the other person is.",
                    "label": 0
                },
                {
                    "sent": "So how do you result compared to the results of poor variant?",
                    "label": 0
                },
                {
                    "sent": "So yeah, they also have similar results.",
                    "label": 0
                },
                {
                    "sent": "So one thing is, as I mentioned, they depend on the alphabet size.",
                    "label": 0
                },
                {
                    "sent": "Secondly, in this case we are saying that we could do as long as you don't have to know the exactly, but you can estimate the alphabet size using something like the good chewing test or something like that.",
                    "label": 0
                },
                {
                    "sent": "That would be like an indirect.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's like adding one more step.",
                    "label": 0
                },
                {
                    "sent": "I mean we're doing it directly without even doing it directly or estimating.",
                    "label": 0
                },
                {
                    "sent": "It's alright.",
                    "label": 0
                },
                {
                    "sent": "So there are many cases where the alphabet size is much larger and yet you can separate them as in the example we saw it, so it would be useful for such cases.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry this may be a very naive and stupid question, but how does this compare to the sort of two using chi squared tests for comparing these things?",
                    "label": 0
                },
                {
                    "sent": "I mean, it seems I already just general statistics, classic statistical approach or you have samples from two different distributions and you do chi squared.",
                    "label": 0
                },
                {
                    "sent": "No, not necessarily.",
                    "label": 0
                },
                {
                    "sent": "You can use a chi squared test to compare.",
                    "label": 0
                },
                {
                    "sent": "Yes, you could have.",
                    "label": 0
                },
                {
                    "sent": "Potentially, you could use a chi square test, in fact, so the results from Benjamin Kelly last year they had.",
                    "label": 0
                },
                {
                    "sent": "They had tried using something similar in.",
                    "label": 0
                },
                {
                    "sent": "Potentially we could.",
                    "label": 0
                },
                {
                    "sent": "We could use.",
                    "label": 0
                },
                {
                    "sent": "Eventually it comes to, but I think even the CHI squared test fails for this large alphabet case.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can provide examples where it doesn't work for large alphabets.",
                    "label": 0
                },
                {
                    "sent": "Probably have time for one more.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}