{
    "id": "mwpstbn56c5xvhqgumgirggixyrbqysj",
    "title": "Action-Gap Phenomenon in Reinforcement Learning",
    "info": {
        "author": [
            "Amir-massoud Farahmand, Vector Institute"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_farahmand_actiongap/",
    "segmentation": [
        [
            "Suppose you have two choices of cheesecake and broccoli salad, and you want to choose the one that matters.",
            "But which one do you choose?"
        ],
        [
            "I believe most people choose the cheesecake because, well, it tastes much better and it's actually an easy choice for most people, because the difference between the quality or the value of their cheesecake compared to the broccoli salad is quite considerable.",
            "So this problem is easy because even though you may not have very accurate estimate after value of these two choices, but because the gap between the value of these two choices is quite considerable, you can choose optimally."
        ],
        [
            "But now consider the case that you have two cakes.",
            "Both of them are quite good, and if your estimate of the value function or the value of each of them is not very accurate, you may choose the wrong one.",
            "But still it's not a big deal because even if you choose the wrong action, you won't.",
            "You won't last much because both of them are good.",
            "So it seems that.",
            "What is actually important for the quality of your decision-making is crucially dependent on the gap between your best choice and the rest, so I try."
        ],
        [
            "To formalize this kind of intuition for a class of reinforcement learning problems that can be described by finite action, discounted Markov decision processes, and I try to answer this question, suppose we have some estimate Q hat of optimal action value function and Now our agent follows the greedy policy with respect to Q heads.",
            "The question is that what would be the performance of these greedy policy compared to the policy of?",
            "Small agents, so it turns out that the answer depends on their distribution of the action gap function, which is defined as the value.",
            "The difference between the value of the best choice and, say, the second best choice.",
            "And also it's appears that if the problem has a favorable action gap regularity, we can get faster convergence rate of the.",
            "Performance Lost Zero compared to the convergence rate of the action of the error in estimating the optimal action value function.",
            "So as very simple kind of example particular case, the left hand side in this equation is the performance loss which can be upper bounded by the error in the estimating of the optimal action value function to the power of 1 plus Zita if data is positive.",
            "The rate that we get is actually faster than the rates.",
            "The rate for performance loss would be faster rate for the estimating of the optimal action value function, and Interestingly this phenomena has similarities to allow noise condition in classification literature.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose you have two choices of cheesecake and broccoli salad, and you want to choose the one that matters.",
                    "label": 0
                },
                {
                    "sent": "But which one do you choose?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I believe most people choose the cheesecake because, well, it tastes much better and it's actually an easy choice for most people, because the difference between the quality or the value of their cheesecake compared to the broccoli salad is quite considerable.",
                    "label": 0
                },
                {
                    "sent": "So this problem is easy because even though you may not have very accurate estimate after value of these two choices, but because the gap between the value of these two choices is quite considerable, you can choose optimally.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now consider the case that you have two cakes.",
                    "label": 0
                },
                {
                    "sent": "Both of them are quite good, and if your estimate of the value function or the value of each of them is not very accurate, you may choose the wrong one.",
                    "label": 0
                },
                {
                    "sent": "But still it's not a big deal because even if you choose the wrong action, you won't.",
                    "label": 1
                },
                {
                    "sent": "You won't last much because both of them are good.",
                    "label": 0
                },
                {
                    "sent": "So it seems that.",
                    "label": 0
                },
                {
                    "sent": "What is actually important for the quality of your decision-making is crucially dependent on the gap between your best choice and the rest, so I try.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To formalize this kind of intuition for a class of reinforcement learning problems that can be described by finite action, discounted Markov decision processes, and I try to answer this question, suppose we have some estimate Q hat of optimal action value function and Now our agent follows the greedy policy with respect to Q heads.",
                    "label": 0
                },
                {
                    "sent": "The question is that what would be the performance of these greedy policy compared to the policy of?",
                    "label": 0
                },
                {
                    "sent": "Small agents, so it turns out that the answer depends on their distribution of the action gap function, which is defined as the value.",
                    "label": 0
                },
                {
                    "sent": "The difference between the value of the best choice and, say, the second best choice.",
                    "label": 0
                },
                {
                    "sent": "And also it's appears that if the problem has a favorable action gap regularity, we can get faster convergence rate of the.",
                    "label": 0
                },
                {
                    "sent": "Performance Lost Zero compared to the convergence rate of the action of the error in estimating the optimal action value function.",
                    "label": 0
                },
                {
                    "sent": "So as very simple kind of example particular case, the left hand side in this equation is the performance loss which can be upper bounded by the error in the estimating of the optimal action value function to the power of 1 plus Zita if data is positive.",
                    "label": 0
                },
                {
                    "sent": "The rate that we get is actually faster than the rates.",
                    "label": 0
                },
                {
                    "sent": "The rate for performance loss would be faster rate for the estimating of the optimal action value function, and Interestingly this phenomena has similarities to allow noise condition in classification literature.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}