{
    "id": "d4mwjtt5zhiygwsw43b5girwrqmckyxv",
    "title": "Normalized Cut meets MRF",
    "info": {
        "author": [
            "Meng Tang, Department of Computer Science, University of Western Ontario"
        ],
        "published": "Oct. 24, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2016_tang_normalized_cut/",
    "segmentation": [
        [
            "Hi everyone, this is MTM.",
            "I'm presenting the joint work when insert Emitree is Maya and Yuri.",
            "I start by a brief overview of normalized cut and Markov random field."
        ],
        [
            "First, low mascot is a very popular graph clustering criterium introduced by Sir and many people have used it for segmentation and image clustering.",
            "It is based on the ratios of associations.",
            "For this talk I'm going to switch to another."
        ],
        [
            "The standard formulation with a fitting matrix.",
            "Here D is a vector of no degrees.",
            "So."
        ],
        [
            "Second Markov random field, or more generally graphic models, minimize and linear combinations of energy potentials defined on clicks.",
            "Let me give some exam."
        ],
        [
            "Polls of common clicks.",
            "The pairwise click or pause model is used in segmentation to achieve edge alignment.",
            "PM"
        ],
        [
            "Pods generalizes to encourage consistent labeling among a group of pixels."
        ],
        [
            "Example of all pixels in one click is the labor cost term.",
            "This will combine."
        ],
        [
            "Nice normalized cut with MF as a single joint energy.",
            "Such combination is not possible before due to the significant difference in optimization, formless card and optimization.",
            "For MF.",
            "Alexa few slides justifies such combination."
        ],
        [
            "Here are some typical normalized cut applications that can benefit from MF.",
            "First, it is widely recognized that normalized cut segmentation comes with a weak edge alignment.",
            "People attempted to address this issue by post processing or approach combines normalized cut with the standard parts that would penalize such weak edges.",
            "Second, it is challenging to incorporate the semi supervision for luminous cut.",
            "These works have to reformulate long.",
            "Let's cut in different ways or unified framework utilizes potencies to incorporate a semi supervision."
        ],
        [
            "The third example is image clustering, where available tags can be used by PM parts for tech consistency prior and the last example is labor cost term that can help determine a number of clusters automatically.",
            "For a normalized cut.",
            "So beyond these four examples or our framework enables integration with any organizable MI potentials."
        ],
        [
            "The actual mascot term can also help.",
            "Typical MF applications, so this is a common graphic model for segmentation where the smoothest term is combined with log probabilities.",
            "If the appearance models are not given, it is common to treat them as variables and iteratively optimized with segmentation S and the appearance model parameters set up.",
            "For now, let's remove the smoothest term.",
            "The remaining likelihood term can be seen as a formulations of K means.",
            "Indeed, if probability P is a single Gaussian of fixed variance, this is exactly K means for general probability model.",
            "This becomes probabilistic means, which is well known in machine learning, let's see."
        ],
        [
            "How adaptive iterative model fitting works when using highly descriptive problem model as it's done in graphic card?",
            "Iterative optimization of the likelihood term corresponds to color space clustering with initializations from colors inside and outside the box, while customer is not a conventional interpretations of model fitting and this becomes more obvious once we realize the segmentation in the color space.",
            "This is a representative results of what happens if the smooth term is omitted in graphpad.",
            "So basically the probabilistic K means needs to.",
            "For clustering, where the highly descriptive models overfit the data and this issue is even more severe with higher dimensional data where the data becomes sparser."
        ],
        [
            "Simply replace the log probabilities by away stab Lish normalized cut.",
            "We can achieve much better color space clustering even without a smooth this term.",
            "So or join the energy of normalized cut and MF brings together the complementary benefit of where established balanced clustering criteria and powerful MI regulation techniques.",
            "So we use a bound optimization from we use.",
            ", Tory opposition from the MF literature can sweet treats the normalized cut term some kind of high order energy potential."
        ],
        [
            "Bond optimization scheme is used.",
            "Let me just briefly give overview.",
            "So instead of optimizing the original energy, which is difficult to optimize.",
            "We optimize the upper bound in red and estimate the current solution to the optimal as abound.",
            "Such scheme is iterative and eat guarantees energy decrease."
        ],
        [
            "As I said earlier, we're going to use combinatorial optimization designed for MF, so the key step is just to derive a unity bound for NC.",
            "For this work, we propose to bound the kernel bound and the spectral bound for normalized cut.",
            "Now first focus on the kernel bound."
        ],
        [
            "We observe normalize, cut objective to be concave.",
            "Then it becomes obvious that the 1st order Taylor expansion is an upper bound.",
            "And the first expansion is also linear with respect to the binary indicator variables as.",
            "Well, easiest Lee iteratively optimizing our kernel bound is equivalent to the kernel K means procedure proposed by Dillon at all.",
            "So besides simplicity, the main advantage of all kernel bound is it allows us to integrate with actual MF constraints."
        ],
        [
            "So now I switch you to the second bound, the spectral bound which we obtain through approximations of Romanesque art objective.",
            "We are interested in rank N approximation of the affinity matrix by minimizing the Frobenius error.",
            "The solution to this problem can be found in many textbooks.",
            "We just need to take the top M eigenvectors or an action values.",
            "Now let's consider no dimensional points using this formula.",
            "For example, we can even visualize them for three dimensional points.",
            "When N = 3.",
            "What's special about this point is that the dot product of these points forms 88, which is approximations of the original affinity matrix A.",
            "In other words, these are approximate isometric embeddings of the original affinity matrix, where we use such isometry arguments, derive the spectral bound which will be clear on the next slide."
        ],
        [
            "So replace a by 80th.",
            "Then it is possible to further rewrite the objective as K means energy on low dimensional embeddings.",
            "Or this is simple linear algebra using the isometry argument discussed earlier and plug in the centres mu?",
            "Actually, we are not the first to make the connections between normalized cut and K means.",
            "The previous work by Buck Jordan and dinner at all shows an equivalence of normalized and cut and K means for exact high dimensional embedding.",
            "However, we propose approximations by K means using a low dimensional embedding."
        ],
        [
            "Having seen the approximations of luminous cutback, K means and remember all goal is to derive an upper bound for normalized cut.",
            "At this point we only need to have bound for K means.",
            "This is indeed trivial.",
            "K means is near with respect to the binary indicator variables and simply plug in the current view.",
            "That's going to be or spectral bound, and it is unary.",
            "Indeed, K means is typically used at discretization heuristic for a specialization of normalized card.",
            "In this work with justified usage of K means for a low rank approximation."
        ],
        [
            "And in our experiments it is confirmed that K means is a good approximation of normalized cut.",
            "An extra degree of freedom of all work is the dimensionality.",
            "With higher dimensions for the embedding, we can obtain a closer approximation, but not necessarily lower energy.",
            "In"
        ],
        [
            "The end this is the framework of our optimization, so we use either kernel bound or Spectra bound and for optimizing the bound we utilized move makings and graph cuts algorithm.",
            "We term or algorithms as Colonel Cut and spectral cut.",
            "We do have lots of experiments in our paper.",
            "I'm just going to highlight, uh?"
        ],
        [
            "Here the goal is to use available image tags to improve image clustering.",
            "We optimize the objective of normalized card combined with the PM parts.",
            "Note that the tax allowances, so the critical thing is the robustness of the PM post model.",
            "As you can see from the plot or kernel kadanna spectral card achieved a boosted image clustering results when small tags used."
        ],
        [
            "We also experiment with Kenneth Neighbor Kernel on the deep features and we observed a similar trend of improved clustering."
        ],
        [
            "This is typical MF applications the motion segmentation, so here only by utilizing all the features can we distinguish the instances.",
            "For example, the docs that have the same appearance.",
            "They are close to each other and they move incoherent way.",
            "When it comes to high dimensional features, model fitting approaches would completely fair.",
            "Or kernel card algorithms achieve a reasonable segmentation for these challenging motion segmentation examples.",
            "We"
        ],
        [
            "For more experiments where come to our posters?",
            "To conclude."
        ],
        [
            "Here we propose two bounds for normalized cuts.",
            "Either through a spectrum or a normalized either through kernel or spectral method work.",
            "And we combine normalized cut with any optimizable MF constraints.",
            "Or you can think a single of our method as combining MF with balanced clustering criterion.",
            "The take home messages that, or a kernel an spectrum method for clustering scales really well.",
            "Too high dimensional features which is everywhere in computer vision, and while those model fitting based.",
            "Approach wood is prone to local minimums.",
            "And that concludes my talk, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone, this is MTM.",
                    "label": 0
                },
                {
                    "sent": "I'm presenting the joint work when insert Emitree is Maya and Yuri.",
                    "label": 0
                },
                {
                    "sent": "I start by a brief overview of normalized cut and Markov random field.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, low mascot is a very popular graph clustering criterium introduced by Sir and many people have used it for segmentation and image clustering.",
                    "label": 0
                },
                {
                    "sent": "It is based on the ratios of associations.",
                    "label": 0
                },
                {
                    "sent": "For this talk I'm going to switch to another.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The standard formulation with a fitting matrix.",
                    "label": 0
                },
                {
                    "sent": "Here D is a vector of no degrees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second Markov random field, or more generally graphic models, minimize and linear combinations of energy potentials defined on clicks.",
                    "label": 0
                },
                {
                    "sent": "Let me give some exam.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Polls of common clicks.",
                    "label": 0
                },
                {
                    "sent": "The pairwise click or pause model is used in segmentation to achieve edge alignment.",
                    "label": 1
                },
                {
                    "sent": "PM",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pods generalizes to encourage consistent labeling among a group of pixels.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of all pixels in one click is the labor cost term.",
                    "label": 0
                },
                {
                    "sent": "This will combine.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice normalized cut with MF as a single joint energy.",
                    "label": 1
                },
                {
                    "sent": "Such combination is not possible before due to the significant difference in optimization, formless card and optimization.",
                    "label": 0
                },
                {
                    "sent": "For MF.",
                    "label": 0
                },
                {
                    "sent": "Alexa few slides justifies such combination.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some typical normalized cut applications that can benefit from MF.",
                    "label": 0
                },
                {
                    "sent": "First, it is widely recognized that normalized cut segmentation comes with a weak edge alignment.",
                    "label": 1
                },
                {
                    "sent": "People attempted to address this issue by post processing or approach combines normalized cut with the standard parts that would penalize such weak edges.",
                    "label": 1
                },
                {
                    "sent": "Second, it is challenging to incorporate the semi supervision for luminous cut.",
                    "label": 0
                },
                {
                    "sent": "These works have to reformulate long.",
                    "label": 0
                },
                {
                    "sent": "Let's cut in different ways or unified framework utilizes potencies to incorporate a semi supervision.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third example is image clustering, where available tags can be used by PM parts for tech consistency prior and the last example is labor cost term that can help determine a number of clusters automatically.",
                    "label": 0
                },
                {
                    "sent": "For a normalized cut.",
                    "label": 0
                },
                {
                    "sent": "So beyond these four examples or our framework enables integration with any organizable MI potentials.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual mascot term can also help.",
                    "label": 0
                },
                {
                    "sent": "Typical MF applications, so this is a common graphic model for segmentation where the smoothest term is combined with log probabilities.",
                    "label": 0
                },
                {
                    "sent": "If the appearance models are not given, it is common to treat them as variables and iteratively optimized with segmentation S and the appearance model parameters set up.",
                    "label": 0
                },
                {
                    "sent": "For now, let's remove the smoothest term.",
                    "label": 0
                },
                {
                    "sent": "The remaining likelihood term can be seen as a formulations of K means.",
                    "label": 0
                },
                {
                    "sent": "Indeed, if probability P is a single Gaussian of fixed variance, this is exactly K means for general probability model.",
                    "label": 0
                },
                {
                    "sent": "This becomes probabilistic means, which is well known in machine learning, let's see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How adaptive iterative model fitting works when using highly descriptive problem model as it's done in graphic card?",
                    "label": 0
                },
                {
                    "sent": "Iterative optimization of the likelihood term corresponds to color space clustering with initializations from colors inside and outside the box, while customer is not a conventional interpretations of model fitting and this becomes more obvious once we realize the segmentation in the color space.",
                    "label": 1
                },
                {
                    "sent": "This is a representative results of what happens if the smooth term is omitted in graphpad.",
                    "label": 0
                },
                {
                    "sent": "So basically the probabilistic K means needs to.",
                    "label": 0
                },
                {
                    "sent": "For clustering, where the highly descriptive models overfit the data and this issue is even more severe with higher dimensional data where the data becomes sparser.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simply replace the log probabilities by away stab Lish normalized cut.",
                    "label": 0
                },
                {
                    "sent": "We can achieve much better color space clustering even without a smooth this term.",
                    "label": 0
                },
                {
                    "sent": "So or join the energy of normalized cut and MF brings together the complementary benefit of where established balanced clustering criteria and powerful MI regulation techniques.",
                    "label": 1
                },
                {
                    "sent": "So we use a bound optimization from we use.",
                    "label": 0
                },
                {
                    "sent": ", Tory opposition from the MF literature can sweet treats the normalized cut term some kind of high order energy potential.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bond optimization scheme is used.",
                    "label": 0
                },
                {
                    "sent": "Let me just briefly give overview.",
                    "label": 0
                },
                {
                    "sent": "So instead of optimizing the original energy, which is difficult to optimize.",
                    "label": 0
                },
                {
                    "sent": "We optimize the upper bound in red and estimate the current solution to the optimal as abound.",
                    "label": 0
                },
                {
                    "sent": "Such scheme is iterative and eat guarantees energy decrease.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said earlier, we're going to use combinatorial optimization designed for MF, so the key step is just to derive a unity bound for NC.",
                    "label": 0
                },
                {
                    "sent": "For this work, we propose to bound the kernel bound and the spectral bound for normalized cut.",
                    "label": 1
                },
                {
                    "sent": "Now first focus on the kernel bound.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We observe normalize, cut objective to be concave.",
                    "label": 0
                },
                {
                    "sent": "Then it becomes obvious that the 1st order Taylor expansion is an upper bound.",
                    "label": 0
                },
                {
                    "sent": "And the first expansion is also linear with respect to the binary indicator variables as.",
                    "label": 0
                },
                {
                    "sent": "Well, easiest Lee iteratively optimizing our kernel bound is equivalent to the kernel K means procedure proposed by Dillon at all.",
                    "label": 0
                },
                {
                    "sent": "So besides simplicity, the main advantage of all kernel bound is it allows us to integrate with actual MF constraints.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I switch you to the second bound, the spectral bound which we obtain through approximations of Romanesque art objective.",
                    "label": 0
                },
                {
                    "sent": "We are interested in rank N approximation of the affinity matrix by minimizing the Frobenius error.",
                    "label": 0
                },
                {
                    "sent": "The solution to this problem can be found in many textbooks.",
                    "label": 0
                },
                {
                    "sent": "We just need to take the top M eigenvectors or an action values.",
                    "label": 1
                },
                {
                    "sent": "Now let's consider no dimensional points using this formula.",
                    "label": 0
                },
                {
                    "sent": "For example, we can even visualize them for three dimensional points.",
                    "label": 0
                },
                {
                    "sent": "When N = 3.",
                    "label": 0
                },
                {
                    "sent": "What's special about this point is that the dot product of these points forms 88, which is approximations of the original affinity matrix A.",
                    "label": 0
                },
                {
                    "sent": "In other words, these are approximate isometric embeddings of the original affinity matrix, where we use such isometry arguments, derive the spectral bound which will be clear on the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So replace a by 80th.",
                    "label": 0
                },
                {
                    "sent": "Then it is possible to further rewrite the objective as K means energy on low dimensional embeddings.",
                    "label": 0
                },
                {
                    "sent": "Or this is simple linear algebra using the isometry argument discussed earlier and plug in the centres mu?",
                    "label": 0
                },
                {
                    "sent": "Actually, we are not the first to make the connections between normalized cut and K means.",
                    "label": 0
                },
                {
                    "sent": "The previous work by Buck Jordan and dinner at all shows an equivalence of normalized and cut and K means for exact high dimensional embedding.",
                    "label": 1
                },
                {
                    "sent": "However, we propose approximations by K means using a low dimensional embedding.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having seen the approximations of luminous cutback, K means and remember all goal is to derive an upper bound for normalized cut.",
                    "label": 0
                },
                {
                    "sent": "At this point we only need to have bound for K means.",
                    "label": 1
                },
                {
                    "sent": "This is indeed trivial.",
                    "label": 0
                },
                {
                    "sent": "K means is near with respect to the binary indicator variables and simply plug in the current view.",
                    "label": 0
                },
                {
                    "sent": "That's going to be or spectral bound, and it is unary.",
                    "label": 1
                },
                {
                    "sent": "Indeed, K means is typically used at discretization heuristic for a specialization of normalized card.",
                    "label": 1
                },
                {
                    "sent": "In this work with justified usage of K means for a low rank approximation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in our experiments it is confirmed that K means is a good approximation of normalized cut.",
                    "label": 0
                },
                {
                    "sent": "An extra degree of freedom of all work is the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "With higher dimensions for the embedding, we can obtain a closer approximation, but not necessarily lower energy.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The end this is the framework of our optimization, so we use either kernel bound or Spectra bound and for optimizing the bound we utilized move makings and graph cuts algorithm.",
                    "label": 1
                },
                {
                    "sent": "We term or algorithms as Colonel Cut and spectral cut.",
                    "label": 1
                },
                {
                    "sent": "We do have lots of experiments in our paper.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to highlight, uh?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the goal is to use available image tags to improve image clustering.",
                    "label": 1
                },
                {
                    "sent": "We optimize the objective of normalized card combined with the PM parts.",
                    "label": 0
                },
                {
                    "sent": "Note that the tax allowances, so the critical thing is the robustness of the PM post model.",
                    "label": 0
                },
                {
                    "sent": "As you can see from the plot or kernel kadanna spectral card achieved a boosted image clustering results when small tags used.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also experiment with Kenneth Neighbor Kernel on the deep features and we observed a similar trend of improved clustering.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is typical MF applications the motion segmentation, so here only by utilizing all the features can we distinguish the instances.",
                    "label": 0
                },
                {
                    "sent": "For example, the docs that have the same appearance.",
                    "label": 0
                },
                {
                    "sent": "They are close to each other and they move incoherent way.",
                    "label": 0
                },
                {
                    "sent": "When it comes to high dimensional features, model fitting approaches would completely fair.",
                    "label": 0
                },
                {
                    "sent": "Or kernel card algorithms achieve a reasonable segmentation for these challenging motion segmentation examples.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For more experiments where come to our posters?",
                    "label": 0
                },
                {
                    "sent": "To conclude.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we propose two bounds for normalized cuts.",
                    "label": 1
                },
                {
                    "sent": "Either through a spectrum or a normalized either through kernel or spectral method work.",
                    "label": 1
                },
                {
                    "sent": "And we combine normalized cut with any optimizable MF constraints.",
                    "label": 0
                },
                {
                    "sent": "Or you can think a single of our method as combining MF with balanced clustering criterion.",
                    "label": 1
                },
                {
                    "sent": "The take home messages that, or a kernel an spectrum method for clustering scales really well.",
                    "label": 0
                },
                {
                    "sent": "Too high dimensional features which is everywhere in computer vision, and while those model fitting based.",
                    "label": 0
                },
                {
                    "sent": "Approach wood is prone to local minimums.",
                    "label": 0
                },
                {
                    "sent": "And that concludes my talk, thanks.",
                    "label": 0
                }
            ]
        }
    }
}