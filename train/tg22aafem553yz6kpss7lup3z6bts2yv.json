{
    "id": "tg22aafem553yz6kpss7lup3z6bts2yv",
    "title": "Learning Deep Generative Models",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_salakhutdinov_generative_models/",
    "segmentation": [
        [
            "So I."
        ],
        [
            "Let me start by basically saying that you know we live in an age of big data, right?",
            "If you look at the space of images, language is the mic working OK?",
            "Is it better now?",
            "OK, fantastic.",
            "I would argue that most of the data that we see is unlabeled, right?",
            "And probably throughout this summer school you've seen a lot of talks that you supervised learning in this particular.",
            "In today's lecture we're going to be talking mostly about unsupervised learning or learning generative models.",
            "And you know one of the things that I'm interested in, and there's a lot of people in our community interested is how can we develop statistical models, models that can discover interesting kinds of structures in unsupervised?",
            "Or maybe semi supervised way, right?"
        ],
        [
            "And what I would argue is that one way of doing that would be to build these deep hierarchical models, generative models, models that support inference, as well as discover structured multiple multiple levels of representation.",
            "And I'll make it more precise as I go through the top."
        ],
        [
            "Let me just show you one application area.",
            "I'm going to talk about it a little bit.",
            "Understanding images how many you know who that is?",
            "Ah, great, now I remember Antonio actually gave a talk here, right?",
            "So this is the picture of Antonio that I took at Nips are year and a half ago.",
            "It was in Montreal's over here and you can ask the system to say, well, can you tag it right?",
            "Can you?",
            "Can you associate words with that image and you can build a system that says strangers, coworkers and such, which is probably reasonable, but then you can go beyond that, right?",
            "You can say, well, can you actually build a system that generates captions or generates natural language descriptions?",
            "And one way of doing that, I would say, kind of you know the.",
            "Straightforward way of doing that would be to find a similar image, maybe maybe using cover that features and then copy the corresponding captions right from the training set.",
            "That would be a nearest neighbor approach if you do that that's nearest neighbor sentence says people taking pictures of a crazy person.",
            "That's actually true.",
            "That was the closest image that the system found.",
            "Antonio really liked it because it was online and you can actually retrieve.",
            "But then if you actually build a model you know this is what the model would say.",
            "Group of people in the crowded area, group of people are walking, talking group of people standing around and walking in sunshine.",
            "So this is 1.",
            "One interesting application error that I'm going to talk about later in the talk."
        ],
        [
            "OK, but for the purposes of this tutorial, what I'm going to do is I'm structuring the tutorial into two parts.",
            "The first part will be focusing on learning undirected graphical models.",
            "So deep undirected graphical models in particular models such as restricted both machines and both machines, and I'll try to go into mathematical details about these models and how we can train them.",
            "And the second part of the tutorial is going to be focusing a little bit more on directed graphical models, so these would be helpful as machines and will go in.",
            "Algorithms and training procedures for variational autoencoders and importance weighted autoencoders.",
            "Because these are the models that are being heavily used across.",
            "If you go to HTML this year, last year you would probably see a lot of people are looking at generative models.",
            "Deep generative models, including variational think orders.",
            "And then I'm also going to talk a little bit about stochastic attention model so hard attention models and show the connection between these models to help those machines.",
            "And then now finish up with some applications.",
            "So some open questions."
        ],
        [
            "OK, let me start with restricted.",
            "Boltzmann is how many of you know about what carbs are?",
            "A few of you that's good, so these are very old models.",
            "I guess they were basically developed back in 86.",
            "You basically you can think of GBM as a Markov random field.",
            "You have stochastic binary hidden variables H and you have stochastic visible variables.",
            "Those are binary as well V. You can think of the variables as pixels in your images and you can think of H as feature detectors.",
            "So hidden or latent latent variables right?",
            "You can specify the joint distribution over the observed invisible variables through that particular equation.",
            "Over here you have pairwise so called.",
            "Sometimes these are called pairwise potentials or pairwise interactions, and you have unary interactions right.",
            "Sometimes these models are also called log linear models, Markov random fields, both machines, But the reason why they call log linear models.",
            "Because if you take the log of the probability, the log of the probability, then you basically have a linear dependence on the parameters W, right?",
            "And you can think of parameters W is trying to learn some sort of correlations between observed and hidden variables.",
            "From that particular model in the structure of this model, you can define the conditional probability and the conditional probability is going to be given by product of so called logistic functions, right?",
            "So sometimes these models are called generative models because what they do is that given the states of the hidden variables, you can generate the data, right?",
            "You can sample the observed variables.",
            "We can see what your images would look like.",
            "So."
        ],
        [
            "What kind of features these models are learning?",
            "If you apply them to, you know these handwritten characters data set, then your features look like little edges, so little strokes.",
            "Right and now given you image, you can basically say, well, this new image is made up by some of some of these.",
            "Combination of these edges.",
            "In these numbers here are given by the probabilities that the particular hidden unit or particular hidden variables being activated, and you typically get sparse representation."
        ],
        [
            "Now how can we do learning in these models?",
            "Well, one way of doing learning is, given a set of ID training examples you want to run model parameters and the way to do it would be to maximize the log likelihood objective.",
            "That's one way of optimizing these models, and this log likelihood objective also should should make sense.",
            "What it's basically saying is that given the training data, if I look at the joint probability of observing the training data, I want the joint probability to be as high as possible, right?",
            "So finally parameters so that this joint probability is as high as possible.",
            "And in the log space is just a sum of logs.",
            "You can differentiate the log likelihood that the standard way of deriving learning learning rules for these models is if you if you take derivative of log language with respect to your parameters, then you know there is a little bit of math.",
            "It's not very difficult, just a little bit of algebra, but it comes down to being the difference between two terms right?",
            "The first term is so-called expected sufficient statistics where expectations driven by the data and the second term is expected.",
            "Sufficient statistics driven by the model.",
            "So the intuition behind the learning rule is basically saying, well, look at the correlations that you see in the data.",
            "Effectively look at the correlations that the model is implying given the parameters of the model, and you try to match the two.",
            "And that's the learning rule for.",
            "Basically, for any kind of undirected graphical model, if you work with Markov random fields, both machines conditional random fields, you know there's a lot of work on CRF's.",
            "Basically, that's that's the matching rule for all of those models.",
            "It turns out that the first expectation is easy to compute because of a particular structure of restricted Boltzmann machine, right?",
            "You don't have any lateral connections between the hidden variables.",
            "We can actually compute the first term exactly.",
            "The second term is something that you cannot compute exactly because it requires summing over exponentially many configurations.",
            "So so in this case."
        ],
        [
            "If you look at the learning rule, the first term is easy to compute.",
            "The second term is the sum of all possible configurations of the visible and hidden variables, which is an exponential salmon.",
            "Typically, you know most successful algorithms are using some form of Markov chain Monte Carlo to approximate the second term.",
            "There's been a lot of work on in graphical models trying to use pseudo likelihood piecewise likelihood and all different variational inference better free approximation, but for these kinds of models it seems that.",
            "Markov chain Monte Carlo, in particular something called persistent contrastive divergent and constructive divergences, is the way to do it."
        ],
        [
            "Now, one of the things about these models is then you can create.",
            "You can model other kinds of distributions.",
            "So for example you can build so-called Gaussian Bernoulli Boltzmann machine restricted both machines where you have stochastic real valued visible variables V and you just change a little bit of a definition of the model.",
            "So you have sort of Gaussian visible variables.",
            "You ask the castec binary latent variables.",
            "And again here the conditional probability is going to be given by the product of normal distributions, right?",
            "So that basically allows you to model, let's say if you want to model grayscale images.",
            "If you want to model real value data, you sort of can create these hybrid graphical models where some variables are binary.",
            "Some variables are real valued."
        ],
        [
            "And if you apply this model to images image kind of data then you find these."
        ],
        [
            "Edge like structure.",
            "So given a new image you can basically do exactly the same thing.",
            "You can say, well this new image is made up by some linear combination of subset of the learn features right?",
            "And these numbers again here given the probabilities of that particular feature appearing in the data."
        ],
        [
            "You can also again take the model modified a little bit.",
            "And you can this.",
            "In this case you can model count data and count data is very useful when you're modeling things like documents, word counts, right?",
            "So in this case the conditional probability is going to be given by the softmax distribution.",
            "So imagine that you have a document that contains the words and the vocabulary size they were working in.",
            "English is maybe 100,000, so K would be over the size of 100,000.",
            "And if you."
        ],
        [
            "Apply this model to working with count data.",
            "In this case it's about 800,000 news stories.",
            "You can find sort of these other interesting topics right, so in this case, again every single document is given by a combination of of these topics, right?",
            "So one thing that I want to highlight here is that you know if you apply these models to count data to find latent topics.",
            "If you applied to image data, you find features so different.",
            "In different areas you can find interesting interesting kind of low level structure."
        ],
        [
            "Right, so you can model different data modalities.",
            "You can basically extend these models to modeling any kind of.",
            "Do other members with exponential family models?",
            "And one of the things about restricted both machines is very it's very easy to infer the states of the hidden variables, right?",
            "So for example here the conditional probability of hidden states given the visible states is again given by the product of logistic functions.",
            "So what does that mean?",
            "That basically means that given the data, you can instantly figure out the distribution of the topics of distribution of features.",
            "So you can.",
            "I can tell you exactly what features you see in your data, what topics issues you should see in the data.",
            "So that's very useful.",
            "Probably, and in particular applications like information retrieval and recognition.",
            "It's an important feature to have."
        ],
        [
            "Sometimes these models are called product models, so that's.",
            "Product of experts and back in the late 90s Jeff Hinton was working with these models along with the other researches, but the idea behind product models is that if you look at the joint distribution, the joint distribution observed in hidden variables.",
            "Its belongs to exponential family models, right?",
            "It's a log linear model, but if you look at the marginal distribution, if you marginalized over the hidden variables, you have this interesting form.",
            "You have bunch of product right?",
            "So here you seeing this product of bunch of functions.",
            "So what does that mean?",
            "What's the intuition here?",
            "The intuition here is following.",
            "Let's say finding these topics in my data right and then if I tell you that topics like government, corruption and oil.",
            "Occur in a document.",
            "These are the topics in the document.",
            "What Putin will have very high probability.",
            "Right, and the reason why is because you know what happens here is that you take the distributions, you multiply them together and you renormalize.",
            "So that's why the product comes in, right?",
            "It's very different from the mixture based models models like mixture of Gaussians or even that mixture.",
            "Models like late dislocation models where you first speak the topic and then you generate the word here.",
            "You take the product of distributions and then you normalize.",
            "You can get very spiky distribution so it can be very precise in terms of what you're modeling."
        ],
        [
            "In these models they do well.",
            "They do much better in terms of information that you will finding similar documents and such."
        ],
        [
            "Now if I step back a little bit and say OK, what's next?",
            "And these models, as I mentioned, ridiculous machines been around for about 20 years.",
            "Well, one thing that"
        ],
        [
            "We can do in the spirit of building hierarchical models, building deep learning models that we can start adding multiple levels of representation, right?",
            "So we can add an additional layer of latent variables.",
            "So what happens here is that you know you, hoping that at the high level you start capturing high level features, right?",
            "So in this case, the first layer you can get your low level correlations in data such as edges.",
            "Or maybe you can capture some low level correlations.",
            "Some correlations within text data like correlations between the words, but as you start adding layers you can start getting.",
            "High level features so you only simple representations, and then you compose them into more complex ones.",
            "You can spell."
        ],
        [
            "Sify again as before, you can specify the distribution over your stochastic variables.",
            "Hidden variables as well as the visible variables.",
            "So now introducing dependencies between hidden variables and all connections here are undirected, so this is an instance of undirected Markov random field where you just have multiple layers of hidden variables.",
            "So what's different from before is that here you have.",
            "The first term here is the same as the ARB term, right?",
            "And you have these two additional terms and these two additional terms are modeling dependencies between H1 and H2, and H2 and H3, right?",
            "So you're modeling dependencies between hidden variables.",
            "It is also very natural notion of bottom up and top down, just by definition of graphical model of this particular graphical model, where conditional probability of a particular variable hidden unit here taking value, one is going to be given by logistic function, but it's going to be given by what's coming from above from what's coming from below.",
            "Right, so that sort of has a very natural notion of bottom up and top down.",
            "An unfortunately in these kinds of models, as we making them more expressive, your hidden variables become dependent even when you conditional input.",
            "So we have to do a little more work to do inference in these models."
        ],
        [
            "So how do we do maximum likelihood learning?",
            "Surprisingly, if you look at the maximum likelihood learning rule, if you look at the log of the probability deliverability with respect to parameters, surprises just the same learning rule.",
            "Write the same learning rules before you just looking at the expected sufficient statistics driven by the data expected.",
            "Sufficient statistics given by the model and trying to match match the two.",
            "The problem with these models is that both expectations are intractable, so we have to do something here, right?",
            "So it's one of those things that you often see that you're trying to make your model more expressive.",
            "Writing your models are more expressive, but there is no free lunch.",
            "You have to pay costs generally right?",
            "So we have to come up with some, maybe more or some way of doing approximate inference, and pretty much most of the."
        ],
        [
            "Generative models so both expectations are intractable, but let me give you an intuition as to what these two expectations are doing.",
            "What these two different terms in learning rule out doing, right?",
            "So the first term, actually, you know the conditional probability here, is no longer factorial, so it's very hard for us."
        ],
        [
            "To compute it, but here's what's happening.",
            "Let's say you have a data manifold, and let's say you observe these data points.",
            "This is the truth.",
            "This is your training data.",
            "What you'd like to do is, you'd like to say, well, I've seen the data, so I'd like to make it more probable, because this is a real handwritten character, right?",
            "And that's effectively what the first term is doing.",
            "The first expectation is doing now.",
            "If you look at some image like this.",
            "Right again, these are 28 by 28 images.",
            "If you look at the number of possible configurations then there are two to 784 possible configurations, right?",
            "And what you effectively want to make sure is that this particular image should have small probability with very exponentially small probability.",
            "So you have this exponential space and you carving out these probability bumps on the real data and you trying to push down on this entire exponential space, right?",
            "And this is effectively this term comes from the definition of so-called partition function.",
            "Which is a very fundamental quantity in a lot of different areas.",
            "Particularly if you look at the whole research agenda taking place as to how do you evaluate, how do you estimate partition function log of the partition function?",
            "Um?",
            "So what can we do?"
        ],
        [
            "Here for these kinds of models we can use variational inference, and we can also use Markov chain Monte Carlo based inference.",
            "Something is called stochastic approximation.",
            "So what I'd like to do is I'd like to go over both of these procedures just because we go through the rest of the tutorial will see variational inference more and more, and I'm sure that when she is going to be giving up his tutorial second half the tutorial, I'm sure he's going to be covering variational inference is, well, how many of you are familiar with variational inference?",
            "Oh, fantastic, that's that's more than I thought.",
            "This is great.",
            "How many of you familiar with Markov chain Monte Carlo based methods?",
            "Ah, wow, that's that's a very impressive, very impressive audience."
        ],
        [
            "So I can go very very quickly, so let's look at the Merc based approximation.",
            "Well, sometimes caustic approximation.",
            "It's actually very simple.",
            "What you're doing is you're going to be updating your parameters of the model and the state sequentially.",
            "OK, so.",
            "You're going to generate a sample.",
            "According to some transition operator, by effectively simulating a Markov chain that leaves your distribution, your P, Theta septin variant.",
            "So for example, that could be Gibbs sampling operator, Metropolis Hastings operator, and deep sampling operator is something that you can do with Indy.",
            "Both machines it's very easy to do, it's just you know sampling states of hidden variables given the other states of hidden variables.",
            "So very simple procedure to implement and then you're going to be updating your model parameters by replacing the intractable expectation with a point estimate.",
            "So you basically.",
            "Getting a sample out of Markov chain?",
            "You say that's your point.",
            "Estimate of expectation, and in practice you simulating mark multiple Markov chains in parallel.",
            "So just take the average and say that's your approximation.",
            "Right so."
        ],
        [
            "What's what's the learning algorithm?",
            "What's the learning looks like.",
            "Well, here's an update.",
            "You take your new parameters are going to parameters.",
            "Plus you know the difference between what the gradient is right?",
            "It's expectation respect to the data minus what you get out of your Markov chain.",
            "Samples that you get out of Markov chain.",
            "Now here's the trick of to analyze convergence of these algorithms is well.",
            "If you add and subtract the true expectation here, this would be the true gradient.",
            "Right, if we could compute the true gradient, they will be very easy for us to do right now, but unfortunately we can't.",
            "So this is the true gradient, and this is something that you would call the perturbation term.",
            "This is the difference between the true expectation versus what you get out of your Markov chain.",
            "Right?",
            "Now it's a little bit tricky to see what's going on here, because your Markov chain is not really like this this term.",
            "He is not really an unbiased estimate of this expectation, right, because it depends on the properties and convergence property.",
            "If your MCMC algorithm so the analysis becomes a little bit tricky, but you can at least show that asymptotically you know you can get to the syntactically stable point, so at least you can guarantee that asymptotically you will reach some local.",
            "Local optimum by following this this algorithm, but of course it relies on sort of a little bit more theoretical justification, which basically says the following.",
            "It basically says well as your learning rate becomes sufficiently small compared to the mixing rate of the Markov chain.",
            "Markov chain is going to stay close to the station distribution, in which case this will be this term.",
            "Here is going to be the correct estimation of your expectation right?",
            "Of course, in practice that really doesn't happen and high dimensional probability.",
            "Escape is very very multi model, so you have to navigate around that space.",
            "There's been a lot of work where you can say, well, you can sort of play with the transition operators.",
            "You can use tempera transitions.",
            "You can use parallels.",
            "Simulated temper is a whole bunch of different different ways of trying to improve it.",
            "But that's sort of the best way for approximating.",
            "Expectation respect."
        ],
        [
            "Small distribution now what about expecting approximating the data driven expectation?",
            "And this is way variational inference come comes in place.",
            "You can use MCMC as well, but it turns out that variational inference is much more stable.",
            "For making approximations here.",
            "So what's the intuition behind variational inference?",
            "So let me just say from the high level perspective, because we're going to see more and more of it through the rest of the tutorial.",
            "What's the intuition here?",
            "Let's say you have an intractable distribution.",
            "The probability of H given V and what you want to do is going to approximate it with a tractable distribution.",
            "Q OK, think of Q, maybe is a fully factorized distribution or its one big gigantic Gaussian distribution.",
            "Something that's easy, easy to compute.",
            "OK, so here's the trick behind variational inference.",
            "It's a beautiful trick, and it works surprisingly well.",
            "If you look at the low probability, you can say, well, it's the log of the joint, right?",
            "Now what I'm going to do is I'm going to multiply and divide by my Q distribution.",
            "There's some conditions you know where the coverage of Q has to be at least as it has to be covering all the states that piece covering so that you don't end up dividing by zero or something like that.",
            "So if you conditions here, but let's say you multiply and divide by Q, right?",
            "So what is going on here?",
            "And then you can say, well, you can take the Q and you can push it.",
            "Or you can take the log and push it inside here, which is effectively something that's called Jensen's inequality.",
            "Because here you saying well.",
            "It's the log of the expectation is greater than expectation of the log, right?",
            "And that's the whole trick.",
            "Now why is this trick useful to us?",
            "So if I push the log inside, why doesn't help me with Boltzmann machines, right?",
            "If you do a little bit more work here, you end up with this expression here.",
            "And this is the expression where the first term is something that's called expected complete data log likelihood.",
            "The second term is the log of the partition function and remember log of the partition function is something that gives us a lot of troubles.",
            "'cause to compute it requires exponential sum or exponential integrals is something that we cannot do.",
            "We cannot compute and this term is known as the entropy of the Q distribution, right?",
            "Now, surprisingly, if I look at this term, here is just linear in the parameters right?",
            "And this is known as a variational bound.",
            "So what does this say?",
            "Well, this is basically saying if I look at the variational bound and I'm trying to optimize with respect to Q distribution.",
            "Notice that it doesn't really matter.",
            "This log of the partition function doesn't really affect me, so it's kind of interesting I have this conditional probability.",
            "I have this probability that has this nasty partition function and I'm converting into the problem where I avoid.",
            "I don't have to even.",
            "Look at my partition function when I'm optimizing with respect to Q, right?",
            "So, so now I can also write it in terms of log of the marginal minus the KL between the Q&P.",
            "So this excuse my approximation, P is my truth right?",
            "So the other way to view variational inference is you.",
            "Basically I'm going to say that.",
            "Well, how can I find my Q distribution such that it's as close as possible to my truth?",
            "In terms of minimizing the KL divergent, yes.",
            "Their purchasing function, as in show up anywhere.",
            "The partition function doesn't show up anywhere, because if I'm optimizing this variational bound with respect to Q.",
            "Only this term depends on Q and this term depends on Q.",
            "With respect to Theta.",
            "No, in this case.",
            "In this case, when I'm optimizing with respect to Theta, I'm using MCMC, so that's a different piece.",
            "So here all I'm trying to do is I'm trying to optimize with respect to Q. I'm trying to basically say find me approximation to my posterior and find the parameters of the Q distribution such that my approximation to the ground truth is as good as possible, right?",
            "In which case it avoids computing the partition function, which is a very neat trick for the variational inference, right?",
            "And again, the other way to think about it is that you basically trying to say find me the Q distribution such a way that it 6 closest possible to my true distribution P. But the problem with my new distribution piece that it has this nasty partition function that I cannot compute and magically with variational inference you can actually optimize this scale right?",
            "Which is which is a surprising fact, so we'll see a lot of that through the rest of the tutorial, right?",
            "So you try to minimize KL between approximating and the true distribution with respective variational parameters, mu.",
            "With respect to the parameters of the Q distribution."
        ],
        [
            "Right, so this is variational bound.",
            "So for Boltzmann machines for Diebold machines you can choose a fully factorized distribution.",
            "That's one way of doing it.",
            "You can do something a little bit more complex if you wish to, but if you doing a fully factorized approximation, then you effectively maximizing this bound with respect to the variational parameters.",
            "And there's just turns out if you workout alot of math is just a sequence of nonlinear equations, so you cycle through these nonlinear equations and you get the best best approximation.",
            "There are ways of speeding things up where you can build a recognition model that tries to approximate.",
            "The parameters of the mean field approximation.",
            "The other thing about mean field approximation, which is true, and for most of the variational inference is that typically if it's a fully factorized distributions you will only capture one mode in the posterior, right?",
            "So most of the time most of the complaints that you would get from using variational approximation is that it can only capture single mode in the posterior distribution.",
            "So if you true posterior distributions multi model, the algorithm will make sure that you only focusing on one month and we'll see some of that.",
            "Later on.",
            "Right so."
        ],
        [
            "Now you are effectively sort of for training these models.",
            "It's almost like doing something similar to EM algorithm expectation maximization algorithm, which is sort of in a pseudo eastep using variational inference is to find approximation to the posterior in the M step, you're running your Markov chain Monte Carlo Stochastic approximation.",
            "You get statistics from your model and then you matching the two expectations.",
            "Right, yeah?",
            "Yeah, it's a good point.",
            "It turns out if you doing variational inference.",
            "Unfortunately, variational inference is sort of the bound goes the wrong way.",
            "So you can only use variational inference to do data dependent expectations.",
            "If you're trying to do the model, expect so if you're trying to approximate using mean field approximation for the log of the partition function, then the equations were the wrong way, so it's like optimizing.",
            "It's like maximizing the upper bound right, which, which is not quite right.",
            "There's been a lot of work on trying to find upper bounds on the log of the partition function, like she waited, belief propagation, for example.",
            "These models are very not very precise.",
            "So specifically for deep Boltzmann machines it's impossible to use variational approximation like mean field approximation to approximate the logger partition function.",
            "Yet get the learning algorithm, get the right learning algorithm.",
            "It's a bit of a subtle point.",
            "If you haven't seen these algorithms before, it's kind of hard to see why, but effectively it's impossible to do."
        ],
        [
            "So."
        ],
        [
            "Question is OK, there is a theory.",
            "We can prove things about it if you're interested in MCMC, you can start playing with fancy transition operators and such, but the end of the day these models actually work.",
            "How good are they?",
            "And I'm going to let you judge that you've probably seen this before because I've done it multiple times, but I'm going to show you I'm going to show you two panels or one panel.",
            "You'll see the real data and we simulated data.",
            "So you can think of that as fake data simulated from."
        ],
        [
            "Model and you have to tell me which one is which.",
            "Alright, so these are different handwritten characters coming from DIFF."
        ],
        [
            "Alphabets around the world.",
            "How many of you third that?",
            "This was simulated in this is real.",
            "Raise your hands.",
            "OK."
        ],
        [
            "What about the other way around?",
            "Still, that's that trick still works.",
            "So if you."
        ],
        [
            "Get the images.",
            "Actually you will see that this is simulated and this is real.",
            "Right, because you know, if you look at an image like this, it's not quite the pixels are not quite as crisp, and if you look at the diversity of possible images real there is a lot more diversity has to do with the fact that Markov chains inability to explore a very highly multimodal space.",
            "Right and the other trick that I'm using is that I'm showing it to you very quickly so you know it's hard for you to see the difference right now.",
            "If you stare it for some time, you will see the difference.",
            "Um?"
        ],
        [
            "Same thing with the handwritten digits.",
            "How many of you think this is?",
            "This is simulated and this is real.",
            "What about the other way around?",
            "This is real and this is simulated.",
            "A few are so few people worked on that list and I can promise you this is this is real and this is simulated and I can promise you I did not pick this particular 0 because a lot of people told me that that particular 0 is what messes the people up.",
            "But that's real.",
            "That's a real data."
        ],
        [
            "You know you can do object.",
            "You can do recognition with these models and compare it to other models and permutation variant version of these models do quite well."
        ],
        [
            "Can also sort of build something a little bit more.",
            "This is an older result where you can build.",
            "Generative models of 3D objects.",
            "So in this case obviously this is generated.",
            "This is real.",
            "One thing that I want to point out is that sometimes we generate these funny things.",
            "Kind of looks like an airplane with no kind of looks like a car with wings, right?",
            "Like if you look at that particular piece right?",
            "This is when the model transitions from one class to another class, so it's able to simulate something something unusual."
        ],
        [
            "And you can also do pattern completion.",
            "It's another application areas where you know what you're seeing here is that if I show you half of the soldier, it sort of fills up the remaining half right.",
            "Same thing with airplanes, and these are new objects at the desktop.",
            "This is something that you don't see at the training time and this is what the real real data looks like.",
            "OK, so."
        ],
        [
            "These kinds of models are useful for modeling hierarchies, right?",
            "You can think about this.",
            "Learning edges, combination of edges, and so forth, and a lot of different deep learning.",
            "Deep generative models sort of follow that particular hierarchy."
        ],
        [
            "So now let me jump."
        ],
        [
            "2.",
            "Directed graphical models.",
            "The counterparts to these models.",
            "One of the early models in that space was Helmholtz machine model back in 95 Hinton.",
            "Diane, Brendan Frey and Radford Neal had a paper in science where they introduced this model.",
            "If you look at the structure of this model, it's a directed graphical model, right?",
            "You have probably.",
            "City over the latent variables and then you generate through multiple layers of nonlinearities.",
            "You generate the data.",
            "There is also an approximate inference network.",
            "There is a Q distribution just like as we've seen with both machines, but in this case Q distribution is just does it in the feed forward way.",
            "Given the data you try to approximate the states of the latent variables right?",
            "So we have this generative model recognition model.",
            "Suggest this before you have a P distribution.",
            "You have a Q distribution, right?",
            "You approximate distribution.",
            "And in the last few years, there's been a lot of work looking at these models.",
            "These models been around for almost 20 years, right?",
            "But it was always very hard to train these models.",
            "Part of it was because of computers were not there yet.",
            "Was because the algorithms were not quite there as well.",
            "In the last few years, there's been a lot of work in that space, and particularly the work of Kingman, valuing welling on the rational team code is the same, but the same time Rezende Shakira and done.",
            "People are deep mind, basically had similar ideas at about the same time the work at Montreal.",
            "They also had a related related wake sleep model that actually show that these models can be trained and also some of the work at Toronto.",
            "Where we've looked at also train stochastic, feedforward feedforward networks.",
            "So in the last sort of three or four years these models are picking up.",
            "Why they're picking up?",
            "Well, one big advantage of directed models is that you can simulate you can.",
            "You can sample the data right in both machines.",
            "You have to run Markov chain Monte Carlo to actually get samples.",
            "In these models you can actually sample the data so it's a.",
            "It's a very simple process.",
            "The problem is that you know how do you optimize Q distribution?",
            "How do you find the right recognition distribution?",
            "So let me dive into that."
        ],
        [
            "Now in the space of of deep models, there's been sort of a whole bunch of family of models.",
            "There is a Helmholtz machine was back in 95, then there was a deep belief network over here, and it's kind of has an interesting architecture, has an interesting structures in DBM, followed by sigmoid belief network, so it's a hybrid model and has undirected and directed graphical model sitting with it.",
            "In it, and then there's a D bolsa machine where everything is undirected, right?",
            "So you have these different classes of models and it's kind of useful to know about all of them an.",
            "Advantages and disadvantages of these different models.",
            "All of them have advantages and disadvantages.",
            "OK, so now I'm going to be focusing on the first one, the Helmholtz machine.",
            "So let me show."
        ],
        [
            "Are you a motivating example?",
            "Motivating example is that can we generate images from natural language descriptions whynott?",
            "Can we build a model that can generate generate images?",
            "So for example, if I give you a set, the stop sign is flying in blue skies.",
            "That's what the model can generate.",
            "Or, you know, a yellow school buses flying in blue skies, so you get a yellow school bus.",
            "Or this is very hard to generate precise school bus, although there are approaches.",
            "It's been basically developed in the last half a year where people are getting better and better generative models.",
            "It's a race we can generate.",
            "Better models or hear elephants are flying in blue skies.",
            "Commercial plane flying in disguise, right?",
            "So you can basically and these kinds of models a lot of them are based on these directed models, so generative."
        ],
        [
            "Active model, so for example, here's one particular model where you have an input as a sentence.",
            "There's something that's called bidirectional STM that you've had a tutorial on recurrent neural networks, and the thing that I want to point out here is that there is a generative model P that generates the data.",
            "It's based on the work of DeepMind, Drone network of Gregor at all, and there is also recognition Model Q right that given the data tries to infer the distribution of a latent variables.",
            "So at the high at the Birds Eye view, at the high level view you have this generative model.",
            "We have this recognition model.",
            "It's just the way you structure.",
            "The model is a little bit different."
        ],
        [
            "You can, you know you can flip the colors.",
            "You can say a school bus is parked in the parking lot versus a red school bus and look at those red school buses right?",
            "At least it's trying to.",
            "Green school bus.",
            "It's trying to make buses be green and blue school bus, even though there's nothing like there's nothing like a red school bus or bull school bus right?",
            "We actually try to check.",
            "I don't think there are a lot of green buses or blue buses in the training set, but there are a lot of blue cars and green cars and stuff like that, right?",
            "So it's sort of."
        ],
        [
            "Times to capture it.",
            "You can also sort of do interesting things like you know a plane is flying in clear skies versus rainy Sky, so it has some notion of what the meaning of clear versus rainy.",
            "So the only difference between these two captions is clear versus Rainier, right?",
            "And it changes the background or elephants of working in dry grass field versus green grass field, right?",
            "Obviously it's hard to see elephants, but at least it's trying to understand dry versus green."
        ],
        [
            "You can put the objects chocolate desert versus bowl of bananas.",
            "I don't know if you see both bananas, but chocolate dessert.",
            "Maybe you see something.",
            "There is an example where it fails, you know, vintage photo of a cat or vintage photo of a dog.",
            "I don't think you can tell the difference between the two.",
            "This is where adversarial networks that hopefully will be covering as well.",
            "Might help us 'cause you can't really see the difference between the two."
        ],
        [
            "You know you can also compare it to other models.",
            "There's a little bit of an older comparison.",
            "You can compare it to lapghan.",
            "You can compare to convolutional decommission variation on the encoders fully connected in quarters and so forth.",
            "You know a lot of different models have their advantages and disadvantages."
        ],
        [
            "Thing about.",
            "Helmholtz machines in variational encoder.",
            "Then I'm going to talk about now is that you can actually evaluate them.",
            "At least you can estimate what is the lower bound on the average test log probabilities, right?",
            "The numbers themselves are probably very hard to make the meanings of, but at least you can see that you're not overfitting right.",
            "At least you can see that what you have on the training set is similar.",
            "What you have on the test set, so you know that these models are not heavily overfitted to the training."
        ],
        [
            "Data.",
            "But you can do fun things like novel scene compositions, and this is the funniest piece about this work is that you can generate a toilet seat, sofas in the bathroom, or you can say toilet seat is open in the grass field.",
            "Right, so you can see, maybe you can, maybe you can't, but sort of looks reasonable.",
            "And when we published this paper, I like to make this joke is that my student Elman Muncie move who was working on this particular project?",
            "Googled it so you can just you can just Google it or see softening the grass field and in a Google image search and this is what it comes up with.",
            "Right so then you basically saying?",
            "Well, Google already solved the problem.",
            "So why do we need it?",
            "But then, remarkably, after a few weeks, if you actually Google this particular caption, This image comes before this image, so we've convinced Google that is a much better representation than this because a lot of people are clicking on that image right?",
            "And based on the click rate that reranked the images appropriately.",
            "So which is which is great.",
            "And then let me just."
        ],
        [
            "Skip skip on that, so the overall model.",
            "You know you can think of it as a variational encoders or stitched in time, but I'm going to be focusing now is I wouldn't.",
            "I wanted to give you precise details of what these models are.",
            "OK, so that we have a little bit better understanding of of this class of models.",
            "And then again you can apply them a lot of different different application air."
        ],
        [
            "So, OK, so variational open order, yes?"
        ],
        [
            "Generated images other than.",
            "Yeah, you can ask humans to evaluate the images.",
            "I mean image evaluation.",
            "That's a very difficult.",
            "Task in general because in Canada log probabilities, but if your distribution over the observed data which is in our case is a Galaxy is not appropriate, is not quite what people believe.",
            "That is, the right perception is then it might not be the right metric, right?",
            "So yes, you can ask humans to humans to evaluate, but there's no, there's no really good solution because it becomes very expensive to do it so.",
            "There's a lot of work basically in the space of trying to figure out how do we evaluate the quality.",
            "The generative models of images, right?",
            "The fact that you can generate pretty looking images doesn't really mean much, because I can have a really dumb generative model that picks a training example and shows it to you and you will not see the difference.",
            "So this is amazing generative model, but it's useless.",
            "So that's still in very active area of research."
        ],
        [
            "OK, so let me get into the variational thinkers.",
            "The variational encoders they define a generative process by stitching together conditional probabilities, right?",
            "So here if I say, what's the probability of the input, the probability of the input is given by defining this joint probability, which is the prior.",
            "Times bunch of conditionals and then you sum out the states of all the hidden variables.",
            "So you can think of this as as just directed graphical model, right?",
            "So you have a generative process, you write down the conditionals, you can sum out all the latent variables, hidden variables from the model.",
            "So Theta here is going to be noting the parameters of the model.",
            "L is going to be the number of stochastic layers.",
            "And key assumptions that we're going to be making is that the sampling from these conditional distributions can be done easily, and these conditional probabilities can be evaluated so each conditional probability should be tractable, and we should be able to evaluate it, right?",
            "All of these conditionals.",
            "Now each term here can have a complicated nonlinear relationship.",
            "OK, so it's not that these conditions are just bunch of Gaussians, right?",
            "They can have nonlinear structure.",
            "So here's here's an example."
        ],
        [
            "Here is 1 example where you have a stochastic layer.",
            "You have a deterministic layer, you have another stochastically.",
            "Have the data.",
            "So this term here the conditional probability of H1 given H2.",
            "Right, this conditional probability is effectively one layer neural network OK?",
            "So it can have nonlinear structure sitting in it.",
            "It's not just bunch of Gaussian layers stitched together, in which case one model is 1 big gigantic Gaussian.",
            "Right, you have nonlinearities sitting in your model, but they just deterministic nonlinearities."
        ],
        [
            "Now, in addition to the generative network, you're also going to have a recognition network.",
            "Right, so there's going to be a bottom up recognition network, so in this case the joint distribution over all your hidden variables is just going to be again factorized.",
            "This way, just bunch of conditionals stitch together in this way.",
            "And again, each term here can denote a complicated nonlinear relationship, right?",
            "So your recognition model can have stochastic layers and has deterministic layers, so you can you know your recognition model can be quite powerful.",
            "This is the most general way of defining it.",
            "Now we're going to be making assumption that the prior distribution.",
            "Over your latent variable is going to be Gaussian.",
            "OK. And the conditionals are also going to be Gaussians with diagonal covariance is you can make it a bit more expressive, but for the sake of this presentation is going to be Gaussian with Daniel Covariances.",
            "OK, now again, I want to emphasize that doesn't mean that this entire model is just one big gigantic Gaussian right?",
            "Because these conditionals can have non linearity in them, right?",
            "So you mean of your distribution can have a nonlinear relationship to the states of the layer above right?",
            "And get you have a nonlinear relationship for the mean as well As for the variances.",
            "Right, so you can pretty much model any kinds of distributions that."
        ],
        [
            "That you want.",
            "Now the variational autoencoder is again by the name.",
            "They are trained to maximize the variational bound, and as we've seen before, what is the variational bound?",
            "It's the log of the expectation is greater than expectation of the log, right?",
            "So we're going to be looking at this bound here, where Q is our approximation.",
            "It will have its own parameters.",
            "We have to find what those parameters are.",
            "The generative model has its own parameters.",
            "We're going to find out what the parameters of the generative models are.",
            "So how do we do that?",
            "And again, to emphasize as before, the actual marginal log probabilities given by the lower bound is given by the.",
            "Actual probability minus this KL divergences, right?",
            "So you effectively when you optimizing these algorithms using variational arguments is you effectively trading off the data log likelihood and the KL divergent from the true prior right?",
            "So you sort of have these kind of two competing objectives.",
            "You trying to make the likelihood be better at the same time you're trying to make sure that whatever your approximation is, it's actually good approximation.",
            "Right 'cause you're trying to basically push this term up.",
            "Right, you cannot have.",
            "You can't put like.",
            "You can't push the log likelihood if it means that your approximation to the true posterior is really bad, because the sum of the two is going to be.",
            "Small, so you sort of have this interesting tradeoff between between the two objectives.",
            "OK, now one of the things about optimizing deviation bars is that it's very hard to optimize variation bound with respect to the recognition network.",
            "Intuitively, if you had the recognition network, you could just sample from Q and optimize with respect to your generative network P, right?",
            "But the problem is that how do you optimize for Q?",
            "That's a tough question, and the key idea of Kingman Welling was to use repatriation trick.",
            "And as I mentioned before, there were a few other papers about the same time.",
            "There's a work coming from deep mind from Shakira.",
            "Mohammed's group was going to be talking next on the same relationship, so it came up and about about the same time.",
            "So let me explain.",
            "What the trick does?"
        ],
        [
            "It's a very simple trick and let me explain it for the case of the Gaussians.",
            "So imagine that you have a recognition distribution which is Gaussian.",
            "So let's look at this recognition.",
            "Distribution is a Gaussian distribution.",
            "The mean depends on the states of the hidden variables in a non linear fashion, right?",
            "So it depends on HL minus one.",
            "There is some nonlinear relationship and also the variance depends on HL minus one.",
            "Right, so we have this form.",
            "Right now, Alternatively, you can express this term here in terms of auxiliary variables.",
            "So what you can do is you can say well sample epsilon at layer L from normal 01 right?",
            "And just write this H term is a function of this sample epsilon, the depending on HL minus one the layer above.",
            "In this form.",
            "Right?",
            "The distribution of HL if you marginalized epsilons exactly going to this right.",
            "So notice that if I give you."
        ],
        [
            "Epsilon this H here becomes deterministic function, right?",
            "There's no stick assist here, everything is deterministic.",
            "Right, So what it does is basically says, well we sample these epsilons with just acoustic and then everything else becomes deterministic, right?",
            "Once I give you epsilon everything is deterministic.",
            "Right, so now what does that mean?",
            "That means that the recognition distribution can be expressed in terms of deterministic mapping.",
            "Right, if I give you epsilons at every single stochastic layer, or if it's a one layer, just one epsilon.",
            "If it's multiple stochastic, lays multiple epsilons per layer for each layer, then this H is going to be deterministic.",
            "Why is this useful for us?",
            "So you basically have a deterministic encoder.",
            "Why is it interesting?",
            "Important for us is that notice that this epsilon the distribution of epsilon does not depend on parameters of the model, and that's the key.",
            "Right, absolutely just normal zero identity doesn't really depend on the parameters of recognition on generative model.",
            "So how can we?"
        ],
        [
            "Make use of this fact.",
            "Let's look at the gradients with respect to both recognition and generative model, right?",
            "So this is the gradient with respect to.",
            "With respect to objective, the variational objective right now what I can do is I can say, well it's the gradient, but I can do the trick.",
            "I can say sample these epsilons from normal zero identity.",
            "Nothing has changed and now I'm basically saying my H Now depends on the epsilon right?",
            "So this is the same as this, nothing has changed, I'm just doing simple repolarization.",
            "But why is it important for us?",
            "Well it's important for us because this expectation does not really depend on Theta.",
            "So what I can do is I can push this derivative.",
            "Insight.",
            "Of expectation, right?",
            "So this has an interesting interpretation.",
            "This basically saying look change my parameters by taking expectation.",
            "Take the average and then move the average.",
            "This has slightly different expect different interpretations.",
            "Basically says take the gradient and then take an expectation it's like like expectation of the gradients versus gradient of expectation right, which turns out to be very useful property because it reduces the variance of your estimates quite substantially.",
            "The other thing to note is that this mapping here is deterministic, so this entire whole thing here is just one deterministic autoencoder.",
            "Right, which given epsilon's the whole thing becomes deterministic, so we can use backpropagation algorithm.",
            "We can actually compute the gradients precisely, so it's almost like back propagating through the states of the latent variables.",
            "So ingredients can be computed by back problems."
        ],
        [
            "Right?",
            "So now if you computing the gradients you what you basically doing is drawing case samples from epsilon.",
            "You take the average.",
            "Right way W he is defined by the joint divided by Q.",
            "This is something that sometimes it's called importance weight.",
            "Right, because it's basically sort of looks at how important your generative versus the recognition model is.",
            "But this is just a Monte Carlo approximation.",
            "Right, and that's the whole trick in practice.",
            "For the sake of computation, most people just use one sample.",
            "So you sample from epsilon you sample from epsilon, which is normal 01, and then you just back propagate through the auto encoder, right?",
            "And so the variance of the variational auto encoders is quite low because it uses the log likelihood gradients with respect to the latent variables, right?",
            "Which is important.",
            "So that's that's the key trick.",
            "Anne, it's being applied a lot in a lot of different.",
            "Like for example, the images that I've shown you generating images actually using variational autoencoders, and you'll see more and more.",
            "It's actually surprising 'cause one of the beauty of this algorithm is that it's very easy to get to work.",
            "No, it's it's you see a lot of people basically using it and it's it works surprisingly well."
        ],
        [
            "OK, so now let's look at the variation assumptions right.",
            "Deviation assumptions must be approximately satisfied.",
            "So what is that?",
            "What these assumptions?",
            "Well, the posterior distribution must be approximately factorial, which is a common practice right?",
            "Whenever you queue distribution, it's typically factorized distribution, and it has to be predictable from a feedforward neural network, right?",
            "And what we can do is I'm going to show you one other trick where we can relax these assumptions by using a title lower bound on the marginal log likelihood.",
            "OK, so it's going to get a little bit.",
            "This is 1 trick which basically allows you to get much tighter variational lower bounds.",
            "So what is?"
        ],
        [
            "Trick, the trick is very simple.",
            "Let's say you're going to be using the following what we call case sample importance.",
            "Weighting off the log likelihood.",
            "So let's look at this expression here.",
            "What is this expression here?",
            "Instead of sampling 1 hidden state, you're going to be sampling tastet, so you're going to be running your feet forward recognition model K times.",
            "Right, so you can even join K samples and then you're going to be taking the average.",
            "Over those K samples, that's all in variational encoders is typically 1 sample.",
            "Write in variational inference in general is just a single feedforward path in importance weighted encoders.",
            "It's multiple passes, and then you average right?",
            "So what is this by you?",
            "Or we can also say it's the log of the average of the importance weights, right?",
            "So it's just taking the average obviously small compute 'cause you have to draw K samples instead of 1 sample.",
            "Ann, you have to take the average in these H1 up to each case.",
            "Sample from the recognition network so it's more computationally intensive.",
            "And these are sometimes called a normalized importance weights and I still should point out there's going to be a great work coming from deep mind and basically looking at extension of these case sample bounds.",
            "444 for the recognition network, particularly when you're looking at discrete variables, you're dealing with models with discrete variables.",
            "Everything that I'm talking with variational to encourage the using recommendation trick, and typically most of them are assuming that you have Gaussian weighting variables.",
            "If you're working with discrete latent variables, then it's the realization trick doesn't really work there, so there's some ways of improving these systems, and that was done by Jimmy and his editor at Deep Mind if you're interested."
        ],
        [
            "OK, so let's look at this bound.",
            "It's the sample average.",
            "Also, it's the average what you can do is you can actually show that this is a lower bound on the marginal probabilities.",
            "We can basically say it is a lower bound, which is good because we maximizing their lower bound, right?",
            "You have your likelihood log likelihood you have a lower bound you pushing on the lower bound right?",
            "Instead of justice makes use of a very simple variational, very simple against inequality and special case when K is one, we just join 1 sample, then it becomes a variational objective fashion.",
            "Objective 1 interesting thing about this bound is that using more samples you can improve the tightness of the bound.",
            "Right so."
        ],
        [
            "You can show is that.",
            "You know, as you get more samples, your bound becomes tighter and if you get infinite number of samples you actually recover the marginal probability.",
            "Alright, so maybe just to highlight a little bit.",
            "Instead of in machine learning community, the statistics community that people who like doing variational inference and people who like doing much Markov chain Monte Carlo based inference right?",
            "Because whenever you doing variational inference you always making assumptions on your Q distribution.",
            "Your recognition model, right?",
            "And it will never get.",
            "You'll never get it quite right.",
            "Right is also going to be an approximation, but it's faster to optimize right?",
            "And then Markov chain Monte Carlo and you can show that asymptotically you'll get the right thing.",
            "That's why a lot of statisticians like MCMC, because they close.",
            "Can proof that I simplistically I will get the truth.",
            "I will get the true posterior probability if I run it for long enough.",
            "Of course, in practice nobody running for long enough, so still approximation anyways, right?",
            "So this is kind of an argument that bridge is between the two.",
            "Areas which basically say look if it's computationally intensive, you only use one sample.",
            "It's a lower bound if you can compute if you can spend more time computing then you can effectively get a much tighter lower bound right?",
            "So at least you can get closer to the ground truth.",
            "Right, so how do you compute the?"
        ],
        [
            "Audience you can use exactly the same trick as a representation trick.",
            "You taking these expectations, you know reproduction.",
            "I'm in terms of epsilons, right?",
            "You pushing the gradients inside, and if you do a little bit more work here, it just turns out to be.",
            "You know this is the gradient where these W's adjust importance weights.",
            "And these are normalized importance weights.",
            "Which are quite easy to compute, right?",
            "So now instead of going."
        ],
        [
            "Through the equations you can actually get the Monte Carlo estimates right, just as in variational encoder you get the Monte Carlo estimates.",
            "What's the difference between the two?",
            "Right here the two equations for importance weight on highways that we call them highways versus vasc.",
            "And if you look at this expression, they're exactly the same.",
            "So in both cases, when you computing the gradients that exactly the same, it's just that in one case we using importance weights in another case is just the average.",
            "Right, why is important weights important here?",
            "Well, what they effectively doing is they basically saying well if you drawing 10 samples.",
            "Effectively intuition here.",
            "If you join 10 samples from recognition model for every one of those samples, you can look at what's the probability that each sample could generate the data and if one of them hit the appropriate mode is more probable than its weight is going to be higher.",
            "So it's almost like you selecting better samples.",
            "It's a way of selecting better samples.",
            "You join multiple things.",
            "And then you selecting better ones as opposed to just averaging so that sort of intensive computations.",
            "They're pretty much the same."
        ],
        [
            "Right now, what's the intuition behind variational encoders and importance weighted autoencoders?",
            "Here's here's the intuition.",
            "Gradient you have the deterministic encoder just like in other encoders and you have a deterministic decoder, right?",
            "So it just becomes an alter.",
            "The first piece here of the gradient that you're computing is just an auto encoder, right?",
            "Given the data you encode and you decode.",
            "The decoder is basically trying to encourage the generative model to assign high probability to every single conditional.",
            "Right, that's the job of the decoder, and they encode is basically trying to make the parameters of the encoding network to just the latent spit states, so that the generative model would give high probabilities to those states is exactly what the auto encoder is doing.",
            "Just encode and decode and you're trying to match match match the two."
        ],
        [
            "And then there is a second term which is an important term.",
            "It's kind of interesting term.",
            "The second term is basically saying make sure that the recognition network has a spread out distribution of the predictions.",
            "Never make you incredibly precise.",
            "Right, so these are two terms, 'cause if it's precise it will collapse and then you become deterministic system, right?",
            "So it's kind of interesting.",
            "It's the same thing as in variational inference where you have this entropy term on the Q, which effectively says make sure that you Q distribution is spread out.",
            "So the first term is trying to reconstruct the second term is trying to spread things out right?",
            "That's the intuition behind behind these models in terms of compute."
        ],
        [
            "It's the dominant cost.",
            "You have a forward pass, have a backward pass and in practice you know you can compute these things in parallel.",
            "You can also do a single.",
            "To compute these importance weights, you only need a feedforward path, which is typically easy to do.",
            "So do multiple importance, multiple forward passes and then you can just.",
            "Basically you know approximate the sum by sampling according to W. So there's a ways of speeding things up for these models.",
            "They are more expensive than just using the rational thinkers with single sample.",
            "Obvious if you use multiple samples it's more expensive, so as I mentioned before, there is no free lunch.",
            "If you have a better algorithm, typically for require more compute or more more work."
        ],
        [
            "So here are kind of like a couple of architectures that that you can try.",
            "You know you can have.",
            "Typically you have a single stochastic layer and you have deterministic layers, or you can have multiple stochastic layers and you can have multiple deterministic layers.",
            "We can sort of build these different different hierarchies.",
            "One thing."
        ],
        [
            "I want to point out which is an important thing is that if your recognition network has a stochastic layer right?",
            "Let's say you have a recognition model.",
            "It sort of goes through nonlinearities and then is stochastic layer.",
            "Here is a Gaussian layer.",
            "We have another non linear inches and inferring the posterior distribution.",
            "Then you can say that the proximate marginal posterior distribution can be multi model, right?",
            "Because when you marginalizing out over H1, this distribution can have multiple modes.",
            "So in principle, these systems can handle multimodal posteriors.",
            "Right?",
            "In theory they can, as opposed to you know, whenever you do variational inference when you assuming your distribution is fully factorized, in which case it can only handle single mode.",
            "So these systems can be much more flexible, and they can handle multi modality."
        ],
        [
            "Right?",
            "In terms of in terms of results, you know these are sort of a little bit more."
        ],
        [
            "Each result will show you real results."
        ],
        [
            "In the later part, but you know, these systems do quite well, right and."
        ],
        [
            "And you know one of the things that we've tried doing is that if you're using variational think orders to train the system using variational think orders to get some negative log likelihood, and then if you proceed training using importance on multiple samples, I ways you can get much better likelihood versus if you try training with highways and then followed with autoencoders, you get worse numbers.",
            "So somehow drawing multiple samples do give us gain, right?",
            "It can carve your posterior distributions can be quite quite flexible in the results.",
            "Yeah yeah, it's sorry I skipped through that."
        ],
        [
            "So I apologize, yeah, so this is this is the case where you have a 222 N network.",
            "You know if you're using VS with 50 samples, just the average you get this versus if you're using importance sampling, you get these numbers, so it's sort of again these samples they carve out that you sort of hit if you hit something with high probability, then the weight goes up.",
            "If you had something with low probability that goes out so you can think about is adjusting the importance of every single sample, which seems to be quite quite important."
        ],
        [
            "Yes, so the last slide and you had the architecture with the number of deterministic in myself."
        ],
        [
            "So the architecture is there.",
            "Is there any particular way that you choose or can decide how many terministic layers you have before each stochastic sampling?",
            "Or is it just kind of heuristic empirically?",
            "So yeah, this is a tough question, so I can answer maybe just very quickly.",
            "My thinking is in principle if you have a single stochastic layer, would say it's a Gaussian or uniform.",
            "If you have multiple nonlinearities when you generate the data, you can pretty much model any distribution 'cause you can take a Gaussian distribution and more fit through nonlinearities.",
            "And get whatever you want effectively, right?",
            "So in principle, from modeling perspective you only need one stochastic layer, right?",
            "From optimization perspective, it's not clear we found that whenever you have a single stochastic layer, optimization becomes easier for us just because we can start matching things up here.",
            "Right, so you because you know when within auto encoder you sort of trying to match what the recognition versus what the generative model is saying as opposed to going through multiple layers and just matching at the top.",
            "So from modeling perspective a single stochastic layer is enough.",
            "From optimization perspective, you might need some stochastic layers.",
            "Terministic yeah, same thing is for the number determines.",
            "There's no clear there's no clear answer there, yeah.",
            "Let's actually do you want to say something about the Russian, like for example, if you.",
            "This is one of the big features of these style of models compared to something like a people to machine where having these deterministic nonlinear layers really complicates things.",
            "Yeah, that's right, that's right.",
            "So it's like in models like both machines having deterministic layers can complicate things because very hard to define the energy functions in the right way.",
            "That's right, that's actually one of the would be interesting research areas to figure out how you can do undirected models, but how can you sort of have a mixed mix here?",
            "That's true, that's one of the highlights of Helmholtz machines in variational encoders that guarantees deterministic layers, and you can use backpropagation, right?",
            "So it's an auto encoder piece, plus the entropy switch.",
            "Yeah, well, the principle you can define an energy function that that has.",
            "Normally done in your domestic computation for each energy term, for example, linking related layers to some money or energy time.",
            "Of course, you want to be able to solve exactly to one layer using the other layers, as in Gibbs, but you could do.",
            "Other forms of inference.",
            "You can you can sort of like do like sort of like contrasting backdrop type things, but it's.",
            "Yeah, maybe maybe yeah, it just becomes it's.",
            "Anytime you have undirected connections and you have nonlinear potentials, if the parameters of these nonlinear potentials depend non linearly on the states, then it becomes it's no longer log.",
            "Linear model becomes a little bit.",
            "Sampling becomes might be an issue.",
            "Maybe?",
            "Yeah I don't think it's being fried.",
            "Yep, that's right.",
            "There are no papers in this space, yes.",
            "Yep.",
            "But that's an interesting.",
            "That would be an interesting thing to try doing to incorporate deterministic systems, enable some machines and see what can be done there absolutely."
        ],
        [
            "OK, so one other piece.",
            "One quick piece that I wanted to point out and I wanted to point out these hard attention models because there's been a lot of work on trying to build attention mechanism and you've probably seen some attention mechanisms discussed before in the summer school and also sort of highlight that they become a special cases of Helmholtz machines be."
        ],
        [
            "Basically, in the same framework.",
            "So this is a caption generation system that was built by Jamie Kearse at Toronto back in 2014, actually and then 2015.",
            "So this is the case where you have an image and you have some neural language model predicting language model recurrent network that generates captions right and sort of."
        ],
        [
            "As reasonably well, in our case, the encoder was a CNN and then you have a recurrent net for join for embedding in your language model.",
            "For decoding the sentences here."
        ],
        [
            "Just to show you so that so that you guys can relax a little bit from all the equations that I've been showing to you.",
            "This is some of the fun things that the model can generate, right?",
            "Like for this image generates the two birds are trying to be seen in the water.",
            "Right, which is something that a human would never generate, so at least we can generate.",
            "We can see that the model can generalize."
        ],
        [
            "Just can't tell that once every month.",
            "Yeah, I mean the question is can account I actually I don't.",
            "I don't want to claim that it can count.",
            "Yes, that's right.",
            "In this case account.",
            "This is also The funny thing.",
            "It's like the handlebars are trying to ride a bike rack right so?",
            "There are sort of like things that it can generate, which is which is, which is nonsense."
        ],
        [
            "There's also been.",
            "This is in collaboration with Montreal, where this is a system that uses visual attention.",
            "Right now, speaking up in the computer vision field where if you're generating is captured, a man riding a horse in a field, then whenever you generating a manic focus on the man riding a horse, it sort of focus in the horses head and the feeling focuses on.",
            "Right, so it's kind of like trying to solve both problems and it's."
        ],
        [
            "And one of the visual attention algorithm.",
            "Motivations for visual attention is that imagine that you're trying to process videos.",
            "Instead of processing every single frame you would like to process only small piece of each frame, right?",
            "And it also adds a little bit of degree of interpretability, like where the model is looking at when it's generating certain things."
        ],
        [
            "And here's just some examples that I want to show.",
            "You know, for videos this is using something it's called soft attention, but it sort of focuses on on different on different pieces when you process."
        ],
        [
            "Videos, right?",
            "So there's been a.",
            "A set of research that's taking place in sort of called recurrent attention models, maybe just taking quickly show you what they are and making an action to reinforcement learning.",
            "You have seen some amazing talks on reinforcement learning yesterday.",
            "I believe it was reinforcement learning day, so imagine you have an image right?",
            "And then you sample in action action here could be the location.",
            "Of where you should be looking next.",
            "So imagine you have a system that you just have these multiple glimpses.",
            "You trying to figure out where you should be looking next, and you have a stochastic system where given an action given distribution of a possible location, so possible gauge locations you can sample and so forth, right?",
            "So you're going through that process and recurrent network.",
            "Can you classify things right?",
            "You can.",
            "Basically your goal is to classify the digit 4, for example right?",
            "But you have all of these stochastic latent variables that you can think of them as softmax, or Gaussians that specify which location you should be looking at.",
            "Right and there are two classes of models that are hard attention models where you sample actions or these gays locations right so that you can process only small pieces of each image or there is a soft attention where you're taking expectations as opposed to taking as opposed to sampling."
        ],
        [
            "And there's been a lot of work on in that space.",
            "I'm just listing a few.",
            "There's a work coming from Montreal that was using attention.",
            "Was introducing attention models for translation.",
            "There was one of the first models using soft attention and recurrent network at about the same time where people at deep mind have been sort of using.",
            "Stochastic hard attention models the work of Rodney.",
            "Sort of.",
            "We're looking at these stochastic models that are showing Hinton back in 2010.",
            "They were looking at sort of attention models and of course people have been looking at that space for a very long time since the 80s, right?"
        ],
        [
            "So let me quickly give you the mathematical details and hopefully convince you that these classes of models are pretty much the same as Helmholtz machines, or directed graphical models, just applied slightly different in a different context.",
            "You know, So what you want to do, you want to maximize the probability of the correct class by marginalizing over actions.",
            "So you can think of actions as your latent variables, right?",
            "Think of action that basically says where I should be looking in the image, and there are exponentially many ways of looking at images.",
            "Right now I look at the image here and here and here and here or here and here and here and here, right?",
            "So this space is exponential, but you can view it as just another latent variable.",
            "Model W is the set of the parameters of the recurrent network A is now your latent variable set of.",
            "Actions that you need to take an X is the input or an image or video frame and whatever."
        ],
        [
            "Whatever that is, right?",
            "So one way to do things, and that was done by Jimmy and by Vlad is to look at the variational bound right now, friend variational bound.",
            "Wake you here is again some approximation to the posterior over the gays location at anytime you have some latent variables, some unknown variables, you can have some approximation to what the true distribution posterior distribution should look like.",
            "Now one interesting fact is that if you approximating distribution, is the prior right UQ.",
            "Here is just the prior prior over the actions, then the variational bound becomes quite simple, it just becomes this expression here.",
            "Write an."
        ],
        [
            "With variational learning, what you can do is you can say well, if I'm just going to assume that you know it's just the prior, you take the respective model parameters.",
            "You have these terms.",
            "The precise definition of these terms is not important.",
            "What is important is that you have this really bad term here.",
            "Popping up.",
            "What is this term doing?",
            "If you predict in class and let's say the probability of your correct prediction is .0001.",
            "Take the log of that you have a huge negative number.",
            "Right, so it really messes up your gradients becomes very unstable, so a lot of people were introducing heuristics.",
            "Like for example you can replace this term with 01 discrete variable, which is kind of leads to something that's going to reinforce algorithm.",
            "Like if you hit the correct class, it's one if you're hitting wrong class at 0, so you bounding is you have to find some way of bounding this term.",
            "Right, which is which."
        ],
        [
            "A little bit problematic, but then again you can use stochastic estimation.",
            "You can draw samples from the prior and just approximate.",
            "And there's a whole bunch of things called control various techniques where people are trying to figure out how can they reduce the estimates.",
            "The variance in the estimates, right and?"
        ],
        [
            "So here you know you sample from the prior given image.",
            "You just run the network forward just like a recurrent network running forward, and then you can you can estimate."
        ],
        [
            "One of the key observations is that just like in importance weighted autoencoders, just like in highways, what you can do is you can actually try to take derivative of the log likelihood directly.",
            "If you actually do that, you have this expression where Z here is the sum over these guys, but what's important is that this term here is the posterior.",
            "Right, so maybe just step back a little bit and to say if I could compute exact posterior over actions.",
            "Right then problem would have been solved.",
            "Right, so it's almost like saying I have an image.",
            "I see an airplane in the image.",
            "Given that I see an airplane in the image and the image itself, where should I be looking right?",
            "What's the sequence of gaze locations I should be taking?",
            "And maybe your true posterior would be say we look at the Sky's plans are typically in the skies, right?",
            "But unfortunately you cannot use.",
            "You cannot compute this past exactly.",
            "So what you can?"
        ],
        [
            "You can use important sampling, So what can you do?",
            "Again, you have to introduce some approximating distribution.",
            "Q, right?",
            "So there has to be some approximation and then you can use important sampling, right?",
            "You can draw multiple samples, compute the important weights and you can get the estimates of the gradients right?",
            "So what are all the high level picture?",
            "I put the equations here for those of you who are interested in precise details, but the high level picture here is that you have some approximation.",
            "You draw samples from this approximation using important sampling to basically refine your estimates of the true posterior right."
        ],
        [
            "And so what's the difference between the two approaches?",
            "The difference between the two approaches is that they basically the same.",
            "It turns out that just like invasion of encoders and Hwy, they pretty much the same.",
            "The computers the same except 4.",
            "Here you use important weights and you don't have this bad term here.",
            "That messes up the estimates of the gradients.",
            "And of course, the performance of important sampling is very going to be heavily relied on.",
            "The quality of your approximation, the quality of the approximation of the posterior right on your recognition network on your Q network.",
            "How good is it?",
            "And there is some work related.",
            "There was a work if your Q distribution is the pride and it's something similar to what Charlie Tongue a student at University of Toronto was doing back in 2015.",
            "And it's also very similar in spirit to the related Wake sleep model that was developed in Montreal."
        ],
        [
            "Another key observation here is that with a finite number of samples you can basically show that what we estimated with a finite number of samples is actually.",
            "The lower bound, so in expectation we optimizing the lower bound on the marginal log likelihood.",
            "Exactly the same ways and highways and the become becomes tighter as we increase the number of samples.",
            "So that's one.",
            "Take home message that I would like to communicate is that as you increase the number of samples, you can make your bound variational bound.",
            "You can make it tighter, of course by how much and what's the variance of those estimated, since it's a little bit hard to say."
        ],
        [
            "But this estimator is at least as accurate as the variational bound, right?",
            "So that's one thing that we can, we can."
        ],
        [
            "It also has a beautiful relationship to Helmholtz machines, right?",
            "Because what is this?",
            "What is this model here?",
            "Yes, the castec units, which you can think of actions or what the policy is.",
            "You have deterministic units.",
            "Yes, the cast in Terministic unit.",
            "So you can basically view it as just big gigantic neural network with stochastic and deterministic units."
        ],
        [
            "So you can basically view it as a conditional helpless machine, right?",
            "An you can use wake sleep algorithm.",
            "You can use any way to wake sleep algorithm you can you know if your distribution of your actions Gaussians you can use representation trick and.",
            "And sort of various ways of learning good attention.",
            "Policies right?",
            "So that's one message that I want to communicate is that the whole space of different algorithms?",
            "Just again you can view them as variants of host machines, yes?",
            "Now you can change the direction, whereas before you were generating parameters variables.",
            "Now it looks like you are going to reservations to generating those latent variables.",
            "No, you can basically view it as a conditional model right where you sort of.",
            "Given this, you generate the latent variable and this state of this variable effects what goes in here, which affects this state and effects what's going here.",
            "In effect, if you just if you just take this model, turn it upside down, it will really look like.",
            "Conditional helmholz machine.",
            "This particular state will affect everything that goes on here and this state that you sample will affect what's going on here.",
            "Right so."
        ],
        [
            "Let me just basically saying you can also build the recognition model so a lot of these models they do look like there is a recognition network.",
            "There's a generative network, right?",
            "The two networks that are interplaying together, so there's a P network genitive, not networking, is a recognition network."
        ],
        [
            "Right, so in this case again, you can construct the Q, your recognition network, which is a network that takes into account where you should be looking.",
            "It takes into account the class label, right?",
            "It takes into account image and what is it that you're trying to recognize to give you the posterior, the proper approximate posterior probability of where you should be looking right?",
            "Let me just keep for the training.",
            "You can train these models using standard using standard techniques.",
            "Here's one thing that you can."
        ],
        [
            "You can also sort of you know, if using attention you can not only choose the location, but also you can choose this scale so you can have assistance that choose where to look at what scale you want to look right.",
            "These are just some preliminary examples that do show, like for example.",
            "I think on this.",
            "You know?",
            "You're basically having both of them at the same time, which is typically very hard to hard to."
        ],
        [
            "So hard versus soft.",
            "Soft attention models there.",
            "Computationally expensive because you need to.",
            "You need to examine every single location in the image.",
            "If you're doing it from images, but they are deterministic, there might be a little bit easy to train because they can be trained by using back propagation algorithm.",
            "In terms of computational terms, are attention models the computation more efficient because you can only look at sub pieces.",
            "You can only analyze.",
            "Specific areas in the images of particularly working with video applications, but there stochastically requires some form of sampling or some form of having an approximation.",
            "The way you should be looking right.",
            "And research is taking place in both fronts."
        ],
        [
            "Now let me just finish off by having about five 510 minutes.",
            "Let me just finish off by showing you some applications, maybe pointing out some open."
        ],
        [
            "Open problems.",
            "So the few interesting areas that a lot of us in the deep learning community are working on is 1 area is unsupervised."
        ],
        [
            "Right, I've talked a lot about derivative models and the themes of the date works today.",
            "Summer School is looking at generative models or doing a supervised learning things like unsupervised learning, transfer learning 1 short learning are the areas where we have to make more progress.",
            "There is a whole subset of subgroups of people working on reasoning attention.",
            "I've shown you some attention models and memory.",
            "There is also an interesting work taking place in natural language.",
            "Understanding this is 1 area where.",
            "Deep learning can make a big impact, and I'm going to show you some examples of that and also the area of reinforcement learning, deep reinforcement learning, and you've seen some of that yesterday I believe.",
            "Let me just show you some work on natural language processing and natural language and."
        ],
        [
            "Setting and supervised learning there is a.",
            "There's an area sort of called sequence to sequence learning.",
            "You've covered that.",
            "OK, that's good.",
            "So your given sequence, you have another output sequence here."
        ],
        [
            "So A1 model which unit you can try to encode sentences.",
            "So given a sequence of sentences right, you can take the middle sentence and you can try to predict the context around the two sentences, right?",
            "Just like in Word, two vec repres."
        ],
        [
            "Patient and you can sort of encode the sentence.",
            "You can predict you can generate the previous sentence and you can generate the forward segments right?",
            "So trying to predict the context around the sentence right and so."
        ],
        [
            "For this particular model, you can think of it as just the recurrent network predicting forward and backward sentences.",
            "You can optimize.",
            "There is no statistics.",
            "It's fully deterministic, and if you train it on large enough data set which is allowed."
        ],
        [
            "1000 books you can get pretty interesting results.",
            "You can actually get pretty good results.",
            "Um, you know, there's nothing linguistic.",
            "There's no linguistic information sitting in these models is just crunching through the data and you pretty much hit state of the art on tasks like semantic relatedness task.",
            "Right across all different models that use specific features redesigned trying to do handcrafted, handcrafted."
        ],
        [
            "Here's here's what the task is.",
            "Maybe I can show you and get you excited about the task.",
            "The task is the following.",
            "I give you 2 sentence is and I tell you are they semantically similar or not on the scale of 1 to 5.",
            "The ground truth is you ask people they ask 10 people and I take the average and that's the ground truth.",
            "So if I show you a man is driving a car, a car is being driven by the man.",
            "The ground truth is 5 because people say it's the same.",
            "The prediction is 4.9.",
            "A little girl is looking at a woman in costume, a girl, a little girl in costume.",
            "Looks like a woman.",
            "Hopes and this is the ground truth is 2.9 the predictions 3.5 which is not too far.",
            "This basically means that they are unrelated, but maybe there is some relationship, but here's a failure example.",
            "This is where the model fails and this is something we need to improve.",
            "A person is performing tricks on a motorcycle.",
            "The performer is tricking a person on the motorcycle, right?",
            "So it takes a little bit of understanding of what these two sentences are.",
            "The model doesn't differentiate between the two.",
            "Play some African leaders.",
            "You just basically think of it as ask people two sentences.",
            "Do you think they are semantically the same or not?",
            "Number two, I don't understand what I groundskeeper two point 92.9 so 2.9 basically means that you ask 10 people.",
            "They give you bunch of opinions, the average, and that's 2.9.",
            "They don't report the variance, so maybe somebody said five.",
            "It's the same thing.",
            "Or somebody said 11 basically means it's completely irrelevant, like I'm giving a talk and Microsoft crashed yesterday.",
            "There is no relationship between these two.",
            "Yeah, yeah, I think that semantically they basically mean do they mean the same thing?",
            "That's right, so question two that half the people just get it wrong.",
            "I you know, but at the same time you know there is some like like if you compare these two sentences versus a sentence like I mentioned right now, right?",
            "Like these two sentences are a little bit more similar than.",
            "Yeah, sure.",
            "By related.",
            "Yeah, I mean it's for relatedness.",
            "You know it's just.",
            "It's just people's opinion.",
            "Or do they mean the same thing?",
            "Here's"
        ],
        [
            "Another, here's another sort of fun thing.",
            "The fun thing to look at, and I'm almost done.",
            "You can basically generate.",
            "The model it generates little stories, so this model was trained by Jamie curious on 7000 romantic novels and given an image you basically ask the model.",
            "It's a recurrent neural net model to generate the description right?",
            "So it's kind of like sort of does funny things like she's beautiful, but the truth is I don't know what to do with the sun was just starting to fade away, leaving people scattered around the Atlantic Ocean.",
            "So it's sort of does you know it sort of tries to capture some features of images and.",
            "Generate coherent semantic, syntactically coherent pieces semantically is not very coherent, so we still have a."
        ],
        [
            "And for improvement, this is another area that was coming out from Chris dies Group as well as a student at CMU which basically you can build these hierarchical RNS taking taking words and characters, characters and words, putting them together."
        ],
        [
            "And try to solve multiple tasks like parts of speech chunking, named entity recognition and the remarkable thing that I want to communicate here is that you basically achieving state of the art results by using these models, which was to me was a little bit surprising compared to a lot of linguistic features that are people using for these very well defined very well defined tasks.",
            "Let me just skip one shot.",
            "Learning peace and maybe."
        ],
        [
            "Point out that you know there's also some work taking place in."
        ],
        [
            "Deep reinforcement learning where you can try to build systems that can do transfer learning or can play multiple games right in the space of can you build a system that looks at 20 games and understand something about those gates, and then you start playing the new game apps to the new game.",
            "Much quicker, right?",
            "So these are sort of."
        ],
        [
            "Multiple multiple."
        ],
        [
            "Researchers because you already covered the 1st morning, let me skip over that.",
            "So let me just sum."
        ],
        [
            "Rise by saying that.",
            "You know there's been a lot of exciting developments in the space of deep learning in the last.",
            "In the last, I guess, 567 years, ranging from the space of detection, speech recognition, tagging, building hierarchies, working in multi model space and these models do improve upon current state of the art in a lot of different application areas.",
            "And it's due to three really things.",
            "It's due to the fact that we have more data right?",
            "It's due to the fact that we have more compute.",
            "And it's also due to the fact that the algorithms that have been developed we can train these systems much more efficiently than we used to.",
            "Let's say 10 years ago.",
            "OK, so let me let me stop."
        ],
        [
            "Here, let me also thank all the students have been involved in a lot of different projects in my lab, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start by basically saying that you know we live in an age of big data, right?",
                    "label": 0
                },
                {
                    "sent": "If you look at the space of images, language is the mic working OK?",
                    "label": 0
                },
                {
                    "sent": "Is it better now?",
                    "label": 0
                },
                {
                    "sent": "OK, fantastic.",
                    "label": 0
                },
                {
                    "sent": "I would argue that most of the data that we see is unlabeled, right?",
                    "label": 1
                },
                {
                    "sent": "And probably throughout this summer school you've seen a lot of talks that you supervised learning in this particular.",
                    "label": 0
                },
                {
                    "sent": "In today's lecture we're going to be talking mostly about unsupervised learning or learning generative models.",
                    "label": 1
                },
                {
                    "sent": "And you know one of the things that I'm interested in, and there's a lot of people in our community interested is how can we develop statistical models, models that can discover interesting kinds of structures in unsupervised?",
                    "label": 1
                },
                {
                    "sent": "Or maybe semi supervised way, right?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I would argue is that one way of doing that would be to build these deep hierarchical models, generative models, models that support inference, as well as discover structured multiple multiple levels of representation.",
                    "label": 0
                },
                {
                    "sent": "And I'll make it more precise as I go through the top.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just show you one application area.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Understanding images how many you know who that is?",
                    "label": 0
                },
                {
                    "sent": "Ah, great, now I remember Antonio actually gave a talk here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the picture of Antonio that I took at Nips are year and a half ago.",
                    "label": 0
                },
                {
                    "sent": "It was in Montreal's over here and you can ask the system to say, well, can you tag it right?",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Can you associate words with that image and you can build a system that says strangers, coworkers and such, which is probably reasonable, but then you can go beyond that, right?",
                    "label": 0
                },
                {
                    "sent": "You can say, well, can you actually build a system that generates captions or generates natural language descriptions?",
                    "label": 0
                },
                {
                    "sent": "And one way of doing that, I would say, kind of you know the.",
                    "label": 0
                },
                {
                    "sent": "Straightforward way of doing that would be to find a similar image, maybe maybe using cover that features and then copy the corresponding captions right from the training set.",
                    "label": 0
                },
                {
                    "sent": "That would be a nearest neighbor approach if you do that that's nearest neighbor sentence says people taking pictures of a crazy person.",
                    "label": 1
                },
                {
                    "sent": "That's actually true.",
                    "label": 0
                },
                {
                    "sent": "That was the closest image that the system found.",
                    "label": 0
                },
                {
                    "sent": "Antonio really liked it because it was online and you can actually retrieve.",
                    "label": 0
                },
                {
                    "sent": "But then if you actually build a model you know this is what the model would say.",
                    "label": 0
                },
                {
                    "sent": "Group of people in the crowded area, group of people are walking, talking group of people standing around and walking in sunshine.",
                    "label": 1
                },
                {
                    "sent": "So this is 1.",
                    "label": 0
                },
                {
                    "sent": "One interesting application error that I'm going to talk about later in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but for the purposes of this tutorial, what I'm going to do is I'm structuring the tutorial into two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part will be focusing on learning undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "So deep undirected graphical models in particular models such as restricted both machines and both machines, and I'll try to go into mathematical details about these models and how we can train them.",
                    "label": 1
                },
                {
                    "sent": "And the second part of the tutorial is going to be focusing a little bit more on directed graphical models, so these would be helpful as machines and will go in.",
                    "label": 1
                },
                {
                    "sent": "Algorithms and training procedures for variational autoencoders and importance weighted autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Because these are the models that are being heavily used across.",
                    "label": 0
                },
                {
                    "sent": "If you go to HTML this year, last year you would probably see a lot of people are looking at generative models.",
                    "label": 0
                },
                {
                    "sent": "Deep generative models, including variational think orders.",
                    "label": 0
                },
                {
                    "sent": "And then I'm also going to talk a little bit about stochastic attention model so hard attention models and show the connection between these models to help those machines.",
                    "label": 0
                },
                {
                    "sent": "And then now finish up with some applications.",
                    "label": 0
                },
                {
                    "sent": "So some open questions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me start with restricted.",
                    "label": 0
                },
                {
                    "sent": "Boltzmann is how many of you know about what carbs are?",
                    "label": 1
                },
                {
                    "sent": "A few of you that's good, so these are very old models.",
                    "label": 0
                },
                {
                    "sent": "I guess they were basically developed back in 86.",
                    "label": 0
                },
                {
                    "sent": "You basically you can think of GBM as a Markov random field.",
                    "label": 1
                },
                {
                    "sent": "You have stochastic binary hidden variables H and you have stochastic visible variables.",
                    "label": 1
                },
                {
                    "sent": "Those are binary as well V. You can think of the variables as pixels in your images and you can think of H as feature detectors.",
                    "label": 0
                },
                {
                    "sent": "So hidden or latent latent variables right?",
                    "label": 0
                },
                {
                    "sent": "You can specify the joint distribution over the observed invisible variables through that particular equation.",
                    "label": 0
                },
                {
                    "sent": "Over here you have pairwise so called.",
                    "label": 0
                },
                {
                    "sent": "Sometimes these are called pairwise potentials or pairwise interactions, and you have unary interactions right.",
                    "label": 0
                },
                {
                    "sent": "Sometimes these models are also called log linear models, Markov random fields, both machines, But the reason why they call log linear models.",
                    "label": 1
                },
                {
                    "sent": "Because if you take the log of the probability, the log of the probability, then you basically have a linear dependence on the parameters W, right?",
                    "label": 0
                },
                {
                    "sent": "And you can think of parameters W is trying to learn some sort of correlations between observed and hidden variables.",
                    "label": 0
                },
                {
                    "sent": "From that particular model in the structure of this model, you can define the conditional probability and the conditional probability is going to be given by product of so called logistic functions, right?",
                    "label": 0
                },
                {
                    "sent": "So sometimes these models are called generative models because what they do is that given the states of the hidden variables, you can generate the data, right?",
                    "label": 0
                },
                {
                    "sent": "You can sample the observed variables.",
                    "label": 0
                },
                {
                    "sent": "We can see what your images would look like.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What kind of features these models are learning?",
                    "label": 0
                },
                {
                    "sent": "If you apply them to, you know these handwritten characters data set, then your features look like little edges, so little strokes.",
                    "label": 0
                },
                {
                    "sent": "Right and now given you image, you can basically say, well, this new image is made up by some of some of these.",
                    "label": 0
                },
                {
                    "sent": "Combination of these edges.",
                    "label": 0
                },
                {
                    "sent": "In these numbers here are given by the probabilities that the particular hidden unit or particular hidden variables being activated, and you typically get sparse representation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now how can we do learning in these models?",
                    "label": 0
                },
                {
                    "sent": "Well, one way of doing learning is, given a set of ID training examples you want to run model parameters and the way to do it would be to maximize the log likelihood objective.",
                    "label": 1
                },
                {
                    "sent": "That's one way of optimizing these models, and this log likelihood objective also should should make sense.",
                    "label": 0
                },
                {
                    "sent": "What it's basically saying is that given the training data, if I look at the joint probability of observing the training data, I want the joint probability to be as high as possible, right?",
                    "label": 0
                },
                {
                    "sent": "So finally parameters so that this joint probability is as high as possible.",
                    "label": 0
                },
                {
                    "sent": "And in the log space is just a sum of logs.",
                    "label": 0
                },
                {
                    "sent": "You can differentiate the log likelihood that the standard way of deriving learning learning rules for these models is if you if you take derivative of log language with respect to your parameters, then you know there is a little bit of math.",
                    "label": 0
                },
                {
                    "sent": "It's not very difficult, just a little bit of algebra, but it comes down to being the difference between two terms right?",
                    "label": 0
                },
                {
                    "sent": "The first term is so-called expected sufficient statistics where expectations driven by the data and the second term is expected.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics driven by the model.",
                    "label": 0
                },
                {
                    "sent": "So the intuition behind the learning rule is basically saying, well, look at the correlations that you see in the data.",
                    "label": 0
                },
                {
                    "sent": "Effectively look at the correlations that the model is implying given the parameters of the model, and you try to match the two.",
                    "label": 0
                },
                {
                    "sent": "And that's the learning rule for.",
                    "label": 0
                },
                {
                    "sent": "Basically, for any kind of undirected graphical model, if you work with Markov random fields, both machines conditional random fields, you know there's a lot of work on CRF's.",
                    "label": 0
                },
                {
                    "sent": "Basically, that's that's the matching rule for all of those models.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the first expectation is easy to compute because of a particular structure of restricted Boltzmann machine, right?",
                    "label": 0
                },
                {
                    "sent": "You don't have any lateral connections between the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "We can actually compute the first term exactly.",
                    "label": 0
                },
                {
                    "sent": "The second term is something that you cannot compute exactly because it requires summing over exponentially many configurations.",
                    "label": 0
                },
                {
                    "sent": "So so in this case.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at the learning rule, the first term is easy to compute.",
                    "label": 1
                },
                {
                    "sent": "The second term is the sum of all possible configurations of the visible and hidden variables, which is an exponential salmon.",
                    "label": 0
                },
                {
                    "sent": "Typically, you know most successful algorithms are using some form of Markov chain Monte Carlo to approximate the second term.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work on in graphical models trying to use pseudo likelihood piecewise likelihood and all different variational inference better free approximation, but for these kinds of models it seems that.",
                    "label": 0
                },
                {
                    "sent": "Markov chain Monte Carlo, in particular something called persistent contrastive divergent and constructive divergences, is the way to do it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, one of the things about these models is then you can create.",
                    "label": 0
                },
                {
                    "sent": "You can model other kinds of distributions.",
                    "label": 0
                },
                {
                    "sent": "So for example you can build so-called Gaussian Bernoulli Boltzmann machine restricted both machines where you have stochastic real valued visible variables V and you just change a little bit of a definition of the model.",
                    "label": 1
                },
                {
                    "sent": "So you have sort of Gaussian visible variables.",
                    "label": 0
                },
                {
                    "sent": "You ask the castec binary latent variables.",
                    "label": 0
                },
                {
                    "sent": "And again here the conditional probability is going to be given by the product of normal distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So that basically allows you to model, let's say if you want to model grayscale images.",
                    "label": 0
                },
                {
                    "sent": "If you want to model real value data, you sort of can create these hybrid graphical models where some variables are binary.",
                    "label": 0
                },
                {
                    "sent": "Some variables are real valued.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you apply this model to images image kind of data then you find these.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edge like structure.",
                    "label": 0
                },
                {
                    "sent": "So given a new image you can basically do exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "You can say, well this new image is made up by some linear combination of subset of the learn features right?",
                    "label": 0
                },
                {
                    "sent": "And these numbers again here given the probabilities of that particular feature appearing in the data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also again take the model modified a little bit.",
                    "label": 0
                },
                {
                    "sent": "And you can this.",
                    "label": 0
                },
                {
                    "sent": "In this case you can model count data and count data is very useful when you're modeling things like documents, word counts, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case the conditional probability is going to be given by the softmax distribution.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have a document that contains the words and the vocabulary size they were working in.",
                    "label": 0
                },
                {
                    "sent": "English is maybe 100,000, so K would be over the size of 100,000.",
                    "label": 0
                },
                {
                    "sent": "And if you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply this model to working with count data.",
                    "label": 0
                },
                {
                    "sent": "In this case it's about 800,000 news stories.",
                    "label": 0
                },
                {
                    "sent": "You can find sort of these other interesting topics right, so in this case, again every single document is given by a combination of of these topics, right?",
                    "label": 0
                },
                {
                    "sent": "So one thing that I want to highlight here is that you know if you apply these models to count data to find latent topics.",
                    "label": 0
                },
                {
                    "sent": "If you applied to image data, you find features so different.",
                    "label": 0
                },
                {
                    "sent": "In different areas you can find interesting interesting kind of low level structure.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so you can model different data modalities.",
                    "label": 1
                },
                {
                    "sent": "You can basically extend these models to modeling any kind of.",
                    "label": 0
                },
                {
                    "sent": "Do other members with exponential family models?",
                    "label": 0
                },
                {
                    "sent": "And one of the things about restricted both machines is very it's very easy to infer the states of the hidden variables, right?",
                    "label": 1
                },
                {
                    "sent": "So for example here the conditional probability of hidden states given the visible states is again given by the product of logistic functions.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That basically means that given the data, you can instantly figure out the distribution of the topics of distribution of features.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "I can tell you exactly what features you see in your data, what topics issues you should see in the data.",
                    "label": 0
                },
                {
                    "sent": "So that's very useful.",
                    "label": 0
                },
                {
                    "sent": "Probably, and in particular applications like information retrieval and recognition.",
                    "label": 0
                },
                {
                    "sent": "It's an important feature to have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sometimes these models are called product models, so that's.",
                    "label": 0
                },
                {
                    "sent": "Product of experts and back in the late 90s Jeff Hinton was working with these models along with the other researches, but the idea behind product models is that if you look at the joint distribution, the joint distribution observed in hidden variables.",
                    "label": 1
                },
                {
                    "sent": "Its belongs to exponential family models, right?",
                    "label": 0
                },
                {
                    "sent": "It's a log linear model, but if you look at the marginal distribution, if you marginalized over the hidden variables, you have this interesting form.",
                    "label": 0
                },
                {
                    "sent": "You have bunch of product right?",
                    "label": 1
                },
                {
                    "sent": "So here you seeing this product of bunch of functions.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "What's the intuition here?",
                    "label": 0
                },
                {
                    "sent": "The intuition here is following.",
                    "label": 0
                },
                {
                    "sent": "Let's say finding these topics in my data right and then if I tell you that topics like government, corruption and oil.",
                    "label": 0
                },
                {
                    "sent": "Occur in a document.",
                    "label": 0
                },
                {
                    "sent": "These are the topics in the document.",
                    "label": 1
                },
                {
                    "sent": "What Putin will have very high probability.",
                    "label": 0
                },
                {
                    "sent": "Right, and the reason why is because you know what happens here is that you take the distributions, you multiply them together and you renormalize.",
                    "label": 0
                },
                {
                    "sent": "So that's why the product comes in, right?",
                    "label": 0
                },
                {
                    "sent": "It's very different from the mixture based models models like mixture of Gaussians or even that mixture.",
                    "label": 0
                },
                {
                    "sent": "Models like late dislocation models where you first speak the topic and then you generate the word here.",
                    "label": 0
                },
                {
                    "sent": "You take the product of distributions and then you normalize.",
                    "label": 0
                },
                {
                    "sent": "You can get very spiky distribution so it can be very precise in terms of what you're modeling.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In these models they do well.",
                    "label": 0
                },
                {
                    "sent": "They do much better in terms of information that you will finding similar documents and such.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if I step back a little bit and say OK, what's next?",
                    "label": 0
                },
                {
                    "sent": "And these models, as I mentioned, ridiculous machines been around for about 20 years.",
                    "label": 0
                },
                {
                    "sent": "Well, one thing that",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do in the spirit of building hierarchical models, building deep learning models that we can start adding multiple levels of representation, right?",
                    "label": 0
                },
                {
                    "sent": "So we can add an additional layer of latent variables.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that you know you, hoping that at the high level you start capturing high level features, right?",
                    "label": 1
                },
                {
                    "sent": "So in this case, the first layer you can get your low level correlations in data such as edges.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you can capture some low level correlations.",
                    "label": 0
                },
                {
                    "sent": "Some correlations within text data like correlations between the words, but as you start adding layers you can start getting.",
                    "label": 0
                },
                {
                    "sent": "High level features so you only simple representations, and then you compose them into more complex ones.",
                    "label": 1
                },
                {
                    "sent": "You can spell.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sify again as before, you can specify the distribution over your stochastic variables.",
                    "label": 0
                },
                {
                    "sent": "Hidden variables as well as the visible variables.",
                    "label": 0
                },
                {
                    "sent": "So now introducing dependencies between hidden variables and all connections here are undirected, so this is an instance of undirected Markov random field where you just have multiple layers of hidden variables.",
                    "label": 1
                },
                {
                    "sent": "So what's different from before is that here you have.",
                    "label": 0
                },
                {
                    "sent": "The first term here is the same as the ARB term, right?",
                    "label": 1
                },
                {
                    "sent": "And you have these two additional terms and these two additional terms are modeling dependencies between H1 and H2, and H2 and H3, right?",
                    "label": 0
                },
                {
                    "sent": "So you're modeling dependencies between hidden variables.",
                    "label": 0
                },
                {
                    "sent": "It is also very natural notion of bottom up and top down, just by definition of graphical model of this particular graphical model, where conditional probability of a particular variable hidden unit here taking value, one is going to be given by logistic function, but it's going to be given by what's coming from above from what's coming from below.",
                    "label": 1
                },
                {
                    "sent": "Right, so that sort of has a very natural notion of bottom up and top down.",
                    "label": 0
                },
                {
                    "sent": "An unfortunately in these kinds of models, as we making them more expressive, your hidden variables become dependent even when you conditional input.",
                    "label": 0
                },
                {
                    "sent": "So we have to do a little more work to do inference in these models.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we do maximum likelihood learning?",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, if you look at the maximum likelihood learning rule, if you look at the log of the probability deliverability with respect to parameters, surprises just the same learning rule.",
                    "label": 1
                },
                {
                    "sent": "Write the same learning rules before you just looking at the expected sufficient statistics driven by the data expected.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics given by the model and trying to match match the two.",
                    "label": 1
                },
                {
                    "sent": "The problem with these models is that both expectations are intractable, so we have to do something here, right?",
                    "label": 0
                },
                {
                    "sent": "So it's one of those things that you often see that you're trying to make your model more expressive.",
                    "label": 0
                },
                {
                    "sent": "Writing your models are more expressive, but there is no free lunch.",
                    "label": 0
                },
                {
                    "sent": "You have to pay costs generally right?",
                    "label": 0
                },
                {
                    "sent": "So we have to come up with some, maybe more or some way of doing approximate inference, and pretty much most of the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generative models so both expectations are intractable, but let me give you an intuition as to what these two expectations are doing.",
                    "label": 1
                },
                {
                    "sent": "What these two different terms in learning rule out doing, right?",
                    "label": 1
                },
                {
                    "sent": "So the first term, actually, you know the conditional probability here, is no longer factorial, so it's very hard for us.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compute it, but here's what's happening.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a data manifold, and let's say you observe these data points.",
                    "label": 0
                },
                {
                    "sent": "This is the truth.",
                    "label": 0
                },
                {
                    "sent": "This is your training data.",
                    "label": 0
                },
                {
                    "sent": "What you'd like to do is, you'd like to say, well, I've seen the data, so I'd like to make it more probable, because this is a real handwritten character, right?",
                    "label": 0
                },
                {
                    "sent": "And that's effectively what the first term is doing.",
                    "label": 0
                },
                {
                    "sent": "The first expectation is doing now.",
                    "label": 0
                },
                {
                    "sent": "If you look at some image like this.",
                    "label": 0
                },
                {
                    "sent": "Right again, these are 28 by 28 images.",
                    "label": 0
                },
                {
                    "sent": "If you look at the number of possible configurations then there are two to 784 possible configurations, right?",
                    "label": 0
                },
                {
                    "sent": "And what you effectively want to make sure is that this particular image should have small probability with very exponentially small probability.",
                    "label": 0
                },
                {
                    "sent": "So you have this exponential space and you carving out these probability bumps on the real data and you trying to push down on this entire exponential space, right?",
                    "label": 0
                },
                {
                    "sent": "And this is effectively this term comes from the definition of so-called partition function.",
                    "label": 0
                },
                {
                    "sent": "Which is a very fundamental quantity in a lot of different areas.",
                    "label": 0
                },
                {
                    "sent": "Particularly if you look at the whole research agenda taking place as to how do you evaluate, how do you estimate partition function log of the partition function?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here for these kinds of models we can use variational inference, and we can also use Markov chain Monte Carlo based inference.",
                    "label": 1
                },
                {
                    "sent": "Something is called stochastic approximation.",
                    "label": 1
                },
                {
                    "sent": "So what I'd like to do is I'd like to go over both of these procedures just because we go through the rest of the tutorial will see variational inference more and more, and I'm sure that when she is going to be giving up his tutorial second half the tutorial, I'm sure he's going to be covering variational inference is, well, how many of you are familiar with variational inference?",
                    "label": 0
                },
                {
                    "sent": "Oh, fantastic, that's that's more than I thought.",
                    "label": 0
                },
                {
                    "sent": "This is great.",
                    "label": 0
                },
                {
                    "sent": "How many of you familiar with Markov chain Monte Carlo based methods?",
                    "label": 0
                },
                {
                    "sent": "Ah, wow, that's that's a very impressive, very impressive audience.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I can go very very quickly, so let's look at the Merc based approximation.",
                    "label": 0
                },
                {
                    "sent": "Well, sometimes caustic approximation.",
                    "label": 0
                },
                {
                    "sent": "It's actually very simple.",
                    "label": 0
                },
                {
                    "sent": "What you're doing is you're going to be updating your parameters of the model and the state sequentially.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You're going to generate a sample.",
                    "label": 0
                },
                {
                    "sent": "According to some transition operator, by effectively simulating a Markov chain that leaves your distribution, your P, Theta septin variant.",
                    "label": 1
                },
                {
                    "sent": "So for example, that could be Gibbs sampling operator, Metropolis Hastings operator, and deep sampling operator is something that you can do with Indy.",
                    "label": 0
                },
                {
                    "sent": "Both machines it's very easy to do, it's just you know sampling states of hidden variables given the other states of hidden variables.",
                    "label": 1
                },
                {
                    "sent": "So very simple procedure to implement and then you're going to be updating your model parameters by replacing the intractable expectation with a point estimate.",
                    "label": 0
                },
                {
                    "sent": "So you basically.",
                    "label": 0
                },
                {
                    "sent": "Getting a sample out of Markov chain?",
                    "label": 0
                },
                {
                    "sent": "You say that's your point.",
                    "label": 1
                },
                {
                    "sent": "Estimate of expectation, and in practice you simulating mark multiple Markov chains in parallel.",
                    "label": 0
                },
                {
                    "sent": "So just take the average and say that's your approximation.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's what's the learning algorithm?",
                    "label": 1
                },
                {
                    "sent": "What's the learning looks like.",
                    "label": 0
                },
                {
                    "sent": "Well, here's an update.",
                    "label": 0
                },
                {
                    "sent": "You take your new parameters are going to parameters.",
                    "label": 0
                },
                {
                    "sent": "Plus you know the difference between what the gradient is right?",
                    "label": 0
                },
                {
                    "sent": "It's expectation respect to the data minus what you get out of your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Samples that you get out of Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Now here's the trick of to analyze convergence of these algorithms is well.",
                    "label": 0
                },
                {
                    "sent": "If you add and subtract the true expectation here, this would be the true gradient.",
                    "label": 0
                },
                {
                    "sent": "Right, if we could compute the true gradient, they will be very easy for us to do right now, but unfortunately we can't.",
                    "label": 0
                },
                {
                    "sent": "So this is the true gradient, and this is something that you would call the perturbation term.",
                    "label": 1
                },
                {
                    "sent": "This is the difference between the true expectation versus what you get out of your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now it's a little bit tricky to see what's going on here, because your Markov chain is not really like this this term.",
                    "label": 0
                },
                {
                    "sent": "He is not really an unbiased estimate of this expectation, right, because it depends on the properties and convergence property.",
                    "label": 0
                },
                {
                    "sent": "If your MCMC algorithm so the analysis becomes a little bit tricky, but you can at least show that asymptotically you know you can get to the syntactically stable point, so at least you can guarantee that asymptotically you will reach some local.",
                    "label": 0
                },
                {
                    "sent": "Local optimum by following this this algorithm, but of course it relies on sort of a little bit more theoretical justification, which basically says the following.",
                    "label": 0
                },
                {
                    "sent": "It basically says well as your learning rate becomes sufficiently small compared to the mixing rate of the Markov chain.",
                    "label": 1
                },
                {
                    "sent": "Markov chain is going to stay close to the station distribution, in which case this will be this term.",
                    "label": 0
                },
                {
                    "sent": "Here is going to be the correct estimation of your expectation right?",
                    "label": 0
                },
                {
                    "sent": "Of course, in practice that really doesn't happen and high dimensional probability.",
                    "label": 0
                },
                {
                    "sent": "Escape is very very multi model, so you have to navigate around that space.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work where you can say, well, you can sort of play with the transition operators.",
                    "label": 1
                },
                {
                    "sent": "You can use tempera transitions.",
                    "label": 0
                },
                {
                    "sent": "You can use parallels.",
                    "label": 0
                },
                {
                    "sent": "Simulated temper is a whole bunch of different different ways of trying to improve it.",
                    "label": 0
                },
                {
                    "sent": "But that's sort of the best way for approximating.",
                    "label": 0
                },
                {
                    "sent": "Expectation respect.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Small distribution now what about expecting approximating the data driven expectation?",
                    "label": 0
                },
                {
                    "sent": "And this is way variational inference come comes in place.",
                    "label": 1
                },
                {
                    "sent": "You can use MCMC as well, but it turns out that variational inference is much more stable.",
                    "label": 0
                },
                {
                    "sent": "For making approximations here.",
                    "label": 0
                },
                {
                    "sent": "So what's the intuition behind variational inference?",
                    "label": 0
                },
                {
                    "sent": "So let me just say from the high level perspective, because we're going to see more and more of it through the rest of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "What's the intuition here?",
                    "label": 0
                },
                {
                    "sent": "Let's say you have an intractable distribution.",
                    "label": 1
                },
                {
                    "sent": "The probability of H given V and what you want to do is going to approximate it with a tractable distribution.",
                    "label": 0
                },
                {
                    "sent": "Q OK, think of Q, maybe is a fully factorized distribution or its one big gigantic Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Something that's easy, easy to compute.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the trick behind variational inference.",
                    "label": 0
                },
                {
                    "sent": "It's a beautiful trick, and it works surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "If you look at the low probability, you can say, well, it's the log of the joint, right?",
                    "label": 0
                },
                {
                    "sent": "Now what I'm going to do is I'm going to multiply and divide by my Q distribution.",
                    "label": 0
                },
                {
                    "sent": "There's some conditions you know where the coverage of Q has to be at least as it has to be covering all the states that piece covering so that you don't end up dividing by zero or something like that.",
                    "label": 0
                },
                {
                    "sent": "So if you conditions here, but let's say you multiply and divide by Q, right?",
                    "label": 0
                },
                {
                    "sent": "So what is going on here?",
                    "label": 0
                },
                {
                    "sent": "And then you can say, well, you can take the Q and you can push it.",
                    "label": 0
                },
                {
                    "sent": "Or you can take the log and push it inside here, which is effectively something that's called Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "Because here you saying well.",
                    "label": 0
                },
                {
                    "sent": "It's the log of the expectation is greater than expectation of the log, right?",
                    "label": 0
                },
                {
                    "sent": "And that's the whole trick.",
                    "label": 0
                },
                {
                    "sent": "Now why is this trick useful to us?",
                    "label": 0
                },
                {
                    "sent": "So if I push the log inside, why doesn't help me with Boltzmann machines, right?",
                    "label": 0
                },
                {
                    "sent": "If you do a little bit more work here, you end up with this expression here.",
                    "label": 0
                },
                {
                    "sent": "And this is the expression where the first term is something that's called expected complete data log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The second term is the log of the partition function and remember log of the partition function is something that gives us a lot of troubles.",
                    "label": 0
                },
                {
                    "sent": "'cause to compute it requires exponential sum or exponential integrals is something that we cannot do.",
                    "label": 0
                },
                {
                    "sent": "We cannot compute and this term is known as the entropy of the Q distribution, right?",
                    "label": 0
                },
                {
                    "sent": "Now, surprisingly, if I look at this term, here is just linear in the parameters right?",
                    "label": 0
                },
                {
                    "sent": "And this is known as a variational bound.",
                    "label": 0
                },
                {
                    "sent": "So what does this say?",
                    "label": 0
                },
                {
                    "sent": "Well, this is basically saying if I look at the variational bound and I'm trying to optimize with respect to Q distribution.",
                    "label": 0
                },
                {
                    "sent": "Notice that it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "This log of the partition function doesn't really affect me, so it's kind of interesting I have this conditional probability.",
                    "label": 0
                },
                {
                    "sent": "I have this probability that has this nasty partition function and I'm converting into the problem where I avoid.",
                    "label": 0
                },
                {
                    "sent": "I don't have to even.",
                    "label": 0
                },
                {
                    "sent": "Look at my partition function when I'm optimizing with respect to Q, right?",
                    "label": 0
                },
                {
                    "sent": "So, so now I can also write it in terms of log of the marginal minus the KL between the Q&P.",
                    "label": 0
                },
                {
                    "sent": "So this excuse my approximation, P is my truth right?",
                    "label": 0
                },
                {
                    "sent": "So the other way to view variational inference is you.",
                    "label": 0
                },
                {
                    "sent": "Basically I'm going to say that.",
                    "label": 0
                },
                {
                    "sent": "Well, how can I find my Q distribution such that it's as close as possible to my truth?",
                    "label": 0
                },
                {
                    "sent": "In terms of minimizing the KL divergent, yes.",
                    "label": 0
                },
                {
                    "sent": "Their purchasing function, as in show up anywhere.",
                    "label": 0
                },
                {
                    "sent": "The partition function doesn't show up anywhere, because if I'm optimizing this variational bound with respect to Q.",
                    "label": 0
                },
                {
                    "sent": "Only this term depends on Q and this term depends on Q.",
                    "label": 0
                },
                {
                    "sent": "With respect to Theta.",
                    "label": 0
                },
                {
                    "sent": "No, in this case.",
                    "label": 0
                },
                {
                    "sent": "In this case, when I'm optimizing with respect to Theta, I'm using MCMC, so that's a different piece.",
                    "label": 0
                },
                {
                    "sent": "So here all I'm trying to do is I'm trying to optimize with respect to Q. I'm trying to basically say find me approximation to my posterior and find the parameters of the Q distribution such that my approximation to the ground truth is as good as possible, right?",
                    "label": 0
                },
                {
                    "sent": "In which case it avoids computing the partition function, which is a very neat trick for the variational inference, right?",
                    "label": 0
                },
                {
                    "sent": "And again, the other way to think about it is that you basically trying to say find me the Q distribution such a way that it 6 closest possible to my true distribution P. But the problem with my new distribution piece that it has this nasty partition function that I cannot compute and magically with variational inference you can actually optimize this scale right?",
                    "label": 0
                },
                {
                    "sent": "Which is which is a surprising fact, so we'll see a lot of that through the rest of the tutorial, right?",
                    "label": 0
                },
                {
                    "sent": "So you try to minimize KL between approximating and the true distribution with respective variational parameters, mu.",
                    "label": 1
                },
                {
                    "sent": "With respect to the parameters of the Q distribution.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is variational bound.",
                    "label": 0
                },
                {
                    "sent": "So for Boltzmann machines for Diebold machines you can choose a fully factorized distribution.",
                    "label": 1
                },
                {
                    "sent": "That's one way of doing it.",
                    "label": 0
                },
                {
                    "sent": "You can do something a little bit more complex if you wish to, but if you doing a fully factorized approximation, then you effectively maximizing this bound with respect to the variational parameters.",
                    "label": 0
                },
                {
                    "sent": "And there's just turns out if you workout alot of math is just a sequence of nonlinear equations, so you cycle through these nonlinear equations and you get the best best approximation.",
                    "label": 0
                },
                {
                    "sent": "There are ways of speeding things up where you can build a recognition model that tries to approximate.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the mean field approximation.",
                    "label": 0
                },
                {
                    "sent": "The other thing about mean field approximation, which is true, and for most of the variational inference is that typically if it's a fully factorized distributions you will only capture one mode in the posterior, right?",
                    "label": 0
                },
                {
                    "sent": "So most of the time most of the complaints that you would get from using variational approximation is that it can only capture single mode in the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you true posterior distributions multi model, the algorithm will make sure that you only focusing on one month and we'll see some of that.",
                    "label": 0
                },
                {
                    "sent": "Later on.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you are effectively sort of for training these models.",
                    "label": 0
                },
                {
                    "sent": "It's almost like doing something similar to EM algorithm expectation maximization algorithm, which is sort of in a pseudo eastep using variational inference is to find approximation to the posterior in the M step, you're running your Markov chain Monte Carlo Stochastic approximation.",
                    "label": 1
                },
                {
                    "sent": "You get statistics from your model and then you matching the two expectations.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good point.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you doing variational inference.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, variational inference is sort of the bound goes the wrong way.",
                    "label": 0
                },
                {
                    "sent": "So you can only use variational inference to do data dependent expectations.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do the model, expect so if you're trying to approximate using mean field approximation for the log of the partition function, then the equations were the wrong way, so it's like optimizing.",
                    "label": 0
                },
                {
                    "sent": "It's like maximizing the upper bound right, which, which is not quite right.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work on trying to find upper bounds on the log of the partition function, like she waited, belief propagation, for example.",
                    "label": 0
                },
                {
                    "sent": "These models are very not very precise.",
                    "label": 0
                },
                {
                    "sent": "So specifically for deep Boltzmann machines it's impossible to use variational approximation like mean field approximation to approximate the logger partition function.",
                    "label": 0
                },
                {
                    "sent": "Yet get the learning algorithm, get the right learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a subtle point.",
                    "label": 0
                },
                {
                    "sent": "If you haven't seen these algorithms before, it's kind of hard to see why, but effectively it's impossible to do.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question is OK, there is a theory.",
                    "label": 0
                },
                {
                    "sent": "We can prove things about it if you're interested in MCMC, you can start playing with fancy transition operators and such, but the end of the day these models actually work.",
                    "label": 0
                },
                {
                    "sent": "How good are they?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to let you judge that you've probably seen this before because I've done it multiple times, but I'm going to show you I'm going to show you two panels or one panel.",
                    "label": 0
                },
                {
                    "sent": "You'll see the real data and we simulated data.",
                    "label": 0
                },
                {
                    "sent": "So you can think of that as fake data simulated from.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model and you have to tell me which one is which.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these are different handwritten characters coming from DIFF.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alphabets around the world.",
                    "label": 0
                },
                {
                    "sent": "How many of you third that?",
                    "label": 0
                },
                {
                    "sent": "This was simulated in this is real.",
                    "label": 0
                },
                {
                    "sent": "Raise your hands.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What about the other way around?",
                    "label": 0
                },
                {
                    "sent": "Still, that's that trick still works.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get the images.",
                    "label": 0
                },
                {
                    "sent": "Actually you will see that this is simulated and this is real.",
                    "label": 0
                },
                {
                    "sent": "Right, because you know, if you look at an image like this, it's not quite the pixels are not quite as crisp, and if you look at the diversity of possible images real there is a lot more diversity has to do with the fact that Markov chains inability to explore a very highly multimodal space.",
                    "label": 0
                },
                {
                    "sent": "Right and the other trick that I'm using is that I'm showing it to you very quickly so you know it's hard for you to see the difference right now.",
                    "label": 0
                },
                {
                    "sent": "If you stare it for some time, you will see the difference.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing with the handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "How many of you think this is?",
                    "label": 0
                },
                {
                    "sent": "This is simulated and this is real.",
                    "label": 0
                },
                {
                    "sent": "What about the other way around?",
                    "label": 0
                },
                {
                    "sent": "This is real and this is simulated.",
                    "label": 0
                },
                {
                    "sent": "A few are so few people worked on that list and I can promise you this is this is real and this is simulated and I can promise you I did not pick this particular 0 because a lot of people told me that that particular 0 is what messes the people up.",
                    "label": 0
                },
                {
                    "sent": "But that's real.",
                    "label": 0
                },
                {
                    "sent": "That's a real data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you can do object.",
                    "label": 0
                },
                {
                    "sent": "You can do recognition with these models and compare it to other models and permutation variant version of these models do quite well.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can also sort of build something a little bit more.",
                    "label": 0
                },
                {
                    "sent": "This is an older result where you can build.",
                    "label": 0
                },
                {
                    "sent": "Generative models of 3D objects.",
                    "label": 0
                },
                {
                    "sent": "So in this case obviously this is generated.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "One thing that I want to point out is that sometimes we generate these funny things.",
                    "label": 0
                },
                {
                    "sent": "Kind of looks like an airplane with no kind of looks like a car with wings, right?",
                    "label": 0
                },
                {
                    "sent": "Like if you look at that particular piece right?",
                    "label": 0
                },
                {
                    "sent": "This is when the model transitions from one class to another class, so it's able to simulate something something unusual.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can also do pattern completion.",
                    "label": 0
                },
                {
                    "sent": "It's another application areas where you know what you're seeing here is that if I show you half of the soldier, it sort of fills up the remaining half right.",
                    "label": 0
                },
                {
                    "sent": "Same thing with airplanes, and these are new objects at the desktop.",
                    "label": 0
                },
                {
                    "sent": "This is something that you don't see at the training time and this is what the real real data looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These kinds of models are useful for modeling hierarchies, right?",
                    "label": 0
                },
                {
                    "sent": "You can think about this.",
                    "label": 0
                },
                {
                    "sent": "Learning edges, combination of edges, and so forth, and a lot of different deep learning.",
                    "label": 1
                },
                {
                    "sent": "Deep generative models sort of follow that particular hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let me jump.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "The counterparts to these models.",
                    "label": 0
                },
                {
                    "sent": "One of the early models in that space was Helmholtz machine model back in 95 Hinton.",
                    "label": 1
                },
                {
                    "sent": "Diane, Brendan Frey and Radford Neal had a paper in science where they introduced this model.",
                    "label": 0
                },
                {
                    "sent": "If you look at the structure of this model, it's a directed graphical model, right?",
                    "label": 0
                },
                {
                    "sent": "You have probably.",
                    "label": 0
                },
                {
                    "sent": "City over the latent variables and then you generate through multiple layers of nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "You generate the data.",
                    "label": 1
                },
                {
                    "sent": "There is also an approximate inference network.",
                    "label": 0
                },
                {
                    "sent": "There is a Q distribution just like as we've seen with both machines, but in this case Q distribution is just does it in the feed forward way.",
                    "label": 0
                },
                {
                    "sent": "Given the data you try to approximate the states of the latent variables right?",
                    "label": 0
                },
                {
                    "sent": "So we have this generative model recognition model.",
                    "label": 0
                },
                {
                    "sent": "Suggest this before you have a P distribution.",
                    "label": 0
                },
                {
                    "sent": "You have a Q distribution, right?",
                    "label": 0
                },
                {
                    "sent": "You approximate distribution.",
                    "label": 0
                },
                {
                    "sent": "And in the last few years, there's been a lot of work looking at these models.",
                    "label": 0
                },
                {
                    "sent": "These models been around for almost 20 years, right?",
                    "label": 0
                },
                {
                    "sent": "But it was always very hard to train these models.",
                    "label": 0
                },
                {
                    "sent": "Part of it was because of computers were not there yet.",
                    "label": 0
                },
                {
                    "sent": "Was because the algorithms were not quite there as well.",
                    "label": 1
                },
                {
                    "sent": "In the last few years, there's been a lot of work in that space, and particularly the work of Kingman, valuing welling on the rational team code is the same, but the same time Rezende Shakira and done.",
                    "label": 0
                },
                {
                    "sent": "People are deep mind, basically had similar ideas at about the same time the work at Montreal.",
                    "label": 0
                },
                {
                    "sent": "They also had a related related wake sleep model that actually show that these models can be trained and also some of the work at Toronto.",
                    "label": 0
                },
                {
                    "sent": "Where we've looked at also train stochastic, feedforward feedforward networks.",
                    "label": 0
                },
                {
                    "sent": "So in the last sort of three or four years these models are picking up.",
                    "label": 0
                },
                {
                    "sent": "Why they're picking up?",
                    "label": 0
                },
                {
                    "sent": "Well, one big advantage of directed models is that you can simulate you can.",
                    "label": 0
                },
                {
                    "sent": "You can sample the data right in both machines.",
                    "label": 0
                },
                {
                    "sent": "You have to run Markov chain Monte Carlo to actually get samples.",
                    "label": 0
                },
                {
                    "sent": "In these models you can actually sample the data so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple process.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you know how do you optimize Q distribution?",
                    "label": 0
                },
                {
                    "sent": "How do you find the right recognition distribution?",
                    "label": 0
                },
                {
                    "sent": "So let me dive into that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in the space of of deep models, there's been sort of a whole bunch of family of models.",
                    "label": 0
                },
                {
                    "sent": "There is a Helmholtz machine was back in 95, then there was a deep belief network over here, and it's kind of has an interesting architecture, has an interesting structures in DBM, followed by sigmoid belief network, so it's a hybrid model and has undirected and directed graphical model sitting with it.",
                    "label": 1
                },
                {
                    "sent": "In it, and then there's a D bolsa machine where everything is undirected, right?",
                    "label": 0
                },
                {
                    "sent": "So you have these different classes of models and it's kind of useful to know about all of them an.",
                    "label": 0
                },
                {
                    "sent": "Advantages and disadvantages of these different models.",
                    "label": 0
                },
                {
                    "sent": "All of them have advantages and disadvantages.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to be focusing on the first one, the Helmholtz machine.",
                    "label": 0
                },
                {
                    "sent": "So let me show.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are you a motivating example?",
                    "label": 0
                },
                {
                    "sent": "Motivating example is that can we generate images from natural language descriptions whynott?",
                    "label": 1
                },
                {
                    "sent": "Can we build a model that can generate generate images?",
                    "label": 1
                },
                {
                    "sent": "So for example, if I give you a set, the stop sign is flying in blue skies.",
                    "label": 1
                },
                {
                    "sent": "That's what the model can generate.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, a yellow school buses flying in blue skies, so you get a yellow school bus.",
                    "label": 0
                },
                {
                    "sent": "Or this is very hard to generate precise school bus, although there are approaches.",
                    "label": 1
                },
                {
                    "sent": "It's been basically developed in the last half a year where people are getting better and better generative models.",
                    "label": 0
                },
                {
                    "sent": "It's a race we can generate.",
                    "label": 0
                },
                {
                    "sent": "Better models or hear elephants are flying in blue skies.",
                    "label": 0
                },
                {
                    "sent": "Commercial plane flying in disguise, right?",
                    "label": 0
                },
                {
                    "sent": "So you can basically and these kinds of models a lot of them are based on these directed models, so generative.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active model, so for example, here's one particular model where you have an input as a sentence.",
                    "label": 0
                },
                {
                    "sent": "There's something that's called bidirectional STM that you've had a tutorial on recurrent neural networks, and the thing that I want to point out here is that there is a generative model P that generates the data.",
                    "label": 0
                },
                {
                    "sent": "It's based on the work of DeepMind, Drone network of Gregor at all, and there is also recognition Model Q right that given the data tries to infer the distribution of a latent variables.",
                    "label": 0
                },
                {
                    "sent": "So at the high at the Birds Eye view, at the high level view you have this generative model.",
                    "label": 0
                },
                {
                    "sent": "We have this recognition model.",
                    "label": 0
                },
                {
                    "sent": "It's just the way you structure.",
                    "label": 0
                },
                {
                    "sent": "The model is a little bit different.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can, you know you can flip the colors.",
                    "label": 0
                },
                {
                    "sent": "You can say a school bus is parked in the parking lot versus a red school bus and look at those red school buses right?",
                    "label": 1
                },
                {
                    "sent": "At least it's trying to.",
                    "label": 0
                },
                {
                    "sent": "Green school bus.",
                    "label": 0
                },
                {
                    "sent": "It's trying to make buses be green and blue school bus, even though there's nothing like there's nothing like a red school bus or bull school bus right?",
                    "label": 0
                },
                {
                    "sent": "We actually try to check.",
                    "label": 0
                },
                {
                    "sent": "I don't think there are a lot of green buses or blue buses in the training set, but there are a lot of blue cars and green cars and stuff like that, right?",
                    "label": 0
                },
                {
                    "sent": "So it's sort of.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times to capture it.",
                    "label": 0
                },
                {
                    "sent": "You can also sort of do interesting things like you know a plane is flying in clear skies versus rainy Sky, so it has some notion of what the meaning of clear versus rainy.",
                    "label": 1
                },
                {
                    "sent": "So the only difference between these two captions is clear versus Rainier, right?",
                    "label": 0
                },
                {
                    "sent": "And it changes the background or elephants of working in dry grass field versus green grass field, right?",
                    "label": 1
                },
                {
                    "sent": "Obviously it's hard to see elephants, but at least it's trying to understand dry versus green.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can put the objects chocolate desert versus bowl of bananas.",
                    "label": 1
                },
                {
                    "sent": "I don't know if you see both bananas, but chocolate dessert.",
                    "label": 0
                },
                {
                    "sent": "Maybe you see something.",
                    "label": 0
                },
                {
                    "sent": "There is an example where it fails, you know, vintage photo of a cat or vintage photo of a dog.",
                    "label": 1
                },
                {
                    "sent": "I don't think you can tell the difference between the two.",
                    "label": 0
                },
                {
                    "sent": "This is where adversarial networks that hopefully will be covering as well.",
                    "label": 0
                },
                {
                    "sent": "Might help us 'cause you can't really see the difference between the two.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you can also compare it to other models.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of an older comparison.",
                    "label": 0
                },
                {
                    "sent": "You can compare it to lapghan.",
                    "label": 0
                },
                {
                    "sent": "You can compare to convolutional decommission variation on the encoders fully connected in quarters and so forth.",
                    "label": 0
                },
                {
                    "sent": "You know a lot of different models have their advantages and disadvantages.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing about.",
                    "label": 0
                },
                {
                    "sent": "Helmholtz machines in variational encoder.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about now is that you can actually evaluate them.",
                    "label": 0
                },
                {
                    "sent": "At least you can estimate what is the lower bound on the average test log probabilities, right?",
                    "label": 1
                },
                {
                    "sent": "The numbers themselves are probably very hard to make the meanings of, but at least you can see that you're not overfitting right.",
                    "label": 1
                },
                {
                    "sent": "At least you can see that what you have on the training set is similar.",
                    "label": 0
                },
                {
                    "sent": "What you have on the test set, so you know that these models are not heavily overfitted to the training.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "But you can do fun things like novel scene compositions, and this is the funniest piece about this work is that you can generate a toilet seat, sofas in the bathroom, or you can say toilet seat is open in the grass field.",
                    "label": 1
                },
                {
                    "sent": "Right, so you can see, maybe you can, maybe you can't, but sort of looks reasonable.",
                    "label": 0
                },
                {
                    "sent": "And when we published this paper, I like to make this joke is that my student Elman Muncie move who was working on this particular project?",
                    "label": 0
                },
                {
                    "sent": "Googled it so you can just you can just Google it or see softening the grass field and in a Google image search and this is what it comes up with.",
                    "label": 0
                },
                {
                    "sent": "Right so then you basically saying?",
                    "label": 0
                },
                {
                    "sent": "Well, Google already solved the problem.",
                    "label": 0
                },
                {
                    "sent": "So why do we need it?",
                    "label": 0
                },
                {
                    "sent": "But then, remarkably, after a few weeks, if you actually Google this particular caption, This image comes before this image, so we've convinced Google that is a much better representation than this because a lot of people are clicking on that image right?",
                    "label": 0
                },
                {
                    "sent": "And based on the click rate that reranked the images appropriately.",
                    "label": 0
                },
                {
                    "sent": "So which is which is great.",
                    "label": 0
                },
                {
                    "sent": "And then let me just.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skip skip on that, so the overall model.",
                    "label": 1
                },
                {
                    "sent": "You know you can think of it as a variational encoders or stitched in time, but I'm going to be focusing now is I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "I wanted to give you precise details of what these models are.",
                    "label": 0
                },
                {
                    "sent": "OK, so that we have a little bit better understanding of of this class of models.",
                    "label": 0
                },
                {
                    "sent": "And then again you can apply them a lot of different different application air.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, OK, so variational open order, yes?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generated images other than.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can ask humans to evaluate the images.",
                    "label": 0
                },
                {
                    "sent": "I mean image evaluation.",
                    "label": 0
                },
                {
                    "sent": "That's a very difficult.",
                    "label": 0
                },
                {
                    "sent": "Task in general because in Canada log probabilities, but if your distribution over the observed data which is in our case is a Galaxy is not appropriate, is not quite what people believe.",
                    "label": 0
                },
                {
                    "sent": "That is, the right perception is then it might not be the right metric, right?",
                    "label": 0
                },
                {
                    "sent": "So yes, you can ask humans to humans to evaluate, but there's no, there's no really good solution because it becomes very expensive to do it so.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work basically in the space of trying to figure out how do we evaluate the quality.",
                    "label": 0
                },
                {
                    "sent": "The generative models of images, right?",
                    "label": 0
                },
                {
                    "sent": "The fact that you can generate pretty looking images doesn't really mean much, because I can have a really dumb generative model that picks a training example and shows it to you and you will not see the difference.",
                    "label": 0
                },
                {
                    "sent": "So this is amazing generative model, but it's useless.",
                    "label": 0
                },
                {
                    "sent": "So that's still in very active area of research.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me get into the variational thinkers.",
                    "label": 0
                },
                {
                    "sent": "The variational encoders they define a generative process by stitching together conditional probabilities, right?",
                    "label": 1
                },
                {
                    "sent": "So here if I say, what's the probability of the input, the probability of the input is given by defining this joint probability, which is the prior.",
                    "label": 0
                },
                {
                    "sent": "Times bunch of conditionals and then you sum out the states of all the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as as just directed graphical model, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a generative process, you write down the conditionals, you can sum out all the latent variables, hidden variables from the model.",
                    "label": 1
                },
                {
                    "sent": "So Theta here is going to be noting the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "L is going to be the number of stochastic layers.",
                    "label": 1
                },
                {
                    "sent": "And key assumptions that we're going to be making is that the sampling from these conditional distributions can be done easily, and these conditional probabilities can be evaluated so each conditional probability should be tractable, and we should be able to evaluate it, right?",
                    "label": 0
                },
                {
                    "sent": "All of these conditionals.",
                    "label": 1
                },
                {
                    "sent": "Now each term here can have a complicated nonlinear relationship.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not that these conditions are just bunch of Gaussians, right?",
                    "label": 0
                },
                {
                    "sent": "They can have nonlinear structure.",
                    "label": 0
                },
                {
                    "sent": "So here's here's an example.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is 1 example where you have a stochastic layer.",
                    "label": 1
                },
                {
                    "sent": "You have a deterministic layer, you have another stochastically.",
                    "label": 1
                },
                {
                    "sent": "Have the data.",
                    "label": 0
                },
                {
                    "sent": "So this term here the conditional probability of H1 given H2.",
                    "label": 0
                },
                {
                    "sent": "Right, this conditional probability is effectively one layer neural network OK?",
                    "label": 0
                },
                {
                    "sent": "So it can have nonlinear structure sitting in it.",
                    "label": 0
                },
                {
                    "sent": "It's not just bunch of Gaussian layers stitched together, in which case one model is 1 big gigantic Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Right, you have nonlinearities sitting in your model, but they just deterministic nonlinearities.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in addition to the generative network, you're also going to have a recognition network.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's going to be a bottom up recognition network, so in this case the joint distribution over all your hidden variables is just going to be again factorized.",
                    "label": 0
                },
                {
                    "sent": "This way, just bunch of conditionals stitch together in this way.",
                    "label": 0
                },
                {
                    "sent": "And again, each term here can denote a complicated nonlinear relationship, right?",
                    "label": 1
                },
                {
                    "sent": "So your recognition model can have stochastic layers and has deterministic layers, so you can you know your recognition model can be quite powerful.",
                    "label": 0
                },
                {
                    "sent": "This is the most general way of defining it.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to be making assumption that the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Over your latent variable is going to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK. And the conditionals are also going to be Gaussians with diagonal covariance is you can make it a bit more expressive, but for the sake of this presentation is going to be Gaussian with Daniel Covariances.",
                    "label": 1
                },
                {
                    "sent": "OK, now again, I want to emphasize that doesn't mean that this entire model is just one big gigantic Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "Because these conditionals can have non linearity in them, right?",
                    "label": 0
                },
                {
                    "sent": "So you mean of your distribution can have a nonlinear relationship to the states of the layer above right?",
                    "label": 0
                },
                {
                    "sent": "And get you have a nonlinear relationship for the mean as well As for the variances.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can pretty much model any kinds of distributions that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you want.",
                    "label": 0
                },
                {
                    "sent": "Now the variational autoencoder is again by the name.",
                    "label": 0
                },
                {
                    "sent": "They are trained to maximize the variational bound, and as we've seen before, what is the variational bound?",
                    "label": 1
                },
                {
                    "sent": "It's the log of the expectation is greater than expectation of the log, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to be looking at this bound here, where Q is our approximation.",
                    "label": 0
                },
                {
                    "sent": "It will have its own parameters.",
                    "label": 0
                },
                {
                    "sent": "We have to find what those parameters are.",
                    "label": 0
                },
                {
                    "sent": "The generative model has its own parameters.",
                    "label": 0
                },
                {
                    "sent": "We're going to find out what the parameters of the generative models are.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "And again, to emphasize as before, the actual marginal log probabilities given by the lower bound is given by the.",
                    "label": 0
                },
                {
                    "sent": "Actual probability minus this KL divergences, right?",
                    "label": 0
                },
                {
                    "sent": "So you effectively when you optimizing these algorithms using variational arguments is you effectively trading off the data log likelihood and the KL divergent from the true prior right?",
                    "label": 1
                },
                {
                    "sent": "So you sort of have these kind of two competing objectives.",
                    "label": 0
                },
                {
                    "sent": "You trying to make the likelihood be better at the same time you're trying to make sure that whatever your approximation is, it's actually good approximation.",
                    "label": 0
                },
                {
                    "sent": "Right 'cause you're trying to basically push this term up.",
                    "label": 0
                },
                {
                    "sent": "Right, you cannot have.",
                    "label": 0
                },
                {
                    "sent": "You can't put like.",
                    "label": 0
                },
                {
                    "sent": "You can't push the log likelihood if it means that your approximation to the true posterior is really bad, because the sum of the two is going to be.",
                    "label": 1
                },
                {
                    "sent": "Small, so you sort of have this interesting tradeoff between between the two objectives.",
                    "label": 0
                },
                {
                    "sent": "OK, now one of the things about optimizing deviation bars is that it's very hard to optimize variation bound with respect to the recognition network.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, if you had the recognition network, you could just sample from Q and optimize with respect to your generative network P, right?",
                    "label": 0
                },
                {
                    "sent": "But the problem is that how do you optimize for Q?",
                    "label": 1
                },
                {
                    "sent": "That's a tough question, and the key idea of Kingman Welling was to use repatriation trick.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, there were a few other papers about the same time.",
                    "label": 0
                },
                {
                    "sent": "There's a work coming from deep mind from Shakira.",
                    "label": 0
                },
                {
                    "sent": "Mohammed's group was going to be talking next on the same relationship, so it came up and about about the same time.",
                    "label": 0
                },
                {
                    "sent": "So let me explain.",
                    "label": 0
                },
                {
                    "sent": "What the trick does?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a very simple trick and let me explain it for the case of the Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have a recognition distribution which is Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So let's look at this recognition.",
                    "label": 0
                },
                {
                    "sent": "Distribution is a Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "The mean depends on the states of the hidden variables in a non linear fashion, right?",
                    "label": 0
                },
                {
                    "sent": "So it depends on HL minus one.",
                    "label": 0
                },
                {
                    "sent": "There is some nonlinear relationship and also the variance depends on HL minus one.",
                    "label": 0
                },
                {
                    "sent": "Right, so we have this form.",
                    "label": 1
                },
                {
                    "sent": "Right now, Alternatively, you can express this term here in terms of auxiliary variables.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can say well sample epsilon at layer L from normal 01 right?",
                    "label": 0
                },
                {
                    "sent": "And just write this H term is a function of this sample epsilon, the depending on HL minus one the layer above.",
                    "label": 0
                },
                {
                    "sent": "In this form.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The distribution of HL if you marginalized epsilons exactly going to this right.",
                    "label": 0
                },
                {
                    "sent": "So notice that if I give you.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Epsilon this H here becomes deterministic function, right?",
                    "label": 0
                },
                {
                    "sent": "There's no stick assist here, everything is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Right, So what it does is basically says, well we sample these epsilons with just acoustic and then everything else becomes deterministic, right?",
                    "label": 0
                },
                {
                    "sent": "Once I give you epsilon everything is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Right, so now what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that the recognition distribution can be expressed in terms of deterministic mapping.",
                    "label": 1
                },
                {
                    "sent": "Right, if I give you epsilons at every single stochastic layer, or if it's a one layer, just one epsilon.",
                    "label": 0
                },
                {
                    "sent": "If it's multiple stochastic, lays multiple epsilons per layer for each layer, then this H is going to be deterministic.",
                    "label": 0
                },
                {
                    "sent": "Why is this useful for us?",
                    "label": 1
                },
                {
                    "sent": "So you basically have a deterministic encoder.",
                    "label": 0
                },
                {
                    "sent": "Why is it interesting?",
                    "label": 0
                },
                {
                    "sent": "Important for us is that notice that this epsilon the distribution of epsilon does not depend on parameters of the model, and that's the key.",
                    "label": 0
                },
                {
                    "sent": "Right, absolutely just normal zero identity doesn't really depend on the parameters of recognition on generative model.",
                    "label": 0
                },
                {
                    "sent": "So how can we?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make use of this fact.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the gradients with respect to both recognition and generative model, right?",
                    "label": 1
                },
                {
                    "sent": "So this is the gradient with respect to.",
                    "label": 0
                },
                {
                    "sent": "With respect to objective, the variational objective right now what I can do is I can say, well it's the gradient, but I can do the trick.",
                    "label": 0
                },
                {
                    "sent": "I can say sample these epsilons from normal zero identity.",
                    "label": 0
                },
                {
                    "sent": "Nothing has changed and now I'm basically saying my H Now depends on the epsilon right?",
                    "label": 0
                },
                {
                    "sent": "So this is the same as this, nothing has changed, I'm just doing simple repolarization.",
                    "label": 0
                },
                {
                    "sent": "But why is it important for us?",
                    "label": 0
                },
                {
                    "sent": "Well it's important for us because this expectation does not really depend on Theta.",
                    "label": 0
                },
                {
                    "sent": "So what I can do is I can push this derivative.",
                    "label": 0
                },
                {
                    "sent": "Insight.",
                    "label": 0
                },
                {
                    "sent": "Of expectation, right?",
                    "label": 0
                },
                {
                    "sent": "So this has an interesting interpretation.",
                    "label": 0
                },
                {
                    "sent": "This basically saying look change my parameters by taking expectation.",
                    "label": 0
                },
                {
                    "sent": "Take the average and then move the average.",
                    "label": 0
                },
                {
                    "sent": "This has slightly different expect different interpretations.",
                    "label": 0
                },
                {
                    "sent": "Basically says take the gradient and then take an expectation it's like like expectation of the gradients versus gradient of expectation right, which turns out to be very useful property because it reduces the variance of your estimates quite substantially.",
                    "label": 0
                },
                {
                    "sent": "The other thing to note is that this mapping here is deterministic, so this entire whole thing here is just one deterministic autoencoder.",
                    "label": 0
                },
                {
                    "sent": "Right, which given epsilon's the whole thing becomes deterministic, so we can use backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can actually compute the gradients precisely, so it's almost like back propagating through the states of the latent variables.",
                    "label": 0
                },
                {
                    "sent": "So ingredients can be computed by back problems.",
                    "label": 1
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So now if you computing the gradients you what you basically doing is drawing case samples from epsilon.",
                    "label": 1
                },
                {
                    "sent": "You take the average.",
                    "label": 0
                },
                {
                    "sent": "Right way W he is defined by the joint divided by Q.",
                    "label": 0
                },
                {
                    "sent": "This is something that sometimes it's called importance weight.",
                    "label": 0
                },
                {
                    "sent": "Right, because it's basically sort of looks at how important your generative versus the recognition model is.",
                    "label": 0
                },
                {
                    "sent": "But this is just a Monte Carlo approximation.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's the whole trick in practice.",
                    "label": 0
                },
                {
                    "sent": "For the sake of computation, most people just use one sample.",
                    "label": 0
                },
                {
                    "sent": "So you sample from epsilon you sample from epsilon, which is normal 01, and then you just back propagate through the auto encoder, right?",
                    "label": 0
                },
                {
                    "sent": "And so the variance of the variational auto encoders is quite low because it uses the log likelihood gradients with respect to the latent variables, right?",
                    "label": 1
                },
                {
                    "sent": "Which is important.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the key trick.",
                    "label": 0
                },
                {
                    "sent": "Anne, it's being applied a lot in a lot of different.",
                    "label": 0
                },
                {
                    "sent": "Like for example, the images that I've shown you generating images actually using variational autoencoders, and you'll see more and more.",
                    "label": 0
                },
                {
                    "sent": "It's actually surprising 'cause one of the beauty of this algorithm is that it's very easy to get to work.",
                    "label": 0
                },
                {
                    "sent": "No, it's it's you see a lot of people basically using it and it's it works surprisingly well.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's look at the variation assumptions right.",
                    "label": 0
                },
                {
                    "sent": "Deviation assumptions must be approximately satisfied.",
                    "label": 1
                },
                {
                    "sent": "So what is that?",
                    "label": 0
                },
                {
                    "sent": "What these assumptions?",
                    "label": 0
                },
                {
                    "sent": "Well, the posterior distribution must be approximately factorial, which is a common practice right?",
                    "label": 1
                },
                {
                    "sent": "Whenever you queue distribution, it's typically factorized distribution, and it has to be predictable from a feedforward neural network, right?",
                    "label": 0
                },
                {
                    "sent": "And what we can do is I'm going to show you one other trick where we can relax these assumptions by using a title lower bound on the marginal log likelihood.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's going to get a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is 1 trick which basically allows you to get much tighter variational lower bounds.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trick, the trick is very simple.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're going to be using the following what we call case sample importance.",
                    "label": 1
                },
                {
                    "sent": "Weighting off the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this expression here.",
                    "label": 0
                },
                {
                    "sent": "What is this expression here?",
                    "label": 0
                },
                {
                    "sent": "Instead of sampling 1 hidden state, you're going to be sampling tastet, so you're going to be running your feet forward recognition model K times.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can even join K samples and then you're going to be taking the average.",
                    "label": 0
                },
                {
                    "sent": "Over those K samples, that's all in variational encoders is typically 1 sample.",
                    "label": 1
                },
                {
                    "sent": "Write in variational inference in general is just a single feedforward path in importance weighted encoders.",
                    "label": 0
                },
                {
                    "sent": "It's multiple passes, and then you average right?",
                    "label": 0
                },
                {
                    "sent": "So what is this by you?",
                    "label": 1
                },
                {
                    "sent": "Or we can also say it's the log of the average of the importance weights, right?",
                    "label": 1
                },
                {
                    "sent": "So it's just taking the average obviously small compute 'cause you have to draw K samples instead of 1 sample.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have to take the average in these H1 up to each case.",
                    "label": 0
                },
                {
                    "sent": "Sample from the recognition network so it's more computationally intensive.",
                    "label": 0
                },
                {
                    "sent": "And these are sometimes called a normalized importance weights and I still should point out there's going to be a great work coming from deep mind and basically looking at extension of these case sample bounds.",
                    "label": 0
                },
                {
                    "sent": "444 for the recognition network, particularly when you're looking at discrete variables, you're dealing with models with discrete variables.",
                    "label": 0
                },
                {
                    "sent": "Everything that I'm talking with variational to encourage the using recommendation trick, and typically most of them are assuming that you have Gaussian weighting variables.",
                    "label": 0
                },
                {
                    "sent": "If you're working with discrete latent variables, then it's the realization trick doesn't really work there, so there's some ways of improving these systems, and that was done by Jimmy and his editor at Deep Mind if you're interested.",
                    "label": 1
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at this bound.",
                    "label": 0
                },
                {
                    "sent": "It's the sample average.",
                    "label": 0
                },
                {
                    "sent": "Also, it's the average what you can do is you can actually show that this is a lower bound on the marginal probabilities.",
                    "label": 1
                },
                {
                    "sent": "We can basically say it is a lower bound, which is good because we maximizing their lower bound, right?",
                    "label": 0
                },
                {
                    "sent": "You have your likelihood log likelihood you have a lower bound you pushing on the lower bound right?",
                    "label": 0
                },
                {
                    "sent": "Instead of justice makes use of a very simple variational, very simple against inequality and special case when K is one, we just join 1 sample, then it becomes a variational objective fashion.",
                    "label": 1
                },
                {
                    "sent": "Objective 1 interesting thing about this bound is that using more samples you can improve the tightness of the bound.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can show is that.",
                    "label": 0
                },
                {
                    "sent": "You know, as you get more samples, your bound becomes tighter and if you get infinite number of samples you actually recover the marginal probability.",
                    "label": 1
                },
                {
                    "sent": "Alright, so maybe just to highlight a little bit.",
                    "label": 0
                },
                {
                    "sent": "Instead of in machine learning community, the statistics community that people who like doing variational inference and people who like doing much Markov chain Monte Carlo based inference right?",
                    "label": 0
                },
                {
                    "sent": "Because whenever you doing variational inference you always making assumptions on your Q distribution.",
                    "label": 0
                },
                {
                    "sent": "Your recognition model, right?",
                    "label": 0
                },
                {
                    "sent": "And it will never get.",
                    "label": 0
                },
                {
                    "sent": "You'll never get it quite right.",
                    "label": 0
                },
                {
                    "sent": "Right is also going to be an approximation, but it's faster to optimize right?",
                    "label": 0
                },
                {
                    "sent": "And then Markov chain Monte Carlo and you can show that asymptotically you'll get the right thing.",
                    "label": 0
                },
                {
                    "sent": "That's why a lot of statisticians like MCMC, because they close.",
                    "label": 0
                },
                {
                    "sent": "Can proof that I simplistically I will get the truth.",
                    "label": 0
                },
                {
                    "sent": "I will get the true posterior probability if I run it for long enough.",
                    "label": 0
                },
                {
                    "sent": "Of course, in practice nobody running for long enough, so still approximation anyways, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of an argument that bridge is between the two.",
                    "label": 0
                },
                {
                    "sent": "Areas which basically say look if it's computationally intensive, you only use one sample.",
                    "label": 0
                },
                {
                    "sent": "It's a lower bound if you can compute if you can spend more time computing then you can effectively get a much tighter lower bound right?",
                    "label": 1
                },
                {
                    "sent": "So at least you can get closer to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Right, so how do you compute the?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Audience you can use exactly the same trick as a representation trick.",
                    "label": 1
                },
                {
                    "sent": "You taking these expectations, you know reproduction.",
                    "label": 0
                },
                {
                    "sent": "I'm in terms of epsilons, right?",
                    "label": 0
                },
                {
                    "sent": "You pushing the gradients inside, and if you do a little bit more work here, it just turns out to be.",
                    "label": 1
                },
                {
                    "sent": "You know this is the gradient where these W's adjust importance weights.",
                    "label": 0
                },
                {
                    "sent": "And these are normalized importance weights.",
                    "label": 1
                },
                {
                    "sent": "Which are quite easy to compute, right?",
                    "label": 0
                },
                {
                    "sent": "So now instead of going.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through the equations you can actually get the Monte Carlo estimates right, just as in variational encoder you get the Monte Carlo estimates.",
                    "label": 0
                },
                {
                    "sent": "What's the difference between the two?",
                    "label": 0
                },
                {
                    "sent": "Right here the two equations for importance weight on highways that we call them highways versus vasc.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this expression, they're exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So in both cases, when you computing the gradients that exactly the same, it's just that in one case we using importance weights in another case is just the average.",
                    "label": 0
                },
                {
                    "sent": "Right, why is important weights important here?",
                    "label": 0
                },
                {
                    "sent": "Well, what they effectively doing is they basically saying well if you drawing 10 samples.",
                    "label": 0
                },
                {
                    "sent": "Effectively intuition here.",
                    "label": 0
                },
                {
                    "sent": "If you join 10 samples from recognition model for every one of those samples, you can look at what's the probability that each sample could generate the data and if one of them hit the appropriate mode is more probable than its weight is going to be higher.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like you selecting better samples.",
                    "label": 0
                },
                {
                    "sent": "It's a way of selecting better samples.",
                    "label": 0
                },
                {
                    "sent": "You join multiple things.",
                    "label": 0
                },
                {
                    "sent": "And then you selecting better ones as opposed to just averaging so that sort of intensive computations.",
                    "label": 0
                },
                {
                    "sent": "They're pretty much the same.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now, what's the intuition behind variational encoders and importance weighted autoencoders?",
                    "label": 0
                },
                {
                    "sent": "Here's here's the intuition.",
                    "label": 0
                },
                {
                    "sent": "Gradient you have the deterministic encoder just like in other encoders and you have a deterministic decoder, right?",
                    "label": 1
                },
                {
                    "sent": "So it just becomes an alter.",
                    "label": 0
                },
                {
                    "sent": "The first piece here of the gradient that you're computing is just an auto encoder, right?",
                    "label": 0
                },
                {
                    "sent": "Given the data you encode and you decode.",
                    "label": 0
                },
                {
                    "sent": "The decoder is basically trying to encourage the generative model to assign high probability to every single conditional.",
                    "label": 1
                },
                {
                    "sent": "Right, that's the job of the decoder, and they encode is basically trying to make the parameters of the encoding network to just the latent spit states, so that the generative model would give high probabilities to those states is exactly what the auto encoder is doing.",
                    "label": 0
                },
                {
                    "sent": "Just encode and decode and you're trying to match match match the two.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there is a second term which is an important term.",
                    "label": 0
                },
                {
                    "sent": "It's kind of interesting term.",
                    "label": 0
                },
                {
                    "sent": "The second term is basically saying make sure that the recognition network has a spread out distribution of the predictions.",
                    "label": 1
                },
                {
                    "sent": "Never make you incredibly precise.",
                    "label": 1
                },
                {
                    "sent": "Right, so these are two terms, 'cause if it's precise it will collapse and then you become deterministic system, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing as in variational inference where you have this entropy term on the Q, which effectively says make sure that you Q distribution is spread out.",
                    "label": 0
                },
                {
                    "sent": "So the first term is trying to reconstruct the second term is trying to spread things out right?",
                    "label": 0
                },
                {
                    "sent": "That's the intuition behind behind these models in terms of compute.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the dominant cost.",
                    "label": 0
                },
                {
                    "sent": "You have a forward pass, have a backward pass and in practice you know you can compute these things in parallel.",
                    "label": 1
                },
                {
                    "sent": "You can also do a single.",
                    "label": 1
                },
                {
                    "sent": "To compute these importance weights, you only need a feedforward path, which is typically easy to do.",
                    "label": 1
                },
                {
                    "sent": "So do multiple importance, multiple forward passes and then you can just.",
                    "label": 0
                },
                {
                    "sent": "Basically you know approximate the sum by sampling according to W. So there's a ways of speeding things up for these models.",
                    "label": 0
                },
                {
                    "sent": "They are more expensive than just using the rational thinkers with single sample.",
                    "label": 0
                },
                {
                    "sent": "Obvious if you use multiple samples it's more expensive, so as I mentioned before, there is no free lunch.",
                    "label": 0
                },
                {
                    "sent": "If you have a better algorithm, typically for require more compute or more more work.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are kind of like a couple of architectures that that you can try.",
                    "label": 1
                },
                {
                    "sent": "You know you can have.",
                    "label": 0
                },
                {
                    "sent": "Typically you have a single stochastic layer and you have deterministic layers, or you can have multiple stochastic layers and you can have multiple deterministic layers.",
                    "label": 1
                },
                {
                    "sent": "We can sort of build these different different hierarchies.",
                    "label": 0
                },
                {
                    "sent": "One thing.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to point out which is an important thing is that if your recognition network has a stochastic layer right?",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a recognition model.",
                    "label": 0
                },
                {
                    "sent": "It sort of goes through nonlinearities and then is stochastic layer.",
                    "label": 0
                },
                {
                    "sent": "Here is a Gaussian layer.",
                    "label": 0
                },
                {
                    "sent": "We have another non linear inches and inferring the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you can say that the proximate marginal posterior distribution can be multi model, right?",
                    "label": 0
                },
                {
                    "sent": "Because when you marginalizing out over H1, this distribution can have multiple modes.",
                    "label": 0
                },
                {
                    "sent": "So in principle, these systems can handle multimodal posteriors.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "In theory they can, as opposed to you know, whenever you do variational inference when you assuming your distribution is fully factorized, in which case it can only handle single mode.",
                    "label": 0
                },
                {
                    "sent": "So these systems can be much more flexible, and they can handle multi modality.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "In terms of in terms of results, you know these are sort of a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each result will show you real results.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the later part, but you know, these systems do quite well, right and.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know one of the things that we've tried doing is that if you're using variational think orders to train the system using variational think orders to get some negative log likelihood, and then if you proceed training using importance on multiple samples, I ways you can get much better likelihood versus if you try training with highways and then followed with autoencoders, you get worse numbers.",
                    "label": 0
                },
                {
                    "sent": "So somehow drawing multiple samples do give us gain, right?",
                    "label": 0
                },
                {
                    "sent": "It can carve your posterior distributions can be quite quite flexible in the results.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, it's sorry I skipped through that.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I apologize, yeah, so this is this is the case where you have a 222 N network.",
                    "label": 0
                },
                {
                    "sent": "You know if you're using VS with 50 samples, just the average you get this versus if you're using importance sampling, you get these numbers, so it's sort of again these samples they carve out that you sort of hit if you hit something with high probability, then the weight goes up.",
                    "label": 0
                },
                {
                    "sent": "If you had something with low probability that goes out so you can think about is adjusting the importance of every single sample, which seems to be quite quite important.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, so the last slide and you had the architecture with the number of deterministic in myself.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the architecture is there.",
                    "label": 0
                },
                {
                    "sent": "Is there any particular way that you choose or can decide how many terministic layers you have before each stochastic sampling?",
                    "label": 0
                },
                {
                    "sent": "Or is it just kind of heuristic empirically?",
                    "label": 0
                },
                {
                    "sent": "So yeah, this is a tough question, so I can answer maybe just very quickly.",
                    "label": 0
                },
                {
                    "sent": "My thinking is in principle if you have a single stochastic layer, would say it's a Gaussian or uniform.",
                    "label": 0
                },
                {
                    "sent": "If you have multiple nonlinearities when you generate the data, you can pretty much model any distribution 'cause you can take a Gaussian distribution and more fit through nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "And get whatever you want effectively, right?",
                    "label": 0
                },
                {
                    "sent": "So in principle, from modeling perspective you only need one stochastic layer, right?",
                    "label": 0
                },
                {
                    "sent": "From optimization perspective, it's not clear we found that whenever you have a single stochastic layer, optimization becomes easier for us just because we can start matching things up here.",
                    "label": 0
                },
                {
                    "sent": "Right, so you because you know when within auto encoder you sort of trying to match what the recognition versus what the generative model is saying as opposed to going through multiple layers and just matching at the top.",
                    "label": 0
                },
                {
                    "sent": "So from modeling perspective a single stochastic layer is enough.",
                    "label": 0
                },
                {
                    "sent": "From optimization perspective, you might need some stochastic layers.",
                    "label": 0
                },
                {
                    "sent": "Terministic yeah, same thing is for the number determines.",
                    "label": 0
                },
                {
                    "sent": "There's no clear there's no clear answer there, yeah.",
                    "label": 0
                },
                {
                    "sent": "Let's actually do you want to say something about the Russian, like for example, if you.",
                    "label": 0
                },
                {
                    "sent": "This is one of the big features of these style of models compared to something like a people to machine where having these deterministic nonlinear layers really complicates things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "So it's like in models like both machines having deterministic layers can complicate things because very hard to define the energy functions in the right way.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's actually one of the would be interesting research areas to figure out how you can do undirected models, but how can you sort of have a mixed mix here?",
                    "label": 0
                },
                {
                    "sent": "That's true, that's one of the highlights of Helmholtz machines in variational encoders that guarantees deterministic layers, and you can use backpropagation, right?",
                    "label": 0
                },
                {
                    "sent": "So it's an auto encoder piece, plus the entropy switch.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, the principle you can define an energy function that that has.",
                    "label": 0
                },
                {
                    "sent": "Normally done in your domestic computation for each energy term, for example, linking related layers to some money or energy time.",
                    "label": 0
                },
                {
                    "sent": "Of course, you want to be able to solve exactly to one layer using the other layers, as in Gibbs, but you could do.",
                    "label": 0
                },
                {
                    "sent": "Other forms of inference.",
                    "label": 0
                },
                {
                    "sent": "You can you can sort of like do like sort of like contrasting backdrop type things, but it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe maybe yeah, it just becomes it's.",
                    "label": 0
                },
                {
                    "sent": "Anytime you have undirected connections and you have nonlinear potentials, if the parameters of these nonlinear potentials depend non linearly on the states, then it becomes it's no longer log.",
                    "label": 0
                },
                {
                    "sent": "Linear model becomes a little bit.",
                    "label": 0
                },
                {
                    "sent": "Sampling becomes might be an issue.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Yeah I don't think it's being fried.",
                    "label": 0
                },
                {
                    "sent": "Yep, that's right.",
                    "label": 0
                },
                {
                    "sent": "There are no papers in this space, yes.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "But that's an interesting.",
                    "label": 0
                },
                {
                    "sent": "That would be an interesting thing to try doing to incorporate deterministic systems, enable some machines and see what can be done there absolutely.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so one other piece.",
                    "label": 0
                },
                {
                    "sent": "One quick piece that I wanted to point out and I wanted to point out these hard attention models because there's been a lot of work on trying to build attention mechanism and you've probably seen some attention mechanisms discussed before in the summer school and also sort of highlight that they become a special cases of Helmholtz machines be.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, in the same framework.",
                    "label": 0
                },
                {
                    "sent": "So this is a caption generation system that was built by Jamie Kearse at Toronto back in 2014, actually and then 2015.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where you have an image and you have some neural language model predicting language model recurrent network that generates captions right and sort of.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As reasonably well, in our case, the encoder was a CNN and then you have a recurrent net for join for embedding in your language model.",
                    "label": 0
                },
                {
                    "sent": "For decoding the sentences here.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to show you so that so that you guys can relax a little bit from all the equations that I've been showing to you.",
                    "label": 0
                },
                {
                    "sent": "This is some of the fun things that the model can generate, right?",
                    "label": 0
                },
                {
                    "sent": "Like for this image generates the two birds are trying to be seen in the water.",
                    "label": 0
                },
                {
                    "sent": "Right, which is something that a human would never generate, so at least we can generate.",
                    "label": 0
                },
                {
                    "sent": "We can see that the model can generalize.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just can't tell that once every month.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean the question is can account I actually I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't want to claim that it can count.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "In this case account.",
                    "label": 0
                },
                {
                    "sent": "This is also The funny thing.",
                    "label": 0
                },
                {
                    "sent": "It's like the handlebars are trying to ride a bike rack right so?",
                    "label": 0
                },
                {
                    "sent": "There are sort of like things that it can generate, which is which is, which is nonsense.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's also been.",
                    "label": 0
                },
                {
                    "sent": "This is in collaboration with Montreal, where this is a system that uses visual attention.",
                    "label": 0
                },
                {
                    "sent": "Right now, speaking up in the computer vision field where if you're generating is captured, a man riding a horse in a field, then whenever you generating a manic focus on the man riding a horse, it sort of focus in the horses head and the feeling focuses on.",
                    "label": 1
                },
                {
                    "sent": "Right, so it's kind of like trying to solve both problems and it's.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the visual attention algorithm.",
                    "label": 0
                },
                {
                    "sent": "Motivations for visual attention is that imagine that you're trying to process videos.",
                    "label": 0
                },
                {
                    "sent": "Instead of processing every single frame you would like to process only small piece of each frame, right?",
                    "label": 1
                },
                {
                    "sent": "And it also adds a little bit of degree of interpretability, like where the model is looking at when it's generating certain things.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's just some examples that I want to show.",
                    "label": 0
                },
                {
                    "sent": "You know, for videos this is using something it's called soft attention, but it sort of focuses on on different on different pieces when you process.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Videos, right?",
                    "label": 0
                },
                {
                    "sent": "So there's been a.",
                    "label": 0
                },
                {
                    "sent": "A set of research that's taking place in sort of called recurrent attention models, maybe just taking quickly show you what they are and making an action to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "You have seen some amazing talks on reinforcement learning yesterday.",
                    "label": 0
                },
                {
                    "sent": "I believe it was reinforcement learning day, so imagine you have an image right?",
                    "label": 0
                },
                {
                    "sent": "And then you sample in action action here could be the location.",
                    "label": 0
                },
                {
                    "sent": "Of where you should be looking next.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a system that you just have these multiple glimpses.",
                    "label": 0
                },
                {
                    "sent": "You trying to figure out where you should be looking next, and you have a stochastic system where given an action given distribution of a possible location, so possible gauge locations you can sample and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So you're going through that process and recurrent network.",
                    "label": 0
                },
                {
                    "sent": "Can you classify things right?",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Basically your goal is to classify the digit 4, for example right?",
                    "label": 0
                },
                {
                    "sent": "But you have all of these stochastic latent variables that you can think of them as softmax, or Gaussians that specify which location you should be looking at.",
                    "label": 0
                },
                {
                    "sent": "Right and there are two classes of models that are hard attention models where you sample actions or these gays locations right so that you can process only small pieces of each image or there is a soft attention where you're taking expectations as opposed to taking as opposed to sampling.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's been a lot of work on in that space.",
                    "label": 0
                },
                {
                    "sent": "I'm just listing a few.",
                    "label": 0
                },
                {
                    "sent": "There's a work coming from Montreal that was using attention.",
                    "label": 0
                },
                {
                    "sent": "Was introducing attention models for translation.",
                    "label": 0
                },
                {
                    "sent": "There was one of the first models using soft attention and recurrent network at about the same time where people at deep mind have been sort of using.",
                    "label": 0
                },
                {
                    "sent": "Stochastic hard attention models the work of Rodney.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "We're looking at these stochastic models that are showing Hinton back in 2010.",
                    "label": 0
                },
                {
                    "sent": "They were looking at sort of attention models and of course people have been looking at that space for a very long time since the 80s, right?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly give you the mathematical details and hopefully convince you that these classes of models are pretty much the same as Helmholtz machines, or directed graphical models, just applied slightly different in a different context.",
                    "label": 0
                },
                {
                    "sent": "You know, So what you want to do, you want to maximize the probability of the correct class by marginalizing over actions.",
                    "label": 1
                },
                {
                    "sent": "So you can think of actions as your latent variables, right?",
                    "label": 0
                },
                {
                    "sent": "Think of action that basically says where I should be looking in the image, and there are exponentially many ways of looking at images.",
                    "label": 0
                },
                {
                    "sent": "Right now I look at the image here and here and here and here or here and here and here and here, right?",
                    "label": 0
                },
                {
                    "sent": "So this space is exponential, but you can view it as just another latent variable.",
                    "label": 1
                },
                {
                    "sent": "Model W is the set of the parameters of the recurrent network A is now your latent variable set of.",
                    "label": 1
                },
                {
                    "sent": "Actions that you need to take an X is the input or an image or video frame and whatever.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whatever that is, right?",
                    "label": 0
                },
                {
                    "sent": "So one way to do things, and that was done by Jimmy and by Vlad is to look at the variational bound right now, friend variational bound.",
                    "label": 0
                },
                {
                    "sent": "Wake you here is again some approximation to the posterior over the gays location at anytime you have some latent variables, some unknown variables, you can have some approximation to what the true distribution posterior distribution should look like.",
                    "label": 0
                },
                {
                    "sent": "Now one interesting fact is that if you approximating distribution, is the prior right UQ.",
                    "label": 1
                },
                {
                    "sent": "Here is just the prior prior over the actions, then the variational bound becomes quite simple, it just becomes this expression here.",
                    "label": 1
                },
                {
                    "sent": "Write an.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With variational learning, what you can do is you can say well, if I'm just going to assume that you know it's just the prior, you take the respective model parameters.",
                    "label": 0
                },
                {
                    "sent": "You have these terms.",
                    "label": 0
                },
                {
                    "sent": "The precise definition of these terms is not important.",
                    "label": 0
                },
                {
                    "sent": "What is important is that you have this really bad term here.",
                    "label": 1
                },
                {
                    "sent": "Popping up.",
                    "label": 0
                },
                {
                    "sent": "What is this term doing?",
                    "label": 0
                },
                {
                    "sent": "If you predict in class and let's say the probability of your correct prediction is .0001.",
                    "label": 0
                },
                {
                    "sent": "Take the log of that you have a huge negative number.",
                    "label": 0
                },
                {
                    "sent": "Right, so it really messes up your gradients becomes very unstable, so a lot of people were introducing heuristics.",
                    "label": 0
                },
                {
                    "sent": "Like for example you can replace this term with 01 discrete variable, which is kind of leads to something that's going to reinforce algorithm.",
                    "label": 1
                },
                {
                    "sent": "Like if you hit the correct class, it's one if you're hitting wrong class at 0, so you bounding is you have to find some way of bounding this term.",
                    "label": 0
                },
                {
                    "sent": "Right, which is which.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit problematic, but then again you can use stochastic estimation.",
                    "label": 1
                },
                {
                    "sent": "You can draw samples from the prior and just approximate.",
                    "label": 1
                },
                {
                    "sent": "And there's a whole bunch of things called control various techniques where people are trying to figure out how can they reduce the estimates.",
                    "label": 0
                },
                {
                    "sent": "The variance in the estimates, right and?",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you know you sample from the prior given image.",
                    "label": 0
                },
                {
                    "sent": "You just run the network forward just like a recurrent network running forward, and then you can you can estimate.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the key observations is that just like in importance weighted autoencoders, just like in highways, what you can do is you can actually try to take derivative of the log likelihood directly.",
                    "label": 0
                },
                {
                    "sent": "If you actually do that, you have this expression where Z here is the sum over these guys, but what's important is that this term here is the posterior.",
                    "label": 0
                },
                {
                    "sent": "Right, so maybe just step back a little bit and to say if I could compute exact posterior over actions.",
                    "label": 0
                },
                {
                    "sent": "Right then problem would have been solved.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's almost like saying I have an image.",
                    "label": 0
                },
                {
                    "sent": "I see an airplane in the image.",
                    "label": 0
                },
                {
                    "sent": "Given that I see an airplane in the image and the image itself, where should I be looking right?",
                    "label": 0
                },
                {
                    "sent": "What's the sequence of gaze locations I should be taking?",
                    "label": 0
                },
                {
                    "sent": "And maybe your true posterior would be say we look at the Sky's plans are typically in the skies, right?",
                    "label": 0
                },
                {
                    "sent": "But unfortunately you cannot use.",
                    "label": 0
                },
                {
                    "sent": "You cannot compute this past exactly.",
                    "label": 0
                },
                {
                    "sent": "So what you can?",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can use important sampling, So what can you do?",
                    "label": 0
                },
                {
                    "sent": "Again, you have to introduce some approximating distribution.",
                    "label": 1
                },
                {
                    "sent": "Q, right?",
                    "label": 0
                },
                {
                    "sent": "So there has to be some approximation and then you can use important sampling, right?",
                    "label": 1
                },
                {
                    "sent": "You can draw multiple samples, compute the important weights and you can get the estimates of the gradients right?",
                    "label": 0
                },
                {
                    "sent": "So what are all the high level picture?",
                    "label": 0
                },
                {
                    "sent": "I put the equations here for those of you who are interested in precise details, but the high level picture here is that you have some approximation.",
                    "label": 0
                },
                {
                    "sent": "You draw samples from this approximation using important sampling to basically refine your estimates of the true posterior right.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so what's the difference between the two approaches?",
                    "label": 1
                },
                {
                    "sent": "The difference between the two approaches is that they basically the same.",
                    "label": 0
                },
                {
                    "sent": "It turns out that just like invasion of encoders and Hwy, they pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "The computers the same except 4.",
                    "label": 1
                },
                {
                    "sent": "Here you use important weights and you don't have this bad term here.",
                    "label": 0
                },
                {
                    "sent": "That messes up the estimates of the gradients.",
                    "label": 1
                },
                {
                    "sent": "And of course, the performance of important sampling is very going to be heavily relied on.",
                    "label": 1
                },
                {
                    "sent": "The quality of your approximation, the quality of the approximation of the posterior right on your recognition network on your Q network.",
                    "label": 1
                },
                {
                    "sent": "How good is it?",
                    "label": 0
                },
                {
                    "sent": "And there is some work related.",
                    "label": 0
                },
                {
                    "sent": "There was a work if your Q distribution is the pride and it's something similar to what Charlie Tongue a student at University of Toronto was doing back in 2015.",
                    "label": 0
                },
                {
                    "sent": "And it's also very similar in spirit to the related Wake sleep model that was developed in Montreal.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another key observation here is that with a finite number of samples you can basically show that what we estimated with a finite number of samples is actually.",
                    "label": 1
                },
                {
                    "sent": "The lower bound, so in expectation we optimizing the lower bound on the marginal log likelihood.",
                    "label": 1
                },
                {
                    "sent": "Exactly the same ways and highways and the become becomes tighter as we increase the number of samples.",
                    "label": 1
                },
                {
                    "sent": "So that's one.",
                    "label": 0
                },
                {
                    "sent": "Take home message that I would like to communicate is that as you increase the number of samples, you can make your bound variational bound.",
                    "label": 0
                },
                {
                    "sent": "You can make it tighter, of course by how much and what's the variance of those estimated, since it's a little bit hard to say.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this estimator is at least as accurate as the variational bound, right?",
                    "label": 0
                },
                {
                    "sent": "So that's one thing that we can, we can.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It also has a beautiful relationship to Helmholtz machines, right?",
                    "label": 1
                },
                {
                    "sent": "Because what is this?",
                    "label": 0
                },
                {
                    "sent": "What is this model here?",
                    "label": 0
                },
                {
                    "sent": "Yes, the castec units, which you can think of actions or what the policy is.",
                    "label": 0
                },
                {
                    "sent": "You have deterministic units.",
                    "label": 0
                },
                {
                    "sent": "Yes, the cast in Terministic unit.",
                    "label": 0
                },
                {
                    "sent": "So you can basically view it as just big gigantic neural network with stochastic and deterministic units.",
                    "label": 1
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can basically view it as a conditional helpless machine, right?",
                    "label": 1
                },
                {
                    "sent": "An you can use wake sleep algorithm.",
                    "label": 1
                },
                {
                    "sent": "You can use any way to wake sleep algorithm you can you know if your distribution of your actions Gaussians you can use representation trick and.",
                    "label": 0
                },
                {
                    "sent": "And sort of various ways of learning good attention.",
                    "label": 0
                },
                {
                    "sent": "Policies right?",
                    "label": 0
                },
                {
                    "sent": "So that's one message that I want to communicate is that the whole space of different algorithms?",
                    "label": 0
                },
                {
                    "sent": "Just again you can view them as variants of host machines, yes?",
                    "label": 0
                },
                {
                    "sent": "Now you can change the direction, whereas before you were generating parameters variables.",
                    "label": 0
                },
                {
                    "sent": "Now it looks like you are going to reservations to generating those latent variables.",
                    "label": 0
                },
                {
                    "sent": "No, you can basically view it as a conditional model right where you sort of.",
                    "label": 1
                },
                {
                    "sent": "Given this, you generate the latent variable and this state of this variable effects what goes in here, which affects this state and effects what's going here.",
                    "label": 0
                },
                {
                    "sent": "In effect, if you just if you just take this model, turn it upside down, it will really look like.",
                    "label": 0
                },
                {
                    "sent": "Conditional helmholz machine.",
                    "label": 0
                },
                {
                    "sent": "This particular state will affect everything that goes on here and this state that you sample will affect what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just basically saying you can also build the recognition model so a lot of these models they do look like there is a recognition network.",
                    "label": 0
                },
                {
                    "sent": "There's a generative network, right?",
                    "label": 0
                },
                {
                    "sent": "The two networks that are interplaying together, so there's a P network genitive, not networking, is a recognition network.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so in this case again, you can construct the Q, your recognition network, which is a network that takes into account where you should be looking.",
                    "label": 0
                },
                {
                    "sent": "It takes into account the class label, right?",
                    "label": 1
                },
                {
                    "sent": "It takes into account image and what is it that you're trying to recognize to give you the posterior, the proper approximate posterior probability of where you should be looking right?",
                    "label": 0
                },
                {
                    "sent": "Let me just keep for the training.",
                    "label": 0
                },
                {
                    "sent": "You can train these models using standard using standard techniques.",
                    "label": 0
                },
                {
                    "sent": "Here's one thing that you can.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also sort of you know, if using attention you can not only choose the location, but also you can choose this scale so you can have assistance that choose where to look at what scale you want to look right.",
                    "label": 0
                },
                {
                    "sent": "These are just some preliminary examples that do show, like for example.",
                    "label": 0
                },
                {
                    "sent": "I think on this.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "You're basically having both of them at the same time, which is typically very hard to hard to.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hard versus soft.",
                    "label": 0
                },
                {
                    "sent": "Soft attention models there.",
                    "label": 0
                },
                {
                    "sent": "Computationally expensive because you need to.",
                    "label": 0
                },
                {
                    "sent": "You need to examine every single location in the image.",
                    "label": 1
                },
                {
                    "sent": "If you're doing it from images, but they are deterministic, there might be a little bit easy to train because they can be trained by using back propagation algorithm.",
                    "label": 1
                },
                {
                    "sent": "In terms of computational terms, are attention models the computation more efficient because you can only look at sub pieces.",
                    "label": 1
                },
                {
                    "sent": "You can only analyze.",
                    "label": 0
                },
                {
                    "sent": "Specific areas in the images of particularly working with video applications, but there stochastically requires some form of sampling or some form of having an approximation.",
                    "label": 0
                },
                {
                    "sent": "The way you should be looking right.",
                    "label": 1
                },
                {
                    "sent": "And research is taking place in both fronts.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let me just finish off by having about five 510 minutes.",
                    "label": 0
                },
                {
                    "sent": "Let me just finish off by showing you some applications, maybe pointing out some open.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Open problems.",
                    "label": 0
                },
                {
                    "sent": "So the few interesting areas that a lot of us in the deep learning community are working on is 1 area is unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, I've talked a lot about derivative models and the themes of the date works today.",
                    "label": 0
                },
                {
                    "sent": "Summer School is looking at generative models or doing a supervised learning things like unsupervised learning, transfer learning 1 short learning are the areas where we have to make more progress.",
                    "label": 1
                },
                {
                    "sent": "There is a whole subset of subgroups of people working on reasoning attention.",
                    "label": 1
                },
                {
                    "sent": "I've shown you some attention models and memory.",
                    "label": 0
                },
                {
                    "sent": "There is also an interesting work taking place in natural language.",
                    "label": 0
                },
                {
                    "sent": "Understanding this is 1 area where.",
                    "label": 0
                },
                {
                    "sent": "Deep learning can make a big impact, and I'm going to show you some examples of that and also the area of reinforcement learning, deep reinforcement learning, and you've seen some of that yesterday I believe.",
                    "label": 0
                },
                {
                    "sent": "Let me just show you some work on natural language processing and natural language and.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting and supervised learning there is a.",
                    "label": 0
                },
                {
                    "sent": "There's an area sort of called sequence to sequence learning.",
                    "label": 1
                },
                {
                    "sent": "You've covered that.",
                    "label": 0
                },
                {
                    "sent": "OK, that's good.",
                    "label": 1
                },
                {
                    "sent": "So your given sequence, you have another output sequence here.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So A1 model which unit you can try to encode sentences.",
                    "label": 0
                },
                {
                    "sent": "So given a sequence of sentences right, you can take the middle sentence and you can try to predict the context around the two sentences, right?",
                    "label": 0
                },
                {
                    "sent": "Just like in Word, two vec repres.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient and you can sort of encode the sentence.",
                    "label": 1
                },
                {
                    "sent": "You can predict you can generate the previous sentence and you can generate the forward segments right?",
                    "label": 0
                },
                {
                    "sent": "So trying to predict the context around the sentence right and so.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this particular model, you can think of it as just the recurrent network predicting forward and backward sentences.",
                    "label": 0
                },
                {
                    "sent": "You can optimize.",
                    "label": 0
                },
                {
                    "sent": "There is no statistics.",
                    "label": 0
                },
                {
                    "sent": "It's fully deterministic, and if you train it on large enough data set which is allowed.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1000 books you can get pretty interesting results.",
                    "label": 0
                },
                {
                    "sent": "You can actually get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "Um, you know, there's nothing linguistic.",
                    "label": 0
                },
                {
                    "sent": "There's no linguistic information sitting in these models is just crunching through the data and you pretty much hit state of the art on tasks like semantic relatedness task.",
                    "label": 0
                },
                {
                    "sent": "Right across all different models that use specific features redesigned trying to do handcrafted, handcrafted.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's here's what the task is.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can show you and get you excited about the task.",
                    "label": 0
                },
                {
                    "sent": "The task is the following.",
                    "label": 0
                },
                {
                    "sent": "I give you 2 sentence is and I tell you are they semantically similar or not on the scale of 1 to 5.",
                    "label": 0
                },
                {
                    "sent": "The ground truth is you ask people they ask 10 people and I take the average and that's the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So if I show you a man is driving a car, a car is being driven by the man.",
                    "label": 1
                },
                {
                    "sent": "The ground truth is 5 because people say it's the same.",
                    "label": 0
                },
                {
                    "sent": "The prediction is 4.9.",
                    "label": 1
                },
                {
                    "sent": "A little girl is looking at a woman in costume, a girl, a little girl in costume.",
                    "label": 0
                },
                {
                    "sent": "Looks like a woman.",
                    "label": 0
                },
                {
                    "sent": "Hopes and this is the ground truth is 2.9 the predictions 3.5 which is not too far.",
                    "label": 0
                },
                {
                    "sent": "This basically means that they are unrelated, but maybe there is some relationship, but here's a failure example.",
                    "label": 1
                },
                {
                    "sent": "This is where the model fails and this is something we need to improve.",
                    "label": 0
                },
                {
                    "sent": "A person is performing tricks on a motorcycle.",
                    "label": 0
                },
                {
                    "sent": "The performer is tricking a person on the motorcycle, right?",
                    "label": 0
                },
                {
                    "sent": "So it takes a little bit of understanding of what these two sentences are.",
                    "label": 0
                },
                {
                    "sent": "The model doesn't differentiate between the two.",
                    "label": 0
                },
                {
                    "sent": "Play some African leaders.",
                    "label": 0
                },
                {
                    "sent": "You just basically think of it as ask people two sentences.",
                    "label": 0
                },
                {
                    "sent": "Do you think they are semantically the same or not?",
                    "label": 0
                },
                {
                    "sent": "Number two, I don't understand what I groundskeeper two point 92.9 so 2.9 basically means that you ask 10 people.",
                    "label": 0
                },
                {
                    "sent": "They give you bunch of opinions, the average, and that's 2.9.",
                    "label": 0
                },
                {
                    "sent": "They don't report the variance, so maybe somebody said five.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing.",
                    "label": 0
                },
                {
                    "sent": "Or somebody said 11 basically means it's completely irrelevant, like I'm giving a talk and Microsoft crashed yesterday.",
                    "label": 0
                },
                {
                    "sent": "There is no relationship between these two.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I think that semantically they basically mean do they mean the same thing?",
                    "label": 0
                },
                {
                    "sent": "That's right, so question two that half the people just get it wrong.",
                    "label": 0
                },
                {
                    "sent": "I you know, but at the same time you know there is some like like if you compare these two sentences versus a sentence like I mentioned right now, right?",
                    "label": 0
                },
                {
                    "sent": "Like these two sentences are a little bit more similar than.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "By related.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it's for relatedness.",
                    "label": 0
                },
                {
                    "sent": "You know it's just.",
                    "label": 0
                },
                {
                    "sent": "It's just people's opinion.",
                    "label": 0
                },
                {
                    "sent": "Or do they mean the same thing?",
                    "label": 0
                },
                {
                    "sent": "Here's",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another, here's another sort of fun thing.",
                    "label": 0
                },
                {
                    "sent": "The fun thing to look at, and I'm almost done.",
                    "label": 0
                },
                {
                    "sent": "You can basically generate.",
                    "label": 0
                },
                {
                    "sent": "The model it generates little stories, so this model was trained by Jamie curious on 7000 romantic novels and given an image you basically ask the model.",
                    "label": 0
                },
                {
                    "sent": "It's a recurrent neural net model to generate the description right?",
                    "label": 1
                },
                {
                    "sent": "So it's kind of like sort of does funny things like she's beautiful, but the truth is I don't know what to do with the sun was just starting to fade away, leaving people scattered around the Atlantic Ocean.",
                    "label": 1
                },
                {
                    "sent": "So it's sort of does you know it sort of tries to capture some features of images and.",
                    "label": 0
                },
                {
                    "sent": "Generate coherent semantic, syntactically coherent pieces semantically is not very coherent, so we still have a.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for improvement, this is another area that was coming out from Chris dies Group as well as a student at CMU which basically you can build these hierarchical RNS taking taking words and characters, characters and words, putting them together.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And try to solve multiple tasks like parts of speech chunking, named entity recognition and the remarkable thing that I want to communicate here is that you basically achieving state of the art results by using these models, which was to me was a little bit surprising compared to a lot of linguistic features that are people using for these very well defined very well defined tasks.",
                    "label": 0
                },
                {
                    "sent": "Let me just skip one shot.",
                    "label": 0
                },
                {
                    "sent": "Learning peace and maybe.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point out that you know there's also some work taking place in.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deep reinforcement learning where you can try to build systems that can do transfer learning or can play multiple games right in the space of can you build a system that looks at 20 games and understand something about those gates, and then you start playing the new game apps to the new game.",
                    "label": 0
                },
                {
                    "sent": "Much quicker, right?",
                    "label": 0
                },
                {
                    "sent": "So these are sort of.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiple multiple.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Researchers because you already covered the 1st morning, let me skip over that.",
                    "label": 0
                },
                {
                    "sent": "So let me just sum.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rise by saying that.",
                    "label": 0
                },
                {
                    "sent": "You know there's been a lot of exciting developments in the space of deep learning in the last.",
                    "label": 1
                },
                {
                    "sent": "In the last, I guess, 567 years, ranging from the space of detection, speech recognition, tagging, building hierarchies, working in multi model space and these models do improve upon current state of the art in a lot of different application areas.",
                    "label": 1
                },
                {
                    "sent": "And it's due to three really things.",
                    "label": 0
                },
                {
                    "sent": "It's due to the fact that we have more data right?",
                    "label": 0
                },
                {
                    "sent": "It's due to the fact that we have more compute.",
                    "label": 0
                },
                {
                    "sent": "And it's also due to the fact that the algorithms that have been developed we can train these systems much more efficiently than we used to.",
                    "label": 0
                },
                {
                    "sent": "Let's say 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me let me stop.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, let me also thank all the students have been involved in a lot of different projects in my lab, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}