{
    "id": "r5txx5dfhqaafaq6ymuxpqqz7b4crd4z",
    "title": "Should Model Architecture Reflect Linguistic Structure?",
    "info": {
        "author": [
            "Chris Dyer, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_dyer_model_architecture/",
    "segmentation": [
        [
            "Alright, thank you very much for the invitation to be here.",
            "So the title of my talk asks whether we should be worried about putting linguistic structure into the architectures that are neural networks have and you know there is this famous Internet law Betteridge's law of headlines that says any question and a title can be answered negatively.",
            "But in my talk I'm going to attempt to give an affirmative answer to this.",
            "I mean, how else could I you know with these sort of deep?",
            "Learning luminaries here in the front row do anything else so."
        ],
        [
            "Basically I'm going to start out by talking about a couple of problems in language that a learner of language has to deal with and use this to kind of think about what we want our machine learners to do.",
            "So language is famous for exhibiting what's sometimes called arbitrariness, and this is just the idea that there is no relationship between the forms of particularly words and their meanings.",
            "So if we take the word car.",
            "And the semantics here rather than being represented by vector, I've represented by a little cartoon underneath.",
            "If we subtract the see in adibi to it, we end up with a particular meaning transformation.",
            "If we do the same thing starting with Cat, we end up with a very different sort of transformation.",
            "Another place where we see arbitrariness is when we look across languages.",
            "So the word car in English looks like this and a whole bunch of other different languages.",
            "And so basically.",
            "The idea behind arbitrariness is that there is no motivated relationship between what the what is being represented by the language and the object out in the world."
        ],
        [
            "The other hand, language avails itself, quite famously of compositionality, so if we have something like John dances, and we change John to Mary, we're going to get a particularly regular transformation of the semantics, which I've now written in this sort of predicate argument, kind of notation.",
            "So if we have a different predicate there sings, we get the same regularity of transformation.",
            "And so these two things you can kind of think about as being sort of opposite extremes."
        ],
        [
            "We can say we've got to both memorize things, and we've got to be able to generalize these, and this is probably true.",
            "In in almost every domain."
        ],
        [
            "So this gives us what I call the classical neural model of language, which is where we say that the memorization happens at the lexecon level, where we might say take each word in a language project, take one hot embedding, projected into some low dimensional vector space, and get a word embedding and then."
        ],
        [
            "Repeat this for all of the words in an input and then above.",
            "This is where sort of the interesting compositional stuff happens.",
            "So we learn some function.",
            "You know it's often a recurrent neural net or content or recursive neural letter.",
            "Whatever else is out there to make some prediction, and so in this kind of schematic, what we've got really is memorization, which is happening up in the top part, where each word just has a look up.",
            "Into a table of representations.",
            "And then we have generalization down below where we are learning to combine these elements in in new ways."
        ],
        [
            "Now the problem with this is it's challenged in at least two ways.",
            "So the first is well there idioms out there.",
            "So if we look at this pair of sentences and their semantics, we can see this nice sort of motivated alternation when we switch football for bucket.",
            "But if we change the predicate, we no longer get this regular transformation of meaning.",
            "Of course, John kicked the bucket, mean something like Die John.",
            "So at the basically what this means is that at the sentential propositional level, we also have this kind of unmotivated sort of aspect, so we need a learner who cannot just deal with kind of regular compositionality, but also memorize when it needs to.",
            "The second problem for this model is morphology, which is that if we go out into certain domains, we realize that the each word form is not actually independent of all the others, and this idea is actually getting out there quite a lot.",
            "That maybe we don't want to have we want to compute some kind of representations from the forms of words rather than trying to have a bunch of independent parameters.",
            "So on Twitter we see things like this, and of course in English even we have inflectional morphology where we?",
            "If we add an S to a word, it changes its meeting in a predictable way and we see this where if we have a different word that we get the same meaning transformation and of course for those of you who don't speak or who speak more interesting languages than English, you'll be well aware that you can do a whole lot more than what English does, so these are sort of the two learning challenges that we have in to deal with.",
            "I'm going to talk about this here at the beginning by.",
            "Discussing."
        ],
        [
            "Positional words, so the classic model, of course, is that we have, for each word particular embedding.",
            "That's just a look up.",
            "You'll get used to my graphical notation here in a second.",
            "It's not very different than anything else.",
            "And we can replace this with something that reads the contents of the word character by character.",
            "I'm using recurrent neural networks to do this, else itms, bidirectional ones and putting them together to computer representation of the word.",
            "Now we might say that the left model is the memorization model in the right hand model.",
            "Is the generalization model.",
            "We might also think, well, this is absurd because most.",
            "Things that happen at the lexical level amount to memorization and so."
        ],
        [
            "What we want to ask is, does a compositional model have the capacity to learn the arbitrariness of this mapping that we see in this form function, relationship in language and we might actually think so.",
            "I mean those of us who work do a lot of work with RNS, realize how easily they overfit to a lot of problems.",
            "In fact, that seems to be their common failure mode.",
            "So rather than seeing this as a as a problem, we can actually do.",
            "We actually get benefit from this and then the second question is if we go beyond sort of standard edited English into other languages and domains, are we going to maybe see more improvements where this form function mapping?",
            "So this relationship between the form of the word and its meaning is a little bit more regular?",
            "Maybe we're going to see some further improvements, so we've done it."
        ],
        [
            "Much of work on this in my lab and I'm gonna just review a couple of examples using dependency parsing.",
            "My apologies for those of you who are solving real AI, this is a sort of toy problem.",
            "We in NLP like to work on the idea.",
            "If you haven't seen this as you get an input sentence.",
            "I saw her duck.",
            "This is an ambiguous sentence, so it can mean either seeing a duck or seeing somebody ducking.",
            "And maybe these are these correspond to different analysis of the sentence.",
            "And so the analysis that we're interested in are these dependency relations.",
            "So every word in the sentence picks out a governor, its head, and these have to form a tree.",
            "And this of course drives a lot of classic work in NLP and what we're going to do to test these models that I just outlined is we're going to use a neural network parser that basically reads each word in the input in as a vector computed variously, either as a as a look up or as a one as the output of one of these compositional functions.",
            "So we're going to compare the memorization and generalization models.",
            "So there are a bunch of neural network parsers out there.",
            "This is sort of.",
            "There's a little cottage industry in NLP these days of building these things.",
            "You can ask me if you're interested in the details of which one but."
        ],
        [
            "The main thing I want to show is what the pattern of results looks like.",
            "So if we look at English so English being a language where we actually think this compositional character model wouldn't work terribly well because English doesn't do a lot with morphology, the words themselves.",
            "Even when we pronounce them the same, we don't usually have a regular mapping between between the various forms, so the fact that we actually maybe slightly improve.",
            "Over the word, the look up baseline is really quite exciting, so we can say that these are roughly equivalent, but I have at least at various points in my past.",
            "Ben, a linguist, and I'm interested in other languages and the crazy things they do.",
            "So these are single words in Turkish and Hungarian, at least.",
            "My Turkish and Hungarian.",
            "People I've presented this to have not ever strenuously objected to this.",
            "I I'm not even gonna try."
        ],
        [
            "So.",
            "It's very unlikely that we're going to see one of these words in any training corpus, no matter how large, no matter if we've got the entire web or not, well, probably now because they're crawling.",
            "You know these slides somewhere, but."
        ],
        [
            "This people use they create new words like this all the time and the languages that create these words.",
            "We can group them together and call them agglutinative languages or languages with agglutinative morphology, and so we can look at a whole group of them, and we can see that the improvements here by switching from characters to this from words to this character based model are significant, so we've increased this by an average of six or seven points, which in the parsing world is people are very excited by.",
            "The old days of handcrafted features for these sorts of things are gone just with a C and replaced just with sequences of characters.",
            "So this is nice, and it also sort of flatters what we think we understand about about languages in between.",
            "These sort of extreme agglutinative languages like Turkish and Baskin finish an the boring ones like English are what are sometimes called the fusional languages, which have sort of a little bit less craziness and their morphology.",
            "And in fact, we see the improvements that we get in those languages as being sort of somewhere between those two.",
            "The improvements aren't as great as they were neglect native languages, but their their bigger nevertheless and just to sort of round things off so.",
            "Chinese is another language which has sort of allegedly even less morphology than English, and there we see again sort of no real difference from the from the baseline.",
            "So basically, at least for a toy task like dependency parsing, we can do pretty well by computing word representations using these sequences of.",
            "Characters, which is kind of nice."
        ],
        [
            "For a slightly more interesting task, we also looked at language modeling and said can we replace the embeddings that are used to drive language models with these computed embeddings and I know there's been work from variety of other groups at this point on this, but it's all been sort of relatively positive, but one of the things that I think is really interesting about these computed embeddings is you can ask them things.",
            "You can't ask standard embedding models, so if we query this model for say, increased or John and get the five nearest neighbors.",
            "Well, we see actually what we usually see when you run word to VEC and get sort of synonyms and words that sort of functionally behave like the queries.",
            "Now the thing we can do is query them with what are called nonce words so words that don't exist and we can also see sort of.",
            "Perhaps intuitively sensible words so.",
            "I don't know where Noah Shire is, but somewhere between Nottinghamshire and Bucharest, maybe.",
            "And my student who did this was busy PhD ING and he thought it was some mix of all of those things apparently so.",
            "We haven't yet put the Jabberwocky into this, but that's would be interesting as well.",
            "OK, so."
        ],
        [
            "Just to summarize, this sort of character versus word modeling part.",
            "So first there's a lot of exciting work going on in a lot of exciting places.",
            "Most people are using comp Nets over the characters rather than these sequential models.",
            "I will confess that I just haven't had people run the comparisons, so it's on the list, but I want to now move on to something that's maybe a little bit more controversial where we're really going with some sort of.",
            "Linguistic knowledge that might not directly be in the signal."
        ],
        [
            "So we can think about this as saying, well, what if we had access to the morphemes that were present in these words?",
            "So what if?"
        ],
        [
            "Change the word cats into cat, plus a plural marker so we have finite state transducers that can do these things very well.",
            "This is one of the big successes in natural language processing.",
            "They problem is that for a given type you don't necessarily there may be a lot of ambiguities, so the word on ionize could also be unionized in English, and if we just see the word, we don't know which one it is, and so an FST that operates on types isn't going to be able to distinguish those.",
            "So I will have to deal with that."
        ],
        [
            "And so we've done some initial work on what we call or what is commonly called open vocabulary language models where we want to predict we want to model all of Sigma star.",
            "So where Sigma is not the words in a finite vocabulary that you've sort of said, well, this is in the vocabulary and this is out of the vocabulary.",
            "This is the standard language modeling game we play.",
            "Instead, Sigma is the inventory of characters, so we can really generate with some probability sequences of anything."
        ],
        [
            "We're looking at this in Turkish because, well, we have money from the government and they were interested in Turkish for some reason.",
            "So here's a sentence in circus going from the top to bottom, and we have one of these wonderful morphological analyzers that was written back in the 90s and it spits out things like this.",
            "So for every word type it gives us a list of possible analysis and these lists are actually quite short there.",
            "Usually you know an average length, two or three.",
            "And So what we need to do, what we want to do is use the knowledge that's present in this FST to help structure the neural networks that we are using to model language, and so let's look at what happens when we want to condition just on one of these words.",
            "So we're going to be using basically an RNN language model.",
            "We're just going to be replacing the input representations with from vlookups to something a little more interesting.",
            "So let's focus on this word, 4th from the bottom.",
            "So there are two analysis here and what we're going to do is run kind of LST M over this to get a embedding of each of these routes plus the sequence of ethics is.",
            "And we're just going to pull these together because we don't want to actually model which one was meant in context.",
            "Go on and we can also say, well, we also know that characters worked pretty well in the past, so we can run a bidirectional STM over that and then we can also look up words if we happen to have an embedding of the word, or we can have some unknown word talking and we can sort of aggregate all of this stuff together."
        ],
        [
            "Then we can go to generate this and we are not going to sort of into great detail on this, But basically the output process is a little bit more complicated because we have several ways to generate.",
            "We can either generate a word by spelling it out one character at a time.",
            "We can also just sample it from a short dictionary, or we can generate a root and a sequence of affix is and ask the finite state transducer to give us the surface form, and we're going to marginalise all of these different things.",
            "So we've basically got all of these different pieces.",
            "And then."
        ],
        [
            "The question is, how well does this work, so these are very large numbers in part due to data.",
            "We've run this on English and don't worry, the numbers are reasonable, but the thing to note here is that as we add more information over the characters, we do get better and better perplexity, so these are perplexities.",
            "So lower is better and so this is.",
            "This has been quite nice."
        ],
        [
            "So.",
            "My summary of this character versus word modeling stuff is so for those of us who are fortunate to just be able to work in English or maybe Chinese, the gains are maybe not hugely important, but certainly in morphologically rich languages or in domains where content is generated by users and people are spelling things out in sort of new and creative ways, it does matter quite a bit.",
            "Another thing I didn't mention here, but is quite important is we can get away with far fewer parameters to represent words in this way, and this is quite interesting because it's you know we've in some of those cases on the language models, we were able to reduce the model size by an order of magnitude, which is really getting quite quite small, so this question of how much information do you need to store a language to know a language is looks very different than it used to.",
            "So."
        ],
        [
            "Now I want to move on to talk about somewhere else where we might be able to add a little bit of linguistic structural knowledge.",
            "So I'm going to move on for the second part of this talk to talking about syntax.",
            "Which is of course you know where where the connectionists and the traditional symbolic types have traditionally like to really argue, so I am going to start out by claiming that it is not controversial that language is hierarchical.",
            "Now.",
            "The details of this hierarchy are up for debate.",
            "The theorists love to argue about this, but language is essentially hierarchical, and so you can see this in some cases like this.",
            "So if I say.",
            "The talk I gave did not appeal to anybody.",
            "Well, that's wrong, so we have to get rid of the not right?",
            "So the talk I gave appealed to anybody wait, but now it's not grammatical.",
            "So this is an instance of what's called negative polarity item licensing, so all many languages.",
            "Most languages have these negative polarity items that have to be present.",
            "There has to be a negative word that licenses them, so a reasonable generalization hypothesis that both the linguist or machine learning algorithm might make is that for anybody to be present, it has to come after.",
            "The word not or hardly or one of these other NPI licensors."
        ],
        [
            "And then we can look at a second data point.",
            "We can say the talk.",
            "I did not give appealed to anybody.",
            "And it's still word salad.",
            "And actually there apparently are some dialects of English for some of these are kind of marginal.",
            "I don't have to speak one, so pick your favorite language or the facts are a little more robust.",
            "There are certainly other.",
            "It's not just Npis.",
            "As I said that that show this so.",
            "The claim is that this is due to particular structural that.",
            "Languages first organized into hierarchically organized structures and that particular conceptions of these structures are what license the presence of this, this anybody, and so you can see this on the left.",
            "The talk I gave did not appeal to anyone, which is semantically bad because you all love my talk and but syntactically fine, you'll notice that the knot is in this particular.",
            "Sort of, I am the uncle of the antibody relationship and in the right hand side where we've got the talk, I did not give appealed to anybody, which is the syntactically and semantically bad one.",
            "We don't have that configuration, and it turns out that if you look for this phenomenon across languages, there are whole bunch of independent reasons for identifying that these kinds of groupings of sentence is that that people, language users perceive them and that they are the ones that license or that control.",
            "Whether this will be judged as acceptable.",
            "So really, the generalization is that not must in some sense structurally precede anybody.",
            "So.",
            "There are many theories.",
            "As I said of the details of the structure and I think.",
            "Well, I'm not going to speculate too much on what the linguists are up to, but I think you guys are on, you know, coming closer to getting important insights than they are at this point.",
            "But I do want to emphasize that the psychology psychological reality of structural sensitivity is not empirically controversial.",
            "There are marginal cases and languages that are claimed to be marginal, but we don't know what I want to contrast this with is a view where linearly the presence of an NPI would be the thing that governed, not.",
            "We just don't see languages like that.",
            "Although you could imagine for any sort of unbiased or sort of vaguely biased learner.",
            "That would be a perfectly reasonable language to to acquire.",
            "And finally, this is not I'm using Npis as a as an example here.",
            "There are many other facts and language that show this kind of structural sensitivity.",
            "So what we've done in the last few months in my lab is build a attempt to take a very slight extension to recurrent neural networks and say they also we want to model sequences of words just like we do in language modeling.",
            "But we also want to model.",
            "We want to group the words into this kind of hierarchical configuration, and we want to see if this.",
            "If this had some value so."
        ],
        [
            "So this is just sort of 1 theory of hierarchy.",
            "I don't think most linguists would particularly like this.",
            "I think this is just a sort of minimal step in that direction.",
            "So what we're going to do to the generation process is we're going to add what you might call some control symbols.",
            "So as you're going along and generating a sequence of words periodically, what we're going to do is we're going to get a signal that says.",
            "Now I want you to compress.",
            "The last an items that you've generated and form a constituent and all that's going to mean for us is we're going to erase them from the history of our RNN, and we're going to run a composition function over them, and then we're going to replace those items on the, you know at the end of this RNN, so you can think about this for those of you who are familiar with shift reduce type parsing.",
            "This is kind of similar to a reduce operation.",
            "We are shifting is we're not parsing here we're generating, so it's going to look a little bit different.",
            "But that's sort of the build constituent operation if you will.",
            "So the RNN then has to predict basically the next symbol rather than just conditional on a sequence of previously predicted output symbols.",
            "It should predict the next symbol based on a mixture of terminal symbols, an possibly composed constituents.",
            "It will decide what what comes next, but it also has to predict these control symbols which are going to decide how big the constituents are.",
            "So we're going to call these recurrent neural network grammars, and all this means is that, well, they're based on recurrent neural networks is sort of driving these things and their grammars because they're going to be jointly generating affectively these tree structures and the sequence of words, which is all the grammar does.",
            "So I think the easiest way to see this is just to go through the example generation of the sentence and you'll see examples of these control symbols which at least for the channel peers in the audience will look familiar, and the mechanics of.",
            "Sort of what it means to compose something into a composition."
        ],
        [
            "So let's start as we always do by generating a special symbol called S and we're going to push this onto a stack, and the decisions that so on the left I'm going to write out the terminals that are being generated by this and in the middle I'm going to write out the symbols that the RNN is conditioning on when it's making the next decision.",
            "So we might generate an NP.",
            "These are non terminal symbols.",
            "These are these control symbols and now finally I'm going to get down to generating a terminal symbol V and we'll put it now in two places and we'll build up the hungry cat and at this point we might say, well, I have built what I think is a single constituent an I want to in the future treat this as a unit for any future predictions so.",
            "If we predict a reduce operation here, you can think about this as closing this parentheses and that's when I said, you know, the control operations are there to decide sort of how big these constituents are.",
            "They also sort of define the type and things like that.",
            "So we've got the sequence of four things at the top of the stack, which represent kind of a kind of unit.",
            "And what we do then is we pop those off and we replace.",
            "That those four symbols with an embedding of a single symbol representing this entire noun phrase, and of course you know everyone, is familiar with Richard searchers, recursive neural network.",
            "This is very much this idea, so we then replace those four symbols with this one, and the process continues and we complete.",
            "We finish this process rather than when we get to a stop symbol when we instead have a single.",
            "Element, a single constituent constituent, left on the stack so."
        ],
        [
            "How do we do this syntactic composition?",
            "I just want to say a couple of words about this, because again, this is somewhere where we found it interesting to put in some linguistic structure.",
            "So the thing to note about this is this is designated as an NP a noun phrase, and there's this theory in linguistics, which is basically a empirical generalization that constituents have a particular word in them that is, is their head.",
            "It's gives most of the properties too.",
            "That phrase, so basically, if you get rid of all the words in here, you're probably the one you would want to keep this cat and we'd call that the head.",
            "But of course, you know linguists don't necessarily argue about the existence of heads.",
            "They do argue about which one is the head, so some people think it's So what we're going to do is we're going to build the representation as follows.",
            "We're going to start off and say, well, I'm in NP.",
            "I'm going to read this into an RNN, and I'm going to sort of notify the RN.",
            "What is it you're looking for?",
            "And then I'm going to read from left to right, the embeddings of the children, until I get to a stop symbol, and then this is going to be left to right embedding of this week."
        ],
        [
            "Also repeat this process from right to left.",
            "Again, we're going to start with this NP because we want to.",
            "We want to be able to sort of look for the head that we we need to find.",
            "Repeat this process and then we combine the two.",
            "Now of course this is a.",
            "We have terminal symbols here down at the bottom.",
            "In general, we're going to have other recursively composed representations.",
            "So if instead of hungry we had."
        ],
        [
            "The adjectival phrase, very hungry this would have."
        ],
        [
            "Course, just have an embedding in the same space and we would compute the representation of this more complicated constituent in the completely straightforward way."
        ],
        [
            "OK, so this is, you know, sort of minor extensions to existing recursive neural network architectures.",
            "We kind of like this because it sort of captures this notion of headedness and can deal with any number of children."
        ],
        [
            "So I'm going to talk a little bit here about some of the empirical work we've we've done with this stuff so.",
            "Our energies are joint models of sequences and words together with a tree structure, and so this means we can ask questions about you, know any of the conditional or marginal distributions that this gives rise to, such as what's the mode of the conditional distribution over trees given the sentence, also known as parsing, and we can also ask how well does this work as a language model, and the hope is that it will work well at both.",
            "So.",
            "It turns out that any sequence of these actions that I've given you can convert that to a tree structure, and vice versa.",
            "So we are just training this in a fully supervised way.",
            "At this point we could of course treat the you know these non generation these control actions as latent variables will learn them with our L and turn this into a grammar induction problem.",
            "I think this is super exciting and we're looking into this."
        ],
        [
            "But for now I'm going to just talk about the supervised results, so right, so we've got this joint distribution and we're interested in these two inference questions.",
            "P of X for a given X&P of Y given X forgiven X so traditionally those of you who have worked with other probabilistic grammars know that.",
            "Well, here's where we.",
            "Usually we have this nasty summation problem.",
            "There are an exponential or maybe worse number of trees.",
            "For any sentence, we need to marginalized their search over that.",
            "Unfortunately, because we condition on everything with RNN's that goes away.",
            "But Fortunately we can come up with a very nice important sampling algorithm, and we have a very convenient proposal distribution here where we discriminatively train this parser on sentence is an I don't have time to go into the details of this.",
            "We've got a paper coming out at nacle this summer on this.",
            "If you want to read about it."
        ],
        [
            "OK, so briefly I want to show some parsing results, so two things to note.",
            "First, the discriminative model here is.",
            "Not actually as good as the generative model.",
            "This is a bit unusual and.",
            "Kind of exciting to see I think you know, since.",
            "Grammars sort of.",
            "The syntax that underlies the Penn Treebank was designed to was designed more to explain how sentences came to be.",
            "The fact that we've got sort of a good match for the generative process might explain some of this.",
            "The thing that I also found."
        ],
        [
            "Very interesting here is that as a language model, this is actually quite good, so we worked very hard to get very solid basslines on here 2 tasks, English and Chinese, and we found that we do outperform these quite nicely.",
            "So so this is shows that basically a single model can be both a good parser and a good language model, and it's been a very long time since that's been the case."
        ],
        [
            "OK, so just to conclude here facts about language, so arbitrariness and compositionality existed all levels.",
            "I hope I've somewhat convinced you that language is sensitive to hierarchy, not just bear strings and my sort of guiding hypothesis has been that if we pay attention to these things when designing our models will end up with better models than when we ignore them."
        ],
        [
            "Thanks and I'll take any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thank you very much for the invitation to be here.",
                    "label": 0
                },
                {
                    "sent": "So the title of my talk asks whether we should be worried about putting linguistic structure into the architectures that are neural networks have and you know there is this famous Internet law Betteridge's law of headlines that says any question and a title can be answered negatively.",
                    "label": 0
                },
                {
                    "sent": "But in my talk I'm going to attempt to give an affirmative answer to this.",
                    "label": 0
                },
                {
                    "sent": "I mean, how else could I you know with these sort of deep?",
                    "label": 0
                },
                {
                    "sent": "Learning luminaries here in the front row do anything else so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically I'm going to start out by talking about a couple of problems in language that a learner of language has to deal with and use this to kind of think about what we want our machine learners to do.",
                    "label": 0
                },
                {
                    "sent": "So language is famous for exhibiting what's sometimes called arbitrariness, and this is just the idea that there is no relationship between the forms of particularly words and their meanings.",
                    "label": 0
                },
                {
                    "sent": "So if we take the word car.",
                    "label": 0
                },
                {
                    "sent": "And the semantics here rather than being represented by vector, I've represented by a little cartoon underneath.",
                    "label": 0
                },
                {
                    "sent": "If we subtract the see in adibi to it, we end up with a particular meaning transformation.",
                    "label": 0
                },
                {
                    "sent": "If we do the same thing starting with Cat, we end up with a very different sort of transformation.",
                    "label": 0
                },
                {
                    "sent": "Another place where we see arbitrariness is when we look across languages.",
                    "label": 1
                },
                {
                    "sent": "So the word car in English looks like this and a whole bunch of other different languages.",
                    "label": 0
                },
                {
                    "sent": "And so basically.",
                    "label": 0
                },
                {
                    "sent": "The idea behind arbitrariness is that there is no motivated relationship between what the what is being represented by the language and the object out in the world.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other hand, language avails itself, quite famously of compositionality, so if we have something like John dances, and we change John to Mary, we're going to get a particularly regular transformation of the semantics, which I've now written in this sort of predicate argument, kind of notation.",
                    "label": 0
                },
                {
                    "sent": "So if we have a different predicate there sings, we get the same regularity of transformation.",
                    "label": 0
                },
                {
                    "sent": "And so these two things you can kind of think about as being sort of opposite extremes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can say we've got to both memorize things, and we've got to be able to generalize these, and this is probably true.",
                    "label": 0
                },
                {
                    "sent": "In in almost every domain.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this gives us what I call the classical neural model of language, which is where we say that the memorization happens at the lexecon level, where we might say take each word in a language project, take one hot embedding, projected into some low dimensional vector space, and get a word embedding and then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Repeat this for all of the words in an input and then above.",
                    "label": 0
                },
                {
                    "sent": "This is where sort of the interesting compositional stuff happens.",
                    "label": 0
                },
                {
                    "sent": "So we learn some function.",
                    "label": 0
                },
                {
                    "sent": "You know it's often a recurrent neural net or content or recursive neural letter.",
                    "label": 1
                },
                {
                    "sent": "Whatever else is out there to make some prediction, and so in this kind of schematic, what we've got really is memorization, which is happening up in the top part, where each word just has a look up.",
                    "label": 0
                },
                {
                    "sent": "Into a table of representations.",
                    "label": 0
                },
                {
                    "sent": "And then we have generalization down below where we are learning to combine these elements in in new ways.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem with this is it's challenged in at least two ways.",
                    "label": 0
                },
                {
                    "sent": "So the first is well there idioms out there.",
                    "label": 1
                },
                {
                    "sent": "So if we look at this pair of sentences and their semantics, we can see this nice sort of motivated alternation when we switch football for bucket.",
                    "label": 0
                },
                {
                    "sent": "But if we change the predicate, we no longer get this regular transformation of meaning.",
                    "label": 0
                },
                {
                    "sent": "Of course, John kicked the bucket, mean something like Die John.",
                    "label": 1
                },
                {
                    "sent": "So at the basically what this means is that at the sentential propositional level, we also have this kind of unmotivated sort of aspect, so we need a learner who cannot just deal with kind of regular compositionality, but also memorize when it needs to.",
                    "label": 0
                },
                {
                    "sent": "The second problem for this model is morphology, which is that if we go out into certain domains, we realize that the each word form is not actually independent of all the others, and this idea is actually getting out there quite a lot.",
                    "label": 0
                },
                {
                    "sent": "That maybe we don't want to have we want to compute some kind of representations from the forms of words rather than trying to have a bunch of independent parameters.",
                    "label": 0
                },
                {
                    "sent": "So on Twitter we see things like this, and of course in English even we have inflectional morphology where we?",
                    "label": 0
                },
                {
                    "sent": "If we add an S to a word, it changes its meeting in a predictable way and we see this where if we have a different word that we get the same meaning transformation and of course for those of you who don't speak or who speak more interesting languages than English, you'll be well aware that you can do a whole lot more than what English does, so these are sort of the two learning challenges that we have in to deal with.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about this here at the beginning by.",
                    "label": 0
                },
                {
                    "sent": "Discussing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positional words, so the classic model, of course, is that we have, for each word particular embedding.",
                    "label": 0
                },
                {
                    "sent": "That's just a look up.",
                    "label": 0
                },
                {
                    "sent": "You'll get used to my graphical notation here in a second.",
                    "label": 0
                },
                {
                    "sent": "It's not very different than anything else.",
                    "label": 0
                },
                {
                    "sent": "And we can replace this with something that reads the contents of the word character by character.",
                    "label": 0
                },
                {
                    "sent": "I'm using recurrent neural networks to do this, else itms, bidirectional ones and putting them together to computer representation of the word.",
                    "label": 0
                },
                {
                    "sent": "Now we might say that the left model is the memorization model in the right hand model.",
                    "label": 0
                },
                {
                    "sent": "Is the generalization model.",
                    "label": 0
                },
                {
                    "sent": "We might also think, well, this is absurd because most.",
                    "label": 0
                },
                {
                    "sent": "Things that happen at the lexical level amount to memorization and so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we want to ask is, does a compositional model have the capacity to learn the arbitrariness of this mapping that we see in this form function, relationship in language and we might actually think so.",
                    "label": 1
                },
                {
                    "sent": "I mean those of us who work do a lot of work with RNS, realize how easily they overfit to a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "In fact, that seems to be their common failure mode.",
                    "label": 0
                },
                {
                    "sent": "So rather than seeing this as a as a problem, we can actually do.",
                    "label": 0
                },
                {
                    "sent": "We actually get benefit from this and then the second question is if we go beyond sort of standard edited English into other languages and domains, are we going to maybe see more improvements where this form function mapping?",
                    "label": 0
                },
                {
                    "sent": "So this relationship between the form of the word and its meaning is a little bit more regular?",
                    "label": 0
                },
                {
                    "sent": "Maybe we're going to see some further improvements, so we've done it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much of work on this in my lab and I'm gonna just review a couple of examples using dependency parsing.",
                    "label": 1
                },
                {
                    "sent": "My apologies for those of you who are solving real AI, this is a sort of toy problem.",
                    "label": 0
                },
                {
                    "sent": "We in NLP like to work on the idea.",
                    "label": 0
                },
                {
                    "sent": "If you haven't seen this as you get an input sentence.",
                    "label": 0
                },
                {
                    "sent": "I saw her duck.",
                    "label": 0
                },
                {
                    "sent": "This is an ambiguous sentence, so it can mean either seeing a duck or seeing somebody ducking.",
                    "label": 0
                },
                {
                    "sent": "And maybe these are these correspond to different analysis of the sentence.",
                    "label": 0
                },
                {
                    "sent": "And so the analysis that we're interested in are these dependency relations.",
                    "label": 0
                },
                {
                    "sent": "So every word in the sentence picks out a governor, its head, and these have to form a tree.",
                    "label": 0
                },
                {
                    "sent": "And this of course drives a lot of classic work in NLP and what we're going to do to test these models that I just outlined is we're going to use a neural network parser that basically reads each word in the input in as a vector computed variously, either as a as a look up or as a one as the output of one of these compositional functions.",
                    "label": 0
                },
                {
                    "sent": "So we're going to compare the memorization and generalization models.",
                    "label": 0
                },
                {
                    "sent": "So there are a bunch of neural network parsers out there.",
                    "label": 0
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "There's a little cottage industry in NLP these days of building these things.",
                    "label": 0
                },
                {
                    "sent": "You can ask me if you're interested in the details of which one but.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main thing I want to show is what the pattern of results looks like.",
                    "label": 0
                },
                {
                    "sent": "So if we look at English so English being a language where we actually think this compositional character model wouldn't work terribly well because English doesn't do a lot with morphology, the words themselves.",
                    "label": 0
                },
                {
                    "sent": "Even when we pronounce them the same, we don't usually have a regular mapping between between the various forms, so the fact that we actually maybe slightly improve.",
                    "label": 0
                },
                {
                    "sent": "Over the word, the look up baseline is really quite exciting, so we can say that these are roughly equivalent, but I have at least at various points in my past.",
                    "label": 0
                },
                {
                    "sent": "Ben, a linguist, and I'm interested in other languages and the crazy things they do.",
                    "label": 0
                },
                {
                    "sent": "So these are single words in Turkish and Hungarian, at least.",
                    "label": 0
                },
                {
                    "sent": "My Turkish and Hungarian.",
                    "label": 0
                },
                {
                    "sent": "People I've presented this to have not ever strenuously objected to this.",
                    "label": 0
                },
                {
                    "sent": "I I'm not even gonna try.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's very unlikely that we're going to see one of these words in any training corpus, no matter how large, no matter if we've got the entire web or not, well, probably now because they're crawling.",
                    "label": 0
                },
                {
                    "sent": "You know these slides somewhere, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This people use they create new words like this all the time and the languages that create these words.",
                    "label": 0
                },
                {
                    "sent": "We can group them together and call them agglutinative languages or languages with agglutinative morphology, and so we can look at a whole group of them, and we can see that the improvements here by switching from characters to this from words to this character based model are significant, so we've increased this by an average of six or seven points, which in the parsing world is people are very excited by.",
                    "label": 0
                },
                {
                    "sent": "The old days of handcrafted features for these sorts of things are gone just with a C and replaced just with sequences of characters.",
                    "label": 0
                },
                {
                    "sent": "So this is nice, and it also sort of flatters what we think we understand about about languages in between.",
                    "label": 0
                },
                {
                    "sent": "These sort of extreme agglutinative languages like Turkish and Baskin finish an the boring ones like English are what are sometimes called the fusional languages, which have sort of a little bit less craziness and their morphology.",
                    "label": 1
                },
                {
                    "sent": "And in fact, we see the improvements that we get in those languages as being sort of somewhere between those two.",
                    "label": 0
                },
                {
                    "sent": "The improvements aren't as great as they were neglect native languages, but their their bigger nevertheless and just to sort of round things off so.",
                    "label": 0
                },
                {
                    "sent": "Chinese is another language which has sort of allegedly even less morphology than English, and there we see again sort of no real difference from the from the baseline.",
                    "label": 1
                },
                {
                    "sent": "So basically, at least for a toy task like dependency parsing, we can do pretty well by computing word representations using these sequences of.",
                    "label": 0
                },
                {
                    "sent": "Characters, which is kind of nice.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a slightly more interesting task, we also looked at language modeling and said can we replace the embeddings that are used to drive language models with these computed embeddings and I know there's been work from variety of other groups at this point on this, but it's all been sort of relatively positive, but one of the things that I think is really interesting about these computed embeddings is you can ask them things.",
                    "label": 0
                },
                {
                    "sent": "You can't ask standard embedding models, so if we query this model for say, increased or John and get the five nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Well, we see actually what we usually see when you run word to VEC and get sort of synonyms and words that sort of functionally behave like the queries.",
                    "label": 0
                },
                {
                    "sent": "Now the thing we can do is query them with what are called nonce words so words that don't exist and we can also see sort of.",
                    "label": 0
                },
                {
                    "sent": "Perhaps intuitively sensible words so.",
                    "label": 0
                },
                {
                    "sent": "I don't know where Noah Shire is, but somewhere between Nottinghamshire and Bucharest, maybe.",
                    "label": 0
                },
                {
                    "sent": "And my student who did this was busy PhD ING and he thought it was some mix of all of those things apparently so.",
                    "label": 0
                },
                {
                    "sent": "We haven't yet put the Jabberwocky into this, but that's would be interesting as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to summarize, this sort of character versus word modeling part.",
                    "label": 0
                },
                {
                    "sent": "So first there's a lot of exciting work going on in a lot of exciting places.",
                    "label": 0
                },
                {
                    "sent": "Most people are using comp Nets over the characters rather than these sequential models.",
                    "label": 0
                },
                {
                    "sent": "I will confess that I just haven't had people run the comparisons, so it's on the list, but I want to now move on to something that's maybe a little bit more controversial where we're really going with some sort of.",
                    "label": 0
                },
                {
                    "sent": "Linguistic knowledge that might not directly be in the signal.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can think about this as saying, well, what if we had access to the morphemes that were present in these words?",
                    "label": 0
                },
                {
                    "sent": "So what if?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change the word cats into cat, plus a plural marker so we have finite state transducers that can do these things very well.",
                    "label": 0
                },
                {
                    "sent": "This is one of the big successes in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "They problem is that for a given type you don't necessarily there may be a lot of ambiguities, so the word on ionize could also be unionized in English, and if we just see the word, we don't know which one it is, and so an FST that operates on types isn't going to be able to distinguish those.",
                    "label": 0
                },
                {
                    "sent": "So I will have to deal with that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we've done some initial work on what we call or what is commonly called open vocabulary language models where we want to predict we want to model all of Sigma star.",
                    "label": 0
                },
                {
                    "sent": "So where Sigma is not the words in a finite vocabulary that you've sort of said, well, this is in the vocabulary and this is out of the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "This is the standard language modeling game we play.",
                    "label": 0
                },
                {
                    "sent": "Instead, Sigma is the inventory of characters, so we can really generate with some probability sequences of anything.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're looking at this in Turkish because, well, we have money from the government and they were interested in Turkish for some reason.",
                    "label": 0
                },
                {
                    "sent": "So here's a sentence in circus going from the top to bottom, and we have one of these wonderful morphological analyzers that was written back in the 90s and it spits out things like this.",
                    "label": 0
                },
                {
                    "sent": "So for every word type it gives us a list of possible analysis and these lists are actually quite short there.",
                    "label": 0
                },
                {
                    "sent": "Usually you know an average length, two or three.",
                    "label": 0
                },
                {
                    "sent": "And So what we need to do, what we want to do is use the knowledge that's present in this FST to help structure the neural networks that we are using to model language, and so let's look at what happens when we want to condition just on one of these words.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be using basically an RNN language model.",
                    "label": 0
                },
                {
                    "sent": "We're just going to be replacing the input representations with from vlookups to something a little more interesting.",
                    "label": 0
                },
                {
                    "sent": "So let's focus on this word, 4th from the bottom.",
                    "label": 0
                },
                {
                    "sent": "So there are two analysis here and what we're going to do is run kind of LST M over this to get a embedding of each of these routes plus the sequence of ethics is.",
                    "label": 0
                },
                {
                    "sent": "And we're just going to pull these together because we don't want to actually model which one was meant in context.",
                    "label": 0
                },
                {
                    "sent": "Go on and we can also say, well, we also know that characters worked pretty well in the past, so we can run a bidirectional STM over that and then we can also look up words if we happen to have an embedding of the word, or we can have some unknown word talking and we can sort of aggregate all of this stuff together.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can go to generate this and we are not going to sort of into great detail on this, But basically the output process is a little bit more complicated because we have several ways to generate.",
                    "label": 0
                },
                {
                    "sent": "We can either generate a word by spelling it out one character at a time.",
                    "label": 0
                },
                {
                    "sent": "We can also just sample it from a short dictionary, or we can generate a root and a sequence of affix is and ask the finite state transducer to give us the surface form, and we're going to marginalise all of these different things.",
                    "label": 0
                },
                {
                    "sent": "So we've basically got all of these different pieces.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question is, how well does this work, so these are very large numbers in part due to data.",
                    "label": 0
                },
                {
                    "sent": "We've run this on English and don't worry, the numbers are reasonable, but the thing to note here is that as we add more information over the characters, we do get better and better perplexity, so these are perplexities.",
                    "label": 0
                },
                {
                    "sent": "So lower is better and so this is.",
                    "label": 0
                },
                {
                    "sent": "This has been quite nice.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "My summary of this character versus word modeling stuff is so for those of us who are fortunate to just be able to work in English or maybe Chinese, the gains are maybe not hugely important, but certainly in morphologically rich languages or in domains where content is generated by users and people are spelling things out in sort of new and creative ways, it does matter quite a bit.",
                    "label": 1
                },
                {
                    "sent": "Another thing I didn't mention here, but is quite important is we can get away with far fewer parameters to represent words in this way, and this is quite interesting because it's you know we've in some of those cases on the language models, we were able to reduce the model size by an order of magnitude, which is really getting quite quite small, so this question of how much information do you need to store a language to know a language is looks very different than it used to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I want to move on to talk about somewhere else where we might be able to add a little bit of linguistic structural knowledge.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to move on for the second part of this talk to talking about syntax.",
                    "label": 0
                },
                {
                    "sent": "Which is of course you know where where the connectionists and the traditional symbolic types have traditionally like to really argue, so I am going to start out by claiming that it is not controversial that language is hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The details of this hierarchy are up for debate.",
                    "label": 0
                },
                {
                    "sent": "The theorists love to argue about this, but language is essentially hierarchical, and so you can see this in some cases like this.",
                    "label": 0
                },
                {
                    "sent": "So if I say.",
                    "label": 0
                },
                {
                    "sent": "The talk I gave did not appeal to anybody.",
                    "label": 1
                },
                {
                    "sent": "Well, that's wrong, so we have to get rid of the not right?",
                    "label": 1
                },
                {
                    "sent": "So the talk I gave appealed to anybody wait, but now it's not grammatical.",
                    "label": 0
                },
                {
                    "sent": "So this is an instance of what's called negative polarity item licensing, so all many languages.",
                    "label": 0
                },
                {
                    "sent": "Most languages have these negative polarity items that have to be present.",
                    "label": 0
                },
                {
                    "sent": "There has to be a negative word that licenses them, so a reasonable generalization hypothesis that both the linguist or machine learning algorithm might make is that for anybody to be present, it has to come after.",
                    "label": 0
                },
                {
                    "sent": "The word not or hardly or one of these other NPI licensors.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we can look at a second data point.",
                    "label": 0
                },
                {
                    "sent": "We can say the talk.",
                    "label": 0
                },
                {
                    "sent": "I did not give appealed to anybody.",
                    "label": 1
                },
                {
                    "sent": "And it's still word salad.",
                    "label": 0
                },
                {
                    "sent": "And actually there apparently are some dialects of English for some of these are kind of marginal.",
                    "label": 0
                },
                {
                    "sent": "I don't have to speak one, so pick your favorite language or the facts are a little more robust.",
                    "label": 0
                },
                {
                    "sent": "There are certainly other.",
                    "label": 0
                },
                {
                    "sent": "It's not just Npis.",
                    "label": 0
                },
                {
                    "sent": "As I said that that show this so.",
                    "label": 0
                },
                {
                    "sent": "The claim is that this is due to particular structural that.",
                    "label": 0
                },
                {
                    "sent": "Languages first organized into hierarchically organized structures and that particular conceptions of these structures are what license the presence of this, this anybody, and so you can see this on the left.",
                    "label": 0
                },
                {
                    "sent": "The talk I gave did not appeal to anyone, which is semantically bad because you all love my talk and but syntactically fine, you'll notice that the knot is in this particular.",
                    "label": 1
                },
                {
                    "sent": "Sort of, I am the uncle of the antibody relationship and in the right hand side where we've got the talk, I did not give appealed to anybody, which is the syntactically and semantically bad one.",
                    "label": 1
                },
                {
                    "sent": "We don't have that configuration, and it turns out that if you look for this phenomenon across languages, there are whole bunch of independent reasons for identifying that these kinds of groupings of sentence is that that people, language users perceive them and that they are the ones that license or that control.",
                    "label": 1
                },
                {
                    "sent": "Whether this will be judged as acceptable.",
                    "label": 1
                },
                {
                    "sent": "So really, the generalization is that not must in some sense structurally precede anybody.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are many theories.",
                    "label": 0
                },
                {
                    "sent": "As I said of the details of the structure and I think.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not going to speculate too much on what the linguists are up to, but I think you guys are on, you know, coming closer to getting important insights than they are at this point.",
                    "label": 1
                },
                {
                    "sent": "But I do want to emphasize that the psychology psychological reality of structural sensitivity is not empirically controversial.",
                    "label": 0
                },
                {
                    "sent": "There are marginal cases and languages that are claimed to be marginal, but we don't know what I want to contrast this with is a view where linearly the presence of an NPI would be the thing that governed, not.",
                    "label": 0
                },
                {
                    "sent": "We just don't see languages like that.",
                    "label": 0
                },
                {
                    "sent": "Although you could imagine for any sort of unbiased or sort of vaguely biased learner.",
                    "label": 0
                },
                {
                    "sent": "That would be a perfectly reasonable language to to acquire.",
                    "label": 0
                },
                {
                    "sent": "And finally, this is not I'm using Npis as a as an example here.",
                    "label": 0
                },
                {
                    "sent": "There are many other facts and language that show this kind of structural sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So what we've done in the last few months in my lab is build a attempt to take a very slight extension to recurrent neural networks and say they also we want to model sequences of words just like we do in language modeling.",
                    "label": 0
                },
                {
                    "sent": "But we also want to model.",
                    "label": 0
                },
                {
                    "sent": "We want to group the words into this kind of hierarchical configuration, and we want to see if this.",
                    "label": 0
                },
                {
                    "sent": "If this had some value so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just sort of 1 theory of hierarchy.",
                    "label": 1
                },
                {
                    "sent": "I don't think most linguists would particularly like this.",
                    "label": 0
                },
                {
                    "sent": "I think this is just a sort of minimal step in that direction.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do to the generation process is we're going to add what you might call some control symbols.",
                    "label": 0
                },
                {
                    "sent": "So as you're going along and generating a sequence of words periodically, what we're going to do is we're going to get a signal that says.",
                    "label": 0
                },
                {
                    "sent": "Now I want you to compress.",
                    "label": 1
                },
                {
                    "sent": "The last an items that you've generated and form a constituent and all that's going to mean for us is we're going to erase them from the history of our RNN, and we're going to run a composition function over them, and then we're going to replace those items on the, you know at the end of this RNN, so you can think about this for those of you who are familiar with shift reduce type parsing.",
                    "label": 0
                },
                {
                    "sent": "This is kind of similar to a reduce operation.",
                    "label": 0
                },
                {
                    "sent": "We are shifting is we're not parsing here we're generating, so it's going to look a little bit different.",
                    "label": 0
                },
                {
                    "sent": "But that's sort of the build constituent operation if you will.",
                    "label": 0
                },
                {
                    "sent": "So the RNN then has to predict basically the next symbol rather than just conditional on a sequence of previously predicted output symbols.",
                    "label": 1
                },
                {
                    "sent": "It should predict the next symbol based on a mixture of terminal symbols, an possibly composed constituents.",
                    "label": 0
                },
                {
                    "sent": "It will decide what what comes next, but it also has to predict these control symbols which are going to decide how big the constituents are.",
                    "label": 1
                },
                {
                    "sent": "So we're going to call these recurrent neural network grammars, and all this means is that, well, they're based on recurrent neural networks is sort of driving these things and their grammars because they're going to be jointly generating affectively these tree structures and the sequence of words, which is all the grammar does.",
                    "label": 1
                },
                {
                    "sent": "So I think the easiest way to see this is just to go through the example generation of the sentence and you'll see examples of these control symbols which at least for the channel peers in the audience will look familiar, and the mechanics of.",
                    "label": 0
                },
                {
                    "sent": "Sort of what it means to compose something into a composition.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start as we always do by generating a special symbol called S and we're going to push this onto a stack, and the decisions that so on the left I'm going to write out the terminals that are being generated by this and in the middle I'm going to write out the symbols that the RNN is conditioning on when it's making the next decision.",
                    "label": 0
                },
                {
                    "sent": "So we might generate an NP.",
                    "label": 0
                },
                {
                    "sent": "These are non terminal symbols.",
                    "label": 0
                },
                {
                    "sent": "These are these control symbols and now finally I'm going to get down to generating a terminal symbol V and we'll put it now in two places and we'll build up the hungry cat and at this point we might say, well, I have built what I think is a single constituent an I want to in the future treat this as a unit for any future predictions so.",
                    "label": 0
                },
                {
                    "sent": "If we predict a reduce operation here, you can think about this as closing this parentheses and that's when I said, you know, the control operations are there to decide sort of how big these constituents are.",
                    "label": 0
                },
                {
                    "sent": "They also sort of define the type and things like that.",
                    "label": 0
                },
                {
                    "sent": "So we've got the sequence of four things at the top of the stack, which represent kind of a kind of unit.",
                    "label": 0
                },
                {
                    "sent": "And what we do then is we pop those off and we replace.",
                    "label": 0
                },
                {
                    "sent": "That those four symbols with an embedding of a single symbol representing this entire noun phrase, and of course you know everyone, is familiar with Richard searchers, recursive neural network.",
                    "label": 0
                },
                {
                    "sent": "This is very much this idea, so we then replace those four symbols with this one, and the process continues and we complete.",
                    "label": 0
                },
                {
                    "sent": "We finish this process rather than when we get to a stop symbol when we instead have a single.",
                    "label": 0
                },
                {
                    "sent": "Element, a single constituent constituent, left on the stack so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we do this syntactic composition?",
                    "label": 1
                },
                {
                    "sent": "I just want to say a couple of words about this, because again, this is somewhere where we found it interesting to put in some linguistic structure.",
                    "label": 0
                },
                {
                    "sent": "So the thing to note about this is this is designated as an NP a noun phrase, and there's this theory in linguistics, which is basically a empirical generalization that constituents have a particular word in them that is, is their head.",
                    "label": 0
                },
                {
                    "sent": "It's gives most of the properties too.",
                    "label": 0
                },
                {
                    "sent": "That phrase, so basically, if you get rid of all the words in here, you're probably the one you would want to keep this cat and we'd call that the head.",
                    "label": 0
                },
                {
                    "sent": "But of course, you know linguists don't necessarily argue about the existence of heads.",
                    "label": 0
                },
                {
                    "sent": "They do argue about which one is the head, so some people think it's So what we're going to do is we're going to build the representation as follows.",
                    "label": 0
                },
                {
                    "sent": "We're going to start off and say, well, I'm in NP.",
                    "label": 0
                },
                {
                    "sent": "I'm going to read this into an RNN, and I'm going to sort of notify the RN.",
                    "label": 0
                },
                {
                    "sent": "What is it you're looking for?",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to read from left to right, the embeddings of the children, until I get to a stop symbol, and then this is going to be left to right embedding of this week.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also repeat this process from right to left.",
                    "label": 0
                },
                {
                    "sent": "Again, we're going to start with this NP because we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to sort of look for the head that we we need to find.",
                    "label": 0
                },
                {
                    "sent": "Repeat this process and then we combine the two.",
                    "label": 0
                },
                {
                    "sent": "Now of course this is a.",
                    "label": 0
                },
                {
                    "sent": "We have terminal symbols here down at the bottom.",
                    "label": 0
                },
                {
                    "sent": "In general, we're going to have other recursively composed representations.",
                    "label": 0
                },
                {
                    "sent": "So if instead of hungry we had.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The adjectival phrase, very hungry this would have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Course, just have an embedding in the same space and we would compute the representation of this more complicated constituent in the completely straightforward way.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is, you know, sort of minor extensions to existing recursive neural network architectures.",
                    "label": 0
                },
                {
                    "sent": "We kind of like this because it sort of captures this notion of headedness and can deal with any number of children.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk a little bit here about some of the empirical work we've we've done with this stuff so.",
                    "label": 0
                },
                {
                    "sent": "Our energies are joint models of sequences and words together with a tree structure, and so this means we can ask questions about you, know any of the conditional or marginal distributions that this gives rise to, such as what's the mode of the conditional distribution over trees given the sentence, also known as parsing, and we can also ask how well does this work as a language model, and the hope is that it will work well at both.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns out that any sequence of these actions that I've given you can convert that to a tree structure, and vice versa.",
                    "label": 1
                },
                {
                    "sent": "So we are just training this in a fully supervised way.",
                    "label": 0
                },
                {
                    "sent": "At this point we could of course treat the you know these non generation these control actions as latent variables will learn them with our L and turn this into a grammar induction problem.",
                    "label": 1
                },
                {
                    "sent": "I think this is super exciting and we're looking into this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for now I'm going to just talk about the supervised results, so right, so we've got this joint distribution and we're interested in these two inference questions.",
                    "label": 1
                },
                {
                    "sent": "P of X for a given X&P of Y given X forgiven X so traditionally those of you who have worked with other probabilistic grammars know that.",
                    "label": 1
                },
                {
                    "sent": "Well, here's where we.",
                    "label": 0
                },
                {
                    "sent": "Usually we have this nasty summation problem.",
                    "label": 0
                },
                {
                    "sent": "There are an exponential or maybe worse number of trees.",
                    "label": 1
                },
                {
                    "sent": "For any sentence, we need to marginalized their search over that.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, because we condition on everything with RNN's that goes away.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately we can come up with a very nice important sampling algorithm, and we have a very convenient proposal distribution here where we discriminatively train this parser on sentence is an I don't have time to go into the details of this.",
                    "label": 0
                },
                {
                    "sent": "We've got a paper coming out at nacle this summer on this.",
                    "label": 0
                },
                {
                    "sent": "If you want to read about it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so briefly I want to show some parsing results, so two things to note.",
                    "label": 1
                },
                {
                    "sent": "First, the discriminative model here is.",
                    "label": 1
                },
                {
                    "sent": "Not actually as good as the generative model.",
                    "label": 0
                },
                {
                    "sent": "This is a bit unusual and.",
                    "label": 0
                },
                {
                    "sent": "Kind of exciting to see I think you know, since.",
                    "label": 0
                },
                {
                    "sent": "Grammars sort of.",
                    "label": 0
                },
                {
                    "sent": "The syntax that underlies the Penn Treebank was designed to was designed more to explain how sentences came to be.",
                    "label": 0
                },
                {
                    "sent": "The fact that we've got sort of a good match for the generative process might explain some of this.",
                    "label": 0
                },
                {
                    "sent": "The thing that I also found.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very interesting here is that as a language model, this is actually quite good, so we worked very hard to get very solid basslines on here 2 tasks, English and Chinese, and we found that we do outperform these quite nicely.",
                    "label": 0
                },
                {
                    "sent": "So so this is shows that basically a single model can be both a good parser and a good language model, and it's been a very long time since that's been the case.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to conclude here facts about language, so arbitrariness and compositionality existed all levels.",
                    "label": 0
                },
                {
                    "sent": "I hope I've somewhat convinced you that language is sensitive to hierarchy, not just bear strings and my sort of guiding hypothesis has been that if we pay attention to these things when designing our models will end up with better models than when we ignore them.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks and I'll take any questions.",
                    "label": 0
                }
            ]
        }
    }
}