{
    "id": "sf62wtc7ggxvjax2gsvj3eflwjmyj6gn",
    "title": "Graphical Models via Generalized Linear Models",
    "info": {
        "author": [
            "Eunho Yang, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization",
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Machine Learning->Structured Data"
        ]
    },
    "url": "http://videolectures.net/nips2012_yang_models/",
    "segmentation": [
        [
            "My name is, you know, young from the University of Texas, Austin.",
            "Going to present the paper, graphical models PR generalized linear models.",
            "And this is the joint work with Professor Pretty Bobby Kumar.",
            "Genebra, Ellen and junglee."
        ],
        [
            "As background, let me briefly refresh your memory on undirected graphical models or Markov random fields.",
            "Undirected graphical model specifies a joint distribution over multiple random variables using graphs.",
            "So let's say we are interested in the random vector X with P random variables.",
            "And we have a undirected graph G / P notes corresponding to P random variables.",
            "Then the graph can be understood in a way that if there is no edge between 2 random variables, they are conditionally independent.",
            "Given the rest of the nodes.",
            "By Hammersley, Clifford theorem, given the graph structure, the joint distribution can be written as the product of local factors feature, which depends only on a click or fully connected sub graph.",
            "So in this figure, for example, if you have three clicks AB&C, then the joint probability distribution can be factorized over the potential functions on clicks AB&C.",
            "Graphical models are really useful in many real world applications."
        ],
        [
            "Where are we have huge number of variables with rich dependencies between them.",
            "So for example, in statistical physics we use Ising model for binary categorical data where variables represent the spins of the molecules and so on.",
            "Or in image processing, as another example.",
            "We use Potts model for general categorical data, where the variables here are the pixel values.",
            "And sometimes we have continuous data in many applique."
        ],
        [
            "Such as microarray data in bioinformatics for continuous multivariate data.",
            "The popular way is to assume that data follows Gaussian, an fit the cash fit.",
            "Caution graphical models, Gaussian MRF.",
            "So for these Ising model pass model or caution MRF several methods have been proposed so far for inference or learning.",
            "Our problems with strong statistical guarantees.",
            "But what if?"
        ],
        [
            "Actual data is not Gaussian nor categorical.",
            "For example, we might want to model the continuous but skewed data like.",
            "Network school time.",
            "Time spent on multiple websites, and so on where we might want to multi variate count data like website visit counts, incident reports or next generation genomic data based on RNA fragment counts.",
            "For these types of data, the standard inference or learning techniques may not be suitable because most of of the existing works only valid for kacian or categorical graphical models.",
            "So one way to handle these types of data is Gaussian Ising the data 1st and then feeding cows MRF, which is actually equivalent to caution copula.",
            "While we have some other approaches.",
            "But still they have some problems.",
            "But really what we really want is directly fit the parametric graphical models."
        ],
        [
            "So then what can we do to find the solution?",
            "Let's revisit revisit the simple univariate case.",
            "For univariate data we have a good understanding of picking appropriate static models for different types of data.",
            "To name a few, we can fit the Gaussian distribution for detailed continuous data, we can use exponential distribution.",
            "For time interval data, and similarly we can use personally or multinomial distribution for categorical data.",
            "And we have person distribution for account data and we have so many other universe distributions for different types of data.",
            "So here is our key question.",
            "If I have one."
        ],
        [
            "Count values variable.",
            "I notified the person distribution, but what if I have 100 such count values variables?",
            "Then how do I feed the joint distribution?",
            "So can you systematically from the univariate distribution to multivariate graphical models?",
            "For caution case we have multi very caution.",
            "Multivariate Gaussian distribution or Gaussian MRF and similarly for our newly distribution we have Ising model.",
            "But"
        ],
        [
            "What about other cases?",
            "What about other University distributions?",
            "So we start from the University exponential family distribution, the exponential family."
        ],
        [
            "Capture the broad class of the most interesting universe, univariate distribution we care about and they have this form exponential of linear combination of the sufficient statistics.",
            "Here the function B is the sufficient statistics data is the parameter anadia is log normalization constant.",
            "So."
        ],
        [
            "This work we extend University exponential family distribution to multi very graphical models.",
            "So we know that for caution MRF each variable follows universal constant distribution, keeping all other random variables.",
            "Similarly for Ising model, each variable follows Bernoulli distribution.",
            "Given the rest of the nodes.",
            "So more generally, we introduce a new class of graphical models.",
            "So here's our simple and natural idea.",
            "Each node conditioned on everybody else, follows some univariate exponential family distribution, while the Wilder dependencies between the random variables.",
            "Are modeled by the graphical model structure."
        ],
        [
            "Then the question is, would there always be a joint distribution?",
            "And what is the form of our joint distribution we should take for this new family of graphical models?"
        ],
        [
            "So let's approach the questions by making the most general assumptions possible.",
            "First, we assume that the North conditional distribution.",
            "I specified by the University exponential family distribution, so in this equation conditional distribution is exponential family form.",
            "Here the sufficient statistics B and the function C are given by our choice and for the parameter of exponential family.",
            "I'm going to allow any general function on the rest of the note.",
            "So that's the function E in the equation.",
            "And at the same time I want my joint distribution to be a graphical model with clique size at most K. So these two conditions are very general and we're not losing anything here.",
            "So we're just assuming that my load conditional distribution follows some univariate exponential family distribution and might."
        ],
        [
            "Joint distribution is going to be a graphical model, that's it.",
            "Then the question is again, or will the joint distribution exist?",
            "First of all, an what form will it take if it, if it exists?"
        ],
        [
            "OK, we have a theorem that says that under the previous general assumptions, the joint distribution always exists and necessarily has the following form and it is also exponential family form with sufficient statistics given as the tensor tensor product for tensor product functions of the function be with Kate order dependencies.",
            "So here K is specified by the previous assumption, the maximum clique size an what is the function be?",
            "Function B was the sufficient statistics of the nodes conditional University exponential family distribution of our choice in the previous assumption."
        ],
        [
            "So that's our theorem, and let's look at some example.",
            "If you focus on the pairwise graphical model, that is, tensor products of size at most two, and the linear sufficient statistics for the function B, then the joint distribution necessarily has the following form."
        ],
        [
            "And it's not conditional.",
            "Distribution has to become so called generalized linear models where the parameter of the exponential family is the linear combination of all other random variables."
        ],
        [
            "K this pairwise so family actually includes Gaussian MRF for Ising models, but besides them we can also get the multivariate prasan graphical models if we choose.",
            "The domain of X is the non negative integer an the function C is minus log right?",
            "Then the joint distribution or person graphical models is like this and it's North conditional distribution follows University person distribution where the parameter of the Canonical parameter of the person distribution is the linear combination of all other random variables or other neighbors.",
            "So one caveat here is that the person graphical models are restricted to capture only the negative dependencies between the random variables.",
            "That is the theater St should be negative.",
            "I mean data is in the joint distribution, otherwise the log partition function will blow up.",
            "K so this is another example."
        ],
        [
            "Exponential graphical models where the node conditional distribution now follows exponential distribution.",
            "Again, for exponential graphical models, the parameter DST should be negative for some of log partition function.",
            "Ann, this pairwise family includes many other graphical models such that node conditional distribution follows exponential University exponential family distribution."
        ],
        [
            "K The second part of the paper is on the structure learning for our new family."
        ],
        [
            "Models.",
            "So in this task we are given an IID samples from the true underlying unknown distribution and our goal is to estimate the graph structure, and by Hammersley Clifford theorem.",
            "This is equivalent to finding the nonzero DST in the joint probability distribution.",
            "For simplicity, here we are going to focus only on the pairwise cell family."
        ],
        [
            "This is our learning method.",
            "I would be very brief here.",
            "All learning method is naturally suggested by the way we construct it the way we construct it our graphical models.",
            "So instead of learning graph structure jointly, we learn the node conditional distribution on the sparsity constraint.",
            "So this allows us to estimate the neighborhood structure of each node individually.",
            "So finally we can recover the overall graph structure."
        ],
        [
            "OK, this is our main theorem of the paper, which is on the statistical guarantees of our learning method.",
            "So under under the appropriate conditions that are standard in high dimensional literature and the appropriate selection of the regularization parameter and if the number of samples and is greater than the scale log PUD is the maximum node degree, then we have the solution is unique and the neighborhood estimation is correct with high probability.",
            "So, beyond the Ising model or Gaussian MRF, we can successfully recover for successfully recover the graph structure for any pairwise.",
            "So family of our graph or model by this theorem.",
            "That's our main theorem."
        ],
        [
            "And let's move on to the experiment results.",
            "The first experiment is on the simulated data set, where we know the true distribution to confirm the threshold behavior.",
            "By our theorem, here, we assume that.",
            "So here we assume the person graphical model with four nearest nearest neighbor grid structure.",
            "As a true distribution.",
            "So in the left figure the X axis is the number of samples we are given an each point on each curve represents the empirical success probability of overall overall graph structure recovery out of 50 trials.",
            "That's the Y axis, and we have four different curves for different problem sizes.",
            "So if you get the right figure, we re scale the X axis motivated by our DRAM, that's M by log P. Then we can see that all different cards are well aligned."
        ],
        [
            "For real world problems, we applied our learning so we apply our method to learning two genomic networks.",
            "To demonstrate the versatility of our models.",
            "So here we have micro RNA expression data which is count across about 500 breast cancer patients.",
            "So to learn the interactions of micro RNA fragments, we wanted to fit the person graphical model.",
            "But since the person graphical models only allow the negative dependencies, so we.",
            "Clustered similar micro RNA fragments and then fit the graphical model over the cluster centroids.",
            "So this is the graph structure.",
            "We got an Interestingly 3 hub nodes that we found are actually very relevant.",
            "Known to be very relevant, so other hub nodes that we found around for the research in this field."
        ],
        [
            "And in the Rustics last experiment, we learned the copy number aberration network for some brain cancer.",
            "So here each node represents the copy number variation of a particular genomic reason.",
            "And the data type of copy number variation is categorized into one of three groups.",
            "Normal, amplified or delete it.",
            "So we fit the multinomial graphical models and got the aberration network as shown in the figure.",
            "So in conclusion."
        ],
        [
            "Brought in the class of off the shelf graphical models by extending univariate exponential family distribution to multi variate graphical models.",
            "So this allows us to use graphical model machinery beyond binary or categorical location data and we also show that we can estimate such graphical models on there some standard conditions.",
            "So that's yeah, this is the end of my presentation."
        ],
        [
            "Thank you so much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is, you know, young from the University of Texas, Austin.",
                    "label": 1
                },
                {
                    "sent": "Going to present the paper, graphical models PR generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "And this is the joint work with Professor Pretty Bobby Kumar.",
                    "label": 0
                },
                {
                    "sent": "Genebra, Ellen and junglee.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As background, let me briefly refresh your memory on undirected graphical models or Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "Undirected graphical model specifies a joint distribution over multiple random variables using graphs.",
                    "label": 1
                },
                {
                    "sent": "So let's say we are interested in the random vector X with P random variables.",
                    "label": 0
                },
                {
                    "sent": "And we have a undirected graph G / P notes corresponding to P random variables.",
                    "label": 0
                },
                {
                    "sent": "Then the graph can be understood in a way that if there is no edge between 2 random variables, they are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "Given the rest of the nodes.",
                    "label": 0
                },
                {
                    "sent": "By Hammersley, Clifford theorem, given the graph structure, the joint distribution can be written as the product of local factors feature, which depends only on a click or fully connected sub graph.",
                    "label": 1
                },
                {
                    "sent": "So in this figure, for example, if you have three clicks AB&C, then the joint probability distribution can be factorized over the potential functions on clicks AB&C.",
                    "label": 0
                },
                {
                    "sent": "Graphical models are really useful in many real world applications.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where are we have huge number of variables with rich dependencies between them.",
                    "label": 0
                },
                {
                    "sent": "So for example, in statistical physics we use Ising model for binary categorical data where variables represent the spins of the molecules and so on.",
                    "label": 1
                },
                {
                    "sent": "Or in image processing, as another example.",
                    "label": 1
                },
                {
                    "sent": "We use Potts model for general categorical data, where the variables here are the pixel values.",
                    "label": 0
                },
                {
                    "sent": "And sometimes we have continuous data in many applique.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such as microarray data in bioinformatics for continuous multivariate data.",
                    "label": 1
                },
                {
                    "sent": "The popular way is to assume that data follows Gaussian, an fit the cash fit.",
                    "label": 1
                },
                {
                    "sent": "Caution graphical models, Gaussian MRF.",
                    "label": 0
                },
                {
                    "sent": "So for these Ising model pass model or caution MRF several methods have been proposed so far for inference or learning.",
                    "label": 0
                },
                {
                    "sent": "Our problems with strong statistical guarantees.",
                    "label": 0
                },
                {
                    "sent": "But what if?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actual data is not Gaussian nor categorical.",
                    "label": 1
                },
                {
                    "sent": "For example, we might want to model the continuous but skewed data like.",
                    "label": 0
                },
                {
                    "sent": "Network school time.",
                    "label": 0
                },
                {
                    "sent": "Time spent on multiple websites, and so on where we might want to multi variate count data like website visit counts, incident reports or next generation genomic data based on RNA fragment counts.",
                    "label": 1
                },
                {
                    "sent": "For these types of data, the standard inference or learning techniques may not be suitable because most of of the existing works only valid for kacian or categorical graphical models.",
                    "label": 0
                },
                {
                    "sent": "So one way to handle these types of data is Gaussian Ising the data 1st and then feeding cows MRF, which is actually equivalent to caution copula.",
                    "label": 0
                },
                {
                    "sent": "While we have some other approaches.",
                    "label": 0
                },
                {
                    "sent": "But still they have some problems.",
                    "label": 0
                },
                {
                    "sent": "But really what we really want is directly fit the parametric graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then what can we do to find the solution?",
                    "label": 0
                },
                {
                    "sent": "Let's revisit revisit the simple univariate case.",
                    "label": 0
                },
                {
                    "sent": "For univariate data we have a good understanding of picking appropriate static models for different types of data.",
                    "label": 1
                },
                {
                    "sent": "To name a few, we can fit the Gaussian distribution for detailed continuous data, we can use exponential distribution.",
                    "label": 1
                },
                {
                    "sent": "For time interval data, and similarly we can use personally or multinomial distribution for categorical data.",
                    "label": 0
                },
                {
                    "sent": "And we have person distribution for account data and we have so many other universe distributions for different types of data.",
                    "label": 0
                },
                {
                    "sent": "So here is our key question.",
                    "label": 0
                },
                {
                    "sent": "If I have one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Count values variable.",
                    "label": 0
                },
                {
                    "sent": "I notified the person distribution, but what if I have 100 such count values variables?",
                    "label": 0
                },
                {
                    "sent": "Then how do I feed the joint distribution?",
                    "label": 0
                },
                {
                    "sent": "So can you systematically from the univariate distribution to multivariate graphical models?",
                    "label": 1
                },
                {
                    "sent": "For caution case we have multi very caution.",
                    "label": 0
                },
                {
                    "sent": "Multivariate Gaussian distribution or Gaussian MRF and similarly for our newly distribution we have Ising model.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about other cases?",
                    "label": 0
                },
                {
                    "sent": "What about other University distributions?",
                    "label": 1
                },
                {
                    "sent": "So we start from the University exponential family distribution, the exponential family.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Capture the broad class of the most interesting universe, univariate distribution we care about and they have this form exponential of linear combination of the sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "Here the function B is the sufficient statistics data is the parameter anadia is log normalization constant.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work we extend University exponential family distribution to multi very graphical models.",
                    "label": 0
                },
                {
                    "sent": "So we know that for caution MRF each variable follows universal constant distribution, keeping all other random variables.",
                    "label": 0
                },
                {
                    "sent": "Similarly for Ising model, each variable follows Bernoulli distribution.",
                    "label": 1
                },
                {
                    "sent": "Given the rest of the nodes.",
                    "label": 0
                },
                {
                    "sent": "So more generally, we introduce a new class of graphical models.",
                    "label": 1
                },
                {
                    "sent": "So here's our simple and natural idea.",
                    "label": 1
                },
                {
                    "sent": "Each node conditioned on everybody else, follows some univariate exponential family distribution, while the Wilder dependencies between the random variables.",
                    "label": 0
                },
                {
                    "sent": "Are modeled by the graphical model structure.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the question is, would there always be a joint distribution?",
                    "label": 0
                },
                {
                    "sent": "And what is the form of our joint distribution we should take for this new family of graphical models?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's approach the questions by making the most general assumptions possible.",
                    "label": 1
                },
                {
                    "sent": "First, we assume that the North conditional distribution.",
                    "label": 1
                },
                {
                    "sent": "I specified by the University exponential family distribution, so in this equation conditional distribution is exponential family form.",
                    "label": 0
                },
                {
                    "sent": "Here the sufficient statistics B and the function C are given by our choice and for the parameter of exponential family.",
                    "label": 0
                },
                {
                    "sent": "I'm going to allow any general function on the rest of the note.",
                    "label": 1
                },
                {
                    "sent": "So that's the function E in the equation.",
                    "label": 0
                },
                {
                    "sent": "And at the same time I want my joint distribution to be a graphical model with clique size at most K. So these two conditions are very general and we're not losing anything here.",
                    "label": 0
                },
                {
                    "sent": "So we're just assuming that my load conditional distribution follows some univariate exponential family distribution and might.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint distribution is going to be a graphical model, that's it.",
                    "label": 1
                },
                {
                    "sent": "Then the question is again, or will the joint distribution exist?",
                    "label": 0
                },
                {
                    "sent": "First of all, an what form will it take if it, if it exists?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we have a theorem that says that under the previous general assumptions, the joint distribution always exists and necessarily has the following form and it is also exponential family form with sufficient statistics given as the tensor tensor product for tensor product functions of the function be with Kate order dependencies.",
                    "label": 1
                },
                {
                    "sent": "So here K is specified by the previous assumption, the maximum clique size an what is the function be?",
                    "label": 1
                },
                {
                    "sent": "Function B was the sufficient statistics of the nodes conditional University exponential family distribution of our choice in the previous assumption.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's our theorem, and let's look at some example.",
                    "label": 0
                },
                {
                    "sent": "If you focus on the pairwise graphical model, that is, tensor products of size at most two, and the linear sufficient statistics for the function B, then the joint distribution necessarily has the following form.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's not conditional.",
                    "label": 0
                },
                {
                    "sent": "Distribution has to become so called generalized linear models where the parameter of the exponential family is the linear combination of all other random variables.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K this pairwise so family actually includes Gaussian MRF for Ising models, but besides them we can also get the multivariate prasan graphical models if we choose.",
                    "label": 1
                },
                {
                    "sent": "The domain of X is the non negative integer an the function C is minus log right?",
                    "label": 0
                },
                {
                    "sent": "Then the joint distribution or person graphical models is like this and it's North conditional distribution follows University person distribution where the parameter of the Canonical parameter of the person distribution is the linear combination of all other random variables or other neighbors.",
                    "label": 0
                },
                {
                    "sent": "So one caveat here is that the person graphical models are restricted to capture only the negative dependencies between the random variables.",
                    "label": 1
                },
                {
                    "sent": "That is the theater St should be negative.",
                    "label": 0
                },
                {
                    "sent": "I mean data is in the joint distribution, otherwise the log partition function will blow up.",
                    "label": 1
                },
                {
                    "sent": "K so this is another example.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exponential graphical models where the node conditional distribution now follows exponential distribution.",
                    "label": 1
                },
                {
                    "sent": "Again, for exponential graphical models, the parameter DST should be negative for some of log partition function.",
                    "label": 1
                },
                {
                    "sent": "Ann, this pairwise family includes many other graphical models such that node conditional distribution follows exponential University exponential family distribution.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K The second part of the paper is on the structure learning for our new family.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "So in this task we are given an IID samples from the true underlying unknown distribution and our goal is to estimate the graph structure, and by Hammersley Clifford theorem.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to finding the nonzero DST in the joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, here we are going to focus only on the pairwise cell family.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our learning method.",
                    "label": 0
                },
                {
                    "sent": "I would be very brief here.",
                    "label": 0
                },
                {
                    "sent": "All learning method is naturally suggested by the way we construct it the way we construct it our graphical models.",
                    "label": 1
                },
                {
                    "sent": "So instead of learning graph structure jointly, we learn the node conditional distribution on the sparsity constraint.",
                    "label": 1
                },
                {
                    "sent": "So this allows us to estimate the neighborhood structure of each node individually.",
                    "label": 1
                },
                {
                    "sent": "So finally we can recover the overall graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is our main theorem of the paper, which is on the statistical guarantees of our learning method.",
                    "label": 1
                },
                {
                    "sent": "So under under the appropriate conditions that are standard in high dimensional literature and the appropriate selection of the regularization parameter and if the number of samples and is greater than the scale log PUD is the maximum node degree, then we have the solution is unique and the neighborhood estimation is correct with high probability.",
                    "label": 1
                },
                {
                    "sent": "So, beyond the Ising model or Gaussian MRF, we can successfully recover for successfully recover the graph structure for any pairwise.",
                    "label": 1
                },
                {
                    "sent": "So family of our graph or model by this theorem.",
                    "label": 0
                },
                {
                    "sent": "That's our main theorem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let's move on to the experiment results.",
                    "label": 0
                },
                {
                    "sent": "The first experiment is on the simulated data set, where we know the true distribution to confirm the threshold behavior.",
                    "label": 1
                },
                {
                    "sent": "By our theorem, here, we assume that.",
                    "label": 0
                },
                {
                    "sent": "So here we assume the person graphical model with four nearest nearest neighbor grid structure.",
                    "label": 1
                },
                {
                    "sent": "As a true distribution.",
                    "label": 0
                },
                {
                    "sent": "So in the left figure the X axis is the number of samples we are given an each point on each curve represents the empirical success probability of overall overall graph structure recovery out of 50 trials.",
                    "label": 1
                },
                {
                    "sent": "That's the Y axis, and we have four different curves for different problem sizes.",
                    "label": 0
                },
                {
                    "sent": "So if you get the right figure, we re scale the X axis motivated by our DRAM, that's M by log P. Then we can see that all different cards are well aligned.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For real world problems, we applied our learning so we apply our method to learning two genomic networks.",
                    "label": 0
                },
                {
                    "sent": "To demonstrate the versatility of our models.",
                    "label": 0
                },
                {
                    "sent": "So here we have micro RNA expression data which is count across about 500 breast cancer patients.",
                    "label": 1
                },
                {
                    "sent": "So to learn the interactions of micro RNA fragments, we wanted to fit the person graphical model.",
                    "label": 0
                },
                {
                    "sent": "But since the person graphical models only allow the negative dependencies, so we.",
                    "label": 0
                },
                {
                    "sent": "Clustered similar micro RNA fragments and then fit the graphical model over the cluster centroids.",
                    "label": 0
                },
                {
                    "sent": "So this is the graph structure.",
                    "label": 0
                },
                {
                    "sent": "We got an Interestingly 3 hub nodes that we found are actually very relevant.",
                    "label": 1
                },
                {
                    "sent": "Known to be very relevant, so other hub nodes that we found around for the research in this field.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the Rustics last experiment, we learned the copy number aberration network for some brain cancer.",
                    "label": 1
                },
                {
                    "sent": "So here each node represents the copy number variation of a particular genomic reason.",
                    "label": 0
                },
                {
                    "sent": "And the data type of copy number variation is categorized into one of three groups.",
                    "label": 0
                },
                {
                    "sent": "Normal, amplified or delete it.",
                    "label": 1
                },
                {
                    "sent": "So we fit the multinomial graphical models and got the aberration network as shown in the figure.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brought in the class of off the shelf graphical models by extending univariate exponential family distribution to multi variate graphical models.",
                    "label": 1
                },
                {
                    "sent": "So this allows us to use graphical model machinery beyond binary or categorical location data and we also show that we can estimate such graphical models on there some standard conditions.",
                    "label": 1
                },
                {
                    "sent": "So that's yeah, this is the end of my presentation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}