{
    "id": "yoaglkrbcwea243rofw5u6ac7iiasyuw",
    "title": "Top-down vs. bottom-up methods for hierarchical classification",
    "info": {
        "author": [
            "Claudio Gentile, University of Insubria"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/oh06_gentile_tdvbm/",
    "segmentation": [
        [
            "Shall we start our plans?",
            "Yeah, I'll be talking about hierarchical classification.",
            "And basically.",
            "I'll tell you about recent feature.",
            "Recent research we've been doing that I've been doing with these two guys, Nicole Oceans of Yankee and.",
            "Luca is anybody?",
            "OK. Hierarchical classification is a very well studied research subject.",
            "There have been many papers on hierarchical classifications."
        ],
        [
            "This is colder in Sami domain Chen.",
            "This wasn't ACNL 97.",
            "Hoffman, Jeremy to an all this is horosho at all.",
            "More recent stuff.",
            "So there are many many models of hierarchy of classification.",
            "There is no real.",
            "Have a, say agreement of what the best model is and many models means many algorithms, many approaches and so on.",
            "Top down approaches, bottom up approaches local global.",
            "Online match.",
            "Pachinko allocation methods and so on.",
            "There are quite a lot.",
            "The outline of this talk is the following.",
            "First of all, I will introduce my own hierarchical classification framework.",
            "Then I'll be talking about bottom up algorithm explaining the moment, but bottom up is which we call BS VM.",
            "It's a combination of.",
            "Bayes optimal classifier for a given.",
            "Model generating the labels will see in a moment with SVM.",
            "It's a base optimal classifier with respect to, you know.",
            "Model that generates subtrees as labels, 'cause here labels are subtrees.",
            "And then I report some report on some experiments we've made comparing this PST EM algorithm, which is a bottom up and global approach to a baseline algorithm which is called HVM, which.",
            "As mentioned the previous talk as well on both artificial and real world medium size datasets, and this is basically experimental work.",
            "And then I'll talk about some you know.",
            "More theoretical work.",
            "It's an online.",
            "It's a regret analysis of an online algorithm.",
            "Talk about the model specific parametric model for the labels.",
            "An algorithm derived from this model, and I regret analysis.",
            "So it's a half and half.",
            "Experimental theoretical."
        ],
        [
            "So the model.",
            "In our case, hierarchy is given to us ahead of time.",
            "You're given a hierarchy hierarchy, just a taxonomy is just a tree forest.",
            "A bunch of trees.",
            "You see one taxonomy on the left.",
            "The same taxonomy is on the right label.",
            "What we call a multilabel.",
            "Actually it's just a way of.",
            "Picking nodes within this taxonomy in some way that you know is consistent with that taxonomy.",
            "Idea of structuring things and multi label is a legal multi label if it is a union of paths within the tree within the trees.",
            "So for instance 1 two is a path 1 three is another path.",
            "6A10 is another path and so the multi label 12368 ten is a legal multi label OK. What it is?",
            "Yes, basically it's Supper Club Union of parts, meaning that whenever you choose something here, everything on.",
            "On the top should be to be chosen.",
            "On the right hand side you see.",
            "And illegal mostly label not up or closed.",
            "Basically if you choose this one, you have to choose this one as well.",
            "And this is not a legal multilabel.",
            "We can, of course, associated with each you know, multi label binary vectoring with the obvious meaning.",
            "This one is chosen.",
            "This one is chosen.",
            "This one, this one is not chosen and so on, OK. An example it's a pair.",
            "X is an instance vector for simplicity, and the is the associated legal multilabel.",
            "One question, are you doing also with higher fees, or just with Reese's Moralistic case would be harvest tags, for instance.",
            "Yeah, we could.",
            "We could extend it to dogs, but this case I'm talking Bout 3 users, but there are some subtleties around in the theoretical side, but I think we are pretty confident that we could replace the tags so the input X is a tree or axis.",
            "Whatever you like.",
            "You could kernelized this thing.",
            "So X could be, you know, everything that is compatible with discover machinery.",
            "You can think of X to be a bunch of real numbers.",
            "So where is the tree structure here?",
            "Is that restructure the output?",
            "So I mean it may be mentioned in the document classifications, I will I will OK X is just a way of encoding and document.",
            "K bag of words, something and document can be classified, not just being a single topic, say talk about soccer here, but we can talk about, you know.",
            "What's what's about sports, sports and politics and soccer as well?",
            "That really is part of the prior knowledge.",
            "Yeah, yeah, it's given it's given."
        ],
        [
            "How do we generate multi labels?",
            "We are given an instance, say a document for instance.",
            "And we associate with a multi label random binary vector Capital V and this capital N is the number of topics.",
            "The number of nodes in the tree.",
            "And we basically do the following.",
            "The conditional distribution of G of V given X is the product of these probability factors.",
            "Here what we do is to associate.",
            "We basically build a simple generative model for the labels.",
            "We ask the shade with each node in the hierarchy, a conditional distribution pizza by of X is the probability is the distribution of.",
            "These are by the value associated with this note I given the value associated with the parent node.",
            "And the instance being X OK, so for instance here.",
            "Yeah.",
            "You see this little hierarchy on the on the left.",
            "Basically do.",
            "Distribution of V1V5.",
            "Since we have only five nodes, even X.",
            "Is given by.",
            "Be one of X.",
            "Let me write it.",
            "Go away.",
            "So the probability of.",
            "The one given X times the probability of B2 given the parent that parent node, so V2 given P1 and then X times the probability of V3 even the 1X times the probability.",
            "Before even the parent node which is 3V3 and X times probability V5, given node V3, which is still the parent node and X OK.",
            "So we have this.",
            "We had this number so we have this distributions here.",
            "Associated with each node.",
            "And.",
            "This implicit is saying that whenever we are given.",
            "The value.",
            "About parent node of a given no apparent node.",
            "Say we are given the value of three.",
            "Here then.",
            "By giving.",
            "Means the probability that all of them are one.",
            "Not really, not really, not yet.",
            "The probability that the one we fight take on some value which is either 01.",
            "How do I fix this problem in the bottom line here?",
            "Yeah, I think I think you got that.",
            "If we fix the value of three.",
            "Then conditioned on the value of three being something, say one, then these two guys.",
            "The value of these two children are independent variable.",
            "OK, this may not be a good assumption for hierarchies.",
            "But recall that.",
            "Here a label might be a subtree as well.",
            "So if we choose this, no, we might well choose this one as well.",
            "So they're not, say, mutually exclusive.",
            "We might enforce some, you know, negative correlation among children, but this we didn't do.",
            "It's an open question, let's say.",
            "OK, and of course, as she was sort of mentioning we want to generate with this model illegal multi label in the sense of the previous previous slide.",
            "So we want to.",
            "Sort of enforce that if this guy is not chosen is this guy gets labeled zero, then everything underneath gets labeled zero as well, right?",
            "OK, if this guy is 1 then these two are independent can be.",
            "I had to take values 01 independently.",
            "If this is zero then these two guys are zero for sure.",
            "OK. And this is basically about this blog online thing.",
            "For every possible instance vector."
        ],
        [
            "Loss function.",
            "So we have a way of generating labels.",
            "How do we measure?",
            "The accuracy of our algorithms.",
            "We had this hierarchical loss function H loss.",
            "And this is it works as follows.",
            "Basically you had to want to compare two labelings to legal labelings to legal multi labels.",
            "You have a blue prediction and the red Label two label so.",
            "In comparing these two in sort of comparing the discrepancy between the two, what you have to do this is loss function.",
            "Say just consider as you go downwards the tree.",
            "You just consider the node where you first encounter a mistake.",
            "So if I go downwards here from 124, this is a mistake and now because it's not taken here while it is taken here, this is a mistake and node and.",
            "Everything underneath this node.",
            "This mistake in node is irrelevant.",
            "OK, this is counted as a mistake and it is weighted accordingly.",
            "But then all later mistakes underneath the subtree rooted at this, this node four are irrelevant.",
            "For instance, 8 is a mistaken node as well, because it is taking here is not taking here.",
            "But this does not count as a mistake OK. Symmetric is false, positive and negative.",
            "What do you mean by symmetric?",
            "Close if you do not label the node, it was.",
            "Shouldn't be enabled, or if you label a note that should not have been.",
            "I so yeah, from this point of view, yeah, this is symmetric from this point of view, this symmetric.",
            "In other words, if this is not taken, this is taken, yet it will be the same.",
            "Another example is.",
            "Heaven is another.",
            "Mistaken note.",
            "Everything straight because you say that you only count the first node, right?",
            "Thing is, taking the other direction then it's most severe if you keep making ones download.",
            "It is not implied by being the one.",
            "Yeah, we.",
            "I think we could.",
            "We could wait the two.",
            "Yeah, we could wait the two in a different way but.",
            "If you had to do is full then you are forced to have zeros at 8:00 and 9:00 absolute, but if you had wanted for the two different cases, one is 40789 or one is full.",
            "Count them as the same mistake and the cost is the same course.",
            "Yeah, I think we could.",
            "We could extend it.",
            "I think we could extend it, by the way.",
            "OK, seven mistaken.",
            "These guys do not count are not taking into consideration and you also might want to.",
            "You know, put weights on the.",
            "The nodes cost coefficients in a node.",
            "Two, you know.",
            "To account for the relative importance of the mistakes that you are making.",
            "So if you're making mistakes at the roots are more severe mistakes than if you made mistakes, at least something like this people do, I mean.",
            "In the standard age loss, they wait to hire at the roots, then at the.",
            "What do you mean?",
            "The Standard age law?",
            "So standard.",
            "Chase we had OK we had to ski.",
            "The parents of this paper.",
            "OK, we have two schemes here.",
            "Either we chose this this coefficient to be one.",
            "Everything is 1 or we choose.",
            "Say we get one costs one to this, we've cost 1/2 to this and we give cost 1 third, 116 in a way that the sum of these three guys is exactly the same as.",
            "But there are many other alternative schemes.",
            "So, so each notice just one player in this talk, yes.",
            "You could you could generalize, and of course everything actually goes through with, you know with the generative model, it's embarrassing.",
            "You would have to make different choices.",
            "Yeah, probably probably.",
            "2 pounds."
        ],
        [
            "So once we have, you know this simple generative model for the labels and we have loss.",
            "We can define Bayes optimal classifier for this.",
            "Which is obviously, you know, the one that the labeling that minimizes the expected loss given given X being the.",
            "The inspector and it turns out that it can be computed very easily as a standard bottom up message passing algorithm.",
            "So it was like this.",
            "I'd like to recall it because then our algorithm is based on this scheme, so we are given this conditional distributions associated with nodes, which I call pizza by.",
            "So this is.",
            "The one piece of one piece of two and so on piece of three, and so on.",
            "The deep these are all for the P1 is equal to 1 indivisible, right?",
            "Maybe twice.",
            "Say it again, please.",
            "So this is only four 431 equal to 1.",
            "Well, at this stage this takes, yeah yeah yeah, this guy will generate legal multi labels anyway, so this definition of P1 X well this holds in general, but then you had to choose whether you want to be 0 here or not.",
            "Well, I mean pizza by of X can be either.",
            "Some distribution can be 0 depending on whether the parent was zero, not.",
            "But what you mean is P1 of X is the probability that the one is 1.",
            "So yeah, OK. OK, well, I got that.",
            "So yeah, it's probably that this guy condition on this B1.",
            "That's what you go yeah, yeah, yeah yeah yeah OK. And it works like this.",
            "We're given these probabilities basically, and leaves get labeled as follows.",
            "Pizza by say pizza at 7.",
            "Is larger than 1/2, then it gets labeled 1, otherwise yes, label OK.",
            "So for instance, this is less than 1/2 and so they get gas levels.",
            "Or this is large in the hospital.",
            "And then we build messages, say."
        ],
        [
            "Based on these values, so the message is passed upwards.",
            "Is either 1 -- P seven.",
            "If this guy is labeled one or P7, if this guy gets labeled zero, it was labeled one, so it's 1 -- P seven.",
            "So 1 -- P Seven is passed upwards.",
            "This guy was labeled 0 so P8 is the message minus B 9.",
            "This little one and these three messages are collected by the node six which computes the sum Sigma OK. By the way, this is assuming that the cost coefficients are one or all one.",
            "If we have cost coefficients, we had to multiply each message by the cost coefficient associated with each node.",
            "OK, these things.",
            "OK, these messages are collected by 6.",
            "And six computes its own label.",
            "According to this rule."
        ],
        [
            "Is if it's corresponding piece by piece up six is larger than 1 / 2.",
            "Minus Sigma and it gets labeled 1 otherwise gets labeled zero Sigma.",
            "I recall the Sigma was this some of the messages that.",
            "That flowing upwards.",
            "Kate.",
            "And then be 6.",
            "Second phase there.",
            "Then you go back down, correct things below or if I mean, let's say that was labeled zero, what would happen to the one here?",
            "Yeah, then everything gets labeled 0, so you have to go back and correct the 7:00 and 9:00.",
            "Pretty much, yeah, yeah.",
            "Yeah, I was.",
            "I forgot to mention this line.",
            "If this guy gets labeled zero by some sheer accident then everything is underneath.",
            "Yes, labels."
        ],
        [
            "OK, so you know this message gets passed upwards and so on, OK?"
        ],
        [
            "So what's this algorithm be SVN?",
            "So BSM works as follows.",
            "We have an SVN sitting at each node.",
            "On the hierarchy that CMP and SVM here and so on.",
            "Why SVN?",
            "There's nothing special in this game, by the way.",
            "She could be pretty much everything at this point.",
            "Any say any linear classifier will work?",
            "Anyway, we have an SVN and each SVN is delivering a weight vector including, say, biased or something.",
            "This is work.",
            "We feed each SVM with a subset of the training set.",
            "So each node filters out examples for its kids.",
            "Basically.",
            "So this guy here gets trained only with those examples.",
            "That the parent doje.",
            "Has labeled one OK.",
            "It just says 0, so you don't even pass this downward.",
            "OK.",
            "So basically the root nodes.",
            "Our training with all labels daily starting with a small subset of labels, they are less important so.",
            "And then we associate.",
            "Sorry, then we approximate these pizza by of X, which of course are unavailable with the outcome of the SVM at each node by fitting a sigmoid using the so called Platt method.",
            "You know we had this sigmoid here.",
            "This is the weight vector produced by the SVM.",
            "And yeah, there's a way of producing probabilities out of SVN, and we have to fit parameters here and we do it through cross validation on the training set.",
            "Just use logic question why you want expert.",
            "Appliance service and now for now we don't have any.",
            "Well yeah, yeah yeah.",
            "We're planning to do so actually.",
            "Why I do or why I don't?",
            "No digital software, just our first stop, tend to do that if we didn't do logistic regression SVM since this went well.",
            "As well.",
            "We are really doing theoretical work.",
            "You know this is not theoretical, come on.",
            "You want to stay consistent, all the stuff you do this way.",
            "Don't be consistent.",
            "Yeah, I know.",
            "I know this is not.",
            "I'm not playing this theoretical role.",
            "But I am really concerned about running time here.",
            "So maybe logistic regression, but maybe no more than me on this.",
            "Additional cost really.",
            "I'm planning to do that.",
            "Thing is that the student was was supposed to do that just left.",
            "So because you're starting to talk about this classifier, all those that want to do this, it won't get you there.",
            "Yeah, it's just a way of, you know, sort of making things have principles in a way, that's all.",
            "Amazon.",
            "Only me because as hard as well, yeah.",
            "There it says holy charter.",
            "I also involve is that essentially if you do this way when they compute the probability, it won't be consistent.",
            "Anyway.",
            "And then once we fit this in Lloyds we play this bottom up game.",
            "OK we have these things instead of that rupee isibaya we propagate upwards.",
            "So once you.",
            "Once you start thinking of how these things work, you realize that what this algorithm is actually doing is trying to infer some good thresholds for SVM's at each node.",
            "So basically this boils down to."
        ],
        [
            "Using this bottom up scheme boils down to using SVM with modified thresholds at each node.",
            "Well, the threshold at node J that's actually depends on the behavior of children underneath.",
            "Your claim is that our JS, independent of the particular example or.",
            "Kouji is in Ferd, from the from the training set.",
            "So we aren't evaluation phase now.",
            "OK question is how can you determine before you see the test example.",
            "No, you have to see the test example.",
            "You had to see what the children, how the children work on the test example so it's they do actually depend on what the children are doing.",
            "Basically, it works as follows.",
            "If the children here.",
            "Have a very small margin in magnitude, say pizza by had disclosed 1/2.",
            "This means that this guy here is close to 0 in magnitude.",
            "This gives a 1/2.",
            "This means that the children are not very, you know, opinionated.",
            "I don't know.",
            "Maybe.",
            "Maybe yes, maybe no.",
            "And this forces.",
            "The parent node to have a very positive threshold.",
            "Very positive threshold, meaning that makes it harder for him to be one.",
            "OK, if the kids are not basically they don't know what they're doing.",
            "The parent node says.",
            "Well, maybe it's better.",
            "I stick to zero and.",
            "Stick to 0 by everything underneath OK?",
            "'cause I'm undecided as well.",
            "On the other hand, if.",
            "This margin here.",
            "Is either very positive or negative.",
            "This means that this piece of X is very either is either close to zero, close to 1.",
            "Then the threshold will be zero with the closest zero.",
            "And.",
            "The parent node has the freedom to be, you know, to be labeled according to its own local room.",
            "This is a.",
            "The intuition for the if it's the eye of X is very child, being very.",
            "Positive right?",
            "Obviously intuition, like if one of the children thinks that it's a while, then obviously want Tobias J also to be alone.",
            "'cause you know if JS can't be zero?",
            "I mean the I can't be one in J0 right?",
            "So right there doesn't seem to be a reason for the other way around, but yeah, but think about the children independent here so.",
            "So the way I see it is that there's really an implication from below to above, right logical indication of child is 1, then parent must be one.",
            "Yes.",
            "Station doesn't work the other way round.",
            "Zero, and that implies nothing about that.",
            "Yeah yeah, exactly thanks.",
            "2nd works sends to imply that if the child is very sure of being zero, then that OJ is also the threshold is also reduces tend to make it count more like that's the thing that was already right.",
            "Yeah, I don't know how to fix this, by the way, so I don't have an answer to this.",
            "We thought about if you be glad to hear suggestions for you, but that effect should be more pronounced, is there?",
            "Children right then, that tends to be true if there are only two and one of them already says 0, then you have to rely on the other one.",
            "Yeah, if there are more children.",
            "If they are more children.",
            "Basically the message that they're passing are affecting the magnet of this town."
        ],
        [
            "Here and yeah, that makes a difference.",
            "Because they are adding up the messages coming from children.",
            "Anymore.",
            "This one I skipped.",
            "And then what we did was to experimentally compare this bottom up."
        ],
        [
            "Scheme with the top down scheme, which was this H SVN, which is basically the same training scheme.",
            "You have an SVN sitting at each node and each SVN is fed with examples that are not filtered out by the parent node.",
            "But then what changes is the label assignment, namely the way we assign labels once we have trained.",
            "Things once we had trainees at the end, so it's top down instead of being bottom up.",
            "So each label each sorry node gets labeled according to the value of a linear threshold function, either if it's a route, so it has no parent, or if it is not a root and the parent node has been labeled one.",
            "Otherwise, guest labels you.",
            "OK, it's very reasonable.",
            "Thing.",
            "And unfortunately, I should say this scheme here is independent of the cost coefficients of the loss function.",
            "Which I don't know how to incorporate in this evaluation scheme.",
            "So if anyone has an idea?",
            "Glad to hear.",
            "Let me tell."
        ],
        [
            "But the experiments we made experiments on four datasets.",
            "Basically two of them were real world.",
            "I subset of the writer corpus, loading one the first 100,000 documents.",
            "This resulted in a hierarchy of 100 nodes, 4 threes, height three, and there is this interesting parameter here.",
            "Just the average number of paths or label recorder.",
            "That label is just a subtree, so it's union of parts.",
            "So the average number of paths per label here is 1.5, so it's a multi label and multipath labeling.",
            "Think we turn this into a. I3 by the way, it was a dog originally, or maybe this one I don't remember.",
            "I don't want or this one it was.",
            "It was a dog and we turn into a tree.",
            "Yeah.",
            "But we could perhaps play with with ISIS, where the other one is.",
            "They also met Corpus, actually specific subtree on the ultimate corpus, or medical abstracts.",
            "And while you see 55,000 documents, 94 nodes, the depth was four the average number of paths for multi label is 1.53 and we took the first case 5 order chunks.",
            "So we trained on one chunk and tested on the next chunk and we did it five times.",
            "On the second one, we took five random splits.",
            "And then all the results are basically averaged over the chunks over the random chat over the random splits or over the chunks.",
            "And then we also.",
            "Generated two synthetic datasets.",
            "Why did we do that?",
            "Well, it's because we we are not very happy of the results we got here so we.",
            "We try and find a better fortune with synthetic datasets.",
            "Turns out with this.",
            "OK, do synthetic datasets.",
            "40,000 examples, three completes or no trees.",
            "939 nodes height 2 an.",
            "We played around with.",
            "You know this little.",
            "Parameters generating things so we wanted to generate datasets having there.",
            "A different number of different numbers of paths per label.",
            "2.66 On the first one synthetic one and one point 28.",
            "For the second one.",
            "Again, we took four order chunks trained on one chunk and test on the next one.",
            "Sorry."
        ],
        [
            "I have lots.",
            "I only have numbers here.",
            "And what you can see from this?",
            "From these preliminary experiments is that basically.",
            "That bottom up approach be SVN is always beating is always beating the top down approach.",
            "Sometimes the difference between the two is really marginal.",
            "Here is really marginalis.",
            "Here is sort of significant on these two is really.",
            "Is very large.",
            "Are you using different values?",
            "Now these are H loss values here.",
            "See I see OK, the CIS here are chosen to be one I think for Bob in order.",
            "Yeah, because the comparison would be unfair.",
            "Otherwise.",
            "Innocence right?",
            "So we chose values, see eye to be one for the SVM.",
            "I don't think it makes a huge amount of difference.",
            "It's it's not, it's not yeah.",
            "Yeah, I agree.",
            "I mean, I'm not claiming any anything.",
            "See how is equal to 1, so this is this one.",
            "If you make it stupid Phoenix in this way.",
            "No, no sorry.",
            "OK, if the C1 ether the Costco features are one, these are three mistakes, so the H loss is 3 here."
        ],
        [
            "I mean, you could talk.",
            "It would create turn it into production where you don't have even crude nodes.",
            "That's true, that's true.",
            "That's correct, that's correct.",
            "In the age of.",
            "Yeah, but you know, yeah, of course the challenge is to get something less than one."
        ],
        [
            "How do we get there?",
            "We had more than one here.",
            "Yeah, I don't know why it will happen here.",
            "So this is larger than one.",
            "And this is larger one as well.",
            "Yeah, The thing is that I didn't.",
            "I didn't run the experiments myself this way.",
            "Let's see."
        ],
        [
            "OK, I don't remember exactly what happened, maybe.",
            "But this is just a guess.",
            "Maybe instead of setting the coefficients to one.",
            "We use this other scheme here."
        ],
        [
            "This is something like one.",
            "This is a one half 1/2.",
            "This is again 1/2.",
            "This is 161 over six 1 / 6 and 1 / 6.",
            "Yeah.",
            "Because you only charge for the yeah, yeah right, yeah, right yeah.",
            "Right yeah, right.",
            "Yeah, right, I don't remember.",
            "I had to, sorry.",
            "I had to check the paper.",
            "Is it a forest?"
        ],
        [
            "Sorry.",
            "Singletree yeah, I mean you could top atop dummy node.",
            "If you will seize, do not compensate for it.",
            "Being apart from this.",
            "Oh, I see.",
            "Yeah yeah yeah yeah OK you save me OK great OK yeah you have multiple trees so yeah multiple routes.",
            "Not meaningful unless we know that we want to have run a fair comparison, you know.",
            "And so basically we didn't cheat at all in this.",
            "Didn't want to cheat in some sense.",
            "And yeah, the difference between the real world data set is not is not as significant perhaps, so we try and see what happens chunk wise for each.",
            "Chunk"
        ],
        [
            "And.",
            "So the difference between.",
            "The performance of the two on each chunk is not very large.",
            "Sometimes it's ridiculous, you know, sometimes it's the same.",
            "Still, there is a definite statistical trend that sort of suggests that the D as the end at the bottom up approach is doing always.",
            "You didn't do any statistics over this, no?",
            "I mean, I'm not claiming again this is a significant what we report we're reporting.",
            "Here is the result for each chunk, and I guess you can see all chunks.",
            "They are doing better by a small amount, of course, but they're doing better.",
            "Here they're doing, I would say significantly better, But basically this one was the one that we were trying to understand.",
            "There's no this data set.",
            "The differences here are not really significant.",
            "I'm not claiming their significance.",
            "So on this data set, basically 2, the two approaches are are the same.",
            "On this one, they're not."
        ],
        [
            "And you know, one could argue about.",
            "What happens?",
            "Level wise.",
            "Not wanna go into this these numbers here but basically one.",
            "What one can conclude is that.",
            "The two algorithms are sort of performing similarly at the roots."
        ],
        [
            "But at least nodes the bottom up approach tends to outperform the top down one.",
            "Maybe it's just because it's starting from the least, starting off from the leaves.",
            "But it's it's better out at lower levels.",
            "So they are similar at the roots, which are counting more, but the BSM is slightly better on.",
            "At the least.",
            "OK."
        ],
        [
            "And now we turn to some more theoretical work.",
            "My can argue I can.",
            "Can you use the logistic regression here as well, yeah.",
            "Well, we have an analysis for a special case for special parametric model.",
            "Updates of these probabilities here.",
            "So basically what we do is to associate with each node.",
            "Parameter vector normalized to one and claim that the probability that the right node gets level one given.",
            "The value of the parent node is 1.",
            "Is obtained this way one plus user by times dot X / 2.",
            "So this is a probability value.",
            "OK, this is a parameter.",
            "So parametric model, simple parametric model, linear parametric model.",
            "And we did this for all nodes.",
            "And once again, we want to enforce legal multi labels and so we have this condition as well.",
            "So."
        ],
        [
            "What we wanted to do was to try and learn.",
            "In an online protocol.",
            "Good hierarchical predictor.",
            "And align protocol works as follows.",
            "You there is an algorithm here that receives at each time step an instance vector.",
            "Then it is required to produce a prediction, which is a legal multi label.",
            "Then it receives that feedback and then you know it keeps iterating.",
            "This way it updates its internal state and so on.",
            "Can we measure?",
            "The accuracy of this algorithm.",
            "Against the H loss function that I mentioned.",
            "But we do not measure the actual time.",
            "How much time do I have?",
            "Five 10510 we do not measure.",
            "The H loss per say, but we actually measure the age loss of the algorithm compared to the H loss of a given comparison predictor.",
            "So it's a regret analysis.",
            "A cumulative regret analysis.",
            "The comparison predictors that top down compared to that knows basically knows."
        ],
        [
            "The parameter is sitting at each node here, so it knows.",
            "Hopes."
        ],
        [
            "It knows the parametric model here."
        ],
        [
            "But it is not the biggest optimal classifier for this.",
            "Parametric model.",
            "So it's basically mimicking this HVN way of.",
            "Label label assignment.",
            "It's top down.",
            "Once again, node is labeled according to the value of the actual function.",
            "If it's a root or.",
            "It's the it's the child of a node that has been labeled one.",
            "It is not based off, at least because it does not depend on the cost coefficients.",
            "So we are comparing really comparing our classifier to a classifier which is not a base optimal one.",
            "Which we again do not know how to compute.",
            "OK, so the best we could do is this.",
            "And again we measure the cumulative regret the sum overall try overall examples of the expected of the risk.",
            "Basically the expected loss of the algorithm compared discounted by the expected loss of this top down compared to here.",
            "And since we wanted to compare to a top down competitor.",
            "Where is natural way?",
            "To go is to use a top down predictor.",
            "OK. Well, the top down predictor basically is doing.",
            "Local approximation to each parameter here.",
            "So it stores at each node weight vector which is meant to approximate the corresponding parameters."
        ],
        [
            "You survive.",
            "And they will stop down this in the same way.",
            "OK. And the update is basically similar to what H SVM was doing.",
            "There is a filtering rule here.",
            "And now an example is passed to a node.",
            "I only if the parent node was labeled one.",
            "On that example.",
            "OK, otherwise not best.",
            "And it's top down, and once again, independent.",
            "Of the season by.",
            "Which we don't know how to incorporate in a top down scheme.",
            "And.",
            "The algorithm."
        ],
        [
            "Is.",
            "Naturally.",
            "He is naturally derived from the parametric model because we have a basically a regularizer, square squares predictor sitting at each node.",
            "Which is a.",
            "Nothing tactically unbiased estimator.",
            "Of the parameter at each node.",
            "Well, I want to get into the details here.",
            "It naturally arises from the parametric model because this margin.",
            "This estimated margin is almost conditionally unbiased estimate of the true margin at each node and its accuracy actually depends on the number of examples that each node sees.",
            "Remember that root nodes see old labels while leaf nodes see.",
            "Only a small amount of labels.",
            "So we are very good at root labels park.",
            "We are poor at at the leaf nodes and you know it can be, you know, running dual variables and all these things.",
            "Anne."
        ],
        [
            "Then we have it bound.",
            "Which actually.",
            "Makes no assumption, no assumptions on the way.",
            "The instance vectors are generated.",
            "So there are pointwise bounds.",
            "So this guy here is constant.",
            "This guy is a random variable generated according to the generative model blah blah blah.",
            "But The X Factor can be worst case as well.",
            "Sorry can be worst case worst case, but it has to be generated ahead of time, so we let an adversary generate the instance vectors here.",
            "But we forced the adversary to.",
            "To choose them ahead of time before knowing what the algorithm is doing.",
            "OK, so it's a lose adversary, so weak after sorry.",
            "And then what we can prove?",
            "Is a regret bound or the following for we have two three ingredients in this bound.",
            "One is Delta squared, the other one is this capital C sub I and the third ingredient is this eigenvalue thing.",
            "Yeah, you have that.",
            "I inverse dependence on the minimal margin over all examples you have dependence on the contribution of.",
            "The cost coefficients sitting at each node and the subtree root underneath.",
            "And you also have a contribution view today.",
            "The eigen structure of the examples that each node.",
            "Seize after training.",
            "OK, and by the way, this is a so-called logarithmic cumulative regret, so instantaneously this is would be a fast rate of convergence.",
            "OK.",
            "The expectation there is over everything over all these over these at the end and the labels over these guys, right?",
            "So so on a particular run of the algorithm, obviously, yeah.",
            "Conclusions.",
            "Framework.",
            "I wouldn't say it's great, but it's a framework."
        ],
        [
            "Hierarchical classification.",
            "We've been trying to improve a baseline top down label assignment scheme known as HSV N, by about a map based optimal like algorithm.",
            "And I believe what we did was provide some sort of a modular approach.",
            "Instead of being principled, it more effective machine learning practice coming to be SVN thing is more like a practical thing.",
            "We could replace SVM by other things as well.",
            "Logistic regression, of course, would be one of the candidates.",
            "How we made some preliminary experiments with that lab to have more experiments on this?",
            "And they also showed a no line top down algorithm and its regret analysis.",
            "Based on our regularised squared algorithm.",
            "OK, open questions.",
            "Got to kind of kinds of open questions.",
            "Experimental."
        ],
        [
            "OK, there's a clear advantage of the bottom of the bottom up approach on synthetic datasets, and it's less clear on real world datasets.",
            "Which we are clearly more interested in.",
            "I don't know why exactly we do not know why.",
            "That might be the case that the real world datasets are just noisy.",
            "Too noisy too to make this base like scheme to work well, or it might well be the case that since we are.",
            "We have this independence assumption.",
            "This is not me.",
            "Way to go.",
            "Once I fixed the value of this value.",
            "Once I said this to one, the value of these guys are independent random variables, which is not need not be a good assumption in hierarchical classification.",
            "But it simplifies matter quite a lot.",
            "And of course, as Tom was mentioning, for instance, replace SVM by better algorithms that are more suitable for probability estimations, such as logistic regression as well.",
            "Thanks.",
            "On the theoretical side.",
            "Our regret analysis.",
            "Was not refering to the base optimal classifier.",
            "We are not comparing against the base optimal classifier for that particular model, which we do not know how to compute.",
            "Maybe a logistic model again would be a good.",
            "Candidate to try to.",
            "To compare to.",
            "Once we have an analysis cologist model.",
            "And once again, even in the theoretical work, we would like to remove this independence assumption, which is perhaps not.",
            "Promising promising Ave thank you, I'm done.",
            "Please yeah.",
            "So it seems that in your classification problem you want to compute the maximum posterior solution or four over the the silence of the piece, right?",
            "You're talking about the BSM thing.",
            "Doesn't matter where it doesn't matter, OK?",
            "To compute the maximum posteriori, yeah, yeah yeah, it's it's basically captured by this thing.",
            "Yes, please, thank you.",
            "So, so why we so?",
            "Why don't you use kind of?",
            "Understand it algorithm.",
            "Like kind of standard algorithm.",
            "So for any Markov models there is the algorithm.",
            "Yeah, this is a tree, but there is an equivalent of 50 to be automated.",
            "That's it, that's it.",
            "Once you do that, then also So what did you do?",
            "So why don't you do bottom and then go back again, which is what we doing?",
            "That's right.",
            "So why did you do that?",
            "No, I I I I go bottom up and then top down only if this guy gets labeled 0.",
            "So you're clean.",
            "Dennis Bachman in this top down.",
            "It's because I want to know whether one is better than the other.",
            "Wait, what do you mean?",
            "So which one is equivalent to that area?",
            "Part I?",
            "I guess this one, the bottom up one I guess.",
            "Yeah, because you make a hard assignment, but you wouldn't do it.",
            "The joint probability and then calculate the marginals with having maybe even the markers, not even the math assignment of the labels.",
            "Exactly come out right, right, right?",
            "If you formulated it as effective raw and it, but do you agree that this is the definition of Bayes optimal classifier?",
            "For this this line here?",
            "Is the one that minimizes the conditional risk.",
            "If you agree with that, then you should agree with this algorithm as well.",
            "It's a hard assignment.",
            "OK. And then after that.",
            "You have this algorithm.",
            "Yeah, this is basically the bottom up.",
            "Let's say I'm inspired by this bottomup scheme to build an algorithm that is sort of.",
            "Combining.",
            "Probabilities.",
            "You know?",
            "Yeah."
        ],
        [
            "My problem is I want to combine what's happening at each local E at each node in some way.",
            "In some way, either by combining and bottom up fresh or in the top down or whatever else, something.",
            "I think that if you would apply.",
            "Go back and forth and then you're done and then you have to match.",
            "So I don't know why this is.",
            "I believe this is the Viterbi thing.",
            "OK, so why do you need something else?",
            "Because there's not, this seems to be only going up.",
            "Well, why do I need something else?",
            "'cause I, I'm not sure this is the best thing you can do.",
            "I know you're busy, but no, I'm just kidding, sorry.",
            "I'm not, I'm not being Bayesian here.",
            "You know 'cause I'm approximating.",
            "Everything here is approximated, so it's just a scheme that sort of inspires the way of combining.",
            "The local the local information here.",
            "So this would be the best thing you could do if you had this piece of pie available.",
            "OK, yeah, and then you and then that's that's you're perfectly right.",
            "Then you're done.",
            "But these things are not available.",
            "We are approximating them in some weird way and still we are applying.",
            "We are still running this game over bottom, nothing, but we are not at all.",
            "Indeed, that the combined thing would be would be a good thing to do.",
            "OK, is that?",
            "Yeah, because I mean if you would would look at it as kind of 1st doing estimation and then do inference right then.",
            "This part.",
            "That we had would be the same after estimation and then you would have some influence and influence.",
            "It's kind of.",
            "Then probably the thing that you do is you have to do it.",
            "Yeah, but that's still do you think?",
            "Do you think this is a?",
            "A good way of estimating this, for instance, a good way of estimating this probabilities.",
            "Once I have this probabilities then I'm in good shape.",
            "Sure.",
            "Save one gives me a an approximation to it.",
            "Why should I wear?",
            "Should I actually play bottom up?",
            "I mean, there's no, there's no.",
            "It seems to me there's no guarantee that.",
            "The overall thing will still work, so I think I think.",
            "Then if you do inference, you need a 2 pass process box.",
            "It depends how you define the probability of a child.",
            "Being negative, given the parents negative or something and.",
            "If you have the the probability that a child is negative given the parent was classified negative a 0 which is in this model, then you don't need to do to pass it because it determines that.",
            "So there is this magic parameter.",
            "We tried doing it, getting the probabilities out and doing inference.",
            "Then you have this magic parameter which is OK.",
            "Sometimes when my like my parents classified as negative.",
            "I actually want to change my child, flip the label of my child because it makes the whole thing more likely had in France will correct parent labels in some sense, but you have this magic parameter that you gotta pick out of and and they work any better.",
            "Or once you pick.",
            "If you play this round, yeah yeah.",
            "No obvious way to center, but once you once you set in hindsight that there's a definite yes.",
            "Basically you can.",
            "It depends on how big you gotta really big tree and you made a mistake up there somewhere right then, if there's enough kind of probability, mass inference will flip.",
            "But if you you know that that's popped up right, yeah?",
            "If you, if you say the parent, if the parents make it classified as negative, then the child is negative with probability one.",
            "Right, yeah, you don't have to do that.",
            "I think that's maybe.",
            "Going down never changes in labor.",
            "Down a bit.",
            "You had to go down a bit a little bit, yeah?",
            "Because you have, yeah.",
            "Correct, if there were any children.",
            "One is just one layer.",
            "You do it once once.",
            "Any suggestions?",
            "Really would be glad.",
            "Yeah, other people.",
            "There are some people.",
            "Who are they?",
            "Who are they?",
            "OK. Model that has global method to try to do even more complicated.",
            "I'll be glad to.",
            "Basically you have a complete late model.",
            "That has a structure out of right, right, right, right.",
            "Some people more favorable about global methods rather than they say go local method like this won't work.",
            "Yeah, yeah.",
            "Well this bottom up thing that I showed you was a sort of a global method because because of the.",
            "On the thresholds.",
            "Where is it?",
            "Estimates in the training in the training, yeah."
        ],
        [
            "Yeah, that's true.",
            "Yeah, this is a weak global method.",
            "There's somebody at least what about running time?",
            "Server running.",
            "Longer work so.",
            "Kind of strange to me is that you you need to build this.",
            "Different inference.",
            "Kramer so the inference and the training do not necessarily match, so the structure that's right, that's right framework, those captured.",
            "Yeah, you build a logistic regression into it.",
            "You could write it as one big graphical model.",
            "You could do joint inference in that model.",
            "I mean, I don't know if it makes any big difference in practice, but it would kind of unify the thing alright, but would it work in practice?",
            "I mean, would it be practical?",
            "You think you think it would be practical doing local message passing, right?",
            "What about training training?",
            "Sing CRF.",
            "OK.",
            "I did some work on that.",
            "Hierarchical.",
            "Sequences experimentally they do not come up to be honest.",
            "Base optimality is one advantage, right?",
            "We are also working.",
            "But we deal with this car is which are as it 4230 is bigger, so let's say.",
            "600,600 thousand million documents.",
            "You're very patient.",
            "You're very patient.",
            "Like this, otherwise it's not useful, so so this would be used for labeling search results or.",
            "Assigning.",
            "The documents so.",
            "Things in different ways.",
            "So.",
            "About this, I didn't want to talk about this here, but still.",
            "So how somehow we combine little bit of indexing methods so so the nature of data changes if you're dealing with this large, very large hierarchies, usually you cannot infer anything from the top level.",
            "The balls, let's say 15 levels deep.",
            "Basically everything is lost.",
            "How, how deep are the hierarchies by the way?",
            "78 So much on this transition because it's.",
            "I said, what would you do instead?",
            "So we say that this is then.",
            "Addition problem into 500,000 dollars 600,000 classes.",
            "Which are somehow similar or connected but.",
            "When presenting the results, also the presentation results are different.",
            "Then it's not just 01.",
            "Usually this kind of higher is always human.",
            "Already human."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shall we start our plans?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll be talking about hierarchical classification.",
                    "label": 1
                },
                {
                    "sent": "And basically.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about recent feature.",
                    "label": 0
                },
                {
                    "sent": "Recent research we've been doing that I've been doing with these two guys, Nicole Oceans of Yankee and.",
                    "label": 0
                },
                {
                    "sent": "Luca is anybody?",
                    "label": 0
                },
                {
                    "sent": "OK. Hierarchical classification is a very well studied research subject.",
                    "label": 0
                },
                {
                    "sent": "There have been many papers on hierarchical classifications.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is colder in Sami domain Chen.",
                    "label": 0
                },
                {
                    "sent": "This wasn't ACNL 97.",
                    "label": 0
                },
                {
                    "sent": "Hoffman, Jeremy to an all this is horosho at all.",
                    "label": 0
                },
                {
                    "sent": "More recent stuff.",
                    "label": 0
                },
                {
                    "sent": "So there are many many models of hierarchy of classification.",
                    "label": 0
                },
                {
                    "sent": "There is no real.",
                    "label": 0
                },
                {
                    "sent": "Have a, say agreement of what the best model is and many models means many algorithms, many approaches and so on.",
                    "label": 0
                },
                {
                    "sent": "Top down approaches, bottom up approaches local global.",
                    "label": 0
                },
                {
                    "sent": "Online match.",
                    "label": 0
                },
                {
                    "sent": "Pachinko allocation methods and so on.",
                    "label": 0
                },
                {
                    "sent": "There are quite a lot.",
                    "label": 0
                },
                {
                    "sent": "The outline of this talk is the following.",
                    "label": 0
                },
                {
                    "sent": "First of all, I will introduce my own hierarchical classification framework.",
                    "label": 1
                },
                {
                    "sent": "Then I'll be talking about bottom up algorithm explaining the moment, but bottom up is which we call BS VM.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of.",
                    "label": 1
                },
                {
                    "sent": "Bayes optimal classifier for a given.",
                    "label": 0
                },
                {
                    "sent": "Model generating the labels will see in a moment with SVM.",
                    "label": 0
                },
                {
                    "sent": "It's a base optimal classifier with respect to, you know.",
                    "label": 1
                },
                {
                    "sent": "Model that generates subtrees as labels, 'cause here labels are subtrees.",
                    "label": 0
                },
                {
                    "sent": "And then I report some report on some experiments we've made comparing this PST EM algorithm, which is a bottom up and global approach to a baseline algorithm which is called HVM, which.",
                    "label": 0
                },
                {
                    "sent": "As mentioned the previous talk as well on both artificial and real world medium size datasets, and this is basically experimental work.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about some you know.",
                    "label": 0
                },
                {
                    "sent": "More theoretical work.",
                    "label": 0
                },
                {
                    "sent": "It's an online.",
                    "label": 0
                },
                {
                    "sent": "It's a regret analysis of an online algorithm.",
                    "label": 0
                },
                {
                    "sent": "Talk about the model specific parametric model for the labels.",
                    "label": 0
                },
                {
                    "sent": "An algorithm derived from this model, and I regret analysis.",
                    "label": 0
                },
                {
                    "sent": "So it's a half and half.",
                    "label": 0
                },
                {
                    "sent": "Experimental theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model.",
                    "label": 0
                },
                {
                    "sent": "In our case, hierarchy is given to us ahead of time.",
                    "label": 0
                },
                {
                    "sent": "You're given a hierarchy hierarchy, just a taxonomy is just a tree forest.",
                    "label": 1
                },
                {
                    "sent": "A bunch of trees.",
                    "label": 0
                },
                {
                    "sent": "You see one taxonomy on the left.",
                    "label": 0
                },
                {
                    "sent": "The same taxonomy is on the right label.",
                    "label": 0
                },
                {
                    "sent": "What we call a multilabel.",
                    "label": 0
                },
                {
                    "sent": "Actually it's just a way of.",
                    "label": 0
                },
                {
                    "sent": "Picking nodes within this taxonomy in some way that you know is consistent with that taxonomy.",
                    "label": 0
                },
                {
                    "sent": "Idea of structuring things and multi label is a legal multi label if it is a union of paths within the tree within the trees.",
                    "label": 1
                },
                {
                    "sent": "So for instance 1 two is a path 1 three is another path.",
                    "label": 0
                },
                {
                    "sent": "6A10 is another path and so the multi label 12368 ten is a legal multi label OK. What it is?",
                    "label": 0
                },
                {
                    "sent": "Yes, basically it's Supper Club Union of parts, meaning that whenever you choose something here, everything on.",
                    "label": 0
                },
                {
                    "sent": "On the top should be to be chosen.",
                    "label": 0
                },
                {
                    "sent": "On the right hand side you see.",
                    "label": 0
                },
                {
                    "sent": "And illegal mostly label not up or closed.",
                    "label": 0
                },
                {
                    "sent": "Basically if you choose this one, you have to choose this one as well.",
                    "label": 1
                },
                {
                    "sent": "And this is not a legal multilabel.",
                    "label": 0
                },
                {
                    "sent": "We can, of course, associated with each you know, multi label binary vectoring with the obvious meaning.",
                    "label": 0
                },
                {
                    "sent": "This one is chosen.",
                    "label": 0
                },
                {
                    "sent": "This one is chosen.",
                    "label": 0
                },
                {
                    "sent": "This one, this one is not chosen and so on, OK. An example it's a pair.",
                    "label": 0
                },
                {
                    "sent": "X is an instance vector for simplicity, and the is the associated legal multilabel.",
                    "label": 0
                },
                {
                    "sent": "One question, are you doing also with higher fees, or just with Reese's Moralistic case would be harvest tags, for instance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we could.",
                    "label": 0
                },
                {
                    "sent": "We could extend it to dogs, but this case I'm talking Bout 3 users, but there are some subtleties around in the theoretical side, but I think we are pretty confident that we could replace the tags so the input X is a tree or axis.",
                    "label": 0
                },
                {
                    "sent": "Whatever you like.",
                    "label": 0
                },
                {
                    "sent": "You could kernelized this thing.",
                    "label": 0
                },
                {
                    "sent": "So X could be, you know, everything that is compatible with discover machinery.",
                    "label": 0
                },
                {
                    "sent": "You can think of X to be a bunch of real numbers.",
                    "label": 0
                },
                {
                    "sent": "So where is the tree structure here?",
                    "label": 0
                },
                {
                    "sent": "Is that restructure the output?",
                    "label": 0
                },
                {
                    "sent": "So I mean it may be mentioned in the document classifications, I will I will OK X is just a way of encoding and document.",
                    "label": 0
                },
                {
                    "sent": "K bag of words, something and document can be classified, not just being a single topic, say talk about soccer here, but we can talk about, you know.",
                    "label": 0
                },
                {
                    "sent": "What's what's about sports, sports and politics and soccer as well?",
                    "label": 0
                },
                {
                    "sent": "That really is part of the prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, it's given it's given.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we generate multi labels?",
                    "label": 0
                },
                {
                    "sent": "We are given an instance, say a document for instance.",
                    "label": 0
                },
                {
                    "sent": "And we associate with a multi label random binary vector Capital V and this capital N is the number of topics.",
                    "label": 0
                },
                {
                    "sent": "The number of nodes in the tree.",
                    "label": 0
                },
                {
                    "sent": "And we basically do the following.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution of G of V given X is the product of these probability factors.",
                    "label": 0
                },
                {
                    "sent": "Here what we do is to associate.",
                    "label": 0
                },
                {
                    "sent": "We basically build a simple generative model for the labels.",
                    "label": 0
                },
                {
                    "sent": "We ask the shade with each node in the hierarchy, a conditional distribution pizza by of X is the probability is the distribution of.",
                    "label": 0
                },
                {
                    "sent": "These are by the value associated with this note I given the value associated with the parent node.",
                    "label": 0
                },
                {
                    "sent": "And the instance being X OK, so for instance here.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You see this little hierarchy on the on the left.",
                    "label": 0
                },
                {
                    "sent": "Basically do.",
                    "label": 0
                },
                {
                    "sent": "Distribution of V1V5.",
                    "label": 0
                },
                {
                    "sent": "Since we have only five nodes, even X.",
                    "label": 0
                },
                {
                    "sent": "Is given by.",
                    "label": 0
                },
                {
                    "sent": "Be one of X.",
                    "label": 0
                },
                {
                    "sent": "Let me write it.",
                    "label": 0
                },
                {
                    "sent": "Go away.",
                    "label": 0
                },
                {
                    "sent": "So the probability of.",
                    "label": 0
                },
                {
                    "sent": "The one given X times the probability of B2 given the parent that parent node, so V2 given P1 and then X times the probability of V3 even the 1X times the probability.",
                    "label": 0
                },
                {
                    "sent": "Before even the parent node which is 3V3 and X times probability V5, given node V3, which is still the parent node and X OK.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "We had this number so we have this distributions here.",
                    "label": 0
                },
                {
                    "sent": "Associated with each node.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This implicit is saying that whenever we are given.",
                    "label": 0
                },
                {
                    "sent": "The value.",
                    "label": 0
                },
                {
                    "sent": "About parent node of a given no apparent node.",
                    "label": 0
                },
                {
                    "sent": "Say we are given the value of three.",
                    "label": 0
                },
                {
                    "sent": "Here then.",
                    "label": 0
                },
                {
                    "sent": "By giving.",
                    "label": 0
                },
                {
                    "sent": "Means the probability that all of them are one.",
                    "label": 0
                },
                {
                    "sent": "Not really, not really, not yet.",
                    "label": 0
                },
                {
                    "sent": "The probability that the one we fight take on some value which is either 01.",
                    "label": 0
                },
                {
                    "sent": "How do I fix this problem in the bottom line here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I think you got that.",
                    "label": 0
                },
                {
                    "sent": "If we fix the value of three.",
                    "label": 0
                },
                {
                    "sent": "Then conditioned on the value of three being something, say one, then these two guys.",
                    "label": 0
                },
                {
                    "sent": "The value of these two children are independent variable.",
                    "label": 0
                },
                {
                    "sent": "OK, this may not be a good assumption for hierarchies.",
                    "label": 0
                },
                {
                    "sent": "But recall that.",
                    "label": 0
                },
                {
                    "sent": "Here a label might be a subtree as well.",
                    "label": 0
                },
                {
                    "sent": "So if we choose this, no, we might well choose this one as well.",
                    "label": 0
                },
                {
                    "sent": "So they're not, say, mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "We might enforce some, you know, negative correlation among children, but this we didn't do.",
                    "label": 0
                },
                {
                    "sent": "It's an open question, let's say.",
                    "label": 0
                },
                {
                    "sent": "OK, and of course, as she was sort of mentioning we want to generate with this model illegal multi label in the sense of the previous previous slide.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "Sort of enforce that if this guy is not chosen is this guy gets labeled zero, then everything underneath gets labeled zero as well, right?",
                    "label": 0
                },
                {
                    "sent": "OK, if this guy is 1 then these two are independent can be.",
                    "label": 0
                },
                {
                    "sent": "I had to take values 01 independently.",
                    "label": 0
                },
                {
                    "sent": "If this is zero then these two guys are zero for sure.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is basically about this blog online thing.",
                    "label": 0
                },
                {
                    "sent": "For every possible instance vector.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Loss function.",
                    "label": 0
                },
                {
                    "sent": "So we have a way of generating labels.",
                    "label": 0
                },
                {
                    "sent": "How do we measure?",
                    "label": 0
                },
                {
                    "sent": "The accuracy of our algorithms.",
                    "label": 0
                },
                {
                    "sent": "We had this hierarchical loss function H loss.",
                    "label": 1
                },
                {
                    "sent": "And this is it works as follows.",
                    "label": 0
                },
                {
                    "sent": "Basically you had to want to compare two labelings to legal labelings to legal multi labels.",
                    "label": 0
                },
                {
                    "sent": "You have a blue prediction and the red Label two label so.",
                    "label": 0
                },
                {
                    "sent": "In comparing these two in sort of comparing the discrepancy between the two, what you have to do this is loss function.",
                    "label": 0
                },
                {
                    "sent": "Say just consider as you go downwards the tree.",
                    "label": 0
                },
                {
                    "sent": "You just consider the node where you first encounter a mistake.",
                    "label": 0
                },
                {
                    "sent": "So if I go downwards here from 124, this is a mistake and now because it's not taken here while it is taken here, this is a mistake and node and.",
                    "label": 0
                },
                {
                    "sent": "Everything underneath this node.",
                    "label": 0
                },
                {
                    "sent": "This mistake in node is irrelevant.",
                    "label": 0
                },
                {
                    "sent": "OK, this is counted as a mistake and it is weighted accordingly.",
                    "label": 0
                },
                {
                    "sent": "But then all later mistakes underneath the subtree rooted at this, this node four are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "For instance, 8 is a mistaken node as well, because it is taking here is not taking here.",
                    "label": 0
                },
                {
                    "sent": "But this does not count as a mistake OK. Symmetric is false, positive and negative.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by symmetric?",
                    "label": 0
                },
                {
                    "sent": "Close if you do not label the node, it was.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't be enabled, or if you label a note that should not have been.",
                    "label": 0
                },
                {
                    "sent": "I so yeah, from this point of view, yeah, this is symmetric from this point of view, this symmetric.",
                    "label": 0
                },
                {
                    "sent": "In other words, if this is not taken, this is taken, yet it will be the same.",
                    "label": 0
                },
                {
                    "sent": "Another example is.",
                    "label": 0
                },
                {
                    "sent": "Heaven is another.",
                    "label": 0
                },
                {
                    "sent": "Mistaken note.",
                    "label": 0
                },
                {
                    "sent": "Everything straight because you say that you only count the first node, right?",
                    "label": 0
                },
                {
                    "sent": "Thing is, taking the other direction then it's most severe if you keep making ones download.",
                    "label": 0
                },
                {
                    "sent": "It is not implied by being the one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we.",
                    "label": 0
                },
                {
                    "sent": "I think we could.",
                    "label": 0
                },
                {
                    "sent": "We could wait the two.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we could wait the two in a different way but.",
                    "label": 0
                },
                {
                    "sent": "If you had to do is full then you are forced to have zeros at 8:00 and 9:00 absolute, but if you had wanted for the two different cases, one is 40789 or one is full.",
                    "label": 0
                },
                {
                    "sent": "Count them as the same mistake and the cost is the same course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think we could.",
                    "label": 0
                },
                {
                    "sent": "We could extend it.",
                    "label": 0
                },
                {
                    "sent": "I think we could extend it, by the way.",
                    "label": 0
                },
                {
                    "sent": "OK, seven mistaken.",
                    "label": 0
                },
                {
                    "sent": "These guys do not count are not taking into consideration and you also might want to.",
                    "label": 0
                },
                {
                    "sent": "You know, put weights on the.",
                    "label": 0
                },
                {
                    "sent": "The nodes cost coefficients in a node.",
                    "label": 0
                },
                {
                    "sent": "Two, you know.",
                    "label": 0
                },
                {
                    "sent": "To account for the relative importance of the mistakes that you are making.",
                    "label": 0
                },
                {
                    "sent": "So if you're making mistakes at the roots are more severe mistakes than if you made mistakes, at least something like this people do, I mean.",
                    "label": 0
                },
                {
                    "sent": "In the standard age loss, they wait to hire at the roots, then at the.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "The Standard age law?",
                    "label": 0
                },
                {
                    "sent": "So standard.",
                    "label": 0
                },
                {
                    "sent": "Chase we had OK we had to ski.",
                    "label": 0
                },
                {
                    "sent": "The parents of this paper.",
                    "label": 0
                },
                {
                    "sent": "OK, we have two schemes here.",
                    "label": 0
                },
                {
                    "sent": "Either we chose this this coefficient to be one.",
                    "label": 0
                },
                {
                    "sent": "Everything is 1 or we choose.",
                    "label": 0
                },
                {
                    "sent": "Say we get one costs one to this, we've cost 1/2 to this and we give cost 1 third, 116 in a way that the sum of these three guys is exactly the same as.",
                    "label": 0
                },
                {
                    "sent": "But there are many other alternative schemes.",
                    "label": 0
                },
                {
                    "sent": "So, so each notice just one player in this talk, yes.",
                    "label": 0
                },
                {
                    "sent": "You could you could generalize, and of course everything actually goes through with, you know with the generative model, it's embarrassing.",
                    "label": 0
                },
                {
                    "sent": "You would have to make different choices.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably probably.",
                    "label": 0
                },
                {
                    "sent": "2 pounds.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have, you know this simple generative model for the labels and we have loss.",
                    "label": 0
                },
                {
                    "sent": "We can define Bayes optimal classifier for this.",
                    "label": 1
                },
                {
                    "sent": "Which is obviously, you know, the one that the labeling that minimizes the expected loss given given X being the.",
                    "label": 0
                },
                {
                    "sent": "The inspector and it turns out that it can be computed very easily as a standard bottom up message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it was like this.",
                    "label": 0
                },
                {
                    "sent": "I'd like to recall it because then our algorithm is based on this scheme, so we are given this conditional distributions associated with nodes, which I call pizza by.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The one piece of one piece of two and so on piece of three, and so on.",
                    "label": 0
                },
                {
                    "sent": "The deep these are all for the P1 is equal to 1 indivisible, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe twice.",
                    "label": 0
                },
                {
                    "sent": "Say it again, please.",
                    "label": 0
                },
                {
                    "sent": "So this is only four 431 equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Well, at this stage this takes, yeah yeah yeah, this guy will generate legal multi labels anyway, so this definition of P1 X well this holds in general, but then you had to choose whether you want to be 0 here or not.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean pizza by of X can be either.",
                    "label": 0
                },
                {
                    "sent": "Some distribution can be 0 depending on whether the parent was zero, not.",
                    "label": 0
                },
                {
                    "sent": "But what you mean is P1 of X is the probability that the one is 1.",
                    "label": 0
                },
                {
                    "sent": "So yeah, OK. OK, well, I got that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's probably that this guy condition on this B1.",
                    "label": 0
                },
                {
                    "sent": "That's what you go yeah, yeah, yeah yeah yeah OK. And it works like this.",
                    "label": 0
                },
                {
                    "sent": "We're given these probabilities basically, and leaves get labeled as follows.",
                    "label": 0
                },
                {
                    "sent": "Pizza by say pizza at 7.",
                    "label": 0
                },
                {
                    "sent": "Is larger than 1/2, then it gets labeled 1, otherwise yes, label OK.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this is less than 1/2 and so they get gas levels.",
                    "label": 0
                },
                {
                    "sent": "Or this is large in the hospital.",
                    "label": 0
                },
                {
                    "sent": "And then we build messages, say.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on these values, so the message is passed upwards.",
                    "label": 0
                },
                {
                    "sent": "Is either 1 -- P seven.",
                    "label": 0
                },
                {
                    "sent": "If this guy is labeled one or P7, if this guy gets labeled zero, it was labeled one, so it's 1 -- P seven.",
                    "label": 0
                },
                {
                    "sent": "So 1 -- P Seven is passed upwards.",
                    "label": 0
                },
                {
                    "sent": "This guy was labeled 0 so P8 is the message minus B 9.",
                    "label": 0
                },
                {
                    "sent": "This little one and these three messages are collected by the node six which computes the sum Sigma OK. By the way, this is assuming that the cost coefficients are one or all one.",
                    "label": 0
                },
                {
                    "sent": "If we have cost coefficients, we had to multiply each message by the cost coefficient associated with each node.",
                    "label": 0
                },
                {
                    "sent": "OK, these things.",
                    "label": 0
                },
                {
                    "sent": "OK, these messages are collected by 6.",
                    "label": 0
                },
                {
                    "sent": "And six computes its own label.",
                    "label": 0
                },
                {
                    "sent": "According to this rule.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is if it's corresponding piece by piece up six is larger than 1 / 2.",
                    "label": 0
                },
                {
                    "sent": "Minus Sigma and it gets labeled 1 otherwise gets labeled zero Sigma.",
                    "label": 0
                },
                {
                    "sent": "I recall the Sigma was this some of the messages that.",
                    "label": 0
                },
                {
                    "sent": "That flowing upwards.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                },
                {
                    "sent": "And then be 6.",
                    "label": 0
                },
                {
                    "sent": "Second phase there.",
                    "label": 0
                },
                {
                    "sent": "Then you go back down, correct things below or if I mean, let's say that was labeled zero, what would happen to the one here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, then everything gets labeled 0, so you have to go back and correct the 7:00 and 9:00.",
                    "label": 0
                },
                {
                    "sent": "Pretty much, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was.",
                    "label": 0
                },
                {
                    "sent": "I forgot to mention this line.",
                    "label": 0
                },
                {
                    "sent": "If this guy gets labeled zero by some sheer accident then everything is underneath.",
                    "label": 0
                },
                {
                    "sent": "Yes, labels.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you know this message gets passed upwards and so on, OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's this algorithm be SVN?",
                    "label": 0
                },
                {
                    "sent": "So BSM works as follows.",
                    "label": 0
                },
                {
                    "sent": "We have an SVN sitting at each node.",
                    "label": 1
                },
                {
                    "sent": "On the hierarchy that CMP and SVM here and so on.",
                    "label": 0
                },
                {
                    "sent": "Why SVN?",
                    "label": 0
                },
                {
                    "sent": "There's nothing special in this game, by the way.",
                    "label": 0
                },
                {
                    "sent": "She could be pretty much everything at this point.",
                    "label": 0
                },
                {
                    "sent": "Any say any linear classifier will work?",
                    "label": 0
                },
                {
                    "sent": "Anyway, we have an SVN and each SVN is delivering a weight vector including, say, biased or something.",
                    "label": 0
                },
                {
                    "sent": "This is work.",
                    "label": 0
                },
                {
                    "sent": "We feed each SVM with a subset of the training set.",
                    "label": 0
                },
                {
                    "sent": "So each node filters out examples for its kids.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So this guy here gets trained only with those examples.",
                    "label": 0
                },
                {
                    "sent": "That the parent doje.",
                    "label": 0
                },
                {
                    "sent": "Has labeled one OK.",
                    "label": 0
                },
                {
                    "sent": "It just says 0, so you don't even pass this downward.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically the root nodes.",
                    "label": 0
                },
                {
                    "sent": "Our training with all labels daily starting with a small subset of labels, they are less important so.",
                    "label": 0
                },
                {
                    "sent": "And then we associate.",
                    "label": 0
                },
                {
                    "sent": "Sorry, then we approximate these pizza by of X, which of course are unavailable with the outcome of the SVM at each node by fitting a sigmoid using the so called Platt method.",
                    "label": 0
                },
                {
                    "sent": "You know we had this sigmoid here.",
                    "label": 0
                },
                {
                    "sent": "This is the weight vector produced by the SVM.",
                    "label": 1
                },
                {
                    "sent": "And yeah, there's a way of producing probabilities out of SVN, and we have to fit parameters here and we do it through cross validation on the training set.",
                    "label": 0
                },
                {
                    "sent": "Just use logic question why you want expert.",
                    "label": 0
                },
                {
                    "sent": "Appliance service and now for now we don't have any.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "We're planning to do so actually.",
                    "label": 0
                },
                {
                    "sent": "Why I do or why I don't?",
                    "label": 0
                },
                {
                    "sent": "No digital software, just our first stop, tend to do that if we didn't do logistic regression SVM since this went well.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "We are really doing theoretical work.",
                    "label": 0
                },
                {
                    "sent": "You know this is not theoretical, come on.",
                    "label": 0
                },
                {
                    "sent": "You want to stay consistent, all the stuff you do this way.",
                    "label": 0
                },
                {
                    "sent": "Don't be consistent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know.",
                    "label": 0
                },
                {
                    "sent": "I know this is not.",
                    "label": 0
                },
                {
                    "sent": "I'm not playing this theoretical role.",
                    "label": 0
                },
                {
                    "sent": "But I am really concerned about running time here.",
                    "label": 0
                },
                {
                    "sent": "So maybe logistic regression, but maybe no more than me on this.",
                    "label": 0
                },
                {
                    "sent": "Additional cost really.",
                    "label": 0
                },
                {
                    "sent": "I'm planning to do that.",
                    "label": 0
                },
                {
                    "sent": "Thing is that the student was was supposed to do that just left.",
                    "label": 0
                },
                {
                    "sent": "So because you're starting to talk about this classifier, all those that want to do this, it won't get you there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just a way of, you know, sort of making things have principles in a way, that's all.",
                    "label": 0
                },
                {
                    "sent": "Amazon.",
                    "label": 0
                },
                {
                    "sent": "Only me because as hard as well, yeah.",
                    "label": 0
                },
                {
                    "sent": "There it says holy charter.",
                    "label": 0
                },
                {
                    "sent": "I also involve is that essentially if you do this way when they compute the probability, it won't be consistent.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "And then once we fit this in Lloyds we play this bottom up game.",
                    "label": 0
                },
                {
                    "sent": "OK we have these things instead of that rupee isibaya we propagate upwards.",
                    "label": 0
                },
                {
                    "sent": "So once you.",
                    "label": 0
                },
                {
                    "sent": "Once you start thinking of how these things work, you realize that what this algorithm is actually doing is trying to infer some good thresholds for SVM's at each node.",
                    "label": 0
                },
                {
                    "sent": "So basically this boils down to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this bottom up scheme boils down to using SVM with modified thresholds at each node.",
                    "label": 1
                },
                {
                    "sent": "Well, the threshold at node J that's actually depends on the behavior of children underneath.",
                    "label": 1
                },
                {
                    "sent": "Your claim is that our JS, independent of the particular example or.",
                    "label": 0
                },
                {
                    "sent": "Kouji is in Ferd, from the from the training set.",
                    "label": 0
                },
                {
                    "sent": "So we aren't evaluation phase now.",
                    "label": 0
                },
                {
                    "sent": "OK question is how can you determine before you see the test example.",
                    "label": 0
                },
                {
                    "sent": "No, you have to see the test example.",
                    "label": 0
                },
                {
                    "sent": "You had to see what the children, how the children work on the test example so it's they do actually depend on what the children are doing.",
                    "label": 0
                },
                {
                    "sent": "Basically, it works as follows.",
                    "label": 0
                },
                {
                    "sent": "If the children here.",
                    "label": 0
                },
                {
                    "sent": "Have a very small margin in magnitude, say pizza by had disclosed 1/2.",
                    "label": 0
                },
                {
                    "sent": "This means that this guy here is close to 0 in magnitude.",
                    "label": 0
                },
                {
                    "sent": "This gives a 1/2.",
                    "label": 0
                },
                {
                    "sent": "This means that the children are not very, you know, opinionated.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe yes, maybe no.",
                    "label": 0
                },
                {
                    "sent": "And this forces.",
                    "label": 0
                },
                {
                    "sent": "The parent node to have a very positive threshold.",
                    "label": 0
                },
                {
                    "sent": "Very positive threshold, meaning that makes it harder for him to be one.",
                    "label": 0
                },
                {
                    "sent": "OK, if the kids are not basically they don't know what they're doing.",
                    "label": 0
                },
                {
                    "sent": "The parent node says.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe it's better.",
                    "label": 0
                },
                {
                    "sent": "I stick to zero and.",
                    "label": 0
                },
                {
                    "sent": "Stick to 0 by everything underneath OK?",
                    "label": 0
                },
                {
                    "sent": "'cause I'm undecided as well.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if.",
                    "label": 0
                },
                {
                    "sent": "This margin here.",
                    "label": 0
                },
                {
                    "sent": "Is either very positive or negative.",
                    "label": 0
                },
                {
                    "sent": "This means that this piece of X is very either is either close to zero, close to 1.",
                    "label": 0
                },
                {
                    "sent": "Then the threshold will be zero with the closest zero.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The parent node has the freedom to be, you know, to be labeled according to its own local room.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "The intuition for the if it's the eye of X is very child, being very.",
                    "label": 0
                },
                {
                    "sent": "Positive right?",
                    "label": 0
                },
                {
                    "sent": "Obviously intuition, like if one of the children thinks that it's a while, then obviously want Tobias J also to be alone.",
                    "label": 0
                },
                {
                    "sent": "'cause you know if JS can't be zero?",
                    "label": 0
                },
                {
                    "sent": "I mean the I can't be one in J0 right?",
                    "label": 0
                },
                {
                    "sent": "So right there doesn't seem to be a reason for the other way around, but yeah, but think about the children independent here so.",
                    "label": 0
                },
                {
                    "sent": "So the way I see it is that there's really an implication from below to above, right logical indication of child is 1, then parent must be one.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Station doesn't work the other way round.",
                    "label": 0
                },
                {
                    "sent": "Zero, and that implies nothing about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, exactly thanks.",
                    "label": 0
                },
                {
                    "sent": "2nd works sends to imply that if the child is very sure of being zero, then that OJ is also the threshold is also reduces tend to make it count more like that's the thing that was already right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know how to fix this, by the way, so I don't have an answer to this.",
                    "label": 0
                },
                {
                    "sent": "We thought about if you be glad to hear suggestions for you, but that effect should be more pronounced, is there?",
                    "label": 0
                },
                {
                    "sent": "Children right then, that tends to be true if there are only two and one of them already says 0, then you have to rely on the other one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if there are more children.",
                    "label": 0
                },
                {
                    "sent": "If they are more children.",
                    "label": 0
                },
                {
                    "sent": "Basically the message that they're passing are affecting the magnet of this town.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here and yeah, that makes a difference.",
                    "label": 0
                },
                {
                    "sent": "Because they are adding up the messages coming from children.",
                    "label": 0
                },
                {
                    "sent": "Anymore.",
                    "label": 0
                },
                {
                    "sent": "This one I skipped.",
                    "label": 0
                },
                {
                    "sent": "And then what we did was to experimentally compare this bottom up.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scheme with the top down scheme, which was this H SVN, which is basically the same training scheme.",
                    "label": 1
                },
                {
                    "sent": "You have an SVN sitting at each node and each SVN is fed with examples that are not filtered out by the parent node.",
                    "label": 1
                },
                {
                    "sent": "But then what changes is the label assignment, namely the way we assign labels once we have trained.",
                    "label": 0
                },
                {
                    "sent": "Things once we had trainees at the end, so it's top down instead of being bottom up.",
                    "label": 0
                },
                {
                    "sent": "So each label each sorry node gets labeled according to the value of a linear threshold function, either if it's a route, so it has no parent, or if it is not a root and the parent node has been labeled one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, guest labels you.",
                    "label": 1
                },
                {
                    "sent": "OK, it's very reasonable.",
                    "label": 0
                },
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately, I should say this scheme here is independent of the cost coefficients of the loss function.",
                    "label": 0
                },
                {
                    "sent": "Which I don't know how to incorporate in this evaluation scheme.",
                    "label": 0
                },
                {
                    "sent": "So if anyone has an idea?",
                    "label": 0
                },
                {
                    "sent": "Glad to hear.",
                    "label": 0
                },
                {
                    "sent": "Let me tell.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the experiments we made experiments on four datasets.",
                    "label": 0
                },
                {
                    "sent": "Basically two of them were real world.",
                    "label": 0
                },
                {
                    "sent": "I subset of the writer corpus, loading one the first 100,000 documents.",
                    "label": 0
                },
                {
                    "sent": "This resulted in a hierarchy of 100 nodes, 4 threes, height three, and there is this interesting parameter here.",
                    "label": 1
                },
                {
                    "sent": "Just the average number of paths or label recorder.",
                    "label": 0
                },
                {
                    "sent": "That label is just a subtree, so it's union of parts.",
                    "label": 0
                },
                {
                    "sent": "So the average number of paths per label here is 1.5, so it's a multi label and multipath labeling.",
                    "label": 0
                },
                {
                    "sent": "Think we turn this into a. I3 by the way, it was a dog originally, or maybe this one I don't remember.",
                    "label": 0
                },
                {
                    "sent": "I don't want or this one it was.",
                    "label": 0
                },
                {
                    "sent": "It was a dog and we turn into a tree.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But we could perhaps play with with ISIS, where the other one is.",
                    "label": 0
                },
                {
                    "sent": "They also met Corpus, actually specific subtree on the ultimate corpus, or medical abstracts.",
                    "label": 1
                },
                {
                    "sent": "And while you see 55,000 documents, 94 nodes, the depth was four the average number of paths for multi label is 1.53 and we took the first case 5 order chunks.",
                    "label": 0
                },
                {
                    "sent": "So we trained on one chunk and tested on the next chunk and we did it five times.",
                    "label": 0
                },
                {
                    "sent": "On the second one, we took five random splits.",
                    "label": 1
                },
                {
                    "sent": "And then all the results are basically averaged over the chunks over the random chat over the random splits or over the chunks.",
                    "label": 0
                },
                {
                    "sent": "And then we also.",
                    "label": 0
                },
                {
                    "sent": "Generated two synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "Why did we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because we we are not very happy of the results we got here so we.",
                    "label": 1
                },
                {
                    "sent": "We try and find a better fortune with synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "Turns out with this.",
                    "label": 0
                },
                {
                    "sent": "OK, do synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "40,000 examples, three completes or no trees.",
                    "label": 0
                },
                {
                    "sent": "939 nodes height 2 an.",
                    "label": 1
                },
                {
                    "sent": "We played around with.",
                    "label": 0
                },
                {
                    "sent": "You know this little.",
                    "label": 0
                },
                {
                    "sent": "Parameters generating things so we wanted to generate datasets having there.",
                    "label": 0
                },
                {
                    "sent": "A different number of different numbers of paths per label.",
                    "label": 0
                },
                {
                    "sent": "2.66 On the first one synthetic one and one point 28.",
                    "label": 0
                },
                {
                    "sent": "For the second one.",
                    "label": 0
                },
                {
                    "sent": "Again, we took four order chunks trained on one chunk and test on the next one.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have lots.",
                    "label": 0
                },
                {
                    "sent": "I only have numbers here.",
                    "label": 0
                },
                {
                    "sent": "And what you can see from this?",
                    "label": 0
                },
                {
                    "sent": "From these preliminary experiments is that basically.",
                    "label": 0
                },
                {
                    "sent": "That bottom up approach be SVN is always beating is always beating the top down approach.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the difference between the two is really marginal.",
                    "label": 0
                },
                {
                    "sent": "Here is really marginalis.",
                    "label": 0
                },
                {
                    "sent": "Here is sort of significant on these two is really.",
                    "label": 0
                },
                {
                    "sent": "Is very large.",
                    "label": 0
                },
                {
                    "sent": "Are you using different values?",
                    "label": 0
                },
                {
                    "sent": "Now these are H loss values here.",
                    "label": 0
                },
                {
                    "sent": "See I see OK, the CIS here are chosen to be one I think for Bob in order.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because the comparison would be unfair.",
                    "label": 0
                },
                {
                    "sent": "Otherwise.",
                    "label": 0
                },
                {
                    "sent": "Innocence right?",
                    "label": 0
                },
                {
                    "sent": "So we chose values, see eye to be one for the SVM.",
                    "label": 0
                },
                {
                    "sent": "I don't think it makes a huge amount of difference.",
                    "label": 0
                },
                {
                    "sent": "It's it's not, it's not yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not claiming any anything.",
                    "label": 0
                },
                {
                    "sent": "See how is equal to 1, so this is this one.",
                    "label": 0
                },
                {
                    "sent": "If you make it stupid Phoenix in this way.",
                    "label": 0
                },
                {
                    "sent": "No, no sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, if the C1 ether the Costco features are one, these are three mistakes, so the H loss is 3 here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, you could talk.",
                    "label": 0
                },
                {
                    "sent": "It would create turn it into production where you don't have even crude nodes.",
                    "label": 0
                },
                {
                    "sent": "That's true, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's correct, that's correct.",
                    "label": 0
                },
                {
                    "sent": "In the age of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you know, yeah, of course the challenge is to get something less than one.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we get there?",
                    "label": 0
                },
                {
                    "sent": "We had more than one here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know why it will happen here.",
                    "label": 0
                },
                {
                    "sent": "So this is larger than one.",
                    "label": 0
                },
                {
                    "sent": "And this is larger one as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, The thing is that I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't run the experiments myself this way.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I don't remember exactly what happened, maybe.",
                    "label": 0
                },
                {
                    "sent": "But this is just a guess.",
                    "label": 0
                },
                {
                    "sent": "Maybe instead of setting the coefficients to one.",
                    "label": 0
                },
                {
                    "sent": "We use this other scheme here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is something like one.",
                    "label": 0
                },
                {
                    "sent": "This is a one half 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is again 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is 161 over six 1 / 6 and 1 / 6.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Because you only charge for the yeah, yeah right, yeah, right yeah.",
                    "label": 0
                },
                {
                    "sent": "Right yeah, right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right, I don't remember.",
                    "label": 0
                },
                {
                    "sent": "I had to, sorry.",
                    "label": 0
                },
                {
                    "sent": "I had to check the paper.",
                    "label": 0
                },
                {
                    "sent": "Is it a forest?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Singletree yeah, I mean you could top atop dummy node.",
                    "label": 0
                },
                {
                    "sent": "If you will seize, do not compensate for it.",
                    "label": 0
                },
                {
                    "sent": "Being apart from this.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah OK you save me OK great OK yeah you have multiple trees so yeah multiple routes.",
                    "label": 0
                },
                {
                    "sent": "Not meaningful unless we know that we want to have run a fair comparison, you know.",
                    "label": 0
                },
                {
                    "sent": "And so basically we didn't cheat at all in this.",
                    "label": 0
                },
                {
                    "sent": "Didn't want to cheat in some sense.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the difference between the real world data set is not is not as significant perhaps, so we try and see what happens chunk wise for each.",
                    "label": 0
                },
                {
                    "sent": "Chunk",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the difference between.",
                    "label": 0
                },
                {
                    "sent": "The performance of the two on each chunk is not very large.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's ridiculous, you know, sometimes it's the same.",
                    "label": 0
                },
                {
                    "sent": "Still, there is a definite statistical trend that sort of suggests that the D as the end at the bottom up approach is doing always.",
                    "label": 0
                },
                {
                    "sent": "You didn't do any statistics over this, no?",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not claiming again this is a significant what we report we're reporting.",
                    "label": 0
                },
                {
                    "sent": "Here is the result for each chunk, and I guess you can see all chunks.",
                    "label": 0
                },
                {
                    "sent": "They are doing better by a small amount, of course, but they're doing better.",
                    "label": 0
                },
                {
                    "sent": "Here they're doing, I would say significantly better, But basically this one was the one that we were trying to understand.",
                    "label": 0
                },
                {
                    "sent": "There's no this data set.",
                    "label": 0
                },
                {
                    "sent": "The differences here are not really significant.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming their significance.",
                    "label": 0
                },
                {
                    "sent": "So on this data set, basically 2, the two approaches are are the same.",
                    "label": 0
                },
                {
                    "sent": "On this one, they're not.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, one could argue about.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "Level wise.",
                    "label": 0
                },
                {
                    "sent": "Not wanna go into this these numbers here but basically one.",
                    "label": 0
                },
                {
                    "sent": "What one can conclude is that.",
                    "label": 0
                },
                {
                    "sent": "The two algorithms are sort of performing similarly at the roots.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But at least nodes the bottom up approach tends to outperform the top down one.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's just because it's starting from the least, starting off from the leaves.",
                    "label": 0
                },
                {
                    "sent": "But it's it's better out at lower levels.",
                    "label": 0
                },
                {
                    "sent": "So they are similar at the roots, which are counting more, but the BSM is slightly better on.",
                    "label": 0
                },
                {
                    "sent": "At the least.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we turn to some more theoretical work.",
                    "label": 0
                },
                {
                    "sent": "My can argue I can.",
                    "label": 0
                },
                {
                    "sent": "Can you use the logistic regression here as well, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, we have an analysis for a special case for special parametric model.",
                    "label": 0
                },
                {
                    "sent": "Updates of these probabilities here.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do is to associate with each node.",
                    "label": 0
                },
                {
                    "sent": "Parameter vector normalized to one and claim that the probability that the right node gets level one given.",
                    "label": 0
                },
                {
                    "sent": "The value of the parent node is 1.",
                    "label": 0
                },
                {
                    "sent": "Is obtained this way one plus user by times dot X / 2.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability value.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a parameter.",
                    "label": 0
                },
                {
                    "sent": "So parametric model, simple parametric model, linear parametric model.",
                    "label": 0
                },
                {
                    "sent": "And we did this for all nodes.",
                    "label": 0
                },
                {
                    "sent": "And once again, we want to enforce legal multi labels and so we have this condition as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we wanted to do was to try and learn.",
                    "label": 0
                },
                {
                    "sent": "In an online protocol.",
                    "label": 0
                },
                {
                    "sent": "Good hierarchical predictor.",
                    "label": 0
                },
                {
                    "sent": "And align protocol works as follows.",
                    "label": 0
                },
                {
                    "sent": "You there is an algorithm here that receives at each time step an instance vector.",
                    "label": 0
                },
                {
                    "sent": "Then it is required to produce a prediction, which is a legal multi label.",
                    "label": 0
                },
                {
                    "sent": "Then it receives that feedback and then you know it keeps iterating.",
                    "label": 0
                },
                {
                    "sent": "This way it updates its internal state and so on.",
                    "label": 0
                },
                {
                    "sent": "Can we measure?",
                    "label": 0
                },
                {
                    "sent": "The accuracy of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Against the H loss function that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "But we do not measure the actual time.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Five 10510 we do not measure.",
                    "label": 0
                },
                {
                    "sent": "The H loss per say, but we actually measure the age loss of the algorithm compared to the H loss of a given comparison predictor.",
                    "label": 0
                },
                {
                    "sent": "So it's a regret analysis.",
                    "label": 0
                },
                {
                    "sent": "A cumulative regret analysis.",
                    "label": 0
                },
                {
                    "sent": "The comparison predictors that top down compared to that knows basically knows.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The parameter is sitting at each node here, so it knows.",
                    "label": 0
                },
                {
                    "sent": "Hopes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It knows the parametric model here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it is not the biggest optimal classifier for this.",
                    "label": 0
                },
                {
                    "sent": "Parametric model.",
                    "label": 0
                },
                {
                    "sent": "So it's basically mimicking this HVN way of.",
                    "label": 0
                },
                {
                    "sent": "Label label assignment.",
                    "label": 0
                },
                {
                    "sent": "It's top down.",
                    "label": 0
                },
                {
                    "sent": "Once again, node is labeled according to the value of the actual function.",
                    "label": 0
                },
                {
                    "sent": "If it's a root or.",
                    "label": 0
                },
                {
                    "sent": "It's the it's the child of a node that has been labeled one.",
                    "label": 0
                },
                {
                    "sent": "It is not based off, at least because it does not depend on the cost coefficients.",
                    "label": 0
                },
                {
                    "sent": "So we are comparing really comparing our classifier to a classifier which is not a base optimal one.",
                    "label": 0
                },
                {
                    "sent": "Which we again do not know how to compute.",
                    "label": 0
                },
                {
                    "sent": "OK, so the best we could do is this.",
                    "label": 0
                },
                {
                    "sent": "And again we measure the cumulative regret the sum overall try overall examples of the expected of the risk.",
                    "label": 0
                },
                {
                    "sent": "Basically the expected loss of the algorithm compared discounted by the expected loss of this top down compared to here.",
                    "label": 0
                },
                {
                    "sent": "And since we wanted to compare to a top down competitor.",
                    "label": 0
                },
                {
                    "sent": "Where is natural way?",
                    "label": 0
                },
                {
                    "sent": "To go is to use a top down predictor.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, the top down predictor basically is doing.",
                    "label": 0
                },
                {
                    "sent": "Local approximation to each parameter here.",
                    "label": 0
                },
                {
                    "sent": "So it stores at each node weight vector which is meant to approximate the corresponding parameters.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You survive.",
                    "label": 0
                },
                {
                    "sent": "And they will stop down this in the same way.",
                    "label": 0
                },
                {
                    "sent": "OK. And the update is basically similar to what H SVM was doing.",
                    "label": 0
                },
                {
                    "sent": "There is a filtering rule here.",
                    "label": 0
                },
                {
                    "sent": "And now an example is passed to a node.",
                    "label": 1
                },
                {
                    "sent": "I only if the parent node was labeled one.",
                    "label": 0
                },
                {
                    "sent": "On that example.",
                    "label": 0
                },
                {
                    "sent": "OK, otherwise not best.",
                    "label": 0
                },
                {
                    "sent": "And it's top down, and once again, independent.",
                    "label": 1
                },
                {
                    "sent": "Of the season by.",
                    "label": 0
                },
                {
                    "sent": "Which we don't know how to incorporate in a top down scheme.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Naturally.",
                    "label": 0
                },
                {
                    "sent": "He is naturally derived from the parametric model because we have a basically a regularizer, square squares predictor sitting at each node.",
                    "label": 0
                },
                {
                    "sent": "Which is a.",
                    "label": 0
                },
                {
                    "sent": "Nothing tactically unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "Of the parameter at each node.",
                    "label": 0
                },
                {
                    "sent": "Well, I want to get into the details here.",
                    "label": 0
                },
                {
                    "sent": "It naturally arises from the parametric model because this margin.",
                    "label": 1
                },
                {
                    "sent": "This estimated margin is almost conditionally unbiased estimate of the true margin at each node and its accuracy actually depends on the number of examples that each node sees.",
                    "label": 0
                },
                {
                    "sent": "Remember that root nodes see old labels while leaf nodes see.",
                    "label": 0
                },
                {
                    "sent": "Only a small amount of labels.",
                    "label": 0
                },
                {
                    "sent": "So we are very good at root labels park.",
                    "label": 0
                },
                {
                    "sent": "We are poor at at the leaf nodes and you know it can be, you know, running dual variables and all these things.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we have it bound.",
                    "label": 0
                },
                {
                    "sent": "Which actually.",
                    "label": 0
                },
                {
                    "sent": "Makes no assumption, no assumptions on the way.",
                    "label": 0
                },
                {
                    "sent": "The instance vectors are generated.",
                    "label": 0
                },
                {
                    "sent": "So there are pointwise bounds.",
                    "label": 0
                },
                {
                    "sent": "So this guy here is constant.",
                    "label": 0
                },
                {
                    "sent": "This guy is a random variable generated according to the generative model blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "But The X Factor can be worst case as well.",
                    "label": 0
                },
                {
                    "sent": "Sorry can be worst case worst case, but it has to be generated ahead of time, so we let an adversary generate the instance vectors here.",
                    "label": 0
                },
                {
                    "sent": "But we forced the adversary to.",
                    "label": 0
                },
                {
                    "sent": "To choose them ahead of time before knowing what the algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a lose adversary, so weak after sorry.",
                    "label": 0
                },
                {
                    "sent": "And then what we can prove?",
                    "label": 0
                },
                {
                    "sent": "Is a regret bound or the following for we have two three ingredients in this bound.",
                    "label": 0
                },
                {
                    "sent": "One is Delta squared, the other one is this capital C sub I and the third ingredient is this eigenvalue thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have that.",
                    "label": 0
                },
                {
                    "sent": "I inverse dependence on the minimal margin over all examples you have dependence on the contribution of.",
                    "label": 0
                },
                {
                    "sent": "The cost coefficients sitting at each node and the subtree root underneath.",
                    "label": 0
                },
                {
                    "sent": "And you also have a contribution view today.",
                    "label": 0
                },
                {
                    "sent": "The eigen structure of the examples that each node.",
                    "label": 0
                },
                {
                    "sent": "Seize after training.",
                    "label": 0
                },
                {
                    "sent": "OK, and by the way, this is a so-called logarithmic cumulative regret, so instantaneously this is would be a fast rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The expectation there is over everything over all these over these at the end and the labels over these guys, right?",
                    "label": 0
                },
                {
                    "sent": "So so on a particular run of the algorithm, obviously, yeah.",
                    "label": 0
                },
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say it's great, but it's a framework.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "We've been trying to improve a baseline top down label assignment scheme known as HSV N, by about a map based optimal like algorithm.",
                    "label": 1
                },
                {
                    "sent": "And I believe what we did was provide some sort of a modular approach.",
                    "label": 0
                },
                {
                    "sent": "Instead of being principled, it more effective machine learning practice coming to be SVN thing is more like a practical thing.",
                    "label": 0
                },
                {
                    "sent": "We could replace SVM by other things as well.",
                    "label": 1
                },
                {
                    "sent": "Logistic regression, of course, would be one of the candidates.",
                    "label": 1
                },
                {
                    "sent": "How we made some preliminary experiments with that lab to have more experiments on this?",
                    "label": 0
                },
                {
                    "sent": "And they also showed a no line top down algorithm and its regret analysis.",
                    "label": 0
                },
                {
                    "sent": "Based on our regularised squared algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, open questions.",
                    "label": 0
                },
                {
                    "sent": "Got to kind of kinds of open questions.",
                    "label": 0
                },
                {
                    "sent": "Experimental.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there's a clear advantage of the bottom of the bottom up approach on synthetic datasets, and it's less clear on real world datasets.",
                    "label": 1
                },
                {
                    "sent": "Which we are clearly more interested in.",
                    "label": 0
                },
                {
                    "sent": "I don't know why exactly we do not know why.",
                    "label": 0
                },
                {
                    "sent": "That might be the case that the real world datasets are just noisy.",
                    "label": 0
                },
                {
                    "sent": "Too noisy too to make this base like scheme to work well, or it might well be the case that since we are.",
                    "label": 0
                },
                {
                    "sent": "We have this independence assumption.",
                    "label": 0
                },
                {
                    "sent": "This is not me.",
                    "label": 0
                },
                {
                    "sent": "Way to go.",
                    "label": 0
                },
                {
                    "sent": "Once I fixed the value of this value.",
                    "label": 0
                },
                {
                    "sent": "Once I said this to one, the value of these guys are independent random variables, which is not need not be a good assumption in hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "But it simplifies matter quite a lot.",
                    "label": 1
                },
                {
                    "sent": "And of course, as Tom was mentioning, for instance, replace SVM by better algorithms that are more suitable for probability estimations, such as logistic regression as well.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "On the theoretical side.",
                    "label": 0
                },
                {
                    "sent": "Our regret analysis.",
                    "label": 0
                },
                {
                    "sent": "Was not refering to the base optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "We are not comparing against the base optimal classifier for that particular model, which we do not know how to compute.",
                    "label": 0
                },
                {
                    "sent": "Maybe a logistic model again would be a good.",
                    "label": 0
                },
                {
                    "sent": "Candidate to try to.",
                    "label": 0
                },
                {
                    "sent": "To compare to.",
                    "label": 0
                },
                {
                    "sent": "Once we have an analysis cologist model.",
                    "label": 0
                },
                {
                    "sent": "And once again, even in the theoretical work, we would like to remove this independence assumption, which is perhaps not.",
                    "label": 0
                },
                {
                    "sent": "Promising promising Ave thank you, I'm done.",
                    "label": 0
                },
                {
                    "sent": "Please yeah.",
                    "label": 0
                },
                {
                    "sent": "So it seems that in your classification problem you want to compute the maximum posterior solution or four over the the silence of the piece, right?",
                    "label": 0
                },
                {
                    "sent": "You're talking about the BSM thing.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter where it doesn't matter, OK?",
                    "label": 0
                },
                {
                    "sent": "To compute the maximum posteriori, yeah, yeah yeah, it's it's basically captured by this thing.",
                    "label": 0
                },
                {
                    "sent": "Yes, please, thank you.",
                    "label": 0
                },
                {
                    "sent": "So, so why we so?",
                    "label": 0
                },
                {
                    "sent": "Why don't you use kind of?",
                    "label": 0
                },
                {
                    "sent": "Understand it algorithm.",
                    "label": 0
                },
                {
                    "sent": "Like kind of standard algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for any Markov models there is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a tree, but there is an equivalent of 50 to be automated.",
                    "label": 0
                },
                {
                    "sent": "That's it, that's it.",
                    "label": 0
                },
                {
                    "sent": "Once you do that, then also So what did you do?",
                    "label": 0
                },
                {
                    "sent": "So why don't you do bottom and then go back again, which is what we doing?",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "So why did you do that?",
                    "label": 0
                },
                {
                    "sent": "No, I I I I go bottom up and then top down only if this guy gets labeled 0.",
                    "label": 0
                },
                {
                    "sent": "So you're clean.",
                    "label": 0
                },
                {
                    "sent": "Dennis Bachman in this top down.",
                    "label": 0
                },
                {
                    "sent": "It's because I want to know whether one is better than the other.",
                    "label": 0
                },
                {
                    "sent": "Wait, what do you mean?",
                    "label": 0
                },
                {
                    "sent": "So which one is equivalent to that area?",
                    "label": 0
                },
                {
                    "sent": "Part I?",
                    "label": 0
                },
                {
                    "sent": "I guess this one, the bottom up one I guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because you make a hard assignment, but you wouldn't do it.",
                    "label": 0
                },
                {
                    "sent": "The joint probability and then calculate the marginals with having maybe even the markers, not even the math assignment of the labels.",
                    "label": 0
                },
                {
                    "sent": "Exactly come out right, right, right?",
                    "label": 0
                },
                {
                    "sent": "If you formulated it as effective raw and it, but do you agree that this is the definition of Bayes optimal classifier?",
                    "label": 0
                },
                {
                    "sent": "For this this line here?",
                    "label": 0
                },
                {
                    "sent": "Is the one that minimizes the conditional risk.",
                    "label": 0
                },
                {
                    "sent": "If you agree with that, then you should agree with this algorithm as well.",
                    "label": 0
                },
                {
                    "sent": "It's a hard assignment.",
                    "label": 0
                },
                {
                    "sent": "OK. And then after that.",
                    "label": 0
                },
                {
                    "sent": "You have this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is basically the bottom up.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm inspired by this bottomup scheme to build an algorithm that is sort of.",
                    "label": 0
                },
                {
                    "sent": "Combining.",
                    "label": 0
                },
                {
                    "sent": "Probabilities.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My problem is I want to combine what's happening at each local E at each node in some way.",
                    "label": 1
                },
                {
                    "sent": "In some way, either by combining and bottom up fresh or in the top down or whatever else, something.",
                    "label": 0
                },
                {
                    "sent": "I think that if you would apply.",
                    "label": 0
                },
                {
                    "sent": "Go back and forth and then you're done and then you have to match.",
                    "label": 0
                },
                {
                    "sent": "So I don't know why this is.",
                    "label": 0
                },
                {
                    "sent": "I believe this is the Viterbi thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do you need something else?",
                    "label": 0
                },
                {
                    "sent": "Because there's not, this seems to be only going up.",
                    "label": 0
                },
                {
                    "sent": "Well, why do I need something else?",
                    "label": 0
                },
                {
                    "sent": "'cause I, I'm not sure this is the best thing you can do.",
                    "label": 0
                },
                {
                    "sent": "I know you're busy, but no, I'm just kidding, sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm not, I'm not being Bayesian here.",
                    "label": 0
                },
                {
                    "sent": "You know 'cause I'm approximating.",
                    "label": 0
                },
                {
                    "sent": "Everything here is approximated, so it's just a scheme that sort of inspires the way of combining.",
                    "label": 0
                },
                {
                    "sent": "The local the local information here.",
                    "label": 0
                },
                {
                    "sent": "So this would be the best thing you could do if you had this piece of pie available.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, and then you and then that's that's you're perfectly right.",
                    "label": 0
                },
                {
                    "sent": "Then you're done.",
                    "label": 0
                },
                {
                    "sent": "But these things are not available.",
                    "label": 0
                },
                {
                    "sent": "We are approximating them in some weird way and still we are applying.",
                    "label": 0
                },
                {
                    "sent": "We are still running this game over bottom, nothing, but we are not at all.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that the combined thing would be would be a good thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, is that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, because I mean if you would would look at it as kind of 1st doing estimation and then do inference right then.",
                    "label": 0
                },
                {
                    "sent": "This part.",
                    "label": 0
                },
                {
                    "sent": "That we had would be the same after estimation and then you would have some influence and influence.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "Then probably the thing that you do is you have to do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that's still do you think?",
                    "label": 0
                },
                {
                    "sent": "Do you think this is a?",
                    "label": 0
                },
                {
                    "sent": "A good way of estimating this, for instance, a good way of estimating this probabilities.",
                    "label": 0
                },
                {
                    "sent": "Once I have this probabilities then I'm in good shape.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Save one gives me a an approximation to it.",
                    "label": 0
                },
                {
                    "sent": "Why should I wear?",
                    "label": 0
                },
                {
                    "sent": "Should I actually play bottom up?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no, there's no.",
                    "label": 0
                },
                {
                    "sent": "It seems to me there's no guarantee that.",
                    "label": 0
                },
                {
                    "sent": "The overall thing will still work, so I think I think.",
                    "label": 0
                },
                {
                    "sent": "Then if you do inference, you need a 2 pass process box.",
                    "label": 0
                },
                {
                    "sent": "It depends how you define the probability of a child.",
                    "label": 0
                },
                {
                    "sent": "Being negative, given the parents negative or something and.",
                    "label": 0
                },
                {
                    "sent": "If you have the the probability that a child is negative given the parent was classified negative a 0 which is in this model, then you don't need to do to pass it because it determines that.",
                    "label": 0
                },
                {
                    "sent": "So there is this magic parameter.",
                    "label": 0
                },
                {
                    "sent": "We tried doing it, getting the probabilities out and doing inference.",
                    "label": 0
                },
                {
                    "sent": "Then you have this magic parameter which is OK.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when my like my parents classified as negative.",
                    "label": 0
                },
                {
                    "sent": "I actually want to change my child, flip the label of my child because it makes the whole thing more likely had in France will correct parent labels in some sense, but you have this magic parameter that you gotta pick out of and and they work any better.",
                    "label": 0
                },
                {
                    "sent": "Or once you pick.",
                    "label": 0
                },
                {
                    "sent": "If you play this round, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "No obvious way to center, but once you once you set in hindsight that there's a definite yes.",
                    "label": 0
                },
                {
                    "sent": "Basically you can.",
                    "label": 0
                },
                {
                    "sent": "It depends on how big you gotta really big tree and you made a mistake up there somewhere right then, if there's enough kind of probability, mass inference will flip.",
                    "label": 0
                },
                {
                    "sent": "But if you you know that that's popped up right, yeah?",
                    "label": 0
                },
                {
                    "sent": "If you, if you say the parent, if the parents make it classified as negative, then the child is negative with probability one.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, you don't have to do that.",
                    "label": 0
                },
                {
                    "sent": "I think that's maybe.",
                    "label": 0
                },
                {
                    "sent": "Going down never changes in labor.",
                    "label": 0
                },
                {
                    "sent": "Down a bit.",
                    "label": 0
                },
                {
                    "sent": "You had to go down a bit a little bit, yeah?",
                    "label": 0
                },
                {
                    "sent": "Because you have, yeah.",
                    "label": 0
                },
                {
                    "sent": "Correct, if there were any children.",
                    "label": 0
                },
                {
                    "sent": "One is just one layer.",
                    "label": 0
                },
                {
                    "sent": "You do it once once.",
                    "label": 0
                },
                {
                    "sent": "Any suggestions?",
                    "label": 0
                },
                {
                    "sent": "Really would be glad.",
                    "label": 0
                },
                {
                    "sent": "Yeah, other people.",
                    "label": 0
                },
                {
                    "sent": "There are some people.",
                    "label": 0
                },
                {
                    "sent": "Who are they?",
                    "label": 0
                },
                {
                    "sent": "Who are they?",
                    "label": 0
                },
                {
                    "sent": "OK. Model that has global method to try to do even more complicated.",
                    "label": 0
                },
                {
                    "sent": "I'll be glad to.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a complete late model.",
                    "label": 0
                },
                {
                    "sent": "That has a structure out of right, right, right, right.",
                    "label": 0
                },
                {
                    "sent": "Some people more favorable about global methods rather than they say go local method like this won't work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well this bottom up thing that I showed you was a sort of a global method because because of the.",
                    "label": 0
                },
                {
                    "sent": "On the thresholds.",
                    "label": 0
                },
                {
                    "sent": "Where is it?",
                    "label": 0
                },
                {
                    "sent": "Estimates in the training in the training, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a weak global method.",
                    "label": 0
                },
                {
                    "sent": "There's somebody at least what about running time?",
                    "label": 0
                },
                {
                    "sent": "Server running.",
                    "label": 0
                },
                {
                    "sent": "Longer work so.",
                    "label": 0
                },
                {
                    "sent": "Kind of strange to me is that you you need to build this.",
                    "label": 0
                },
                {
                    "sent": "Different inference.",
                    "label": 0
                },
                {
                    "sent": "Kramer so the inference and the training do not necessarily match, so the structure that's right, that's right framework, those captured.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you build a logistic regression into it.",
                    "label": 0
                },
                {
                    "sent": "You could write it as one big graphical model.",
                    "label": 0
                },
                {
                    "sent": "You could do joint inference in that model.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know if it makes any big difference in practice, but it would kind of unify the thing alright, but would it work in practice?",
                    "label": 0
                },
                {
                    "sent": "I mean, would it be practical?",
                    "label": 0
                },
                {
                    "sent": "You think you think it would be practical doing local message passing, right?",
                    "label": 0
                },
                {
                    "sent": "What about training training?",
                    "label": 0
                },
                {
                    "sent": "Sing CRF.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I did some work on that.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Sequences experimentally they do not come up to be honest.",
                    "label": 0
                },
                {
                    "sent": "Base optimality is one advantage, right?",
                    "label": 0
                },
                {
                    "sent": "We are also working.",
                    "label": 0
                },
                {
                    "sent": "But we deal with this car is which are as it 4230 is bigger, so let's say.",
                    "label": 0
                },
                {
                    "sent": "600,600 thousand million documents.",
                    "label": 0
                },
                {
                    "sent": "You're very patient.",
                    "label": 0
                },
                {
                    "sent": "You're very patient.",
                    "label": 0
                },
                {
                    "sent": "Like this, otherwise it's not useful, so so this would be used for labeling search results or.",
                    "label": 0
                },
                {
                    "sent": "Assigning.",
                    "label": 0
                },
                {
                    "sent": "The documents so.",
                    "label": 0
                },
                {
                    "sent": "Things in different ways.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "About this, I didn't want to talk about this here, but still.",
                    "label": 0
                },
                {
                    "sent": "So how somehow we combine little bit of indexing methods so so the nature of data changes if you're dealing with this large, very large hierarchies, usually you cannot infer anything from the top level.",
                    "label": 0
                },
                {
                    "sent": "The balls, let's say 15 levels deep.",
                    "label": 0
                },
                {
                    "sent": "Basically everything is lost.",
                    "label": 0
                },
                {
                    "sent": "How, how deep are the hierarchies by the way?",
                    "label": 0
                },
                {
                    "sent": "78 So much on this transition because it's.",
                    "label": 0
                },
                {
                    "sent": "I said, what would you do instead?",
                    "label": 0
                },
                {
                    "sent": "So we say that this is then.",
                    "label": 0
                },
                {
                    "sent": "Addition problem into 500,000 dollars 600,000 classes.",
                    "label": 0
                },
                {
                    "sent": "Which are somehow similar or connected but.",
                    "label": 0
                },
                {
                    "sent": "When presenting the results, also the presentation results are different.",
                    "label": 0
                },
                {
                    "sent": "Then it's not just 01.",
                    "label": 0
                },
                {
                    "sent": "Usually this kind of higher is always human.",
                    "label": 0
                },
                {
                    "sent": "Already human.",
                    "label": 0
                }
            ]
        }
    }
}