{
    "id": "u6t3enyggm4q26e2gkovl7lmjne5ckoy",
    "title": "Machine Learning in Acoustic Signal Processing",
    "info": {
        "author": [
            "Mark Hasegawa-Johnson, Beckman Institute for Advanced Science and Technology, University of Illinois at Urbana-Champaign"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/mlss09us_hasegawa-johnson_mlasp/",
    "segmentation": [
        [
            "Alright, sounds are more or less OK, so I'm going to.",
            "I'm going to be talking about quite a number of of quite a number of projects actually put together in the context of an overview of all of the methods of machine learning that have been applied for the for the task of doing acoustic signal processing and by acoustic signal processing I mean things like speech recognition, tracking of source location, detection of nonspeech audio events, and answering questions about human speech, perceptual capability."
        ],
        [
            "And audio perceptual capability.",
            "So they're going to be maybe 10 or 15 applications touched on briefly, and for that reason the outline of my talk looks more or less like an outline of the field of machine learning, so I'm going to talk about different criteria that you can choose in order to do that, you can use in order to decide what kind of pattern recognizer to apply to which particular problem in statistical acoustics.",
            "To talk about this ancient division of the the methods of machine learning into discriminative and Bayesian methods were a discriminative method is essentially one that's trained in order to do as well as possible on a specific application of Bayesian method is 1, in which you pay attention to stochastic normalization of all of the of all of the functions estimated so that they become probabilities so that you can string them together in relatively complicated graphical structures in order to, in order to model more more complicated problems, and then a technique that's been.",
            "Particularly useful for me is to is to take those two methods and combine them.",
            "That is to create innocence a neural network in which the hidden nodes are trained for specific tasks, and then the outputs of those hidden nodes are then fed into a Bayesian inference engine."
        ],
        [
            "So in general, when I talk to acousticians, they're interested in answering a particular question about the nature of the universe, or about the nature of the human animal and the way in which we answer questions about the nature of the human animal is this.",
            "Is this little method that was developed by Francis Bacon called the scientific method, namely, hypothesize, observe, and test.",
            "That is, before you actually observe any of the data, you construct a hypothesis about what might be happening in the real world based on your understanding of reality.",
            "And then you collect data as necessary to test that hypothesis to determine whether the hypothesis or the null hypothesis is more like."
        ],
        [
            "Machine learning turns that on its head.",
            "Of course, in a pattern recognition application, we first we first collect the data and then we test the hypothesis and then we formed the hypothesis.",
            "More specifically, we start out with an infinite set of hypothesis.",
            "A hypothesis space, some kind of parameterized universal approximator, that is, a space of functions with which given adequate complexity is able to approximate any function, is able to learn any function, and then we train that space of functions using a training set an we test, we do something.",
            "Finally a little bit similar to to a scientific hypothesis test by using a separate testing database in order to determine whether the weather that whether our hypothesis for the form of what's going on in the real world actually matches."
        ],
        [
            "We observe in the real world, so here's an example of a way in which pattern recognition techniques can be combined with the scientific method.",
            "This is an ongoing set of experiments that I have going on up in my lab.",
            "We have a whole bunch of undergraduates of naive listeners listening to listening to arbitrary mixtures of speech and nonspeech audio.",
            "In this case, it's actually meeting room audio, so people talking about a particular topic and they type on their computers and they closed doors and they and they squeak their chairs and they drop books on the table and things like that, and we asked the subjects to tell us.",
            "When something happens that attracts their attention, this is some of you may recognize that the bottom up versus top down division of attention.",
            "This has been very finely developed in the visual processing community and has not received much attention in the auditory processing community.",
            "We want to know is there something other than just loudness that attracts one's attention to a particular acoustic event?",
            "So we have.",
            "We have two hypothesis here.",
            "Number one, we hypothesize a particular signal model for what makes an acoustic event perceptually salient.",
            "In this case, we look at we look at attributes of the center surround, difference in loudness, and in features like Tony City and frequency contrast and temporal contrast and the second hypothesis is that those same subjects, when asked to detect events like Chair, Squeak an footstep, will be more easily able to detect events that they've previously marked as being salient, or that somebody else has marked as being perceptually salient.",
            "That is your ability to recognize an acoustic event is correlated with the bottom up the bottom up attention grabbing features of that of that event.",
            "Now this box over here.",
            "This acoustic event detector has nothing to do with this site scientific hypothesis.",
            "It's just a black box.",
            "We want to be able to feed in features about the mean rate of the impulses on the auditory nerve, and we want to be able to feed in some estimate of the perceptual salience of what's happening.",
            "And we want to get out some kind of estimate of whether the acoustic event was detected or not.",
            "So we so we put in whatever pattern recognition algorithms we can we can in here we take a set of universal approximators in this case.",
            "Adaboost fed into a into a hidden Markov model.",
            "We trained the train the Adaboost HMM hybrid in order to detect the events as well as possible.",
            "And then we see whether it, whether it matches the performance of human listeners, and if so, then we determined that in fact this bar is important for the as an input for the algorithm."
        ],
        [
            "Alright, that's that's the general structure of innocence.",
            "That's in one slide overview.",
            "That's a description of what I'm going to be talking about today.",
            "How do you?",
            "How do you take a pattern recognition algorithm and apply it in order to answer questions about the real world?",
            "Here a few of the criteria that you might use in order to choose those algorithms.",
            "First of all, is your hypothesis shallow or deep?",
            "In a sense, a discriminative, discriminative algorithm, the discriminative pattern recognition algorithm is extremely useful if.",
            "On your hypothesis can be written in closed form because if it can be written in closed form then the then the error can be minimized in closed form.",
            "If not then you may need to invoke some kind of intermediate variables and if you need intermediate variables then you need some kind of Bayesian architecture."
        ],
        [
            "Second, how much training data do you have in order to learn the form of your hypothesis?",
            "If you have, if you have 100,000 training examples, then number one structural risk minimization is going to be too computationally expensive.",
            "Most of the time and #2 it doesn't make a big difference.",
            "In fact, we don't see much difference beyond about 10,000 training examples between empirical risk minimization and structural risk minimization.",
            "If, on the other hand, you have few that fewer than 1000 training tokens, then with any good universal approximator you're going to over learn the database.",
            "By using straight and empirical risk minimization."
        ],
        [
            "3rd, does your hypothesis depend on time?",
            "That is, does the past state of the system contribute to the way it behaves in the future?"
        ],
        [
            "And second, does is the output of the hypothesis a real number or a or an integer an these last two things, dynamic state and function range serve as a binary categorization of all of the algorithms of machine learning into 4 words that I'm going to use again and again for the rest of the talk.",
            "Those are classification where the output is an integer and it has no hidden state regression where the output is a real vector and there is no hidden state recognition where the output is a series of integers.",
            "And state matters.",
            "An tracking where the output is a series of real valued vectors and the state of the system matters."
        ],
        [
            "Um?",
            "And then we have to train them.",
            "So taking the chart on the previous page and dividing the world into discriminative versus Bayesian methods, first of all, we might look at discriminative methods because they are easy empirical risk minimization.",
            "Using discriminative methods has been was solved by the perceptron in the 1960s.",
            "We essentially choose some kind of universal approximator.",
            "We create an."
        ],
        [
            "Parametric and then we minimize the error metric.",
            "There are at least three or four classes of universal approximators available to us, so universal approximator is essentially a Riemann integrator by the Riemann integral theorem.",
            "Any function over any finite domain can be approximated by an infinite number of tall thin boxes.",
            "The sigmoidal neural network actually doesn't model those.",
            "That is a series of tall thin boxes.",
            "It models, it is a series of tall of tall, thin step functions."
        ],
        [
            "The the mixture Gaussian models.",
            "It is a series of smooth, tall thin boxes and piecewise constants or piecewise linear systems like classification trees and tenuous neighbors do actually model the function as a series of tall thin boxes.",
            "Alright, I'm going to be talking primary."
        ],
        [
            "About sigmoidal networks and mixture Gaussians.",
            "As we go the other piece that you need for discriminative training is some kind of differentiable error metric and most of the algorithms that will talk about can be can be trained using either a Minkowski norm or using some kind of likelihood function which is in effect a log arhythmic Minkowski norm.",
            "So when kowski norm, so the best norm, usually the one that we really want is the zero norm.",
            "We really want to find we really want to say that the hypothesis.",
            "Each sub Theta is correct if it's exactly equal to the label that we're trying to get.",
            "The problem is that that's not a very trainable error metric.",
            "If in fact the hypothesis is not exactly equal to the target label, what do we do next?",
            "And so, and so we have a series of."
        ],
        [
            "Trainable error metrics.",
            "For example, the Manhattan distance in the Euclidean distance.",
            "Where essentially, if we train a neural network to minimize the to minimize the Manhattan distance, we wind up with a neural network that computes the computes the op posteriori median of the of the label distribution.",
            "If we minimize the Euclidean distance, we create a neural network that computes the posteriori mean of the label distribution."
        ],
        [
            "And then finally we apply the chain rule in order to minimize.",
            "That is, for each of the training tokens until the error stops stop changing, we take our parameter set and we adjusted in the negative direction of the gradient.",
            "And we adjust Ada as necessary in order to minimize the minimize the error along the direction of the line that we've chosen, where the direction of the line is can be broken down because of the structure of the universal approximator.",
            "So here's where Riemann actually comes in to help us.",
            "The Universal Approximator says that the functional approximation that we're creating is the sum of a very very large number of tall thin boxes, and each of those tall thin boxes is adjusted only in order to pull in order to approximate the training samples that fall close to that tall thin box.",
            "So we compute the derivative of the error with respect to the hypothesis at that particular at that particular training sample, and then find the gradient of the of that particular hypothesis with respect to the parameter set."
        ],
        [
            "Alright, the applications of this in audio processing or so ubiquitous that I haven't actually given any here.",
            "But, for example, this is.",
            "We use this as the input to to automatic speech recognition into acoustic event detection, and I'll describe one of those systems later when I can add a little bit more color to it."
        ],
        [
            "In the meantime, let me talk about some of the ways in which discriminative training can be extended to more interesting problems.",
            "For example, I haven't described anything having to do with real valued outputs or with systems that depend on time.",
            "Those can be easily, easily put into a discrete, purely discriminative framework by just by just plugging and plugging in the previous time steps hypothesis.",
            "As one of the observations at the current time step, and then we get and then we get a recursive neural network in which the.",
            "The hypothesis is now not is now still the sum of a large number of step functions, but each of.",
            "But each of those step functions is a function, not just to the input variables, but also of the state variable and the state variable is fed back from the from the output of the neural network to the inputs we train over all of the training tokens, but now each of the hypothesis at any given training token at the training token exoti depends on the value of the hypothesis at X sub T + 1, the gradient, the.",
            "The hypothesis anexity depends on the value of the output index of T -- 1.",
            "Therefore, to minimize the error at X of T, we need to we need to pay attention to the error at X sub T + 1 and so on all the way to the end of time.",
            "So we get this algorithm called backpropagation through time.",
            "If we simply differentiate the error with respect to the with respect to the hypothesis, then we get an effect that depends on all of the future times as well.",
            "That's still just an order, an operation for each iteration of the gradient descent it's order."
        ],
        [
            "And so we can compute, for example over this is this is an RN that was trained over a 7 hour training database.",
            "This is the Boston University radio news corpus which is a corpus of speech where the locations of Pichak since that is a promise of prosodically.",
            "Prominent syllables have been marked, and there those targets are shown here in yellow.",
            "The Gibbs ringing at the edge of each target is is not part of the target.",
            "The target is basically zero when there's no pitch accent, and then in every syllable where there's a pitch accent, the target goes to one, and then it goes back to zero and the blue line, which is almost visible.",
            "The blue line is F0 is the pitch of the talker during each of these syllables, and you can see that there's fairly consistent pitch movements all the way through the utterance.",
            "Some of this is noise, but all of the fairly smooth sections are the pitch of the talker, as the talker produces a.",
            "A sentence.",
            "In this case, I think the sentence is wanted.",
            "Chief Justice of the Massachusetts Supreme Court and you can see the pitch accent on the 1st syllable of the word wanted and the pitch accent on the 1st syllable.",
            "The word Massachusetts and no, sorry this would be Chief Justice of the Massachusetts Supreme Court.",
            "And the F0 tends to track the locations of the pitch accents.",
            "The blue tends to be higher during these yellow, but not consistently, and in fact it's really hard to pick out a pattern in the blue line that tells you exactly what pitch accent is, but if you know something about the state of the system, if you've fed the state back into a recursive neural network and then trained it in order to minimize the error over over the training half of the same database, then you can get a pretty good estimate of the locations of the pitch accents, and that output is shown with the.",
            "With the pink line here.",
            "Alright."
        ],
        [
            "Another way in which discriminative algorithms can be pushed a little bit is by reducing the size of the training corpus, so minimizing the error on the training corpus works really well if you have at least 10,000 tokens, but if you have fewer than 1000 tokens, you need to regularize the error term using some kind of generalization term.",
            "Vapnik intervening cause showed that, among others, showed that the probability of the error on on a novel corpus drawn from the same probability density function can be bounded by actually a number of different upper bounds.",
            "And there are quite a number of these available now.",
            "For example, the support vector machine proposed by Vapnik said that let's recast the classifier, recast H sub Theta of X as a dot product in some higher dimensional space and then the the error bound the upper bound on the difference between the training and the test corpus.",
            "Is proportional to the the Euclidean norm of the of the projection of the classifier into that higher space.",
            "Minimum description length turns out to be a similar kind of Minkowski norm of the classifier itself.",
            "So if we can, if we can convert the classifier whatever the parameter set of the classifier is, convert that into a binary program and compute the Kolmogorov description length of that program, then the Minkowski the sorry, the minimum description length, says that the difference between the training and the test corpus.",
            "The expected difference between the training and the test corpus error.",
            "Is bounded by the Zero Norm, the number of bits essentially in the in the binary program describing the classifier.",
            "So we get this upper bound on the test corpus error that matters a lot when when the training corpus error is is under estimated."
        ],
        [
            "Too little data as an example of a way in which we've used this, there's a lot of good evidence to suggest.",
            "For example, for each food we showed in 1973 that that most of the information about the place of articulation of the constant at the beginning of a syllable is encoded in the syllable within about the 1st, 20 to 30 milliseconds after the release of the consonant.",
            "That is to say, if you cut out all of them, so some of the things that Dylan was showing you earlier, if you cut out all of the constant region.",
            "And only play the vowel region, and then if you were to also cut out most of the vowel region and only play back the first, the first 70 milliseconds after the constant release or the first 50 milliseconds after the constant release.",
            "If you play tokens like this to listeners in quiet, they can usually get the place of articulation of the consonant.",
            "If, on the other hand, you cut out all of the voice part and only play the constant part for most consonants, that's a very hard.",
            "It's very hard to determine what the place of articulation is obviously for stop consonants.",
            "All that would be playing back as silence for a nasal consonant.",
            "You'll be playing back a murmur and more, and which if you couldn't see my lips, would be very hard to distinguish.",
            "For Affricative, you can do a pretty good job, because the frication spectrum is usually distinctive, but not necessarily for things like Theta an F. So all of the information is really encoded in that first 50 milliseconds or so, immediately before through about 40 to 50 milliseconds after the release.",
            "The problem is that those acoustic landmarks are relatively few and far between in a 14 hour database like the timid or like the TIMIT training database, there are typically typically 200 to 2000 examples of each of those of each of those landmarks, so we don't have very many of them to go by, so we can't just try to train a classifier that minimizes the error on the training database.",
            "We have to pay some attention.",
            "To the possible differences between the training and the test database, and Partha was the first one to propose that support vector machines are tailor made to do this task in order to generalize from the training to the test database, here are some of our results if you train.",
            "If you train a support vector machine to detect, for example, onset of sonar and see it can does it does the job with 86% accuracy on a task where chance is 50% to detect the onset of a constant.",
            "It does that with 78% accuracy, and so on."
        ],
        [
            "Alright, 1/3 way in which discriminative classifiers could be pushed a little bit is to is to combine dynamics and sparse data.",
            "So for example, if the landmark detectors have to be trained using a small."
        ],
        [
            "All data set, but we need.",
            "But we need some kind of information about the dynamics overtime in order to do correct classification."
        ],
        [
            "Then it's hard to do that using purely discriminative methods and.",
            "In fact, the only way that I've there are there have been a few methods developed recently to do this discriminatively, but it's extremely computationally intensive.",
            "We get much more reasonable methods using using Bayesian inference."
        ],
        [
            "And so let me introduce Bayesian inference.",
            "The other sort of superclass of machine learning methods are the set of Bayesian methods.",
            "That is to say, methods in which methods in which the function that's being estimated by the universal approximator must be a correctly normalized probability density.",
            "It can't be simply an arbitrary function, it must estimate the likelihood of the label variable Y given the observation X, and that that.",
            "That function estimation problem then naturally gives you any other function estimation problem.",
            "If you can correctly estimate the probability density of Y given X, then you can minimize the probability of error by just choosing the maximum of posteriori probability value of Y.",
            "If you can correctly estimate the probability density of Y given X, then you can choose the minimum mean squared error value of Y is your output in a real valued tracking or regression problem?",
            "So if you can do good function estimation for the probability density itself, then you've solved every other problem.",
            "The disadvantage, of course, is that learning the probability density is usually harder than learning the function that you wanted in the 1st place.",
            "Usually you require more data and usually with the same size database you're subject to a larger amount of error in estimating the probability density.",
            "Then you would be in simply estimating Y is equal to H of X directly.",
            "The big advantage is that with correct normalization you can string together a whole sequence of these latent variables in order to in order to build."
        ],
        [
            "Actively complex models, so here's perhaps the most well, at least in speech.",
            "The most famous example of a Bayesian system hidden Markov model is a system in which the label at each time depends on the labels at all of the previous times, as well as depending on the acoustic observations.",
            "That is to say, we suppose that that your speech articulation system is some set of moving tongue and lips, and so on, and some set of intentions and a vocal fold that where the vocal folds approximate and then the vocal folds are separate and that in effect there's only a discrete finite number of settings of all of that apparatus that matter.",
            "So we move through this space of finite states.",
            "This discrete space of state variables, and in a particular state.",
            "The speaker produces a particular set of acoustic Spectra.",
            "So that in effect, the probability of a particular set of acoustic Spectra given a particular set of uttered words.",
            "Is the is the probability of the current word given the previous word times the probability that the speaker's lips and tongue and Gladys and so on transition from the state that they were in at time T -- 1 to the state there in time time T given the word that you're in times the probability of getting in a particular acoustic spectrum given that the lips and tongue and Gladys and soft palate and so on are in a particular state.",
            "These things are this is this is the usual decomposition that's used in speech recognition and hidden Markov models.",
            "I think you've seen in Karen's talk on Tuesday that we can decompose it further in order to bring more information to bear.",
            "But the usual decomposition is to talk about the language model, which is a look up table.",
            "The pronunciation model, which is a look up table in the acoustic model, which is itself some kind of universal approximator, like like a mixture of Gaussians."
        ],
        [
            "Learning in a Bayesian system is usually some variation of maximum likelihood learning, and as in a discriminative system, if you have fewer than 1000 tokens, you need to do some kind of regularization.",
            "The usual tip so two of the common kinds of regularization are maximum of posteriori probability estimation, where instead of maximizing the probability of the data set, we maximize the probability of the parameters.",
            "That is, we maximize the probability of the log prior of the log probability of Theta with no information, and then the log probability of the data set given the parameters.",
            "In effect, this log P of Theta serves as a regularization term, just as the generalization error was a regularization term in discriminative methods.",
            "So it serves in effect as an upper bound on the on the difference that we believe to be possible between in the training and the test database.",
            "Maximum entropy winds up having the same form or can be rewritten in the same form, except that we assume that we don't know anything at all about P of Theta.",
            "We don't know anything at all about the prior except that it's a probability density.",
            "And since in the space of probability densities those with large entropy are are much more numerous than those with small entropy, we estimate the log prior of Theta to be by the by the entropy of the probability density implied by Theta.",
            "That is the measure of the probability densities with with high entropy is larger than the measure of those with low entropy."
        ],
        [
            "Alright.",
            "And then in testing, once we've learned these parameters, we can compute the.",
            "I said that given the given the likelihood function, you know everything that there is to know you know how to compute the minimum probability of error classifier.",
            "And you also know how to compute the minimum mean squared error estimate of Y.",
            "That can be that can be computed from all of these intrinsic variables by just marginalizing by just summing over all of the possible values of the intrinsic variables.",
            "So in training we use maximum likelihood and testing.",
            "We use a minimum probability."
        ],
        [
            "Our classifier.",
            "Um?",
            "Rather than give you speech recognition examples of a hidden Markov model, I thought I would show you something that you may not have seen before.",
            "Hidden Markov models can be used for regression just as much as they can be used for recognition.",
            "Here's how if we if we have Gaussian states, if each state has a.",
            "If each state says that the acoustic spectrum is Gaussian distributed, then that is as shown here.",
            "That is, the probability of X given the state variable.",
            "Is the is E to the minus one half X -- X bar and y -- y barware?",
            "Now why is a real valued vector that we're trying to estimate?",
            "Then we can.",
            "We can compute the minimum mean squared error estimate of this real valued vector by just summing over the posterior probability of the states.",
            "The linear regression formula.",
            "This is the linear regression formula that one gets from jointly Gaussian random variables.",
            "You take the mean value the op priore mean of Y of the Y vector plus X minus it's a priority mean multiplied by the correlation between between X&Y.",
            "So here's the here's the linear regression formula for jointly Gaussian random variables.",
            "And we sum that over all of the different possible Gaussians that might apply to the data weighted by the posterior probability that that's the correct Gaussian.",
            "This gives us a non linear regression for Y given X."
        ],
        [
            "That can be extended to a hidden Markov model by by including state.",
            "So for example, we can assume that that X&Y are not just jointly Gaussian with each other, but they're also jointly Gaussian with the hidden state variable Y at the previous time, and the result is exactly like the regression that I showed in the previous formula, except that now the correlation between the correlation between between X&Y depends on varies as a function of time depends on what our current estimate of the of the Y variable might be, an essentially rather than computing.",
            "Rather than computing awaited sum of linear regression formulas, now I'm computing a weighted sum of Kalman filters.",
            "Where the Kalman filter is linear regression with."
        ],
        [
            "Parameters updated overtime.",
            "Here's an example of an experiment that we did using comparing HMM regression and and switching Kalman smoother.",
            "The results were unfortunately not too compelling, so the the task was to try to estimate the position of the tongue given the acoustic spectrum.",
            "So we have a database of.",
            "We have a database of of tracked pellets on the surface of the tongue and a set of a set of matched acoustic Spectra and we use we use HMM regression.",
            "That is, we assume that there's no dependence of the current position of the articulators on their previous position except as specified by the state of the hidden Markov model.",
            "And we also use a switching Kalman smoother.",
            "That is, we use the Bayesian network shown."
        ],
        [
            "Previous page where the articulators, which are the wise depends not only on the current discrete state which is the S, but they also depend on the previous positions of the articulators which the previous previous Y vector."
        ],
        [
            "And we find a consistent but extremely small under .1 millimeter reduction of the tracking error using the switching Kalman smoother as opposed to using the hidden Markov model regression.",
            "And we haven't been able to prove statistical significance because there are no database is large enough to prove statistical significance within OH point 1 millimeter difference."
        ],
        [
            "Alright, finally I'd like to talk about the methods that I've actually found most useful in my own research.",
            "Those are methods that link together discriminative techniques and Bayesian techniques, using each for the tasks for which each one is most.",
            "Each one is best suited.",
            "That is to say, we assume that that that time matters.",
            "We assume that latent variables matter and that therefore some kind of Bayesian technique is important.",
            "You need to estimate the probability of the labels given the observations with reference to some kind of some kinds of hidden variables.",
            "But we assume also that the observations by themselves are not really enough to give you very good information about what those hidden variables might be that you can get a much better estimate of those hidden variables by computing some kind of transformation of the observations.",
            "These transformations we trained then using using a small amount of labeled data using a small amount of labeled data, we can train a neural network or a support vector machine in order to estimate, say, the phone.",
            "A logical distinctive features, say, estimate those landmark positions that I showed you earlier.",
            "Given the acoustic spectrum, and then using those SVM trained on relatively small amount of data.",
            "We then plug those in as the front end.",
            "For a Bayesian system like a hidden Markov model.",
            "So the training database includes a relatively small amount of data in which the landmarks.",
            "These capital these F vectors are labeled and a relatively large amount of data in which they are not labeled.",
            "But we can learn we can learn the relationship between XY and F hat in effect using the."
        ],
        [
            "Using the Bayesian inference.",
            "We train the train.",
            "We learn the F variables.",
            "We train a local discriminative classifier in order to estimate these F variables.",
            "The locations of the landmarks using using some kind of Minkowski norm minimization.",
            "And then we learn a joint probability density for F&X&Y using Bayesian methods.",
            "So if I put both F&X into this, this is what's called Attenda method, where the hidden Markov model or the dynamic Bayesian network pays attention to both the outputs of the support vector machines and the original acoustic spectrum."
        ],
        [
            "Here's an example of a system that we've had pretty good luck with.",
            "We take the the original acoustic distribution of sounds related to different phonemes, and we passed them through a support vector machine in order to compute this.",
            "Some kind of implicit Hilbert space that is, the support vector machine is a linear classifier in implicit Hilbert space, and if the SVM is trained in order to minimize the in order to minimize the structural risk over some usually relatively small training database.",
            "Then it's in effect, computing a projection into into a Hilbert space where the clouds are separated as well as they possibly can be.",
            "But if it's computing a projection into a space where the clouds are separated then, then the HMM has less to learn.",
            "The hmm really only has to learn only has to learn the likelihood function in this in this better space you have a question.",
            "The training data for the SVM is is a set of a set of well.",
            "In this case it's ntim it it's it's, it's a.",
            "It's a relatively small database where the SVM outputs are labeled, so we have we have a large database, the switchboard database for example, where we don't know what the phoneme transcriptions are, and we have a small database like end, timid where we do, and so we train the SVM is using intimite and then and then train the HMM using using a larger chunk of switchboard.",
            "That makes sense.",
            "OK."
        ],
        [
            "Alright, and here here error rates on here error rates on telephone speech.",
            "This is actually on the test corpus of intimite and although sorry these are accuracies on intimite, the baseline system, just an FCC hidden Markov model doesn't get a very good accuracy.",
            "It gets almost 40% accuracy with about with about 35 Gaussians per mixture Gaussian the the tandem systems using support vector using the outputs of the phmsa's features.",
            "Gets a better accuracy.",
            "Still, we're not.",
            "You know, these are relatively low accuracies compared to what you can do with 16 kilohertz sample data, but these are pretty close to the state of the art for for telephone band speech, so phoneme recognition accuracy on telephone speech runs around 40% and we can get it as high as 44% using a hybrid system with.",
            "With SPMS computing the distinctive features at each time."
        ],
        [
            "Here's another example of a tandem system.",
            "This one was done by Vikram Mitra at at the University of Maryland, where we've actually compared the switching Kalman filter system that I showed you before to a system where recursive neural network.",
            "So a system that already has a discriminative system that already has some state in it tries to estimate the tries to estimate the articulator positions.",
            "The opening of the velum, the local, the construction location of the tongue body, that construction.",
            "Degree of the tongue body.",
            "The construction location of the tongue tip and the construction degree of the tongue.",
            "Tongue tip and I tried to get him to send me a copy that had the raw and outputs without common filtering, smoothing and he refused because it's too noisy.",
            "He was willing to show me on his own computer, but basically without common smoothing this is.",
            "It tracks pretty well, at least according to the training it should track with a lower mean squared error, but it has all of these little high frequency mistakes that one would never actually that a human would never make in trying to.",
            "Go from this acoustic spectrum to the true tongue body construction location.",
            "Those high frequency mistakes can be smoothed out by assuming that the articulators have some dynamic state that the articulator position doesn't change too rapidly as a function of time that can be done using either a simple lowpass filter or using a Kalman smoother.",
            "So in a sense, we're using the recursive neural net to compute these locations and then smoothing them by feeding the output of the neural network into a common into a common smoother.",
            "And that gives us the best performance that we."
        ],
        [
            "Had so far.",
            "Two more examples of tandem systems, uh, another one in which we've.",
            "In which we've had reasonable success is the detection of nonspeech acoustic events.",
            "Here's the description of this task from the clear acoustic event detection detection task description.",
            "Essentially, this is related to the task that I showed you at the beginning of the talk, where we have we have 15 people sitting around a table in a conference room, and they occasionally drop binders on the table, and we want to detect the binder drops, and we want to detect the chair squeaks, and we want to detect the spoons dropped in coffee cups, and we want to detect footsteps.",
            "And we want to detect keyboard clicks.",
            "This is a really difficult task, especially when you start talking about keyboard clicks, because if speech is the noise and these are these are these are speech, engine, speech and language engineers, so they're talking all the time, so there's no.",
            "There's no break in the background noise if the speech is the background noise, the keyboard click is occurring at a signal to noise ratio of around around minus 10 -- 15 DB.",
            "It's very hard to detect anything at that kind of negative SNR.",
            "The spectral structure of a keyboard click is quite different from the spectral structure of speech.",
            "It's not even clear that the band between 500 and 3000 Hertz is the is the correct band in which to be looking for information about a keyboard click.",
            "And finally, each of these events has a very different spectral structure.",
            "A door slam doesn't look very much like like a."
        ],
        [
            "Clicking keyboards, so here a couple of slides to demonstrate those problems.",
            "Here's here's an example where the signal to noise ratio is not actually negative.",
            "We have a binder dropped on the table here at here at the center time in the middle of the utterance.",
            "Good morning, everybody, I'm glad you could all come.",
            "The speech is the SNR in this case is if you consider the binder to be the signal and everything else to be the speech the SNR is on the order of five DB.",
            "And that's relatively good, and that's much better than you would get with footsteps.",
            "Well, here's an example.",
            "Here's paper rustling at the beginning, and if you compute the overall signal level of the paper rustling compared to the overall signal level of the speech over the entire utterance, the SNR would be negative."
        ],
        [
            "The spectral structures are quite different.",
            "Here's the spectrogram of a set of footsteps.",
            "You can see them here and here, here, here, here, here.",
            "Here's the spectral structure of somebody jingling.",
            "A set of keys as they're trying to take their car keys out or their office keys out.",
            "And then here's the spectrogram of speech, and you can see that acoustic features that have been developed for speech or not going to be terribly useful for detecting footsteps, and likewise the ones developed for footsteps, may not be useful for detecting."
        ],
        [
            "Jangling keys so the first problem we have to solve is trying to figure out what what kind of summary of the spectrogram that we want to use for this recognition task.",
            "We've taken the the most successful thing that we've actually implemented and tested so far, basically over generates every possible acoustic feature that you might possibly imagine would be useful for this task.",
            "So MFC Mel frequency capsular coefficients, perceptual LPC, computed with a few different frame sizes, energy zero crossing rate, and then we, and then we use a machine learning algorithm to choose the best features we want to choose the we want to compute a matrix W where each row of.",
            "Where each row of W is an indicator vector, each row of W just picks out one of the features from X.",
            "That's going to be that's going to be most useful for the task of detecting at least one of those nonspeech audio events.",
            "And then given this vector of features F that are selected by the W matrix, we then want to compute a hidden Markov model of the underlying events and.",
            "If you're trying to do event classification in silence, if you have a waveform which is just a chair, squeak and another, which is just a key Jingle.",
            "In fact, you don't need this hidden Markov modeling, but if you if you're trying to detect them in a continuous full hour recording of a meeting room, this becomes extremely important.",
            "This was in fact the the analysis that the clear team did suggested that this was the reason that our system won the competition, because we can, we can actually track footsteps better.",
            "For example better than anybody else, because we know that.",
            "If there are footsteps at time T, there usually are also footsteps at time T + 1, and the same is not true of door slams."
        ],
        [
            "That feature selection process that I described a moment ago.",
            "It turns out to be pretty similar to Adaboost, actually, but it can be.",
            "It can be formulated also.",
            "It can also be Bayesian formulated.",
            "There's an interesting duality between the discriminative feature selection and the Bayesian formulation.",
            "That is to say, if the only thing we were able to observe was this was this one feature WK transpose X where WK is an indicator vector and X is the original spectrum.",
            "Then the then it's not too hard actually to construct a kernel estimate of the probability density of the joint probability density between this one feature and the output label that we're looking for.",
            "A1 dimensional kernel density estimator doesn't require that much training data data, but if we can construct a pretty good estimate of the probability density of that one feature correlated with the existence versus non existence of a particular kind of acoustic event, then we can also construct a pretty good estimate of the of the Bayes error of the minimum possible error that we can achieve using that feature.",
            "That minimum possible error is just the just the integral overall over over Y and over X.",
            "Of the probability that the that the Bayes optimal label is not equal to the true label.",
            "That is, we take over all of the X&Y space we integrate over all of the regions where where the label is not equal to the estimated label.",
            "We can estimate that on a database, of course, by just summing the integrand over all the training tokens."
        ],
        [
            "There's more, though we can, we can estimate by summing over the training database, all of the training tokens for which the Bayes optimal estimator given that feature is not equal to the true label given that feature.",
            "But that makes the assumption that the classifier only gets one choice that we don't pay attention to.",
            "The second and third and 4th choices of the classifier if we allow the 2nd and 3rd and 4th choices, the classifier to be considered, then we could compute this sort of softened version of the.",
            "Bayes error estimate.",
            "That is to say, we we sum over all of the database.",
            "The rank of the correct label given the feature.",
            "So we take this take this joint probability density of the the label and the feature, and we look at that for all of the different labels and we find is the correct label.",
            "The first choice or the second choice, or the third choice of the fourth choice.",
            "That gives us a metric that we want to minimize over the entire training database."
        ],
        [
            "And that's often does turn out to be useful.",
            "The accuracies are the accuracies are about 24% with a simple MFC C Mel frequency.",
            "Kestrel coefficient recognizer with the Hard Bayes estimate it jumps up to 30% accuracy and with the Soft Bayes estimate it jumps up to 3232% accuracy.",
            "This is still a hard task with classification.",
            "If you know that a particular waveform is one of these non speech audio events, you can classify it with with 9598% accuracy.",
            "Using just using support vector machine.",
            "If you don't know where they occur in a meeting and if you have these negative SNR situations, then the accuracies are still well, 32 percent is the best that's been reported yet."
        ],
        [
            "OK, and then one more application of these hybrid methods that I want to talk about is the problem of is back to the basic problem of automatic speech recognition using a neural network to compute features that are then observed by a hidden Markov model.",
            "This actually is a lot like the problem that I just showed you.",
            "We want to compute.",
            "We want to compute features that are a vector multiplied by the input.",
            "We have an observation we want to multiply it by some kind of selection vector, and then that's going to be the feature.",
            "Except now the selection two things are changed, one, the selection vector is no longer an indicator, it's just come some kind of steerable linear summation over the input feature space and 2nd will allow a second layer in the in the neural network will allow us will allow sigmoid.",
            "Hidden nodes that are then recombined in order to compute the output features.",
            "Alright, and then we'll compute a hidden Markov model too."
        ],
        [
            "Maybe it does.",
            "Alright, the hidden Markov model itself is trained using the Baum Welch algorithm.",
            "Baum Welch algorithm, the."
        ],
        [
            "The the hybrid system is usually trained by by taking a database as I described before, where the phonemes are labeled and by computing by minimizing the difference between this F and the target value of F. But there's another thing that can be."
        ],
        [
            "And we can take this Baum Welch algorithm which computes not really the log probability of the database, but the expected log probability of the database where the expectation is over the posterior probability density of the hidden variables that that Bayesian training."
        ],
        [
            "Material can actually be fed back into the neural network.",
            "Once you sit down and try to write the derivative of the of the expected log probability with respect to each of the parameters in the neural network, it's actually not that hard to write it down.",
            "You wind up with something that looks like the sum over all of the training tokens of the sum over all possible hidden states of the Baum Welch probability.",
            "This is the posterior probability of the state variable at time T given all of the observations available to you multiplied by then the usual neural network backpropagation.",
            "Terms."
        ],
        [
            "There's a problem with that though, and the problem with that is the problem of spurious Maxima, and in order to talk about the problem of serious Maxima, let me back up to a standard sort of standard problem that I teach in my in my machine learning class.",
            "If you have a mixture Gaussian and you're trying to train the mixture Gaussian in order to maximize the likelihood of a training data set, it's always possible to train it such that the likelihood of the training data set is infinite.",
            "In other words, it's always possible to generate a mixture Gaussian that represents any particular training data set.",
            "With infinite probability, and it's done as follows, we."
        ],
        [
            "One of the Gaussians and nonzero variance and at least one of them is zero variance, and then we set the mean of the Gaussian with zero variance equal to any particular training token.",
            "It doesn't matter which one, just pick one, set the mean of the Gaussian equal to that training token, and give it 0 variance.",
            "That Gaussian therefore gives the tree."
        ],
        [
            "Data set, infinite probability and therefore the the."
        ],
        [
            "And therefore when you add together all of those Gaussians, the training data set has infinite probability.",
            "This is a Canonical case of overtraining.",
            "This is something we would like to avoid by some kind of regular."
        ],
        [
            "Nation.",
            "In the bomber back propagation algorithm, they showed on the previous slide.",
            "This is this is equivalent to training the neural network so that it outputs zero valued features for a large subset of the database.",
            "So if the weight vector is on the output layer or on the hidden nodes go to zero, then we get, then we get infinite probability Gaussians.",
            "So here's here's the general form of the solution.",
            "The general form of the solution is that we want to constrain the norm of the transform computed by."
        ],
        [
            "A neural network.",
            "Here are two ways that that can be done.",
            "One is straightforward constrained optimization using Lagrangian multipliers to constrain the norm of the output layer and the norm of the hidden layer to each be one.",
            "Here's a somewhat trickier method.",
            "We can constrain the Jacobian of the transform computed by the entire neural network to be a volume preserving to be a unit Jacobian.",
            "That's now."
        ],
        [
            "Possible in general, but there's a particular class of transforms that's extensively studied in statistical mechanics that does this for us, so here's the analogy.",
            "The analogy between the speech recognition problem and the statistical mechanics problem take the input vector X and arbitrarily divided.",
            "It doesn't even matter which features go where, but arbitrarily divided into 2 sub vectors.",
            "Take the output vector Y and arbitrarily divide it into 2 sub vectors.",
            "That is the output of the neural network.",
            "I guess this should be F. I'm sorry for the change in notation.",
            "Take the output of the neural network and arbitrarily divide it into 2 sub vectors and interpret them as follows.",
            "In the context of statistical mechanics, called the first sub vector, whatever it happens to be, call it the positions of some mysterious set of objects in our in our imagined mechanical system and call the SEC subvector of the set of velocities in our mysterious system.",
            "X1 and X2 represent the positions at a particular time.",
            "Y1 and Y2 represent the positions at some future time, sometime progressed.",
            "An arbitrary amount of time into the future in this imagined mechanical system.",
            "In any in any non dissipative system, any system in which energy is conserved.",
            "The the total energy of the system at the first time.",
            "The total energy of X is equal to the total energy of Y.",
            "And therefore the entire system.",
            "If the if the system is energy conserving, the entire mechanical system has to be has to be a volume preserving transform.",
            "The transformation from any X to any Y has to be a volume preserving transform.",
            "That is its Jacobian has unit determinant.",
            "We can impose that by computing something called the kinetic energy and something called the potential energy and requiring that the sum of those two things is constant overtime.",
            "And here's one transformation that that maintains the constancy of those two things.",
            "We could perhaps estimate the gradient of V and the gradient of T directly, except that those things need to be irrotational, and so it's the easiest way to get a neural network to compute.",
            "An irrotational transform turns out to actually train the network to compute V into compute T directly.",
            "So we set up a.",
            "Two neural networks, one to compute the potential energy, one to compute the kinetic energy we train them to do so, but we train beyond the fact that this has to be scalar and this has to be scalar.",
            "There's no other constraint on the form of this, so we train the form of that function."
        ],
        [
            "In order to minimize error on the training database and because of because because of the form because that function has those two scalars in it.",
            "Because it's a simplistic transform, we're guaranteed a volume preserving transformation computed by this neural network and therefore were guaranteed that this problem of spurious Maxima doesn't occur.",
            "So here's a particular system where this is actually on TIMIT phone recognition accuracies using a baseline system where 74% using a maximum likelihood linear transform, the same architecture that I just showed you, but without the non linearity.",
            "We get a 75% accuracy and using a nonlinear maximum likelihood transform transform we get a 76% accuracy.",
            "So these are small but statistically significant improvements by this."
        ],
        [
            "Non linearity.",
            "Alright, so lots and lots of different different experiments I've described.",
            "Essentially, there are three points that I want to that I want to specify.",
            "First of all, we can.",
            "We can use pattern recognition for any for any scientific question.",
            "For any hypothesis test that you want to compute, pattern recognition is backwards from scientific hypothesis testing in the sense that rather than choosing a hypothesis, we choose a hypothesis space family of universal appeal."
        ],
        [
            "Oximeters.",
            "2nd for any hypothesis that includes hidden variables, it helps to estimate probability densities rather than to estimate the function directly."
        ],
        [
            "And 3rd and finally we get the benefits of both of those methods by if we have a smaller auxiliary database where we can train these discriminative classifiers and then a larger larger database where we can train the Bayesian system to compute inference and that's."
        ],
        [
            "I have questions.",
            "Are there any questions?",
            "OK.",
            "Yes.",
            "We haven't gone back to look at what the classifier found for us.",
            "We fed in events with a whole bunch of different, so we used.",
            "First of all, we used a frame rate of.",
            "Actually, I can't remember whether the basic frame rate was five or 10 milliseconds, but we used a fixed frame rate.",
            "But then we computed features over a variety of timescales centered at that at that frame spacing, so we computed features with with 10 millisecond windows and with 25 millisecond windows, and with and with 50 millisecond windows and and allowed the algorithm to choose for us which features were relevant.",
            "Feature went into the pool, if it was it reduced the reduced the data rank over the entire training data set.",
            "So essentially if it.",
            "What what wound up happening is that one feature would be chosen because it was useful for one class of acoustic events.",
            "A different feature might be chosen because it was useful for a different class of acoustic events and.",
            "And I'm afraid the answer to your question is I don't know because we haven't gone back to look and see what time scales were useful for which events.",
            "Thanks any other questions?",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, sounds are more or less OK, so I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about quite a number of of quite a number of projects actually put together in the context of an overview of all of the methods of machine learning that have been applied for the for the task of doing acoustic signal processing and by acoustic signal processing I mean things like speech recognition, tracking of source location, detection of nonspeech audio events, and answering questions about human speech, perceptual capability.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And audio perceptual capability.",
                    "label": 0
                },
                {
                    "sent": "So they're going to be maybe 10 or 15 applications touched on briefly, and for that reason the outline of my talk looks more or less like an outline of the field of machine learning, so I'm going to talk about different criteria that you can choose in order to do that, you can use in order to decide what kind of pattern recognizer to apply to which particular problem in statistical acoustics.",
                    "label": 0
                },
                {
                    "sent": "To talk about this ancient division of the the methods of machine learning into discriminative and Bayesian methods were a discriminative method is essentially one that's trained in order to do as well as possible on a specific application of Bayesian method is 1, in which you pay attention to stochastic normalization of all of the of all of the functions estimated so that they become probabilities so that you can string them together in relatively complicated graphical structures in order to, in order to model more more complicated problems, and then a technique that's been.",
                    "label": 0
                },
                {
                    "sent": "Particularly useful for me is to is to take those two methods and combine them.",
                    "label": 0
                },
                {
                    "sent": "That is to create innocence a neural network in which the hidden nodes are trained for specific tasks, and then the outputs of those hidden nodes are then fed into a Bayesian inference engine.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, when I talk to acousticians, they're interested in answering a particular question about the nature of the universe, or about the nature of the human animal and the way in which we answer questions about the nature of the human animal is this.",
                    "label": 0
                },
                {
                    "sent": "Is this little method that was developed by Francis Bacon called the scientific method, namely, hypothesize, observe, and test.",
                    "label": 1
                },
                {
                    "sent": "That is, before you actually observe any of the data, you construct a hypothesis about what might be happening in the real world based on your understanding of reality.",
                    "label": 1
                },
                {
                    "sent": "And then you collect data as necessary to test that hypothesis to determine whether the hypothesis or the null hypothesis is more like.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning turns that on its head.",
                    "label": 0
                },
                {
                    "sent": "Of course, in a pattern recognition application, we first we first collect the data and then we test the hypothesis and then we formed the hypothesis.",
                    "label": 1
                },
                {
                    "sent": "More specifically, we start out with an infinite set of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "A hypothesis space, some kind of parameterized universal approximator, that is, a space of functions with which given adequate complexity is able to approximate any function, is able to learn any function, and then we train that space of functions using a training set an we test, we do something.",
                    "label": 0
                },
                {
                    "sent": "Finally a little bit similar to to a scientific hypothesis test by using a separate testing database in order to determine whether the weather that whether our hypothesis for the form of what's going on in the real world actually matches.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We observe in the real world, so here's an example of a way in which pattern recognition techniques can be combined with the scientific method.",
                    "label": 1
                },
                {
                    "sent": "This is an ongoing set of experiments that I have going on up in my lab.",
                    "label": 0
                },
                {
                    "sent": "We have a whole bunch of undergraduates of naive listeners listening to listening to arbitrary mixtures of speech and nonspeech audio.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's actually meeting room audio, so people talking about a particular topic and they type on their computers and they closed doors and they and they squeak their chairs and they drop books on the table and things like that, and we asked the subjects to tell us.",
                    "label": 0
                },
                {
                    "sent": "When something happens that attracts their attention, this is some of you may recognize that the bottom up versus top down division of attention.",
                    "label": 0
                },
                {
                    "sent": "This has been very finely developed in the visual processing community and has not received much attention in the auditory processing community.",
                    "label": 0
                },
                {
                    "sent": "We want to know is there something other than just loudness that attracts one's attention to a particular acoustic event?",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We have two hypothesis here.",
                    "label": 0
                },
                {
                    "sent": "Number one, we hypothesize a particular signal model for what makes an acoustic event perceptually salient.",
                    "label": 0
                },
                {
                    "sent": "In this case, we look at we look at attributes of the center surround, difference in loudness, and in features like Tony City and frequency contrast and temporal contrast and the second hypothesis is that those same subjects, when asked to detect events like Chair, Squeak an footstep, will be more easily able to detect events that they've previously marked as being salient, or that somebody else has marked as being perceptually salient.",
                    "label": 0
                },
                {
                    "sent": "That is your ability to recognize an acoustic event is correlated with the bottom up the bottom up attention grabbing features of that of that event.",
                    "label": 0
                },
                {
                    "sent": "Now this box over here.",
                    "label": 0
                },
                {
                    "sent": "This acoustic event detector has nothing to do with this site scientific hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It's just a black box.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to feed in features about the mean rate of the impulses on the auditory nerve, and we want to be able to feed in some estimate of the perceptual salience of what's happening.",
                    "label": 0
                },
                {
                    "sent": "And we want to get out some kind of estimate of whether the acoustic event was detected or not.",
                    "label": 0
                },
                {
                    "sent": "So we so we put in whatever pattern recognition algorithms we can we can in here we take a set of universal approximators in this case.",
                    "label": 0
                },
                {
                    "sent": "Adaboost fed into a into a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "We trained the train the Adaboost HMM hybrid in order to detect the events as well as possible.",
                    "label": 0
                },
                {
                    "sent": "And then we see whether it, whether it matches the performance of human listeners, and if so, then we determined that in fact this bar is important for the as an input for the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, that's that's the general structure of innocence.",
                    "label": 1
                },
                {
                    "sent": "That's in one slide overview.",
                    "label": 0
                },
                {
                    "sent": "That's a description of what I'm going to be talking about today.",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 1
                },
                {
                    "sent": "How do you take a pattern recognition algorithm and apply it in order to answer questions about the real world?",
                    "label": 1
                },
                {
                    "sent": "Here a few of the criteria that you might use in order to choose those algorithms.",
                    "label": 0
                },
                {
                    "sent": "First of all, is your hypothesis shallow or deep?",
                    "label": 0
                },
                {
                    "sent": "In a sense, a discriminative, discriminative algorithm, the discriminative pattern recognition algorithm is extremely useful if.",
                    "label": 0
                },
                {
                    "sent": "On your hypothesis can be written in closed form because if it can be written in closed form then the then the error can be minimized in closed form.",
                    "label": 0
                },
                {
                    "sent": "If not then you may need to invoke some kind of intermediate variables and if you need intermediate variables then you need some kind of Bayesian architecture.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second, how much training data do you have in order to learn the form of your hypothesis?",
                    "label": 0
                },
                {
                    "sent": "If you have, if you have 100,000 training examples, then number one structural risk minimization is going to be too computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "Most of the time and #2 it doesn't make a big difference.",
                    "label": 1
                },
                {
                    "sent": "In fact, we don't see much difference beyond about 10,000 training examples between empirical risk minimization and structural risk minimization.",
                    "label": 1
                },
                {
                    "sent": "If, on the other hand, you have few that fewer than 1000 training tokens, then with any good universal approximator you're going to over learn the database.",
                    "label": 0
                },
                {
                    "sent": "By using straight and empirical risk minimization.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd, does your hypothesis depend on time?",
                    "label": 0
                },
                {
                    "sent": "That is, does the past state of the system contribute to the way it behaves in the future?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And second, does is the output of the hypothesis a real number or a or an integer an these last two things, dynamic state and function range serve as a binary categorization of all of the algorithms of machine learning into 4 words that I'm going to use again and again for the rest of the talk.",
                    "label": 1
                },
                {
                    "sent": "Those are classification where the output is an integer and it has no hidden state regression where the output is a real vector and there is no hidden state recognition where the output is a series of integers.",
                    "label": 1
                },
                {
                    "sent": "And state matters.",
                    "label": 0
                },
                {
                    "sent": "An tracking where the output is a series of real valued vectors and the state of the system matters.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then we have to train them.",
                    "label": 0
                },
                {
                    "sent": "So taking the chart on the previous page and dividing the world into discriminative versus Bayesian methods, first of all, we might look at discriminative methods because they are easy empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Using discriminative methods has been was solved by the perceptron in the 1960s.",
                    "label": 1
                },
                {
                    "sent": "We essentially choose some kind of universal approximator.",
                    "label": 1
                },
                {
                    "sent": "We create an.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parametric and then we minimize the error metric.",
                    "label": 0
                },
                {
                    "sent": "There are at least three or four classes of universal approximators available to us, so universal approximator is essentially a Riemann integrator by the Riemann integral theorem.",
                    "label": 1
                },
                {
                    "sent": "Any function over any finite domain can be approximated by an infinite number of tall thin boxes.",
                    "label": 0
                },
                {
                    "sent": "The sigmoidal neural network actually doesn't model those.",
                    "label": 1
                },
                {
                    "sent": "That is a series of tall thin boxes.",
                    "label": 0
                },
                {
                    "sent": "It models, it is a series of tall of tall, thin step functions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the mixture Gaussian models.",
                    "label": 1
                },
                {
                    "sent": "It is a series of smooth, tall thin boxes and piecewise constants or piecewise linear systems like classification trees and tenuous neighbors do actually model the function as a series of tall thin boxes.",
                    "label": 1
                },
                {
                    "sent": "Alright, I'm going to be talking primary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About sigmoidal networks and mixture Gaussians.",
                    "label": 0
                },
                {
                    "sent": "As we go the other piece that you need for discriminative training is some kind of differentiable error metric and most of the algorithms that will talk about can be can be trained using either a Minkowski norm or using some kind of likelihood function which is in effect a log arhythmic Minkowski norm.",
                    "label": 0
                },
                {
                    "sent": "So when kowski norm, so the best norm, usually the one that we really want is the zero norm.",
                    "label": 1
                },
                {
                    "sent": "We really want to find we really want to say that the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Each sub Theta is correct if it's exactly equal to the label that we're trying to get.",
                    "label": 1
                },
                {
                    "sent": "The problem is that that's not a very trainable error metric.",
                    "label": 0
                },
                {
                    "sent": "If in fact the hypothesis is not exactly equal to the target label, what do we do next?",
                    "label": 1
                },
                {
                    "sent": "And so, and so we have a series of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trainable error metrics.",
                    "label": 0
                },
                {
                    "sent": "For example, the Manhattan distance in the Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "Where essentially, if we train a neural network to minimize the to minimize the Manhattan distance, we wind up with a neural network that computes the computes the op posteriori median of the of the label distribution.",
                    "label": 0
                },
                {
                    "sent": "If we minimize the Euclidean distance, we create a neural network that computes the posteriori mean of the label distribution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally we apply the chain rule in order to minimize.",
                    "label": 1
                },
                {
                    "sent": "That is, for each of the training tokens until the error stops stop changing, we take our parameter set and we adjusted in the negative direction of the gradient.",
                    "label": 0
                },
                {
                    "sent": "And we adjust Ada as necessary in order to minimize the minimize the error along the direction of the line that we've chosen, where the direction of the line is can be broken down because of the structure of the universal approximator.",
                    "label": 0
                },
                {
                    "sent": "So here's where Riemann actually comes in to help us.",
                    "label": 0
                },
                {
                    "sent": "The Universal Approximator says that the functional approximation that we're creating is the sum of a very very large number of tall thin boxes, and each of those tall thin boxes is adjusted only in order to pull in order to approximate the training samples that fall close to that tall thin box.",
                    "label": 0
                },
                {
                    "sent": "So we compute the derivative of the error with respect to the hypothesis at that particular at that particular training sample, and then find the gradient of the of that particular hypothesis with respect to the parameter set.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, the applications of this in audio processing or so ubiquitous that I haven't actually given any here.",
                    "label": 0
                },
                {
                    "sent": "But, for example, this is.",
                    "label": 0
                },
                {
                    "sent": "We use this as the input to to automatic speech recognition into acoustic event detection, and I'll describe one of those systems later when I can add a little bit more color to it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the meantime, let me talk about some of the ways in which discriminative training can be extended to more interesting problems.",
                    "label": 0
                },
                {
                    "sent": "For example, I haven't described anything having to do with real valued outputs or with systems that depend on time.",
                    "label": 0
                },
                {
                    "sent": "Those can be easily, easily put into a discrete, purely discriminative framework by just by just plugging and plugging in the previous time steps hypothesis.",
                    "label": 0
                },
                {
                    "sent": "As one of the observations at the current time step, and then we get and then we get a recursive neural network in which the.",
                    "label": 1
                },
                {
                    "sent": "The hypothesis is now not is now still the sum of a large number of step functions, but each of.",
                    "label": 0
                },
                {
                    "sent": "But each of those step functions is a function, not just to the input variables, but also of the state variable and the state variable is fed back from the from the output of the neural network to the inputs we train over all of the training tokens, but now each of the hypothesis at any given training token at the training token exoti depends on the value of the hypothesis at X sub T + 1, the gradient, the.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis anexity depends on the value of the output index of T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Therefore, to minimize the error at X of T, we need to we need to pay attention to the error at X sub T + 1 and so on all the way to the end of time.",
                    "label": 0
                },
                {
                    "sent": "So we get this algorithm called backpropagation through time.",
                    "label": 1
                },
                {
                    "sent": "If we simply differentiate the error with respect to the with respect to the hypothesis, then we get an effect that depends on all of the future times as well.",
                    "label": 0
                },
                {
                    "sent": "That's still just an order, an operation for each iteration of the gradient descent it's order.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we can compute, for example over this is this is an RN that was trained over a 7 hour training database.",
                    "label": 0
                },
                {
                    "sent": "This is the Boston University radio news corpus which is a corpus of speech where the locations of Pichak since that is a promise of prosodically.",
                    "label": 0
                },
                {
                    "sent": "Prominent syllables have been marked, and there those targets are shown here in yellow.",
                    "label": 0
                },
                {
                    "sent": "The Gibbs ringing at the edge of each target is is not part of the target.",
                    "label": 0
                },
                {
                    "sent": "The target is basically zero when there's no pitch accent, and then in every syllable where there's a pitch accent, the target goes to one, and then it goes back to zero and the blue line, which is almost visible.",
                    "label": 0
                },
                {
                    "sent": "The blue line is F0 is the pitch of the talker during each of these syllables, and you can see that there's fairly consistent pitch movements all the way through the utterance.",
                    "label": 0
                },
                {
                    "sent": "Some of this is noise, but all of the fairly smooth sections are the pitch of the talker, as the talker produces a.",
                    "label": 1
                },
                {
                    "sent": "A sentence.",
                    "label": 0
                },
                {
                    "sent": "In this case, I think the sentence is wanted.",
                    "label": 0
                },
                {
                    "sent": "Chief Justice of the Massachusetts Supreme Court and you can see the pitch accent on the 1st syllable of the word wanted and the pitch accent on the 1st syllable.",
                    "label": 1
                },
                {
                    "sent": "The word Massachusetts and no, sorry this would be Chief Justice of the Massachusetts Supreme Court.",
                    "label": 1
                },
                {
                    "sent": "And the F0 tends to track the locations of the pitch accents.",
                    "label": 0
                },
                {
                    "sent": "The blue tends to be higher during these yellow, but not consistently, and in fact it's really hard to pick out a pattern in the blue line that tells you exactly what pitch accent is, but if you know something about the state of the system, if you've fed the state back into a recursive neural network and then trained it in order to minimize the error over over the training half of the same database, then you can get a pretty good estimate of the locations of the pitch accents, and that output is shown with the.",
                    "label": 0
                },
                {
                    "sent": "With the pink line here.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another way in which discriminative algorithms can be pushed a little bit is by reducing the size of the training corpus, so minimizing the error on the training corpus works really well if you have at least 10,000 tokens, but if you have fewer than 1000 tokens, you need to regularize the error term using some kind of generalization term.",
                    "label": 0
                },
                {
                    "sent": "Vapnik intervening cause showed that, among others, showed that the probability of the error on on a novel corpus drawn from the same probability density function can be bounded by actually a number of different upper bounds.",
                    "label": 0
                },
                {
                    "sent": "And there are quite a number of these available now.",
                    "label": 0
                },
                {
                    "sent": "For example, the support vector machine proposed by Vapnik said that let's recast the classifier, recast H sub Theta of X as a dot product in some higher dimensional space and then the the error bound the upper bound on the difference between the training and the test corpus.",
                    "label": 1
                },
                {
                    "sent": "Is proportional to the the Euclidean norm of the of the projection of the classifier into that higher space.",
                    "label": 0
                },
                {
                    "sent": "Minimum description length turns out to be a similar kind of Minkowski norm of the classifier itself.",
                    "label": 1
                },
                {
                    "sent": "So if we can, if we can convert the classifier whatever the parameter set of the classifier is, convert that into a binary program and compute the Kolmogorov description length of that program, then the Minkowski the sorry, the minimum description length, says that the difference between the training and the test corpus.",
                    "label": 0
                },
                {
                    "sent": "The expected difference between the training and the test corpus error.",
                    "label": 1
                },
                {
                    "sent": "Is bounded by the Zero Norm, the number of bits essentially in the in the binary program describing the classifier.",
                    "label": 0
                },
                {
                    "sent": "So we get this upper bound on the test corpus error that matters a lot when when the training corpus error is is under estimated.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Too little data as an example of a way in which we've used this, there's a lot of good evidence to suggest.",
                    "label": 0
                },
                {
                    "sent": "For example, for each food we showed in 1973 that that most of the information about the place of articulation of the constant at the beginning of a syllable is encoded in the syllable within about the 1st, 20 to 30 milliseconds after the release of the consonant.",
                    "label": 0
                },
                {
                    "sent": "That is to say, if you cut out all of them, so some of the things that Dylan was showing you earlier, if you cut out all of the constant region.",
                    "label": 0
                },
                {
                    "sent": "And only play the vowel region, and then if you were to also cut out most of the vowel region and only play back the first, the first 70 milliseconds after the constant release or the first 50 milliseconds after the constant release.",
                    "label": 0
                },
                {
                    "sent": "If you play tokens like this to listeners in quiet, they can usually get the place of articulation of the consonant.",
                    "label": 0
                },
                {
                    "sent": "If, on the other hand, you cut out all of the voice part and only play the constant part for most consonants, that's a very hard.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to determine what the place of articulation is obviously for stop consonants.",
                    "label": 0
                },
                {
                    "sent": "All that would be playing back as silence for a nasal consonant.",
                    "label": 0
                },
                {
                    "sent": "You'll be playing back a murmur and more, and which if you couldn't see my lips, would be very hard to distinguish.",
                    "label": 0
                },
                {
                    "sent": "For Affricative, you can do a pretty good job, because the frication spectrum is usually distinctive, but not necessarily for things like Theta an F. So all of the information is really encoded in that first 50 milliseconds or so, immediately before through about 40 to 50 milliseconds after the release.",
                    "label": 0
                },
                {
                    "sent": "The problem is that those acoustic landmarks are relatively few and far between in a 14 hour database like the timid or like the TIMIT training database, there are typically typically 200 to 2000 examples of each of those of each of those landmarks, so we don't have very many of them to go by, so we can't just try to train a classifier that minimizes the error on the training database.",
                    "label": 0
                },
                {
                    "sent": "We have to pay some attention.",
                    "label": 0
                },
                {
                    "sent": "To the possible differences between the training and the test database, and Partha was the first one to propose that support vector machines are tailor made to do this task in order to generalize from the training to the test database, here are some of our results if you train.",
                    "label": 0
                },
                {
                    "sent": "If you train a support vector machine to detect, for example, onset of sonar and see it can does it does the job with 86% accuracy on a task where chance is 50% to detect the onset of a constant.",
                    "label": 0
                },
                {
                    "sent": "It does that with 78% accuracy, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, 1/3 way in which discriminative classifiers could be pushed a little bit is to is to combine dynamics and sparse data.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the landmark detectors have to be trained using a small.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All data set, but we need.",
                    "label": 0
                },
                {
                    "sent": "But we need some kind of information about the dynamics overtime in order to do correct classification.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it's hard to do that using purely discriminative methods and.",
                    "label": 1
                },
                {
                    "sent": "In fact, the only way that I've there are there have been a few methods developed recently to do this discriminatively, but it's extremely computationally intensive.",
                    "label": 1
                },
                {
                    "sent": "We get much more reasonable methods using using Bayesian inference.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so let me introduce Bayesian inference.",
                    "label": 1
                },
                {
                    "sent": "The other sort of superclass of machine learning methods are the set of Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "That is to say, methods in which methods in which the function that's being estimated by the universal approximator must be a correctly normalized probability density.",
                    "label": 0
                },
                {
                    "sent": "It can't be simply an arbitrary function, it must estimate the likelihood of the label variable Y given the observation X, and that that.",
                    "label": 0
                },
                {
                    "sent": "That function estimation problem then naturally gives you any other function estimation problem.",
                    "label": 0
                },
                {
                    "sent": "If you can correctly estimate the probability density of Y given X, then you can minimize the probability of error by just choosing the maximum of posteriori probability value of Y.",
                    "label": 0
                },
                {
                    "sent": "If you can correctly estimate the probability density of Y given X, then you can choose the minimum mean squared error value of Y is your output in a real valued tracking or regression problem?",
                    "label": 0
                },
                {
                    "sent": "So if you can do good function estimation for the probability density itself, then you've solved every other problem.",
                    "label": 0
                },
                {
                    "sent": "The disadvantage, of course, is that learning the probability density is usually harder than learning the function that you wanted in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "Usually you require more data and usually with the same size database you're subject to a larger amount of error in estimating the probability density.",
                    "label": 1
                },
                {
                    "sent": "Then you would be in simply estimating Y is equal to H of X directly.",
                    "label": 0
                },
                {
                    "sent": "The big advantage is that with correct normalization you can string together a whole sequence of these latent variables in order to in order to build.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actively complex models, so here's perhaps the most well, at least in speech.",
                    "label": 0
                },
                {
                    "sent": "The most famous example of a Bayesian system hidden Markov model is a system in which the label at each time depends on the labels at all of the previous times, as well as depending on the acoustic observations.",
                    "label": 1
                },
                {
                    "sent": "That is to say, we suppose that that your speech articulation system is some set of moving tongue and lips, and so on, and some set of intentions and a vocal fold that where the vocal folds approximate and then the vocal folds are separate and that in effect there's only a discrete finite number of settings of all of that apparatus that matter.",
                    "label": 0
                },
                {
                    "sent": "So we move through this space of finite states.",
                    "label": 0
                },
                {
                    "sent": "This discrete space of state variables, and in a particular state.",
                    "label": 0
                },
                {
                    "sent": "The speaker produces a particular set of acoustic Spectra.",
                    "label": 0
                },
                {
                    "sent": "So that in effect, the probability of a particular set of acoustic Spectra given a particular set of uttered words.",
                    "label": 0
                },
                {
                    "sent": "Is the is the probability of the current word given the previous word times the probability that the speaker's lips and tongue and Gladys and so on transition from the state that they were in at time T -- 1 to the state there in time time T given the word that you're in times the probability of getting in a particular acoustic spectrum given that the lips and tongue and Gladys and soft palate and so on are in a particular state.",
                    "label": 0
                },
                {
                    "sent": "These things are this is this is the usual decomposition that's used in speech recognition and hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "I think you've seen in Karen's talk on Tuesday that we can decompose it further in order to bring more information to bear.",
                    "label": 1
                },
                {
                    "sent": "But the usual decomposition is to talk about the language model, which is a look up table.",
                    "label": 1
                },
                {
                    "sent": "The pronunciation model, which is a look up table in the acoustic model, which is itself some kind of universal approximator, like like a mixture of Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning in a Bayesian system is usually some variation of maximum likelihood learning, and as in a discriminative system, if you have fewer than 1000 tokens, you need to do some kind of regularization.",
                    "label": 0
                },
                {
                    "sent": "The usual tip so two of the common kinds of regularization are maximum of posteriori probability estimation, where instead of maximizing the probability of the data set, we maximize the probability of the parameters.",
                    "label": 0
                },
                {
                    "sent": "That is, we maximize the probability of the log prior of the log probability of Theta with no information, and then the log probability of the data set given the parameters.",
                    "label": 0
                },
                {
                    "sent": "In effect, this log P of Theta serves as a regularization term, just as the generalization error was a regularization term in discriminative methods.",
                    "label": 0
                },
                {
                    "sent": "So it serves in effect as an upper bound on the on the difference that we believe to be possible between in the training and the test database.",
                    "label": 0
                },
                {
                    "sent": "Maximum entropy winds up having the same form or can be rewritten in the same form, except that we assume that we don't know anything at all about P of Theta.",
                    "label": 0
                },
                {
                    "sent": "We don't know anything at all about the prior except that it's a probability density.",
                    "label": 0
                },
                {
                    "sent": "And since in the space of probability densities those with large entropy are are much more numerous than those with small entropy, we estimate the log prior of Theta to be by the by the entropy of the probability density implied by Theta.",
                    "label": 0
                },
                {
                    "sent": "That is the measure of the probability densities with with high entropy is larger than the measure of those with low entropy.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And then in testing, once we've learned these parameters, we can compute the.",
                    "label": 0
                },
                {
                    "sent": "I said that given the given the likelihood function, you know everything that there is to know you know how to compute the minimum probability of error classifier.",
                    "label": 1
                },
                {
                    "sent": "And you also know how to compute the minimum mean squared error estimate of Y.",
                    "label": 0
                },
                {
                    "sent": "That can be that can be computed from all of these intrinsic variables by just marginalizing by just summing over all of the possible values of the intrinsic variables.",
                    "label": 1
                },
                {
                    "sent": "So in training we use maximum likelihood and testing.",
                    "label": 0
                },
                {
                    "sent": "We use a minimum probability.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our classifier.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Rather than give you speech recognition examples of a hidden Markov model, I thought I would show you something that you may not have seen before.",
                    "label": 0
                },
                {
                    "sent": "Hidden Markov models can be used for regression just as much as they can be used for recognition.",
                    "label": 0
                },
                {
                    "sent": "Here's how if we if we have Gaussian states, if each state has a.",
                    "label": 0
                },
                {
                    "sent": "If each state says that the acoustic spectrum is Gaussian distributed, then that is as shown here.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of X given the state variable.",
                    "label": 0
                },
                {
                    "sent": "Is the is E to the minus one half X -- X bar and y -- y barware?",
                    "label": 0
                },
                {
                    "sent": "Now why is a real valued vector that we're trying to estimate?",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "We can compute the minimum mean squared error estimate of this real valued vector by just summing over the posterior probability of the states.",
                    "label": 0
                },
                {
                    "sent": "The linear regression formula.",
                    "label": 0
                },
                {
                    "sent": "This is the linear regression formula that one gets from jointly Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "You take the mean value the op priore mean of Y of the Y vector plus X minus it's a priority mean multiplied by the correlation between between X&Y.",
                    "label": 0
                },
                {
                    "sent": "So here's the here's the linear regression formula for jointly Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "And we sum that over all of the different possible Gaussians that might apply to the data weighted by the posterior probability that that's the correct Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This gives us a non linear regression for Y given X.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That can be extended to a hidden Markov model by by including state.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can assume that that X&Y are not just jointly Gaussian with each other, but they're also jointly Gaussian with the hidden state variable Y at the previous time, and the result is exactly like the regression that I showed in the previous formula, except that now the correlation between the correlation between between X&Y depends on varies as a function of time depends on what our current estimate of the of the Y variable might be, an essentially rather than computing.",
                    "label": 0
                },
                {
                    "sent": "Rather than computing awaited sum of linear regression formulas, now I'm computing a weighted sum of Kalman filters.",
                    "label": 0
                },
                {
                    "sent": "Where the Kalman filter is linear regression with.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameters updated overtime.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of an experiment that we did using comparing HMM regression and and switching Kalman smoother.",
                    "label": 1
                },
                {
                    "sent": "The results were unfortunately not too compelling, so the the task was to try to estimate the position of the tongue given the acoustic spectrum.",
                    "label": 0
                },
                {
                    "sent": "So we have a database of.",
                    "label": 0
                },
                {
                    "sent": "We have a database of of tracked pellets on the surface of the tongue and a set of a set of matched acoustic Spectra and we use we use HMM regression.",
                    "label": 0
                },
                {
                    "sent": "That is, we assume that there's no dependence of the current position of the articulators on their previous position except as specified by the state of the hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "And we also use a switching Kalman smoother.",
                    "label": 0
                },
                {
                    "sent": "That is, we use the Bayesian network shown.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous page where the articulators, which are the wise depends not only on the current discrete state which is the S, but they also depend on the previous positions of the articulators which the previous previous Y vector.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we find a consistent but extremely small under .1 millimeter reduction of the tracking error using the switching Kalman smoother as opposed to using the hidden Markov model regression.",
                    "label": 0
                },
                {
                    "sent": "And we haven't been able to prove statistical significance because there are no database is large enough to prove statistical significance within OH point 1 millimeter difference.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, finally I'd like to talk about the methods that I've actually found most useful in my own research.",
                    "label": 0
                },
                {
                    "sent": "Those are methods that link together discriminative techniques and Bayesian techniques, using each for the tasks for which each one is most.",
                    "label": 0
                },
                {
                    "sent": "Each one is best suited.",
                    "label": 0
                },
                {
                    "sent": "That is to say, we assume that that that time matters.",
                    "label": 0
                },
                {
                    "sent": "We assume that latent variables matter and that therefore some kind of Bayesian technique is important.",
                    "label": 0
                },
                {
                    "sent": "You need to estimate the probability of the labels given the observations with reference to some kind of some kinds of hidden variables.",
                    "label": 0
                },
                {
                    "sent": "But we assume also that the observations by themselves are not really enough to give you very good information about what those hidden variables might be that you can get a much better estimate of those hidden variables by computing some kind of transformation of the observations.",
                    "label": 0
                },
                {
                    "sent": "These transformations we trained then using using a small amount of labeled data using a small amount of labeled data, we can train a neural network or a support vector machine in order to estimate, say, the phone.",
                    "label": 0
                },
                {
                    "sent": "A logical distinctive features, say, estimate those landmark positions that I showed you earlier.",
                    "label": 1
                },
                {
                    "sent": "Given the acoustic spectrum, and then using those SVM trained on relatively small amount of data.",
                    "label": 0
                },
                {
                    "sent": "We then plug those in as the front end.",
                    "label": 0
                },
                {
                    "sent": "For a Bayesian system like a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "So the training database includes a relatively small amount of data in which the landmarks.",
                    "label": 1
                },
                {
                    "sent": "These capital these F vectors are labeled and a relatively large amount of data in which they are not labeled.",
                    "label": 0
                },
                {
                    "sent": "But we can learn we can learn the relationship between XY and F hat in effect using the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using the Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "We train the train.",
                    "label": 0
                },
                {
                    "sent": "We learn the F variables.",
                    "label": 0
                },
                {
                    "sent": "We train a local discriminative classifier in order to estimate these F variables.",
                    "label": 0
                },
                {
                    "sent": "The locations of the landmarks using using some kind of Minkowski norm minimization.",
                    "label": 0
                },
                {
                    "sent": "And then we learn a joint probability density for F&X&Y using Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "So if I put both F&X into this, this is what's called Attenda method, where the hidden Markov model or the dynamic Bayesian network pays attention to both the outputs of the support vector machines and the original acoustic spectrum.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of a system that we've had pretty good luck with.",
                    "label": 0
                },
                {
                    "sent": "We take the the original acoustic distribution of sounds related to different phonemes, and we passed them through a support vector machine in order to compute this.",
                    "label": 0
                },
                {
                    "sent": "Some kind of implicit Hilbert space that is, the support vector machine is a linear classifier in implicit Hilbert space, and if the SVM is trained in order to minimize the in order to minimize the structural risk over some usually relatively small training database.",
                    "label": 0
                },
                {
                    "sent": "Then it's in effect, computing a projection into into a Hilbert space where the clouds are separated as well as they possibly can be.",
                    "label": 0
                },
                {
                    "sent": "But if it's computing a projection into a space where the clouds are separated then, then the HMM has less to learn.",
                    "label": 0
                },
                {
                    "sent": "The hmm really only has to learn only has to learn the likelihood function in this in this better space you have a question.",
                    "label": 0
                },
                {
                    "sent": "The training data for the SVM is is a set of a set of well.",
                    "label": 0
                },
                {
                    "sent": "In this case it's ntim it it's it's, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a relatively small database where the SVM outputs are labeled, so we have we have a large database, the switchboard database for example, where we don't know what the phoneme transcriptions are, and we have a small database like end, timid where we do, and so we train the SVM is using intimite and then and then train the HMM using using a larger chunk of switchboard.",
                    "label": 0
                },
                {
                    "sent": "That makes sense.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and here here error rates on here error rates on telephone speech.",
                    "label": 0
                },
                {
                    "sent": "This is actually on the test corpus of intimite and although sorry these are accuracies on intimite, the baseline system, just an FCC hidden Markov model doesn't get a very good accuracy.",
                    "label": 0
                },
                {
                    "sent": "It gets almost 40% accuracy with about with about 35 Gaussians per mixture Gaussian the the tandem systems using support vector using the outputs of the phmsa's features.",
                    "label": 0
                },
                {
                    "sent": "Gets a better accuracy.",
                    "label": 0
                },
                {
                    "sent": "Still, we're not.",
                    "label": 0
                },
                {
                    "sent": "You know, these are relatively low accuracies compared to what you can do with 16 kilohertz sample data, but these are pretty close to the state of the art for for telephone band speech, so phoneme recognition accuracy on telephone speech runs around 40% and we can get it as high as 44% using a hybrid system with.",
                    "label": 0
                },
                {
                    "sent": "With SPMS computing the distinctive features at each time.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example of a tandem system.",
                    "label": 0
                },
                {
                    "sent": "This one was done by Vikram Mitra at at the University of Maryland, where we've actually compared the switching Kalman filter system that I showed you before to a system where recursive neural network.",
                    "label": 0
                },
                {
                    "sent": "So a system that already has a discriminative system that already has some state in it tries to estimate the tries to estimate the articulator positions.",
                    "label": 0
                },
                {
                    "sent": "The opening of the velum, the local, the construction location of the tongue body, that construction.",
                    "label": 0
                },
                {
                    "sent": "Degree of the tongue body.",
                    "label": 0
                },
                {
                    "sent": "The construction location of the tongue tip and the construction degree of the tongue.",
                    "label": 0
                },
                {
                    "sent": "Tongue tip and I tried to get him to send me a copy that had the raw and outputs without common filtering, smoothing and he refused because it's too noisy.",
                    "label": 0
                },
                {
                    "sent": "He was willing to show me on his own computer, but basically without common smoothing this is.",
                    "label": 0
                },
                {
                    "sent": "It tracks pretty well, at least according to the training it should track with a lower mean squared error, but it has all of these little high frequency mistakes that one would never actually that a human would never make in trying to.",
                    "label": 0
                },
                {
                    "sent": "Go from this acoustic spectrum to the true tongue body construction location.",
                    "label": 0
                },
                {
                    "sent": "Those high frequency mistakes can be smoothed out by assuming that the articulators have some dynamic state that the articulator position doesn't change too rapidly as a function of time that can be done using either a simple lowpass filter or using a Kalman smoother.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, we're using the recursive neural net to compute these locations and then smoothing them by feeding the output of the neural network into a common into a common smoother.",
                    "label": 0
                },
                {
                    "sent": "And that gives us the best performance that we.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Had so far.",
                    "label": 0
                },
                {
                    "sent": "Two more examples of tandem systems, uh, another one in which we've.",
                    "label": 0
                },
                {
                    "sent": "In which we've had reasonable success is the detection of nonspeech acoustic events.",
                    "label": 1
                },
                {
                    "sent": "Here's the description of this task from the clear acoustic event detection detection task description.",
                    "label": 0
                },
                {
                    "sent": "Essentially, this is related to the task that I showed you at the beginning of the talk, where we have we have 15 people sitting around a table in a conference room, and they occasionally drop binders on the table, and we want to detect the binder drops, and we want to detect the chair squeaks, and we want to detect the spoons dropped in coffee cups, and we want to detect footsteps.",
                    "label": 0
                },
                {
                    "sent": "And we want to detect keyboard clicks.",
                    "label": 1
                },
                {
                    "sent": "This is a really difficult task, especially when you start talking about keyboard clicks, because if speech is the noise and these are these are these are speech, engine, speech and language engineers, so they're talking all the time, so there's no.",
                    "label": 0
                },
                {
                    "sent": "There's no break in the background noise if the speech is the background noise, the keyboard click is occurring at a signal to noise ratio of around around minus 10 -- 15 DB.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to detect anything at that kind of negative SNR.",
                    "label": 1
                },
                {
                    "sent": "The spectral structure of a keyboard click is quite different from the spectral structure of speech.",
                    "label": 0
                },
                {
                    "sent": "It's not even clear that the band between 500 and 3000 Hertz is the is the correct band in which to be looking for information about a keyboard click.",
                    "label": 1
                },
                {
                    "sent": "And finally, each of these events has a very different spectral structure.",
                    "label": 0
                },
                {
                    "sent": "A door slam doesn't look very much like like a.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clicking keyboards, so here a couple of slides to demonstrate those problems.",
                    "label": 0
                },
                {
                    "sent": "Here's here's an example where the signal to noise ratio is not actually negative.",
                    "label": 0
                },
                {
                    "sent": "We have a binder dropped on the table here at here at the center time in the middle of the utterance.",
                    "label": 0
                },
                {
                    "sent": "Good morning, everybody, I'm glad you could all come.",
                    "label": 0
                },
                {
                    "sent": "The speech is the SNR in this case is if you consider the binder to be the signal and everything else to be the speech the SNR is on the order of five DB.",
                    "label": 0
                },
                {
                    "sent": "And that's relatively good, and that's much better than you would get with footsteps.",
                    "label": 0
                },
                {
                    "sent": "Well, here's an example.",
                    "label": 0
                },
                {
                    "sent": "Here's paper rustling at the beginning, and if you compute the overall signal level of the paper rustling compared to the overall signal level of the speech over the entire utterance, the SNR would be negative.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The spectral structures are quite different.",
                    "label": 0
                },
                {
                    "sent": "Here's the spectrogram of a set of footsteps.",
                    "label": 0
                },
                {
                    "sent": "You can see them here and here, here, here, here, here.",
                    "label": 0
                },
                {
                    "sent": "Here's the spectral structure of somebody jingling.",
                    "label": 1
                },
                {
                    "sent": "A set of keys as they're trying to take their car keys out or their office keys out.",
                    "label": 0
                },
                {
                    "sent": "And then here's the spectrogram of speech, and you can see that acoustic features that have been developed for speech or not going to be terribly useful for detecting footsteps, and likewise the ones developed for footsteps, may not be useful for detecting.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jangling keys so the first problem we have to solve is trying to figure out what what kind of summary of the spectrogram that we want to use for this recognition task.",
                    "label": 0
                },
                {
                    "sent": "We've taken the the most successful thing that we've actually implemented and tested so far, basically over generates every possible acoustic feature that you might possibly imagine would be useful for this task.",
                    "label": 0
                },
                {
                    "sent": "So MFC Mel frequency capsular coefficients, perceptual LPC, computed with a few different frame sizes, energy zero crossing rate, and then we, and then we use a machine learning algorithm to choose the best features we want to choose the we want to compute a matrix W where each row of.",
                    "label": 0
                },
                {
                    "sent": "Where each row of W is an indicator vector, each row of W just picks out one of the features from X.",
                    "label": 1
                },
                {
                    "sent": "That's going to be that's going to be most useful for the task of detecting at least one of those nonspeech audio events.",
                    "label": 1
                },
                {
                    "sent": "And then given this vector of features F that are selected by the W matrix, we then want to compute a hidden Markov model of the underlying events and.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do event classification in silence, if you have a waveform which is just a chair, squeak and another, which is just a key Jingle.",
                    "label": 0
                },
                {
                    "sent": "In fact, you don't need this hidden Markov modeling, but if you if you're trying to detect them in a continuous full hour recording of a meeting room, this becomes extremely important.",
                    "label": 0
                },
                {
                    "sent": "This was in fact the the analysis that the clear team did suggested that this was the reason that our system won the competition, because we can, we can actually track footsteps better.",
                    "label": 0
                },
                {
                    "sent": "For example better than anybody else, because we know that.",
                    "label": 0
                },
                {
                    "sent": "If there are footsteps at time T, there usually are also footsteps at time T + 1, and the same is not true of door slams.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That feature selection process that I described a moment ago.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be pretty similar to Adaboost, actually, but it can be.",
                    "label": 0
                },
                {
                    "sent": "It can be formulated also.",
                    "label": 0
                },
                {
                    "sent": "It can also be Bayesian formulated.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting duality between the discriminative feature selection and the Bayesian formulation.",
                    "label": 1
                },
                {
                    "sent": "That is to say, if the only thing we were able to observe was this was this one feature WK transpose X where WK is an indicator vector and X is the original spectrum.",
                    "label": 1
                },
                {
                    "sent": "Then the then it's not too hard actually to construct a kernel estimate of the probability density of the joint probability density between this one feature and the output label that we're looking for.",
                    "label": 0
                },
                {
                    "sent": "A1 dimensional kernel density estimator doesn't require that much training data data, but if we can construct a pretty good estimate of the probability density of that one feature correlated with the existence versus non existence of a particular kind of acoustic event, then we can also construct a pretty good estimate of the of the Bayes error of the minimum possible error that we can achieve using that feature.",
                    "label": 0
                },
                {
                    "sent": "That minimum possible error is just the just the integral overall over over Y and over X.",
                    "label": 0
                },
                {
                    "sent": "Of the probability that the that the Bayes optimal label is not equal to the true label.",
                    "label": 1
                },
                {
                    "sent": "That is, we take over all of the X&Y space we integrate over all of the regions where where the label is not equal to the estimated label.",
                    "label": 0
                },
                {
                    "sent": "We can estimate that on a database, of course, by just summing the integrand over all the training tokens.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's more, though we can, we can estimate by summing over the training database, all of the training tokens for which the Bayes optimal estimator given that feature is not equal to the true label given that feature.",
                    "label": 0
                },
                {
                    "sent": "But that makes the assumption that the classifier only gets one choice that we don't pay attention to.",
                    "label": 0
                },
                {
                    "sent": "The second and third and 4th choices of the classifier if we allow the 2nd and 3rd and 4th choices, the classifier to be considered, then we could compute this sort of softened version of the.",
                    "label": 0
                },
                {
                    "sent": "Bayes error estimate.",
                    "label": 0
                },
                {
                    "sent": "That is to say, we we sum over all of the database.",
                    "label": 0
                },
                {
                    "sent": "The rank of the correct label given the feature.",
                    "label": 0
                },
                {
                    "sent": "So we take this take this joint probability density of the the label and the feature, and we look at that for all of the different labels and we find is the correct label.",
                    "label": 0
                },
                {
                    "sent": "The first choice or the second choice, or the third choice of the fourth choice.",
                    "label": 0
                },
                {
                    "sent": "That gives us a metric that we want to minimize over the entire training database.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's often does turn out to be useful.",
                    "label": 0
                },
                {
                    "sent": "The accuracies are the accuracies are about 24% with a simple MFC C Mel frequency.",
                    "label": 0
                },
                {
                    "sent": "Kestrel coefficient recognizer with the Hard Bayes estimate it jumps up to 30% accuracy and with the Soft Bayes estimate it jumps up to 3232% accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is still a hard task with classification.",
                    "label": 0
                },
                {
                    "sent": "If you know that a particular waveform is one of these non speech audio events, you can classify it with with 9598% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Using just using support vector machine.",
                    "label": 0
                },
                {
                    "sent": "If you don't know where they occur in a meeting and if you have these negative SNR situations, then the accuracies are still well, 32 percent is the best that's been reported yet.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then one more application of these hybrid methods that I want to talk about is the problem of is back to the basic problem of automatic speech recognition using a neural network to compute features that are then observed by a hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "This actually is a lot like the problem that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "We want to compute.",
                    "label": 0
                },
                {
                    "sent": "We want to compute features that are a vector multiplied by the input.",
                    "label": 0
                },
                {
                    "sent": "We have an observation we want to multiply it by some kind of selection vector, and then that's going to be the feature.",
                    "label": 0
                },
                {
                    "sent": "Except now the selection two things are changed, one, the selection vector is no longer an indicator, it's just come some kind of steerable linear summation over the input feature space and 2nd will allow a second layer in the in the neural network will allow us will allow sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Hidden nodes that are then recombined in order to compute the output features.",
                    "label": 0
                },
                {
                    "sent": "Alright, and then we'll compute a hidden Markov model too.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe it does.",
                    "label": 0
                },
                {
                    "sent": "Alright, the hidden Markov model itself is trained using the Baum Welch algorithm.",
                    "label": 1
                },
                {
                    "sent": "Baum Welch algorithm, the.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the hybrid system is usually trained by by taking a database as I described before, where the phonemes are labeled and by computing by minimizing the difference between this F and the target value of F. But there's another thing that can be.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can take this Baum Welch algorithm which computes not really the log probability of the database, but the expected log probability of the database where the expectation is over the posterior probability density of the hidden variables that that Bayesian training.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Material can actually be fed back into the neural network.",
                    "label": 0
                },
                {
                    "sent": "Once you sit down and try to write the derivative of the of the expected log probability with respect to each of the parameters in the neural network, it's actually not that hard to write it down.",
                    "label": 1
                },
                {
                    "sent": "You wind up with something that looks like the sum over all of the training tokens of the sum over all possible hidden states of the Baum Welch probability.",
                    "label": 0
                },
                {
                    "sent": "This is the posterior probability of the state variable at time T given all of the observations available to you multiplied by then the usual neural network backpropagation.",
                    "label": 0
                },
                {
                    "sent": "Terms.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a problem with that though, and the problem with that is the problem of spurious Maxima, and in order to talk about the problem of serious Maxima, let me back up to a standard sort of standard problem that I teach in my in my machine learning class.",
                    "label": 0
                },
                {
                    "sent": "If you have a mixture Gaussian and you're trying to train the mixture Gaussian in order to maximize the likelihood of a training data set, it's always possible to train it such that the likelihood of the training data set is infinite.",
                    "label": 1
                },
                {
                    "sent": "In other words, it's always possible to generate a mixture Gaussian that represents any particular training data set.",
                    "label": 0
                },
                {
                    "sent": "With infinite probability, and it's done as follows, we.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the Gaussians and nonzero variance and at least one of them is zero variance, and then we set the mean of the Gaussian with zero variance equal to any particular training token.",
                    "label": 1
                },
                {
                    "sent": "It doesn't matter which one, just pick one, set the mean of the Gaussian equal to that training token, and give it 0 variance.",
                    "label": 0
                },
                {
                    "sent": "That Gaussian therefore gives the tree.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data set, infinite probability and therefore the the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And therefore when you add together all of those Gaussians, the training data set has infinite probability.",
                    "label": 0
                },
                {
                    "sent": "This is a Canonical case of overtraining.",
                    "label": 0
                },
                {
                    "sent": "This is something we would like to avoid by some kind of regular.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "In the bomber back propagation algorithm, they showed on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "This is this is equivalent to training the neural network so that it outputs zero valued features for a large subset of the database.",
                    "label": 1
                },
                {
                    "sent": "So if the weight vector is on the output layer or on the hidden nodes go to zero, then we get, then we get infinite probability Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So here's here's the general form of the solution.",
                    "label": 1
                },
                {
                    "sent": "The general form of the solution is that we want to constrain the norm of the transform computed by.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A neural network.",
                    "label": 0
                },
                {
                    "sent": "Here are two ways that that can be done.",
                    "label": 0
                },
                {
                    "sent": "One is straightforward constrained optimization using Lagrangian multipliers to constrain the norm of the output layer and the norm of the hidden layer to each be one.",
                    "label": 0
                },
                {
                    "sent": "Here's a somewhat trickier method.",
                    "label": 0
                },
                {
                    "sent": "We can constrain the Jacobian of the transform computed by the entire neural network to be a volume preserving to be a unit Jacobian.",
                    "label": 1
                },
                {
                    "sent": "That's now.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possible in general, but there's a particular class of transforms that's extensively studied in statistical mechanics that does this for us, so here's the analogy.",
                    "label": 0
                },
                {
                    "sent": "The analogy between the speech recognition problem and the statistical mechanics problem take the input vector X and arbitrarily divided.",
                    "label": 0
                },
                {
                    "sent": "It doesn't even matter which features go where, but arbitrarily divided into 2 sub vectors.",
                    "label": 0
                },
                {
                    "sent": "Take the output vector Y and arbitrarily divide it into 2 sub vectors.",
                    "label": 0
                },
                {
                    "sent": "That is the output of the neural network.",
                    "label": 0
                },
                {
                    "sent": "I guess this should be F. I'm sorry for the change in notation.",
                    "label": 0
                },
                {
                    "sent": "Take the output of the neural network and arbitrarily divide it into 2 sub vectors and interpret them as follows.",
                    "label": 1
                },
                {
                    "sent": "In the context of statistical mechanics, called the first sub vector, whatever it happens to be, call it the positions of some mysterious set of objects in our in our imagined mechanical system and call the SEC subvector of the set of velocities in our mysterious system.",
                    "label": 1
                },
                {
                    "sent": "X1 and X2 represent the positions at a particular time.",
                    "label": 0
                },
                {
                    "sent": "Y1 and Y2 represent the positions at some future time, sometime progressed.",
                    "label": 0
                },
                {
                    "sent": "An arbitrary amount of time into the future in this imagined mechanical system.",
                    "label": 0
                },
                {
                    "sent": "In any in any non dissipative system, any system in which energy is conserved.",
                    "label": 0
                },
                {
                    "sent": "The the total energy of the system at the first time.",
                    "label": 0
                },
                {
                    "sent": "The total energy of X is equal to the total energy of Y.",
                    "label": 0
                },
                {
                    "sent": "And therefore the entire system.",
                    "label": 0
                },
                {
                    "sent": "If the if the system is energy conserving, the entire mechanical system has to be has to be a volume preserving transform.",
                    "label": 0
                },
                {
                    "sent": "The transformation from any X to any Y has to be a volume preserving transform.",
                    "label": 0
                },
                {
                    "sent": "That is its Jacobian has unit determinant.",
                    "label": 0
                },
                {
                    "sent": "We can impose that by computing something called the kinetic energy and something called the potential energy and requiring that the sum of those two things is constant overtime.",
                    "label": 1
                },
                {
                    "sent": "And here's one transformation that that maintains the constancy of those two things.",
                    "label": 1
                },
                {
                    "sent": "We could perhaps estimate the gradient of V and the gradient of T directly, except that those things need to be irrotational, and so it's the easiest way to get a neural network to compute.",
                    "label": 0
                },
                {
                    "sent": "An irrotational transform turns out to actually train the network to compute V into compute T directly.",
                    "label": 0
                },
                {
                    "sent": "So we set up a.",
                    "label": 0
                },
                {
                    "sent": "Two neural networks, one to compute the potential energy, one to compute the kinetic energy we train them to do so, but we train beyond the fact that this has to be scalar and this has to be scalar.",
                    "label": 0
                },
                {
                    "sent": "There's no other constraint on the form of this, so we train the form of that function.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to minimize error on the training database and because of because because of the form because that function has those two scalars in it.",
                    "label": 0
                },
                {
                    "sent": "Because it's a simplistic transform, we're guaranteed a volume preserving transformation computed by this neural network and therefore were guaranteed that this problem of spurious Maxima doesn't occur.",
                    "label": 0
                },
                {
                    "sent": "So here's a particular system where this is actually on TIMIT phone recognition accuracies using a baseline system where 74% using a maximum likelihood linear transform, the same architecture that I just showed you, but without the non linearity.",
                    "label": 1
                },
                {
                    "sent": "We get a 75% accuracy and using a nonlinear maximum likelihood transform transform we get a 76% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So these are small but statistically significant improvements by this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Non linearity.",
                    "label": 0
                },
                {
                    "sent": "Alright, so lots and lots of different different experiments I've described.",
                    "label": 0
                },
                {
                    "sent": "Essentially, there are three points that I want to that I want to specify.",
                    "label": 0
                },
                {
                    "sent": "First of all, we can.",
                    "label": 0
                },
                {
                    "sent": "We can use pattern recognition for any for any scientific question.",
                    "label": 1
                },
                {
                    "sent": "For any hypothesis test that you want to compute, pattern recognition is backwards from scientific hypothesis testing in the sense that rather than choosing a hypothesis, we choose a hypothesis space family of universal appeal.",
                    "label": 1
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oximeters.",
                    "label": 0
                },
                {
                    "sent": "2nd for any hypothesis that includes hidden variables, it helps to estimate probability densities rather than to estimate the function directly.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And 3rd and finally we get the benefits of both of those methods by if we have a smaller auxiliary database where we can train these discriminative classifiers and then a larger larger database where we can train the Bayesian system to compute inference and that's.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have questions.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We haven't gone back to look at what the classifier found for us.",
                    "label": 0
                },
                {
                    "sent": "We fed in events with a whole bunch of different, so we used.",
                    "label": 0
                },
                {
                    "sent": "First of all, we used a frame rate of.",
                    "label": 0
                },
                {
                    "sent": "Actually, I can't remember whether the basic frame rate was five or 10 milliseconds, but we used a fixed frame rate.",
                    "label": 0
                },
                {
                    "sent": "But then we computed features over a variety of timescales centered at that at that frame spacing, so we computed features with with 10 millisecond windows and with 25 millisecond windows, and with and with 50 millisecond windows and and allowed the algorithm to choose for us which features were relevant.",
                    "label": 0
                },
                {
                    "sent": "Feature went into the pool, if it was it reduced the reduced the data rank over the entire training data set.",
                    "label": 0
                },
                {
                    "sent": "So essentially if it.",
                    "label": 0
                },
                {
                    "sent": "What what wound up happening is that one feature would be chosen because it was useful for one class of acoustic events.",
                    "label": 0
                },
                {
                    "sent": "A different feature might be chosen because it was useful for a different class of acoustic events and.",
                    "label": 0
                },
                {
                    "sent": "And I'm afraid the answer to your question is I don't know because we haven't gone back to look and see what time scales were useful for which events.",
                    "label": 0
                },
                {
                    "sent": "Thanks any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}