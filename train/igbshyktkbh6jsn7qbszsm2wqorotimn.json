{
    "id": "igbshyktkbh6jsn7qbszsm2wqorotimn",
    "title": "Achieving both High Precision and High Recall in Near-Duplicate Detection",
    "info": {
        "published": "Nov. 19, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Business->Management->Knowledge Management"
        ]
    },
    "url": "http://videolectures.net/cikm08_wang_abhpahr/",
    "segmentation": [
        [
            "OK money, ladies and gentlemen, I am late.",
            "I came here with Professor Lee Charming.",
            "Our work is about near duplicates detection.",
            "We face such a problem.",
            "Given a set of 400 million web pages and 10 computers, how to partition it into into sets with similar pages with high precision and high recall in less than one week?"
        ],
        [
            "OK.",
            "Person.",
            "Wait a minute, please.",
            "Resolution.",
            "OK, thank you.",
            "Yeah.",
            "Knock, knock.",
            "Messenger.",
            "Screen.",
            "And before very sorry for this mistake.",
            "OK. Now I'll turn into a way to describe this work is LCS based algorithm for partitioning large scale websites into subsets of similar pages.",
            "First let me explain some of the keywords.",
            "Error CS The longest common subsequence.",
            "Here's an example, the LCS 8 NB is C ABA.",
            "Best means we use Celsius as a similarity measurement pair.",
            "Rabs is the resemblance of NB.",
            "Web pages we focus on topic type, article type pages such as news or blog articles.",
            "But not navigational pages.",
            "Large set we have more than 400 million pages to process, so absolutely is very important algorithm.",
            "We aimed both at precision and recall.",
            "As described in this way, this work is not about given to the pages.",
            "How to tell if they are similar accurately and effectively?",
            "This is a kind of micro problem, but is about.",
            "Given a large set of web pages, how to partition it into subsets of similar web pages, effectively with high quality and recently?",
            "This is a macro problem, so it will use some of some solutions of the micro problem.",
            "But because our objective is at a higher level, so we have a larger solution space to work with.",
            "In this sense, what are people is about is really a framework of algorithms.",
            "And for better precision and recall, we decided to use LCS for the measurement."
        ],
        [
            "Having choosing the environment, the call challenge, our work is the efficiency of the whole process.",
            "This involves two aspect.",
            "First given 2 pages A&B.",
            "The brute force algorithms to compute the LCS onb is time consuming.",
            "Here is 8 * B.",
            "Under, in theory we need to do this for each pair with a 400 million pages.",
            "There are two neater lines of thinking to make this task easier.",
            "First we need to find a more everything to algorithm to compute ourselves LCS rather than brute force algorithm.",
            "On the second, divide and conquer we employ a two phase process.",
            "First we need to pre partition the large scale large scale set into some kind of smaller subsets.",
            "On the 2nd, we let the pairwise comparison only occurs within each subset."
        ],
        [
            "OK, there are two primary decisions for the two aspect.",
            "First, we use my earth difference algorithm to compute L set to computer LCS.",
            "Here is the length of the shortest edit script out and be the most similar envy.",
            "The shorter D and this algorithm works more fast.",
            "Under second, we let the preperations subsets be subsets are perhaps similar web pages and search DS presumably small in each subset, and it's helpful to reduce the computing time to run my Earth algorithm.",
            "We have some deep considerations for better result in the 1st."
        ],
        [
            "Out for higher performance over the process.",
            "This involves 2 steps.",
            "First, we further reduce T and A+B before applying my Earth algorithm.",
            "This is done as such.",
            "We felt out portions.",
            "Our page, which unlikely exists in the LCS of the two pages.",
            "Under second, we avoid complete pairwise comparison.",
            "In perhaps similar success.",
            "Without pages in each subset, by some criterion such as lens and the number of fingerprints.",
            "And then each page only compare with pages after 8 and not similar with the pages before 8.",
            "Here's an example.",
            "Suppose the order of the sorted pages is ABCDEF.",
            "We first compare a with BCDF.",
            "If CMD turn out to be similar with a, then a CD for us at our similarities at the next, we compare D. We compare be only with E&F but another to be compared with C&D."
        ],
        [
            "For higher quality of the result.",
            "We first improved recall.",
            "We allow perhaps similar pages.",
            "The subset be reasonably large, but avoid too large for absence.",
            "This is that as follows.",
            "First we compute a set of fingerprints for each page.",
            "And they're coming.",
            "Fingerprints define a collection of initial equivalent sets of pages.",
            "The initial perhaps similar pages set.",
            "Other then we employ some partial transitivity to obtain a larger equivalent set.",
            "This is the perhaps similar set.",
            "And to make sure they form a partition of the original set, it means that one page won't exist in two sets.",
            "And to improve to improve precision, we felt out portions of our page which are not relevant to the topical content.",
            "Our page.",
            "For example, the uniform templates used in creating pages with their website."
        ],
        [
            "The evaluation was done on a data set of 400 million topical type web pages from Chinese Web Archive web informal.",
            "After Phase one, the 600, four 100 million properties were divided into 46 million, or perhaps similar sets.",
            "I'm the first to refine this, says two generated to generate 68 meeting similar sets.",
            "We then randomized 1000 sets for humiliation.",
            "Do you compare with the same hash and shingling imprecision, recall and evidence?"
        ],
        [
            "Is done as follows.",
            "First the month or themselves were chosen and the 2nd for each set.",
            "We're coming expansion we choose a representative page, we call it RP.",
            "Which has the most similar.",
            "Pages in this set.",
            "Under the precedent is defined as.",
            "The epsilon of the ratio between the number of two similarities of RP in this set and the number of operators in this set.",
            "And then we divide this value by the same point set.",
            "The recall is committed as the symptom of the number of two similarities.",
            "Our IP detected by LCS or Cuisine and the number of the two similarities RRP detected by some hash.",
            "We then divide this value by the number of the same process."
        ],
        [
            "The result is illustrated in table one.",
            "For overall precision, else at all rhythm, get our president.",
            "Our zero point 95.",
            "While some hash was zero, point 72 and causing or zero point 82.",
            "At the same time, LCS gets as much recall, 1.86 as much as seem harsh and closing gate one point 19 as much recall as in harsh.",
            "We also did some experiment 4 pages on the same website.",
            "The three algorithms Fund almost the same number of two similarities in the same websites, while the preceding words zero point 52 zero point, 60 three and zero point 91 respectively.",
            "In terms of evidence A.",
            "Processing 400 million webpages.",
            "Sympress spent 94 hours and cuisine spent 68 hours and LCS spent 118 hours.",
            "However, when considering, then need to split word in languages such as Chinese.",
            "The cuisine we all spent extra 534 hours.",
            "The moment of the evidence and precision we are being illustrated more clearly in Table 2.",
            "Without sleep, 1 means that without pre partitioning the set into subsets of similar web pages.",
            "Our already done.",
            "We all spend several years to compute the whole set.",
            "And I just need one, but without using the filter instead.",
            "We still need 5000 and 1770 hours 700 hours to process, but others need to.",
            "The present time is only 118.",
            "For precision.",
            "The precision with us perhaps similar properties is 0.67 and after LCS comparison.",
            "The final precision of our algorithm is 0.95."
        ],
        [
            "OK, let's make a summary.",
            "Using six PC servers, we spent about 120 hours impersonating 430 million web pages into 68 million subsets of similar pages.",
            "With much better precision and recall that mainstream algorithms such as seem harsh and cuisine, and the performance is close.",
            "Now White could the president so be improved?",
            "First we used LCS as a similar similarity measurement.",
            "Intuitively, it reflects.",
            "It reflects the true match between the parties.",
            "It also the true similarity.",
            "And the second other algorithms such as SIM card and shingling conceptually use the probability of similarity as a measurement and they are interactive.",
            "White can we achieve acceptable performance for the task?",
            "We adopt A divide and conquer strategy with one turning of each key points."
        ],
        [
            "There is zero sometime left so are introduced.",
            "Some details of the algorithm OK.",
            "I think someone will be interested in the details of how I computed the fingerprints of a page.",
            "This involves this steps.",
            "First we transform age original page in two sets of sentences.",
            "This is done by removing all the HTML markups and formatting instructions, and then we compute MD5 value for every sentence.",
            "Under distributed this value to a certain.",
            "Wooden bucket according to the result of the value of model.",
            "Under each bucket, we select on such as equals three smallest value to compute a new amplifier value card fingerprint.",
            "And at last we use these fingerprints to index and class cluster pages to form sets or perhaps similar web pages."
        ],
        [
            "Under the table with the filter step is as follows.",
            "I fall to pieces, pitch a underpaid be.",
            "First we use a sliding window and rubbing hard function to compute half value for each fragment or pay to be.",
            "These values are saved into the hash table.",
            "Then we compute the hash values for PA in the same way.",
            "Call each hash value away.",
            "We check if the same value exists in the hard stable orbit.",
            "If a match was found, then we reserved the corresponding fragment of a.",
            "Too expensive gadget, respectively."
        ],
        [
            "And there are some materials our computing LCS.",
            "First, after the two of the truth or going steps, we have crude computed errors.",
            "Yes, with acceptable ever since.",
            "However, there is still a problem.",
            "That is, the templates on the same websites also exist in LCS and made a bad influence on precision.",
            "Under there is a disk discovery.",
            "The templates often exist in the head and end, pushing our page with other noise information.",
            "So it made the distribution of LCS incoherently in the head until portions and current in the main text of the original page.",
            "Based on this observation, we choose a possible portion of LCS which was current and existed in the central portion of our page.",
            "To compute the similarity and dissimilarity is more precise than the original 1."
        ],
        [
            "OK, this is end of the presentation and thank you and I'm very sorry for the mistake at the begin.",
            "Thanks sweet."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK money, ladies and gentlemen, I am late.",
                    "label": 0
                },
                {
                    "sent": "I came here with Professor Lee Charming.",
                    "label": 0
                },
                {
                    "sent": "Our work is about near duplicates detection.",
                    "label": 0
                },
                {
                    "sent": "We face such a problem.",
                    "label": 0
                },
                {
                    "sent": "Given a set of 400 million web pages and 10 computers, how to partition it into into sets with similar pages with high precision and high recall in less than one week?",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Person.",
                    "label": 0
                },
                {
                    "sent": "Wait a minute, please.",
                    "label": 0
                },
                {
                    "sent": "Resolution.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Knock, knock.",
                    "label": 0
                },
                {
                    "sent": "Messenger.",
                    "label": 0
                },
                {
                    "sent": "Screen.",
                    "label": 0
                },
                {
                    "sent": "And before very sorry for this mistake.",
                    "label": 0
                },
                {
                    "sent": "OK. Now I'll turn into a way to describe this work is LCS based algorithm for partitioning large scale websites into subsets of similar pages.",
                    "label": 1
                },
                {
                    "sent": "First let me explain some of the keywords.",
                    "label": 0
                },
                {
                    "sent": "Error CS The longest common subsequence.",
                    "label": 0
                },
                {
                    "sent": "Here's an example, the LCS 8 NB is C ABA.",
                    "label": 0
                },
                {
                    "sent": "Best means we use Celsius as a similarity measurement pair.",
                    "label": 0
                },
                {
                    "sent": "Rabs is the resemblance of NB.",
                    "label": 1
                },
                {
                    "sent": "Web pages we focus on topic type, article type pages such as news or blog articles.",
                    "label": 0
                },
                {
                    "sent": "But not navigational pages.",
                    "label": 1
                },
                {
                    "sent": "Large set we have more than 400 million pages to process, so absolutely is very important algorithm.",
                    "label": 1
                },
                {
                    "sent": "We aimed both at precision and recall.",
                    "label": 0
                },
                {
                    "sent": "As described in this way, this work is not about given to the pages.",
                    "label": 1
                },
                {
                    "sent": "How to tell if they are similar accurately and effectively?",
                    "label": 0
                },
                {
                    "sent": "This is a kind of micro problem, but is about.",
                    "label": 1
                },
                {
                    "sent": "Given a large set of web pages, how to partition it into subsets of similar web pages, effectively with high quality and recently?",
                    "label": 1
                },
                {
                    "sent": "This is a macro problem, so it will use some of some solutions of the micro problem.",
                    "label": 1
                },
                {
                    "sent": "But because our objective is at a higher level, so we have a larger solution space to work with.",
                    "label": 0
                },
                {
                    "sent": "In this sense, what are people is about is really a framework of algorithms.",
                    "label": 0
                },
                {
                    "sent": "And for better precision and recall, we decided to use LCS for the measurement.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having choosing the environment, the call challenge, our work is the efficiency of the whole process.",
                    "label": 1
                },
                {
                    "sent": "This involves two aspect.",
                    "label": 0
                },
                {
                    "sent": "First given 2 pages A&B.",
                    "label": 1
                },
                {
                    "sent": "The brute force algorithms to compute the LCS onb is time consuming.",
                    "label": 0
                },
                {
                    "sent": "Here is 8 * B.",
                    "label": 1
                },
                {
                    "sent": "Under, in theory we need to do this for each pair with a 400 million pages.",
                    "label": 1
                },
                {
                    "sent": "There are two neater lines of thinking to make this task easier.",
                    "label": 0
                },
                {
                    "sent": "First we need to find a more everything to algorithm to compute ourselves LCS rather than brute force algorithm.",
                    "label": 0
                },
                {
                    "sent": "On the second, divide and conquer we employ a two phase process.",
                    "label": 0
                },
                {
                    "sent": "First we need to pre partition the large scale large scale set into some kind of smaller subsets.",
                    "label": 1
                },
                {
                    "sent": "On the 2nd, we let the pairwise comparison only occurs within each subset.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there are two primary decisions for the two aspect.",
                    "label": 1
                },
                {
                    "sent": "First, we use my earth difference algorithm to compute L set to computer LCS.",
                    "label": 0
                },
                {
                    "sent": "Here is the length of the shortest edit script out and be the most similar envy.",
                    "label": 1
                },
                {
                    "sent": "The shorter D and this algorithm works more fast.",
                    "label": 0
                },
                {
                    "sent": "Under second, we let the preperations subsets be subsets are perhaps similar web pages and search DS presumably small in each subset, and it's helpful to reduce the computing time to run my Earth algorithm.",
                    "label": 1
                },
                {
                    "sent": "We have some deep considerations for better result in the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out for higher performance over the process.",
                    "label": 0
                },
                {
                    "sent": "This involves 2 steps.",
                    "label": 0
                },
                {
                    "sent": "First, we further reduce T and A+B before applying my Earth algorithm.",
                    "label": 1
                },
                {
                    "sent": "This is done as such.",
                    "label": 0
                },
                {
                    "sent": "We felt out portions.",
                    "label": 0
                },
                {
                    "sent": "Our page, which unlikely exists in the LCS of the two pages.",
                    "label": 1
                },
                {
                    "sent": "Under second, we avoid complete pairwise comparison.",
                    "label": 0
                },
                {
                    "sent": "In perhaps similar success.",
                    "label": 1
                },
                {
                    "sent": "Without pages in each subset, by some criterion such as lens and the number of fingerprints.",
                    "label": 1
                },
                {
                    "sent": "And then each page only compare with pages after 8 and not similar with the pages before 8.",
                    "label": 1
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose the order of the sorted pages is ABCDEF.",
                    "label": 0
                },
                {
                    "sent": "We first compare a with BCDF.",
                    "label": 0
                },
                {
                    "sent": "If CMD turn out to be similar with a, then a CD for us at our similarities at the next, we compare D. We compare be only with E&F but another to be compared with C&D.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For higher quality of the result.",
                    "label": 1
                },
                {
                    "sent": "We first improved recall.",
                    "label": 0
                },
                {
                    "sent": "We allow perhaps similar pages.",
                    "label": 0
                },
                {
                    "sent": "The subset be reasonably large, but avoid too large for absence.",
                    "label": 1
                },
                {
                    "sent": "This is that as follows.",
                    "label": 1
                },
                {
                    "sent": "First we compute a set of fingerprints for each page.",
                    "label": 1
                },
                {
                    "sent": "And they're coming.",
                    "label": 1
                },
                {
                    "sent": "Fingerprints define a collection of initial equivalent sets of pages.",
                    "label": 1
                },
                {
                    "sent": "The initial perhaps similar pages set.",
                    "label": 0
                },
                {
                    "sent": "Other then we employ some partial transitivity to obtain a larger equivalent set.",
                    "label": 0
                },
                {
                    "sent": "This is the perhaps similar set.",
                    "label": 0
                },
                {
                    "sent": "And to make sure they form a partition of the original set, it means that one page won't exist in two sets.",
                    "label": 0
                },
                {
                    "sent": "And to improve to improve precision, we felt out portions of our page which are not relevant to the topical content.",
                    "label": 1
                },
                {
                    "sent": "Our page.",
                    "label": 0
                },
                {
                    "sent": "For example, the uniform templates used in creating pages with their website.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The evaluation was done on a data set of 400 million topical type web pages from Chinese Web Archive web informal.",
                    "label": 1
                },
                {
                    "sent": "After Phase one, the 600, four 100 million properties were divided into 46 million, or perhaps similar sets.",
                    "label": 0
                },
                {
                    "sent": "I'm the first to refine this, says two generated to generate 68 meeting similar sets.",
                    "label": 0
                },
                {
                    "sent": "We then randomized 1000 sets for humiliation.",
                    "label": 0
                },
                {
                    "sent": "Do you compare with the same hash and shingling imprecision, recall and evidence?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is done as follows.",
                    "label": 0
                },
                {
                    "sent": "First the month or themselves were chosen and the 2nd for each set.",
                    "label": 1
                },
                {
                    "sent": "We're coming expansion we choose a representative page, we call it RP.",
                    "label": 1
                },
                {
                    "sent": "Which has the most similar.",
                    "label": 1
                },
                {
                    "sent": "Pages in this set.",
                    "label": 1
                },
                {
                    "sent": "Under the precedent is defined as.",
                    "label": 0
                },
                {
                    "sent": "The epsilon of the ratio between the number of two similarities of RP in this set and the number of operators in this set.",
                    "label": 1
                },
                {
                    "sent": "And then we divide this value by the same point set.",
                    "label": 1
                },
                {
                    "sent": "The recall is committed as the symptom of the number of two similarities.",
                    "label": 0
                },
                {
                    "sent": "Our IP detected by LCS or Cuisine and the number of the two similarities RRP detected by some hash.",
                    "label": 0
                },
                {
                    "sent": "We then divide this value by the number of the same process.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The result is illustrated in table one.",
                    "label": 0
                },
                {
                    "sent": "For overall precision, else at all rhythm, get our president.",
                    "label": 0
                },
                {
                    "sent": "Our zero point 95.",
                    "label": 0
                },
                {
                    "sent": "While some hash was zero, point 72 and causing or zero point 82.",
                    "label": 0
                },
                {
                    "sent": "At the same time, LCS gets as much recall, 1.86 as much as seem harsh and closing gate one point 19 as much recall as in harsh.",
                    "label": 0
                },
                {
                    "sent": "We also did some experiment 4 pages on the same website.",
                    "label": 0
                },
                {
                    "sent": "The three algorithms Fund almost the same number of two similarities in the same websites, while the preceding words zero point 52 zero point, 60 three and zero point 91 respectively.",
                    "label": 0
                },
                {
                    "sent": "In terms of evidence A.",
                    "label": 0
                },
                {
                    "sent": "Processing 400 million webpages.",
                    "label": 0
                },
                {
                    "sent": "Sympress spent 94 hours and cuisine spent 68 hours and LCS spent 118 hours.",
                    "label": 0
                },
                {
                    "sent": "However, when considering, then need to split word in languages such as Chinese.",
                    "label": 0
                },
                {
                    "sent": "The cuisine we all spent extra 534 hours.",
                    "label": 0
                },
                {
                    "sent": "The moment of the evidence and precision we are being illustrated more clearly in Table 2.",
                    "label": 0
                },
                {
                    "sent": "Without sleep, 1 means that without pre partitioning the set into subsets of similar web pages.",
                    "label": 0
                },
                {
                    "sent": "Our already done.",
                    "label": 0
                },
                {
                    "sent": "We all spend several years to compute the whole set.",
                    "label": 0
                },
                {
                    "sent": "And I just need one, but without using the filter instead.",
                    "label": 0
                },
                {
                    "sent": "We still need 5000 and 1770 hours 700 hours to process, but others need to.",
                    "label": 0
                },
                {
                    "sent": "The present time is only 118.",
                    "label": 0
                },
                {
                    "sent": "For precision.",
                    "label": 0
                },
                {
                    "sent": "The precision with us perhaps similar properties is 0.67 and after LCS comparison.",
                    "label": 1
                },
                {
                    "sent": "The final precision of our algorithm is 0.95.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's make a summary.",
                    "label": 0
                },
                {
                    "sent": "Using six PC servers, we spent about 120 hours impersonating 430 million web pages into 68 million subsets of similar pages.",
                    "label": 1
                },
                {
                    "sent": "With much better precision and recall that mainstream algorithms such as seem harsh and cuisine, and the performance is close.",
                    "label": 1
                },
                {
                    "sent": "Now White could the president so be improved?",
                    "label": 1
                },
                {
                    "sent": "First we used LCS as a similar similarity measurement.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, it reflects.",
                    "label": 0
                },
                {
                    "sent": "It reflects the true match between the parties.",
                    "label": 0
                },
                {
                    "sent": "It also the true similarity.",
                    "label": 1
                },
                {
                    "sent": "And the second other algorithms such as SIM card and shingling conceptually use the probability of similarity as a measurement and they are interactive.",
                    "label": 0
                },
                {
                    "sent": "White can we achieve acceptable performance for the task?",
                    "label": 0
                },
                {
                    "sent": "We adopt A divide and conquer strategy with one turning of each key points.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is zero sometime left so are introduced.",
                    "label": 0
                },
                {
                    "sent": "Some details of the algorithm OK.",
                    "label": 0
                },
                {
                    "sent": "I think someone will be interested in the details of how I computed the fingerprints of a page.",
                    "label": 0
                },
                {
                    "sent": "This involves this steps.",
                    "label": 0
                },
                {
                    "sent": "First we transform age original page in two sets of sentences.",
                    "label": 1
                },
                {
                    "sent": "This is done by removing all the HTML markups and formatting instructions, and then we compute MD5 value for every sentence.",
                    "label": 1
                },
                {
                    "sent": "Under distributed this value to a certain.",
                    "label": 1
                },
                {
                    "sent": "Wooden bucket according to the result of the value of model.",
                    "label": 1
                },
                {
                    "sent": "Under each bucket, we select on such as equals three smallest value to compute a new amplifier value card fingerprint.",
                    "label": 0
                },
                {
                    "sent": "And at last we use these fingerprints to index and class cluster pages to form sets or perhaps similar web pages.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under the table with the filter step is as follows.",
                    "label": 0
                },
                {
                    "sent": "I fall to pieces, pitch a underpaid be.",
                    "label": 0
                },
                {
                    "sent": "First we use a sliding window and rubbing hard function to compute half value for each fragment or pay to be.",
                    "label": 0
                },
                {
                    "sent": "These values are saved into the hash table.",
                    "label": 1
                },
                {
                    "sent": "Then we compute the hash values for PA in the same way.",
                    "label": 1
                },
                {
                    "sent": "Call each hash value away.",
                    "label": 1
                },
                {
                    "sent": "We check if the same value exists in the hard stable orbit.",
                    "label": 1
                },
                {
                    "sent": "If a match was found, then we reserved the corresponding fragment of a.",
                    "label": 0
                },
                {
                    "sent": "Too expensive gadget, respectively.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are some materials our computing LCS.",
                    "label": 0
                },
                {
                    "sent": "First, after the two of the truth or going steps, we have crude computed errors.",
                    "label": 0
                },
                {
                    "sent": "Yes, with acceptable ever since.",
                    "label": 0
                },
                {
                    "sent": "However, there is still a problem.",
                    "label": 0
                },
                {
                    "sent": "That is, the templates on the same websites also exist in LCS and made a bad influence on precision.",
                    "label": 1
                },
                {
                    "sent": "Under there is a disk discovery.",
                    "label": 0
                },
                {
                    "sent": "The templates often exist in the head and end, pushing our page with other noise information.",
                    "label": 1
                },
                {
                    "sent": "So it made the distribution of LCS incoherently in the head until portions and current in the main text of the original page.",
                    "label": 1
                },
                {
                    "sent": "Based on this observation, we choose a possible portion of LCS which was current and existed in the central portion of our page.",
                    "label": 0
                },
                {
                    "sent": "To compute the similarity and dissimilarity is more precise than the original 1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is end of the presentation and thank you and I'm very sorry for the mistake at the begin.",
                    "label": 0
                },
                {
                    "sent": "Thanks sweet.",
                    "label": 0
                }
            ]
        }
    }
}