{
    "id": "5436pcxsjli27ketcurjhrbdgggkg7ch",
    "title": "Volume Regularization for Binary Classification",
    "info": {
        "author": [
            "Koby Crammer, Department of Electrical Engineering, Technion - Israel Institute of Technology"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_crammer_volume_regularization/",
    "segmentation": [
        [
            "Let me go straight to the bottom line.",
            "We have an algorithm for binary classification and we use there, not a single weight vector as we all do, but a box of them and what we see here we compare the performance of our algorithm versus the performance of both SVM in another algorithm called Arrow.",
            "In here we have 30 NLP problems and when a point is above the diagonal line, it means that our algorithm performs better than with algorithms.",
            "We have two variants of the algorithm that why we have both columns.",
            "We also."
        ],
        [
            "Evaluated our algorithm on some other data set and he wanted to see how well we do in the presence of noise.",
            "So we used all 45 bills of USB pairs of USPS data sets, and we compared again.",
            "Let's say our algorithm called Bow bag of weights and let's say support vector, machine and count how many times one algorithm was better than the other and if the here higher is better.",
            "So when there is no noise, we see that SVM is better about 20 times while when there is more noise 1020 or more or even 30 labeled noise.",
            "Then our algorithm performs better and you can see similar results on the right hand side.",
            "So what do we do?"
        ],
        [
            "Usually what we do is what you call empirical risk minimization, maybe with regularization.",
            "So given a training set of inputs X and output Y, we minimize the expected loss over the training set plus some regularization.",
            "And let's say we pick one weight vector that is doing well.",
            "What we do we replace the weight vector with a box of weight vectors and we say once you give me a box, I will pick the worst weight vector in that box and I will evaluate the objective using the worst thing.",
            "In fact, we go more than that.",
            "And we take the Super Moon with the worst case for each and every example, and even for the organization.",
            "So the bottom line, what we have in our paper and you can come today to the poster and seat.",
            "We have two batch formulations.",
            "We have an optimization algorithm that we show that is guaranteed to converge or all the optimization problems are convex.",
            "We have generalization bound that are derived from the pack, gave a framework and essentially show that with algorithm with minimizing a variant of this.",
            "In bound and use already the results of the experiment.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me go straight to the bottom line.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm for binary classification and we use there, not a single weight vector as we all do, but a box of them and what we see here we compare the performance of our algorithm versus the performance of both SVM in another algorithm called Arrow.",
                    "label": 1
                },
                {
                    "sent": "In here we have 30 NLP problems and when a point is above the diagonal line, it means that our algorithm performs better than with algorithms.",
                    "label": 0
                },
                {
                    "sent": "We have two variants of the algorithm that why we have both columns.",
                    "label": 0
                },
                {
                    "sent": "We also.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluated our algorithm on some other data set and he wanted to see how well we do in the presence of noise.",
                    "label": 0
                },
                {
                    "sent": "So we used all 45 bills of USB pairs of USPS data sets, and we compared again.",
                    "label": 0
                },
                {
                    "sent": "Let's say our algorithm called Bow bag of weights and let's say support vector, machine and count how many times one algorithm was better than the other and if the here higher is better.",
                    "label": 0
                },
                {
                    "sent": "So when there is no noise, we see that SVM is better about 20 times while when there is more noise 1020 or more or even 30 labeled noise.",
                    "label": 0
                },
                {
                    "sent": "Then our algorithm performs better and you can see similar results on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Usually what we do is what you call empirical risk minimization, maybe with regularization.",
                    "label": 0
                },
                {
                    "sent": "So given a training set of inputs X and output Y, we minimize the expected loss over the training set plus some regularization.",
                    "label": 0
                },
                {
                    "sent": "And let's say we pick one weight vector that is doing well.",
                    "label": 0
                },
                {
                    "sent": "What we do we replace the weight vector with a box of weight vectors and we say once you give me a box, I will pick the worst weight vector in that box and I will evaluate the objective using the worst thing.",
                    "label": 0
                },
                {
                    "sent": "In fact, we go more than that.",
                    "label": 0
                },
                {
                    "sent": "And we take the Super Moon with the worst case for each and every example, and even for the organization.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line, what we have in our paper and you can come today to the poster and seat.",
                    "label": 0
                },
                {
                    "sent": "We have two batch formulations.",
                    "label": 0
                },
                {
                    "sent": "We have an optimization algorithm that we show that is guaranteed to converge or all the optimization problems are convex.",
                    "label": 0
                },
                {
                    "sent": "We have generalization bound that are derived from the pack, gave a framework and essentially show that with algorithm with minimizing a variant of this.",
                    "label": 0
                },
                {
                    "sent": "In bound and use already the results of the experiment.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}