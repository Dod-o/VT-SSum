{
    "id": "k2i5wwlxa3cocrznraubnkxvr76xg2ji",
    "title": "Information Dynamics and the Perception of Temporal Structure in Music",
    "info": {
        "author": [
            "Samer A. Abdallah, Queen Mary, University of London"
        ],
        "published": "Dec. 29, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Arts->Music",
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/mbc07_abdallah_idt/",
    "segmentation": [
        [
            "I'm going to be talking about actually fairly fairly simple application of information theory too, in a setting of a dynamic probabilistic models with intention of applying them to music.",
            "Until we're calling this."
        ],
        [
            "Information dynamics for short.",
            "So here's a brief outline of the talk.",
            "First of all, I'm going to be talking about a little bit about the sort of motivation which comes from various ideas that have come up over the ages to do with.",
            "Why is that?",
            "Music elicits the kind of response is it?",
            "Does the emotional, emotional responses, and just the general subjective experience of listening to music.",
            "And then I'll introduce you introduce, well, just talk about.",
            "Anyway, the the kind of the sort of the basic framework that will be working in, which is the idea of an observer who has a probabilistic model of what the music is doing.",
            "And then to illustrate those ideas, I'll just talk about possibly one of the simplest models that we could think about using in this situation Markov chain.",
            "And then at that point it slightly old place to have related work, but it's it's, I think by that stage will have introduced their ideas to be able to compare meaningful meaningfully with what's gone before, and then I'll be talking about a couple of applications.",
            "One is experiment where we apply this sort of Markov chain information dynamics idea to a couple of pieces of minimalist music to see if we can get any kind of sensible analysis of the music out of them.",
            "And the second application is actually a gesture recognition application, which I haven't written in the.",
            "In on there, but it's it's just a recognition.",
            "Isn't information dynamics in a hidden Markov model this time, and then I'll round off with some future work summary conclusions."
        ],
        [
            "Right?"
        ],
        [
            "So part one.",
            "So it's often been observed that.",
            "One of the one of the salient features of music listening to music is that it creates expectations.",
            "People talk about implication and expectancy and uncertainty and tension in these sort of things, and that's been talked about in the last century, notably by Leonard Leonard B. Meyer and Eugene Narmour and so on.",
            "But it's been it's been discussed for a lot a lot earlier than that.",
            "I quite like this.",
            "The statement by Edward Handzlik, who put it quite nicely.",
            "I think the most important factor in the mental process which accompanies the active listening to music and which converts it to a source of pleasure is the intellectual satisfaction, which listening to rise from continually following and anticipating the composers intentions.",
            "Now to see his expectations fulfilled and now to find themselves agreeably mistaken.",
            "It is a matter of course, of this intellectual flux and reflux.",
            "This perpetual giving and receiving takes place unconsciously and with the rapidity of lightning flashes, something that's a fairly nice statement of what we do.",
            "Which we would like to sort of get a grip on in a theoretical sense."
        ],
        [
            "Right, so I'm going to do a little little experiment on you to just illustrate the healthier the idea of unfolding message so.",
            "I'm sure you can probably guess what's going to happen, but at this stage is obviously an incomplete sentence, and so you may be wondering what what's going to come next."
        ],
        [
            "Anaza as the words appear you."
        ],
        [
            "May have guessed what the word is going to be.",
            "Or you might know."
        ],
        [
            "You might be surprised by it."
        ],
        [
            "That one you might have guessed at that point at the next word."
        ],
        [
            "Going to be time so not not too surprised.",
            "Now there's a comma.",
            "But at this point you're probably not sure about what's going to come next, because the comma is sort of a structural break, and it means that there's going to be greater uncertainty about the next."
        ],
        [
            "So it just, well, I'm just going to finish up in in one day.",
            "I'm fairly sure nobody gets that.",
            "So that really illustrates So what we want to focus on is this idea of a phenomenon phenomenon that unfolds in time, and we want to focus on how the listener sort of forms and modifies their expectations as things happen.",
            "And we want to try and do this in a in a quantitative way.",
            "Anne."
        ],
        [
            "Well, anyway, that's just what I've just said, really.",
            "I mean, my sort of more or less made the same the same arguments.",
            "Now my work in the 50s came shortly after.",
            "Publication Shannon's information theory which really.",
            "Gave us the the sort of mathematical tools that seem sort of appropriate for the job here that we're trying to do.",
            "And so since then, there's been various various parts of work on applying information theory to musical phenomena and not just music but also to aesthetics in general, and trying to see if there's any relationship between.",
            "The primary information theoretic concepts, and notably entropy people focused on entropy, restaurant entropy and aesthetic value, or just kind of judgments of pleasing this that people make and these experiments.",
            "Within the context, both of of sequences, sequences of sounds, musical sounds or pitches, and also geometric shapes and the experiments.",
            "Let's just see what?",
            "Yes, let's get back the experiments generally focused on presenting stimuli of what was intended to be varying levels of uncertainty and then getting people to rate them somehow, often on a verbal scale.",
            "I'm thinking of the experiments of the line in the 70s.",
            "Um?",
            "But really, what was?",
            "Well, indeed they did discover a certain relationship between sort of complexity of the stimulus, the entropy uncertainty, and how well people rated it and.",
            "Uh, and.",
            "Quite often they found this relationship where something was very, very predictable or very very entry was low.",
            "For example, people would rate it sort of fairly low, not particularly interesting, and then as the complexity of the stimulus increase and people would sort of like it, but the ratings would increase up to a certain point, and then they would drop off again as things started to get a bit bit complex.",
            "I'm."
        ],
        [
            "I think I've probably derailed myself from the slides now, but.",
            "OK, so I'll just carry on from.",
            "From this pic here.",
            "We are we want to take the viewpoint of an observer who who is actively making predictions and formulating degrees of belief about what what might be happening and their reasoning under under under conditions of uncertainty.",
            "And also you don't know what we don't know exactly what's going to happen next.",
            "And there are some very fairly sort of general arguments, so there's of Cox and James and so on.",
            "Who are people who argue that really the best way to reason under uncertainties to use probability theory?",
            "Because it's the only consistent way to do it.",
            "So we kind of take that on board and and really not worry about any other ways of dealing with uncertainty.",
            "There are others.",
            "But sort of taking that assumption, so we're going to take the view."
        ],
        [
            "Have a probabilistic observer.",
            "And we're going to.",
            "We're going to assume that they their expectations are probabilistic, probabilistic in nature as letter, Mayer also suggested.",
            "So your knowledge of a musical style, for example, is statistical in nature in terms of what things tend to follow other things.",
            "What notes tend to follow each other?",
            "What cause and so on?",
            "And these models are.",
            "Are built up over overall sort of a whole lifetime of listening to different styles of music.",
            "So when you come to a new piece of music, you already have this sort of sort of baggage that you know you have some expectations about what's going to happen given given at the start of the peaceful Europe, which might depend on the context of what you already know about it.",
            "But importantly, as well as the as well as your your knowledge of style in general and that you've gleaned from many listening experiences.",
            "There's also a sense in which, as the piece opens.",
            "You immediately start to build up an idea of what patterns and what what themes the composer is is using, and this is something that we can do very quickly.",
            "As soon as there's an opening.",
            "As soon as a dead are, then the other day that I was sort of follow on."
        ],
        [
            "So so, so we so we'd really like to balance this idea of learning learning over sort of background learning, and also on the on the job learning, which happens very quickly as soon as the piece starts.",
            "So in that case we we now have a several probabilistic model which we want to apply, and as soon as the first first first three events come in, then you immediately want to start revising your beliefs about what's going to happen next.",
            "So in that case.",
            "At the beginning, you've got this sort of the piece unfolding in front sort of opening up in front of you.",
            "You might have if you just imagine some sort of vague distribution over what's going to happen in the future.",
            "Every time a new event occurs, also something happens.",
            "You can modify those distributions.",
            "You can change.",
            "You can revise your beliefs about what's going to happen.",
            "And so this is where the information information theoretic ideas come in.",
            "Because when we have distributions we can compute compute entropies and quantify the concept of uncertainty.",
            "And we can also.",
            "Quantify what happens when a distribution changes when we revise our beliefs over distribution changes, we can measure the we can measure essentially the distance between the two distributions and get a get a feeling for how much your beliefs are changed and.",
            "In a quite a quite a meaningful sense, this gives you a measure of how much information you've received about things that you haven't observed yet.",
            "So so an event happens, something happens and you change your distributions.",
            "The distance between those two quantify the amount of information that was in that particular observation, and so as the events that have happened one after the other, each one carries a certain amount of information, and so we can quantify that using."
        ],
        [
            "The tools of information theory."
        ],
        [
            "Sing us, just move on from that.",
            "So the interesting question here is that we have a set of quantitative tools which.",
            "Which we have to we have the mass for and we have on the.",
            "On the other hand, we have a set of subjective phenomena and relating to the way people describe musical experience or things like tension and surprise and.",
            "A closure and and and that sort of thing, so we'd like to see if there's a if we can really make a relationship between the quantitative quantitative theory that we have and the subjective ideas.",
            "And in that way, trying to build a build a bridge between the quantitative side and the experiential aspects."
        ],
        [
            "Right, right after I've managed to intersect with my slides once again.",
            "So bear line in the 70s talked about these these information theoretic quantities, entropy and so on and and others and called them qualitative variables.",
            "He was trying to.",
            "He was trying to evoke the idea that there not to do with the surface the musical surface.",
            "The details of the musical surface not to do with what events happen.",
            "It's not even to do with the mode.",
            "It's not even today with it being musical or video or similar.",
            "It's just to do with patterns and patterns of probability and expectation.",
            "And so he coined this term information aesthetics too.",
            "To describe the study of how these quality variables relate to this subjective experience."
        ],
        [
            "So this slide is basically a summary of where we've got two.",
            "Now that's sort of our our set of core assumptions, or at least our hypothesis.",
            "So we are probabilistic observer.",
            "As we listen, we maintain a model of what's what's coming.",
            "And as events unfold, we we change our prediction."
        ],
        [
            "And.",
            "We can count.",
            "We can characterize these.",
            "This sort of belief state in terms of entropies, for uncertainties about various things that might be happening.",
            "Different variables.",
            "And also we can quantify the amount of information we receive from each observation about things of interest that might happen in the future."
        ],
        [
            "And because all of this is now happening in a dynamic sense, we can we can.",
            "We can put all these quantities quantities against time now so we can plot uncertainty, entropy against time and see if there are.",
            "You know if there are moments when uncertainties particularly or particularly at the same time, we can plot information and we can complete surprise and so on, and so in.",
            "That way we get a small.",
            "We get this kind of low dimensional representation, a few signals, three or four that are moving through time in a certain way.",
            "And they can sort of make almost gestures.",
            "They can do sort of move around in various ways.",
            "And what we'd like to find out is if.",
            "If I if if this somehow captures an essence of what the listening experience is, so really would like to be able to just kind of throw away all of the details of what's happening on the musical surface and and even the internals of the model variables inside the model and just focus on this.",
            "This dynamic?",
            "These shapes, that they're that have been generated by the model.",
            "Anne."
        ],
        [
            "So what are the?",
            "What are they sort of desirable features of this approach?",
            "One is, we've abstracted away all the medium specific details.",
            "And focused really just on.",
            "Probabilities and entries and things that are sort of a model in that sense.",
            "So that's the sort of appeals to a sense of being able to compare different types of experience.",
            "For example, you could look at information in music, but you could also equally look at it in cinema or in Dancer, in drama.",
            "And because we were looking at because we've abstracted sort of this kind of abstract level, we can conceive conceivably think about comparing different types of experience, different types of temporal experiences that happen in different media."
        ],
        [
            "We've been talking about this.",
            "This model based observer, but we haven't actually said what what the model is, and that's open, so I'm not actually.",
            "This talk is not about a particular model.",
            "Not advocating any particular way of analyzing pieces.",
            "You know, analyzing musical data, but really, just how you interpret what the model is doing and how you.",
            "How we might be able to sort of?",
            "Cleaned something about the structure with independently of what the model is.",
            "I mean, obviously there are still questions of some models might fit much better than others, and it would make sense to use those rather rather than rather than simple models that don't fit."
        ],
        [
            "Again, sort of.",
            "Coming back to the to the modeling issue as we start to use more more complex models with with hidden variables and sort of laser processes.",
            "We we sort of automatically get a added richness to the information dynamic description because say for example in hidden Markov model we've got this sort of layer of hidden states which is which is going along as well as the layer of observed features and so we can talk about as events happen.",
            "We can we can think about whether or not we've gained information about the observations that we're about to receive.",
            "But we can also think about whether or not we've received information about the layer of hidden variables or even the parameters of hidden Markov model if you decide it there.",
            "Changing so as you make the model richer, you get a richer information dynamic.",
            "Sort of reduction of what it's doing."
        ],
        [
            "And finally, we've taken the taking the viewpoint of the observer, so we're talking about sort of subjective probabilities in the sense very much in the sense of DEFINITY.",
            "That that school of thought so.",
            "Although, although we're very unlikely to be able to explain a particular any particular person's response, the the model itself, the idea itself is very much subjective.",
            "One, because it depends on the experience and that the the model you're actually using has had and it depends on what it's been exposed to before and what kind of training is having all sorts of all sorts of things like that."
        ],
        [
            "OK, I think.",
            "Alright, and then OK, just so.",
            "So just to finish off this this background section this is.",
            "This is a little bit off a little bit.",
            "Off topic but.",
            "There is some.",
            "There is potentially a nice correspondence between the idea of information dynamics and also what various philosophers have written about.",
            "About music and about ascetic experiences, and they often talk about.",
            "Let's talk about this idea of abstracted form or dynamic form or shape.",
            "Davis reviews this review.",
            "Several contributions that he calls his groups under the heading contour theories and it's this idea that the way that music.",
            "Evokes responses or recalls or suggest things is to do because it is to do with just the dynamic.",
            "The temporal patterns that it creates.",
            "And it can because it can create those in any using any any surface feature.",
            "It's really.",
            "It's really the form itself that matters.",
            "And so when he talks about a contour, he's talking about this contour in a completely abstracted space.",
            "I'm sort of in a similar vein.",
            "We've got Suzanne Langer talking about morphology of different.",
            "Have feelings and again focusing on the idea that certain temporal patterns might evoke different different emotions and the same again with stones, vitality, effects.",
            "So.",
            "So it's it's appealing to think that the information description might some level connect with this words.",
            "Obviously not going to answer this talk."
        ],
        [
            "OK, so just a little bit of their background mathematics system.",
            "Nothing too, nothing too strenuous, but.",
            "So we've got.",
            "First of all, we have the entropy as a measure of the uncertainty in a distribution.",
            "Its measure of the spread.",
            "And also you can think of it as.",
            "If.",
            "If if the log of the probability or sorry for minus the log of the probability of something is can be considered a sense of how surprising it is.",
            "So if it's low probability, it's more surprising than the entropy is the expectation of the surprising this."
        ],
        [
            "I think I skipped one anyway.",
            "Yeah, and the sort of the second useful idea is that of the callback library.",
            "Divergent switch measures is a measure of distance between probability distributions.",
            "The way I've written it varies is sort of to foreshadow what we're going to do a bit later, and I've written here.",
            "This is the callback libel divergent between this distribution PX given D&PX&PX.",
            "Given these meant to imply or distribution over over X given some data D some evidence and PX on its own, is the distribution that before you receive the data, so this distance?",
            "And after that is expressed like that.",
            "But the idea is that that we can use this this distance to quantify the amount of information that was indeed about X because we've received and it's changed our distribution over X."
        ],
        [
            "And finally, we've got the mutual information itself, which is.",
            "A measure of dependence between between random variables and."
        ],
        [
            "I will say that.",
            "If I go back to this thing.",
            "If if if the itself, so I'm putting the screen.",
            "If the itself is is yet to be observed, but you have some district you know have some distribution for D, then the then you could work out what the expectation of this information is.",
            "The expectation of this covered."
        ],
        [
            "And that is the mutual information.",
            "They are the most sort of useful expression for mutual information for our purposes.",
            "Is this so?",
            "This is saying that the mutual information between X&Y is our uncertainty about X.",
            "Before we've seen I before we seen Y minus uncertainty about X after we've seen why.",
            "So it's this again, it captures this idea of change in beliefs."
        ],
        [
            "OK.",
            "Right so.",
            "We just sort of take the.",
            "Take the simplest possible first step, which is to consider.",
            "Stimulus, which consists which consists of a sequence of, let's say, discrete symbols.",
            "And at any particular time, we've observed a number of these things.",
            "And one is about to arrive, and we've got some in the future that we don't know about.",
            "So we can sort of summarize the state of our observer using a few probability distributions over well over S for the present, and also why the future.",
            "I've kind of used to denote the past X the present and why the future."
        ],
        [
            "Let's see now.",
            "So.",
            "This is the sort of.",
            "Potentially troublesome Venn diagram of information theory, but it will serve our purposes.",
            "The reason is potentially troublesome is that it's possible for this error in the middle to be negative.",
            "We won't worry about that.",
            "Just for visualization purposes.",
            "So if we imagine the.",
            "How can I point?",
            "Maybe I can go over here.",
            "Right, so this represents.",
            "At.",
            "Let's see now this imagine this represents what we already know.",
            "HAZ and this area here.",
            "Represents its entropy of X given Z, so that's represents our uncertainty about about the next thing which is going to happen given what we've seen so far.",
            "And we also have some uncertainty about why as well, which is this area here, which is the future after X?",
            "Anne.",
            "And one of the things that we were going to focus on a bit later on is is this error in the middle here.",
            "So what does this mean?",
            "It means?",
            "It's the information in X about Y.",
            "Given that we already know zed.",
            "And it is that area, so there's significant that will come up with a bit later on."
        ],
        [
            "Right, so in this framework we can now formulate a few quantities which are.",
            "Which we think are interesting.",
            "So one is the surprising Ness of.",
            "That the current observation X, given what we've seen so far.",
            "So we're going to quantify that as the probability of the probability of X given the past."
        ],
        [
            "At the moment by moment surprising this.",
            "So next thing is are expected.",
            "Surprising this for the next symbol, so this is now before we see X we've got said we can we have an expectation for the surprising list of X and that is the entropy of X given the current context.",
            "So that was also, we've got that by averaging over things.",
            "Sorry, averaging over what's about to happen."
        ],
        [
            "But on the other hand, we could also.",
            "This is a.",
            "A little bit to think about, but it's it's saying.",
            "Given the next thing that happens, exit some.",
            "Let it be some symbol.",
            "It will be.",
            "Surprising to some degree, depending on what just happened previously.",
            "If we now average over all the things that could have led to X.",
            "We can get a kind of an average surprising this for that particular symbol.",
            "As it occurs, sort of, imagine in in the in the collection collected fantasies of the model for example.",
            "And the reason why that might be interesting is that if a particular member of this particular symbol always tends to be surprising, then.",
            "Then it might be significant in some sense, whereas if another another symbol always tends to be sort of not surprising, then it's perhaps it should sort of blend into the background.",
            "It's not very significant, it doesn't really change your beliefs in any way, and it's you know it's never what you.",
            "Don't expect."
        ],
        [
            "Right and the last thing is to average over all things that might possibly happen, and that's that's just the entropy rate of the process.",
            "Which is a reasonable measure of of the randomness of the process, basically."
        ],
        [
            "Right, so the next thing is.",
            "Is the idea of predicting information which goes back to this intersection on the on the Venn diagram that I pointed out earlier?",
            "So the definition we're using here.",
            "Is is this thing?",
            "So as I said before, it's the information in X about the observed future.",
            "Given that we've observed everything in the past up until now.",
            "And it's.",
            "It's this callback lively divergance between the two distributions before and after you observe X, so that's before we observe X and that's after you observe X."
        ],
        [
            "So in the same way that we did for the surprising this week and now average, the predictive information in different ways to get sort of difference of global statistics about the symbols in the state space and also the process in general.",
            "So we've got this, this this instantaneous predictive information you like if you like.",
            "So it depends on.",
            "It's a function of two variables, one of which is the current symbol and the other is the context."
        ],
        [
            "So the next thing is we could average over what might be about to happen, so that gives us a measure of how much information we expect to receive receive from the next symbol.",
            "And, well, that's some speculation about relationship between that possible, possibly inattention."
        ],
        [
            "The other thing we can do is average over what might have led to a particular symbol.",
            "And that that is a measure of essentially how informative that particular sample tends to be in all the context that it occurs."
        ],
        [
            "And finally we can average over everything and that gives us the sort of average predictive information information rate of the process as a whole."
        ],
        [
            "Right?",
            "I and.",
            "Another thing we could do well, this is sort of.",
            "If you have a model which has some other some other parameters, so some hidden parameters that you're updating as you observe as you make observations, then you can also update your beliefs about those and consider that you've received information about the parameters from each observation."
        ],
        [
            "OK."
        ],
        [
            "Select Anne so going back to this work by Ballineen Co workers in the 70s.",
            "Anne.",
            "They often found this sort of U shaped curve as relationship between some overall randomness and some sort of measure of value as reported by human subjects.",
            "And this goes back to the wind curve, which wasn't defined in terms of entropies, but it's a similar sort of idea.",
            "And so the observation is, as I said before, see if something is very.",
            "Very terministic and predictable.",
            "It's boring, but by the same token, if it's if it's 2 random then people find it.",
            "Incoherent, unstructured.",
            "This sort of things.",
            "And there's a sort of sweet spot in the middle where where things are most pleasing.",
            "And also, if if it's the first time you're exposed to something which might initially be 2, two might seem too random, so we come back to the subjectivity now and dependence on the observers model.",
            "It might initially seem very random after repeated exposure.",
            "It tends to move along the curve that way, so it's possible for something which is initially overwhelming to become quite pleasing later on.",
            "So you might think of I don't know some.",
            "Messana Lake, like Coltrane or something like that, something which really really shocks you at 1st and after awhile.",
            "Kind of wonder what this was.",
            "On the other hand, if you take up a pop song, it might start off right at the top of the curve and be sort of quite nice.",
            "But after after a few listens, you're sick of it, it's it's.",
            "It slips down the down the other side.",
            "So people talk about this idea of having a balance between order and chaos and unity and diversity in various.",
            "Various terms like that, but.",
            "This there isn't really a.",
            "Doesn't really strong explanation for why there's an optimum in the middle.",
            "I mean, if there's a balance, then what determines the weighting of the balance resulting?",
            "Where does this come from?",
            "This is where our idea of predictive information comes in.",
            "Because it turns out that if you examine the predictive information rate of a process.",
            "Anne.",
            "And how it varies with the overall random, the overall entropy rate of randomness of the process.",
            "It also shows this U shaped behavior with an optimum in the middle.",
            "And."
        ],
        [
            "Oh, that moved.",
            "So try to.",
            "It will be difficult to visualize why that's the case, but I've tried to these slides.",
            "So this these represent three different processes that might be happening, and the black, the black dots represent things that we've observed, and the red ones are the ones that we haven't observed.",
            "And the density of the red.",
            "Relates to our uncertainty about what it's what it's going to be.",
            "So white means very uncertain and red means sure.",
            "So if you have a process which is very predictable and you've observed the first step or something, then you already have a pretty good idea of how you know how it's going to carry on.",
            "Whereas if it's very random in the sense that each successive observation is independent of the previous one and they run correlated, then even if you observe the first one, you have no idea about these later ones and then this is something in the middle which which is sort of somewhere in between.",
            "But if you watch what happens as you as we advance in time, then we graduate."
        ],
        [
            "Observe things one by one."
        ],
        [
            "So that's going right."
        ],
        [
            "So what you're seeing is that.",
            "As we observe these things really nothing much has changed with these ones because we already knew what they're going to be.",
            "So uncertainty about these things didn't change.",
            "Similarly, uncertainty about these things didn't change either because we were had no idea before and we have no idea offered.",
            "So the only case in which our beliefs are actually being revised as we're going along with this middle case.",
            "It's really the only one where."
        ],
        [
            "Whether it comes with."
        ],
        [
            "I'll just leave."
        ],
        [
            "You can see."
        ],
        [
            "So that that really captures what's going on in the predictive information rate.",
            "There's a point in the middle where the predictive information rate is highest.",
            "And and this is something that there's really only arbitrary decision that we've made to get, that is to say that we were making one observation at a time.",
            "So it's sort of timescale.",
            "But apart from that, there's no kind of arbitrariness about where the optimum should be depends on depends really on the model that the observer is using to make their predictions."
        ],
        [
            "Right, so that's a that's all in the sort of case of general general statistical model, but we're going to now look at the simple case of a Markov chain to see how this."
        ],
        [
            "Still works out.",
            "Right, I'm sure most of you know what Markov chain is, but this just to define our terms, we've got S to represent the successive States and the transition matrix is A and the probability for the initial state I've called.",
            "Pie.",
            "There an actually I've given it in a in a superscript there because we're going to restrict ourselves to Markov chains where the.",
            "The marginal distribution for a single observation is is stationary, so we're talking about stationary Markov chains only.",
            "So it means that the the distribution for the initial state is actually a function of the transition matrix under some some assumptions.",
            "Constraints."
        ],
        [
            "OK, well because it's such a simple model of the concept of the quantities I talked about before.",
            "Very easy to to compute in terms of.",
            "The entropy rate of the Markov chain."
        ],
        [
            "So well, there's no need to dwell on that too much."
        ],
        [
            "But we get some.",
            "We get some some expressions which come out.",
            "Look at the paper to find out where they come from, but well, the I guess they on this kind of vaguely interesting is this one here.",
            "So it turns out that the predictive information rate is the difference between the entropy rate of the two step process and the entropy rate of the one step process.",
            "So a squared is the transition matrix between.",
            "One state and the next one sort of skip after you skip one."
        ],
        [
            "OK.",
            "Right so.",
            "So that means we can now easily generate lots of transition matrices and workout what they're predicting.",
            "Information rate is and plot it, and also obviously entropy rate as well.",
            "So we find that this is this is 5 by 5 transition matrices.",
            "Drew sort of several couple thousand of them or something.",
            "They are drawn from a from a Dirichlet.",
            "Each column of the transition matrix is drawn from a different distribution.",
            "That's not so important.",
            "At least covers the space, so we find that the transition matrices with the highest entropy rate are the ones where each successive observation is independent of the law.",
            "So the transition matrix is essentially flat, and each column is the same, so there's no correlation between successive elements at the other end of the scale.",
            "With the lowest entropy rate, these are essentially the deterministic Markov chains, where the transition matrix is very sparse and it just tends to follow deterministic pattern and then in the middle we've got all this stuff going on where we find that some of them have high product information right and some of them some of them don't, and these are.",
            "Those are the draws that correspond to those points.",
            "Yeah."
        ],
        [
            "So we can now draw sequences from those Markov chains, see what they look like.",
            "So the top one is is just the repetition of the same state, so it's obviously.",
            "Predictable and boring, repetitive and everything else, and the bottom one is is the one with maximal entropy rate.",
            "So every state is independent, the previous one, and then these two ones in the middle both have more or less the same entropy rate.",
            "They're kind of in the middle of the range, but this one has a high predictive information and this one has low particular information.",
            "And you probably get some sense of.",
            "Of there being.",
            "Well, pattern or structure in this one is difficult to know what word to use without prejudice, prejudicing the result.",
            "What I can do at this stage is.",
            "To sonifi these to let you think about them with your ears instead of with your eyes.",
            "Right so.",
            "Actually what I will do first is.",
            "To my eyes.",
            "You said.",
            "This is just the.",
            "The the the distribute the scatterplot of.",
            "Oh no, that's not the one.",
            "That's the one I meant to get.",
            "So this is the scatter plot again with entropy on one side and predict information right on the other.",
            "And if you look at.",
            "So if I click on these and the transition matrix should come up here.",
            "Any luck?",
            "Yeah so.",
            "So they were.",
            "So that's sort of the high entropy rate ones are up there.",
            "High prediction prediction information is there and then.",
            "Deterministic ones are down there.",
            "Like that?",
            "OK anyway, so if I say.",
            "Jester was gonna recover the.",
            "Some.",
            "OK, alright, so that's pretty obviously the one where nothing is happening.",
            "So I'll just stop that.",
            "Now this one.",
            "This one should be the one where there's no correlations at all.",
            "OK, so it's just going to noodle along.",
            "So my observation is that you're initially you're initially inclined to be charitable and find little melodic patterns, but after it goes on, you realize there's no pain whatsoever.",
            "And there's there's nothing that recurs.",
            "There's no recognizable things really, just just some carries on like that.",
            "Right now this is the one with the intermediate entropy rate, but load particular information.",
            "It's probably a picture of it somewhere.",
            "That's it, yeah, so I'll stop that before you're going nuts.",
            "Right and then the last one is.",
            "Alright, so this is this is this one.",
            "OK, so it's not Mozart anything but.",
            "But it does have sort of things that recur that you start to recognize.",
            "And it sort of has this beginnings of structure and it seems to also have this sort of phases or it goes into one and then the other thing.",
            "I'll stop there."
        ],
        [
            "I'm gonna have to.",
            "I'm in a pinch.",
            "OK, right, so this so this product information, right?",
            "So we well actually we haven't done experiments to test on human listeners.",
            "Whether they find these less painful than the other ones, but that's something we're planning to do at the moment.",
            "One thing you can do is actually optimize the transition matrix numerically to find the ones that maximize the product information, right and?",
            "Those are the 1st.",
            "The first 4."
        ],
        [
            "OK, well I will move on 'cause it's not."
        ],
        [
            "Much time left.",
            "To work.",
            "Some of the most relevant concepts in literature from before we've got you like and Tish peas predictive information.",
            "Which."
        ],
        [
            "That's.",
            "So the idea here is that you take a long process and you break it into two adjacent blocks and you measure the mutual information between one and the other, and then you let one of them just go off to Infinity.",
            "And that gives you so that tells you information in one chunk about the rest of the whole thing in One Direction.",
            "I'm.",
            "So this is also a very interesting quantity because it seems to capture.",
            "Something intrinsic about the complexity of the process and or sort of the intrinsic demand dimensionality, which parameter space, depending on how it behaves as this.",
            "The second block goes off to Infinity.",
            "Time."
        ],
        [
            "We've also.",
            "And."
        ],
        [
            "But but it's a global measure in the same way as the average product information, which is a global measure.",
            "But it can't tell you about what's happening moment by moment which is."
        ],
        [
            "What we'd also like to do?",
            "We've also got an information rate proposed by slow down of which is.",
            "It's yes, it's the mutual information in the entire past about about the present.",
            "If you like for the next current observation.",
            "Anne.",
            "And for Markov chains it reduces to something in terms of the the initial probability distribution for one state and the entropy rate.",
            "I'm now jumping off.",
            "Obviously this has this sort of inverted U relationship.",
            "But"
        ],
        [
            "At least the way I understand it, it doesn't because.",
            "It's.",
            "It's maximal for sequences where.",
            "Where the initial well essentially for Markov chains, its maximum when when uncertainty before we've seen anything is very great, whereas as soon as well as after we've seen the first thing would become completely sure about everything to the future.",
            "And so that's that, would we obtain that situation if the probability distribution for the initial state is uniform, but then the transition matrix is deterministic cycling, and that's what would maximize this particular information rate.",
            "So it doesn't have this kind of.",
            "If actually falls into the deterministic pattern, which we would.",
            "My sister, my sister degree is boring."
        ],
        [
            "So.",
            "A very brief, quickly skipped over few slides back, was this idea of as as we observe events, we get information about model parameters.",
            "That's essentially the same as it involves Bayesian surprise, and they talk about how.",
            "This is model for attention basically so.",
            "The salience of something is to do with how much it changes our our model parameters."
        ],
        [
            "OK."
        ],
        [
            "Awesome."
        ],
        [
            "OK, so we tried out this sum this Markov chain model on on a piece of minimalist music by Philip Glass, which we.",
            "We chose, we chose the glass.",
            "Well, actually quite deliberately, because if I go back to what?",
            "Landon Landon Mayer was saying about about our knowledge of style.",
            "He he focused on the interaction between extra and intra open stylistic norms, which is again this sort of background knowledge that we have.",
            "And then what we learn from the piece of music.",
            "Now because our model is very simple and really can't encode very much about music in general, we wanted to focus on a piece of music where it was really the entropa style that was more significant than the extra stuff and that kind of that's what leads to minimalist music because.",
            "Very often it doesn't rely so much on compositional conventions and styles, and really just builds up the structure as it's going along.",
            "And we also choose two for two pages because.",
            "'cause the notes happen at a steady rate and it means that we can just encode it as as the Markov chain without too many too many problems."
        ],
        [
            "So that's sort of 1 one subtlety that we had was that we wanted to be able to model the change in the transition matrix as you go along through the piece.",
            "Bottom of the different processes that composer puts us through, so this this is sort of a slightly more ad hoc element of the model, but we so we model the observer has a has a belief about the transition matrix, which we encode as a Jewish distribution over transition matrices over the columns.",
            "So it has these.",
            "Feature of the parameters of the Jewish way distribution, and then each at each time step.",
            "Before we get the new bit of data and change our beliefs, we also spread out the digital distribution a bit to model the idea that the transition matrix might have changed.",
            "It's sort of almost like a diffusion process.",
            "So that means as each observation arrives, it interacts with you.",
            "Sort of forget slightly what happened and then a new observation rising and you."
        ],
        [
            "The transition matrix.",
            "So from that we can computer and information in each observation about the transition matrix so.",
            "So we put a few of the few things that come out of the model, so the top one is the entropy of the next observation before it's been seen.",
            "The next one is how much information we have expect to receive from the from the observation.",
            "This is the surprising this, and this is the information which is information in that in that observation about the rest of the future.",
            "So we've got.",
            "We get this sort of.",
            "Very strong structure coming out.",
            "Very clear spikes that are are very closely related to the structure of the piece as annotated in the score as marked by by musicologists.",
            "These these thick black lines here or the sectional sections of the piece, and then I don't know if you can see these light Gray lines.",
            "Sort of in here and here.",
            "Yeah, they marked sort of essentially phrase boundaries.",
            "So we're looking in here.",
            "There's there's a.",
            "There's a section where there's a repeating phrase which gradually gets longer and longer and longer, and then it gets shorter and shorter and shorter.",
            "So all those boundaries have been marked out very clearly what's?",
            "What can I say is that?",
            "When you look at the information that we're gaining about the transition matrix, it comes out with this very spiky signal, very sparse signal.",
            "Each of these is is 1 exactly one event.",
            "This big spike, and each of these, and we compare those with.",
            "It's the six most surprising moments as picked picked out by Keith Potter, who is a music college music ologist, an expert on minimalist music.",
            "So we find good agreement between those those structural points and these points of model surprises, as it were even to the point that on the score this line here is marked as a section boundary.",
            "But most musicologists who look at it actually placed the boundary a bit later, where the speakers, which is where there is as well.",
            "OK."
        ],
        [
            "After"
        ],
        [
            "This very quickly we did the same thing with another piece of music by Philip Cost Glass called gratis, which is much less.",
            "Systematically structured, I didn't get to play 2 pages to let you find out how exactly how mind numbing it is, but gratis is also painful to listen to, but in a different way.",
            "But we're getting some structural similarity.",
            "We haven't got a we have done a musical analysis of the peace, our colleagues Markus Pierson Grant Wiggins have done, and they're using a similar approach to us.",
            "And so these sort of stuff here isn't just noise, it actually correspond.",
            "Well, not all of it somewhere.",
            "Responds to structure.",
            "Particular moments in music that have signal."
        ],
        [
            "Right, so the last thing is.",
            "Probably don't have time for now.",
            "Applying this stuff to to gesture recognition so so in a nutshell, the idea was to take these these.",
            "These are three accelerometer signals from a wee controller and we wanted to be able to sort of conduct with it and have the system detect the events corresponding to the beats and then send them to.",
            "It was in the context of an opera for disabled performance, so we had we had a blind singer who needed to be able to get the beat from the conductor.",
            "So that the guts of the model is basically to change this into a sequence of discrete states so that we can do the HMM stuff the Markov."
        ],
        [
            "Information dynamics.",
            "So when we train hmm on the data and we get these Markov chains where this fairly sparse structure.",
            "So basically that the state sort of wanders around in a typical ways and we find that some of these states are basically some of the states are not significant in the sense that.",
            "Someone in the middle is here.",
            "So when you're here, you already know you're going here and you know where you're going.",
            "But at some point there are junctions where something can happen one way or the other, and these are the significant points there in the sense of this is we need to pay attention and depending what happens, you decide whether it's a gesture or that kind of gesture.",
            "So the information I'm going to analysis when we look for moments of high information rate, or we look for symbols that carry a lot of information they pick out."
        ],
        [
            "Quite accurately, the these points, so here.",
            "So this.",
            "Basically this is the one to look at here, this is.",
            "This is the information in each symbol about the weather information and the color denotes the identity of the symbol in the state space, and the brightness denotes the amount of information.",
            "So it's completely specified down to one symbol at at a time to mark the event.",
            "OK, I think that is."
        ],
        [
            "It I will skip."
        ],
        [
            "Already."
        ],
        [
            "Albayzin all of that."
        ],
        [
            "Do we have yes."
        ],
        [
            "So yes, so one thing that we particularly look at is the idea of uncertainty about distribute about durations in times is something we want to go on.",
            "So basically, as you're waiting, if you have a, if you have an event and then you're waiting for the next one to happen and you have some sort of distribution about how long it's going to take.",
            "So something like that, even while nothing is happening, you're getting information because you know that the duration is greater than the amount of time you waited, so I think that's something interesting to look at and too late."
        ],
        [
            "Doctor imperception"
        ],
        [
            "Wow.",
            "That's it.",
            "Natural.",
            "Structure.",
            "Yes yes yes.",
            "Yeah.",
            "Yes.",
            "Yeah, I mean, it basically comes down to the model and as we're saying, this is a kind of a model agnostic thing, so you still have to still have to come up with a good model for for long term dependent processes, so that's still open, yeah?",
            "So.",
            "Coding.",
            "Yes, yeah.",
            "Yeah, well we're looking at ways of relating this general response, so we've working with someone at Goldsmiths College here.",
            "I mean, even though the first we had today about the Iran and five responses to expectancy violation, I mean I think these are.",
            "These are very relevant.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to be talking about actually fairly fairly simple application of information theory too, in a setting of a dynamic probabilistic models with intention of applying them to music.",
                    "label": 0
                },
                {
                    "sent": "Until we're calling this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information dynamics for short.",
                    "label": 0
                },
                {
                    "sent": "So here's a brief outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm going to be talking about a little bit about the sort of motivation which comes from various ideas that have come up over the ages to do with.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Music elicits the kind of response is it?",
                    "label": 0
                },
                {
                    "sent": "Does the emotional, emotional responses, and just the general subjective experience of listening to music.",
                    "label": 0
                },
                {
                    "sent": "And then I'll introduce you introduce, well, just talk about.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the the kind of the sort of the basic framework that will be working in, which is the idea of an observer who has a probabilistic model of what the music is doing.",
                    "label": 0
                },
                {
                    "sent": "And then to illustrate those ideas, I'll just talk about possibly one of the simplest models that we could think about using in this situation Markov chain.",
                    "label": 0
                },
                {
                    "sent": "And then at that point it slightly old place to have related work, but it's it's, I think by that stage will have introduced their ideas to be able to compare meaningful meaningfully with what's gone before, and then I'll be talking about a couple of applications.",
                    "label": 0
                },
                {
                    "sent": "One is experiment where we apply this sort of Markov chain information dynamics idea to a couple of pieces of minimalist music to see if we can get any kind of sensible analysis of the music out of them.",
                    "label": 0
                },
                {
                    "sent": "And the second application is actually a gesture recognition application, which I haven't written in the.",
                    "label": 0
                },
                {
                    "sent": "In on there, but it's it's just a recognition.",
                    "label": 0
                },
                {
                    "sent": "Isn't information dynamics in a hidden Markov model this time, and then I'll round off with some future work summary conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So part one.",
                    "label": 0
                },
                {
                    "sent": "So it's often been observed that.",
                    "label": 0
                },
                {
                    "sent": "One of the one of the salient features of music listening to music is that it creates expectations.",
                    "label": 0
                },
                {
                    "sent": "People talk about implication and expectancy and uncertainty and tension in these sort of things, and that's been talked about in the last century, notably by Leonard Leonard B. Meyer and Eugene Narmour and so on.",
                    "label": 0
                },
                {
                    "sent": "But it's been it's been discussed for a lot a lot earlier than that.",
                    "label": 0
                },
                {
                    "sent": "I quite like this.",
                    "label": 0
                },
                {
                    "sent": "The statement by Edward Handzlik, who put it quite nicely.",
                    "label": 0
                },
                {
                    "sent": "I think the most important factor in the mental process which accompanies the active listening to music and which converts it to a source of pleasure is the intellectual satisfaction, which listening to rise from continually following and anticipating the composers intentions.",
                    "label": 1
                },
                {
                    "sent": "Now to see his expectations fulfilled and now to find themselves agreeably mistaken.",
                    "label": 1
                },
                {
                    "sent": "It is a matter of course, of this intellectual flux and reflux.",
                    "label": 1
                },
                {
                    "sent": "This perpetual giving and receiving takes place unconsciously and with the rapidity of lightning flashes, something that's a fairly nice statement of what we do.",
                    "label": 0
                },
                {
                    "sent": "Which we would like to sort of get a grip on in a theoretical sense.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so I'm going to do a little little experiment on you to just illustrate the healthier the idea of unfolding message so.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you can probably guess what's going to happen, but at this stage is obviously an incomplete sentence, and so you may be wondering what what's going to come next.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anaza as the words appear you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "May have guessed what the word is going to be.",
                    "label": 0
                },
                {
                    "sent": "Or you might know.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might be surprised by it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one you might have guessed at that point at the next word.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be time so not not too surprised.",
                    "label": 0
                },
                {
                    "sent": "Now there's a comma.",
                    "label": 0
                },
                {
                    "sent": "But at this point you're probably not sure about what's going to come next, because the comma is sort of a structural break, and it means that there's going to be greater uncertainty about the next.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it just, well, I'm just going to finish up in in one day.",
                    "label": 0
                },
                {
                    "sent": "I'm fairly sure nobody gets that.",
                    "label": 0
                },
                {
                    "sent": "So that really illustrates So what we want to focus on is this idea of a phenomenon phenomenon that unfolds in time, and we want to focus on how the listener sort of forms and modifies their expectations as things happen.",
                    "label": 1
                },
                {
                    "sent": "And we want to try and do this in a in a quantitative way.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, anyway, that's just what I've just said, really.",
                    "label": 0
                },
                {
                    "sent": "I mean, my sort of more or less made the same the same arguments.",
                    "label": 0
                },
                {
                    "sent": "Now my work in the 50s came shortly after.",
                    "label": 0
                },
                {
                    "sent": "Publication Shannon's information theory which really.",
                    "label": 0
                },
                {
                    "sent": "Gave us the the sort of mathematical tools that seem sort of appropriate for the job here that we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "And so since then, there's been various various parts of work on applying information theory to musical phenomena and not just music but also to aesthetics in general, and trying to see if there's any relationship between.",
                    "label": 0
                },
                {
                    "sent": "The primary information theoretic concepts, and notably entropy people focused on entropy, restaurant entropy and aesthetic value, or just kind of judgments of pleasing this that people make and these experiments.",
                    "label": 0
                },
                {
                    "sent": "Within the context, both of of sequences, sequences of sounds, musical sounds or pitches, and also geometric shapes and the experiments.",
                    "label": 0
                },
                {
                    "sent": "Let's just see what?",
                    "label": 0
                },
                {
                    "sent": "Yes, let's get back the experiments generally focused on presenting stimuli of what was intended to be varying levels of uncertainty and then getting people to rate them somehow, often on a verbal scale.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking of the experiments of the line in the 70s.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But really, what was?",
                    "label": 0
                },
                {
                    "sent": "Well, indeed they did discover a certain relationship between sort of complexity of the stimulus, the entropy uncertainty, and how well people rated it and.",
                    "label": 0
                },
                {
                    "sent": "Uh, and.",
                    "label": 0
                },
                {
                    "sent": "Quite often they found this relationship where something was very, very predictable or very very entry was low.",
                    "label": 0
                },
                {
                    "sent": "For example, people would rate it sort of fairly low, not particularly interesting, and then as the complexity of the stimulus increase and people would sort of like it, but the ratings would increase up to a certain point, and then they would drop off again as things started to get a bit bit complex.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I've probably derailed myself from the slides now, but.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll just carry on from.",
                    "label": 0
                },
                {
                    "sent": "From this pic here.",
                    "label": 0
                },
                {
                    "sent": "We are we want to take the viewpoint of an observer who who is actively making predictions and formulating degrees of belief about what what might be happening and their reasoning under under under conditions of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "And also you don't know what we don't know exactly what's going to happen next.",
                    "label": 0
                },
                {
                    "sent": "And there are some very fairly sort of general arguments, so there's of Cox and James and so on.",
                    "label": 1
                },
                {
                    "sent": "Who are people who argue that really the best way to reason under uncertainties to use probability theory?",
                    "label": 1
                },
                {
                    "sent": "Because it's the only consistent way to do it.",
                    "label": 0
                },
                {
                    "sent": "So we kind of take that on board and and really not worry about any other ways of dealing with uncertainty.",
                    "label": 0
                },
                {
                    "sent": "There are others.",
                    "label": 0
                },
                {
                    "sent": "But sort of taking that assumption, so we're going to take the view.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a probabilistic observer.",
                    "label": 0
                },
                {
                    "sent": "And we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that they their expectations are probabilistic, probabilistic in nature as letter, Mayer also suggested.",
                    "label": 0
                },
                {
                    "sent": "So your knowledge of a musical style, for example, is statistical in nature in terms of what things tend to follow other things.",
                    "label": 0
                },
                {
                    "sent": "What notes tend to follow each other?",
                    "label": 0
                },
                {
                    "sent": "What cause and so on?",
                    "label": 0
                },
                {
                    "sent": "And these models are.",
                    "label": 0
                },
                {
                    "sent": "Are built up over overall sort of a whole lifetime of listening to different styles of music.",
                    "label": 1
                },
                {
                    "sent": "So when you come to a new piece of music, you already have this sort of sort of baggage that you know you have some expectations about what's going to happen given given at the start of the peaceful Europe, which might depend on the context of what you already know about it.",
                    "label": 0
                },
                {
                    "sent": "But importantly, as well as the as well as your your knowledge of style in general and that you've gleaned from many listening experiences.",
                    "label": 0
                },
                {
                    "sent": "There's also a sense in which, as the piece opens.",
                    "label": 1
                },
                {
                    "sent": "You immediately start to build up an idea of what patterns and what what themes the composer is is using, and this is something that we can do very quickly.",
                    "label": 0
                },
                {
                    "sent": "As soon as there's an opening.",
                    "label": 0
                },
                {
                    "sent": "As soon as a dead are, then the other day that I was sort of follow on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so, so we so we'd really like to balance this idea of learning learning over sort of background learning, and also on the on the job learning, which happens very quickly as soon as the piece starts.",
                    "label": 0
                },
                {
                    "sent": "So in that case we we now have a several probabilistic model which we want to apply, and as soon as the first first first three events come in, then you immediately want to start revising your beliefs about what's going to happen next.",
                    "label": 0
                },
                {
                    "sent": "So in that case.",
                    "label": 0
                },
                {
                    "sent": "At the beginning, you've got this sort of the piece unfolding in front sort of opening up in front of you.",
                    "label": 0
                },
                {
                    "sent": "You might have if you just imagine some sort of vague distribution over what's going to happen in the future.",
                    "label": 0
                },
                {
                    "sent": "Every time a new event occurs, also something happens.",
                    "label": 0
                },
                {
                    "sent": "You can modify those distributions.",
                    "label": 0
                },
                {
                    "sent": "You can change.",
                    "label": 0
                },
                {
                    "sent": "You can revise your beliefs about what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "And so this is where the information information theoretic ideas come in.",
                    "label": 0
                },
                {
                    "sent": "Because when we have distributions we can compute compute entropies and quantify the concept of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "And we can also.",
                    "label": 0
                },
                {
                    "sent": "Quantify what happens when a distribution changes when we revise our beliefs over distribution changes, we can measure the we can measure essentially the distance between the two distributions and get a get a feeling for how much your beliefs are changed and.",
                    "label": 0
                },
                {
                    "sent": "In a quite a quite a meaningful sense, this gives you a measure of how much information you've received about things that you haven't observed yet.",
                    "label": 0
                },
                {
                    "sent": "So so an event happens, something happens and you change your distributions.",
                    "label": 0
                },
                {
                    "sent": "The distance between those two quantify the amount of information that was in that particular observation, and so as the events that have happened one after the other, each one carries a certain amount of information, and so we can quantify that using.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The tools of information theory.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sing us, just move on from that.",
                    "label": 0
                },
                {
                    "sent": "So the interesting question here is that we have a set of quantitative tools which.",
                    "label": 1
                },
                {
                    "sent": "Which we have to we have the mass for and we have on the.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, we have a set of subjective phenomena and relating to the way people describe musical experience or things like tension and surprise and.",
                    "label": 1
                },
                {
                    "sent": "A closure and and and that sort of thing, so we'd like to see if there's a if we can really make a relationship between the quantitative quantitative theory that we have and the subjective ideas.",
                    "label": 0
                },
                {
                    "sent": "And in that way, trying to build a build a bridge between the quantitative side and the experiential aspects.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, right after I've managed to intersect with my slides once again.",
                    "label": 0
                },
                {
                    "sent": "So bear line in the 70s talked about these these information theoretic quantities, entropy and so on and and others and called them qualitative variables.",
                    "label": 0
                },
                {
                    "sent": "He was trying to.",
                    "label": 0
                },
                {
                    "sent": "He was trying to evoke the idea that there not to do with the surface the musical surface.",
                    "label": 0
                },
                {
                    "sent": "The details of the musical surface not to do with what events happen.",
                    "label": 0
                },
                {
                    "sent": "It's not even to do with the mode.",
                    "label": 0
                },
                {
                    "sent": "It's not even today with it being musical or video or similar.",
                    "label": 0
                },
                {
                    "sent": "It's just to do with patterns and patterns of probability and expectation.",
                    "label": 1
                },
                {
                    "sent": "And so he coined this term information aesthetics too.",
                    "label": 0
                },
                {
                    "sent": "To describe the study of how these quality variables relate to this subjective experience.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this slide is basically a summary of where we've got two.",
                    "label": 0
                },
                {
                    "sent": "Now that's sort of our our set of core assumptions, or at least our hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So we are probabilistic observer.",
                    "label": 0
                },
                {
                    "sent": "As we listen, we maintain a model of what's what's coming.",
                    "label": 1
                },
                {
                    "sent": "And as events unfold, we we change our prediction.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can count.",
                    "label": 0
                },
                {
                    "sent": "We can characterize these.",
                    "label": 0
                },
                {
                    "sent": "This sort of belief state in terms of entropies, for uncertainties about various things that might be happening.",
                    "label": 1
                },
                {
                    "sent": "Different variables.",
                    "label": 1
                },
                {
                    "sent": "And also we can quantify the amount of information we receive from each observation about things of interest that might happen in the future.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because all of this is now happening in a dynamic sense, we can we can.",
                    "label": 0
                },
                {
                    "sent": "We can put all these quantities quantities against time now so we can plot uncertainty, entropy against time and see if there are.",
                    "label": 0
                },
                {
                    "sent": "You know if there are moments when uncertainties particularly or particularly at the same time, we can plot information and we can complete surprise and so on, and so in.",
                    "label": 0
                },
                {
                    "sent": "That way we get a small.",
                    "label": 0
                },
                {
                    "sent": "We get this kind of low dimensional representation, a few signals, three or four that are moving through time in a certain way.",
                    "label": 0
                },
                {
                    "sent": "And they can sort of make almost gestures.",
                    "label": 0
                },
                {
                    "sent": "They can do sort of move around in various ways.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to find out is if.",
                    "label": 0
                },
                {
                    "sent": "If I if if this somehow captures an essence of what the listening experience is, so really would like to be able to just kind of throw away all of the details of what's happening on the musical surface and and even the internals of the model variables inside the model and just focus on this.",
                    "label": 0
                },
                {
                    "sent": "This dynamic?",
                    "label": 0
                },
                {
                    "sent": "These shapes, that they're that have been generated by the model.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the?",
                    "label": 0
                },
                {
                    "sent": "What are they sort of desirable features of this approach?",
                    "label": 1
                },
                {
                    "sent": "One is, we've abstracted away all the medium specific details.",
                    "label": 0
                },
                {
                    "sent": "And focused really just on.",
                    "label": 0
                },
                {
                    "sent": "Probabilities and entries and things that are sort of a model in that sense.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of appeals to a sense of being able to compare different types of experience.",
                    "label": 0
                },
                {
                    "sent": "For example, you could look at information in music, but you could also equally look at it in cinema or in Dancer, in drama.",
                    "label": 0
                },
                {
                    "sent": "And because we were looking at because we've abstracted sort of this kind of abstract level, we can conceive conceivably think about comparing different types of experience, different types of temporal experiences that happen in different media.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've been talking about this.",
                    "label": 0
                },
                {
                    "sent": "This model based observer, but we haven't actually said what what the model is, and that's open, so I'm not actually.",
                    "label": 0
                },
                {
                    "sent": "This talk is not about a particular model.",
                    "label": 0
                },
                {
                    "sent": "Not advocating any particular way of analyzing pieces.",
                    "label": 0
                },
                {
                    "sent": "You know, analyzing musical data, but really, just how you interpret what the model is doing and how you.",
                    "label": 0
                },
                {
                    "sent": "How we might be able to sort of?",
                    "label": 0
                },
                {
                    "sent": "Cleaned something about the structure with independently of what the model is.",
                    "label": 0
                },
                {
                    "sent": "I mean, obviously there are still questions of some models might fit much better than others, and it would make sense to use those rather rather than rather than simple models that don't fit.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, sort of.",
                    "label": 0
                },
                {
                    "sent": "Coming back to the to the modeling issue as we start to use more more complex models with with hidden variables and sort of laser processes.",
                    "label": 0
                },
                {
                    "sent": "We we sort of automatically get a added richness to the information dynamic description because say for example in hidden Markov model we've got this sort of layer of hidden states which is which is going along as well as the layer of observed features and so we can talk about as events happen.",
                    "label": 0
                },
                {
                    "sent": "We can we can think about whether or not we've gained information about the observations that we're about to receive.",
                    "label": 0
                },
                {
                    "sent": "But we can also think about whether or not we've received information about the layer of hidden variables or even the parameters of hidden Markov model if you decide it there.",
                    "label": 0
                },
                {
                    "sent": "Changing so as you make the model richer, you get a richer information dynamic.",
                    "label": 0
                },
                {
                    "sent": "Sort of reduction of what it's doing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we've taken the taking the viewpoint of the observer, so we're talking about sort of subjective probabilities in the sense very much in the sense of DEFINITY.",
                    "label": 0
                },
                {
                    "sent": "That that school of thought so.",
                    "label": 0
                },
                {
                    "sent": "Although, although we're very unlikely to be able to explain a particular any particular person's response, the the model itself, the idea itself is very much subjective.",
                    "label": 0
                },
                {
                    "sent": "One, because it depends on the experience and that the the model you're actually using has had and it depends on what it's been exposed to before and what kind of training is having all sorts of all sorts of things like that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think.",
                    "label": 0
                },
                {
                    "sent": "Alright, and then OK, just so.",
                    "label": 0
                },
                {
                    "sent": "So just to finish off this this background section this is.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit off a little bit.",
                    "label": 1
                },
                {
                    "sent": "Off topic but.",
                    "label": 0
                },
                {
                    "sent": "There is some.",
                    "label": 0
                },
                {
                    "sent": "There is potentially a nice correspondence between the idea of information dynamics and also what various philosophers have written about.",
                    "label": 1
                },
                {
                    "sent": "About music and about ascetic experiences, and they often talk about.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about this idea of abstracted form or dynamic form or shape.",
                    "label": 0
                },
                {
                    "sent": "Davis reviews this review.",
                    "label": 0
                },
                {
                    "sent": "Several contributions that he calls his groups under the heading contour theories and it's this idea that the way that music.",
                    "label": 1
                },
                {
                    "sent": "Evokes responses or recalls or suggest things is to do because it is to do with just the dynamic.",
                    "label": 0
                },
                {
                    "sent": "The temporal patterns that it creates.",
                    "label": 0
                },
                {
                    "sent": "And it can because it can create those in any using any any surface feature.",
                    "label": 0
                },
                {
                    "sent": "It's really.",
                    "label": 0
                },
                {
                    "sent": "It's really the form itself that matters.",
                    "label": 0
                },
                {
                    "sent": "And so when he talks about a contour, he's talking about this contour in a completely abstracted space.",
                    "label": 1
                },
                {
                    "sent": "I'm sort of in a similar vein.",
                    "label": 0
                },
                {
                    "sent": "We've got Suzanne Langer talking about morphology of different.",
                    "label": 0
                },
                {
                    "sent": "Have feelings and again focusing on the idea that certain temporal patterns might evoke different different emotions and the same again with stones, vitality, effects.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So it's it's appealing to think that the information description might some level connect with this words.",
                    "label": 0
                },
                {
                    "sent": "Obviously not going to answer this talk.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just a little bit of their background mathematics system.",
                    "label": 0
                },
                {
                    "sent": "Nothing too, nothing too strenuous, but.",
                    "label": 0
                },
                {
                    "sent": "So we've got.",
                    "label": 0
                },
                {
                    "sent": "First of all, we have the entropy as a measure of the uncertainty in a distribution.",
                    "label": 1
                },
                {
                    "sent": "Its measure of the spread.",
                    "label": 0
                },
                {
                    "sent": "And also you can think of it as.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "If if the log of the probability or sorry for minus the log of the probability of something is can be considered a sense of how surprising it is.",
                    "label": 1
                },
                {
                    "sent": "So if it's low probability, it's more surprising than the entropy is the expectation of the surprising this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I skipped one anyway.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the sort of the second useful idea is that of the callback library.",
                    "label": 0
                },
                {
                    "sent": "Divergent switch measures is a measure of distance between probability distributions.",
                    "label": 1
                },
                {
                    "sent": "The way I've written it varies is sort of to foreshadow what we're going to do a bit later, and I've written here.",
                    "label": 1
                },
                {
                    "sent": "This is the callback libel divergent between this distribution PX given D&PX&PX.",
                    "label": 0
                },
                {
                    "sent": "Given these meant to imply or distribution over over X given some data D some evidence and PX on its own, is the distribution that before you receive the data, so this distance?",
                    "label": 0
                },
                {
                    "sent": "And after that is expressed like that.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that that we can use this this distance to quantify the amount of information that was indeed about X because we've received and it's changed our distribution over X.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we've got the mutual information itself, which is.",
                    "label": 0
                },
                {
                    "sent": "A measure of dependence between between random variables and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will say that.",
                    "label": 0
                },
                {
                    "sent": "If I go back to this thing.",
                    "label": 0
                },
                {
                    "sent": "If if if the itself, so I'm putting the screen.",
                    "label": 0
                },
                {
                    "sent": "If the itself is is yet to be observed, but you have some district you know have some distribution for D, then the then you could work out what the expectation of this information is.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this covered.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that is the mutual information.",
                    "label": 1
                },
                {
                    "sent": "They are the most sort of useful expression for mutual information for our purposes.",
                    "label": 0
                },
                {
                    "sent": "Is this so?",
                    "label": 1
                },
                {
                    "sent": "This is saying that the mutual information between X&Y is our uncertainty about X.",
                    "label": 1
                },
                {
                    "sent": "Before we've seen I before we seen Y minus uncertainty about X after we've seen why.",
                    "label": 0
                },
                {
                    "sent": "So it's this again, it captures this idea of change in beliefs.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "We just sort of take the.",
                    "label": 0
                },
                {
                    "sent": "Take the simplest possible first step, which is to consider.",
                    "label": 0
                },
                {
                    "sent": "Stimulus, which consists which consists of a sequence of, let's say, discrete symbols.",
                    "label": 1
                },
                {
                    "sent": "And at any particular time, we've observed a number of these things.",
                    "label": 1
                },
                {
                    "sent": "And one is about to arrive, and we've got some in the future that we don't know about.",
                    "label": 0
                },
                {
                    "sent": "So we can sort of summarize the state of our observer using a few probability distributions over well over S for the present, and also why the future.",
                    "label": 0
                },
                {
                    "sent": "I've kind of used to denote the past X the present and why the future.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of.",
                    "label": 0
                },
                {
                    "sent": "Potentially troublesome Venn diagram of information theory, but it will serve our purposes.",
                    "label": 0
                },
                {
                    "sent": "The reason is potentially troublesome is that it's possible for this error in the middle to be negative.",
                    "label": 0
                },
                {
                    "sent": "We won't worry about that.",
                    "label": 0
                },
                {
                    "sent": "Just for visualization purposes.",
                    "label": 0
                },
                {
                    "sent": "So if we imagine the.",
                    "label": 0
                },
                {
                    "sent": "How can I point?",
                    "label": 0
                },
                {
                    "sent": "Maybe I can go over here.",
                    "label": 0
                },
                {
                    "sent": "Right, so this represents.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Let's see now this imagine this represents what we already know.",
                    "label": 0
                },
                {
                    "sent": "HAZ and this area here.",
                    "label": 0
                },
                {
                    "sent": "Represents its entropy of X given Z, so that's represents our uncertainty about about the next thing which is going to happen given what we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "And we also have some uncertainty about why as well, which is this area here, which is the future after X?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And one of the things that we were going to focus on a bit later on is is this error in the middle here.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "It means?",
                    "label": 0
                },
                {
                    "sent": "It's the information in X about Y.",
                    "label": 0
                },
                {
                    "sent": "Given that we already know zed.",
                    "label": 0
                },
                {
                    "sent": "And it is that area, so there's significant that will come up with a bit later on.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so in this framework we can now formulate a few quantities which are.",
                    "label": 0
                },
                {
                    "sent": "Which we think are interesting.",
                    "label": 0
                },
                {
                    "sent": "So one is the surprising Ness of.",
                    "label": 0
                },
                {
                    "sent": "That the current observation X, given what we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "So we're going to quantify that as the probability of the probability of X given the past.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the moment by moment surprising this.",
                    "label": 0
                },
                {
                    "sent": "So next thing is are expected.",
                    "label": 0
                },
                {
                    "sent": "Surprising this for the next symbol, so this is now before we see X we've got said we can we have an expectation for the surprising list of X and that is the entropy of X given the current context.",
                    "label": 1
                },
                {
                    "sent": "So that was also, we've got that by averaging over things.",
                    "label": 0
                },
                {
                    "sent": "Sorry, averaging over what's about to happen.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But on the other hand, we could also.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "A little bit to think about, but it's it's saying.",
                    "label": 0
                },
                {
                    "sent": "Given the next thing that happens, exit some.",
                    "label": 0
                },
                {
                    "sent": "Let it be some symbol.",
                    "label": 0
                },
                {
                    "sent": "It will be.",
                    "label": 0
                },
                {
                    "sent": "Surprising to some degree, depending on what just happened previously.",
                    "label": 0
                },
                {
                    "sent": "If we now average over all the things that could have led to X.",
                    "label": 0
                },
                {
                    "sent": "We can get a kind of an average surprising this for that particular symbol.",
                    "label": 0
                },
                {
                    "sent": "As it occurs, sort of, imagine in in the in the collection collected fantasies of the model for example.",
                    "label": 1
                },
                {
                    "sent": "And the reason why that might be interesting is that if a particular member of this particular symbol always tends to be surprising, then.",
                    "label": 0
                },
                {
                    "sent": "Then it might be significant in some sense, whereas if another another symbol always tends to be sort of not surprising, then it's perhaps it should sort of blend into the background.",
                    "label": 0
                },
                {
                    "sent": "It's not very significant, it doesn't really change your beliefs in any way, and it's you know it's never what you.",
                    "label": 0
                },
                {
                    "sent": "Don't expect.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right and the last thing is to average over all things that might possibly happen, and that's that's just the entropy rate of the process.",
                    "label": 0
                },
                {
                    "sent": "Which is a reasonable measure of of the randomness of the process, basically.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so the next thing is.",
                    "label": 0
                },
                {
                    "sent": "Is the idea of predicting information which goes back to this intersection on the on the Venn diagram that I pointed out earlier?",
                    "label": 0
                },
                {
                    "sent": "So the definition we're using here.",
                    "label": 0
                },
                {
                    "sent": "Is is this thing?",
                    "label": 0
                },
                {
                    "sent": "So as I said before, it's the information in X about the observed future.",
                    "label": 0
                },
                {
                    "sent": "Given that we've observed everything in the past up until now.",
                    "label": 1
                },
                {
                    "sent": "And it's.",
                    "label": 0
                },
                {
                    "sent": "It's this callback lively divergance between the two distributions before and after you observe X, so that's before we observe X and that's after you observe X.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the same way that we did for the surprising this week and now average, the predictive information in different ways to get sort of difference of global statistics about the symbols in the state space and also the process in general.",
                    "label": 0
                },
                {
                    "sent": "So we've got this, this this instantaneous predictive information you like if you like.",
                    "label": 0
                },
                {
                    "sent": "So it depends on.",
                    "label": 0
                },
                {
                    "sent": "It's a function of two variables, one of which is the current symbol and the other is the context.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next thing is we could average over what might be about to happen, so that gives us a measure of how much information we expect to receive receive from the next symbol.",
                    "label": 0
                },
                {
                    "sent": "And, well, that's some speculation about relationship between that possible, possibly inattention.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing we can do is average over what might have led to a particular symbol.",
                    "label": 0
                },
                {
                    "sent": "And that that is a measure of essentially how informative that particular sample tends to be in all the context that it occurs.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally we can average over everything and that gives us the sort of average predictive information information rate of the process as a whole.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I and.",
                    "label": 0
                },
                {
                    "sent": "Another thing we could do well, this is sort of.",
                    "label": 0
                },
                {
                    "sent": "If you have a model which has some other some other parameters, so some hidden parameters that you're updating as you observe as you make observations, then you can also update your beliefs about those and consider that you've received information about the parameters from each observation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Select Anne so going back to this work by Ballineen Co workers in the 70s.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "They often found this sort of U shaped curve as relationship between some overall randomness and some sort of measure of value as reported by human subjects.",
                    "label": 1
                },
                {
                    "sent": "And this goes back to the wind curve, which wasn't defined in terms of entropies, but it's a similar sort of idea.",
                    "label": 0
                },
                {
                    "sent": "And so the observation is, as I said before, see if something is very.",
                    "label": 0
                },
                {
                    "sent": "Very terministic and predictable.",
                    "label": 0
                },
                {
                    "sent": "It's boring, but by the same token, if it's if it's 2 random then people find it.",
                    "label": 0
                },
                {
                    "sent": "Incoherent, unstructured.",
                    "label": 0
                },
                {
                    "sent": "This sort of things.",
                    "label": 0
                },
                {
                    "sent": "And there's a sort of sweet spot in the middle where where things are most pleasing.",
                    "label": 0
                },
                {
                    "sent": "And also, if if it's the first time you're exposed to something which might initially be 2, two might seem too random, so we come back to the subjectivity now and dependence on the observers model.",
                    "label": 0
                },
                {
                    "sent": "It might initially seem very random after repeated exposure.",
                    "label": 0
                },
                {
                    "sent": "It tends to move along the curve that way, so it's possible for something which is initially overwhelming to become quite pleasing later on.",
                    "label": 0
                },
                {
                    "sent": "So you might think of I don't know some.",
                    "label": 0
                },
                {
                    "sent": "Messana Lake, like Coltrane or something like that, something which really really shocks you at 1st and after awhile.",
                    "label": 0
                },
                {
                    "sent": "Kind of wonder what this was.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you take up a pop song, it might start off right at the top of the curve and be sort of quite nice.",
                    "label": 0
                },
                {
                    "sent": "But after after a few listens, you're sick of it, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It slips down the down the other side.",
                    "label": 0
                },
                {
                    "sent": "So people talk about this idea of having a balance between order and chaos and unity and diversity in various.",
                    "label": 1
                },
                {
                    "sent": "Various terms like that, but.",
                    "label": 0
                },
                {
                    "sent": "This there isn't really a.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really strong explanation for why there's an optimum in the middle.",
                    "label": 0
                },
                {
                    "sent": "I mean, if there's a balance, then what determines the weighting of the balance resulting?",
                    "label": 0
                },
                {
                    "sent": "Where does this come from?",
                    "label": 0
                },
                {
                    "sent": "This is where our idea of predictive information comes in.",
                    "label": 0
                },
                {
                    "sent": "Because it turns out that if you examine the predictive information rate of a process.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And how it varies with the overall random, the overall entropy rate of randomness of the process.",
                    "label": 0
                },
                {
                    "sent": "It also shows this U shaped behavior with an optimum in the middle.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, that moved.",
                    "label": 0
                },
                {
                    "sent": "So try to.",
                    "label": 0
                },
                {
                    "sent": "It will be difficult to visualize why that's the case, but I've tried to these slides.",
                    "label": 0
                },
                {
                    "sent": "So this these represent three different processes that might be happening, and the black, the black dots represent things that we've observed, and the red ones are the ones that we haven't observed.",
                    "label": 0
                },
                {
                    "sent": "And the density of the red.",
                    "label": 0
                },
                {
                    "sent": "Relates to our uncertainty about what it's what it's going to be.",
                    "label": 0
                },
                {
                    "sent": "So white means very uncertain and red means sure.",
                    "label": 0
                },
                {
                    "sent": "So if you have a process which is very predictable and you've observed the first step or something, then you already have a pretty good idea of how you know how it's going to carry on.",
                    "label": 0
                },
                {
                    "sent": "Whereas if it's very random in the sense that each successive observation is independent of the previous one and they run correlated, then even if you observe the first one, you have no idea about these later ones and then this is something in the middle which which is sort of somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "But if you watch what happens as you as we advance in time, then we graduate.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observe things one by one.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's going right.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you're seeing is that.",
                    "label": 0
                },
                {
                    "sent": "As we observe these things really nothing much has changed with these ones because we already knew what they're going to be.",
                    "label": 0
                },
                {
                    "sent": "So uncertainty about these things didn't change.",
                    "label": 0
                },
                {
                    "sent": "Similarly, uncertainty about these things didn't change either because we were had no idea before and we have no idea offered.",
                    "label": 0
                },
                {
                    "sent": "So the only case in which our beliefs are actually being revised as we're going along with this middle case.",
                    "label": 0
                },
                {
                    "sent": "It's really the only one where.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whether it comes with.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just leave.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that that really captures what's going on in the predictive information rate.",
                    "label": 1
                },
                {
                    "sent": "There's a point in the middle where the predictive information rate is highest.",
                    "label": 0
                },
                {
                    "sent": "And and this is something that there's really only arbitrary decision that we've made to get, that is to say that we were making one observation at a time.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of timescale.",
                    "label": 0
                },
                {
                    "sent": "But apart from that, there's no kind of arbitrariness about where the optimum should be depends on depends really on the model that the observer is using to make their predictions.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so that's a that's all in the sort of case of general general statistical model, but we're going to now look at the simple case of a Markov chain to see how this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still works out.",
                    "label": 0
                },
                {
                    "sent": "Right, I'm sure most of you know what Markov chain is, but this just to define our terms, we've got S to represent the successive States and the transition matrix is A and the probability for the initial state I've called.",
                    "label": 0
                },
                {
                    "sent": "Pie.",
                    "label": 0
                },
                {
                    "sent": "There an actually I've given it in a in a superscript there because we're going to restrict ourselves to Markov chains where the.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution for a single observation is is stationary, so we're talking about stationary Markov chains only.",
                    "label": 0
                },
                {
                    "sent": "So it means that the the distribution for the initial state is actually a function of the transition matrix under some some assumptions.",
                    "label": 0
                },
                {
                    "sent": "Constraints.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well because it's such a simple model of the concept of the quantities I talked about before.",
                    "label": 0
                },
                {
                    "sent": "Very easy to to compute in terms of.",
                    "label": 0
                },
                {
                    "sent": "The entropy rate of the Markov chain.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So well, there's no need to dwell on that too much.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we get some.",
                    "label": 0
                },
                {
                    "sent": "We get some some expressions which come out.",
                    "label": 0
                },
                {
                    "sent": "Look at the paper to find out where they come from, but well, the I guess they on this kind of vaguely interesting is this one here.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the predictive information rate is the difference between the entropy rate of the two step process and the entropy rate of the one step process.",
                    "label": 1
                },
                {
                    "sent": "So a squared is the transition matrix between.",
                    "label": 0
                },
                {
                    "sent": "One state and the next one sort of skip after you skip one.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So that means we can now easily generate lots of transition matrices and workout what they're predicting.",
                    "label": 0
                },
                {
                    "sent": "Information rate is and plot it, and also obviously entropy rate as well.",
                    "label": 0
                },
                {
                    "sent": "So we find that this is this is 5 by 5 transition matrices.",
                    "label": 0
                },
                {
                    "sent": "Drew sort of several couple thousand of them or something.",
                    "label": 0
                },
                {
                    "sent": "They are drawn from a from a Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "Each column of the transition matrix is drawn from a different distribution.",
                    "label": 0
                },
                {
                    "sent": "That's not so important.",
                    "label": 0
                },
                {
                    "sent": "At least covers the space, so we find that the transition matrices with the highest entropy rate are the ones where each successive observation is independent of the law.",
                    "label": 0
                },
                {
                    "sent": "So the transition matrix is essentially flat, and each column is the same, so there's no correlation between successive elements at the other end of the scale.",
                    "label": 0
                },
                {
                    "sent": "With the lowest entropy rate, these are essentially the deterministic Markov chains, where the transition matrix is very sparse and it just tends to follow deterministic pattern and then in the middle we've got all this stuff going on where we find that some of them have high product information right and some of them some of them don't, and these are.",
                    "label": 0
                },
                {
                    "sent": "Those are the draws that correspond to those points.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can now draw sequences from those Markov chains, see what they look like.",
                    "label": 0
                },
                {
                    "sent": "So the top one is is just the repetition of the same state, so it's obviously.",
                    "label": 0
                },
                {
                    "sent": "Predictable and boring, repetitive and everything else, and the bottom one is is the one with maximal entropy rate.",
                    "label": 0
                },
                {
                    "sent": "So every state is independent, the previous one, and then these two ones in the middle both have more or less the same entropy rate.",
                    "label": 0
                },
                {
                    "sent": "They're kind of in the middle of the range, but this one has a high predictive information and this one has low particular information.",
                    "label": 0
                },
                {
                    "sent": "And you probably get some sense of.",
                    "label": 0
                },
                {
                    "sent": "Of there being.",
                    "label": 0
                },
                {
                    "sent": "Well, pattern or structure in this one is difficult to know what word to use without prejudice, prejudicing the result.",
                    "label": 0
                },
                {
                    "sent": "What I can do at this stage is.",
                    "label": 0
                },
                {
                    "sent": "To sonifi these to let you think about them with your ears instead of with your eyes.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Actually what I will do first is.",
                    "label": 0
                },
                {
                    "sent": "To my eyes.",
                    "label": 0
                },
                {
                    "sent": "You said.",
                    "label": 0
                },
                {
                    "sent": "This is just the.",
                    "label": 0
                },
                {
                    "sent": "The the the distribute the scatterplot of.",
                    "label": 0
                },
                {
                    "sent": "Oh no, that's not the one.",
                    "label": 0
                },
                {
                    "sent": "That's the one I meant to get.",
                    "label": 0
                },
                {
                    "sent": "So this is the scatter plot again with entropy on one side and predict information right on the other.",
                    "label": 0
                },
                {
                    "sent": "And if you look at.",
                    "label": 0
                },
                {
                    "sent": "So if I click on these and the transition matrix should come up here.",
                    "label": 0
                },
                {
                    "sent": "Any luck?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "So they were.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the high entropy rate ones are up there.",
                    "label": 0
                },
                {
                    "sent": "High prediction prediction information is there and then.",
                    "label": 0
                },
                {
                    "sent": "Deterministic ones are down there.",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "OK anyway, so if I say.",
                    "label": 0
                },
                {
                    "sent": "Jester was gonna recover the.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, so that's pretty obviously the one where nothing is happening.",
                    "label": 0
                },
                {
                    "sent": "So I'll just stop that.",
                    "label": 0
                },
                {
                    "sent": "Now this one.",
                    "label": 0
                },
                {
                    "sent": "This one should be the one where there's no correlations at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just going to noodle along.",
                    "label": 0
                },
                {
                    "sent": "So my observation is that you're initially you're initially inclined to be charitable and find little melodic patterns, but after it goes on, you realize there's no pain whatsoever.",
                    "label": 0
                },
                {
                    "sent": "And there's there's nothing that recurs.",
                    "label": 0
                },
                {
                    "sent": "There's no recognizable things really, just just some carries on like that.",
                    "label": 0
                },
                {
                    "sent": "Right now this is the one with the intermediate entropy rate, but load particular information.",
                    "label": 0
                },
                {
                    "sent": "It's probably a picture of it somewhere.",
                    "label": 0
                },
                {
                    "sent": "That's it, yeah, so I'll stop that before you're going nuts.",
                    "label": 0
                },
                {
                    "sent": "Right and then the last one is.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is this is this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not Mozart anything but.",
                    "label": 0
                },
                {
                    "sent": "But it does have sort of things that recur that you start to recognize.",
                    "label": 0
                },
                {
                    "sent": "And it sort of has this beginnings of structure and it seems to also have this sort of phases or it goes into one and then the other thing.",
                    "label": 0
                },
                {
                    "sent": "I'll stop there.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm gonna have to.",
                    "label": 0
                },
                {
                    "sent": "I'm in a pinch.",
                    "label": 0
                },
                {
                    "sent": "OK, right, so this so this product information, right?",
                    "label": 0
                },
                {
                    "sent": "So we well actually we haven't done experiments to test on human listeners.",
                    "label": 0
                },
                {
                    "sent": "Whether they find these less painful than the other ones, but that's something we're planning to do at the moment.",
                    "label": 0
                },
                {
                    "sent": "One thing you can do is actually optimize the transition matrix numerically to find the ones that maximize the product information, right and?",
                    "label": 0
                },
                {
                    "sent": "Those are the 1st.",
                    "label": 0
                },
                {
                    "sent": "The first 4.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well I will move on 'cause it's not.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much time left.",
                    "label": 0
                },
                {
                    "sent": "To work.",
                    "label": 0
                },
                {
                    "sent": "Some of the most relevant concepts in literature from before we've got you like and Tish peas predictive information.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that you take a long process and you break it into two adjacent blocks and you measure the mutual information between one and the other, and then you let one of them just go off to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And that gives you so that tells you information in one chunk about the rest of the whole thing in One Direction.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So this is also a very interesting quantity because it seems to capture.",
                    "label": 0
                },
                {
                    "sent": "Something intrinsic about the complexity of the process and or sort of the intrinsic demand dimensionality, which parameter space, depending on how it behaves as this.",
                    "label": 0
                },
                {
                    "sent": "The second block goes off to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Time.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But but it's a global measure in the same way as the average product information, which is a global measure.",
                    "label": 0
                },
                {
                    "sent": "But it can't tell you about what's happening moment by moment which is.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we'd also like to do?",
                    "label": 0
                },
                {
                    "sent": "We've also got an information rate proposed by slow down of which is.",
                    "label": 0
                },
                {
                    "sent": "It's yes, it's the mutual information in the entire past about about the present.",
                    "label": 1
                },
                {
                    "sent": "If you like for the next current observation.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And for Markov chains it reduces to something in terms of the the initial probability distribution for one state and the entropy rate.",
                    "label": 1
                },
                {
                    "sent": "I'm now jumping off.",
                    "label": 0
                },
                {
                    "sent": "Obviously this has this sort of inverted U relationship.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least the way I understand it, it doesn't because.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "It's maximal for sequences where.",
                    "label": 0
                },
                {
                    "sent": "Where the initial well essentially for Markov chains, its maximum when when uncertainty before we've seen anything is very great, whereas as soon as well as after we've seen the first thing would become completely sure about everything to the future.",
                    "label": 0
                },
                {
                    "sent": "And so that's that, would we obtain that situation if the probability distribution for the initial state is uniform, but then the transition matrix is deterministic cycling, and that's what would maximize this particular information rate.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't have this kind of.",
                    "label": 0
                },
                {
                    "sent": "If actually falls into the deterministic pattern, which we would.",
                    "label": 0
                },
                {
                    "sent": "My sister, my sister degree is boring.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A very brief, quickly skipped over few slides back, was this idea of as as we observe events, we get information about model parameters.",
                    "label": 0
                },
                {
                    "sent": "That's essentially the same as it involves Bayesian surprise, and they talk about how.",
                    "label": 0
                },
                {
                    "sent": "This is model for attention basically so.",
                    "label": 0
                },
                {
                    "sent": "The salience of something is to do with how much it changes our our model parameters.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Awesome.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we tried out this sum this Markov chain model on on a piece of minimalist music by Philip Glass, which we.",
                    "label": 0
                },
                {
                    "sent": "We chose, we chose the glass.",
                    "label": 0
                },
                {
                    "sent": "Well, actually quite deliberately, because if I go back to what?",
                    "label": 0
                },
                {
                    "sent": "Landon Landon Mayer was saying about about our knowledge of style.",
                    "label": 0
                },
                {
                    "sent": "He he focused on the interaction between extra and intra open stylistic norms, which is again this sort of background knowledge that we have.",
                    "label": 0
                },
                {
                    "sent": "And then what we learn from the piece of music.",
                    "label": 0
                },
                {
                    "sent": "Now because our model is very simple and really can't encode very much about music in general, we wanted to focus on a piece of music where it was really the entropa style that was more significant than the extra stuff and that kind of that's what leads to minimalist music because.",
                    "label": 0
                },
                {
                    "sent": "Very often it doesn't rely so much on compositional conventions and styles, and really just builds up the structure as it's going along.",
                    "label": 0
                },
                {
                    "sent": "And we also choose two for two pages because.",
                    "label": 0
                },
                {
                    "sent": "'cause the notes happen at a steady rate and it means that we can just encode it as as the Markov chain without too many too many problems.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of 1 one subtlety that we had was that we wanted to be able to model the change in the transition matrix as you go along through the piece.",
                    "label": 0
                },
                {
                    "sent": "Bottom of the different processes that composer puts us through, so this this is sort of a slightly more ad hoc element of the model, but we so we model the observer has a has a belief about the transition matrix, which we encode as a Jewish distribution over transition matrices over the columns.",
                    "label": 0
                },
                {
                    "sent": "So it has these.",
                    "label": 0
                },
                {
                    "sent": "Feature of the parameters of the Jewish way distribution, and then each at each time step.",
                    "label": 0
                },
                {
                    "sent": "Before we get the new bit of data and change our beliefs, we also spread out the digital distribution a bit to model the idea that the transition matrix might have changed.",
                    "label": 0
                },
                {
                    "sent": "It's sort of almost like a diffusion process.",
                    "label": 0
                },
                {
                    "sent": "So that means as each observation arrives, it interacts with you.",
                    "label": 0
                },
                {
                    "sent": "Sort of forget slightly what happened and then a new observation rising and you.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The transition matrix.",
                    "label": 0
                },
                {
                    "sent": "So from that we can computer and information in each observation about the transition matrix so.",
                    "label": 0
                },
                {
                    "sent": "So we put a few of the few things that come out of the model, so the top one is the entropy of the next observation before it's been seen.",
                    "label": 1
                },
                {
                    "sent": "The next one is how much information we have expect to receive from the from the observation.",
                    "label": 0
                },
                {
                    "sent": "This is the surprising this, and this is the information which is information in that in that observation about the rest of the future.",
                    "label": 0
                },
                {
                    "sent": "So we've got.",
                    "label": 1
                },
                {
                    "sent": "We get this sort of.",
                    "label": 0
                },
                {
                    "sent": "Very strong structure coming out.",
                    "label": 0
                },
                {
                    "sent": "Very clear spikes that are are very closely related to the structure of the piece as annotated in the score as marked by by musicologists.",
                    "label": 0
                },
                {
                    "sent": "These these thick black lines here or the sectional sections of the piece, and then I don't know if you can see these light Gray lines.",
                    "label": 0
                },
                {
                    "sent": "Sort of in here and here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they marked sort of essentially phrase boundaries.",
                    "label": 0
                },
                {
                    "sent": "So we're looking in here.",
                    "label": 0
                },
                {
                    "sent": "There's there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a section where there's a repeating phrase which gradually gets longer and longer and longer, and then it gets shorter and shorter and shorter.",
                    "label": 0
                },
                {
                    "sent": "So all those boundaries have been marked out very clearly what's?",
                    "label": 0
                },
                {
                    "sent": "What can I say is that?",
                    "label": 0
                },
                {
                    "sent": "When you look at the information that we're gaining about the transition matrix, it comes out with this very spiky signal, very sparse signal.",
                    "label": 0
                },
                {
                    "sent": "Each of these is is 1 exactly one event.",
                    "label": 0
                },
                {
                    "sent": "This big spike, and each of these, and we compare those with.",
                    "label": 1
                },
                {
                    "sent": "It's the six most surprising moments as picked picked out by Keith Potter, who is a music college music ologist, an expert on minimalist music.",
                    "label": 1
                },
                {
                    "sent": "So we find good agreement between those those structural points and these points of model surprises, as it were even to the point that on the score this line here is marked as a section boundary.",
                    "label": 0
                },
                {
                    "sent": "But most musicologists who look at it actually placed the boundary a bit later, where the speakers, which is where there is as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This very quickly we did the same thing with another piece of music by Philip Cost Glass called gratis, which is much less.",
                    "label": 0
                },
                {
                    "sent": "Systematically structured, I didn't get to play 2 pages to let you find out how exactly how mind numbing it is, but gratis is also painful to listen to, but in a different way.",
                    "label": 0
                },
                {
                    "sent": "But we're getting some structural similarity.",
                    "label": 0
                },
                {
                    "sent": "We haven't got a we have done a musical analysis of the peace, our colleagues Markus Pierson Grant Wiggins have done, and they're using a similar approach to us.",
                    "label": 0
                },
                {
                    "sent": "And so these sort of stuff here isn't just noise, it actually correspond.",
                    "label": 0
                },
                {
                    "sent": "Well, not all of it somewhere.",
                    "label": 0
                },
                {
                    "sent": "Responds to structure.",
                    "label": 0
                },
                {
                    "sent": "Particular moments in music that have signal.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so the last thing is.",
                    "label": 0
                },
                {
                    "sent": "Probably don't have time for now.",
                    "label": 0
                },
                {
                    "sent": "Applying this stuff to to gesture recognition so so in a nutshell, the idea was to take these these.",
                    "label": 0
                },
                {
                    "sent": "These are three accelerometer signals from a wee controller and we wanted to be able to sort of conduct with it and have the system detect the events corresponding to the beats and then send them to.",
                    "label": 0
                },
                {
                    "sent": "It was in the context of an opera for disabled performance, so we had we had a blind singer who needed to be able to get the beat from the conductor.",
                    "label": 0
                },
                {
                    "sent": "So that the guts of the model is basically to change this into a sequence of discrete states so that we can do the HMM stuff the Markov.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information dynamics.",
                    "label": 0
                },
                {
                    "sent": "So when we train hmm on the data and we get these Markov chains where this fairly sparse structure.",
                    "label": 0
                },
                {
                    "sent": "So basically that the state sort of wanders around in a typical ways and we find that some of these states are basically some of the states are not significant in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Someone in the middle is here.",
                    "label": 0
                },
                {
                    "sent": "So when you're here, you already know you're going here and you know where you're going.",
                    "label": 0
                },
                {
                    "sent": "But at some point there are junctions where something can happen one way or the other, and these are the significant points there in the sense of this is we need to pay attention and depending what happens, you decide whether it's a gesture or that kind of gesture.",
                    "label": 0
                },
                {
                    "sent": "So the information I'm going to analysis when we look for moments of high information rate, or we look for symbols that carry a lot of information they pick out.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite accurately, the these points, so here.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the one to look at here, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the information in each symbol about the weather information and the color denotes the identity of the symbol in the state space, and the brightness denotes the amount of information.",
                    "label": 0
                },
                {
                    "sent": "So it's completely specified down to one symbol at at a time to mark the event.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that is.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It I will skip.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Albayzin all of that.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we have yes.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yes, so one thing that we particularly look at is the idea of uncertainty about distribute about durations in times is something we want to go on.",
                    "label": 0
                },
                {
                    "sent": "So basically, as you're waiting, if you have a, if you have an event and then you're waiting for the next one to happen and you have some sort of distribution about how long it's going to take.",
                    "label": 0
                },
                {
                    "sent": "So something like that, even while nothing is happening, you're getting information because you know that the duration is greater than the amount of time you waited, so I think that's something interesting to look at and too late.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doctor imperception",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Natural.",
                    "label": 0
                },
                {
                    "sent": "Structure.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, it basically comes down to the model and as we're saying, this is a kind of a model agnostic thing, so you still have to still have to come up with a good model for for long term dependent processes, so that's still open, yeah?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Coding.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well we're looking at ways of relating this general response, so we've working with someone at Goldsmiths College here.",
                    "label": 0
                },
                {
                    "sent": "I mean, even though the first we had today about the Iran and five responses to expectancy violation, I mean I think these are.",
                    "label": 0
                },
                {
                    "sent": "These are very relevant.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}