{
    "id": "dt3gcb4uyzssfv2wkesw42ea3fc22gv7",
    "title": "Distributed, Real-Time Bayesian Learning in Online Service",
    "info": {
        "author": [
            "Ralf Herbrich, Amazon"
        ],
        "published": "March 27, 2014",
        "recorded": "November 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/acml2013_herbrich_real_time_bayesian_learning/",
    "segmentation": [
        [
            "Thank you, thank you very much.",
            "I try to speak loud so if it's if it's not.",
            "If you can't hear me then I have to lean over here.",
            "So tutorial I'm going to talk about today has two parts and it's it's a lot about my experience that I gathered in the last 1015 years working at these various companies that Scott was just describing.",
            "So in the."
        ],
        [
            "This part we talk about the theory and one of the one of the former frameworks that I got made very good experiences with his graphical models.",
            "So I'll start by introducing water.",
            "Graphical model is also talk about, then they kind of how to go from a problem to a graphical model and then go deeper into the inference inference.",
            "What is inference?",
            "So it's covered in it and how to do inference in graphical models, particularly how to deal with approximate inference.",
            "I'll introduce one of the algorithms that I find is one of the most versatile tools known as the sum product algorithm.",
            "And you'll see that it's a very simple insight.",
            "In the distributive law that makes this algorithm so powerful.",
            "But I like this.",
            "It makes it powerful because it goes from a model.",
            "So you you have your problem, you describe the forward flow of how the data arrives to an algorithm and also touch a little bit up on some more more recent subjects which is called distributed message passing.",
            "What what do you do and how do you do inference in graphical models that are for very large data of a very many parameter?",
            "Very large, this can be another tournament, but I basically mean is is that the amount of data or the amount of parameters is bigger than can be produced in a single computer?",
            "'cause when we think about a few is called machine learning but we we touch very little on machine on the machine aspects of it.",
            "You know when we think about an algorithm or or inference, we often assume some arbitrary as some sort of abstract computational model which is sequential, which has can execute an arbitrary number of steps and storing arbitrary number of states.",
            "That is not how computers work today.",
            "Whether embedded devices or devices in the cloud.",
            "So distributed message passing is an aspect of inference.",
            "For for computations that exceed the capacity of a single computational unit.",
            "Then we'll do a break.",
            "Depending on how we run the time and what I do.",
            "The second part is because the first time you feel it's probably little gets a little dry.",
            "In the end, I'll actually going to fill it with some color by talking about it.",
            "How does this framework apply to the problem of two skills?",
            "So how many people in the audience play Xbox?",
            "Ever in that only you under YouTube?",
            "Here's The funny thing I've been.",
            "You know, I've been talking about this subject for a few times for a few few years.",
            "It's always only one or two people who admit before the talk and about 1/4 of the audience after the talk.",
            "So OK, so I assume there's a few more, so I'll talk about the algorithm and the system that is used to estimate to match players on Xbox Live, the online online gaming network on Xbox.",
            "I'm also going to talk about how this kind of approaches can be used for modeling click through rates in online advertising, attach a little bit about why is click the right click through rate so important, whether it's on an advertising and Bing or in Facebook or in Yahoo and how some of these insights from inference in graphical models are actually going to be used in practice.",
            "And I have two.",
            "It's a lot of a lot of applications that could cover some will have to probably make a choice.",
            "We could either talk about or I could sort of talk about in more detail.",
            "About a recommender system, the recommender system that is now used in Xbox Live video store.",
            "So when you go to videos in the recommended videos you get there.",
            "I think it's actually going to launch fully life with Xbox One in two weeks or a system to learn to imitate experts.",
            "For example, in the Japanese Board game of Go or Chinese game of Wichi.",
            "So all these applications you'll see are applications of this calculus of message passing in graphical models.",
            "So before we start with."
        ],
        [
            "African models or before I start and go into the theory.",
            "One thing I want to say is even though I might talk for up to two hours, I'm really only going to scratch the surface.",
            "So if you want to know more, I recommend a couple of books for the details about it.",
            "One is by date Barber came out about a year ago.",
            "You know half ago.",
            "It's a very good book.",
            "Second one which is a little more broadly brought this by Chris Bishop on pattern recognition machine learning.",
            "The third one is the cost by by Andrew and cause errors actually across by both.",
            "Definitely color and drawing there good, and my personal favorite, which is probably closest to this tutorial is Kevin Murphy's book Machine Learning a probabilistic perspective that really fills in all the details that I might just gloss over a little bit today."
        ],
        [
            "So, graphical models graphical models basically denotes a way of specifying probable solutions, but why?",
            "Why could probably solutions would be so important?",
            "And when I started to work on machine learning, I was actually working more in the field of support vector machines and kernel machines in general.",
            "And the kind of thing that that made me think."
        ],
        [
            "Why probabilities can also be useful, not just for data you observe, but for for assertions you make in terms of parameters.",
            "Is that machine learning?",
            "We kind of want to design systems that uncertainty, so systems that have a calculus that can assign a degree of plausibility to statement a.",
            "So the statement being, for example Scotts Stronger enroll.",
            "So that's a statement we don't know its truth, but how would a system that needs to reason about this statement?",
            "What would we assume for plausibility calculus to be true?",
            "So or Scott skill level is 40 and it's a statement about the parameter we don't know.",
            "Now, if we assume three simple axioms and the first one is, I think very non debatable, which is plausibility is a real number.",
            "So regardless of which statement I ask, if I assume the possibilities are real number, I do make an assumption, but it doesn't feel like a strong line.",
            "The second one is regardless of how I asked the question, the plausibility should stay the same whether I asked whether I say the possible the skill of Scott is is between 35 and 40, or I say the skill is bigger than 35.",
            "And less than 40 or I say than in any other logical Boolean algebra way it shouldn't change, and the third axiom, the third X in that I'm going to postulate that I thought was also very non debatable as one of monotonicity.",
            "So if the plausibility of a statement a under observation C prime is strictly bigger than the plausibility under observation C. So we for example, we've learned one more game has happened and we learn something about Scott skill.",
            "And another observation, B is unaffected by that, so the possibility of B is the same whether it's A and C prime in ANC.",
            "Then the plausibility of both A&B cannot go down.",
            "So if I assume these three axioms on a plausibility measure for possibilities as system assigns to parameters, then there's only one calculus which is that of probability theory.",
            "That's actually the calculus that fulfills these axioms, and that's very powerful, and that was one of the sort of moments for me to consider probability calculus as a calculus.",
            "Also for around statements of the world, like parameters which includes parameters, the second thing I want to say is why probability is so important when you when you build real systems, is because.",
            "There is this.",
            "There's actually sort of a a close loop of inference decision making an inference prediction."
        ],
        [
            "And making so when I talk about inference in next 2 hours, what I really mean is the is the is the task where a system, a machine with computer or data center has a belief over parameters, parameters describe properties of the real world like skilled Scott level or the effect strength of being shown in the first position of an ad or the page being the home page on Facebook.",
            "And I get data and I have a model how data changes of parameters, then inferences that computational step that complete that translates the data and the probability over the parameters or belief into belief of the parameters given the data.",
            "And what will use one of the rules and one of the approximate rules, but this this computational task of basically changing the believing parameters?",
            "That's what I call inference and what it does is it requires the modeler to have a model of data, links to parameters, and it really allows to incorporate all prior information that a person has into a system over the belief of parameters.",
            "The second is prediction that means the system has a certain belief in parameters, receives new data.",
            "And needs to work in probability theory is called marginalized out, some out all the parameters to arrive at the prediction predictive distribution over data.",
            "What are all the possible data in the future?",
            "So if I for example, if Scott and I meet each other after we've got all the beliefs of Scott skills on my skill, that marginalization is assigning probability distribution to how likely Scott to win when we match play and how likely am I to win an and then the last step is decision making and decision making means that.",
            "We now have a need a loss function because it's the first time that the system acts.",
            "So in the case of the matchmaking, it means that the system actually decides to have Scott and me playing against each other.",
            "So that is an action at a system takes and that requires a loss function and requires because there is mistakes that we could do when we reason when the machine reason strongly or data center reason strongly about the truth and top error.",
            "That one is often forgotten 'cause the last function has a very close link to future data.",
            "If the system decides that Scott and I play and it gets one more training example, it can can learn about Scott's in my skills.",
            "If it doesn't do so, it can never learn about it, so there's an intricate link.",
            "It's really quite a cycle between decision making, inferences all the inferences we can make depend on the decisions we made in the past, and it's actually something very important.",
            "If you if you put a machine learning system in practice and there's some recent work on causality that's really capturing this stop error here.",
            "OK."
        ],
        [
            "Enough about why probability so important.",
            "So what is a graphical model in the simplest definition of it is just a graphical representation of a probability solution.",
            "Some of the part of the problem of the variables which are denoted by round around nodes are data.",
            "So that's what we observe and some of the variables are causes like things parameters were interested in.",
            "I say causes here in parentheses because it's I'm not talking about causal inference, just what causes the data to happen.",
            "In what is the model?",
            "It's the relation between the course and the data that's an edge in a graphical model, and then we have 1/3 type of variable, but just latent variables.",
            "I'll show you in a minute you kind of need to introduce them for the forward flow.",
            "So when you look at the real system or you look at a real process like how does it happen that I win when Scott and I play?",
            "We might need to introduce some latent variables, but we're really not interested in inference about their state.",
            "We need to get rid of them.",
            "So the key questions when graphical models got introduced in the 80s was actually the dependency.",
            "So you know when you look at the algebraic form of a joint, probability solution is very hard to answer the question.",
            "Is a variable A&B conditionally independent given the state of a variable C. So the whole statement in Algebra is here and you could prove that if you show that this factorization holds and what graphical models are useful for particular directed graphical models is answering these questions by pictures.",
            "So you see deliberately I kind of show a graph node there.",
            "So the initial way graphical also introduced is to be able to answer exactly these kind of questions.",
            "Is this variable set in that variable set independent of each other conditionally on that verbal set being observed, where this observed from would be data?",
            "The thing that we're going to use graphical models for the rest of the talk is actually for making the task of inference faster, and what is inference?",
            "Well, because we can apply appeal to the rules of probability.",
            "Inference is summing all the latent variables.",
            "So if we use the graphical model to describe all the data, all the variables we have.",
            "Causes latent and data.",
            "Then we always kind of conditioned on data because we observed it.",
            "That's the property of data.",
            "We have it.",
            "What's the problem with parameters?",
            "We don't have them.",
            "What's the property of latent parameters or latent variables?",
            "We don't need them.",
            "So in this case, C is a latent variable.",
            "We need to sum it out.",
            "Now, this summation.",
            "This is all an inference at its course really.",
            "Just summing out all latent variables.",
            "Problem being, if you have many of them, and let's say your binary, the naieve summation is an exponentially complex operation.",
            "So what graphical models are useful for is making that inference step as computational efficient as possible."
        ],
        [
            "So I'm not going to touch up on all the graphical models that were developed, but in the first ones that were more used for denoting dependencies and in dependencies are directed graphical models, also known as Bayesian networks.",
            "So the the way they're drawn as you have variables and the edges are directed so and the semantic of it is that the joint probability of all variables.",
            "So we just take all the nodes in a graph is the probability is the product of each variable conditioned on the set of its parents.",
            "So if we look at this graphical model here we have ferrules ABC.",
            "What that really denotes this picture here is that we're talking about a joint probability.",
            "Of a B&C that has the structural form P of a times P of B times Pfc given AMB.",
            "Because a has nothing that points into BS.",
            "Nothing points it with.",
            "C has two variables that point into it, so that's really what the graphical model means.",
            "It means that we have a certain factorization between the between the joint probability solution OK. And this is ancestral, so it means you can't really have loops in a directed graphical model.",
            "Very, very useful when you model a forward flow and we're going to do this."
        ],
        [
            "The later in the talk.",
            "Second one is undirected graphical models, so here you just have edges between variables and.",
            "The semantics is that the joint probability solution overall variables is the product overall maximum cliques.",
            "This is which is fully connected subsets of nodes and a potential function over the key.",
            "So we look at that undirected graphical model.",
            "That really just means that we have a factorization of the joint probability solution of ABC into one factor being effector of the variable latency, and one of being.",
            "In fact there BNC OK, so these are called potential functions where they get used to get often used when you, when you have temporal data or spatial data where you know local couplings, so the local connectivity really specifies that there is there is a dependency or a local potential coupling between between states you observe.",
            "Um?",
            "And the normalization constant, that's usually the problem in these the other one was naturally for a naturally normalized the directed graphical model.",
            "Undirected graphical models have this normalization constants which which embodies the complex operation.",
            "The exponentially complex operation of summing over Allstate."
        ],
        [
            "Now the one that one data, one graphical model.",
            "It's really become famous in the last 10 years or 15 years is a factor graph which is a which is a nice blend between these two.",
            "It's actually generalization, so factor graph is bipartite graph, so you don't just have variables, but you also effectors that usually black square nodes.",
            "And every the semantic is incredibly simple.",
            "The semantic of affective graph is that U joint probability distribution over all variables.",
            "Is the product of all factors, so you have as many factors as you have black nodes, black squares, and you as many variables as you have round round circles.",
            "So if you look at that factor graph.",
            "For example, we have one here ABC.",
            "It just means that the joint probability solution over a B&C is a function of a times a function of B * A function of ABC.",
            "Not specifying what that function, it doesn't need to be a conditional probability solution, OK?",
            "And."
        ],
        [
            "Factor graphs are powerful because they're more general than directed and in directed graphical models."
        ],
        [
            "You can see that effective immediately, every directed graphical model is a vector graph because the conditional probability solution is to factor.",
            "OK, but there."
        ],
        [
            "I'm more fine grained then a undirected graphical model.",
            "Why is that?",
            "But look at this director.",
            "Look at this factor graph.",
            "That's a factor graph that just says you have one factor and that's the function of Arabian Sea Run Square.",
            "Note 3 variables.",
            "Now we have three orbits.",
            "We have three factors, one AB1BC1 AC.",
            "If we try to express the second Factor II factorization in an undirected graphical model, we cannot do that.",
            "If we have a click with the Navy and a clip between AC and a click between BC, we have a clearly the Mexicans ABC.",
            "So an undirected graphical model is not as as fine grained in really specifying the factorization that the joint probability solution might have.",
            "It's it's convenient, so we could also summarize."
        ],
        [
            "And it's convenient for specifying local couplings.",
            "It's convenient for actually reading of dependencies and dependencies, but it's not as fine grained and powerful as a factor graph.",
            "Now, how does affect the graph relate to base law?",
            "Everyone knows baseless pose.",
            "So base law relates in fact this deep mathematical inference system, and it says that the belief in the parameters given the data is up to normalization.",
            "A function of the belief in the data given the parameters which is known as the likelihood, is a function of the parameters times the prior belief belief without any data.",
            "So if you more concrete if S here specifies the skill of Scott and me, so it's a vector of two numbers, then based law says that the probability of.",
            "Scott scale of my skill given the match outcome Y between our match, that's going to happen is going to be a probability of the match outcome given our skills times the prior probability of our skills that space law and based law is basically the simplest possible factor graph.",
            "It has two factors into variables.",
            "One variable is the data and I shaded it a bit.",
            "The wife and one factor is a one variable.",
            "Is the parameters skills here.",
            "So the way that we get from base law to a complex vector graph is that we actually start to model in forward fashion.",
            "So how could we make a model for for our skills ending up in the match outcome?",
            "Well, one is we could first of all assume that the skill of Scott and the skill of me are independent of each other.",
            "So that just means that this vector graphic just head splits up in two nodes as one as two.",
            "Secondly, we would.",
            "We could assume that we actually going to have.",
            "Some factorizing likelihood.",
            "So if we assume that the skill of Scott in the skill of me lead to a performance that we have in a match, then there is a coupling between our performance and our skill, and then there is a difference in our performance that's actually in demonistic factor D. That's the difference of being SP1 and P2T1 and T2.",
            "And then depending on the sign of the difference.",
            "Scott Windsor, I win.",
            "That's a reasonable model.",
            "And what happened now is that that simple factor graph we had just had two variables and two.",
            "Vectors and having a sort of starting to become a very interesting structure.",
            "What is inference?",
            "As I said, inferences summing all the latent variables.",
            "What are the latent variables in here?",
            "Well, the performance and the D because we're not really interested in performance were interested in Scott skill in my school, and we have reserved white.",
            "So inferences release coming out, the summing over all values.",
            "Overall states of the T variables and the D variable.",
            "And if we do that, then we perform inference because that's effectively computing the posterior as a function of the skills.",
            "The function of the parameters.",
            "So to summarize."
        ],
        [
            "And this sort of three major types of graphical models.",
            "One is used for modeling that's directed graphical model.",
            "It can't have loops.",
            "It has ancestral relationships, but it's very good for forward flow where you know, I assume some, some state, some parameter interest me and they in a forward processing flow generated data.",
            "I'm actually going to log or observed.",
            "Then there is Markov networks, undirected graphical models they use.",
            "They are very useful for local couplings potential.",
            "So when you have sound speech, temporal data very powerful because you can couple over.",
            "You couple in some locality and then there's factor graphs.",
            "They're not very good in reading of dependencies, so I kind of rushed over this, but they're very powerful when it comes to efficient inference.",
            "OK, so let's pick out this."
        ],
        [
            "Example actually, so let's look at how we do."
        ],
        [
            "Inference, so here's that graph that we just looked at turned on its head.",
            "OK, so we have in this one we have 5 variables, VWXY&Z and four factors F1F2F3F four by the way, in case you haven't noticed, I'm going to use the pictures a lot, but I never use algebra in blue, so the algebraic expression that's at the bottom.",
            "OK, so the joint distribution over the five variables is a product of four vectors F1F2F3F four.",
            "So suppose we're interested in W. This is our.",
            "This is a variable that we are interested in.",
            "The parameter of course, so we need to sum over everything that is latent.",
            "So in this in this example, that would be the XY and Z OK.",
            "So if we did it the naive way, that's how we do it.",
            "We just some overall states of Y or all states of X over all states of Y.",
            "Overall states of Z.",
            "So if these are just binary variables that would be with four variables, 16 summations, so we have it.",
            "We actually looking at a sum with 16 summons and each of the summons is a product of four terms.",
            "OK. That's the naive way of doing it.",
            "If we had millions of variables, that's not, we can't do it naively.",
            "Now, if you wanted some if you want to simplify that some.",
            "Then on in algebra as well as in the picture, we notice one thing which is we're interested in a function of W. There really are only two factors that involved W, F1 and F2.",
            "The other factors do not involve W. So what that means is I can actually use the distributive law and I can pull the F1 term all the way out to V. And I'll leave the other ones on the other side.",
            "OK, So what I've just done in algebra, I've just.",
            "Use the distributive law so.",
            "In other words, if I make this simpler, but all I used is that.",
            "A + B * C = 8 * C + B * C. OK, so previously I had a sum of products.",
            "And in those products see occur twice and I just pulled that out and left that some here.",
            "That's that's the step I was just doing in algebra OK. Why could that be useful?",
            "Well, let's count the amount of computation that happened here.",
            "This is where I started from.",
            "So how many computations do I have?",
            "One multiplication, one modification?",
            "One additions are three steps, arithmetic to do here.",
            "How many do I have here?",
            "I have one addition, one multiplication, so I have to.",
            "OK, so by applying this this distributive law I went from 3 computational steps to two.",
            "Now I'm doing this at scale.",
            "I'm doing this again and again.",
            "This is a 50% reduction.",
            "Sorry, 33% reduction in computation and depending on where doing the graph, you'll see that that leads to very very efficient computation.",
            "So I end up with is these two partial terms here.",
            "So I end up with the partial sum of W that is summing over V, But the first term and I end up with the second term, the second partial somewhere some over XY and Z. OK.",
            "So if I give a name to this if I just say these are still functions of W. So let me call them messages.",
            "This is a partial function.",
            "Then what I end up seeing here is that the first rule of the sum product algorithm is that the marginal of a variable is the product of all incoming messages.",
            "And the message being the partial sum of of the neighboring factors.",
            "Overall, variables that are in the in those parts of the parts of the tree.",
            "So it's almost like made W the root of a tree.",
            "And if I sum over all variables in the left part and overall virus in the right part and find more and more, I just need to multiply those functions of W together.",
            "OK.",
            "So what?"
        ],
        [
            "The message what is the form of the message from Affecter to available?",
            "So I just left it formally.",
            "I just left it standing here.",
            "That's actually still a sum over XY and Z.",
            "What I see now is that F2, the factor of 2 doesn't depend on Y&Z.",
            "It just depends on X.",
            "So I can do the same trick again, I can just use the distributive law again.",
            "So if I do that, it means that I can pull F2 out.",
            "To the first summation, so I somewhat over X, but I don't some out over Y&Z yet I bundle these together, they become waiting effect rating terms.",
            "In fact, same trick applies.",
            "Again, I could introduce a shorthand notation for that partial sum.",
            "This is a partial sum and the partial sum.",
            "As a function of X OK, so find other variables that connect to F2.",
            "I would continue having partial sums over those variables as well, so the rule is that in order to compute the message from affect that are available, I just need to some out over all the other variables that are attaching to that factor, and I need to wait by a message.",
            "That is the buyer by a partial sum, that is, the message from that variable to the."
        ],
        [
            "And what's the functional form of that here?",
            "I see again if I look at the message from X to Y from X to F2.",
            "That's the formal definition.",
            "I see that that's just basically summing out over the neighbors of that variable that are not F20XX occurs in three factors in F2 and F3 and F4.",
            "And if I want to compute the message, what's the function of the message from the variable X2F2?",
            "It's coming out over the message that comes from F3 and the partial sum of three and partial sum of the four.",
            "I applied the same district of law again.",
            "OK, so if I put those three together?"
        ],
        [
            "I actually have the sum product algorithm, so some product algorithm is literally three iterations.",
            "That kind of peel a tree.",
            "To its leaves you pick an arbitrary node in the tree.",
            "That's the tree that describes your joint probability model, and then you peel all the way to the leaves of the tree and back OK.",
            "So what's nice about that is.",
            "I I I can actually compute all the marginals at the same time when I have all the messages, because the rules apply anywhere in the tree, I picked an arbitrary node W, not just, you know, constructively arrived at, so that means that the amount of computation I need to do and the amount of storage is really only proportional to the number of messages.",
            "Now, how many messages did I have always had a message on an edge, so if I have a tree, if I have a factor graph with millions of variables in.",
            "Millions of vectors.",
            "The complexity is really just the number of edges, not the exponential number of states of the variables.",
            "In fact, if the factors connect a few variables, it's a very efficient algorithm because it only needs to need to compute a quantity called a message along the edges of that factor graph."
        ],
        [
            "One trick that people often do is they actually express this.",
            "These update rules in in log form, so you take the log of each of these messages in the log of the marginal probabilities.",
            "If the log probability available, it's the sum of the log messages here in order by Lambda and the log messages are coming out.",
            "The actual factor, weighted by the exponentially exponentiated some of the incoming log messages.",
            "Now what's nice about if you look at the 1st and the third rule.",
            "There are totally closed there actually just summations, and if your messages are in the exponential family, so your messages are E to a natural statistics of the parameter T, the function times the parameter vector Theta, then the 1st and the third rule are simply just additions of the theater values.",
            "Alright, so if the message is on the exponential family, then computing the marginal and computing the message from a variable to affecter are just additions.",
            "There's not even multiplications.",
            "Most what is in the exponential family quite."
        ],
        [
            "Distributions are in there just to highlight a few reductions or any exponential family, and the parameters Theta for Gaussian is not the mean variance, it's the mean over the variance in one order variance called the precision mean in the precision.",
            "Bernoulli, binomial, beta, gamma, vichard, like all these distributions on the exponential family.",
            "So."
        ],
        [
            "That's that's a nice trick.",
            "So that means that."
        ],
        [
            "If we are using messages that are in the exponential family, we're ending up having just to do additions and the number of additions is just proportional to the number of edges in the graph.",
            "Secondly,"
        ],
        [
            "Use the second trick, and that's to do with not performing redundant computations.",
            "So pick an arbitrary variable and pick an arbitrary neighboring factor.",
            "So here pick T and then affected it's a neighbor of T. Then the first rule says that the marginal of T is the product of all incoming messages, so in pictures it's all the red errors together.",
            "And the second rule says that if I look at what is the message from the variable to the factor?",
            "OK from T to F. That's actually the incoming other messages.",
            "So in pictures is that?",
            "OK, so if I repeat that that simulation, the marginal of T is the product of all incoming messages.",
            "The message from T to F is the product of all incoming message, except that one you see that one thing is true, but for any pair of variable and neighboring message.",
            "Sorry, bearable and neighboring factor.",
            "You have the quality that the marginal is the product of the incoming and outgoing message.",
            "Right?",
            "The marginal was the product of all of them, and the outgoing message was the program all but the one missing one is the incoming one.",
            "From that from that factor.",
            "So what does that mean?",
            "It means that when you actually implement this in in computer, you're not going.",
            "You don't need to store the messages from the variable to the factors.",
            "You don't need to compute them whenever you need the message from a variable to affect her, just the weighting factor was the waiting term in the in the second update rule.",
            "You can get it by the vision.",
            "You can just divide.",
            "The marginal by the message from the factor to the variable.",
            "And if we're in the exponential family, the product of 2 messages was adding the theaters so that.",
            "The ratio of two messages or margin on the message is that it's just a division.",
            "Sorry, it's just a subtraction, so these are very elementary operations you add in the item subtract."
        ],
        [
            "OK, now that's that's all very nice.",
            "And actually when you when you look in the literature, if you start with messages in the exponential family then there is a limited set of factors that actually leave you leave you in the exponential family.",
            "But in reality when you really want to model some nonlinear dependencies, then the 2nd update rule is regularly going to cut up, pulled you out of the."
        ],
        [
            "So that's one kind of appointment.",
            "Set pointer center or.",
            "Stick the stick somewhere, no OK?",
            "Well then you have to believe me.",
            "See if I can do it the projector.",
            "No this one.",
            "Right, this one has the problem that.",
            "Even though the 1st and the 2nd one.",
            "Thank you so this one.",
            "If this is an exponential family member then this is exponential family.",
            "If this one is exponential family product of them is expensive family, but for arbitrary functions F and we're going to see one in a minute.",
            "If this is a exponential family then this is an exponential family, but something out here might sort of.",
            "Catapult you out of the exponential family and then that whole recursion would break.",
            "And so in practice.",
            "The reason this hasn't been so popular is that there wasn't really a way other than other than look."
        ],
        [
            "At the what's known as the conjugacy, the family of conjugate priors to really apply this technique at scale now."
        ],
        [
            "Around 2001 or run 2000 Tomenga came up with a nice solution to this, and the idea is very very simple.",
            "When you remember the last trick we just did the last trick we just did said that the marginal of a variable is always.",
            "You can pick an arbitrary neighboring factor F. Then the marginal is is always the product of the incoming message from that variables and the outgoing message to that variable.",
            "OK, so this is always the marginal.",
            "What I'm interested in.",
            "What I'm interested in is is is the marginal right?",
            "You know I'm interested in the system, the machine, the data center, learning about the belief of Scott skill or one of the other 50 million members skills or so forth.",
            "So if I approximate the margin notes, well, so all I need to do is I pick a distance measure between probability distributions.",
            "And I pick one such that P hat is again in the exponential family.",
            "Then I do an approximation step on the marginals, but I can compute the approximate message from the factor to the variable by dividing the approximate marginal by the approximate by the actually approximate message from the verbal to the factor.",
            "I'm just using the fact very quality that PFT is incoming outgoing message.",
            "I approximate the margin.",
            "That's the thing I want approximately 1 approximate the messages there means to an end.",
            "Hillary there just partial sum and then I once I've computed approximate marginal.",
            "I'm actually just getting by division that approximate message let me show you some pictures."
        ],
        [
            "OK, suppose we assume a Gaussian belief over my skin over scope skill.",
            "Then the difference of our skills is also going to be ocean.",
            "Let's the blue curve, so the message coming into from the difference variable D in the first slide down to the fact that describes our outcome.",
            "That is a very nice question.",
            "Now we have a match outcome.",
            "Scotland's so that means the difference must have been positive.",
            "So that's the data.",
            "The data tells us the difference with positive, so that's a step function.",
            "What is the posterior of the?",
            "It's the product of the incoming outgoing message, so it's a truncated Gaussian.",
            "That's the true posterior over the difference.",
            "That's not a Gaussian anymore, so we couldn't really use approximately a message passing and workout the message.",
            "So what we do is we approximate the truncated Gaussian with the Gaussian, and if we use the KL divergent, then the Public Library vergence then ends up being just matching the mean and the variance of a truncated Gaussian that is known in closed form.",
            "Closed form is known if you truncated Gaussian at a given point, that what is the mean and the variance of the trunk in Goshen.",
            "So we match that.",
            "So we end up with a black Gaussian.",
            "Again, that's the approximate.",
            "Marginal of the difference variable.",
            "The incoming message was discussion, so what's the approximate message that describes the step function very well for the purpose of inference?",
            "Well, it's the blue curve.",
            "The black Earth divided by the blue curve because the black curve is always the product of the blue curve and the red curve that is in pictures.",
            "But I just described.",
            "And if you divide them, you end up with a Gaussian.",
            "Now previously people tried to approximate messages, but that is a horrible approximation to the message when you think of it.",
            "'cause it's zero, but this is 1 in most of the most of the cases, but the message is just an auxiliary quantities, just an auxiliary partial sum.",
            "So this is actually going to iterate that scheme of approximating the messages.",
            "That way you actually approximating the marginals.",
            "The variables of interest, the skill barriers in this case, and you end up with a very good approximation and."
        ],
        [
            "There's a whole field that's actually studying.",
            "How is the?",
            "How is the algorithms emerge if you use different distance functions?",
            "So the one I was just using is the KL divergent, such that P is the true distribution, meaning I'm using the step function times the Gaussian and approximating it with Q, which is a real Gaussian.",
            "But there is a more general set of divergencies, so as I said in a set already, if I minimize the KL divergences a very it's very easy to show that that's actually matching the moments of the distribution PFT.",
            "There is a generalization of this known as the Alpha Divergent and the Alpha divergences.",
            "A parameter Alpha is the formal definition of the other versions have been two distributions, P&Q and you can show that one of them special cases is at zero.",
            "It's the color vergence.",
            "Sorry I'd wanna stick elevations as we headed and at zero it's the other way around.",
            "What's the difference?",
            "I think again it's easier to describe in picture."
        ],
        [
            "So, and I recommend reading up.",
            "This is a 2005 paper by Tomenga.",
            "Imagine the blue one is the true one.",
            "We want to approximate.",
            "OK, so we have this blue.",
            "Distribution is symmetric around 0.",
            "So that's the true one.",
            "And now we want to find that.",
            "Gaussian that's closest under the elder vergence.",
            "The way around that I just described matching moments that's easy.",
            "The mean of that blue curve of the blue distribution is zero and the variance is pretty wide, so that's the minimizer of the D1.",
            "This distance that ends up in an algorithm.",
            "If you apply that repeatedly in the graph, you get this album known as expectation propagation.",
            "If you use the zero, so you minimize the collective urgency.",
            "Other way around, this is actually the minimizer, meaning it's on the second one that's directly sitting on top of here.",
            "That's the function that's actually a distribution that doesn't match the moments, but it is such that the under the green curve you have most of the you have mass on the blue curve, but not necessarily under the blue curve mass of the green curve.",
            "So you jump on the mode and if you look in the literature, there's like a whole body.",
            "The name for this in the literature is variational Bayes.",
            "So depending on what that Alpha is 01 there's other ones.",
            "Other other ranges that are used as well for Alpha that lead to different algorithms.",
            "It's a choice.",
            "It's basically just the choice of approximation and we will see in a Matchbox case that this is relevant.",
            "You might argue why would you ever want to do that?",
            "The reason why I want to do that because you might have symmetries in your posterior simply because it's it's the same solution, but there's no symmetry breaker in your prioritization, and then this distance measure breaks the symmetry where this one would retain the symmetry and would actually get more uncertain as you started from.",
            "Not the D1 or D1D zero measure is not always the appropriate measure."
        ],
        [
            "OK, that's really.",
            "I think that's really at the heart of distributed at approximate message passing, you start with some product algorithm and then whenever you encounter a factor that is sort of cut that pulled you out of the class of exponential family that you're going to use you project back by picking a distance measure so you.",
            "The recipe release you describe your data and a forward graphical model.",
            "When you do that, then you pick a distance measure.",
            "If you need to approximate the inference and that results in algorithm and we'll see several in the second part of the tutorial.",
            "Now the last problem I want to talk about is distributed message.",
            "And why is that important?",
            "I kind of touched upon it at the beginning."
        ],
        [
            "It's important because the amount of data that that we have available to learn from is really growing up a lot faster is really growing a lot faster than the amount of computer becomes available.",
            "It's something that people have referred to as acts law, or is it covers more.",
            "It is true so far that data roughly doubles in every domain it's to do with the sensors that were available.",
            "So if you look at Facebook's news feed and this is slightly out of date data, so this is publicly available.",
            "If you have, you know 100 billion training examples on a single day, and you have.",
            "In terms of variables, let's say you just want to learn in a Facebook is a person's personal network social network on people.",
            "You have, at least if you had one parameter per user just to buy us, you would have 650 million parameters to change per day.",
            "And if you go over a month, it's a 1.1 billion or 1.3 billion things.",
            "Latest figure.",
            "If you look at the social graph you want to learn.",
            "Let's say you want to learn a friends recommender that's predicting an edge in that network.",
            "You can learn from 130 billion edges today and you have 1 billion.",
            "Users, if you simply had a bias again per user, not even taking into account the user user correlation.",
            "If you look at Google's Pagerank, you have 4 trillion Web Links, 1 trillion web pages.",
            "If you look at Amazon's forecasting so peak day, this is last year December on a single day people.",
            "Got shipped 15.6 million products on a single day.",
            "There's a peak days, probably more this year, so that's at least that many examples you can train from in a single day.",
            "You know which person or which product, which are of the day delivered to 1 ship address.",
            "You have 20,000,000 minimum have actually more 20,000,000 products and these are just products at Amazon orders in the fulfillment center.",
            "If you include the products that Amazon stores and fulfills for others, it's in the range of three to 400 million.",
            "So if you're a single variable that describes the propensity divider product, not even talking about a product user grab, you end up with more parameters than fit in a single machine memory.",
            "I could go on and on, but that's these.",
            "Figures are real and these figures keep growing at almost 50% in a 5200% in a year.",
            "Now the problem is.",
            "If you think of mapping this computation, these additions and multiplications on a single computer, you could say, OK, you know at the core level we use, GPU's is going to be really fast well, but they still only has 86,000 seconds.",
            "So if I have 100 billion examples to train from, that's a million training examples per second.",
            "If I access ram, yeah, it's very very fast to access random access memory in a computer, but you still cannot access in a read way more than 10 to 13 different elements.",
            "And I'm talking raw memory.",
            "X is if I want to write it's one order of magnitude less and tend to the 12 is not that big.",
            "So even if I try to squeeze in a big computer this is a it's bigger.",
            "So I have a few variables per Pew factors for variables.",
            "This is bigger or single machine can do and if I if I want to distribute this stuff I need to also take into account that.",
            "I can really, really only read into a computer a TB per day.",
            "Any faster you need, much faster, faster Internet, faster connections.",
            "So in practice you really need to think of."
        ],
        [
            "But distributing the amount of data or it's stored and distributing the amount of variables because it's exceeding single machine capacity these days?",
            "In fact, all these variables I was talking about there not generate on a single machine that generated on thousands and 10s of thousands of servers.",
            "So let's look at a model and it's very, very typical distributed conditional model.",
            "But that doesn't mean I'm not specifying what that is.",
            "It means I have data.",
            "My data is why?",
            "So?",
            "This could be match outcomes for Xbox Live.",
            "Or it could be bought or not bought customer bought or not bought a product.",
            "Or it could be friends recommendation.",
            "You know Ralph got these 10 people recommended when you log into Facebook and then he chose to friend this person.",
            "I didn't choose to friend that other person.",
            "That's why X is everything that describes the situation on our of the day, what page it was, what region, what MarketWatch IP address.",
            "What's the gamer ID?",
            "All this information and theater is our parameters.",
            "So now all I'm saying is that I assume that the probability of my data is conditionally independent.",
            "So if I condition with the parameters, then every observed sale friends recommendation click share comment on Facebook or or match outcome is independent of each other.",
            "Given I know the contextual situation that all data on the context and other parameters.",
            "And I have a prior over the machine maintains a prior over the order parameters.",
            "So the factor graph to this?",
            "What's the factor graph?",
            "Well, we have observed Y. X is conditioned on always, so I just don't even include it.",
            "You could include X here as well, but it's it's conditioned on it and we never observe.",
            "We never reason about it.",
            "And we have data.",
            "So here, speaker, and here's the wise.",
            "Now I assume it's vectorizing, so that means that's actually the picture.",
            "So the picture to keep in mind for distributed inference or distribute approximate inference is that we have this big graph where there's a huge amount of data items, 100 billion in Facebook's case per day.",
            "And there's a huge amount of parameters, and they're connected whenever that.",
            "Whenever that likelihood term.",
            "Involves a given parameter Theta, for a given data item Yi.",
            "OK, so not every data touches every parameter, so if this is a per user, let's say these are all parameters that are per user propensity to friend someone, or for the propensity to even like the story, then if this was Ralph then disconnects rules parameter.",
            "This was Scott Scott's parameter and so forth, so this connectivity matrix is in general sparse.",
            "In fact you'll see later that in some models this is usually.",
            "This can be a.",
            "Rather constant or very small number of outdegree, meaning that you have fixed number of features.",
            "So every one of your data items connects to a fixed number of variables in their likelihood.",
            "But you might have a power law kind of distribution at the out degree of the parameters to the to the data because some users just are unproportionally, often excess Facebook versus others that are just coming once a month.",
            "So the idea of distributed message passing is actually when pictures very easy to describe, which is affective graph is a graph between variables.",
            "Bipartite graph between variables and their relation to each other.",
            "So all we can do is we just regroup this graph.",
            "So if we regroup the factors together that are describing our data.",
            "And instead of having seven data items, we can say we have three, and this is a little more complex vector.",
            "This vector needs to now compute the outgoing messages to all the variables by taking all these three items into account.",
            "All these two and all these two.",
            "So if you map this two computers, then this is what you might call the mappers.",
            "In MapReduce, these machines that are computing locali independent of each other, the outgoing messages 'cause in message passing here will just be.",
            "You know what would be the posterior each parameter, the posterior of each parameter.",
            "Is the product of all incoming messages is the prior times?",
            "Messages along these edges, but these boxes can compute these messages independent of each other.",
            "And you can do the same grouping used this time around of variables for the variables, so you end up if I put this back in algebra."
        ],
        [
            "You end up basically with taking this big product over data and big product of a parameters into a product of product of data into a product of product of parameters.",
            "And this one I might call a chunk of memory store.",
            "I believe store sing the machine holding a subset of the of the parameters in memory and this one is a single machine storing that subset of data, either because it arrives there or in map reduce it got stored there.",
            "OK, so are we really done algebraically thinking algebra, it's also easy to describe this.",
            "We just went from one factor graph that had too many variables.",
            "There are too many parameters and too many data into another factor graph that now has a controllable number of factors over data and factor or parameters, and we're mapping these factors onto single machines now.",
            "So that means that that product and all the message exchange between them is now communication on a network.",
            "OK, so it's just another way of mapping mapping the multiplication operation on two systems.",
            "This time around we the communication between them is actually a network.",
            "There's some interesting, interesting thing here.",
            "The data factors.",
            "Those will always note which parameters to send a message, that's easy.",
            "Because that's what's specified in the data.",
            "Romita factors one have that property because the parameter factors will receive messages from the data, but they don't encode unless they explicitly store where that data is coming from.",
            "They don't really have a.",
            "They don't really have have this encoded naturally, so the way."
        ],
        [
            "This this is actually implemented in practice and were describing.",
            "Here is a system that was in user Facebook for we didn't train the 10 billion dimensional model over 20 machines memory and 100 machines.",
            "Processing the data is is putting the factors for data into one fleet of machines and that might be Geo distributed because the global service and one set of machines that's actually storing all the parameters for everything.",
            "The machine, let's say there's four of them in each.",
            "Into region, and there's one in each region has now a subset of logical units that are the groups of the grouping of the factors parameter and data.",
            "And the communication protocol very simple.",
            "Whenever a piece of data arrives, it computes the messages.",
            "From that factor to the parameters, it knows where to send to, so it sends the parameters and the message.",
            "We are TCP IP directly to these machines and these machines perform very simple operation.",
            "Remember the marginal is the product of all incoming messages.",
            "They would just multiply messages and if these are Gaussians or anything in the exponential family, they would literally just carry out.",
            "In addition, would be very simple operation.",
            "Now, of course, these methods margin notes need to be copied back into each of these into each of the factors that are using them in order to make this approximation cousin approximate message passing, we need to know what's the current marginal in order to approximate so that one is a little bit tricky.",
            "So the way it was actually done one way that you could do it as many ways is that you have this parameter store actually estimating the amount of change to how fast is is one of these incoming errors.",
            "In unit time, arriving and then you have in a communication protocol.",
            "Whenever an update is requested, you send back the latest marginal as well as the expiry time by which this will happen.",
            "So you need and then.",
            "On.",
            "Here you have copies of those marginals.",
            "On these machines together an expiry time, so you know exactly when to request a red edge again from each of the data vectors.",
            "Now."
        ],
        [
            "One thing I swept over little bit is if you really implemented in a distributed system, there is a few extra complexities that in mathematics on the right, but in practice they are very very very important.",
            "Mainly have to do with the fact that components can break.",
            "So if you have your my Mac here.",
            "It's if any of the memory banks break, this machine is toast if any of the CPU to CPU goes, the machine is toast, but in a distributed systems where I took every single piece of the CPU becomes become a low cost piece of hardware and every single piece of memory bank became a low cost hardware.",
            "This is likely to happen.",
            "So one thing we need to deal with in practice, and you distribute inference is actually the machine Shard.",
            "Consistency machine can break.",
            "It can just simply get rebooted or fail, and that can't lead to the fact that now the whole data center performance.",
            "Computation becomes inconsistent.",
            "So one thing you need to take care of this consistency.",
            "Which machine carries with sharp need to also do a lot with reliability and maintenance.",
            "So one of the things is you, you know, in order to make such a service fast and make the delays very minimal, you do need to store all the parameters and RAM.",
            "Ram is unreliable.",
            "Then the machine failed, so you need to Check Point or or introduce some redundancy in this.",
            "In this red box that I called the parameter store.",
            "And then the rest of it is really quite a.",
            "Pretty quiet techno."
        ],
        [
            "Details, so there is actually a relation.",
            "It's interesting to MapReduce.",
            "MapReduce is this computational framework that almost everyone uses today when you when you distribute your computation and you learn at scale and what's the relation of MapReduce?",
            "Well, if you look at that graphical model for data so we have a parameter vector theater groups that rendered wrong and we have our data then MapReduce can actually be seen as a single step approximation of this message passing.",
            "What does it mean in map reduce?",
            "These are the members and send.",
            "Initiating the members means that you sent this state of the prior.",
            "Effectively this message, right?",
            "The right message here is this message.",
            "You send that to the Members.",
            "The members are the machines that contain the data they locally compute.",
            "Compute the messages from these from these vectors to this parameter and sent them back.",
            "And that's called the reduce stage.",
            "Reduce means that you're sharding you distributing your parameter vector in files.",
            "And then you multiply.",
            "If you just multiply, implement the multiplication method of operation operation of messages in here, then you can see that vanilla map produces a single pass of message passing.",
            "So as long as you're performing this algorithm on distribution on data models, but you don't need to do approximate message passing, there actually equivalent, and it's a very viable mode to do distributed message passing using existing infrastructure such as Hadoop.",
            "The problem arises more as if you have approximate messages.",
            "So like in true skills on a quick prediction factor, or in fact in any model I would work with me, you know, I never got away with a simple two factors.",
            "In this case you need to iterate that loop and every single iteration would be a new map reduce loop.",
            "So we actually really long because you spin up spin up these machines.",
            "The other problem is that if you do it and you do it anyway, you have a huge communication overhead because each of the machines needs a copy of that belief vector over the parameters, and if it can't store does it its own memory.",
            "So if these usually member machines are really crap and small, then we will all fail and the whole computation will break as well.",
            "So does MapReduce disposer constraint on the on the amount of parameters you can actually use, but there's a very close relation.",
            "It's a single path.",
            "Message passing is actually map reduce."
        ],
        [
            "OK, final thing before we before we have a break.",
            "Is to do with approximation quality, so let's look at this conditional model again, and now let's make a specific example where the probability of Y.",
            "So we assume now we have a binary classification.",
            "So we observe an event or we don't.",
            "So it's either plus one or minus one that the probability of plus 1 -- 1 given the parameter vector Theta.",
            "An observation X is the inner product of Theta times X and then mapped to the positive or negative side and we take fires to CDF.",
            "The Gaussian CDF and we assume a factorizing Gaussian belief overall parameters.",
            "So if that's the case, then if you did map, reduce.",
            "Sorry if you did not produce any decent sequential computations, that's one message passing.",
            "Skip this one message that we can do.",
            "We send this message then we send this message because we've all incoming messages.",
            "Then this one let me send a message to a second date, affect your already taking the message from the first day to factor into account when we process the 3rd.",
            "When we take already the message from the 1st and 2nd into account.",
            "That's how you would actually learn.",
            "Step piece online.",
            "Learning online learning is like inference forward that way, just so that's that's what you could do.",
            "That's what you would do if you were to sequentially computer messages in a flow of information.",
            "If you want to parallelize this, you send this out to all nodes and you send it all back.",
            "Any combine it?",
            "The thing is that we have to do approximate message passing.",
            "We have to compute approximate message in Y1 invite two and invite 3 because this is a smooth step function.",
            "The five function here if you look at if you plotted.",
            "OK, so this is multiplying Goshen.",
            "In coming with that one, you're going to end up with a truncated smooth truncated Gaussian."
        ],
        [
            "A lot of the engineering and making machine learning system work in practice actually goes into generating the right features and you usually have like several teams working on sets of features and work independently, and they might discover really the same signal.",
            "So it's an extreme situation of the come up like I think your talk was alluding to this.",
            "They come up with highly correlated features in practice.",
            "So if we did that, this is our feature vector and now we you know I just fixed a target vector Theta generated data for that and I did this sequential and I had one bias feature is not enough.",
            "Then it's just one.",
            "So the red curve and the blue curve shows you the following shows you I have 100 hundred times the same observation.",
            "And now what is the posterior?",
            "So the posterior over my parameter is going to be having a mean and variance, but remember the exponential, feminine, exponential family, the natural way to do think about it, the one in which you perform a stochastic gradient then is actually mean over variance in one over variance.",
            "Precision and precision mean.",
            "So what you see in the blue curve is the posterior after every single sequential updated.",
            "So we get more certain.",
            "Yeah, the variance goes down.",
            "That makes sense.",
            "Every single observation gives us one more piece of data, and the mean is this low of.",
            "So if you divide the.",
            "Why X by Y?",
            "Then you end up with Mew.",
            "So the slope is the mean, so that was actually the mean effect that I was using.",
            "Now if I do this in parallel then I carried out all these approximations independent of the other approximations already happened.",
            "So I end up with the model that's far more confident than it should be.",
            "If I perform the sequential message passing, which I assume I can't do.",
            "So it doesn't seem big problem except the the confidence is 3 times higher than the true confidence should have been like.",
            "This is the effect of distributing the message passing because in this in this regime of having many mappers I couldn't really use in the second update already incorporate the first message.",
            "Now here's how this curve looks if I assume I have 100 time features and I have 100 teams that worked on features and they all came back with highly correlated features.",
            "So now the true mean is still that slope is actually the same.",
            "You can't see it due to the scale.",
            "But the approximate message is horrible.",
            "You know it's a lot less certain, but more importantly, the slope is way off.",
            "This is the true.",
            "This is the true mean.",
            "This is the approximate one.",
            "So if you actually iterate that algorithm, we run one map, reduce run and then do another one another one.",
            "You see that the divergences actually diverging.",
            "The convergence is actually getting worse and worse."
        ],
        [
            "The way that people deal in practice with that is they actually use dampening the same way that if you if you do in stochastic gradient send you scale your gradient more heavily according to the amount of disconnect you have.",
            "So if you did one this.",
            "If you do this one.",
            "In fact if you use a dampening, that's proportional to the number of Members.",
            "Then you see that in the first step this would have been.",
            "This is now scaled back."
        ],
        [
            "This is scaled like this graph.",
            "So just scale it back to the extra range, so just less than one."
        ],
        [
            "You see, the time now step by step, actually approximating even in the power influence the.",
            "The two posterior with power inference.",
            "I'm less certain because I Denton, so I remove a lot of traction.",
            "You remove a lot of the information or data, but but what I can show is that that kind of rule.",
            "If Alpha is 1 /, a number of mappers is actually leading to convergence when I distribute the inference.",
            "OK, we."
        ],
        [
            "That one I want to finish the dry part of the presentation.",
            "Do a little break and then the second part we're going to look at a lot of applications of that calculus.",
            "2 problems of scale, and in the real world."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I try to speak loud so if it's if it's not.",
                    "label": 0
                },
                {
                    "sent": "If you can't hear me then I have to lean over here.",
                    "label": 0
                },
                {
                    "sent": "So tutorial I'm going to talk about today has two parts and it's it's a lot about my experience that I gathered in the last 1015 years working at these various companies that Scott was just describing.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This part we talk about the theory and one of the one of the former frameworks that I got made very good experiences with his graphical models.",
                    "label": 0
                },
                {
                    "sent": "So I'll start by introducing water.",
                    "label": 0
                },
                {
                    "sent": "Graphical model is also talk about, then they kind of how to go from a problem to a graphical model and then go deeper into the inference inference.",
                    "label": 0
                },
                {
                    "sent": "What is inference?",
                    "label": 0
                },
                {
                    "sent": "So it's covered in it and how to do inference in graphical models, particularly how to deal with approximate inference.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce one of the algorithms that I find is one of the most versatile tools known as the sum product algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that it's a very simple insight.",
                    "label": 0
                },
                {
                    "sent": "In the distributive law that makes this algorithm so powerful.",
                    "label": 0
                },
                {
                    "sent": "But I like this.",
                    "label": 0
                },
                {
                    "sent": "It makes it powerful because it goes from a model.",
                    "label": 0
                },
                {
                    "sent": "So you you have your problem, you describe the forward flow of how the data arrives to an algorithm and also touch a little bit up on some more more recent subjects which is called distributed message passing.",
                    "label": 0
                },
                {
                    "sent": "What what do you do and how do you do inference in graphical models that are for very large data of a very many parameter?",
                    "label": 0
                },
                {
                    "sent": "Very large, this can be another tournament, but I basically mean is is that the amount of data or the amount of parameters is bigger than can be produced in a single computer?",
                    "label": 0
                },
                {
                    "sent": "'cause when we think about a few is called machine learning but we we touch very little on machine on the machine aspects of it.",
                    "label": 0
                },
                {
                    "sent": "You know when we think about an algorithm or or inference, we often assume some arbitrary as some sort of abstract computational model which is sequential, which has can execute an arbitrary number of steps and storing arbitrary number of states.",
                    "label": 0
                },
                {
                    "sent": "That is not how computers work today.",
                    "label": 0
                },
                {
                    "sent": "Whether embedded devices or devices in the cloud.",
                    "label": 0
                },
                {
                    "sent": "So distributed message passing is an aspect of inference.",
                    "label": 1
                },
                {
                    "sent": "For for computations that exceed the capacity of a single computational unit.",
                    "label": 0
                },
                {
                    "sent": "Then we'll do a break.",
                    "label": 0
                },
                {
                    "sent": "Depending on how we run the time and what I do.",
                    "label": 0
                },
                {
                    "sent": "The second part is because the first time you feel it's probably little gets a little dry.",
                    "label": 0
                },
                {
                    "sent": "In the end, I'll actually going to fill it with some color by talking about it.",
                    "label": 0
                },
                {
                    "sent": "How does this framework apply to the problem of two skills?",
                    "label": 0
                },
                {
                    "sent": "So how many people in the audience play Xbox?",
                    "label": 0
                },
                {
                    "sent": "Ever in that only you under YouTube?",
                    "label": 0
                },
                {
                    "sent": "Here's The funny thing I've been.",
                    "label": 0
                },
                {
                    "sent": "You know, I've been talking about this subject for a few times for a few few years.",
                    "label": 0
                },
                {
                    "sent": "It's always only one or two people who admit before the talk and about 1/4 of the audience after the talk.",
                    "label": 0
                },
                {
                    "sent": "So OK, so I assume there's a few more, so I'll talk about the algorithm and the system that is used to estimate to match players on Xbox Live, the online online gaming network on Xbox.",
                    "label": 0
                },
                {
                    "sent": "I'm also going to talk about how this kind of approaches can be used for modeling click through rates in online advertising, attach a little bit about why is click the right click through rate so important, whether it's on an advertising and Bing or in Facebook or in Yahoo and how some of these insights from inference in graphical models are actually going to be used in practice.",
                    "label": 0
                },
                {
                    "sent": "And I have two.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of a lot of applications that could cover some will have to probably make a choice.",
                    "label": 0
                },
                {
                    "sent": "We could either talk about or I could sort of talk about in more detail.",
                    "label": 0
                },
                {
                    "sent": "About a recommender system, the recommender system that is now used in Xbox Live video store.",
                    "label": 0
                },
                {
                    "sent": "So when you go to videos in the recommended videos you get there.",
                    "label": 0
                },
                {
                    "sent": "I think it's actually going to launch fully life with Xbox One in two weeks or a system to learn to imitate experts.",
                    "label": 0
                },
                {
                    "sent": "For example, in the Japanese Board game of Go or Chinese game of Wichi.",
                    "label": 1
                },
                {
                    "sent": "So all these applications you'll see are applications of this calculus of message passing in graphical models.",
                    "label": 0
                },
                {
                    "sent": "So before we start with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "African models or before I start and go into the theory.",
                    "label": 0
                },
                {
                    "sent": "One thing I want to say is even though I might talk for up to two hours, I'm really only going to scratch the surface.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know more, I recommend a couple of books for the details about it.",
                    "label": 0
                },
                {
                    "sent": "One is by date Barber came out about a year ago.",
                    "label": 0
                },
                {
                    "sent": "You know half ago.",
                    "label": 0
                },
                {
                    "sent": "It's a very good book.",
                    "label": 0
                },
                {
                    "sent": "Second one which is a little more broadly brought this by Chris Bishop on pattern recognition machine learning.",
                    "label": 0
                },
                {
                    "sent": "The third one is the cost by by Andrew and cause errors actually across by both.",
                    "label": 0
                },
                {
                    "sent": "Definitely color and drawing there good, and my personal favorite, which is probably closest to this tutorial is Kevin Murphy's book Machine Learning a probabilistic perspective that really fills in all the details that I might just gloss over a little bit today.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, graphical models graphical models basically denotes a way of specifying probable solutions, but why?",
                    "label": 1
                },
                {
                    "sent": "Why could probably solutions would be so important?",
                    "label": 0
                },
                {
                    "sent": "And when I started to work on machine learning, I was actually working more in the field of support vector machines and kernel machines in general.",
                    "label": 0
                },
                {
                    "sent": "And the kind of thing that that made me think.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why probabilities can also be useful, not just for data you observe, but for for assertions you make in terms of parameters.",
                    "label": 0
                },
                {
                    "sent": "Is that machine learning?",
                    "label": 0
                },
                {
                    "sent": "We kind of want to design systems that uncertainty, so systems that have a calculus that can assign a degree of plausibility to statement a.",
                    "label": 1
                },
                {
                    "sent": "So the statement being, for example Scotts Stronger enroll.",
                    "label": 0
                },
                {
                    "sent": "So that's a statement we don't know its truth, but how would a system that needs to reason about this statement?",
                    "label": 0
                },
                {
                    "sent": "What would we assume for plausibility calculus to be true?",
                    "label": 0
                },
                {
                    "sent": "So or Scott skill level is 40 and it's a statement about the parameter we don't know.",
                    "label": 0
                },
                {
                    "sent": "Now, if we assume three simple axioms and the first one is, I think very non debatable, which is plausibility is a real number.",
                    "label": 1
                },
                {
                    "sent": "So regardless of which statement I ask, if I assume the possibilities are real number, I do make an assumption, but it doesn't feel like a strong line.",
                    "label": 0
                },
                {
                    "sent": "The second one is regardless of how I asked the question, the plausibility should stay the same whether I asked whether I say the possible the skill of Scott is is between 35 and 40, or I say the skill is bigger than 35.",
                    "label": 0
                },
                {
                    "sent": "And less than 40 or I say than in any other logical Boolean algebra way it shouldn't change, and the third axiom, the third X in that I'm going to postulate that I thought was also very non debatable as one of monotonicity.",
                    "label": 0
                },
                {
                    "sent": "So if the plausibility of a statement a under observation C prime is strictly bigger than the plausibility under observation C. So we for example, we've learned one more game has happened and we learn something about Scott skill.",
                    "label": 0
                },
                {
                    "sent": "And another observation, B is unaffected by that, so the possibility of B is the same whether it's A and C prime in ANC.",
                    "label": 0
                },
                {
                    "sent": "Then the plausibility of both A&B cannot go down.",
                    "label": 0
                },
                {
                    "sent": "So if I assume these three axioms on a plausibility measure for possibilities as system assigns to parameters, then there's only one calculus which is that of probability theory.",
                    "label": 0
                },
                {
                    "sent": "That's actually the calculus that fulfills these axioms, and that's very powerful, and that was one of the sort of moments for me to consider probability calculus as a calculus.",
                    "label": 0
                },
                {
                    "sent": "Also for around statements of the world, like parameters which includes parameters, the second thing I want to say is why probability is so important when you when you build real systems, is because.",
                    "label": 0
                },
                {
                    "sent": "There is this.",
                    "label": 0
                },
                {
                    "sent": "There's actually sort of a a close loop of inference decision making an inference prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And making so when I talk about inference in next 2 hours, what I really mean is the is the is the task where a system, a machine with computer or data center has a belief over parameters, parameters describe properties of the real world like skilled Scott level or the effect strength of being shown in the first position of an ad or the page being the home page on Facebook.",
                    "label": 0
                },
                {
                    "sent": "And I get data and I have a model how data changes of parameters, then inferences that computational step that complete that translates the data and the probability over the parameters or belief into belief of the parameters given the data.",
                    "label": 0
                },
                {
                    "sent": "And what will use one of the rules and one of the approximate rules, but this this computational task of basically changing the believing parameters?",
                    "label": 0
                },
                {
                    "sent": "That's what I call inference and what it does is it requires the modeler to have a model of data, links to parameters, and it really allows to incorporate all prior information that a person has into a system over the belief of parameters.",
                    "label": 1
                },
                {
                    "sent": "The second is prediction that means the system has a certain belief in parameters, receives new data.",
                    "label": 0
                },
                {
                    "sent": "And needs to work in probability theory is called marginalized out, some out all the parameters to arrive at the prediction predictive distribution over data.",
                    "label": 0
                },
                {
                    "sent": "What are all the possible data in the future?",
                    "label": 0
                },
                {
                    "sent": "So if I for example, if Scott and I meet each other after we've got all the beliefs of Scott skills on my skill, that marginalization is assigning probability distribution to how likely Scott to win when we match play and how likely am I to win an and then the last step is decision making and decision making means that.",
                    "label": 0
                },
                {
                    "sent": "We now have a need a loss function because it's the first time that the system acts.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the matchmaking, it means that the system actually decides to have Scott and me playing against each other.",
                    "label": 0
                },
                {
                    "sent": "So that is an action at a system takes and that requires a loss function and requires because there is mistakes that we could do when we reason when the machine reason strongly or data center reason strongly about the truth and top error.",
                    "label": 0
                },
                {
                    "sent": "That one is often forgotten 'cause the last function has a very close link to future data.",
                    "label": 0
                },
                {
                    "sent": "If the system decides that Scott and I play and it gets one more training example, it can can learn about Scott's in my skills.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't do so, it can never learn about it, so there's an intricate link.",
                    "label": 0
                },
                {
                    "sent": "It's really quite a cycle between decision making, inferences all the inferences we can make depend on the decisions we made in the past, and it's actually something very important.",
                    "label": 0
                },
                {
                    "sent": "If you if you put a machine learning system in practice and there's some recent work on causality that's really capturing this stop error here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enough about why probability so important.",
                    "label": 0
                },
                {
                    "sent": "So what is a graphical model in the simplest definition of it is just a graphical representation of a probability solution.",
                    "label": 1
                },
                {
                    "sent": "Some of the part of the problem of the variables which are denoted by round around nodes are data.",
                    "label": 0
                },
                {
                    "sent": "So that's what we observe and some of the variables are causes like things parameters were interested in.",
                    "label": 0
                },
                {
                    "sent": "I say causes here in parentheses because it's I'm not talking about causal inference, just what causes the data to happen.",
                    "label": 0
                },
                {
                    "sent": "In what is the model?",
                    "label": 0
                },
                {
                    "sent": "It's the relation between the course and the data that's an edge in a graphical model, and then we have 1/3 type of variable, but just latent variables.",
                    "label": 0
                },
                {
                    "sent": "I'll show you in a minute you kind of need to introduce them for the forward flow.",
                    "label": 0
                },
                {
                    "sent": "So when you look at the real system or you look at a real process like how does it happen that I win when Scott and I play?",
                    "label": 0
                },
                {
                    "sent": "We might need to introduce some latent variables, but we're really not interested in inference about their state.",
                    "label": 0
                },
                {
                    "sent": "We need to get rid of them.",
                    "label": 0
                },
                {
                    "sent": "So the key questions when graphical models got introduced in the 80s was actually the dependency.",
                    "label": 1
                },
                {
                    "sent": "So you know when you look at the algebraic form of a joint, probability solution is very hard to answer the question.",
                    "label": 0
                },
                {
                    "sent": "Is a variable A&B conditionally independent given the state of a variable C. So the whole statement in Algebra is here and you could prove that if you show that this factorization holds and what graphical models are useful for particular directed graphical models is answering these questions by pictures.",
                    "label": 0
                },
                {
                    "sent": "So you see deliberately I kind of show a graph node there.",
                    "label": 0
                },
                {
                    "sent": "So the initial way graphical also introduced is to be able to answer exactly these kind of questions.",
                    "label": 0
                },
                {
                    "sent": "Is this variable set in that variable set independent of each other conditionally on that verbal set being observed, where this observed from would be data?",
                    "label": 0
                },
                {
                    "sent": "The thing that we're going to use graphical models for the rest of the talk is actually for making the task of inference faster, and what is inference?",
                    "label": 0
                },
                {
                    "sent": "Well, because we can apply appeal to the rules of probability.",
                    "label": 0
                },
                {
                    "sent": "Inference is summing all the latent variables.",
                    "label": 0
                },
                {
                    "sent": "So if we use the graphical model to describe all the data, all the variables we have.",
                    "label": 0
                },
                {
                    "sent": "Causes latent and data.",
                    "label": 0
                },
                {
                    "sent": "Then we always kind of conditioned on data because we observed it.",
                    "label": 0
                },
                {
                    "sent": "That's the property of data.",
                    "label": 0
                },
                {
                    "sent": "We have it.",
                    "label": 0
                },
                {
                    "sent": "What's the problem with parameters?",
                    "label": 0
                },
                {
                    "sent": "We don't have them.",
                    "label": 0
                },
                {
                    "sent": "What's the property of latent parameters or latent variables?",
                    "label": 0
                },
                {
                    "sent": "We don't need them.",
                    "label": 0
                },
                {
                    "sent": "So in this case, C is a latent variable.",
                    "label": 0
                },
                {
                    "sent": "We need to sum it out.",
                    "label": 0
                },
                {
                    "sent": "Now, this summation.",
                    "label": 0
                },
                {
                    "sent": "This is all an inference at its course really.",
                    "label": 0
                },
                {
                    "sent": "Just summing out all latent variables.",
                    "label": 0
                },
                {
                    "sent": "Problem being, if you have many of them, and let's say your binary, the naieve summation is an exponentially complex operation.",
                    "label": 0
                },
                {
                    "sent": "So what graphical models are useful for is making that inference step as computational efficient as possible.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to touch up on all the graphical models that were developed, but in the first ones that were more used for denoting dependencies and in dependencies are directed graphical models, also known as Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "So the the way they're drawn as you have variables and the edges are directed so and the semantic of it is that the joint probability of all variables.",
                    "label": 0
                },
                {
                    "sent": "So we just take all the nodes in a graph is the probability is the product of each variable conditioned on the set of its parents.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this graphical model here we have ferrules ABC.",
                    "label": 0
                },
                {
                    "sent": "What that really denotes this picture here is that we're talking about a joint probability.",
                    "label": 0
                },
                {
                    "sent": "Of a B&C that has the structural form P of a times P of B times Pfc given AMB.",
                    "label": 0
                },
                {
                    "sent": "Because a has nothing that points into BS.",
                    "label": 0
                },
                {
                    "sent": "Nothing points it with.",
                    "label": 0
                },
                {
                    "sent": "C has two variables that point into it, so that's really what the graphical model means.",
                    "label": 0
                },
                {
                    "sent": "It means that we have a certain factorization between the between the joint probability solution OK. And this is ancestral, so it means you can't really have loops in a directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "Very, very useful when you model a forward flow and we're going to do this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The later in the talk.",
                    "label": 0
                },
                {
                    "sent": "Second one is undirected graphical models, so here you just have edges between variables and.",
                    "label": 1
                },
                {
                    "sent": "The semantics is that the joint probability solution overall variables is the product overall maximum cliques.",
                    "label": 0
                },
                {
                    "sent": "This is which is fully connected subsets of nodes and a potential function over the key.",
                    "label": 0
                },
                {
                    "sent": "So we look at that undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "That really just means that we have a factorization of the joint probability solution of ABC into one factor being effector of the variable latency, and one of being.",
                    "label": 0
                },
                {
                    "sent": "In fact there BNC OK, so these are called potential functions where they get used to get often used when you, when you have temporal data or spatial data where you know local couplings, so the local connectivity really specifies that there is there is a dependency or a local potential coupling between between states you observe.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the normalization constant, that's usually the problem in these the other one was naturally for a naturally normalized the directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "Undirected graphical models have this normalization constants which which embodies the complex operation.",
                    "label": 0
                },
                {
                    "sent": "The exponentially complex operation of summing over Allstate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the one that one data, one graphical model.",
                    "label": 0
                },
                {
                    "sent": "It's really become famous in the last 10 years or 15 years is a factor graph which is a which is a nice blend between these two.",
                    "label": 0
                },
                {
                    "sent": "It's actually generalization, so factor graph is bipartite graph, so you don't just have variables, but you also effectors that usually black square nodes.",
                    "label": 0
                },
                {
                    "sent": "And every the semantic is incredibly simple.",
                    "label": 0
                },
                {
                    "sent": "The semantic of affective graph is that U joint probability distribution over all variables.",
                    "label": 0
                },
                {
                    "sent": "Is the product of all factors, so you have as many factors as you have black nodes, black squares, and you as many variables as you have round round circles.",
                    "label": 0
                },
                {
                    "sent": "So if you look at that factor graph.",
                    "label": 0
                },
                {
                    "sent": "For example, we have one here ABC.",
                    "label": 0
                },
                {
                    "sent": "It just means that the joint probability solution over a B&C is a function of a times a function of B * A function of ABC.",
                    "label": 1
                },
                {
                    "sent": "Not specifying what that function, it doesn't need to be a conditional probability solution, OK?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factor graphs are powerful because they're more general than directed and in directed graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that effective immediately, every directed graphical model is a vector graph because the conditional probability solution is to factor.",
                    "label": 0
                },
                {
                    "sent": "OK, but there.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm more fine grained then a undirected graphical model.",
                    "label": 1
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "But look at this director.",
                    "label": 0
                },
                {
                    "sent": "Look at this factor graph.",
                    "label": 0
                },
                {
                    "sent": "That's a factor graph that just says you have one factor and that's the function of Arabian Sea Run Square.",
                    "label": 0
                },
                {
                    "sent": "Note 3 variables.",
                    "label": 0
                },
                {
                    "sent": "Now we have three orbits.",
                    "label": 0
                },
                {
                    "sent": "We have three factors, one AB1BC1 AC.",
                    "label": 0
                },
                {
                    "sent": "If we try to express the second Factor II factorization in an undirected graphical model, we cannot do that.",
                    "label": 0
                },
                {
                    "sent": "If we have a click with the Navy and a clip between AC and a click between BC, we have a clearly the Mexicans ABC.",
                    "label": 0
                },
                {
                    "sent": "So an undirected graphical model is not as as fine grained in really specifying the factorization that the joint probability solution might have.",
                    "label": 0
                },
                {
                    "sent": "It's it's convenient, so we could also summarize.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's convenient for specifying local couplings.",
                    "label": 0
                },
                {
                    "sent": "It's convenient for actually reading of dependencies and dependencies, but it's not as fine grained and powerful as a factor graph.",
                    "label": 0
                },
                {
                    "sent": "Now, how does affect the graph relate to base law?",
                    "label": 0
                },
                {
                    "sent": "Everyone knows baseless pose.",
                    "label": 0
                },
                {
                    "sent": "So base law relates in fact this deep mathematical inference system, and it says that the belief in the parameters given the data is up to normalization.",
                    "label": 0
                },
                {
                    "sent": "A function of the belief in the data given the parameters which is known as the likelihood, is a function of the parameters times the prior belief belief without any data.",
                    "label": 0
                },
                {
                    "sent": "So if you more concrete if S here specifies the skill of Scott and me, so it's a vector of two numbers, then based law says that the probability of.",
                    "label": 0
                },
                {
                    "sent": "Scott scale of my skill given the match outcome Y between our match, that's going to happen is going to be a probability of the match outcome given our skills times the prior probability of our skills that space law and based law is basically the simplest possible factor graph.",
                    "label": 0
                },
                {
                    "sent": "It has two factors into variables.",
                    "label": 0
                },
                {
                    "sent": "One variable is the data and I shaded it a bit.",
                    "label": 0
                },
                {
                    "sent": "The wife and one factor is a one variable.",
                    "label": 0
                },
                {
                    "sent": "Is the parameters skills here.",
                    "label": 0
                },
                {
                    "sent": "So the way that we get from base law to a complex vector graph is that we actually start to model in forward fashion.",
                    "label": 0
                },
                {
                    "sent": "So how could we make a model for for our skills ending up in the match outcome?",
                    "label": 0
                },
                {
                    "sent": "Well, one is we could first of all assume that the skill of Scott and the skill of me are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "So that just means that this vector graphic just head splits up in two nodes as one as two.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we would.",
                    "label": 0
                },
                {
                    "sent": "We could assume that we actually going to have.",
                    "label": 0
                },
                {
                    "sent": "Some factorizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that the skill of Scott in the skill of me lead to a performance that we have in a match, then there is a coupling between our performance and our skill, and then there is a difference in our performance that's actually in demonistic factor D. That's the difference of being SP1 and P2T1 and T2.",
                    "label": 0
                },
                {
                    "sent": "And then depending on the sign of the difference.",
                    "label": 0
                },
                {
                    "sent": "Scott Windsor, I win.",
                    "label": 0
                },
                {
                    "sent": "That's a reasonable model.",
                    "label": 0
                },
                {
                    "sent": "And what happened now is that that simple factor graph we had just had two variables and two.",
                    "label": 0
                },
                {
                    "sent": "Vectors and having a sort of starting to become a very interesting structure.",
                    "label": 0
                },
                {
                    "sent": "What is inference?",
                    "label": 0
                },
                {
                    "sent": "As I said, inferences summing all the latent variables.",
                    "label": 0
                },
                {
                    "sent": "What are the latent variables in here?",
                    "label": 1
                },
                {
                    "sent": "Well, the performance and the D because we're not really interested in performance were interested in Scott skill in my school, and we have reserved white.",
                    "label": 0
                },
                {
                    "sent": "So inferences release coming out, the summing over all values.",
                    "label": 0
                },
                {
                    "sent": "Overall states of the T variables and the D variable.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, then we perform inference because that's effectively computing the posterior as a function of the skills.",
                    "label": 0
                },
                {
                    "sent": "The function of the parameters.",
                    "label": 0
                },
                {
                    "sent": "So to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this sort of three major types of graphical models.",
                    "label": 0
                },
                {
                    "sent": "One is used for modeling that's directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "It can't have loops.",
                    "label": 0
                },
                {
                    "sent": "It has ancestral relationships, but it's very good for forward flow where you know, I assume some, some state, some parameter interest me and they in a forward processing flow generated data.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to log or observed.",
                    "label": 0
                },
                {
                    "sent": "Then there is Markov networks, undirected graphical models they use.",
                    "label": 1
                },
                {
                    "sent": "They are very useful for local couplings potential.",
                    "label": 1
                },
                {
                    "sent": "So when you have sound speech, temporal data very powerful because you can couple over.",
                    "label": 0
                },
                {
                    "sent": "You couple in some locality and then there's factor graphs.",
                    "label": 0
                },
                {
                    "sent": "They're not very good in reading of dependencies, so I kind of rushed over this, but they're very powerful when it comes to efficient inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's pick out this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example actually, so let's look at how we do.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inference, so here's that graph that we just looked at turned on its head.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have in this one we have 5 variables, VWXY&Z and four factors F1F2F3F four by the way, in case you haven't noticed, I'm going to use the pictures a lot, but I never use algebra in blue, so the algebraic expression that's at the bottom.",
                    "label": 0
                },
                {
                    "sent": "OK, so the joint distribution over the five variables is a product of four vectors F1F2F3F four.",
                    "label": 0
                },
                {
                    "sent": "So suppose we're interested in W. This is our.",
                    "label": 0
                },
                {
                    "sent": "This is a variable that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "The parameter of course, so we need to sum over everything that is latent.",
                    "label": 0
                },
                {
                    "sent": "So in this in this example, that would be the XY and Z OK.",
                    "label": 0
                },
                {
                    "sent": "So if we did it the naive way, that's how we do it.",
                    "label": 0
                },
                {
                    "sent": "We just some overall states of Y or all states of X over all states of Y.",
                    "label": 0
                },
                {
                    "sent": "Overall states of Z.",
                    "label": 0
                },
                {
                    "sent": "So if these are just binary variables that would be with four variables, 16 summations, so we have it.",
                    "label": 0
                },
                {
                    "sent": "We actually looking at a sum with 16 summons and each of the summons is a product of four terms.",
                    "label": 0
                },
                {
                    "sent": "OK. That's the naive way of doing it.",
                    "label": 0
                },
                {
                    "sent": "If we had millions of variables, that's not, we can't do it naively.",
                    "label": 0
                },
                {
                    "sent": "Now, if you wanted some if you want to simplify that some.",
                    "label": 0
                },
                {
                    "sent": "Then on in algebra as well as in the picture, we notice one thing which is we're interested in a function of W. There really are only two factors that involved W, F1 and F2.",
                    "label": 0
                },
                {
                    "sent": "The other factors do not involve W. So what that means is I can actually use the distributive law and I can pull the F1 term all the way out to V. And I'll leave the other ones on the other side.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I've just done in algebra, I've just.",
                    "label": 0
                },
                {
                    "sent": "Use the distributive law so.",
                    "label": 0
                },
                {
                    "sent": "In other words, if I make this simpler, but all I used is that.",
                    "label": 0
                },
                {
                    "sent": "A + B * C = 8 * C + B * C. OK, so previously I had a sum of products.",
                    "label": 1
                },
                {
                    "sent": "And in those products see occur twice and I just pulled that out and left that some here.",
                    "label": 0
                },
                {
                    "sent": "That's that's the step I was just doing in algebra OK. Why could that be useful?",
                    "label": 0
                },
                {
                    "sent": "Well, let's count the amount of computation that happened here.",
                    "label": 0
                },
                {
                    "sent": "This is where I started from.",
                    "label": 0
                },
                {
                    "sent": "So how many computations do I have?",
                    "label": 0
                },
                {
                    "sent": "One multiplication, one modification?",
                    "label": 0
                },
                {
                    "sent": "One additions are three steps, arithmetic to do here.",
                    "label": 0
                },
                {
                    "sent": "How many do I have here?",
                    "label": 0
                },
                {
                    "sent": "I have one addition, one multiplication, so I have to.",
                    "label": 0
                },
                {
                    "sent": "OK, so by applying this this distributive law I went from 3 computational steps to two.",
                    "label": 0
                },
                {
                    "sent": "Now I'm doing this at scale.",
                    "label": 0
                },
                {
                    "sent": "I'm doing this again and again.",
                    "label": 0
                },
                {
                    "sent": "This is a 50% reduction.",
                    "label": 0
                },
                {
                    "sent": "Sorry, 33% reduction in computation and depending on where doing the graph, you'll see that that leads to very very efficient computation.",
                    "label": 0
                },
                {
                    "sent": "So I end up with is these two partial terms here.",
                    "label": 0
                },
                {
                    "sent": "So I end up with the partial sum of W that is summing over V, But the first term and I end up with the second term, the second partial somewhere some over XY and Z. OK.",
                    "label": 0
                },
                {
                    "sent": "So if I give a name to this if I just say these are still functions of W. So let me call them messages.",
                    "label": 0
                },
                {
                    "sent": "This is a partial function.",
                    "label": 1
                },
                {
                    "sent": "Then what I end up seeing here is that the first rule of the sum product algorithm is that the marginal of a variable is the product of all incoming messages.",
                    "label": 0
                },
                {
                    "sent": "And the message being the partial sum of of the neighboring factors.",
                    "label": 0
                },
                {
                    "sent": "Overall, variables that are in the in those parts of the parts of the tree.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like made W the root of a tree.",
                    "label": 0
                },
                {
                    "sent": "And if I sum over all variables in the left part and overall virus in the right part and find more and more, I just need to multiply those functions of W together.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The message what is the form of the message from Affecter to available?",
                    "label": 0
                },
                {
                    "sent": "So I just left it formally.",
                    "label": 0
                },
                {
                    "sent": "I just left it standing here.",
                    "label": 0
                },
                {
                    "sent": "That's actually still a sum over XY and Z.",
                    "label": 0
                },
                {
                    "sent": "What I see now is that F2, the factor of 2 doesn't depend on Y&Z.",
                    "label": 0
                },
                {
                    "sent": "It just depends on X.",
                    "label": 0
                },
                {
                    "sent": "So I can do the same trick again, I can just use the distributive law again.",
                    "label": 0
                },
                {
                    "sent": "So if I do that, it means that I can pull F2 out.",
                    "label": 0
                },
                {
                    "sent": "To the first summation, so I somewhat over X, but I don't some out over Y&Z yet I bundle these together, they become waiting effect rating terms.",
                    "label": 0
                },
                {
                    "sent": "In fact, same trick applies.",
                    "label": 0
                },
                {
                    "sent": "Again, I could introduce a shorthand notation for that partial sum.",
                    "label": 0
                },
                {
                    "sent": "This is a partial sum and the partial sum.",
                    "label": 0
                },
                {
                    "sent": "As a function of X OK, so find other variables that connect to F2.",
                    "label": 0
                },
                {
                    "sent": "I would continue having partial sums over those variables as well, so the rule is that in order to compute the message from affect that are available, I just need to some out over all the other variables that are attaching to that factor, and I need to wait by a message.",
                    "label": 0
                },
                {
                    "sent": "That is the buyer by a partial sum, that is, the message from that variable to the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what's the functional form of that here?",
                    "label": 0
                },
                {
                    "sent": "I see again if I look at the message from X to Y from X to F2.",
                    "label": 0
                },
                {
                    "sent": "That's the formal definition.",
                    "label": 0
                },
                {
                    "sent": "I see that that's just basically summing out over the neighbors of that variable that are not F20XX occurs in three factors in F2 and F3 and F4.",
                    "label": 0
                },
                {
                    "sent": "And if I want to compute the message, what's the function of the message from the variable X2F2?",
                    "label": 0
                },
                {
                    "sent": "It's coming out over the message that comes from F3 and the partial sum of three and partial sum of the four.",
                    "label": 0
                },
                {
                    "sent": "I applied the same district of law again.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I put those three together?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I actually have the sum product algorithm, so some product algorithm is literally three iterations.",
                    "label": 0
                },
                {
                    "sent": "That kind of peel a tree.",
                    "label": 0
                },
                {
                    "sent": "To its leaves you pick an arbitrary node in the tree.",
                    "label": 0
                },
                {
                    "sent": "That's the tree that describes your joint probability model, and then you peel all the way to the leaves of the tree and back OK.",
                    "label": 0
                },
                {
                    "sent": "So what's nice about that is.",
                    "label": 0
                },
                {
                    "sent": "I I I can actually compute all the marginals at the same time when I have all the messages, because the rules apply anywhere in the tree, I picked an arbitrary node W, not just, you know, constructively arrived at, so that means that the amount of computation I need to do and the amount of storage is really only proportional to the number of messages.",
                    "label": 1
                },
                {
                    "sent": "Now, how many messages did I have always had a message on an edge, so if I have a tree, if I have a factor graph with millions of variables in.",
                    "label": 0
                },
                {
                    "sent": "Millions of vectors.",
                    "label": 0
                },
                {
                    "sent": "The complexity is really just the number of edges, not the exponential number of states of the variables.",
                    "label": 0
                },
                {
                    "sent": "In fact, if the factors connect a few variables, it's a very efficient algorithm because it only needs to need to compute a quantity called a message along the edges of that factor graph.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One trick that people often do is they actually express this.",
                    "label": 0
                },
                {
                    "sent": "These update rules in in log form, so you take the log of each of these messages in the log of the marginal probabilities.",
                    "label": 0
                },
                {
                    "sent": "If the log probability available, it's the sum of the log messages here in order by Lambda and the log messages are coming out.",
                    "label": 0
                },
                {
                    "sent": "The actual factor, weighted by the exponentially exponentiated some of the incoming log messages.",
                    "label": 0
                },
                {
                    "sent": "Now what's nice about if you look at the 1st and the third rule.",
                    "label": 0
                },
                {
                    "sent": "There are totally closed there actually just summations, and if your messages are in the exponential family, so your messages are E to a natural statistics of the parameter T, the function times the parameter vector Theta, then the 1st and the third rule are simply just additions of the theater values.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if the message is on the exponential family, then computing the marginal and computing the message from a variable to affecter are just additions.",
                    "label": 1
                },
                {
                    "sent": "There's not even multiplications.",
                    "label": 0
                },
                {
                    "sent": "Most what is in the exponential family quite.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distributions are in there just to highlight a few reductions or any exponential family, and the parameters Theta for Gaussian is not the mean variance, it's the mean over the variance in one order variance called the precision mean in the precision.",
                    "label": 0
                },
                {
                    "sent": "Bernoulli, binomial, beta, gamma, vichard, like all these distributions on the exponential family.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's that's a nice trick.",
                    "label": 0
                },
                {
                    "sent": "So that means that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we are using messages that are in the exponential family, we're ending up having just to do additions and the number of additions is just proportional to the number of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "Secondly,",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use the second trick, and that's to do with not performing redundant computations.",
                    "label": 1
                },
                {
                    "sent": "So pick an arbitrary variable and pick an arbitrary neighboring factor.",
                    "label": 0
                },
                {
                    "sent": "So here pick T and then affected it's a neighbor of T. Then the first rule says that the marginal of T is the product of all incoming messages, so in pictures it's all the red errors together.",
                    "label": 0
                },
                {
                    "sent": "And the second rule says that if I look at what is the message from the variable to the factor?",
                    "label": 0
                },
                {
                    "sent": "OK from T to F. That's actually the incoming other messages.",
                    "label": 0
                },
                {
                    "sent": "So in pictures is that?",
                    "label": 0
                },
                {
                    "sent": "OK, so if I repeat that that simulation, the marginal of T is the product of all incoming messages.",
                    "label": 0
                },
                {
                    "sent": "The message from T to F is the product of all incoming message, except that one you see that one thing is true, but for any pair of variable and neighboring message.",
                    "label": 0
                },
                {
                    "sent": "Sorry, bearable and neighboring factor.",
                    "label": 0
                },
                {
                    "sent": "You have the quality that the marginal is the product of the incoming and outgoing message.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The marginal was the product of all of them, and the outgoing message was the program all but the one missing one is the incoming one.",
                    "label": 0
                },
                {
                    "sent": "From that from that factor.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "It means that when you actually implement this in in computer, you're not going.",
                    "label": 0
                },
                {
                    "sent": "You don't need to store the messages from the variable to the factors.",
                    "label": 0
                },
                {
                    "sent": "You don't need to compute them whenever you need the message from a variable to affect her, just the weighting factor was the waiting term in the in the second update rule.",
                    "label": 0
                },
                {
                    "sent": "You can get it by the vision.",
                    "label": 0
                },
                {
                    "sent": "You can just divide.",
                    "label": 0
                },
                {
                    "sent": "The marginal by the message from the factor to the variable.",
                    "label": 0
                },
                {
                    "sent": "And if we're in the exponential family, the product of 2 messages was adding the theaters so that.",
                    "label": 0
                },
                {
                    "sent": "The ratio of two messages or margin on the message is that it's just a division.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it's just a subtraction, so these are very elementary operations you add in the item subtract.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now that's that's all very nice.",
                    "label": 0
                },
                {
                    "sent": "And actually when you when you look in the literature, if you start with messages in the exponential family then there is a limited set of factors that actually leave you leave you in the exponential family.",
                    "label": 0
                },
                {
                    "sent": "But in reality when you really want to model some nonlinear dependencies, then the 2nd update rule is regularly going to cut up, pulled you out of the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's one kind of appointment.",
                    "label": 0
                },
                {
                    "sent": "Set pointer center or.",
                    "label": 0
                },
                {
                    "sent": "Stick the stick somewhere, no OK?",
                    "label": 0
                },
                {
                    "sent": "Well then you have to believe me.",
                    "label": 0
                },
                {
                    "sent": "See if I can do it the projector.",
                    "label": 0
                },
                {
                    "sent": "No this one.",
                    "label": 0
                },
                {
                    "sent": "Right, this one has the problem that.",
                    "label": 0
                },
                {
                    "sent": "Even though the 1st and the 2nd one.",
                    "label": 0
                },
                {
                    "sent": "Thank you so this one.",
                    "label": 0
                },
                {
                    "sent": "If this is an exponential family member then this is exponential family.",
                    "label": 0
                },
                {
                    "sent": "If this one is exponential family product of them is expensive family, but for arbitrary functions F and we're going to see one in a minute.",
                    "label": 0
                },
                {
                    "sent": "If this is a exponential family then this is an exponential family, but something out here might sort of.",
                    "label": 0
                },
                {
                    "sent": "Catapult you out of the exponential family and then that whole recursion would break.",
                    "label": 0
                },
                {
                    "sent": "And so in practice.",
                    "label": 0
                },
                {
                    "sent": "The reason this hasn't been so popular is that there wasn't really a way other than other than look.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the what's known as the conjugacy, the family of conjugate priors to really apply this technique at scale now.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Around 2001 or run 2000 Tomenga came up with a nice solution to this, and the idea is very very simple.",
                    "label": 0
                },
                {
                    "sent": "When you remember the last trick we just did the last trick we just did said that the marginal of a variable is always.",
                    "label": 0
                },
                {
                    "sent": "You can pick an arbitrary neighboring factor F. Then the marginal is is always the product of the incoming message from that variables and the outgoing message to that variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is always the marginal.",
                    "label": 0
                },
                {
                    "sent": "What I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "What I'm interested in is is is the marginal right?",
                    "label": 0
                },
                {
                    "sent": "You know I'm interested in the system, the machine, the data center, learning about the belief of Scott skill or one of the other 50 million members skills or so forth.",
                    "label": 0
                },
                {
                    "sent": "So if I approximate the margin notes, well, so all I need to do is I pick a distance measure between probability distributions.",
                    "label": 0
                },
                {
                    "sent": "And I pick one such that P hat is again in the exponential family.",
                    "label": 0
                },
                {
                    "sent": "Then I do an approximation step on the marginals, but I can compute the approximate message from the factor to the variable by dividing the approximate marginal by the approximate by the actually approximate message from the verbal to the factor.",
                    "label": 0
                },
                {
                    "sent": "I'm just using the fact very quality that PFT is incoming outgoing message.",
                    "label": 0
                },
                {
                    "sent": "I approximate the margin.",
                    "label": 0
                },
                {
                    "sent": "That's the thing I want approximately 1 approximate the messages there means to an end.",
                    "label": 0
                },
                {
                    "sent": "Hillary there just partial sum and then I once I've computed approximate marginal.",
                    "label": 0
                },
                {
                    "sent": "I'm actually just getting by division that approximate message let me show you some pictures.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, suppose we assume a Gaussian belief over my skin over scope skill.",
                    "label": 0
                },
                {
                    "sent": "Then the difference of our skills is also going to be ocean.",
                    "label": 0
                },
                {
                    "sent": "Let's the blue curve, so the message coming into from the difference variable D in the first slide down to the fact that describes our outcome.",
                    "label": 0
                },
                {
                    "sent": "That is a very nice question.",
                    "label": 0
                },
                {
                    "sent": "Now we have a match outcome.",
                    "label": 0
                },
                {
                    "sent": "Scotland's so that means the difference must have been positive.",
                    "label": 0
                },
                {
                    "sent": "So that's the data.",
                    "label": 0
                },
                {
                    "sent": "The data tells us the difference with positive, so that's a step function.",
                    "label": 0
                },
                {
                    "sent": "What is the posterior of the?",
                    "label": 0
                },
                {
                    "sent": "It's the product of the incoming outgoing message, so it's a truncated Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's the true posterior over the difference.",
                    "label": 0
                },
                {
                    "sent": "That's not a Gaussian anymore, so we couldn't really use approximately a message passing and workout the message.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we approximate the truncated Gaussian with the Gaussian, and if we use the KL divergent, then the Public Library vergence then ends up being just matching the mean and the variance of a truncated Gaussian that is known in closed form.",
                    "label": 0
                },
                {
                    "sent": "Closed form is known if you truncated Gaussian at a given point, that what is the mean and the variance of the trunk in Goshen.",
                    "label": 0
                },
                {
                    "sent": "So we match that.",
                    "label": 0
                },
                {
                    "sent": "So we end up with a black Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Again, that's the approximate.",
                    "label": 0
                },
                {
                    "sent": "Marginal of the difference variable.",
                    "label": 1
                },
                {
                    "sent": "The incoming message was discussion, so what's the approximate message that describes the step function very well for the purpose of inference?",
                    "label": 0
                },
                {
                    "sent": "Well, it's the blue curve.",
                    "label": 0
                },
                {
                    "sent": "The black Earth divided by the blue curve because the black curve is always the product of the blue curve and the red curve that is in pictures.",
                    "label": 0
                },
                {
                    "sent": "But I just described.",
                    "label": 0
                },
                {
                    "sent": "And if you divide them, you end up with a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Now previously people tried to approximate messages, but that is a horrible approximation to the message when you think of it.",
                    "label": 0
                },
                {
                    "sent": "'cause it's zero, but this is 1 in most of the most of the cases, but the message is just an auxiliary quantities, just an auxiliary partial sum.",
                    "label": 0
                },
                {
                    "sent": "So this is actually going to iterate that scheme of approximating the messages.",
                    "label": 0
                },
                {
                    "sent": "That way you actually approximating the marginals.",
                    "label": 0
                },
                {
                    "sent": "The variables of interest, the skill barriers in this case, and you end up with a very good approximation and.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a whole field that's actually studying.",
                    "label": 0
                },
                {
                    "sent": "How is the?",
                    "label": 0
                },
                {
                    "sent": "How is the algorithms emerge if you use different distance functions?",
                    "label": 0
                },
                {
                    "sent": "So the one I was just using is the KL divergent, such that P is the true distribution, meaning I'm using the step function times the Gaussian and approximating it with Q, which is a real Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But there is a more general set of divergencies, so as I said in a set already, if I minimize the KL divergences a very it's very easy to show that that's actually matching the moments of the distribution PFT.",
                    "label": 1
                },
                {
                    "sent": "There is a generalization of this known as the Alpha Divergent and the Alpha divergences.",
                    "label": 0
                },
                {
                    "sent": "A parameter Alpha is the formal definition of the other versions have been two distributions, P&Q and you can show that one of them special cases is at zero.",
                    "label": 0
                },
                {
                    "sent": "It's the color vergence.",
                    "label": 0
                },
                {
                    "sent": "Sorry I'd wanna stick elevations as we headed and at zero it's the other way around.",
                    "label": 0
                },
                {
                    "sent": "What's the difference?",
                    "label": 0
                },
                {
                    "sent": "I think again it's easier to describe in picture.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and I recommend reading up.",
                    "label": 0
                },
                {
                    "sent": "This is a 2005 paper by Tomenga.",
                    "label": 0
                },
                {
                    "sent": "Imagine the blue one is the true one.",
                    "label": 0
                },
                {
                    "sent": "We want to approximate.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this blue.",
                    "label": 0
                },
                {
                    "sent": "Distribution is symmetric around 0.",
                    "label": 0
                },
                {
                    "sent": "So that's the true one.",
                    "label": 0
                },
                {
                    "sent": "And now we want to find that.",
                    "label": 0
                },
                {
                    "sent": "Gaussian that's closest under the elder vergence.",
                    "label": 0
                },
                {
                    "sent": "The way around that I just described matching moments that's easy.",
                    "label": 0
                },
                {
                    "sent": "The mean of that blue curve of the blue distribution is zero and the variance is pretty wide, so that's the minimizer of the D1.",
                    "label": 0
                },
                {
                    "sent": "This distance that ends up in an algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you apply that repeatedly in the graph, you get this album known as expectation propagation.",
                    "label": 0
                },
                {
                    "sent": "If you use the zero, so you minimize the collective urgency.",
                    "label": 0
                },
                {
                    "sent": "Other way around, this is actually the minimizer, meaning it's on the second one that's directly sitting on top of here.",
                    "label": 0
                },
                {
                    "sent": "That's the function that's actually a distribution that doesn't match the moments, but it is such that the under the green curve you have most of the you have mass on the blue curve, but not necessarily under the blue curve mass of the green curve.",
                    "label": 0
                },
                {
                    "sent": "So you jump on the mode and if you look in the literature, there's like a whole body.",
                    "label": 0
                },
                {
                    "sent": "The name for this in the literature is variational Bayes.",
                    "label": 0
                },
                {
                    "sent": "So depending on what that Alpha is 01 there's other ones.",
                    "label": 0
                },
                {
                    "sent": "Other other ranges that are used as well for Alpha that lead to different algorithms.",
                    "label": 0
                },
                {
                    "sent": "It's a choice.",
                    "label": 0
                },
                {
                    "sent": "It's basically just the choice of approximation and we will see in a Matchbox case that this is relevant.",
                    "label": 0
                },
                {
                    "sent": "You might argue why would you ever want to do that?",
                    "label": 0
                },
                {
                    "sent": "The reason why I want to do that because you might have symmetries in your posterior simply because it's it's the same solution, but there's no symmetry breaker in your prioritization, and then this distance measure breaks the symmetry where this one would retain the symmetry and would actually get more uncertain as you started from.",
                    "label": 0
                },
                {
                    "sent": "Not the D1 or D1D zero measure is not always the appropriate measure.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's really.",
                    "label": 0
                },
                {
                    "sent": "I think that's really at the heart of distributed at approximate message passing, you start with some product algorithm and then whenever you encounter a factor that is sort of cut that pulled you out of the class of exponential family that you're going to use you project back by picking a distance measure so you.",
                    "label": 0
                },
                {
                    "sent": "The recipe release you describe your data and a forward graphical model.",
                    "label": 0
                },
                {
                    "sent": "When you do that, then you pick a distance measure.",
                    "label": 0
                },
                {
                    "sent": "If you need to approximate the inference and that results in algorithm and we'll see several in the second part of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Now the last problem I want to talk about is distributed message.",
                    "label": 1
                },
                {
                    "sent": "And why is that important?",
                    "label": 0
                },
                {
                    "sent": "I kind of touched upon it at the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's important because the amount of data that that we have available to learn from is really growing up a lot faster is really growing a lot faster than the amount of computer becomes available.",
                    "label": 0
                },
                {
                    "sent": "It's something that people have referred to as acts law, or is it covers more.",
                    "label": 0
                },
                {
                    "sent": "It is true so far that data roughly doubles in every domain it's to do with the sensors that were available.",
                    "label": 0
                },
                {
                    "sent": "So if you look at Facebook's news feed and this is slightly out of date data, so this is publicly available.",
                    "label": 0
                },
                {
                    "sent": "If you have, you know 100 billion training examples on a single day, and you have.",
                    "label": 0
                },
                {
                    "sent": "In terms of variables, let's say you just want to learn in a Facebook is a person's personal network social network on people.",
                    "label": 0
                },
                {
                    "sent": "You have, at least if you had one parameter per user just to buy us, you would have 650 million parameters to change per day.",
                    "label": 0
                },
                {
                    "sent": "And if you go over a month, it's a 1.1 billion or 1.3 billion things.",
                    "label": 0
                },
                {
                    "sent": "Latest figure.",
                    "label": 0
                },
                {
                    "sent": "If you look at the social graph you want to learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say you want to learn a friends recommender that's predicting an edge in that network.",
                    "label": 0
                },
                {
                    "sent": "You can learn from 130 billion edges today and you have 1 billion.",
                    "label": 0
                },
                {
                    "sent": "Users, if you simply had a bias again per user, not even taking into account the user user correlation.",
                    "label": 0
                },
                {
                    "sent": "If you look at Google's Pagerank, you have 4 trillion Web Links, 1 trillion web pages.",
                    "label": 0
                },
                {
                    "sent": "If you look at Amazon's forecasting so peak day, this is last year December on a single day people.",
                    "label": 0
                },
                {
                    "sent": "Got shipped 15.6 million products on a single day.",
                    "label": 0
                },
                {
                    "sent": "There's a peak days, probably more this year, so that's at least that many examples you can train from in a single day.",
                    "label": 0
                },
                {
                    "sent": "You know which person or which product, which are of the day delivered to 1 ship address.",
                    "label": 0
                },
                {
                    "sent": "You have 20,000,000 minimum have actually more 20,000,000 products and these are just products at Amazon orders in the fulfillment center.",
                    "label": 0
                },
                {
                    "sent": "If you include the products that Amazon stores and fulfills for others, it's in the range of three to 400 million.",
                    "label": 0
                },
                {
                    "sent": "So if you're a single variable that describes the propensity divider product, not even talking about a product user grab, you end up with more parameters than fit in a single machine memory.",
                    "label": 0
                },
                {
                    "sent": "I could go on and on, but that's these.",
                    "label": 0
                },
                {
                    "sent": "Figures are real and these figures keep growing at almost 50% in a 5200% in a year.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is.",
                    "label": 0
                },
                {
                    "sent": "If you think of mapping this computation, these additions and multiplications on a single computer, you could say, OK, you know at the core level we use, GPU's is going to be really fast well, but they still only has 86,000 seconds.",
                    "label": 0
                },
                {
                    "sent": "So if I have 100 billion examples to train from, that's a million training examples per second.",
                    "label": 0
                },
                {
                    "sent": "If I access ram, yeah, it's very very fast to access random access memory in a computer, but you still cannot access in a read way more than 10 to 13 different elements.",
                    "label": 0
                },
                {
                    "sent": "And I'm talking raw memory.",
                    "label": 0
                },
                {
                    "sent": "X is if I want to write it's one order of magnitude less and tend to the 12 is not that big.",
                    "label": 0
                },
                {
                    "sent": "So even if I try to squeeze in a big computer this is a it's bigger.",
                    "label": 0
                },
                {
                    "sent": "So I have a few variables per Pew factors for variables.",
                    "label": 0
                },
                {
                    "sent": "This is bigger or single machine can do and if I if I want to distribute this stuff I need to also take into account that.",
                    "label": 0
                },
                {
                    "sent": "I can really, really only read into a computer a TB per day.",
                    "label": 0
                },
                {
                    "sent": "Any faster you need, much faster, faster Internet, faster connections.",
                    "label": 0
                },
                {
                    "sent": "So in practice you really need to think of.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But distributing the amount of data or it's stored and distributing the amount of variables because it's exceeding single machine capacity these days?",
                    "label": 0
                },
                {
                    "sent": "In fact, all these variables I was talking about there not generate on a single machine that generated on thousands and 10s of thousands of servers.",
                    "label": 0
                },
                {
                    "sent": "So let's look at a model and it's very, very typical distributed conditional model.",
                    "label": 1
                },
                {
                    "sent": "But that doesn't mean I'm not specifying what that is.",
                    "label": 0
                },
                {
                    "sent": "It means I have data.",
                    "label": 0
                },
                {
                    "sent": "My data is why?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "This could be match outcomes for Xbox Live.",
                    "label": 0
                },
                {
                    "sent": "Or it could be bought or not bought customer bought or not bought a product.",
                    "label": 0
                },
                {
                    "sent": "Or it could be friends recommendation.",
                    "label": 0
                },
                {
                    "sent": "You know Ralph got these 10 people recommended when you log into Facebook and then he chose to friend this person.",
                    "label": 0
                },
                {
                    "sent": "I didn't choose to friend that other person.",
                    "label": 0
                },
                {
                    "sent": "That's why X is everything that describes the situation on our of the day, what page it was, what region, what MarketWatch IP address.",
                    "label": 0
                },
                {
                    "sent": "What's the gamer ID?",
                    "label": 0
                },
                {
                    "sent": "All this information and theater is our parameters.",
                    "label": 0
                },
                {
                    "sent": "So now all I'm saying is that I assume that the probability of my data is conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "So if I condition with the parameters, then every observed sale friends recommendation click share comment on Facebook or or match outcome is independent of each other.",
                    "label": 0
                },
                {
                    "sent": "Given I know the contextual situation that all data on the context and other parameters.",
                    "label": 0
                },
                {
                    "sent": "And I have a prior over the machine maintains a prior over the order parameters.",
                    "label": 0
                },
                {
                    "sent": "So the factor graph to this?",
                    "label": 0
                },
                {
                    "sent": "What's the factor graph?",
                    "label": 0
                },
                {
                    "sent": "Well, we have observed Y. X is conditioned on always, so I just don't even include it.",
                    "label": 0
                },
                {
                    "sent": "You could include X here as well, but it's it's conditioned on it and we never observe.",
                    "label": 0
                },
                {
                    "sent": "We never reason about it.",
                    "label": 0
                },
                {
                    "sent": "And we have data.",
                    "label": 0
                },
                {
                    "sent": "So here, speaker, and here's the wise.",
                    "label": 0
                },
                {
                    "sent": "Now I assume it's vectorizing, so that means that's actually the picture.",
                    "label": 0
                },
                {
                    "sent": "So the picture to keep in mind for distributed inference or distribute approximate inference is that we have this big graph where there's a huge amount of data items, 100 billion in Facebook's case per day.",
                    "label": 0
                },
                {
                    "sent": "And there's a huge amount of parameters, and they're connected whenever that.",
                    "label": 0
                },
                {
                    "sent": "Whenever that likelihood term.",
                    "label": 0
                },
                {
                    "sent": "Involves a given parameter Theta, for a given data item Yi.",
                    "label": 0
                },
                {
                    "sent": "OK, so not every data touches every parameter, so if this is a per user, let's say these are all parameters that are per user propensity to friend someone, or for the propensity to even like the story, then if this was Ralph then disconnects rules parameter.",
                    "label": 0
                },
                {
                    "sent": "This was Scott Scott's parameter and so forth, so this connectivity matrix is in general sparse.",
                    "label": 0
                },
                {
                    "sent": "In fact you'll see later that in some models this is usually.",
                    "label": 0
                },
                {
                    "sent": "This can be a.",
                    "label": 0
                },
                {
                    "sent": "Rather constant or very small number of outdegree, meaning that you have fixed number of features.",
                    "label": 0
                },
                {
                    "sent": "So every one of your data items connects to a fixed number of variables in their likelihood.",
                    "label": 0
                },
                {
                    "sent": "But you might have a power law kind of distribution at the out degree of the parameters to the to the data because some users just are unproportionally, often excess Facebook versus others that are just coming once a month.",
                    "label": 0
                },
                {
                    "sent": "So the idea of distributed message passing is actually when pictures very easy to describe, which is affective graph is a graph between variables.",
                    "label": 0
                },
                {
                    "sent": "Bipartite graph between variables and their relation to each other.",
                    "label": 0
                },
                {
                    "sent": "So all we can do is we just regroup this graph.",
                    "label": 0
                },
                {
                    "sent": "So if we regroup the factors together that are describing our data.",
                    "label": 0
                },
                {
                    "sent": "And instead of having seven data items, we can say we have three, and this is a little more complex vector.",
                    "label": 0
                },
                {
                    "sent": "This vector needs to now compute the outgoing messages to all the variables by taking all these three items into account.",
                    "label": 0
                },
                {
                    "sent": "All these two and all these two.",
                    "label": 0
                },
                {
                    "sent": "So if you map this two computers, then this is what you might call the mappers.",
                    "label": 1
                },
                {
                    "sent": "In MapReduce, these machines that are computing locali independent of each other, the outgoing messages 'cause in message passing here will just be.",
                    "label": 0
                },
                {
                    "sent": "You know what would be the posterior each parameter, the posterior of each parameter.",
                    "label": 0
                },
                {
                    "sent": "Is the product of all incoming messages is the prior times?",
                    "label": 0
                },
                {
                    "sent": "Messages along these edges, but these boxes can compute these messages independent of each other.",
                    "label": 0
                },
                {
                    "sent": "And you can do the same grouping used this time around of variables for the variables, so you end up if I put this back in algebra.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You end up basically with taking this big product over data and big product of a parameters into a product of product of data into a product of product of parameters.",
                    "label": 0
                },
                {
                    "sent": "And this one I might call a chunk of memory store.",
                    "label": 0
                },
                {
                    "sent": "I believe store sing the machine holding a subset of the of the parameters in memory and this one is a single machine storing that subset of data, either because it arrives there or in map reduce it got stored there.",
                    "label": 0
                },
                {
                    "sent": "OK, so are we really done algebraically thinking algebra, it's also easy to describe this.",
                    "label": 0
                },
                {
                    "sent": "We just went from one factor graph that had too many variables.",
                    "label": 0
                },
                {
                    "sent": "There are too many parameters and too many data into another factor graph that now has a controllable number of factors over data and factor or parameters, and we're mapping these factors onto single machines now.",
                    "label": 0
                },
                {
                    "sent": "So that means that that product and all the message exchange between them is now communication on a network.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just another way of mapping mapping the multiplication operation on two systems.",
                    "label": 0
                },
                {
                    "sent": "This time around we the communication between them is actually a network.",
                    "label": 0
                },
                {
                    "sent": "There's some interesting, interesting thing here.",
                    "label": 0
                },
                {
                    "sent": "The data factors.",
                    "label": 0
                },
                {
                    "sent": "Those will always note which parameters to send a message, that's easy.",
                    "label": 0
                },
                {
                    "sent": "Because that's what's specified in the data.",
                    "label": 0
                },
                {
                    "sent": "Romita factors one have that property because the parameter factors will receive messages from the data, but they don't encode unless they explicitly store where that data is coming from.",
                    "label": 0
                },
                {
                    "sent": "They don't really have a.",
                    "label": 0
                },
                {
                    "sent": "They don't really have have this encoded naturally, so the way.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this is actually implemented in practice and were describing.",
                    "label": 0
                },
                {
                    "sent": "Here is a system that was in user Facebook for we didn't train the 10 billion dimensional model over 20 machines memory and 100 machines.",
                    "label": 0
                },
                {
                    "sent": "Processing the data is is putting the factors for data into one fleet of machines and that might be Geo distributed because the global service and one set of machines that's actually storing all the parameters for everything.",
                    "label": 0
                },
                {
                    "sent": "The machine, let's say there's four of them in each.",
                    "label": 0
                },
                {
                    "sent": "Into region, and there's one in each region has now a subset of logical units that are the groups of the grouping of the factors parameter and data.",
                    "label": 0
                },
                {
                    "sent": "And the communication protocol very simple.",
                    "label": 0
                },
                {
                    "sent": "Whenever a piece of data arrives, it computes the messages.",
                    "label": 0
                },
                {
                    "sent": "From that factor to the parameters, it knows where to send to, so it sends the parameters and the message.",
                    "label": 0
                },
                {
                    "sent": "We are TCP IP directly to these machines and these machines perform very simple operation.",
                    "label": 0
                },
                {
                    "sent": "Remember the marginal is the product of all incoming messages.",
                    "label": 0
                },
                {
                    "sent": "They would just multiply messages and if these are Gaussians or anything in the exponential family, they would literally just carry out.",
                    "label": 0
                },
                {
                    "sent": "In addition, would be very simple operation.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, these methods margin notes need to be copied back into each of these into each of the factors that are using them in order to make this approximation cousin approximate message passing, we need to know what's the current marginal in order to approximate so that one is a little bit tricky.",
                    "label": 0
                },
                {
                    "sent": "So the way it was actually done one way that you could do it as many ways is that you have this parameter store actually estimating the amount of change to how fast is is one of these incoming errors.",
                    "label": 0
                },
                {
                    "sent": "In unit time, arriving and then you have in a communication protocol.",
                    "label": 0
                },
                {
                    "sent": "Whenever an update is requested, you send back the latest marginal as well as the expiry time by which this will happen.",
                    "label": 0
                },
                {
                    "sent": "So you need and then.",
                    "label": 0
                },
                {
                    "sent": "On.",
                    "label": 0
                },
                {
                    "sent": "Here you have copies of those marginals.",
                    "label": 0
                },
                {
                    "sent": "On these machines together an expiry time, so you know exactly when to request a red edge again from each of the data vectors.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing I swept over little bit is if you really implemented in a distributed system, there is a few extra complexities that in mathematics on the right, but in practice they are very very very important.",
                    "label": 0
                },
                {
                    "sent": "Mainly have to do with the fact that components can break.",
                    "label": 0
                },
                {
                    "sent": "So if you have your my Mac here.",
                    "label": 0
                },
                {
                    "sent": "It's if any of the memory banks break, this machine is toast if any of the CPU to CPU goes, the machine is toast, but in a distributed systems where I took every single piece of the CPU becomes become a low cost piece of hardware and every single piece of memory bank became a low cost hardware.",
                    "label": 0
                },
                {
                    "sent": "This is likely to happen.",
                    "label": 0
                },
                {
                    "sent": "So one thing we need to deal with in practice, and you distribute inference is actually the machine Shard.",
                    "label": 0
                },
                {
                    "sent": "Consistency machine can break.",
                    "label": 0
                },
                {
                    "sent": "It can just simply get rebooted or fail, and that can't lead to the fact that now the whole data center performance.",
                    "label": 0
                },
                {
                    "sent": "Computation becomes inconsistent.",
                    "label": 0
                },
                {
                    "sent": "So one thing you need to take care of this consistency.",
                    "label": 0
                },
                {
                    "sent": "Which machine carries with sharp need to also do a lot with reliability and maintenance.",
                    "label": 0
                },
                {
                    "sent": "So one of the things is you, you know, in order to make such a service fast and make the delays very minimal, you do need to store all the parameters and RAM.",
                    "label": 0
                },
                {
                    "sent": "Ram is unreliable.",
                    "label": 0
                },
                {
                    "sent": "Then the machine failed, so you need to Check Point or or introduce some redundancy in this.",
                    "label": 0
                },
                {
                    "sent": "In this red box that I called the parameter store.",
                    "label": 0
                },
                {
                    "sent": "And then the rest of it is really quite a.",
                    "label": 0
                },
                {
                    "sent": "Pretty quiet techno.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Details, so there is actually a relation.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to MapReduce.",
                    "label": 0
                },
                {
                    "sent": "MapReduce is this computational framework that almost everyone uses today when you when you distribute your computation and you learn at scale and what's the relation of MapReduce?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at that graphical model for data so we have a parameter vector theater groups that rendered wrong and we have our data then MapReduce can actually be seen as a single step approximation of this message passing.",
                    "label": 0
                },
                {
                    "sent": "What does it mean in map reduce?",
                    "label": 0
                },
                {
                    "sent": "These are the members and send.",
                    "label": 0
                },
                {
                    "sent": "Initiating the members means that you sent this state of the prior.",
                    "label": 0
                },
                {
                    "sent": "Effectively this message, right?",
                    "label": 0
                },
                {
                    "sent": "The right message here is this message.",
                    "label": 0
                },
                {
                    "sent": "You send that to the Members.",
                    "label": 0
                },
                {
                    "sent": "The members are the machines that contain the data they locally compute.",
                    "label": 0
                },
                {
                    "sent": "Compute the messages from these from these vectors to this parameter and sent them back.",
                    "label": 0
                },
                {
                    "sent": "And that's called the reduce stage.",
                    "label": 0
                },
                {
                    "sent": "Reduce means that you're sharding you distributing your parameter vector in files.",
                    "label": 0
                },
                {
                    "sent": "And then you multiply.",
                    "label": 0
                },
                {
                    "sent": "If you just multiply, implement the multiplication method of operation operation of messages in here, then you can see that vanilla map produces a single pass of message passing.",
                    "label": 0
                },
                {
                    "sent": "So as long as you're performing this algorithm on distribution on data models, but you don't need to do approximate message passing, there actually equivalent, and it's a very viable mode to do distributed message passing using existing infrastructure such as Hadoop.",
                    "label": 0
                },
                {
                    "sent": "The problem arises more as if you have approximate messages.",
                    "label": 0
                },
                {
                    "sent": "So like in true skills on a quick prediction factor, or in fact in any model I would work with me, you know, I never got away with a simple two factors.",
                    "label": 0
                },
                {
                    "sent": "In this case you need to iterate that loop and every single iteration would be a new map reduce loop.",
                    "label": 0
                },
                {
                    "sent": "So we actually really long because you spin up spin up these machines.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that if you do it and you do it anyway, you have a huge communication overhead because each of the machines needs a copy of that belief vector over the parameters, and if it can't store does it its own memory.",
                    "label": 0
                },
                {
                    "sent": "So if these usually member machines are really crap and small, then we will all fail and the whole computation will break as well.",
                    "label": 0
                },
                {
                    "sent": "So does MapReduce disposer constraint on the on the amount of parameters you can actually use, but there's a very close relation.",
                    "label": 0
                },
                {
                    "sent": "It's a single path.",
                    "label": 0
                },
                {
                    "sent": "Message passing is actually map reduce.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, final thing before we before we have a break.",
                    "label": 0
                },
                {
                    "sent": "Is to do with approximation quality, so let's look at this conditional model again, and now let's make a specific example where the probability of Y.",
                    "label": 1
                },
                {
                    "sent": "So we assume now we have a binary classification.",
                    "label": 0
                },
                {
                    "sent": "So we observe an event or we don't.",
                    "label": 0
                },
                {
                    "sent": "So it's either plus one or minus one that the probability of plus 1 -- 1 given the parameter vector Theta.",
                    "label": 0
                },
                {
                    "sent": "An observation X is the inner product of Theta times X and then mapped to the positive or negative side and we take fires to CDF.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian CDF and we assume a factorizing Gaussian belief overall parameters.",
                    "label": 0
                },
                {
                    "sent": "So if that's the case, then if you did map, reduce.",
                    "label": 0
                },
                {
                    "sent": "Sorry if you did not produce any decent sequential computations, that's one message passing.",
                    "label": 0
                },
                {
                    "sent": "Skip this one message that we can do.",
                    "label": 0
                },
                {
                    "sent": "We send this message then we send this message because we've all incoming messages.",
                    "label": 0
                },
                {
                    "sent": "Then this one let me send a message to a second date, affect your already taking the message from the first day to factor into account when we process the 3rd.",
                    "label": 0
                },
                {
                    "sent": "When we take already the message from the 1st and 2nd into account.",
                    "label": 0
                },
                {
                    "sent": "That's how you would actually learn.",
                    "label": 0
                },
                {
                    "sent": "Step piece online.",
                    "label": 0
                },
                {
                    "sent": "Learning online learning is like inference forward that way, just so that's that's what you could do.",
                    "label": 0
                },
                {
                    "sent": "That's what you would do if you were to sequentially computer messages in a flow of information.",
                    "label": 0
                },
                {
                    "sent": "If you want to parallelize this, you send this out to all nodes and you send it all back.",
                    "label": 0
                },
                {
                    "sent": "Any combine it?",
                    "label": 0
                },
                {
                    "sent": "The thing is that we have to do approximate message passing.",
                    "label": 0
                },
                {
                    "sent": "We have to compute approximate message in Y1 invite two and invite 3 because this is a smooth step function.",
                    "label": 0
                },
                {
                    "sent": "The five function here if you look at if you plotted.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is multiplying Goshen.",
                    "label": 0
                },
                {
                    "sent": "In coming with that one, you're going to end up with a truncated smooth truncated Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of the engineering and making machine learning system work in practice actually goes into generating the right features and you usually have like several teams working on sets of features and work independently, and they might discover really the same signal.",
                    "label": 0
                },
                {
                    "sent": "So it's an extreme situation of the come up like I think your talk was alluding to this.",
                    "label": 0
                },
                {
                    "sent": "They come up with highly correlated features in practice.",
                    "label": 0
                },
                {
                    "sent": "So if we did that, this is our feature vector and now we you know I just fixed a target vector Theta generated data for that and I did this sequential and I had one bias feature is not enough.",
                    "label": 0
                },
                {
                    "sent": "Then it's just one.",
                    "label": 0
                },
                {
                    "sent": "So the red curve and the blue curve shows you the following shows you I have 100 hundred times the same observation.",
                    "label": 0
                },
                {
                    "sent": "And now what is the posterior?",
                    "label": 0
                },
                {
                    "sent": "So the posterior over my parameter is going to be having a mean and variance, but remember the exponential, feminine, exponential family, the natural way to do think about it, the one in which you perform a stochastic gradient then is actually mean over variance in one over variance.",
                    "label": 0
                },
                {
                    "sent": "Precision and precision mean.",
                    "label": 0
                },
                {
                    "sent": "So what you see in the blue curve is the posterior after every single sequential updated.",
                    "label": 0
                },
                {
                    "sent": "So we get more certain.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the variance goes down.",
                    "label": 0
                },
                {
                    "sent": "That makes sense.",
                    "label": 0
                },
                {
                    "sent": "Every single observation gives us one more piece of data, and the mean is this low of.",
                    "label": 0
                },
                {
                    "sent": "So if you divide the.",
                    "label": 0
                },
                {
                    "sent": "Why X by Y?",
                    "label": 0
                },
                {
                    "sent": "Then you end up with Mew.",
                    "label": 0
                },
                {
                    "sent": "So the slope is the mean, so that was actually the mean effect that I was using.",
                    "label": 0
                },
                {
                    "sent": "Now if I do this in parallel then I carried out all these approximations independent of the other approximations already happened.",
                    "label": 0
                },
                {
                    "sent": "So I end up with the model that's far more confident than it should be.",
                    "label": 0
                },
                {
                    "sent": "If I perform the sequential message passing, which I assume I can't do.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't seem big problem except the the confidence is 3 times higher than the true confidence should have been like.",
                    "label": 0
                },
                {
                    "sent": "This is the effect of distributing the message passing because in this in this regime of having many mappers I couldn't really use in the second update already incorporate the first message.",
                    "label": 0
                },
                {
                    "sent": "Now here's how this curve looks if I assume I have 100 time features and I have 100 teams that worked on features and they all came back with highly correlated features.",
                    "label": 0
                },
                {
                    "sent": "So now the true mean is still that slope is actually the same.",
                    "label": 0
                },
                {
                    "sent": "You can't see it due to the scale.",
                    "label": 0
                },
                {
                    "sent": "But the approximate message is horrible.",
                    "label": 0
                },
                {
                    "sent": "You know it's a lot less certain, but more importantly, the slope is way off.",
                    "label": 0
                },
                {
                    "sent": "This is the true.",
                    "label": 0
                },
                {
                    "sent": "This is the true mean.",
                    "label": 0
                },
                {
                    "sent": "This is the approximate one.",
                    "label": 0
                },
                {
                    "sent": "So if you actually iterate that algorithm, we run one map, reduce run and then do another one another one.",
                    "label": 0
                },
                {
                    "sent": "You see that the divergences actually diverging.",
                    "label": 0
                },
                {
                    "sent": "The convergence is actually getting worse and worse.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way that people deal in practice with that is they actually use dampening the same way that if you if you do in stochastic gradient send you scale your gradient more heavily according to the amount of disconnect you have.",
                    "label": 0
                },
                {
                    "sent": "So if you did one this.",
                    "label": 0
                },
                {
                    "sent": "If you do this one.",
                    "label": 0
                },
                {
                    "sent": "In fact if you use a dampening, that's proportional to the number of Members.",
                    "label": 0
                },
                {
                    "sent": "Then you see that in the first step this would have been.",
                    "label": 1
                },
                {
                    "sent": "This is now scaled back.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is scaled like this graph.",
                    "label": 0
                },
                {
                    "sent": "So just scale it back to the extra range, so just less than one.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see, the time now step by step, actually approximating even in the power influence the.",
                    "label": 0
                },
                {
                    "sent": "The two posterior with power inference.",
                    "label": 0
                },
                {
                    "sent": "I'm less certain because I Denton, so I remove a lot of traction.",
                    "label": 0
                },
                {
                    "sent": "You remove a lot of the information or data, but but what I can show is that that kind of rule.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is 1 /, a number of mappers is actually leading to convergence when I distribute the inference.",
                    "label": 0
                },
                {
                    "sent": "OK, we.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one I want to finish the dry part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "Do a little break and then the second part we're going to look at a lot of applications of that calculus.",
                    "label": 0
                },
                {
                    "sent": "2 problems of scale, and in the real world.",
                    "label": 0
                }
            ]
        }
    }
}