{
    "id": "z25jgj3ncybin2dccxsd6e4wdru7vkee",
    "title": "Space-indexed Dynamic Programming: Learning to Follow Trajectories",
    "info": {
        "author": [
            "J. Zico Kolter, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/icml08_kotler_sid/",
    "segmentation": [
        [
            "Alright hi I'm Zico Kolter and I'm going to talk today about space index dynamic programming, which is a method for learning to follow trajectory's.",
            "This is work with Adam Codes advisor, Andrew ING Egu, and Trails to hideaway all at Stanford University."
        ],
        [
            "So I'm going to start off by talking a little bit about reinforcement learning and following trajectory's in general.",
            "Then I'm going to talk about our method for doing this, which is called space index dynamic programming, and also introduced the notion of space index dynamical systems.",
            "And finally I'll conclude with some experimental."
        ],
        [
            "Results.",
            "OK, so I'm going to start off talking about reinforcement learning an following trajectory's not also want to act like question about this.",
            "Yesterday.",
            "This is sort of orthogonal approach to what Adam talked about yesterday and I'll be happy to talk about that more if people want at the end.",
            "So the relation between these two approaches but."
        ],
        [
            "It's a for now.",
            "I'll just sort of start talking about about what we did.",
            "So trajectory following in RL is the task of getting a vehicle like a car helicopter to follow some trajectory in space, and we assume the state space is too large.",
            "Discretize directly so we have to.",
            "We can't apply just a tabular RLR, dynamic programming algo."
        ],
        [
            "Adam.",
            "And so people often do in these domains is they apply some non stationary policy, dynamic programming algorithm and the ones we're going to focus on.",
            "Here is the PSD policy search by dynamic programming algorithm and also the differential dynamic programming algorithm which is sort of a classical control algorithm for tasks like this."
        ],
        [
            "And so I want to talk briefly about how these algorithms work in the sort of in a very vague general sense.",
            "So they work basically by first discretizing the state space, or sorry that the time is causing this control task into discrete time steps, and so you see here after T equals approximately 1 second, we're going to the car is going to be approximately in this blue area here.",
            "This represents a distribution over states where we might suppose the car is going to be."
        ],
        [
            "And you know, for time."
        ],
        [
            "Seconds we have the same thing, and so on down the trajectory, and then once we have these distributions over states at each time."
        ],
        [
            "We learn policies backwards in time, starting from the end and proceeding backwards in time to learn a policy for time one.",
            "So in this exam."
        ],
        [
            "Apple we learn policy Pie 5 first."
        ],
        [
            "And then."
        ],
        [
            "\u03a0 four, etc down to \u03c0 one and the way you actually learn these policies depends on the exact dynamic programming algorithm you're using, But in general, this is how all these algorithms in a broad sense."
        ],
        [
            "Auction.",
            "I need vantage of this approach really is that the policy is here can be local.",
            "So if you look at for example T = 5, we're assuming that at time 5 after 5 seconds we're going to be approximately in that area described by that blue circle there, and so the policy doesn't need to encode sort of actions for the entire space, it can just encode actions that will that will function well locally in that distribution.",
            "So for example, the policies we use later on, or linear Paula season they map from the current state directly to an action to take.",
            "Using a linear function and that only works because they can be very local in space, so if we try to do this over the whole state space, no chance is a policy that simple working, but because they can be local we can have much."
        ],
        [
            "Similar policies.",
            "And despite this advantage, the advantages the nice elements of these approaches, there are some problems with these traditional dynamic programming algorithms, and one of the problems these approaches produce time index Paula."
        ],
        [
            "This is what I mean.",
            "What I mean by this is suppose we learn a policy Pi 5 here.",
            "Assuming the car is going to be in this around this distribution of states."
        ],
        [
            "But then just due to natural scarcity of the environment, the car actually reaches this point after 5 seconds, right?",
            "So this is the whole."
        ],
        [
            "This is the problem now, because the policy won't perform very well.",
            "We sort of assume it's going to be one spot.",
            "We've learned a local policy, but because it's OK city, it might not reach there, so it's going to perform badly."
        ],
        [
            "This can be mitigated somewhat by what we call re indexing is actually very common in control.",
            "This is a well known problem, so no one would execute that, only that algorithm.",
            "So people do in practice is instead of executing the policy, correspond to the current time.",
            "They look at all the policies and execute the one whichever one is closest to the current location, regardless of time.",
            "And this is somewhat of a hack because isn't really preserve the nice optimality properties that dynamic programming, and I will talk about later.",
            "Actually, there's even more fundamental problems too.",
            "It doesn't really fix the real problem that's going on here."
        ],
        [
            "So the second problem I want to talk about is that the uncertainty over future states actually makes it hard to learn any good policy with these traditional family programming algorithms so."
        ],
        [
            "What I mean by that is.",
            "The slide I showed before in some ways is somewhat of a lie because I showed a nice tight distribution around the States at time 5, and that isn't really what happens in practice.",
            "What happens in practice is for states that are far ahead in the future.",
            "The distribution over states is going to be very large.",
            "Think about where you're going to be an hour from now.",
            "You're going to have a huge distribution over where you're going to be, so it's very hard to sort of have a nice tight circle that will have it very nice."
        ],
        [
            "Distribution there and the problem is the algorithm.",
            "These DP algorithms require learning a policy that performs well over the entire distribution.",
            "Here we're sort of version problem.",
            "We can't have nice local policies anymore."
        ],
        [
            "Search my source to point out the direction that we're going here.",
            "The approach we take in general, which we call State Space index.",
            "Dynamic programming, is that rather than discretizing by time, we directly take the trajectory discretize directory into what we call space indices, which are hyperplanes parallel.",
            "Sorry, tangent to the trajectory that we're interested in, and this is the basic idea of the approach that we're having here, and I'm going to."
        ],
        [
            "Describe it in a lot more detail now.",
            "So I'm now going to talk about our notion of space index, dynamical systems, and space in this dynamic programming."
        ],
        [
            "OK, so what's really the problem?",
            "Why is this hard?",
            "Why is it hard to do this with any programming in this task?",
            "So the problem here is that unlike the time index case, there's no guarantee that taking an action will move the car to the next plane along this trajectory.",
            "So if you take a space time index system, you know that taking an action will bring to the next time.",
            "But with the cynicism there's no such guarantees, and so because of this, we're going to introduce the notion of what we call a space index dynamical system."
        ],
        [
            "Before describing that, I want to describe how we go about creating the dynamical systems in general and sorting out the time into dental system.",
            "So we start out with a differential equation that models are system dynamics and these actually all these algorithms do require a model of the system in order to work.",
            "That sort of dynamic programming algorithms that I've mentioned there in general."
        ],
        [
            "Wire that here S is that."
        ],
        [
            "Current state U is the control action."
        ],
        [
            "And S dot is a time derivative of the state."
        ],
        [
            "And what we do usually is we apply for example or their integration to create a time minimal system.",
            "So here we say that you know the next day is going to be equal to the current state plus the time derivative plus the derivative of the state times some integration time step.",
            "And that's how we move from a differential equation to a time index technical."
        ],
        [
            "System.",
            "First space index.",
            "Identical system situation is a little bit more complicated because we have to simulate forward until whenever we hit the next space index.",
            "This is exactly what we do.",
            "Instead of simulating forward some, fix them out, we simulate forward until however long it takes until we hit this next space plane along our trajectory, and this is the basic idea."
        ],
        [
            "Of a space in denticle system, so that creates Pinnacle system.",
            "What we do is we are differential equation and we again apply order integration just as we did before.",
            "But here Delta TR integration time step is actually going to be a function of the current state and the current control action."
        ],
        [
            "And actually the very simple analytical form if we know the current state and we know the next, the next space plane we can solve this exactly.",
            "I don't want to spend too much time on these details of the forms, it's not that important, but what is important is that this is going to exist.",
            "The solution for this time time step will exist, as long as the car is making some progress on the trajectory.",
            "Just imagine as long as it's moving somewhat along this path, it's going through at some point.",
            "Hit this next one so we can solve this equation and find it."
        ],
        [
            "OK, so the end result we have though is what we have in the end.",
            "We have dynamical system.",
            "It's indexed by a spatial index variable D rather than time.",
            "So now we can sort of apply dynamic programming directly to this system and run our space index in the programming algorithm."
        ],
        [
            "As well, how this works now so it works very much like the classical then programming algorithms that before the differences.",
            "Now we divide the trajectory into discrete space planes and so this for example represent space plane one.",
            "And I should mention actually there's still going to be distribution over states.",
            "Here we're not going to be for sure in one state, so I reckon this is sort of a distribution over this one plane because we know we were still going to distribution.",
            "We know we're going to have to lie on this space plane no matter what, because we're going to have we're simulating until we hit that spot.",
            "So we're going to definitely lie.",
            "On that space plane."
        ],
        [
            "And we keep doing this now this, you know, simulate one more forward distribution over the."
        ],
        [
            "Equals 2 all the way to the end.",
            "And now we just applied in any program like we do."
        ],
        [
            "Before, so we learn a policy proceeding backwards in time, we learn a policy for the last space index and proceed backwards.",
            "Sorry, not the first time and now backwards in this space indices all the way to the."
        ],
        [
            "One, so here we learn Pi 5 for the last space index."
        ],
        [
            "\u03a0 four, etc."
        ],
        [
            "Down to buy one."
        ],
        [
            "So let's talk about how this solves the problems that I mentioned before with dynamic programming algorithms.",
            "So the first problem I talked about was that the policies from different programming algorithms were time index."
        ],
        [
            "And it should be someone audience.",
            "How this fixes this problem.",
            "Now there's space indexed.",
            "So whereas before we had the problem we may execute policy learned at a different location from where we actually were.",
            "We don't have that problem anymore.",
            "So now we're always going to execute policies based on the current spatial index, which is much more important than the current time index."
        ],
        [
            "The second problem was the uncertainty over future states would grow very large and make it hard to learn any good policy.",
            "So."
        ],
        [
            "So the way that's overcome is that now, whereas before we would have a very wide distribution over the possible states in the future, now we have a relatively tighter one, because now we're only caring about those states that we know they lie on that space plane, so it's going to be much tighter distribution, kind of.",
            "The subtle tradeoff here is."
        ],
        [
            "Now we actually have a wide distribution overtime, so whereas we know we're going to be at that point, we don't really know what time we're going to be there anymore.",
            "It could really be a wide variety of times, but The thing is, it's a trade off for actually very happy to make, because assuming that we have to assume is here, is that the reward function and the dynamics are time invariant.",
            "In that case we don't care about time, and so we're more than happy to trade off uncertainty over time to gain more certainty over where we are in space."
        ],
        [
            "OK so I just wanted to briefly talk about our experience that we did."
        ],
        [
            "On the task that we're concerned with here was we had this RC car in our lab and we want to follow a trajectory around the track, avoiding obstacles that are placed on this track.",
            "And."
        ],
        [
            "Let's talk briefly about the experimental setup.",
            "We implemented a space index version of the PS DP algorithm, which I mentioned before.",
            "This is the policy search by dynamic programming algorithm by Drew Bagnell and others, and the policy that we're learning here is an SVM classifier actually, which chooses the steering angle using an SVM based on the current velocity.",
            "Sorry sorry we have constant velocity and chooses a steering angle based on the current state of the car.",
            "So a very simple policy.",
            "So multiclass SVM policy and we also these algorithms.",
            "Do you need a simulator of the system and so this is a very simple textbook model of the car to learn his policy for these policy dynamics here and we evaluated this PCP algorithm in that I'm in this case in the time in this case with re indexing that I mentioned before in the space index case."
        ],
        [
            "OK, so here is the time index PHP.",
            "Here's what happens and it's actually the best run that we had.",
            "So.",
            "Because it doesn't work very well and in fact practice no one actually does this because of this exact problem.",
            "What time you're at is actually very irrelevant in terms of what policy would execute, because you never really.",
            "Where do you think you're going to be?",
            "The real world is never like a simulator, exactly.",
            "We've learned this for a simulator, and it's not going to perform very well on a real system."
        ],
        [
            "This.",
            "So we can fix this a little bit somewhat by reindexing.",
            "Remember, this is where we execute a policy now, not based on where we are in time, but based on where we are in space.",
            "But we still use the time based dynamic programming to learn the policies, and this performs much better.",
            "As you can imagine, but it still isn't great.",
            "It hits the obstacles here.",
            "If you look at the track, which I'll show you in a second, the actual trajectory it takes and it is not quite optimal, but certainly a lot better.",
            "But the problem here it's this question right there.",
            "Sample the local.",
            "This is an SVM like I was mentioning is an SVM that picks a steering angle based on the current state of the car.",
            "OK anyway, um it works.",
            "It works better than the time in this case certainly, but um doesn't work that well.",
            "And the problem here is what I alluded to before is that we can't even learn very good policies when it is time.",
            "Instead of programming.",
            "We can't even learn a very good policy to begin with.",
            "So no matter how we execute it, we're not going to have great performance on this domain."
        ],
        [
            "So finally, here's the space index version where we both learn and execute the policy based on the state of the current state of the car, and as you can see, it will just do this forever.",
            "It can just drive around the track while avoiding the obstacles indefinitely and.",
            "How to perform the so?",
            "So yeah.",
            "So I should mention that there the car we track the car with the camera above that.",
            "This leads in the car tracking tracking the car this location and we tell the obstacles are we actually can't move them because they don't have markers on them but we could move them around there actually randomly placed on this track which generate random track and put these obstacles down so it knows where they are but they only knows where they are with the features of the classifier.",
            "The classifier that we used to pick the steering angle.",
            "Has a feature that indicates if there's an obstacle nearby, and that's how it learns to avoid obstacles."
        ],
        [
            "So this graph here just shows sort of the graphically.",
            "What happens here.",
            "So on the left we have time index PCP, which just blows up the mess you never wanted.",
            "In practice, in the middle we have the time in his PSP with reindexing, so it's physically better, but it actually isn't.",
            "Still, isn't that good?",
            "Is actually not that good performance for controller on this task we can.",
            "I can handle controller that's better than this in fact, and you can see it sort of sort of doesn't really track the directory at the bottom there hit some of the obstacles etc.",
            "And.",
            "More sort of concretely, the cost, which is arbitrary, it's the cost for minimizing through this process, but the costs which penalizes deviations in the directory and also penalizes hitting an obstacle.",
            "The cough is about 60.",
            "I can describe the deals details are in the paper about what that actually means, but as the cause of trying to minimize so lower costs in this.",
            "In this case a lot better and spit in the PSD pizzas.",
            "There's a lot better, she's lower costs, and just just looking at it qualitatively, tracking tree much better while still avoiding the obstacles on the project."
        ],
        [
            "Tree.",
            "In the paper we also have some experiments on the Stanford Grand Challenge car and a simulated helicopter domain.",
            "Both the space index dynamic programming algorithms which you can see in the paper.",
            "But we have time to talk about too much here."
        ],
        [
            "I just wanna play some Lated work, as I mentioned that the Drew bagnell's paper about the net positive programming and the TDP paper obviously very related.",
            "But also, I think that paper by Laugesen Parin Langford and draws me.",
            "These are also our algorithms, a little bit different, and they don't produce.",
            "They don't really execute any program in the classical sense, but they still still result in time index policies and it will be the work here I think would be applicable to those those algorithms as well.",
            "Difference in the programming has been worked on in there all community a lot by Chris Acheson.",
            "An more recently this last nips by some other people and also in the in the control community.",
            "This is related to an approach called gain scheduling and also model predict control, though definitely different approach and I don't want to get too into those either."
        ],
        [
            "Just to summarize.",
            "The following really benefits from non stationary policies but additional dynamic programming in RL algorithms suffer because their time index.",
            "So in this paper what we did is we presented a notion the notion of a space in this family system and space index dynamic programming, and we showed how these sort of overcome the problems of classical dynamic programming algorithms.",
            "We will finally get to demonstrate the usefulness of these methods on some real world control tasks."
        ],
        [
            "And I said thank you and I should also mention there are videos available on my web page here.",
            "If you want to check out those videos again, thank you.",
            "Questions yes.",
            "First of all, how many control points did you have on the course and how did you select them?",
            "And the second question is.",
            "Maybe another way of thinking about this is you just it was in the same sense as first day in Veria 1973.",
            "You're setting up an SMTP with a bunch of.",
            "Operation you executing policy of a termination condition.",
            "You then choose a new policy to execute you executed.",
            "I think you're just saying that for this problem you want to use that technique with spatial termination conditions would be much more straightforward.",
            "Way to explain what we're doing, I think.",
            "OK, so the first question simpler.",
            "We just got 200 planes on this trajectory there fairly cheap to to add more.",
            "Essentially you have to just run the dynamic programming one more iteration on them, so to enter that one arbitrary we tried it the first time and it just for a second thing.",
            "I certainly think that you could represent.",
            "Sort of could talk about this as a semi Markov decision process, I don't.",
            "So yes, I mean I think in the static wrecked, I think there's a lot of sort of ways of talking about this, about different controller, different parts of the state space, etc.",
            "And that's exactly right.",
            "There's a termination when you hit this next space plane.",
            "I guess the novelty of this really is setting up this process of setting up this process such that we can perform dynamic program directly on this.",
            "This sort of space in particular, and creating a system that will the dynamic system that will terminate at the different points.",
            "And so I think that I think you definitely could talk about it as an SMTP.",
            "We didn't hear an.",
            "I don't know if the necessarily since you're not discounting.",
            "TSMDP, who just look like an MVP.",
            "The notion that is that dynamic programming is time indexed.",
            "It's stage indexed.",
            "If you want to think about right, right so so.",
            "Yeah, I think yeah.",
            "I did that, but this more later I think that sort of the way we're approaching this, and maybe that would be different way of approaching it could definitely lead to the equivalent equivalent algorithms.",
            "In the end, the way we're approaching it is from the viewpoint of these time index dynamic programming algorithms, and we're sort of modifying those directly to account for this space indexed case.",
            "I think if you set it up all as one big MVP or separate maintenance MVP.",
            "Obviously I I don't see how to accomplish some of the issues that come up here, but we can talk about that more.",
            "If you think there's ways of doing it more directly.",
            "Maybe it will be related.",
            "I think your space indexing in your formulas depends on the control value.",
            "Um, so the simulator depends on Delta T."
        ],
        [
            "Yeah that yeah.",
            "Depends on the control value, yes?",
            "OK, so you so that is then dynamic or claims are defined independent control.",
            "Yes, the planet in control but.",
            "No, no, so the innovation timesteps depends on control.",
            "So where how long we have to simulate before we hit the next one will naturally depend on what controller executing, but the actual planes themselves are independent of control germination conditions in Stuart languages and doesn't depend on the policy, right, OK?",
            "Questions, so I wonder if you're in private experiment is a kind of continuous, so you saw the task by discrete discreet apposition.",
            "Yeah, so.",
            "I have a missing.",
            "It's very, roughly speaking.",
            "I see for continuous task points that are near in space also near in time.",
            "So my one of my question is.",
            "I guess maybe the tool approach two approaches US equivalent.",
            "Cause.",
            "Task may be continuous, so near in space points in your space, also nearly time.",
            "So what you are why you achieve two different performance by two approaches may be caused by use different scale.",
            "Into two approaches, and now we have the same scale when we discretize, time is about the same number of discrete time points on the directory.",
            "As space points, the difference really is, it's not that they're not equivalent.",
            "Maybe I can talk about that more later, but know that they're not equivalent to just doing it in time and space.",
            "It's not about how.",
            "Finally we discretize.",
            "It's really more than that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright hi I'm Zico Kolter and I'm going to talk today about space index dynamic programming, which is a method for learning to follow trajectory's.",
                    "label": 0
                },
                {
                    "sent": "This is work with Adam Codes advisor, Andrew ING Egu, and Trails to hideaway all at Stanford University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to start off by talking a little bit about reinforcement learning and following trajectory's in general.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to talk about our method for doing this, which is called space index dynamic programming, and also introduced the notion of space index dynamical systems.",
                    "label": 0
                },
                {
                    "sent": "And finally I'll conclude with some experimental.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to start off talking about reinforcement learning an following trajectory's not also want to act like question about this.",
                    "label": 1
                },
                {
                    "sent": "Yesterday.",
                    "label": 0
                },
                {
                    "sent": "This is sort of orthogonal approach to what Adam talked about yesterday and I'll be happy to talk about that more if people want at the end.",
                    "label": 0
                },
                {
                    "sent": "So the relation between these two approaches but.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a for now.",
                    "label": 0
                },
                {
                    "sent": "I'll just sort of start talking about about what we did.",
                    "label": 0
                },
                {
                    "sent": "So trajectory following in RL is the task of getting a vehicle like a car helicopter to follow some trajectory in space, and we assume the state space is too large.",
                    "label": 1
                },
                {
                    "sent": "Discretize directly so we have to.",
                    "label": 0
                },
                {
                    "sent": "We can't apply just a tabular RLR, dynamic programming algo.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adam.",
                    "label": 0
                },
                {
                    "sent": "And so people often do in these domains is they apply some non stationary policy, dynamic programming algorithm and the ones we're going to focus on.",
                    "label": 0
                },
                {
                    "sent": "Here is the PSD policy search by dynamic programming algorithm and also the differential dynamic programming algorithm which is sort of a classical control algorithm for tasks like this.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I want to talk briefly about how these algorithms work in the sort of in a very vague general sense.",
                    "label": 0
                },
                {
                    "sent": "So they work basically by first discretizing the state space, or sorry that the time is causing this control task into discrete time steps, and so you see here after T equals approximately 1 second, we're going to the car is going to be approximately in this blue area here.",
                    "label": 1
                },
                {
                    "sent": "This represents a distribution over states where we might suppose the car is going to be.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, for time.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seconds we have the same thing, and so on down the trajectory, and then once we have these distributions over states at each time.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn policies backwards in time, starting from the end and proceeding backwards in time to learn a policy for time one.",
                    "label": 0
                },
                {
                    "sent": "So in this exam.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple we learn policy Pie 5 first.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "\u03a0 four, etc down to \u03c0 one and the way you actually learn these policies depends on the exact dynamic programming algorithm you're using, But in general, this is how all these algorithms in a broad sense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Auction.",
                    "label": 0
                },
                {
                    "sent": "I need vantage of this approach really is that the policy is here can be local.",
                    "label": 0
                },
                {
                    "sent": "So if you look at for example T = 5, we're assuming that at time 5 after 5 seconds we're going to be approximately in that area described by that blue circle there, and so the policy doesn't need to encode sort of actions for the entire space, it can just encode actions that will that will function well locally in that distribution.",
                    "label": 0
                },
                {
                    "sent": "So for example, the policies we use later on, or linear Paula season they map from the current state directly to an action to take.",
                    "label": 0
                },
                {
                    "sent": "Using a linear function and that only works because they can be very local in space, so if we try to do this over the whole state space, no chance is a policy that simple working, but because they can be local we can have much.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar policies.",
                    "label": 0
                },
                {
                    "sent": "And despite this advantage, the advantages the nice elements of these approaches, there are some problems with these traditional dynamic programming algorithms, and one of the problems these approaches produce time index Paula.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what I mean.",
                    "label": 0
                },
                {
                    "sent": "What I mean by this is suppose we learn a policy Pi 5 here.",
                    "label": 0
                },
                {
                    "sent": "Assuming the car is going to be in this around this distribution of states.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then just due to natural scarcity of the environment, the car actually reaches this point after 5 seconds, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the whole.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the problem now, because the policy won't perform very well.",
                    "label": 1
                },
                {
                    "sent": "We sort of assume it's going to be one spot.",
                    "label": 0
                },
                {
                    "sent": "We've learned a local policy, but because it's OK city, it might not reach there, so it's going to perform badly.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This can be mitigated somewhat by what we call re indexing is actually very common in control.",
                    "label": 0
                },
                {
                    "sent": "This is a well known problem, so no one would execute that, only that algorithm.",
                    "label": 0
                },
                {
                    "sent": "So people do in practice is instead of executing the policy, correspond to the current time.",
                    "label": 0
                },
                {
                    "sent": "They look at all the policies and execute the one whichever one is closest to the current location, regardless of time.",
                    "label": 1
                },
                {
                    "sent": "And this is somewhat of a hack because isn't really preserve the nice optimality properties that dynamic programming, and I will talk about later.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's even more fundamental problems too.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really fix the real problem that's going on here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second problem I want to talk about is that the uncertainty over future states actually makes it hard to learn any good policy with these traditional family programming algorithms so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I mean by that is.",
                    "label": 0
                },
                {
                    "sent": "The slide I showed before in some ways is somewhat of a lie because I showed a nice tight distribution around the States at time 5, and that isn't really what happens in practice.",
                    "label": 1
                },
                {
                    "sent": "What happens in practice is for states that are far ahead in the future.",
                    "label": 0
                },
                {
                    "sent": "The distribution over states is going to be very large.",
                    "label": 0
                },
                {
                    "sent": "Think about where you're going to be an hour from now.",
                    "label": 0
                },
                {
                    "sent": "You're going to have a huge distribution over where you're going to be, so it's very hard to sort of have a nice tight circle that will have it very nice.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution there and the problem is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "These DP algorithms require learning a policy that performs well over the entire distribution.",
                    "label": 1
                },
                {
                    "sent": "Here we're sort of version problem.",
                    "label": 0
                },
                {
                    "sent": "We can't have nice local policies anymore.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search my source to point out the direction that we're going here.",
                    "label": 0
                },
                {
                    "sent": "The approach we take in general, which we call State Space index.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming, is that rather than discretizing by time, we directly take the trajectory discretize directory into what we call space indices, which are hyperplanes parallel.",
                    "label": 0
                },
                {
                    "sent": "Sorry, tangent to the trajectory that we're interested in, and this is the basic idea of the approach that we're having here, and I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe it in a lot more detail now.",
                    "label": 0
                },
                {
                    "sent": "So I'm now going to talk about our notion of space index, dynamical systems, and space in this dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's really the problem?",
                    "label": 0
                },
                {
                    "sent": "Why is this hard?",
                    "label": 0
                },
                {
                    "sent": "Why is it hard to do this with any programming in this task?",
                    "label": 0
                },
                {
                    "sent": "So the problem here is that unlike the time index case, there's no guarantee that taking an action will move the car to the next plane along this trajectory.",
                    "label": 1
                },
                {
                    "sent": "So if you take a space time index system, you know that taking an action will bring to the next time.",
                    "label": 0
                },
                {
                    "sent": "But with the cynicism there's no such guarantees, and so because of this, we're going to introduce the notion of what we call a space index dynamical system.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before describing that, I want to describe how we go about creating the dynamical systems in general and sorting out the time into dental system.",
                    "label": 1
                },
                {
                    "sent": "So we start out with a differential equation that models are system dynamics and these actually all these algorithms do require a model of the system in order to work.",
                    "label": 0
                },
                {
                    "sent": "That sort of dynamic programming algorithms that I've mentioned there in general.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wire that here S is that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Current state U is the control action.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And S dot is a time derivative of the state.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we do usually is we apply for example or their integration to create a time minimal system.",
                    "label": 0
                },
                {
                    "sent": "So here we say that you know the next day is going to be equal to the current state plus the time derivative plus the derivative of the state times some integration time step.",
                    "label": 0
                },
                {
                    "sent": "And that's how we move from a differential equation to a time index technical.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "First space index.",
                    "label": 0
                },
                {
                    "sent": "Identical system situation is a little bit more complicated because we have to simulate forward until whenever we hit the next space index.",
                    "label": 1
                },
                {
                    "sent": "This is exactly what we do.",
                    "label": 0
                },
                {
                    "sent": "Instead of simulating forward some, fix them out, we simulate forward until however long it takes until we hit this next space plane along our trajectory, and this is the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of a space in denticle system, so that creates Pinnacle system.",
                    "label": 0
                },
                {
                    "sent": "What we do is we are differential equation and we again apply order integration just as we did before.",
                    "label": 0
                },
                {
                    "sent": "But here Delta TR integration time step is actually going to be a function of the current state and the current control action.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually the very simple analytical form if we know the current state and we know the next, the next space plane we can solve this exactly.",
                    "label": 0
                },
                {
                    "sent": "I don't want to spend too much time on these details of the forms, it's not that important, but what is important is that this is going to exist.",
                    "label": 0
                },
                {
                    "sent": "The solution for this time time step will exist, as long as the car is making some progress on the trajectory.",
                    "label": 0
                },
                {
                    "sent": "Just imagine as long as it's moving somewhat along this path, it's going through at some point.",
                    "label": 1
                },
                {
                    "sent": "Hit this next one so we can solve this equation and find it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the end result we have though is what we have in the end.",
                    "label": 0
                },
                {
                    "sent": "We have dynamical system.",
                    "label": 0
                },
                {
                    "sent": "It's indexed by a spatial index variable D rather than time.",
                    "label": 1
                },
                {
                    "sent": "So now we can sort of apply dynamic programming directly to this system and run our space index in the programming algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well, how this works now so it works very much like the classical then programming algorithms that before the differences.",
                    "label": 0
                },
                {
                    "sent": "Now we divide the trajectory into discrete space planes and so this for example represent space plane one.",
                    "label": 0
                },
                {
                    "sent": "And I should mention actually there's still going to be distribution over states.",
                    "label": 1
                },
                {
                    "sent": "Here we're not going to be for sure in one state, so I reckon this is sort of a distribution over this one plane because we know we were still going to distribution.",
                    "label": 0
                },
                {
                    "sent": "We know we're going to have to lie on this space plane no matter what, because we're going to have we're simulating until we hit that spot.",
                    "label": 0
                },
                {
                    "sent": "So we're going to definitely lie.",
                    "label": 0
                },
                {
                    "sent": "On that space plane.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we keep doing this now this, you know, simulate one more forward distribution over the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equals 2 all the way to the end.",
                    "label": 0
                },
                {
                    "sent": "And now we just applied in any program like we do.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before, so we learn a policy proceeding backwards in time, we learn a policy for the last space index and proceed backwards.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not the first time and now backwards in this space indices all the way to the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, so here we learn Pi 5 for the last space index.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "\u03a0 four, etc.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down to buy one.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about how this solves the problems that I mentioned before with dynamic programming algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the first problem I talked about was that the policies from different programming algorithms were time index.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it should be someone audience.",
                    "label": 0
                },
                {
                    "sent": "How this fixes this problem.",
                    "label": 0
                },
                {
                    "sent": "Now there's space indexed.",
                    "label": 0
                },
                {
                    "sent": "So whereas before we had the problem we may execute policy learned at a different location from where we actually were.",
                    "label": 1
                },
                {
                    "sent": "We don't have that problem anymore.",
                    "label": 1
                },
                {
                    "sent": "So now we're always going to execute policies based on the current spatial index, which is much more important than the current time index.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second problem was the uncertainty over future states would grow very large and make it hard to learn any good policy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way that's overcome is that now, whereas before we would have a very wide distribution over the possible states in the future, now we have a relatively tighter one, because now we're only caring about those states that we know they lie on that space plane, so it's going to be much tighter distribution, kind of.",
                    "label": 0
                },
                {
                    "sent": "The subtle tradeoff here is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we actually have a wide distribution overtime, so whereas we know we're going to be at that point, we don't really know what time we're going to be there anymore.",
                    "label": 0
                },
                {
                    "sent": "It could really be a wide variety of times, but The thing is, it's a trade off for actually very happy to make, because assuming that we have to assume is here, is that the reward function and the dynamics are time invariant.",
                    "label": 0
                },
                {
                    "sent": "In that case we don't care about time, and so we're more than happy to trade off uncertainty over time to gain more certainty over where we are in space.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I just wanted to briefly talk about our experience that we did.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the task that we're concerned with here was we had this RC car in our lab and we want to follow a trajectory around the track, avoiding obstacles that are placed on this track.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's talk briefly about the experimental setup.",
                    "label": 1
                },
                {
                    "sent": "We implemented a space index version of the PS DP algorithm, which I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "This is the policy search by dynamic programming algorithm by Drew Bagnell and others, and the policy that we're learning here is an SVM classifier actually, which chooses the steering angle using an SVM based on the current velocity.",
                    "label": 0
                },
                {
                    "sent": "Sorry sorry we have constant velocity and chooses a steering angle based on the current state of the car.",
                    "label": 1
                },
                {
                    "sent": "So a very simple policy.",
                    "label": 0
                },
                {
                    "sent": "So multiclass SVM policy and we also these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Do you need a simulator of the system and so this is a very simple textbook model of the car to learn his policy for these policy dynamics here and we evaluated this PCP algorithm in that I'm in this case in the time in this case with re indexing that I mentioned before in the space index case.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the time index PHP.",
                    "label": 0
                },
                {
                    "sent": "Here's what happens and it's actually the best run that we had.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because it doesn't work very well and in fact practice no one actually does this because of this exact problem.",
                    "label": 0
                },
                {
                    "sent": "What time you're at is actually very irrelevant in terms of what policy would execute, because you never really.",
                    "label": 0
                },
                {
                    "sent": "Where do you think you're going to be?",
                    "label": 0
                },
                {
                    "sent": "The real world is never like a simulator, exactly.",
                    "label": 0
                },
                {
                    "sent": "We've learned this for a simulator, and it's not going to perform very well on a real system.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So we can fix this a little bit somewhat by reindexing.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is where we execute a policy now, not based on where we are in time, but based on where we are in space.",
                    "label": 0
                },
                {
                    "sent": "But we still use the time based dynamic programming to learn the policies, and this performs much better.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, but it still isn't great.",
                    "label": 0
                },
                {
                    "sent": "It hits the obstacles here.",
                    "label": 0
                },
                {
                    "sent": "If you look at the track, which I'll show you in a second, the actual trajectory it takes and it is not quite optimal, but certainly a lot better.",
                    "label": 0
                },
                {
                    "sent": "But the problem here it's this question right there.",
                    "label": 0
                },
                {
                    "sent": "Sample the local.",
                    "label": 0
                },
                {
                    "sent": "This is an SVM like I was mentioning is an SVM that picks a steering angle based on the current state of the car.",
                    "label": 0
                },
                {
                    "sent": "OK anyway, um it works.",
                    "label": 0
                },
                {
                    "sent": "It works better than the time in this case certainly, but um doesn't work that well.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is what I alluded to before is that we can't even learn very good policies when it is time.",
                    "label": 0
                },
                {
                    "sent": "Instead of programming.",
                    "label": 0
                },
                {
                    "sent": "We can't even learn a very good policy to begin with.",
                    "label": 0
                },
                {
                    "sent": "So no matter how we execute it, we're not going to have great performance on this domain.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, here's the space index version where we both learn and execute the policy based on the state of the current state of the car, and as you can see, it will just do this forever.",
                    "label": 0
                },
                {
                    "sent": "It can just drive around the track while avoiding the obstacles indefinitely and.",
                    "label": 0
                },
                {
                    "sent": "How to perform the so?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So I should mention that there the car we track the car with the camera above that.",
                    "label": 0
                },
                {
                    "sent": "This leads in the car tracking tracking the car this location and we tell the obstacles are we actually can't move them because they don't have markers on them but we could move them around there actually randomly placed on this track which generate random track and put these obstacles down so it knows where they are but they only knows where they are with the features of the classifier.",
                    "label": 0
                },
                {
                    "sent": "The classifier that we used to pick the steering angle.",
                    "label": 0
                },
                {
                    "sent": "Has a feature that indicates if there's an obstacle nearby, and that's how it learns to avoid obstacles.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this graph here just shows sort of the graphically.",
                    "label": 0
                },
                {
                    "sent": "What happens here.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have time index PCP, which just blows up the mess you never wanted.",
                    "label": 0
                },
                {
                    "sent": "In practice, in the middle we have the time in his PSP with reindexing, so it's physically better, but it actually isn't.",
                    "label": 1
                },
                {
                    "sent": "Still, isn't that good?",
                    "label": 0
                },
                {
                    "sent": "Is actually not that good performance for controller on this task we can.",
                    "label": 0
                },
                {
                    "sent": "I can handle controller that's better than this in fact, and you can see it sort of sort of doesn't really track the directory at the bottom there hit some of the obstacles etc.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "More sort of concretely, the cost, which is arbitrary, it's the cost for minimizing through this process, but the costs which penalizes deviations in the directory and also penalizes hitting an obstacle.",
                    "label": 0
                },
                {
                    "sent": "The cough is about 60.",
                    "label": 0
                },
                {
                    "sent": "I can describe the deals details are in the paper about what that actually means, but as the cause of trying to minimize so lower costs in this.",
                    "label": 0
                },
                {
                    "sent": "In this case a lot better and spit in the PSD pizzas.",
                    "label": 0
                },
                {
                    "sent": "There's a lot better, she's lower costs, and just just looking at it qualitatively, tracking tree much better while still avoiding the obstacles on the project.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tree.",
                    "label": 0
                },
                {
                    "sent": "In the paper we also have some experiments on the Stanford Grand Challenge car and a simulated helicopter domain.",
                    "label": 1
                },
                {
                    "sent": "Both the space index dynamic programming algorithms which you can see in the paper.",
                    "label": 0
                },
                {
                    "sent": "But we have time to talk about too much here.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just wanna play some Lated work, as I mentioned that the Drew bagnell's paper about the net positive programming and the TDP paper obviously very related.",
                    "label": 0
                },
                {
                    "sent": "But also, I think that paper by Laugesen Parin Langford and draws me.",
                    "label": 0
                },
                {
                    "sent": "These are also our algorithms, a little bit different, and they don't produce.",
                    "label": 0
                },
                {
                    "sent": "They don't really execute any program in the classical sense, but they still still result in time index policies and it will be the work here I think would be applicable to those those algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "Difference in the programming has been worked on in there all community a lot by Chris Acheson.",
                    "label": 0
                },
                {
                    "sent": "An more recently this last nips by some other people and also in the in the control community.",
                    "label": 0
                },
                {
                    "sent": "This is related to an approach called gain scheduling and also model predict control, though definitely different approach and I don't want to get too into those either.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to summarize.",
                    "label": 0
                },
                {
                    "sent": "The following really benefits from non stationary policies but additional dynamic programming in RL algorithms suffer because their time index.",
                    "label": 1
                },
                {
                    "sent": "So in this paper what we did is we presented a notion the notion of a space in this family system and space index dynamic programming, and we showed how these sort of overcome the problems of classical dynamic programming algorithms.",
                    "label": 0
                },
                {
                    "sent": "We will finally get to demonstrate the usefulness of these methods on some real world control tasks.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I said thank you and I should also mention there are videos available on my web page here.",
                    "label": 1
                },
                {
                    "sent": "If you want to check out those videos again, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions yes.",
                    "label": 0
                },
                {
                    "sent": "First of all, how many control points did you have on the course and how did you select them?",
                    "label": 0
                },
                {
                    "sent": "And the second question is.",
                    "label": 0
                },
                {
                    "sent": "Maybe another way of thinking about this is you just it was in the same sense as first day in Veria 1973.",
                    "label": 0
                },
                {
                    "sent": "You're setting up an SMTP with a bunch of.",
                    "label": 0
                },
                {
                    "sent": "Operation you executing policy of a termination condition.",
                    "label": 0
                },
                {
                    "sent": "You then choose a new policy to execute you executed.",
                    "label": 0
                },
                {
                    "sent": "I think you're just saying that for this problem you want to use that technique with spatial termination conditions would be much more straightforward.",
                    "label": 0
                },
                {
                    "sent": "Way to explain what we're doing, I think.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first question simpler.",
                    "label": 0
                },
                {
                    "sent": "We just got 200 planes on this trajectory there fairly cheap to to add more.",
                    "label": 0
                },
                {
                    "sent": "Essentially you have to just run the dynamic programming one more iteration on them, so to enter that one arbitrary we tried it the first time and it just for a second thing.",
                    "label": 0
                },
                {
                    "sent": "I certainly think that you could represent.",
                    "label": 0
                },
                {
                    "sent": "Sort of could talk about this as a semi Markov decision process, I don't.",
                    "label": 0
                },
                {
                    "sent": "So yes, I mean I think in the static wrecked, I think there's a lot of sort of ways of talking about this, about different controller, different parts of the state space, etc.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "There's a termination when you hit this next space plane.",
                    "label": 0
                },
                {
                    "sent": "I guess the novelty of this really is setting up this process of setting up this process such that we can perform dynamic program directly on this.",
                    "label": 0
                },
                {
                    "sent": "This sort of space in particular, and creating a system that will the dynamic system that will terminate at the different points.",
                    "label": 0
                },
                {
                    "sent": "And so I think that I think you definitely could talk about it as an SMTP.",
                    "label": 0
                },
                {
                    "sent": "We didn't hear an.",
                    "label": 0
                },
                {
                    "sent": "I don't know if the necessarily since you're not discounting.",
                    "label": 0
                },
                {
                    "sent": "TSMDP, who just look like an MVP.",
                    "label": 0
                },
                {
                    "sent": "The notion that is that dynamic programming is time indexed.",
                    "label": 0
                },
                {
                    "sent": "It's stage indexed.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about right, right so so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think yeah.",
                    "label": 0
                },
                {
                    "sent": "I did that, but this more later I think that sort of the way we're approaching this, and maybe that would be different way of approaching it could definitely lead to the equivalent equivalent algorithms.",
                    "label": 0
                },
                {
                    "sent": "In the end, the way we're approaching it is from the viewpoint of these time index dynamic programming algorithms, and we're sort of modifying those directly to account for this space indexed case.",
                    "label": 0
                },
                {
                    "sent": "I think if you set it up all as one big MVP or separate maintenance MVP.",
                    "label": 0
                },
                {
                    "sent": "Obviously I I don't see how to accomplish some of the issues that come up here, but we can talk about that more.",
                    "label": 0
                },
                {
                    "sent": "If you think there's ways of doing it more directly.",
                    "label": 0
                },
                {
                    "sent": "Maybe it will be related.",
                    "label": 0
                },
                {
                    "sent": "I think your space indexing in your formulas depends on the control value.",
                    "label": 0
                },
                {
                    "sent": "Um, so the simulator depends on Delta T.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah that yeah.",
                    "label": 0
                },
                {
                    "sent": "Depends on the control value, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, so you so that is then dynamic or claims are defined independent control.",
                    "label": 0
                },
                {
                    "sent": "Yes, the planet in control but.",
                    "label": 0
                },
                {
                    "sent": "No, no, so the innovation timesteps depends on control.",
                    "label": 0
                },
                {
                    "sent": "So where how long we have to simulate before we hit the next one will naturally depend on what controller executing, but the actual planes themselves are independent of control germination conditions in Stuart languages and doesn't depend on the policy, right, OK?",
                    "label": 0
                },
                {
                    "sent": "Questions, so I wonder if you're in private experiment is a kind of continuous, so you saw the task by discrete discreet apposition.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "I have a missing.",
                    "label": 0
                },
                {
                    "sent": "It's very, roughly speaking.",
                    "label": 0
                },
                {
                    "sent": "I see for continuous task points that are near in space also near in time.",
                    "label": 0
                },
                {
                    "sent": "So my one of my question is.",
                    "label": 0
                },
                {
                    "sent": "I guess maybe the tool approach two approaches US equivalent.",
                    "label": 0
                },
                {
                    "sent": "Cause.",
                    "label": 0
                },
                {
                    "sent": "Task may be continuous, so near in space points in your space, also nearly time.",
                    "label": 0
                },
                {
                    "sent": "So what you are why you achieve two different performance by two approaches may be caused by use different scale.",
                    "label": 0
                },
                {
                    "sent": "Into two approaches, and now we have the same scale when we discretize, time is about the same number of discrete time points on the directory.",
                    "label": 0
                },
                {
                    "sent": "As space points, the difference really is, it's not that they're not equivalent.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can talk about that more later, but know that they're not equivalent to just doing it in time and space.",
                    "label": 0
                },
                {
                    "sent": "It's not about how.",
                    "label": 0
                },
                {
                    "sent": "Finally we discretize.",
                    "label": 0
                },
                {
                    "sent": "It's really more than that.",
                    "label": 0
                }
            ]
        }
    }
}