{
    "id": "27sxp5uowyfemaqirromzg6gzw6dbkhd",
    "title": "Learning Overcomplete Latent Variable Models through Tensor Methods",
    "info": {
        "author": [
            "Animashree Anandkumar, University of California, Irvine"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_anandkumar_tensor_methods/",
    "segmentation": [
        [
            "This is joint work with my student Majeed and Rongo from Microsoft who couldn't be here due to visa issues."
        ],
        [
            "So indeed, we know a lot of latent variable models which can be solved by spectral methods, such as in a multiview models, Gaussian mixtures, ICA and so on, and the idea is you know if you try to do Max."
        ],
        [
            "Mum likelihood estimation.",
            "This is a hard problem.",
            "Instead we can solve them through method of moments, while pairwise moments are encoded through matrices.",
            "We need tensors to express higher order moments."
        ],
        [
            "Right on, the idea is for many of these latent variable models, these higher order moments have nice algebraic structures.",
            "For instance, for the multiview model through conditional independence, we have that the you know if you look at the cross moment between the different views, it reduces to a tensor decomposition problem where the components correspond to the unknown parameters that map the hidden variable to the observed variable, and the key point here is the number of components or the rank of this star tensor decomposition.",
            "Is equal to the dimensionality of the hidden variable.",
            "So in this talk what we will see is what happens when the rank exceeds the dimension.",
            "So with matrices that's impossible, right?",
            "So that's not identifiable, but with tensors we give a class of constraints on the variables under which we can actually resolve and learn the parameters.",
            "So that's what I mean by overcomplete in this."
        ],
        [
            "The scenario.",
            "So, so that's the idea that we want to look at the overcomplete models.",
            "So you know the way this problem is challenging.",
            "It's NP hard in general, the decomposition may not exist.",
            "The earlier case that's been well understood is the orthogonal case where you know the only local optimum of the decomposition are the components of the tensor.",
            "But in general this is an unknown problem.",
            "So what we?"
        ],
        [
            "Show is for overcomplete tensors.",
            "If you have incoherent components we will be able to handle such tensor decompositions.",
            "So for instance you can think of the inner product as of the order of one over root D in D dimensions.",
            "These are satisfied by random vectors, but more generally we can handle any level of incoherence between the components and then characterize how the number of components depends on this level of incoherence.",
            "So the idea is, as long as the components are fairly, you know, and correlated with one another, you can have a large number of components, many more components than the dimensionality of the observed variables."
        ],
        [
            "And the method will use to solve is a very simple alternating minimization method.",
            "This is nothing but you can think of it as a nice symmetric version of the power method.",
            "So you just fix two of the components and update the third one and you then do many different initializations and you will get hopefully get a recovery of all the components so."
        ],
        [
            "What we show is, you know, a local convergence result for this power iteration, and then there's some simple correction steps to in fact get all the components consistently, and it can also handle like perturbation in the actual tensor and the key point is for local convergence you can handle up to the dimension of the hidden variable K or the rank K being up to D to the 1.5 for three dimensional for 3rd order tensor.",
            "So the idea is for a tensor you can handle a lot more components than the.",
            "So dimension, so this is what I call like.",
            "You know, the power of using tensor methods and so you know so you can show local convergence.",
            "But getting to these points can be challenging because you need to start quite close to the point.",
            "So you need like constant correlation.",
            "But if the."
        ],
        [
            "On the other hand, the number of components is not too large.",
            "It is still order the dimension.",
            "Then you can do this in polynomial time, so with random restarts.",
            "So you have random initializations and then you take a slice of the tensor and take the top component by doing SVD and using that to initialize the alternating minimization method you can recover the all the components with bounded error where the error is nothing but the perturbation of the tensor.",
            "And so this is the main result of the paper that we can, you know, show recovery of over complete answers.",
            "So we also have tight sample complexity bounds where we carefully do epsilon at covering fault answers and argue you know, tight sample complexity for both local and global convergence and for local convergence we get close to minimax optimal bounds for many models.",
            "So for more details come to my poster, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with my student Majeed and Rongo from Microsoft who couldn't be here due to visa issues.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So indeed, we know a lot of latent variable models which can be solved by spectral methods, such as in a multiview models, Gaussian mixtures, ICA and so on, and the idea is you know if you try to do Max.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mum likelihood estimation.",
                    "label": 0
                },
                {
                    "sent": "This is a hard problem.",
                    "label": 0
                },
                {
                    "sent": "Instead we can solve them through method of moments, while pairwise moments are encoded through matrices.",
                    "label": 0
                },
                {
                    "sent": "We need tensors to express higher order moments.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right on, the idea is for many of these latent variable models, these higher order moments have nice algebraic structures.",
                    "label": 0
                },
                {
                    "sent": "For instance, for the multiview model through conditional independence, we have that the you know if you look at the cross moment between the different views, it reduces to a tensor decomposition problem where the components correspond to the unknown parameters that map the hidden variable to the observed variable, and the key point here is the number of components or the rank of this star tensor decomposition.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the dimensionality of the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "So in this talk what we will see is what happens when the rank exceeds the dimension.",
                    "label": 0
                },
                {
                    "sent": "So with matrices that's impossible, right?",
                    "label": 0
                },
                {
                    "sent": "So that's not identifiable, but with tensors we give a class of constraints on the variables under which we can actually resolve and learn the parameters.",
                    "label": 0
                },
                {
                    "sent": "So that's what I mean by overcomplete in this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The scenario.",
                    "label": 0
                },
                {
                    "sent": "So, so that's the idea that we want to look at the overcomplete models.",
                    "label": 0
                },
                {
                    "sent": "So you know the way this problem is challenging.",
                    "label": 0
                },
                {
                    "sent": "It's NP hard in general, the decomposition may not exist.",
                    "label": 0
                },
                {
                    "sent": "The earlier case that's been well understood is the orthogonal case where you know the only local optimum of the decomposition are the components of the tensor.",
                    "label": 0
                },
                {
                    "sent": "But in general this is an unknown problem.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show is for overcomplete tensors.",
                    "label": 1
                },
                {
                    "sent": "If you have incoherent components we will be able to handle such tensor decompositions.",
                    "label": 0
                },
                {
                    "sent": "So for instance you can think of the inner product as of the order of one over root D in D dimensions.",
                    "label": 0
                },
                {
                    "sent": "These are satisfied by random vectors, but more generally we can handle any level of incoherence between the components and then characterize how the number of components depends on this level of incoherence.",
                    "label": 1
                },
                {
                    "sent": "So the idea is, as long as the components are fairly, you know, and correlated with one another, you can have a large number of components, many more components than the dimensionality of the observed variables.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the method will use to solve is a very simple alternating minimization method.",
                    "label": 0
                },
                {
                    "sent": "This is nothing but you can think of it as a nice symmetric version of the power method.",
                    "label": 0
                },
                {
                    "sent": "So you just fix two of the components and update the third one and you then do many different initializations and you will get hopefully get a recovery of all the components so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we show is, you know, a local convergence result for this power iteration, and then there's some simple correction steps to in fact get all the components consistently, and it can also handle like perturbation in the actual tensor and the key point is for local convergence you can handle up to the dimension of the hidden variable K or the rank K being up to D to the 1.5 for three dimensional for 3rd order tensor.",
                    "label": 0
                },
                {
                    "sent": "So the idea is for a tensor you can handle a lot more components than the.",
                    "label": 0
                },
                {
                    "sent": "So dimension, so this is what I call like.",
                    "label": 0
                },
                {
                    "sent": "You know, the power of using tensor methods and so you know so you can show local convergence.",
                    "label": 0
                },
                {
                    "sent": "But getting to these points can be challenging because you need to start quite close to the point.",
                    "label": 0
                },
                {
                    "sent": "So you need like constant correlation.",
                    "label": 0
                },
                {
                    "sent": "But if the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, the number of components is not too large.",
                    "label": 0
                },
                {
                    "sent": "It is still order the dimension.",
                    "label": 0
                },
                {
                    "sent": "Then you can do this in polynomial time, so with random restarts.",
                    "label": 0
                },
                {
                    "sent": "So you have random initializations and then you take a slice of the tensor and take the top component by doing SVD and using that to initialize the alternating minimization method you can recover the all the components with bounded error where the error is nothing but the perturbation of the tensor.",
                    "label": 0
                },
                {
                    "sent": "And so this is the main result of the paper that we can, you know, show recovery of over complete answers.",
                    "label": 0
                },
                {
                    "sent": "So we also have tight sample complexity bounds where we carefully do epsilon at covering fault answers and argue you know, tight sample complexity for both local and global convergence and for local convergence we get close to minimax optimal bounds for many models.",
                    "label": 0
                },
                {
                    "sent": "So for more details come to my poster, thank you.",
                    "label": 0
                }
            ]
        }
    }
}