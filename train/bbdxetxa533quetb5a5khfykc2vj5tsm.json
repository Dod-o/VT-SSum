{
    "id": "bbdxetxa533quetb5a5khfykc2vj5tsm",
    "title": "Deep Learning for Machine Vision",
    "info": {
        "author": [
            "Adam Coates, Baidu, Inc."
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_coates_machine_vision/",
    "segmentation": [
        [
            "So thanks a lot for the intro guys, so I'm Adam Coates and as I mentioned, I'm currently a postdoc at Stanford, but I'm also a visiting scholar at Indiana University Bloomington.",
            "And when?"
        ],
        [
            "Going to talk about here is actually trying to.",
            "It's going to be a little different from the abstract.",
            "In fact, I want to change some things to try to get enough in here in the amount of time we've got.",
            "One of the things I'm going to try to go through real slowly as the very basics of what do we want machine learning to do?",
            "Why is deep learning important?",
            "Try to give you some of the basic tools for actually doing some deep learning yourselves, and there's sort of a story many of you may have heard about about unsupervised learning, and feature learning that I'll try to get to at the end, but I wanted to make sure to go through some of the low level stuff.",
            "To give you an idea for how to actually use these things so that when you go home and look at some of the resources that I'm going to have linked at the end of the talk, you'll be able to go do an online tutorial and actually play with some of these things and even have enough knowledge behind you to do some real applications.",
            "So I want to start to motivate all this stuff by sort of talking about what is it that we want machine learning to do, especially with computer vision.",
            "The basic idea is that we have these applications where you have some input like.",
            "In image and we want to do something like object recognition.",
            "We want to be able to look at this low level pattern and make a prediction about what's going on.",
            "So for example, an object recognition we want to predict whether there's a cat in the image or not."
        ],
        [
            "For something like detection, the output is a little different.",
            "We want to be able to look at this image and figure out that this is the location of the bicyclist and from."
        ],
        [
            "Or sophisticated application.",
            "For instance segmentation maybe went on to look at this image and figure out the outline of the birds or outline of the objects in the scene."
        ],
        [
            "And one of the.",
            "One of the sort of common approaches to machine learning.",
            "There are whole bunch of different ways of solving these problems, but a pretty large number of these approaches are sort of covered by a common pipeline where we take this input example, an image of a cat, let's say, and then we have some sort of black box system that extracts features.",
            "It tries to extract some sort of higher level patterns.",
            "And then we usually have some final off the shelf machine learning algorithm that we just drop all this information into and it tries to learn to make decisions starting from this sort of higher level representation.",
            "So we might want our machine learning algorithms tell us is this a cat pictured in this image, and the thing that ends up taking all of our time that causes us a lot of grief is this feature extraction piece, because you know we can try to get pretty sophisticated here.",
            "Sometimes you'll see this box taking on some pretty complex engineering where we're trying to build.",
            "Several layers, in fact, of increasingly high level abstractions and the way that we do that, is that we try to take all of our prior knowledge about the world, how images work, what we might see in images, and so forth, and somehow wire them into this box.",
            "And it turns out that if you do a really good job, then you can get great results.",
            "But if you're not a computer vision expert, you haven't done this before.",
            "Then you can end up with really terrible results."
        ],
        [
            "So this is sort of a starting point for talking about deep learning.",
            "What deep learning is and why we hope this can work, because what we'd like to do is keep this same kind of pipeline.",
            "This basic idea that we want to take our input image and then feed it through several modules where we're going to extract higher and higher level of features.",
            "So we might start with some low level features, like detecting edges in an image, and then we want to move to sort of middle level features like recognizing parts and so forth to maybe some higher level structures like recognizing objects.",
            "And then at the very end, only at the very end we're going to have some classifier that makes some application specific decision.",
            "Like, is this a cat where the outlines of an object and so on, and the basic idea is that whenever we build these pipelines where going from sort of low level ideas and getting more and more abstract representations as we go up and whereas we normally build that by hand, we want to somehow train these multiple layers from data and try to discover a representation so that by the time we get to the high level features out here.",
            "If we just train a classifier on the end, we can make really good decisions.",
            "So the idea behind deep learning is to figure out a way to train this whole pipeline of systems and not have to engineer them so much by hand."
        ],
        [
            "So it's worth asking why is it just upfront?",
            "Why do we want to even do this deep learning thing?",
            "There are handful of sort of simple justifications.",
            "One is that we just know sort of inherently from working on this for so many years that some decisions just require a lot of stages of processing.",
            "It's very unlikely that I can see an image and decide whether there's a cat in an image or a dog in the image, just with a linear function is somehow too simple.",
            "We know that it can't be that simple.",
            "And all the systems we currently have, no matter how we built them, involve many different stages of processing.",
            "So we sort of know that we want this long pipeline of decision-making, but unfortunately trying to engineer that by hand is quite difficult.",
            "And we already have been intuitively engineering these sorts of things in computer vision for quite awhile, so if we're already building these kinds of architectures, a very natural thing to try to do is to be able to actually learn them into them from data.",
            "And finally, and I think this was sort of under appreciated early in a lot of this research is that the algorithms I'm going to talk about today are actually based on a pretty small subset of operations that turn out to scale incredibly well with hardware.",
            "So there's a sort of long running phenomenon in machine learning where if you want to get really great results and go to a conference and publish and so forth.",
            "A way to do that is to just get a lot more data with a lot bigger computer.",
            "You can almost always do better, and with a lot of algorithms when we sort of build them by hand, we have all these rules coded in.",
            "It's really hard to sort of scale up and take advantage of all these things because you have so much knowledge wired in and less you're more clever unless you can get increasingly clever, it's harder and harder to get better results.",
            "But if we have a learning algorithm that scales really well with hardware and with more data, then it becomes easier to get results as we get better hardware and we get larger datasets.",
            "So one of the things that's really nice about deep learning is that whenever we get a faster computer or we get a larger data set, we can train a bigger model.",
            "So hopefully you'll see by the end of this that most of these things are very simple, dense linear algebra operations and they run really fast on a computer on it or on a GPU."
        ],
        [
            "So just to sort of also answer another question, up front one of the questions is have we been here before?",
            "So a lot of deep learning spawns from neural networks research which some of you may have been familiar with if you took like an introductory AI course and a lot of that was sort of having its heyday in like the 1980s and so on, so it's worth trying to understand has anything really changed since then?",
            "So the answer to the question have we been here before?",
            "There are two possible answers, yes and no.",
            "And the on the yes side.",
            "We say that the basic ideas here are actually pretty common to pass machine learning algorithms in neural networks.",
            "So for example, if you want to do supervised learning with labeled data, it turns out it's a very straightforward procedure, and I'm going to walk you through that in a second, but also sort of standard machine learning development strategies.",
            "If you take a machine learning course, you learn about how to debug machine learning algorithms.",
            "All that stuff you can carry over.",
            "And a lot of knowledge from problem domains like computer vision.",
            "Different applications has also been carried over, so this is stuff that we sort of know that's old hat.",
            "And so in this sense things haven't changed so much.",
            "But there are a few things that have changed that are pretty critical.",
            "So for example we have much faster computers and a lot more data, and it used to be the case that if you trained a neural network with a very small amount of data in order to get really good performance, you had to wire a lot of your own knowledge into it.",
            "But now that we have huge datasets and really big computers to process them.",
            "We can actually get away without that prior knowledge and what we want is a really flexible algorithm that can just gobble up all that data and make good decisions.",
            "So in that sense, the sort of operating regime where we want to work on applications has changed, so that's very new.",
            "We are better at optimization.",
            "We are better at figuring out how to initialize these neural networks and all these different models.",
            "We want to train.",
            "So there's been a lot of useful research there, and in fact one of the key results that's responsible for bringing all this stuff to you.",
            "Of late it was from Jeff Hinton, Yoshua Bengio back in 2006, which is effectively a very, very cool initialization procedure that made all these training algorithms work much better.",
            "And finally, I think this is also another underappreciated point, which is that.",
            "We now have a ton of empirical evidence, partly from having faster computers and more students running lots and lots of experiments with these things.",
            "We know much better.",
            "What are all the various components that matter and sort of neat fallout of using neural networks and using a common framework for all this is that when your friend tells you at a conference hey, I just tried this new module inside my neural network.",
            "You can go home and re implement that thing very quickly and mix it and match it with all your other components.",
            "So there's been quite a bit of work figuring out.",
            "What are the right modules to be plugging into these things that we can all reuse?"
        ],
        [
            "So this is pretty great and this is.",
            "This is now having a real impact used to be.",
            "That is pretty tough to do a real application because you needed all that prior knowledge.",
            "You had to be a domain expert and deep learning systems now are starting to really come of age and we're seeing a lot of state of the art results not just in computer vision but in lots of different fields.",
            "Most recently very cool result by Alex Kryszewski ileus US cover and Jeff Hinton from NIPS 2012.",
            "Was this basically deep neural network that is currently holding the state of the art results on the image net data set so very large data set they hold a challenge every year called the image net large scale visual recognition challenge and their deep neural network was the 1st place system in this challenge and I'll show you some results from that later.",
            "But hopefully by maybe an hour into this talk you'll have all the basic underpinnings to implement a system like that yourself.",
            "But it's worth saying that outside of vision, these things are also doing incredible things in speech and natural language processing.",
            "So there's a whole bunch of other stuff to go along in those fields that I hope you'll go and read up on at some point.",
            "There's some pretty cool things that's going on outside of vision as well."
        ],
        [
            "So quick outline here 'cause we don't have a ton of time.",
            "I'm going to go through a sort of machine learning, refresher or crash course just to kind of sort of refresh you if you take a machine learning class, but it's been awhile, or if you haven't taken a machine learning class, maybe you've been through like Intro AI or something.",
            "Sort of go sense for how machine learning is done, how it works.",
            "And then we're going to go through a sort of natural extension of that to supervised deep learning and sort of give you some tools for actually training a deep neural network.",
            "So the basic algorithms used there and then also tell you a bit about what are the application specific things we want to use for computer vision, or if you want to use neural networks for vision problems.",
            "Turns out you need some extra ingredients, and I'll tell you what those are.",
            "I'll say a bit about debugging."
        ],
        [
            "Just a couple of slides on that, and then I'll go back to what may be a story more familiar to some of you have been watching deep learning for awhile, which is about unsupervised deep learning that involves using unsupervised algorithms where we don't have labeled data for trying to train deep networks and then at the very end we're going to have some references and resources for you guys to pick up on the web, including a link to a tutorial that you can do online later."
        ],
        [
            "So."
        ],
        [
            "In supervised learning, which is sort of a common mode that we want to operate in, we're going to be given a whole bunch of training examples and our training examples all have a label associated with them.",
            "That's usually given to us by a human or some kind of system with a lot of knowledge in it.",
            "And basically we have a whole bunch of pairs, so the first element of this pair is a vector X, and this is essentially our input data.",
            "It's the thing that we're given an.",
            "Then we have some output Y, which is the variable that we want to predict.",
            "And why could be a vector?",
            "It could be a whole bunch of continuous quantities, or it might be discrete.",
            "So for example, in the case we've been talking about where someone gives me an image and I would like to know, is this a cat, we could represent the cat as a vector of pixel intensities, and then we're going to pass it through some function that I'm just going to call F of X here.",
            "And then we're going to ask it to predict a discrete binary label, either 0 if it's not a cat or a one if it is a cat, and the question for machine learning.",
            "For our supervised learning algorithm is to try to find some F of X so that it correctly predicts the label Y on all of our training examples.",
            "So whatever I put X in, it correctly says whether this is a cat or not.",
            "A cat for all of the examples or as many examples as possible in the training set that we've been given.",
            "And hopefully if all goes well, this learned predictor is going to work on some test data, so that if I give you an image you've never seen before, you actually get the correct prediction out of it."
        ],
        [
            "So we just want to review a very simple binary classification algorithm.",
            "So if you want to actually solve this problem, one thing that you could try doing is to start from a function that has this form.",
            "So it's F of X and then this little; Here just means that the variables to the right are sort of parameters, but you can just think of X and Theta.",
            "Here is being two inputs to F. And F of X is going to have this definition where it's 1 / 1 + X of minus the inner product or dot product of Theta and X.",
            "So this linear function here is really.",
            "Going to try to give positive values to things that are cats and is going to try to give negative values to things that are not cats, and this nonlinear function that sort of wrapped around it, which I'm going to label Sigma everywhere, is a sigmoid function and you can just think of it as a squashing function.",
            "It's just going to take this otherwise linear value and scrunch it down so that it fits into the range between zero and one, and the reason we want to do that is so that we can interpret the output of this function F of X as a probability.",
            "So our goal is to try to train this F of X function to tell us the probability that the thing pictured in this image represented by X is actually a cat.",
            "So we want it to be large when Y is equal to 1, meaning it's a cat and we want it to be small and Y is equal to 0, meaning it's not a cat.",
            "And to do that, I'm not going to go through exactly where this function comes from, But this is an objective function that we want to minimize.",
            "So basically what we want to do here is this cost function or loss function as a function of Theta tells us how well our function F fits our training data.",
            "So what I'm going to do is I'm going to sum over all the examples and whenever the label is one, I want to basically make this value in here large.",
            "Which is actually the probability that Y is 1 an whenever the label says that it's zero.",
            "I want to make this probability very small.",
            "So if we find the Theta that minimizes this, that's also going to be the one that makes good predictions on our training set."
        ],
        [
            "So how do we actually tune Theta to minimize this thing?",
            "One algorithm that we can actually use to do this.",
            "In fact there are lots of algorithms we could choose, but almost all of them share the same basic approach, which is that we need to compute the gradient of this loss function, and this is something you can do by hand for logistic regression if you just go pull out your calculus book and compute a bunch of derivatives, then you find that the gradient looks like this.",
            "And you can pass this to some off the shelf optimizer if you want.",
            "Or you can just implement your own gradient descent algorithm where what we do is we pick some initial guess for Theta that could just be random values, and then what we're going to do is every time we want to make an update, we compute this gradient and then we make a small change to Theta that points in the negative direction.",
            "So remember that the gradient is pointing in the uphill direction.",
            "So if I change Theta in the opposite direction, I'm going to go downhill.",
            "So this is a very simple optimization algorithm.",
            "We're going to return to this with ways to make it much better when we talk about deep learning.",
            "And finally, a sort of technical difference when we actually implement this is when we have a lot of data we don't sum over the whole data set.",
            "We just take a few examples, maybe one example or 100 examples, and compute the gradient from those before we make an update.",
            "That way we can make lots of updates very quickly and hopefully make more progress."
        ],
        [
            "So it turns out this is pretty nice.",
            "This function that I showed you that we want to minimize is convex, so that very simple optimizer that I showed you is always going to take you to some nice minimum once you set the stepsize properly.",
            "And this works for simple problems.",
            "So if I give you these digits of zeros and ones and I give you the labels, it tells you whether there zeros or ones.",
            "It turns out this this classifier will work great, but the reason that it works is because there is some pixels in these things that turn out to be very highly informative.",
            "So for example, the Pixel in the center, it usually tells you whether it's a zero or a one cousin, the zeros it's in the hole, and in a one it's turned on, so it turns out that a linear function works pretty well, but if I give you something more complex.",
            "That, to our eyes seems very simple.",
            "This algorithm will fail completely.",
            "So for example, if I ask you is this a coffee mug you have very little hope and you can take my word for it.",
            "'cause this is a problem that I have played with for quite for quite awhile.",
            "So it's worth asking why."
        ],
        [
            "Is this so darn hard?",
            "And it turns out that you know you and I see a coffee mug here, but of course the computers seen this giant grid of numbers an once you're looking at this grid of numbers is not clear at all what's going on.",
            "Just turns out that Pixel intensities are actually a really poor representation, not just for humans to be reading grids of numbers, but for computers even to understand what's happening in this image and to sort of under."
        ],
        [
            "And more deeply, what's the problem here?",
            "I want to take a sort of extreme example of this, so I'm going to take a picture of my coffee mug here.",
            "And I'm going to represent it not by thousands of pixel intensities, but just by two.",
            "So I'm just going to pick two pixels here, and I'm going to represent this image using only the intensity at those two points.",
            "So for example, this image might get mapped to a 2 dimensional vector where the values are 72160 depending on what the pixel values are.",
            "And then I'm going to plot this point on a little 2D scatter plot where I said that this is a positive example.",
            "This is a coffee mug and then I'm going to do that for a whole bunch of coffee mugs or cats.",
            "Whatever we're working with.",
            "Then I'm also going to do it for a whole bunch of things that are not coffee mugs.",
            "So for instance, coffee cups and four random clutter in the world."
        ],
        [
            "We do this for a whole bunch of points, then we get something that looks like this and now what we're sort of asking our algorithm to do when we're giving it these pixel intensities is."
        ],
        [
            "Please tell me for this new point.",
            "Is this a coffee mug and it's sort of clear in this case that this is totally unfair, that even if we had a brilliant classifier that could somehow memorize all these examples to ask whether this is a coffee mug is totally ambiguous, we can't possibly hope for it to give us the right answer, because this representation that I've chosen is a really bad one on the other."
        ],
        [
            "And if we could somehow figure out, for instance, that I have a way to detect handles, let's say I also have a way to detect cylinders, then if."
        ],
        [
            "Plot these two features or the responses of these two detectors as my as my way of representing this image.",
            "Then it will turn out that all the coffee mugs or things in the upper right corner and all the things that are not coffee mugs are in the lower left corner and now."
        ],
        [
            "Now it's actually a fair game which we asked our machine learning algorithm.",
            "This is in a coffee mug.",
            "Not only can it separate these things into two categories, but there's some chance that you could generalize to new things you hadn't seen before."
        ],
        [
            "And so this is sort of what's behind our standard machine learning pipeline, which is that we know that our machine learning algorithm cannot solve this problem from raw pixels.",
            "So what we end up doing is we hardwire some function Phi into the system, so that when we plug X into fee, it outputs some new representation.",
            "So if X had, say, an pixels, then it's going to output K features and we can just drop those things into our logistic regression algorithm, let's say, and try to do better.",
            "So if you've been doing work with machine learning algorithms in computer vision, you're probably familiar with this kind of pipeline.",
            "There are lots and lots of choices for things we can use for Fi."
        ],
        [
            "So the question is, where do we actually get these features?"
        ],
        [
            "As I mentioned in the sort of overview earlier on, we basically built them by hand at this point, so this is a tried and true approach.",
            "It actually works great to get you started for a whole lot of things, but in reality for the hardest problems like computer vision, a huge huge amount of investment goes into this.",
            "So for example, everyone of course is probably familiar with the SIFT features, which are probably the most some of the most successful engineered features in both computer vision and machine learning in general.",
            "And.",
            "But we also can think of things like super pixels as being another way of kind of transforming our input through a very complex pipeline into some representation that we think the machine learning algorithm can actually deal with."
        ],
        [
            "So that's sort of if you were to take a machine learning classes like one class of algorithms, this is a pipeline you're probably familiar with."
        ],
        [
            "The basic idea behind deep learning is that we saw how to do supervised learning.",
            "When we have these hardwired features, we just take Phi of X, which is going to be fixed.",
            "And then we're going to dump it into our off the shelf machine learning algorithm.",
            "So what we want to do?"
        ],
        [
            "So here is somehow extend this to the case where these features are given by tunable functions, functions that themselves we can actually learn.",
            "So for example, in the case of binary classification, what we wanted to do was get a function that was an estimate for the probability that Y is equal to 1, which is itself now going to be the sigmoid function applied to, not a linear function of the features, but a linear function of yet another function which is computed from the data and the thing that I've added here is W * X, where W is a matrix followed by a Sigma.",
            "Followed by this sigmoid function again.",
            "And you."
        ],
        [
            "And think of this function having two parts.",
            "There's this outer part, which is just the same pieces before it's from our logistic regression classifier.",
            "And then there's this inside part which is basically taking the place of our features.",
            "But this is now a tunable function and it has, as its output is going to be a vector an in this vector.",
            "We basically have each row of W * X, and then the fact that I put the sigmoid around this vector means we're just going to apply that 1 / 1 + X function.",
            "To every element of this vector.",
            "So it's the same as before, but we've replaced our features with this tunable function now, so to do so."
        ],
        [
            "Supervised learning for this two class problem, we're going to do essentially the same thing now, so we're just going to minimize this loss function that we used for logistic regression.",
            "Well, I've just rewritten it now to include these parameters W as something that we have to optimize over so when we go to minimize this function to find the best possible classifier.",
            "We're not only searching for the best classification parameters, Theta were searching for the best features as well.",
            "We're looking for the choice of W that makes that internal feature function down here work as well as it can for the classification that we want to do.",
            "So this is the same as logistic regression, but now we have a function that's got several stages in it.",
            "And another way to."
        ],
        [
            "Sort of think about this or to write it is to think of it as a sort of sequence of little modules where we plug X in.",
            "At the bottom we compute the sigmoid of WX, and that's going to give us a new vector H which we can think of as a feature vector, and then H is sort of forming like a new data set a new input that's just going to go into logistic regression like usual."
        ],
        [
            "So you've probably seen this in a bunch of different ways, but even though I wrote it as sort of a formula and then as a sort of block diagram, you can also just write this down as a neural network, so you've probably seen cartoons like this many times to represent a neural network, and this is just a graphical cartoon of showing you the exact same thing that I already told you.",
            "So for example, each of these little circles here."
        ],
        [
            "People will call them a neuron and I'm going to use that terminology to later much later in the talk, but those are just values in those in those feature vectors or in your input data."
        ],
        [
            "And the idea is that if I have my pixels X from an image, I'm going to feed them into this neural network to compute H and then F, and then I'm going to compute my loss function at the end.",
            "And you can think of the data as flowing upward in this network as we do the computation each of."
        ],
        [
            "These lines here corresponds to a parameter in this network that we have to train.",
            "So for example, this sort of diagonal one right here, is the parameter that determines how H2 gets computed from X1, and so that's W21 the 2nd row first column of W."
        ],
        [
            "And likewise for Theta, so they don't want in beta two and three, and so on up here.",
            "So this is just a cartoon diagram of sort of how that function is put together."
        ],
        [
            "And of course, what we can do is extend this and we can have multiple layers and we have several layers we can think of, not just training one layer of features, but in fact trying to train a stack of features and train multiple stages of computation.",
            "But again, this is just a cartoon."
        ],
        [
            "Moon way of depicting a function that looks like this one where I've got now two matrices of weights and each of them sort of stacked one inside the other."
        ],
        [
            "And I can also draw it as a block diagram like this, where I make the intermediate values explicit.",
            "So these are just three different ways of expressing the exact same function.",
            "So how do we actually train this thing?"
        ],
        [
            "So I'm going to tell you about the backpropagation algorithm, which is actually been around for a long time, but it's sort of key to understand how this works, and then I'm going to give you a whole bunch of tricks, and I'm going to give you lots and lots of information later on how to turn this into a much more powerful system so.",
            "Basically, our goal is to minimize this loss function, so let's go back to the case where we just wanted to do binary classification with this one layer of features.",
            "Our goal is to minimize this thing over Theta and W. And just like before what we need out of this system is a gradient with respect to Theta and a gradient with respect to W. And if we have those then we can do gradient descent or we can run some other optimizer to minimize this function.",
            "So if you want to, you can expand this out and at least the formula to get the gradient with respect to Theta is going to be essentially the same as before.",
            "You could figure that out by hand, but the gradient with respect to W is going to be much more complicated and you don't want to do that by hand, so."
        ],
        [
            "The way that we do this is through the chain rule, which you're probably familiar with from calculus and."
        ],
        [
            "The idea is that let's suppose we have some module.",
            "Let's forget the rest of the function.",
            "For now I've got some little black box black box module that takes in AZ&AW as two inputs and then it produces an output H and I don't know much else about this function, but I'm going to be given a couple of things.",
            "One is I'm going to tell you ahead of time the derivative of your loss function.",
            "Your objective function with respect to the output of this module.",
            "So with respect to each of the elements HJ and then the one thing I'll tell you about this module, in addition to the inputs and outputs, is the derivative of each element of H with respect to each of the elements of Z, or for that matter, each element in W. That's what I'll tell you, and if you apply the chain rule.",
            "What you'll end up with is that the derivative with respect to each of the elements in Z is equal to this summation, where I'm just taking the product of the derivatives with respect to H multiplied by the corresponding derivatives with respect to each of the elements in Z.",
            "And many of you may recognize this if you recall your calculus book that this is the all these elements, the HJS with respect to the K values.",
            "Here are the elements of a Jacobian matrix.",
            "If you don't remember that term, that's OK. Just take my word for it that this simple function.",
            "It can actually be just written as a matrix multiply, but the key thing to take away is that if you know these two pieces of information, I can implement another little black box that takes in the gradient with respect to H and spits out the gradient with respect to Z."
        ],
        [
            "And similarly for this weight matrix, WI can make another little machine that takes in the gradient with respect to H and gives me the gradient with respect to W. So you can take."
        ],
        [
            "Rule and just remember that for any black box model we've got, if we can give you these derivatives, then we can.",
            "Make this change."
        ],
        [
            "And we can build up sort of a toolkit.",
            "We can build a CHEAT SHEET if you will.",
            "That tells us for any of these different functions that we use in our neural network tells us how to take the gradient with respect to H, which is the output an converted into a gradient with respect to Z or a gradient with respect to the parameters W. So for a simple linear function, for example the gradient with respect to Z turns out to be W transpose times the gradient with respect to H and for to get the derivative with respect to the parameters W. Turns out to be the gradient with respect to H * Y transpose and this will give you a matrix that is just your update that you want to make to W. So as you start to do a lot of work with neural networks, or maybe someone tells you about some module they just tried in their neural network at a conference.",
            "Once you have these little pieces, you can just drop it into your system and in fact."
        ],
        [
            "This is how automatic differentiation works in a library like Theano, which is a cool neural networks package that works on GPU's, there will be a link to at the end."
        ],
        [
            "So once we got this, we can just re apply this rule everywhere and every time we've got these feedforward functions where we can compute our output F. We also have a backward module that corresponds to it so that if someone gives us the gradient with respect to F at the very top, that tells us how we can improve our loss.",
            "We can work that backward to find out how to change W and how to change data.",
            "So every time you make a forward pass you can also make a backward pass that tells you how to update.",
            "Each of these variables."
        ],
        [
            "So just as a more concrete example, in the case of this network that I was showing you earlier where we had one layer of features and we have these sigmoid functions in here, we can run this forward to find F. That's just the forward pass to compute the outputs of our system, and then if you can compute the gradient with respect to F from your loss function, which you can do by hand.",
            "Then using the information in our table, we can find how to turn this into a gradient with respect to H, which turns out to be this function.",
            "Here we just took a couple of the rules out of the table to make this, and then once we know that guy we can use a couple more rules from the table to compute the gradient with respect to W. So it turns out to be this formula here so you can see you could actually just implement this as an algorithm on your computer, compute the gradient with respect to H and then the gradient with respect to W and now you can use the same optimizer as before."
        ],
        [
            "So the whole training procedure here boils down to collecting a bunch of training data just like we would do for logistic regression.",
            "Compute the gradients using this forward and backward propagation scheme, and then once we know those gradients then will make us gradient descent step for each of the parameters and we just keep doing this till convergence.",
            "So this is the classic back propagation scheme.",
            "Many of you may have seen before and."
        ],
        [
            "And.",
            "Unfortunately, this historically has not worked so easily.",
            "If it were this simple, I think we figured it out along time ago.",
            "So a lot of the nice things that made logistic regression so easy to implement no longer apply, so this sort of seemingly simple neural network is now a nonconvex optimization problem, which means you can get trapped in local minima.",
            "You can end up in places where the function is decreasing, but it's decreasing so slowly that you can't tell.",
            "Or it may become.",
            "Or you may have a problem with something that's known as the vanishing gradient problem.",
            "So huge issue in early neural networks research, where as you add more and more stages to this neural network, you'll find that the gradient with respect to W down at the very bottom is essentially 0, even though it's obvious you could somehow improve over your random initialization.",
            "Your gradient descent algorithm will keep telling you that the gradient with respect to all these things is 0, and you can't make progress.",
            "And unfortunately, it's just been generally hard to diagnose and debug all of the various problems that come up here.",
            "So."
        ],
        [
            "Get around that.",
            "It turns out that many things matter in order to make this work, it comes down to the details.",
            "So the choice of the nonlinear functions that you use matters a lot, so I use the sigmoid function everywhere, which turns out not to be the best choice in the world.",
            "The initialization of the parameters matters if you just pick a whole bunch of random values for Theta, but they're too big, too small.",
            "What have you then that turns out to cause trouble.",
            "And finally, the optimiser that you use and how you set that up turn out to matter a lot as well.",
            "So before I jump into a whole bunch of extra material on how to actually get this to work, I want to check to see if there are any questions about the sort of basic stuff we went through so far.",
            "Always good.",
            "OK. Alright, so.",
            "I'm going to go through each of these things and say a little bit about what does and what does not."
        ],
        [
            "So for example, the choice of nonlinearities turns out to be pretty critical.",
            "So what function you use inside your network determines what kinds of features and you're going to end up with and how they behave.",
            "So some functions that people have tried or things like that and H function, which is actually very similar to the sigmoid, it's the same shape, but it's been shifted so that at zero it has a zero output instead of instead of 0.5.",
            "Interestingly, you can use like absolute value rectification, which sort of to me doesn't seem like it should have worked so well, but turns out to be pretty useful.",
            "An increasingly something called a rectified linear unit, which is really just a nice name for a function that zero until you reach a threshold value of 0, and then it's strictly linear after that.",
            "And even though these are very simple, and they're not terribly different when you plot them like this, they have.",
            "Huge difference in terms of how, in terms of what kinds of functions you're going to represent and also how easy it is to optimize all the parameters in your network."
        ],
        [
            "So the actual is initialization also matters a lot.",
            "Usually we pick small random values and one thing that you should be careful about is you should try to choose these random values so that you don't accidentally end up in parts of your neuron response or your feature response.",
            "That will cause grief for your optimizer.",
            "So for example, if we're using a sigmoid unit, we really want to avoid these areas way out here at the edge, because if you look at the derivative.",
            "Of this curve, the derivative is really tiny and as a result if you try to use gradient descent whenever your neuron ends up out in this regime, the gradient is going to be essentially zero, and it's going to have a hard time making progress.",
            "So this is one of the problems that causes that vanishing gradient issue that I told you about earlier.",
            "But you also want to avoid things like non differentiable parts of your function, so if you're using absolute value rectification, sometimes you'll want to.",
            "You'll certainly want to avoid setting things to exactly 0, but people will often put like a little epsilon in there so that it smooths out the bottom of these functions.",
            "Anna good good practice when you first start out with these things is to occasionally go through your neural network and actually inspect the unit.",
            "So if you've been running it for a little while and your gradient descent optimizer stops making progress, it's good to just look at a histogram of all the responses.",
            "And if you see everyone at zero and one and nothing in between, that's a bad sign, so that's worth doing.",
            "And finally, we're to caution, often you can put very large values in an.",
            "Your optimizer will look like it's making lots and lots of progress, and you say, oh.",
            "Fantastic, I'll use larger values, but it turns out that these things can actually give you worse models, so the correct scale to set these add is something you sort of got to figure out and for specific situations like these 10 H units, it turns out there's some guidance in the literature on good ranges to set them too, so for example.",
            "For that and H units, Xavier Laurel and some coauthors found this kind of interesting rule where if I have a neuron, and I know how many inputs are feeding into it, that's the fan in and I know how many different neurons depend on it.",
            "That's the fanned out the fan out.",
            "If I take 6 divided by that some and then the square root, and then I sampled the weights from a uniform distribution between minus R and plus are that turns out to be sort of sweet spot.",
            "So that when I forward drop everything through my neural network, all the responses don't saturate and I get everything in.",
            "Kind of a nice range so that when you do gradient descent everything works out for you.",
            "And then as a last to sort of little bit, I'm going to talk about unsupervised pretraining later, which is something you may have heard of before, which is actually a sort of brilliant initialization scheme that can give much better results.",
            "But it's somewhat more difficult to get working."
        ],
        [
            "We also talked about gradient descent and so even though we would love to use sort of off the shelf optimizers that don't take a lot of tuning, in practice when we work on really large scale problems, we're going to use something like stochastic gradient descent, and it turns out that you have to set the step size and stochastic gradient descent very carefully if you set it too large.",
            "You can often make a lot of progress early, but then as you get to the end, things will slow down because you're sort of just bouncing around in space and you're not getting too.",
            "More tightly tuned models and it turns out that up to a factor of two can matter.",
            "So when you're setting this step size, which I've labeled data everywhere in these equations.",
            "When you're setting this step size, it's worth spending some time to get it right and try a whole bunch of versions so some different strategies for setting this.",
            "For example, to just brute force it so you have a big cluster, feel free to spin off a dozen experiments with 10 different sizes, so 10 to the minus 310 to the minus, 410 to the minus 2 whatever, and try a whole bunch of them and just see which one gives you the best performance on some kind of validation set.",
            "If you can't afford that, or for example, if you're working on a really large scale problem, there are a few tricks for sort of guiding you to get started.",
            "An interesting one, which actually I believe is courtesy of Max Welling.",
            "Is to look at a histogram of all of the updates that you're making to your weights.",
            "So run a couple of gradient descent steps and look at a histogram of the size of the update you're making to each of your weights an what you would like to have happen is for a typical update to be something like 1 / 1000 times, the magnitude of the weight.",
            "So if you wanted to completely change that, wait to let's say, zero or negative.",
            "The weight or twice its value.",
            "Something like that.",
            "If you wanted to dramatically alter it, it would take something like a.",
            "1000 iterations and it turns out that if you have a much larger fan in, if you have lots and lots of inputs, then you want to make something smaller than one in 1000, but this is a good way to get started and finally one that I've heard is pretty handy is if you can run maybe two or three step size at a time.",
            "You can race them.",
            "You can have a little test and you see which one reduces the error on your test data most rapidly, but you've got to be a little bit careful with this strategy because if you don't run it long enough.",
            "You can run into the problem I told you earlier where it'll look like you're making lots of progress early, but you actually get a worse model in the wrong in the long run.",
            "And finally.",
            "Most people use some kind of schedule for decreasing the step size gradually as you run.",
            "This usually gives you better convergence guarantees, so very simple one to start out with this recommended by Yoshua Bengio is to keep the step size constant for a little while up to some threshold Tau and then once you've passed Tau iterations you start decaying it like one over the number of iterations.",
            "So there's a whole bunch of different adaptive stepsize schemes, and so on.",
            "But when you first start out, this should get you going for finding a step size that works well.",
            "And finally."
        ],
        [
            "I want to talk about momentum, so this is a pretty cool trick that has been around a long time and genuinely appears to make a huge difference in terms of what works and what doesn't.",
            "So using stochastic gradient you can usually get some networks working, but for deep networks with lots of parameters and some complex functions built in, this is a pretty pretty neat trick that will get you some of the benefits of a fancier method like a second order method without the trouble.",
            "So the basic idea behind momentum is that instead of just using the gradient as my update to my parameters, I'm going to keep a sort of moving average, like our exponentially weighted average of all of my previous gradients, and I'm going to have some parameter mu here, which is between zero and one, and I'm going to have another value held out.",
            "It's the same size as Theta that I've labeled it V. Here for the velocity and.",
            "Essentially what you're going to do is every time you computer gradient, you're going to sum in a small contribution from this gradient while adding in some decade value of your previous velocity.",
            "And then you're going to use that guy for your update to your parameters.",
            "And the into."
        ],
        [
            "Ocean behind this is that it's a little bit like second order information is telling you something about how curved this space is that you're trying to optimize in.",
            "So, for example, if you're trying to optimize a function that's like a very steep Valley, then one problem with gradient descent is you can end up bouncing back and forth between the walls of this Valley without making very much progress in the direction that goes downhill within the Valley.",
            "An when you use momentum is kind of.",
            "Sort of neat property that these oscillations cancel out, whereas in the direction that's going downhill consistently, you'll have all of your updates adding up, so you can think of this like a ball rolling down a Hill so that as it sort of weaves between the Hills it eventually start going straight and it'll roll faster and faster and faster as it goes downhill.",
            "So this is a pretty neat trick."
        ],
        [
            "It works pretty well and there's some recent work from alias US cover.",
            "This suggests, for some very difficult to optimize neural networks using very large values of the momentum can actually help, so the one downside to this is that you have this extra parameter, mu, that you now need to set.",
            "So in addition to this step size that we have to figure out, we also need to come up with some scheme for setting the momentum and some typical advice is to start out with mu fairly small, something like 0.5 so that the momentum isn't doing too much for you.",
            "And then to gradually increase this to something like 0.9 or 0.99.",
            "If you set it too high, you'll eventually start seeing that your objective is oscillating again, and you're not making progress, but somewhere in this range seems to work well, and I think it's worth saying there is a slightly more sophisticated formula than this one that's actually not much harder to implement, called Nesterov's accelerated gradient, and it looks like that also helps quite a bit, so this is a pretty simple method.",
            "It's really fast, you can implement this for all of your neural networks.",
            "Fairly straightforwardly by just changing your gradient descent code, and it appears to make quite a big difference in practice."
        ],
        [
            "Just a couple of other factors that I think are important.",
            "One is that weight decay, which is a sort of.",
            "Basically just a buzzword for adding a squared penalty to the magnitudes of all your weights.",
            "That can help avoid things like saturation and so on.",
            "So if you keep finding that all of your units are saturating no matter what you do, because the weights in your neural network are growing too large, just putting a very weak squared penalty to keep the weight small in your objective function can help.",
            "And finally, if you have a small data set not too large of a neural network, you can always go for a fancy method like elegs.",
            "Are one of these second order methods, and these take a lot less tuning.",
            "In fact, for debugging, this is a really great sanity check."
        ],
        [
            "So that's again we should pause for questions, but this is the basic idea behind.",
            "Building a neural network, getting it working on labeled data, and these are essentially the basic tool kit that you should start from.",
            "So everything I'm going to talk about for the rest of my time here, about 4040 minutes or so is going to be about how to apply this to images and also a bit about unsupervised deep learning.",
            "But do people have questions about the algorithms and stuff gone through so far?",
            "The question is how we try."
        ],
        [
            "Non gradient based optimization methods.",
            "So I guess if you for the cases when you don't know the gradient, you have to go to.",
            "I guess you would call a zeros order optimizer and such optimizers exist.",
            "You basically can only probe the function value.",
            "The problem with that is that you're getting so little information about the shape of this function that that is very hard to make progress on.",
            "The number of parameters that are in these networks.",
            "That said, whenever you see these neural networks implicitly, did we lose audio?",
            "Whenever you see these neural networks implicitly, there are a whole bunch of hyperparameters in there, like how wide the network should be, what should all the optimizer parameters be, and so on.",
            "And we can't really compute gradients with respect to those.",
            "So hyperparameter search methods is currently a pretty interesting area, like how do you search over all these hyperparameters to find the best network?",
            "And those are usually not gradient based.",
            "Any other questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks a lot for the intro guys, so I'm Adam Coates and as I mentioned, I'm currently a postdoc at Stanford, but I'm also a visiting scholar at Indiana University Bloomington.",
                    "label": 0
                },
                {
                    "sent": "And when?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to talk about here is actually trying to.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a little different from the abstract.",
                    "label": 0
                },
                {
                    "sent": "In fact, I want to change some things to try to get enough in here in the amount of time we've got.",
                    "label": 0
                },
                {
                    "sent": "One of the things I'm going to try to go through real slowly as the very basics of what do we want machine learning to do?",
                    "label": 0
                },
                {
                    "sent": "Why is deep learning important?",
                    "label": 0
                },
                {
                    "sent": "Try to give you some of the basic tools for actually doing some deep learning yourselves, and there's sort of a story many of you may have heard about about unsupervised learning, and feature learning that I'll try to get to at the end, but I wanted to make sure to go through some of the low level stuff.",
                    "label": 0
                },
                {
                    "sent": "To give you an idea for how to actually use these things so that when you go home and look at some of the resources that I'm going to have linked at the end of the talk, you'll be able to go do an online tutorial and actually play with some of these things and even have enough knowledge behind you to do some real applications.",
                    "label": 0
                },
                {
                    "sent": "So I want to start to motivate all this stuff by sort of talking about what is it that we want machine learning to do, especially with computer vision.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that we have these applications where you have some input like.",
                    "label": 0
                },
                {
                    "sent": "In image and we want to do something like object recognition.",
                    "label": 1
                },
                {
                    "sent": "We want to be able to look at this low level pattern and make a prediction about what's going on.",
                    "label": 0
                },
                {
                    "sent": "So for example, an object recognition we want to predict whether there's a cat in the image or not.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For something like detection, the output is a little different.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to look at this image and figure out that this is the location of the bicyclist and from.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or sophisticated application.",
                    "label": 0
                },
                {
                    "sent": "For instance segmentation maybe went on to look at this image and figure out the outline of the birds or outline of the objects in the scene.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the.",
                    "label": 0
                },
                {
                    "sent": "One of the sort of common approaches to machine learning.",
                    "label": 1
                },
                {
                    "sent": "There are whole bunch of different ways of solving these problems, but a pretty large number of these approaches are sort of covered by a common pipeline where we take this input example, an image of a cat, let's say, and then we have some sort of black box system that extracts features.",
                    "label": 0
                },
                {
                    "sent": "It tries to extract some sort of higher level patterns.",
                    "label": 0
                },
                {
                    "sent": "And then we usually have some final off the shelf machine learning algorithm that we just drop all this information into and it tries to learn to make decisions starting from this sort of higher level representation.",
                    "label": 1
                },
                {
                    "sent": "So we might want our machine learning algorithms tell us is this a cat pictured in this image, and the thing that ends up taking all of our time that causes us a lot of grief is this feature extraction piece, because you know we can try to get pretty sophisticated here.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you'll see this box taking on some pretty complex engineering where we're trying to build.",
                    "label": 0
                },
                {
                    "sent": "Several layers, in fact, of increasingly high level abstractions and the way that we do that, is that we try to take all of our prior knowledge about the world, how images work, what we might see in images, and so forth, and somehow wire them into this box.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you do a really good job, then you can get great results.",
                    "label": 0
                },
                {
                    "sent": "But if you're not a computer vision expert, you haven't done this before.",
                    "label": 0
                },
                {
                    "sent": "Then you can end up with really terrible results.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is sort of a starting point for talking about deep learning.",
                    "label": 1
                },
                {
                    "sent": "What deep learning is and why we hope this can work, because what we'd like to do is keep this same kind of pipeline.",
                    "label": 0
                },
                {
                    "sent": "This basic idea that we want to take our input image and then feed it through several modules where we're going to extract higher and higher level of features.",
                    "label": 0
                },
                {
                    "sent": "So we might start with some low level features, like detecting edges in an image, and then we want to move to sort of middle level features like recognizing parts and so forth to maybe some higher level structures like recognizing objects.",
                    "label": 0
                },
                {
                    "sent": "And then at the very end, only at the very end we're going to have some classifier that makes some application specific decision.",
                    "label": 0
                },
                {
                    "sent": "Like, is this a cat where the outlines of an object and so on, and the basic idea is that whenever we build these pipelines where going from sort of low level ideas and getting more and more abstract representations as we go up and whereas we normally build that by hand, we want to somehow train these multiple layers from data and try to discover a representation so that by the time we get to the high level features out here.",
                    "label": 1
                },
                {
                    "sent": "If we just train a classifier on the end, we can make really good decisions.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind deep learning is to figure out a way to train this whole pipeline of systems and not have to engineer them so much by hand.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's worth asking why is it just upfront?",
                    "label": 0
                },
                {
                    "sent": "Why do we want to even do this deep learning thing?",
                    "label": 1
                },
                {
                    "sent": "There are handful of sort of simple justifications.",
                    "label": 1
                },
                {
                    "sent": "One is that we just know sort of inherently from working on this for so many years that some decisions just require a lot of stages of processing.",
                    "label": 0
                },
                {
                    "sent": "It's very unlikely that I can see an image and decide whether there's a cat in an image or a dog in the image, just with a linear function is somehow too simple.",
                    "label": 0
                },
                {
                    "sent": "We know that it can't be that simple.",
                    "label": 0
                },
                {
                    "sent": "And all the systems we currently have, no matter how we built them, involve many different stages of processing.",
                    "label": 0
                },
                {
                    "sent": "So we sort of know that we want this long pipeline of decision-making, but unfortunately trying to engineer that by hand is quite difficult.",
                    "label": 0
                },
                {
                    "sent": "And we already have been intuitively engineering these sorts of things in computer vision for quite awhile, so if we're already building these kinds of architectures, a very natural thing to try to do is to be able to actually learn them into them from data.",
                    "label": 0
                },
                {
                    "sent": "And finally, and I think this was sort of under appreciated early in a lot of this research is that the algorithms I'm going to talk about today are actually based on a pretty small subset of operations that turn out to scale incredibly well with hardware.",
                    "label": 0
                },
                {
                    "sent": "So there's a sort of long running phenomenon in machine learning where if you want to get really great results and go to a conference and publish and so forth.",
                    "label": 0
                },
                {
                    "sent": "A way to do that is to just get a lot more data with a lot bigger computer.",
                    "label": 1
                },
                {
                    "sent": "You can almost always do better, and with a lot of algorithms when we sort of build them by hand, we have all these rules coded in.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to sort of scale up and take advantage of all these things because you have so much knowledge wired in and less you're more clever unless you can get increasingly clever, it's harder and harder to get better results.",
                    "label": 1
                },
                {
                    "sent": "But if we have a learning algorithm that scales really well with hardware and with more data, then it becomes easier to get results as we get better hardware and we get larger datasets.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that's really nice about deep learning is that whenever we get a faster computer or we get a larger data set, we can train a bigger model.",
                    "label": 0
                },
                {
                    "sent": "So hopefully you'll see by the end of this that most of these things are very simple, dense linear algebra operations and they run really fast on a computer on it or on a GPU.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to sort of also answer another question, up front one of the questions is have we been here before?",
                    "label": 0
                },
                {
                    "sent": "So a lot of deep learning spawns from neural networks research which some of you may have been familiar with if you took like an introductory AI course and a lot of that was sort of having its heyday in like the 1980s and so on, so it's worth trying to understand has anything really changed since then?",
                    "label": 0
                },
                {
                    "sent": "So the answer to the question have we been here before?",
                    "label": 1
                },
                {
                    "sent": "There are two possible answers, yes and no.",
                    "label": 0
                },
                {
                    "sent": "And the on the yes side.",
                    "label": 1
                },
                {
                    "sent": "We say that the basic ideas here are actually pretty common to pass machine learning algorithms in neural networks.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to do supervised learning with labeled data, it turns out it's a very straightforward procedure, and I'm going to walk you through that in a second, but also sort of standard machine learning development strategies.",
                    "label": 0
                },
                {
                    "sent": "If you take a machine learning course, you learn about how to debug machine learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "All that stuff you can carry over.",
                    "label": 0
                },
                {
                    "sent": "And a lot of knowledge from problem domains like computer vision.",
                    "label": 0
                },
                {
                    "sent": "Different applications has also been carried over, so this is stuff that we sort of know that's old hat.",
                    "label": 0
                },
                {
                    "sent": "And so in this sense things haven't changed so much.",
                    "label": 0
                },
                {
                    "sent": "But there are a few things that have changed that are pretty critical.",
                    "label": 0
                },
                {
                    "sent": "So for example we have much faster computers and a lot more data, and it used to be the case that if you trained a neural network with a very small amount of data in order to get really good performance, you had to wire a lot of your own knowledge into it.",
                    "label": 0
                },
                {
                    "sent": "But now that we have huge datasets and really big computers to process them.",
                    "label": 0
                },
                {
                    "sent": "We can actually get away without that prior knowledge and what we want is a really flexible algorithm that can just gobble up all that data and make good decisions.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, the sort of operating regime where we want to work on applications has changed, so that's very new.",
                    "label": 0
                },
                {
                    "sent": "We are better at optimization.",
                    "label": 0
                },
                {
                    "sent": "We are better at figuring out how to initialize these neural networks and all these different models.",
                    "label": 0
                },
                {
                    "sent": "We want to train.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of useful research there, and in fact one of the key results that's responsible for bringing all this stuff to you.",
                    "label": 0
                },
                {
                    "sent": "Of late it was from Jeff Hinton, Yoshua Bengio back in 2006, which is effectively a very, very cool initialization procedure that made all these training algorithms work much better.",
                    "label": 0
                },
                {
                    "sent": "And finally, I think this is also another underappreciated point, which is that.",
                    "label": 1
                },
                {
                    "sent": "We now have a ton of empirical evidence, partly from having faster computers and more students running lots and lots of experiments with these things.",
                    "label": 0
                },
                {
                    "sent": "We know much better.",
                    "label": 0
                },
                {
                    "sent": "What are all the various components that matter and sort of neat fallout of using neural networks and using a common framework for all this is that when your friend tells you at a conference hey, I just tried this new module inside my neural network.",
                    "label": 0
                },
                {
                    "sent": "You can go home and re implement that thing very quickly and mix it and match it with all your other components.",
                    "label": 0
                },
                {
                    "sent": "So there's been quite a bit of work figuring out.",
                    "label": 0
                },
                {
                    "sent": "What are the right modules to be plugging into these things that we can all reuse?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is pretty great and this is.",
                    "label": 0
                },
                {
                    "sent": "This is now having a real impact used to be.",
                    "label": 0
                },
                {
                    "sent": "That is pretty tough to do a real application because you needed all that prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "You had to be a domain expert and deep learning systems now are starting to really come of age and we're seeing a lot of state of the art results not just in computer vision but in lots of different fields.",
                    "label": 0
                },
                {
                    "sent": "Most recently very cool result by Alex Kryszewski ileus US cover and Jeff Hinton from NIPS 2012.",
                    "label": 0
                },
                {
                    "sent": "Was this basically deep neural network that is currently holding the state of the art results on the image net data set so very large data set they hold a challenge every year called the image net large scale visual recognition challenge and their deep neural network was the 1st place system in this challenge and I'll show you some results from that later.",
                    "label": 0
                },
                {
                    "sent": "But hopefully by maybe an hour into this talk you'll have all the basic underpinnings to implement a system like that yourself.",
                    "label": 0
                },
                {
                    "sent": "But it's worth saying that outside of vision, these things are also doing incredible things in speech and natural language processing.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole bunch of other stuff to go along in those fields that I hope you'll go and read up on at some point.",
                    "label": 0
                },
                {
                    "sent": "There's some pretty cool things that's going on outside of vision as well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quick outline here 'cause we don't have a ton of time.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go through a sort of machine learning, refresher or crash course just to kind of sort of refresh you if you take a machine learning class, but it's been awhile, or if you haven't taken a machine learning class, maybe you've been through like Intro AI or something.",
                    "label": 0
                },
                {
                    "sent": "Sort of go sense for how machine learning is done, how it works.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to go through a sort of natural extension of that to supervised deep learning and sort of give you some tools for actually training a deep neural network.",
                    "label": 1
                },
                {
                    "sent": "So the basic algorithms used there and then also tell you a bit about what are the application specific things we want to use for computer vision, or if you want to use neural networks for vision problems.",
                    "label": 0
                },
                {
                    "sent": "Turns out you need some extra ingredients, and I'll tell you what those are.",
                    "label": 0
                },
                {
                    "sent": "I'll say a bit about debugging.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a couple of slides on that, and then I'll go back to what may be a story more familiar to some of you have been watching deep learning for awhile, which is about unsupervised deep learning that involves using unsupervised algorithms where we don't have labeled data for trying to train deep networks and then at the very end we're going to have some references and resources for you guys to pick up on the web, including a link to a tutorial that you can do online later.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In supervised learning, which is sort of a common mode that we want to operate in, we're going to be given a whole bunch of training examples and our training examples all have a label associated with them.",
                    "label": 0
                },
                {
                    "sent": "That's usually given to us by a human or some kind of system with a lot of knowledge in it.",
                    "label": 0
                },
                {
                    "sent": "And basically we have a whole bunch of pairs, so the first element of this pair is a vector X, and this is essentially our input data.",
                    "label": 0
                },
                {
                    "sent": "It's the thing that we're given an.",
                    "label": 0
                },
                {
                    "sent": "Then we have some output Y, which is the variable that we want to predict.",
                    "label": 1
                },
                {
                    "sent": "And why could be a vector?",
                    "label": 0
                },
                {
                    "sent": "It could be a whole bunch of continuous quantities, or it might be discrete.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the case we've been talking about where someone gives me an image and I would like to know, is this a cat, we could represent the cat as a vector of pixel intensities, and then we're going to pass it through some function that I'm just going to call F of X here.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to ask it to predict a discrete binary label, either 0 if it's not a cat or a one if it is a cat, and the question for machine learning.",
                    "label": 1
                },
                {
                    "sent": "For our supervised learning algorithm is to try to find some F of X so that it correctly predicts the label Y on all of our training examples.",
                    "label": 0
                },
                {
                    "sent": "So whatever I put X in, it correctly says whether this is a cat or not.",
                    "label": 1
                },
                {
                    "sent": "A cat for all of the examples or as many examples as possible in the training set that we've been given.",
                    "label": 0
                },
                {
                    "sent": "And hopefully if all goes well, this learned predictor is going to work on some test data, so that if I give you an image you've never seen before, you actually get the correct prediction out of it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we just want to review a very simple binary classification algorithm.",
                    "label": 1
                },
                {
                    "sent": "So if you want to actually solve this problem, one thing that you could try doing is to start from a function that has this form.",
                    "label": 0
                },
                {
                    "sent": "So it's F of X and then this little; Here just means that the variables to the right are sort of parameters, but you can just think of X and Theta.",
                    "label": 0
                },
                {
                    "sent": "Here is being two inputs to F. And F of X is going to have this definition where it's 1 / 1 + X of minus the inner product or dot product of Theta and X.",
                    "label": 0
                },
                {
                    "sent": "So this linear function here is really.",
                    "label": 0
                },
                {
                    "sent": "Going to try to give positive values to things that are cats and is going to try to give negative values to things that are not cats, and this nonlinear function that sort of wrapped around it, which I'm going to label Sigma everywhere, is a sigmoid function and you can just think of it as a squashing function.",
                    "label": 0
                },
                {
                    "sent": "It's just going to take this otherwise linear value and scrunch it down so that it fits into the range between zero and one, and the reason we want to do that is so that we can interpret the output of this function F of X as a probability.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to try to train this F of X function to tell us the probability that the thing pictured in this image represented by X is actually a cat.",
                    "label": 0
                },
                {
                    "sent": "So we want it to be large when Y is equal to 1, meaning it's a cat and we want it to be small and Y is equal to 0, meaning it's not a cat.",
                    "label": 0
                },
                {
                    "sent": "And to do that, I'm not going to go through exactly where this function comes from, But this is an objective function that we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "So basically what we want to do here is this cost function or loss function as a function of Theta tells us how well our function F fits our training data.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is I'm going to sum over all the examples and whenever the label is one, I want to basically make this value in here large.",
                    "label": 1
                },
                {
                    "sent": "Which is actually the probability that Y is 1 an whenever the label says that it's zero.",
                    "label": 0
                },
                {
                    "sent": "I want to make this probability very small.",
                    "label": 0
                },
                {
                    "sent": "So if we find the Theta that minimizes this, that's also going to be the one that makes good predictions on our training set.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we actually tune Theta to minimize this thing?",
                    "label": 1
                },
                {
                    "sent": "One algorithm that we can actually use to do this.",
                    "label": 0
                },
                {
                    "sent": "In fact there are lots of algorithms we could choose, but almost all of them share the same basic approach, which is that we need to compute the gradient of this loss function, and this is something you can do by hand for logistic regression if you just go pull out your calculus book and compute a bunch of derivatives, then you find that the gradient looks like this.",
                    "label": 0
                },
                {
                    "sent": "And you can pass this to some off the shelf optimizer if you want.",
                    "label": 0
                },
                {
                    "sent": "Or you can just implement your own gradient descent algorithm where what we do is we pick some initial guess for Theta that could just be random values, and then what we're going to do is every time we want to make an update, we compute this gradient and then we make a small change to Theta that points in the negative direction.",
                    "label": 0
                },
                {
                    "sent": "So remember that the gradient is pointing in the uphill direction.",
                    "label": 0
                },
                {
                    "sent": "So if I change Theta in the opposite direction, I'm going to go downhill.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're going to return to this with ways to make it much better when we talk about deep learning.",
                    "label": 0
                },
                {
                    "sent": "And finally, a sort of technical difference when we actually implement this is when we have a lot of data we don't sum over the whole data set.",
                    "label": 0
                },
                {
                    "sent": "We just take a few examples, maybe one example or 100 examples, and compute the gradient from those before we make an update.",
                    "label": 0
                },
                {
                    "sent": "That way we can make lots of updates very quickly and hopefully make more progress.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out this is pretty nice.",
                    "label": 0
                },
                {
                    "sent": "This function that I showed you that we want to minimize is convex, so that very simple optimizer that I showed you is always going to take you to some nice minimum once you set the stepsize properly.",
                    "label": 0
                },
                {
                    "sent": "And this works for simple problems.",
                    "label": 1
                },
                {
                    "sent": "So if I give you these digits of zeros and ones and I give you the labels, it tells you whether there zeros or ones.",
                    "label": 0
                },
                {
                    "sent": "It turns out this this classifier will work great, but the reason that it works is because there is some pixels in these things that turn out to be very highly informative.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Pixel in the center, it usually tells you whether it's a zero or a one cousin, the zeros it's in the hole, and in a one it's turned on, so it turns out that a linear function works pretty well, but if I give you something more complex.",
                    "label": 0
                },
                {
                    "sent": "That, to our eyes seems very simple.",
                    "label": 0
                },
                {
                    "sent": "This algorithm will fail completely.",
                    "label": 1
                },
                {
                    "sent": "So for example, if I ask you is this a coffee mug you have very little hope and you can take my word for it.",
                    "label": 0
                },
                {
                    "sent": "'cause this is a problem that I have played with for quite for quite awhile.",
                    "label": 0
                },
                {
                    "sent": "So it's worth asking why.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this so darn hard?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you know you and I see a coffee mug here, but of course the computers seen this giant grid of numbers an once you're looking at this grid of numbers is not clear at all what's going on.",
                    "label": 0
                },
                {
                    "sent": "Just turns out that Pixel intensities are actually a really poor representation, not just for humans to be reading grids of numbers, but for computers even to understand what's happening in this image and to sort of under.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more deeply, what's the problem here?",
                    "label": 0
                },
                {
                    "sent": "I want to take a sort of extreme example of this, so I'm going to take a picture of my coffee mug here.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to represent it not by thousands of pixel intensities, but just by two.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to pick two pixels here, and I'm going to represent this image using only the intensity at those two points.",
                    "label": 0
                },
                {
                    "sent": "So for example, this image might get mapped to a 2 dimensional vector where the values are 72160 depending on what the pixel values are.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to plot this point on a little 2D scatter plot where I said that this is a positive example.",
                    "label": 0
                },
                {
                    "sent": "This is a coffee mug and then I'm going to do that for a whole bunch of coffee mugs or cats.",
                    "label": 1
                },
                {
                    "sent": "Whatever we're working with.",
                    "label": 0
                },
                {
                    "sent": "Then I'm also going to do it for a whole bunch of things that are not coffee mugs.",
                    "label": 1
                },
                {
                    "sent": "So for instance, coffee cups and four random clutter in the world.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do this for a whole bunch of points, then we get something that looks like this and now what we're sort of asking our algorithm to do when we're giving it these pixel intensities is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please tell me for this new point.",
                    "label": 0
                },
                {
                    "sent": "Is this a coffee mug and it's sort of clear in this case that this is totally unfair, that even if we had a brilliant classifier that could somehow memorize all these examples to ask whether this is a coffee mug is totally ambiguous, we can't possibly hope for it to give us the right answer, because this representation that I've chosen is a really bad one on the other.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we could somehow figure out, for instance, that I have a way to detect handles, let's say I also have a way to detect cylinders, then if.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plot these two features or the responses of these two detectors as my as my way of representing this image.",
                    "label": 0
                },
                {
                    "sent": "Then it will turn out that all the coffee mugs or things in the upper right corner and all the things that are not coffee mugs are in the lower left corner and now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it's actually a fair game which we asked our machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is in a coffee mug.",
                    "label": 1
                },
                {
                    "sent": "Not only can it separate these things into two categories, but there's some chance that you could generalize to new things you hadn't seen before.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this is sort of what's behind our standard machine learning pipeline, which is that we know that our machine learning algorithm cannot solve this problem from raw pixels.",
                    "label": 0
                },
                {
                    "sent": "So what we end up doing is we hardwire some function Phi into the system, so that when we plug X into fee, it outputs some new representation.",
                    "label": 1
                },
                {
                    "sent": "So if X had, say, an pixels, then it's going to output K features and we can just drop those things into our logistic regression algorithm, let's say, and try to do better.",
                    "label": 0
                },
                {
                    "sent": "So if you've been doing work with machine learning algorithms in computer vision, you're probably familiar with this kind of pipeline.",
                    "label": 0
                },
                {
                    "sent": "There are lots and lots of choices for things we can use for Fi.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, where do we actually get these features?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I mentioned in the sort of overview earlier on, we basically built them by hand at this point, so this is a tried and true approach.",
                    "label": 0
                },
                {
                    "sent": "It actually works great to get you started for a whole lot of things, but in reality for the hardest problems like computer vision, a huge huge amount of investment goes into this.",
                    "label": 0
                },
                {
                    "sent": "So for example, everyone of course is probably familiar with the SIFT features, which are probably the most some of the most successful engineered features in both computer vision and machine learning in general.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But we also can think of things like super pixels as being another way of kind of transforming our input through a very complex pipeline into some representation that we think the machine learning algorithm can actually deal with.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of if you were to take a machine learning classes like one class of algorithms, this is a pipeline you're probably familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea behind deep learning is that we saw how to do supervised learning.",
                    "label": 1
                },
                {
                    "sent": "When we have these hardwired features, we just take Phi of X, which is going to be fixed.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to dump it into our off the shelf machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is somehow extend this to the case where these features are given by tunable functions, functions that themselves we can actually learn.",
                    "label": 1
                },
                {
                    "sent": "So for example, in the case of binary classification, what we wanted to do was get a function that was an estimate for the probability that Y is equal to 1, which is itself now going to be the sigmoid function applied to, not a linear function of the features, but a linear function of yet another function which is computed from the data and the thing that I've added here is W * X, where W is a matrix followed by a Sigma.",
                    "label": 0
                },
                {
                    "sent": "Followed by this sigmoid function again.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And think of this function having two parts.",
                    "label": 0
                },
                {
                    "sent": "There's this outer part, which is just the same pieces before it's from our logistic regression classifier.",
                    "label": 1
                },
                {
                    "sent": "And then there's this inside part which is basically taking the place of our features.",
                    "label": 0
                },
                {
                    "sent": "But this is now a tunable function and it has, as its output is going to be a vector an in this vector.",
                    "label": 0
                },
                {
                    "sent": "We basically have each row of W * X, and then the fact that I put the sigmoid around this vector means we're just going to apply that 1 / 1 + X function.",
                    "label": 1
                },
                {
                    "sent": "To every element of this vector.",
                    "label": 0
                },
                {
                    "sent": "So it's the same as before, but we've replaced our features with this tunable function now, so to do so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supervised learning for this two class problem, we're going to do essentially the same thing now, so we're just going to minimize this loss function that we used for logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Well, I've just rewritten it now to include these parameters W as something that we have to optimize over so when we go to minimize this function to find the best possible classifier.",
                    "label": 0
                },
                {
                    "sent": "We're not only searching for the best classification parameters, Theta were searching for the best features as well.",
                    "label": 0
                },
                {
                    "sent": "We're looking for the choice of W that makes that internal feature function down here work as well as it can for the classification that we want to do.",
                    "label": 0
                },
                {
                    "sent": "So this is the same as logistic regression, but now we have a function that's got several stages in it.",
                    "label": 1
                },
                {
                    "sent": "And another way to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of think about this or to write it is to think of it as a sort of sequence of little modules where we plug X in.",
                    "label": 0
                },
                {
                    "sent": "At the bottom we compute the sigmoid of WX, and that's going to give us a new vector H which we can think of as a feature vector, and then H is sort of forming like a new data set a new input that's just going to go into logistic regression like usual.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you've probably seen this in a bunch of different ways, but even though I wrote it as sort of a formula and then as a sort of block diagram, you can also just write this down as a neural network, so you've probably seen cartoons like this many times to represent a neural network, and this is just a graphical cartoon of showing you the exact same thing that I already told you.",
                    "label": 0
                },
                {
                    "sent": "So for example, each of these little circles here.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People will call them a neuron and I'm going to use that terminology to later much later in the talk, but those are just values in those in those feature vectors or in your input data.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is that if I have my pixels X from an image, I'm going to feed them into this neural network to compute H and then F, and then I'm going to compute my loss function at the end.",
                    "label": 0
                },
                {
                    "sent": "And you can think of the data as flowing upward in this network as we do the computation each of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These lines here corresponds to a parameter in this network that we have to train.",
                    "label": 0
                },
                {
                    "sent": "So for example, this sort of diagonal one right here, is the parameter that determines how H2 gets computed from X1, and so that's W21 the 2nd row first column of W.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And likewise for Theta, so they don't want in beta two and three, and so on up here.",
                    "label": 0
                },
                {
                    "sent": "So this is just a cartoon diagram of sort of how that function is put together.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, what we can do is extend this and we can have multiple layers and we have several layers we can think of, not just training one layer of features, but in fact trying to train a stack of features and train multiple stages of computation.",
                    "label": 0
                },
                {
                    "sent": "But again, this is just a cartoon.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moon way of depicting a function that looks like this one where I've got now two matrices of weights and each of them sort of stacked one inside the other.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can also draw it as a block diagram like this, where I make the intermediate values explicit.",
                    "label": 0
                },
                {
                    "sent": "So these are just three different ways of expressing the exact same function.",
                    "label": 0
                },
                {
                    "sent": "So how do we actually train this thing?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to tell you about the backpropagation algorithm, which is actually been around for a long time, but it's sort of key to understand how this works, and then I'm going to give you a whole bunch of tricks, and I'm going to give you lots and lots of information later on how to turn this into a much more powerful system so.",
                    "label": 0
                },
                {
                    "sent": "Basically, our goal is to minimize this loss function, so let's go back to the case where we just wanted to do binary classification with this one layer of features.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to minimize this thing over Theta and W. And just like before what we need out of this system is a gradient with respect to Theta and a gradient with respect to W. And if we have those then we can do gradient descent or we can run some other optimizer to minimize this function.",
                    "label": 1
                },
                {
                    "sent": "So if you want to, you can expand this out and at least the formula to get the gradient with respect to Theta is going to be essentially the same as before.",
                    "label": 0
                },
                {
                    "sent": "You could figure that out by hand, but the gradient with respect to W is going to be much more complicated and you don't want to do that by hand, so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way that we do this is through the chain rule, which you're probably familiar with from calculus and.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is that let's suppose we have some module.",
                    "label": 1
                },
                {
                    "sent": "Let's forget the rest of the function.",
                    "label": 0
                },
                {
                    "sent": "For now I've got some little black box black box module that takes in AZ&AW as two inputs and then it produces an output H and I don't know much else about this function, but I'm going to be given a couple of things.",
                    "label": 0
                },
                {
                    "sent": "One is I'm going to tell you ahead of time the derivative of your loss function.",
                    "label": 0
                },
                {
                    "sent": "Your objective function with respect to the output of this module.",
                    "label": 0
                },
                {
                    "sent": "So with respect to each of the elements HJ and then the one thing I'll tell you about this module, in addition to the inputs and outputs, is the derivative of each element of H with respect to each of the elements of Z, or for that matter, each element in W. That's what I'll tell you, and if you apply the chain rule.",
                    "label": 0
                },
                {
                    "sent": "What you'll end up with is that the derivative with respect to each of the elements in Z is equal to this summation, where I'm just taking the product of the derivatives with respect to H multiplied by the corresponding derivatives with respect to each of the elements in Z.",
                    "label": 0
                },
                {
                    "sent": "And many of you may recognize this if you recall your calculus book that this is the all these elements, the HJS with respect to the K values.",
                    "label": 1
                },
                {
                    "sent": "Here are the elements of a Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "If you don't remember that term, that's OK. Just take my word for it that this simple function.",
                    "label": 0
                },
                {
                    "sent": "It can actually be just written as a matrix multiply, but the key thing to take away is that if you know these two pieces of information, I can implement another little black box that takes in the gradient with respect to H and spits out the gradient with respect to Z.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And similarly for this weight matrix, WI can make another little machine that takes in the gradient with respect to H and gives me the gradient with respect to W. So you can take.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rule and just remember that for any black box model we've got, if we can give you these derivatives, then we can.",
                    "label": 0
                },
                {
                    "sent": "Make this change.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can build up sort of a toolkit.",
                    "label": 0
                },
                {
                    "sent": "We can build a CHEAT SHEET if you will.",
                    "label": 0
                },
                {
                    "sent": "That tells us for any of these different functions that we use in our neural network tells us how to take the gradient with respect to H, which is the output an converted into a gradient with respect to Z or a gradient with respect to the parameters W. So for a simple linear function, for example the gradient with respect to Z turns out to be W transpose times the gradient with respect to H and for to get the derivative with respect to the parameters W. Turns out to be the gradient with respect to H * Y transpose and this will give you a matrix that is just your update that you want to make to W. So as you start to do a lot of work with neural networks, or maybe someone tells you about some module they just tried in their neural network at a conference.",
                    "label": 0
                },
                {
                    "sent": "Once you have these little pieces, you can just drop it into your system and in fact.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is how automatic differentiation works in a library like Theano, which is a cool neural networks package that works on GPU's, there will be a link to at the end.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we got this, we can just re apply this rule everywhere and every time we've got these feedforward functions where we can compute our output F. We also have a backward module that corresponds to it so that if someone gives us the gradient with respect to F at the very top, that tells us how we can improve our loss.",
                    "label": 0
                },
                {
                    "sent": "We can work that backward to find out how to change W and how to change data.",
                    "label": 0
                },
                {
                    "sent": "So every time you make a forward pass you can also make a backward pass that tells you how to update.",
                    "label": 0
                },
                {
                    "sent": "Each of these variables.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just as a more concrete example, in the case of this network that I was showing you earlier where we had one layer of features and we have these sigmoid functions in here, we can run this forward to find F. That's just the forward pass to compute the outputs of our system, and then if you can compute the gradient with respect to F from your loss function, which you can do by hand.",
                    "label": 0
                },
                {
                    "sent": "Then using the information in our table, we can find how to turn this into a gradient with respect to H, which turns out to be this function.",
                    "label": 1
                },
                {
                    "sent": "Here we just took a couple of the rules out of the table to make this, and then once we know that guy we can use a couple more rules from the table to compute the gradient with respect to W. So it turns out to be this formula here so you can see you could actually just implement this as an algorithm on your computer, compute the gradient with respect to H and then the gradient with respect to W and now you can use the same optimizer as before.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the whole training procedure here boils down to collecting a bunch of training data just like we would do for logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Compute the gradients using this forward and backward propagation scheme, and then once we know those gradients then will make us gradient descent step for each of the parameters and we just keep doing this till convergence.",
                    "label": 0
                },
                {
                    "sent": "So this is the classic back propagation scheme.",
                    "label": 0
                },
                {
                    "sent": "Many of you may have seen before and.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this historically has not worked so easily.",
                    "label": 1
                },
                {
                    "sent": "If it were this simple, I think we figured it out along time ago.",
                    "label": 0
                },
                {
                    "sent": "So a lot of the nice things that made logistic regression so easy to implement no longer apply, so this sort of seemingly simple neural network is now a nonconvex optimization problem, which means you can get trapped in local minima.",
                    "label": 0
                },
                {
                    "sent": "You can end up in places where the function is decreasing, but it's decreasing so slowly that you can't tell.",
                    "label": 0
                },
                {
                    "sent": "Or it may become.",
                    "label": 0
                },
                {
                    "sent": "Or you may have a problem with something that's known as the vanishing gradient problem.",
                    "label": 0
                },
                {
                    "sent": "So huge issue in early neural networks research, where as you add more and more stages to this neural network, you'll find that the gradient with respect to W down at the very bottom is essentially 0, even though it's obvious you could somehow improve over your random initialization.",
                    "label": 0
                },
                {
                    "sent": "Your gradient descent algorithm will keep telling you that the gradient with respect to all these things is 0, and you can't make progress.",
                    "label": 1
                },
                {
                    "sent": "And unfortunately, it's just been generally hard to diagnose and debug all of the various problems that come up here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get around that.",
                    "label": 0
                },
                {
                    "sent": "It turns out that many things matter in order to make this work, it comes down to the details.",
                    "label": 1
                },
                {
                    "sent": "So the choice of the nonlinear functions that you use matters a lot, so I use the sigmoid function everywhere, which turns out not to be the best choice in the world.",
                    "label": 0
                },
                {
                    "sent": "The initialization of the parameters matters if you just pick a whole bunch of random values for Theta, but they're too big, too small.",
                    "label": 0
                },
                {
                    "sent": "What have you then that turns out to cause trouble.",
                    "label": 0
                },
                {
                    "sent": "And finally, the optimiser that you use and how you set that up turn out to matter a lot as well.",
                    "label": 1
                },
                {
                    "sent": "So before I jump into a whole bunch of extra material on how to actually get this to work, I want to check to see if there are any questions about the sort of basic stuff we went through so far.",
                    "label": 0
                },
                {
                    "sent": "Always good.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go through each of these things and say a little bit about what does and what does not.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, the choice of nonlinearities turns out to be pretty critical.",
                    "label": 1
                },
                {
                    "sent": "So what function you use inside your network determines what kinds of features and you're going to end up with and how they behave.",
                    "label": 0
                },
                {
                    "sent": "So some functions that people have tried or things like that and H function, which is actually very similar to the sigmoid, it's the same shape, but it's been shifted so that at zero it has a zero output instead of instead of 0.5.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, you can use like absolute value rectification, which sort of to me doesn't seem like it should have worked so well, but turns out to be pretty useful.",
                    "label": 0
                },
                {
                    "sent": "An increasingly something called a rectified linear unit, which is really just a nice name for a function that zero until you reach a threshold value of 0, and then it's strictly linear after that.",
                    "label": 0
                },
                {
                    "sent": "And even though these are very simple, and they're not terribly different when you plot them like this, they have.",
                    "label": 0
                },
                {
                    "sent": "Huge difference in terms of how, in terms of what kinds of functions you're going to represent and also how easy it is to optimize all the parameters in your network.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the actual is initialization also matters a lot.",
                    "label": 0
                },
                {
                    "sent": "Usually we pick small random values and one thing that you should be careful about is you should try to choose these random values so that you don't accidentally end up in parts of your neuron response or your feature response.",
                    "label": 1
                },
                {
                    "sent": "That will cause grief for your optimizer.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we're using a sigmoid unit, we really want to avoid these areas way out here at the edge, because if you look at the derivative.",
                    "label": 0
                },
                {
                    "sent": "Of this curve, the derivative is really tiny and as a result if you try to use gradient descent whenever your neuron ends up out in this regime, the gradient is going to be essentially zero, and it's going to have a hard time making progress.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the problems that causes that vanishing gradient issue that I told you about earlier.",
                    "label": 0
                },
                {
                    "sent": "But you also want to avoid things like non differentiable parts of your function, so if you're using absolute value rectification, sometimes you'll want to.",
                    "label": 0
                },
                {
                    "sent": "You'll certainly want to avoid setting things to exactly 0, but people will often put like a little epsilon in there so that it smooths out the bottom of these functions.",
                    "label": 0
                },
                {
                    "sent": "Anna good good practice when you first start out with these things is to occasionally go through your neural network and actually inspect the unit.",
                    "label": 0
                },
                {
                    "sent": "So if you've been running it for a little while and your gradient descent optimizer stops making progress, it's good to just look at a histogram of all the responses.",
                    "label": 0
                },
                {
                    "sent": "And if you see everyone at zero and one and nothing in between, that's a bad sign, so that's worth doing.",
                    "label": 0
                },
                {
                    "sent": "And finally, we're to caution, often you can put very large values in an.",
                    "label": 0
                },
                {
                    "sent": "Your optimizer will look like it's making lots and lots of progress, and you say, oh.",
                    "label": 0
                },
                {
                    "sent": "Fantastic, I'll use larger values, but it turns out that these things can actually give you worse models, so the correct scale to set these add is something you sort of got to figure out and for specific situations like these 10 H units, it turns out there's some guidance in the literature on good ranges to set them too, so for example.",
                    "label": 0
                },
                {
                    "sent": "For that and H units, Xavier Laurel and some coauthors found this kind of interesting rule where if I have a neuron, and I know how many inputs are feeding into it, that's the fan in and I know how many different neurons depend on it.",
                    "label": 0
                },
                {
                    "sent": "That's the fanned out the fan out.",
                    "label": 0
                },
                {
                    "sent": "If I take 6 divided by that some and then the square root, and then I sampled the weights from a uniform distribution between minus R and plus are that turns out to be sort of sweet spot.",
                    "label": 0
                },
                {
                    "sent": "So that when I forward drop everything through my neural network, all the responses don't saturate and I get everything in.",
                    "label": 0
                },
                {
                    "sent": "Kind of a nice range so that when you do gradient descent everything works out for you.",
                    "label": 0
                },
                {
                    "sent": "And then as a last to sort of little bit, I'm going to talk about unsupervised pretraining later, which is something you may have heard of before, which is actually a sort of brilliant initialization scheme that can give much better results.",
                    "label": 0
                },
                {
                    "sent": "But it's somewhat more difficult to get working.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also talked about gradient descent and so even though we would love to use sort of off the shelf optimizers that don't take a lot of tuning, in practice when we work on really large scale problems, we're going to use something like stochastic gradient descent, and it turns out that you have to set the step size and stochastic gradient descent very carefully if you set it too large.",
                    "label": 0
                },
                {
                    "sent": "You can often make a lot of progress early, but then as you get to the end, things will slow down because you're sort of just bouncing around in space and you're not getting too.",
                    "label": 0
                },
                {
                    "sent": "More tightly tuned models and it turns out that up to a factor of two can matter.",
                    "label": 1
                },
                {
                    "sent": "So when you're setting this step size, which I've labeled data everywhere in these equations.",
                    "label": 0
                },
                {
                    "sent": "When you're setting this step size, it's worth spending some time to get it right and try a whole bunch of versions so some different strategies for setting this.",
                    "label": 0
                },
                {
                    "sent": "For example, to just brute force it so you have a big cluster, feel free to spin off a dozen experiments with 10 different sizes, so 10 to the minus 310 to the minus, 410 to the minus 2 whatever, and try a whole bunch of them and just see which one gives you the best performance on some kind of validation set.",
                    "label": 0
                },
                {
                    "sent": "If you can't afford that, or for example, if you're working on a really large scale problem, there are a few tricks for sort of guiding you to get started.",
                    "label": 0
                },
                {
                    "sent": "An interesting one, which actually I believe is courtesy of Max Welling.",
                    "label": 0
                },
                {
                    "sent": "Is to look at a histogram of all of the updates that you're making to your weights.",
                    "label": 0
                },
                {
                    "sent": "So run a couple of gradient descent steps and look at a histogram of the size of the update you're making to each of your weights an what you would like to have happen is for a typical update to be something like 1 / 1000 times, the magnitude of the weight.",
                    "label": 1
                },
                {
                    "sent": "So if you wanted to completely change that, wait to let's say, zero or negative.",
                    "label": 0
                },
                {
                    "sent": "The weight or twice its value.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to dramatically alter it, it would take something like a.",
                    "label": 0
                },
                {
                    "sent": "1000 iterations and it turns out that if you have a much larger fan in, if you have lots and lots of inputs, then you want to make something smaller than one in 1000, but this is a good way to get started and finally one that I've heard is pretty handy is if you can run maybe two or three step size at a time.",
                    "label": 0
                },
                {
                    "sent": "You can race them.",
                    "label": 0
                },
                {
                    "sent": "You can have a little test and you see which one reduces the error on your test data most rapidly, but you've got to be a little bit careful with this strategy because if you don't run it long enough.",
                    "label": 1
                },
                {
                    "sent": "You can run into the problem I told you earlier where it'll look like you're making lots of progress early, but you actually get a worse model in the wrong in the long run.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 1
                },
                {
                    "sent": "Most people use some kind of schedule for decreasing the step size gradually as you run.",
                    "label": 0
                },
                {
                    "sent": "This usually gives you better convergence guarantees, so very simple one to start out with this recommended by Yoshua Bengio is to keep the step size constant for a little while up to some threshold Tau and then once you've passed Tau iterations you start decaying it like one over the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole bunch of different adaptive stepsize schemes, and so on.",
                    "label": 0
                },
                {
                    "sent": "But when you first start out, this should get you going for finding a step size that works well.",
                    "label": 0
                },
                {
                    "sent": "And finally.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to talk about momentum, so this is a pretty cool trick that has been around a long time and genuinely appears to make a huge difference in terms of what works and what doesn't.",
                    "label": 0
                },
                {
                    "sent": "So using stochastic gradient you can usually get some networks working, but for deep networks with lots of parameters and some complex functions built in, this is a pretty pretty neat trick that will get you some of the benefits of a fancier method like a second order method without the trouble.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea behind momentum is that instead of just using the gradient as my update to my parameters, I'm going to keep a sort of moving average, like our exponentially weighted average of all of my previous gradients, and I'm going to have some parameter mu here, which is between zero and one, and I'm going to have another value held out.",
                    "label": 0
                },
                {
                    "sent": "It's the same size as Theta that I've labeled it V. Here for the velocity and.",
                    "label": 0
                },
                {
                    "sent": "Essentially what you're going to do is every time you computer gradient, you're going to sum in a small contribution from this gradient while adding in some decade value of your previous velocity.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to use that guy for your update to your parameters.",
                    "label": 0
                },
                {
                    "sent": "And the into.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean behind this is that it's a little bit like second order information is telling you something about how curved this space is that you're trying to optimize in.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if you're trying to optimize a function that's like a very steep Valley, then one problem with gradient descent is you can end up bouncing back and forth between the walls of this Valley without making very much progress in the direction that goes downhill within the Valley.",
                    "label": 0
                },
                {
                    "sent": "An when you use momentum is kind of.",
                    "label": 0
                },
                {
                    "sent": "Sort of neat property that these oscillations cancel out, whereas in the direction that's going downhill consistently, you'll have all of your updates adding up, so you can think of this like a ball rolling down a Hill so that as it sort of weaves between the Hills it eventually start going straight and it'll roll faster and faster and faster as it goes downhill.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty neat trick.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works pretty well and there's some recent work from alias US cover.",
                    "label": 0
                },
                {
                    "sent": "This suggests, for some very difficult to optimize neural networks using very large values of the momentum can actually help, so the one downside to this is that you have this extra parameter, mu, that you now need to set.",
                    "label": 0
                },
                {
                    "sent": "So in addition to this step size that we have to figure out, we also need to come up with some scheme for setting the momentum and some typical advice is to start out with mu fairly small, something like 0.5 so that the momentum isn't doing too much for you.",
                    "label": 0
                },
                {
                    "sent": "And then to gradually increase this to something like 0.9 or 0.99.",
                    "label": 1
                },
                {
                    "sent": "If you set it too high, you'll eventually start seeing that your objective is oscillating again, and you're not making progress, but somewhere in this range seems to work well, and I think it's worth saying there is a slightly more sophisticated formula than this one that's actually not much harder to implement, called Nesterov's accelerated gradient, and it looks like that also helps quite a bit, so this is a pretty simple method.",
                    "label": 0
                },
                {
                    "sent": "It's really fast, you can implement this for all of your neural networks.",
                    "label": 0
                },
                {
                    "sent": "Fairly straightforwardly by just changing your gradient descent code, and it appears to make quite a big difference in practice.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a couple of other factors that I think are important.",
                    "label": 1
                },
                {
                    "sent": "One is that weight decay, which is a sort of.",
                    "label": 1
                },
                {
                    "sent": "Basically just a buzzword for adding a squared penalty to the magnitudes of all your weights.",
                    "label": 1
                },
                {
                    "sent": "That can help avoid things like saturation and so on.",
                    "label": 0
                },
                {
                    "sent": "So if you keep finding that all of your units are saturating no matter what you do, because the weights in your neural network are growing too large, just putting a very weak squared penalty to keep the weight small in your objective function can help.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you have a small data set not too large of a neural network, you can always go for a fancy method like elegs.",
                    "label": 0
                },
                {
                    "sent": "Are one of these second order methods, and these take a lot less tuning.",
                    "label": 0
                },
                {
                    "sent": "In fact, for debugging, this is a really great sanity check.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's again we should pause for questions, but this is the basic idea behind.",
                    "label": 0
                },
                {
                    "sent": "Building a neural network, getting it working on labeled data, and these are essentially the basic tool kit that you should start from.",
                    "label": 0
                },
                {
                    "sent": "So everything I'm going to talk about for the rest of my time here, about 4040 minutes or so is going to be about how to apply this to images and also a bit about unsupervised deep learning.",
                    "label": 0
                },
                {
                    "sent": "But do people have questions about the algorithms and stuff gone through so far?",
                    "label": 0
                },
                {
                    "sent": "The question is how we try.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Non gradient based optimization methods.",
                    "label": 0
                },
                {
                    "sent": "So I guess if you for the cases when you don't know the gradient, you have to go to.",
                    "label": 0
                },
                {
                    "sent": "I guess you would call a zeros order optimizer and such optimizers exist.",
                    "label": 0
                },
                {
                    "sent": "You basically can only probe the function value.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is that you're getting so little information about the shape of this function that that is very hard to make progress on.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters that are in these networks.",
                    "label": 0
                },
                {
                    "sent": "That said, whenever you see these neural networks implicitly, did we lose audio?",
                    "label": 0
                },
                {
                    "sent": "Whenever you see these neural networks implicitly, there are a whole bunch of hyperparameters in there, like how wide the network should be, what should all the optimizer parameters be, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we can't really compute gradients with respect to those.",
                    "label": 0
                },
                {
                    "sent": "So hyperparameter search methods is currently a pretty interesting area, like how do you search over all these hyperparameters to find the best network?",
                    "label": 0
                },
                {
                    "sent": "And those are usually not gradient based.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                }
            ]
        }
    }
}