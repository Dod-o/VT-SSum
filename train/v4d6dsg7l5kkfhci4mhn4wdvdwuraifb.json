{
    "id": "v4d6dsg7l5kkfhci4mhn4wdvdwuraifb",
    "title": "Finding structure in data",
    "info": {
        "author": [
            "Joshua B. Tenenbaum, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "June 29, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010_tenenbaum_fsid/",
    "segmentation": [
        [
            "Alright, I'm going to continue a theme that Tom talked about.",
            "Finding structure in data and continuing a theme that he's illustrated.",
            "Very nice section number of themes, but in particular the what we hope is a virtuous and productive cycle of work going back and forth between human human learning and machine learning and trying to simultaneously push the state of the art of fields, which is really what this summer."
        ],
        [
            "School is all about to set all of this in the broader context that we introduced on the first day.",
            "Remember the big question that we're trying to get out?",
            "How does the mind get so much from so little or these problems of induction across perception cognition?",
            "Many in many areas of cognition, language categorization, causal learning, social understanding, and so on.",
            "And in all these cases, what we're looking for are these general reverse engineering principles.",
            "As Nick laid out very nicely before."
        ],
        [
            "Right, So what we've we've been developing here on the sort of computational cognitive science part of this summer school is a set of sort of a toolkit of techniques for answering the basic questions about how this could work.",
            "There.",
            "Basically, these questions of abstract knowledge, right?",
            "How does knowledge guide inference from sparse data?",
            "What form does knowledge take in these different domains, tasks, and how might that knowledge itself be learned?",
            "And Tom in both the causal learning lecture and in the last one has given a very nice.",
            "Very nice examples of how to how to put these ideas together.",
            "The basic idea of Bayesian inference for understanding how knowledge guides inference from data.",
            "The idea of doing probabilistic inference over various kinds of representations, different forms of structure, and then the idea of trying to understand where some of these inductive biases come from.",
            "These abstract knowledge with at least back in the causal learning lecture.",
            "The idea of doing some kind of inference over hierarchy of representations and having knowledge of some more abstract form that gives you some kind of hypothesis space, hypothesis spaces, or priors on priors and so on.",
            "And I'm going to try to illustrate many of these same ideas in the context of finding structure in data building on the kind of categorization issues that Tom talked about and have going up to some higher level knowledge which will also lead us into tomorrow's topic of more generally high level cognition and language.",
            "Another way to put some of the themes I'm going to be getting out here just to take a theme that Tom was putting out there, he said the fundamental question that you're getting at with these nonparametric methods is how much structure exists in the world or in some domain of the world.",
            "And I'm going to try to generalize that here to the question of what kind of structure exists.",
            "How much structure of a certain kind, like how many clusters?",
            "That's that's one version of this, but we might also ask questions of what's the right form of knowledge.",
            "Like if suppose we don't know that our world is clustered.",
            "How could we figure out that clusters is the right kind of structure be looking for, as opposed to some other kind of structure?",
            "And I'll treat all of this without starting with a very concrete case study like Tom did in the causal learning lecture.",
            "I think this is where we can make the most progress and really see the potential for progress.",
            "In this intersection between human and machine learning, is starting with what might seem like a pretty simple, boring, almost like psycho, physically boring Felix here.",
            "Kind of that was too boring for him physically oriented kind of task where we can really dig into the meat of it and then see what sorts of larger questions that opens up.",
            "So here that task is what cognitive psychologist called property induction or sometimes called category based induction 'cause it interacts very closely with and builds on the study of.",
            "Natural categories of objects and it's been studied at least since the work of rips in the 1970s.",
            "Many other famous and important cognitive psychologists have contributed to this latest office in Medine.",
            "Susan Carey.",
            "Steven Sloman, just to name a few.",
            "So here's the kind of task and the kind of phenomena that you might study here.",
            "The stimuli in the taskbar posed looking something like a classic kind of syllogistic argument.",
            "But these aren't deductive arguments are inductive arguments.",
            "They're not supposed to be true or false, or give you an all or none sense of validity, but just give you some graded sense of, well, how likely is the conclusion of the statement under the line to be true given the information above the line, the premises?",
            "I should probably use this point."
        ],
        [
            "They're not just gesture, so here's here's an inductive argument that most people think is pretty strong given that gorillas have T9 hormones and seals have T9 hormones, how likely is it that horses also have T9 hormones?",
            "Many people think horses are pretty similar to gorillas and seals.",
            "Maybe you think about some higher level category like, well, they're all mammals or something like that.",
            "And those those facts seem relevant biological similarity, higher level biological categories.",
            "Because the property here, which is what your reasoning about say, having teen hormones, seems like the kind of thing that you're basic.",
            "Knowledge about the anatomy, Physiology, and biological structure of organisms is relevant for.",
            "Now here are some other inductive arguments.",
            "Say, given that gorillas and seals have some property, how likely is it that anteaters have this property and most people consider this to be a weaker argument?",
            "That one again, maybe because anteaters, well, you know they are also mammals, are left similar to gorillas and seals than horses.",
            "Or how about this one over here?",
            "Gorillas have teen hormones, chimps, monkeys, baboons.",
            "Those are four examples of animals with teen hormones.",
            "How likely is the horses have T9 hormones again here in at least in some cases, people consider this to be a weaker argument.",
            "This one, even though they horses might be.",
            "Fairly similar to any one of these on their own, they don't.",
            "It doesn't seem to be a very diverse sample of mammals.",
            "It seems like, well, maybe this is just a property that's true of primates, so these are the kinds of concepts that have been proposed in cognitive psychology, notions of similarity or typicality diversity that can be can be understood qualitatively, and one way to think about what we're trying to do is to understand these concepts formally in terms of the structure of the inference and the underlying knowledge representations that support these concepts.",
            "So here is it."
        ],
        [
            "Just one slide overview of the set of ideas how we're going to attack these problems, and I should say at the outset that this is work that Charles Kemp did as part of his thesis with me, and it's really all his work.",
            "Basically, I'm very lucky to have been involved in to be able to talk to you about it.",
            "There's two key papers, a psych review paper in the PNS paper that I'll talk about later on.",
            "So here's the idea.",
            "It's a picture of how we take the idea of hierarchical Bayesian models over structured representations and also non parametric representations whose complexity can grow with the data similar to the ideas that Tom talked about but defines in a slightly different way to work with what might be more structured forms of knowledge.",
            "In this hierarchical model there's three levels that we call the level of the data structure and form up here, and these higher level structures together, or we think of is trying to capture.",
            "In some way, this notion of an intuitive theory that cognitive scientists have long thought about is a very general way to think about abstract knowledge that guides inference from data.",
            "But I won't really say much more except that you know the word theory might pop up in the background, but that's what I mean.",
            "Is this set of knowledge at various different kinds of levels?",
            "Here?",
            "It's only the bottom level of the data that's directly observable, and in this case that might consist of various observable properties of animals, like you know whether an elephant has tusks or.",
            "What they eat?",
            "You know, choose various vegetables.",
            "Horse.",
            "Also choose things doesn't have tusks, so there's sort of a matrix of data here consisting of rows, which are these objects or categories, and then columns which are various observable features.",
            "And there's some other features which maybe aren't directly observable, like whether an animal has some biological property, like T9 hormones.",
            "Now, if you think how interesting is it really whether an animal has T9 hormones, then I think about other kinds of very important, but not usually directly observable or not easier.",
            "Cheap to observe properties like whether something is good to eat or is poisonous, or whether something wants to kill you or not.",
            "You know that might be very costly to observe.",
            "The examples there and in all these cases what we want to be able to do is to take sparse observations about some new feature and be able to infer how to generalize to other other instances.",
            "Other categories in this domain, their value for that feature.",
            "So it's our goal is essentially to take this observed data and fill in the missing data of this matrix, and we're going to do that by positing these two levels of knowledge.",
            "Some kind of what we call a structure, some relations over the objects domain like for exam.",
            "Apple and intuitive version of a taxonomic tree where the species correspond to these leaf nodes.",
            "Here and then.",
            "Some way of defining a probabilistic model over the structure of this tree, which which puts priors on the features that we're trying to learn.",
            "Basically well.",
            "In fact, this this this will have an interpretation in some cases, as actually like the kernel of a Bayesian version of kernel methods, Gaussian process and so you can think of this as kind of a structured way of smoothing your very sparsely observed data.",
            "And this in turn also is something which can be learned from the rest of the day, to the much more densely observed part.",
            "And that comes from the fact that we have a probabilistic model underlying this.",
            "So we have.",
            "What's the view this as the likelihood for learning?",
            "Or you could view it as the prior for this novel property.",
            "It's a probabilistic model of how all of these pieces of observed or potentially observable data depend on the structure, and then learning the structure would be doing inference up to this level with a model conditioned by a hypothesis basin prior that's provided by this higher level knowledge.",
            "This higher level knowledge says.",
            "Maybe we're looking to represent the domain, not just with any possible graph, but with in particular a tree structure with the basic objects at the leaf nodes, and then that provides some higher level inductive bias which might help guide our inference, and we can even try to learn at this level.",
            "I mean, this could be something which is which is fixed or given to a learner.",
            "Or we could even think about how we could learn the more abstract structure of domain at this level, so that's the basic picture of what will be trying to do here.",
            "Now I want to."
        ],
        [
            "It off by just giving you a sense for the kind of data, and again a little bit more depth of how people have explained this in the traditional psychological literature.",
            "So here's 2 classic experiments from Oscarson and colleagues.",
            "Where the idea is you give in one experiment, you give a number of arguments that are pretty similar, but might just very.",
            "For example, in which species of animal appear in the premises here.",
            "So in one case you might have cows, horses and rhinos and another one IP seals, squirrels and rhinos for example.",
            "And then the conclusion category might be a general one like all mammals.",
            "So you're asked there to generalize from a few examples to a larger category, or it might be some other category at the same level, like how likely is it that cows and Ryan, if cows and rhinos have this property that horses have this property?",
            "And the property itself, in at least the classic versions of these experiments is sometimes called a blank property, meaning that something you don't know anything about.",
            "But, crucially, you know something about this?",
            "You know something abstract, which is that these properties are biologically relevant and that will become important as we start to go out from this domain and see other kinds of inductive reasoning for which other kinds of knowledge structures might be relevant."
        ],
        [
            "Now there's another kind of data that's typically collected here, which is important for providing the basis for modeling.",
            "Sometimes the basis is provided by similarity judgments, as we've seen in some of the other lectures.",
            "That's a standard way to get insight into mental representation or here it's feature listings.",
            "So in a different experiment, office and colleagues had subjects tell you bout typical features for various animals.",
            "In this case, we have a data set of 48 or 50 animals, all mammals, and.",
            "85 features and here's a here's some examples of the features that people judge to be typically associated with one animal elephant, so they are Gray there, hairless.",
            "They have tough skin there.",
            "Big their boldness, body shape.",
            "They have long legs.",
            "They have a tail and so on.",
            "There's also various kinds of ecological properties like the vegetation there, grazers, they live in the Bush, in the jungle and so on.",
            "They travel on the ground, they're smart, they're timid, so it's the kind of things that a child might hear from their parents.",
            "Read about in picture books.",
            "Hear about or see when they go to the zoo, not genomic high throughput screens or something like that, but just everyday observable properties of animals."
        ],
        [
            "Now you can put these two pieces of data together to try to give a general framework in which many modeling approaches can be compared, and it looks kind of like this, so there's the task that you're trying to model.",
            "Are these judgments about how strong one of these arguments is compared to the others, which you can think of as from?",
            "This is kind of computational formulation that should look more familiar to machine learning.",
            "There's this new property which you can think of as an mostly unobserved column of this matrix, and you observe its value in just a couple of points corresponding to the two premises here.",
            "And your job is to tell me which other.",
            "What are the values of this property at all the on observed entries, and we're going to do that by extracting some more general knowledge from all these other observable features here.",
            "So these these columns here."
        ],
        [
            "Respond to these directly observable features on the."
        ],
        [
            "Last slide.",
            "Somehow we want to extract some knowledge about what these things are like in general and how properties tend to be distributed over these objects that will bridge from the old features to the new property.",
            "So if you're familiar with the idea of semi supervised learning where you're trying to learn some given label, let's say this here the new property.",
            "With a lot of unlabeled data, so here's all the unlabeled data, which is just unlabeled with respect to this new property we're learning.",
            "This is you could see this as a kind of semi supervised learning problem, and there's very close relations between the approaches will develop and those in machine learning, although at the same time they actually go beyond them in an interesting way that would be worthwhile for machine learning people to pursue.",
            "In fact, we've done a little bit of that with Charles.",
            "It's also related to even more general class of problems that you might call sparsely observed matrix completion.",
            "I don't mean sparse matrices, but sparsely observed things so you know if you've seen the Netflix challenge or.",
            "Many other kinds of problems where we have some matrix and we just observe some entries in some rows and sum columns and we want to somehow figure out the underlying structure that can support generalization from the observed parts of the matrix to the rest of it.",
            "So here it's set up in this particular structured way where there's a big observable chunk of the matrix, corresponding to, say, very perceptually in culturali available features and other things that might matter a lot better.",
            "Harder to observe, but more generally, if you're if you somehow represent the abstract structure of this domain, that could be useful for filling in all sorts of an observed entries in.",
            "In your observable data."
        ],
        [
            "So here's an example of a typical cognitive psychological approach.",
            "This problem.",
            "You take a notion like similarity, which you say we're just going to assume that's a basic thing that."
        ],
        [
            "You can compute, you know, for example something like distance in this feature space, or correlation between objects in the feature space.",
            "SO22 species are similar if the rows of features are similar.",
            "Horses and cows here would be very similar horses and seals.",
            "You know less similar 'cause they have fewer features in common.",
            "And then of course you need to do.",
            "You need to go beyond just pairwise similarity if you want to evaluate an argument like this in similarity terms, you have to say well how similar are cows?",
            "Two horses and rhinos as a set of examples and."
        ],
        [
            "In literature, several several ideas have been proposed.",
            "Here's here's a plot for two models, but you could call the Max similarity model and the some similarity model.",
            "So in the some similarity model is maybe the more familiar one.",
            "It's similar to kernel density estimates or exemplar models and categorization, where what you do is you take the similarity to each of your test item.",
            "If you like to each example, or each category in the premises, and then you sum up.",
            "Or you could Alternatively take the average, but you sum up the similarity to each one in the Mac similarity model, you take the Max similarity.",
            "So you say you know.",
            "How similar is each mammal to 2 cows, elephants and horses?",
            "And then what contributes is only the most similar exemplar?",
            "It's interesting here that the maximal remodel works pretty well, and what we mean by working well here is that over the course of one experiment you have a bunch of these data points represents one stimulus, one argument, which again people give you various graded measures of strength, like this one here is considered to be a very strong argument from gorillas.",
            "Meissen seals all mammals.",
            "This one is considered to be a very weak one, perhaps intuitively because cows, elephants and horses, while each one might be a typical.",
            "Mammal there as a Holder.",
            "Not a very diverse sample of mammals.",
            "They're all kind of these four legged herbivore's.",
            "So that gets a relatively weak judgment, and the ratings on on the vertical axis are peoples judgments and the model predictions are on the X axis here, so here's a pretty high correlation between the Mac similarity model and what people say, whereas for the some similarity model which is interesting, this is the more familiar model in terms of, say, generalizing categories from examples.",
            "Indeed, of these example, our models are basically the gold standard in.",
            "Categorization, but here it's not just sort of worst correlation.",
            "It's actually significant negative correlation, which is not something that I don't think I would have anticipated.",
            "It really suggests there's something fundamentally different going on here that we need to understand.",
            "Why is?",
            "Why should similarity work in this particular way?",
            "In this taxonomic domain?",
            "But there's more richness here to this to the study of inductive reasoning beyond similarity, so."
        ],
        [
            "Are these arguments here?",
            "Which of these do you consider to be a stronger argument, given that poodles can bite through?",
            "Why are there for German Shepherds can bite through wire?",
            "Or, given that Dobermans can bite through wire.",
            "Therefore German shepherds can bite through wire.",
            "So how many people choose the first one?",
            "How many people choose the second one?",
            "That's that's typically right, it, usually it's about 2/3, one third here it was about roughly about that.",
            "So those of you who chose the second one.",
            "Very good reason.",
            "Right Dobermans and German Shepherds seem more similar than Poodles and German Shepherds, and we've just been talking all about similarity for the last few minutes now.",
            "Why, though?",
            "Would you choose the first one and then want to say intuitively?",
            "Yeah.",
            "German Shepherd can do it then because dogs can write it.",
            "So it seems like intuitively there's some underlying dimension here that's relevant, like strength or size that you referred to, and you know human.",
            "Right, so we're talking about the Alsatian dog, that's right.",
            "I see, yeah, so that's that's that's even higher level cognition, and I was hoping to get out here.",
            "Alright, so you did a different.",
            "It seemed like you did a different kind of reasoning there, right?",
            "So something that's going to be more asymmetric, right?",
            "If we turned it around and we said, given that German Shepherds can bite through wire, how likely is it that poodles can bite through wire?",
            "You'd say, well, that doesn't help you at all.",
            "'cause German Shepherds are really strong, right?",
            "So this looks very different than similarity Now, but this one here.",
            "Here I'm directly testing if you like the symmetry of these kinds of inductions.",
            "So given that salmon carry some bacteria, how likely is it that grizzly bears carry the same bacteria versus given that grizzly bears carry that bacteria, how likely is it that salmon carry that bacteria?",
            "So how many people choose the first one?",
            "How people choose the second one?",
            "OK, well if you're not from North America, you may not know very much about grizzly bears, but grizzly bears like to eat salmon, and one intuition that many people have here is that if grizzly bears, well, it basically.",
            "Grizzly bears could have gotten this from the salmon, and people prefer to reasoning from a direction of cops to effect than the other way around.",
            "So you can see how this even this disparate leave.",
            "Very simple task of inductive reasoning from properties of natural species can bring in all sorts of different kinds of knowledge that we want to understand.",
            "And this."
        ],
        [
            "Is where the tools of this hierarchical Bayesian approach might be valuable.",
            "So in this case this picture here with it with something like a taxonomic tree, might be a way to capture some of this sort of similarity based taxonomic reasoning, but we might need other forms of knowledge to capture these other kinds of inductive reasoning that go beyond similarities.",
            "So let's try to see how this approach can play out in this domain and will start off developing the basic ideas for similarity based taxonomic reasoning, But then will also show a little bit about how it can work for other forms of knowledge and then come to the question of how do you figure out.",
            "What's the right form of knowledge for a domain?",
            "So first let's see how do we model the basic property induction task.",
            "How do we go from some set of observed features to make intelligent inferences about a very sparsely observed feature?"
        ],
        [
            "And not, perhaps not too surprisingly, this is.",
            "This is the Bayesian part or the most basic Bayesian part, so we can think of very concretely like this.",
            "We can say this is what we want to infer one and further missing entries in this column, and we can consider a hypothesis space of all possible ways to complete this column.",
            "All possible labelings?",
            "Let's just say it's a binary feature that's either true or false for each object or category in the domain.",
            "So we could write down a set of if we have N animals here that is 2 to the N possible binary vectors to the end ways to complete.",
            "From any set of examples.",
            "So here I'm just showing a few of them, and this includes very simple hypotheses like this one which says the property is true for all animals, or this one which says it's true for all but the aquatic mammals, Dolphins and seals.",
            "And then you can have other things down here that look more arbitrary like this.",
            "One is true for horses and shrimps.",
            "Squirrels, seals and rhinos, but not the other animals.",
            "The basic Bayesian analysis of this problem says look what examples we have in the premises that's represented here by X, and we want to compute the conditional probability that the conclusion is true.",
            "So the basic Bayesian analysis says assign some probability to each of these hypothesis.",
            "That's going to be a prior and go in and just look at this ratio here.",
            "Look at how much prior probability there is on the hypothesis, which include both the premise categories and the conclusion category relative to how much prior probability probability there is an hypothesis which just include.",
            "The premises, so if there's a category which is in many or most of the high probability hypothesis that the examples the premise categories are also in, then it's very likely to get how this property an otherwise maybe not so much.",
            "So here I've just written down some arbitrary schematic assignment of priors, which gives this very simple hypothesis that the property is true for all mammals.",
            "Pretty high probability.",
            "These other ones here, you know that are sort of seem like they're neatly taxonomically organized.",
            "Lower but still pretty high probability, and then these kind of arbitrary hypothesis here.",
            "Very low probability and then."
        ],
        [
            "The Bayesian inference here is just."
        ],
        [
            "Simple, what I've done here is I've just ruled out all the hypotheses."
        ],
        [
            "Are inconsistent with the examples, so I'm left with the hypothesis that are consistent and I now see how much of the prior probability on these ones, the ones that include both horses and rhinos, also applies to any one of these other animals.",
            "And then I've just shown that in grade level here that conditional probability, so it's very high for cows because most of these high probability hypotheses, these ones over here, which include horses and rhinos, also include cows.",
            "But let's say it's much lower for Dolphins, because only one of those includes Dolphins.",
            "Does that make sense again, so this should be very elementary, but doing this analysis just raises the interesting question that this this lecture is real."
        ],
        [
            "About which is what is the real structure of this prior?",
            "Is it just a enumerated list of numbers over all possible hypothesis, or if not, what is it and where does it come from?",
            "So if we ask where does it come from?",
            "Here's here's a simple empiricist answer.",
            "Write an answer that says knowledge is grounded in our sensory experience or their experience of the world.",
            "So why don't we do this?",
            "We want to assign a prior probability to any possible way that feature could pattern over the objects in our domain.",
            "Just remember how often in the past we saw features that pattern in some way.",
            "You know how often in the past if we see a feature that was true of justice.",
            "Just you know horses, cows, chimps, gorillas, my squirrels rise and elephants and not Dolphins, seals and then set these priors for a new feature in proportion to our frequency in our past experience.",
            "Does that sound like a good idea?",
            "I mean this very simple should work in some sense, right?",
            "You have to have a lot of data.",
            "Yeah, how much?",
            "Possibilities.",
            "A lot, yeah, not just all the possibilities, but all the possibilities.",
            "Lots of times.",
            "I mean, this goes back to basic issues that these guys talked about right?",
            "Or an also that Tom was talking basic issues of inductive bias.",
            "Remember, I think both Neil and Bernard showed in the context of say, learning about binary digits.",
            "How much data would we have to see to learn an intelligent prior overall binary vectors in a 60 by 60 image space, right?",
            "I mean, it's huge and you've got the same sort of problem here.",
            "I'm just listing 10 animals, but there's lots and lots of animals we know about and animals are just a small fraction of the concepts that we know about.",
            "So if you wanted to actually estimate a distribution over all possible ways of labeling all possible things that you know that would just take a ridiculous amount of data.",
            "So basically sort of bias variance issues.",
            "Just say that's never going to work, but also more interesting Lee, right?",
            "We want to understand where these priors come from, even if it might be the case, you know the brain has lots of connections, maybe at some level in the brain.",
            "The prior does look a lot like this.",
            "Maybe it is just a billions and billions of numbers just laid on top of each other, but from a more cognitive level.",
            "There's real knowledge here.",
            "It's not just high dimensional distributions, right?",
            "So if we take."
        ],
        [
            "This is just a generic illustration of inductive bias, but we've seen a lot of these examples so."
        ],
        [
            "Skip over this if we take these different kinds of reasoning phenomenon that we showed.",
            "You know the basic kind of taxonomic similarity, but also these cases, like reasoning from poodles.",
            "Dobermans where you appeal to some dimension like strength or these sort of food.",
            "Web kinds of reasoning.",
            "This more causal character, it seems like not just our different priors relevant, but qualitatively different forms of knowledge are being used in these different cases, and we want to understand what those forms of knowledge are and how they give rise to the numbers that Bayesian inference needs."
        ],
        [
            "Let's try to see how this goes and I'll first show you how we can develop a prior for this taxonomic similarity and then branch out to other forms of knowledge.",
            "So the basic starting point here is the idea of some kind of a taxonomic tree, and there's a number of reasons to think about organizing the structure of biological species this way.",
            "First of all, of course, because you know what we know about the actual structure of the world, evolutionary theory tells us that species are generated from some kind of a branching process with, so make sense to think of putting the basic categories at the leaf nodes of a tree.",
            "There's a long history in cognitive psychology of positing something like tree structured semantic hierarchies, so this is a classic.",
            "Example from the work of Collins and Quillian.",
            "It's like the oldest data structure in the book literally.",
            "Now here the actual trees were going to use look a little bit more like evolutionary trees and that they might be kind of have this more arbitrary branching structure.",
            "But again, they're not going to be grounded in any real evolutionary science.",
            "It's more intuitive version.",
            "Imagine people sort of looking around in the world and coming to this kind of tree now."
        ],
        [
            "So that's going to be our basic structure, and then we need some way of defining a probability model over the properties of animals.",
            "In terms of this tree.",
            "And here we're going to use an idea which is which is borrowed from machine learning.",
            "The basic idea that we can think of some notion of smoothness as the basis of similarity.",
            "So intuitively we want to say properties which which which very smoothly over the tree should have higher probability than ones which vary in a more arbitrary way.",
            "And this is a nice notion because it's not just specific to tree structure.",
            "You can define this on any graph and will use that too.",
            "Make this a much broader model, so here is between.",
            "These are two tree structured graphs.",
            "This one is a special case of a tree that's a chain, and here are two cases of smoothly varying features, right where intuitively you only have to cut the graph in a small number of places to separate and say the positive and negative instances, whereas these are much less smooth ones and we want to say intuitively a feature that's distributed like this should have high probability.",
            "If this is the underlying structure and a feature distributed in this way should have low probability, and there's some math behind this."
        ],
        [
            "I'm not going to go into the details of the math, but I'm happy to talk about it.",
            "Or really, I just point you to the work of Jerry Zhu, John Lafferty and Zubin Ghahremani, which we build on.",
            "There's a number of related ideas, but this is the work that we most directly build on, and it has has a number of different interpretations.",
            "You can interpret this in terms of diffusion like the idea.",
            "Imagine a property kind of diffusing over a graph and and that's a way to capture smoothness.",
            "Or you can.",
            "There's also a connection to two kernels from a Gaussian process formalism where you can think of this as like a.",
            "You know the covariance using the graph to capture the covariance structure of a Gaussian model, where each node each object is a random variable and you're trying to capture a high dimensional distribution, right?",
            "What's the joint distribution of how properties can be distributed over all objects and intuitively want to say objects which are close in the graph should have high covariance.",
            "They're likely to share the same values of features whatever they are.",
            "This is a particularly simple way to write down the math where we say F, INF, J represent the feature value for objects I&J, and then there's an edge.",
            "There's edges between some objects in the graph, and those edges can have a length DJ and intuitively want to say things which are connected by short edges.",
            "That's high coupling, so they should.",
            "We should penalize a distribution of features in which they are very different, and that's what this quadratic model does here.",
            "And then we treat that as an energy which we exponentiate to get a probability model over the entire over the entire set of objects.",
            "And you can interpret this.",
            "The link between the idea of the Laplacian of a graph and the covariance matrix of this big high dimensional Gaussian which is has a lot of interesting math behind it, but I'm not going to dwell on it here, that's the basic idea.",
            "Hopefully should be should be clear and just think intuitively objects which are close from the graph should should go very highly in terms of their feature values and that idea."
        ],
        [
            "Is very powerful, so if we took this tree structure that I put there, which hopefully you know I didn't dwell on it, but hopefully it seems fairly intuitive.",
            "We have say the primates in one branch of the tree we have the rodents, we have the Dolphins and seals, the aquatic mammals.",
            "They're in another branch.",
            "We have the big four legged herbivores over there so on I'll tell you in a minute where we get that tree.",
            "But let's say we're given that tree and we define the smoothness prior over it and then we use that to provide the predictions for novel properties.",
            "Well, it provides a very very good model for a number of datasets, so I'm just showing these are five different datasets.",
            "It's the same model in each case, but different judgments are being predicted along the Y axis and again dots down here are ones which the model says are very low, likely to generalize from the examples to the test item or the conclusion.",
            "These ones are very high and people agree very much.",
            "There's very high correlation, so again, it's not perfect, but for high level cognition, it's pretty compelling.",
            "Impressive results here, so this is, these are just two different."
        ],
        [
            "Assets and here are three others that are a little bit smaller with different mammals."
        ],
        [
            "The next question you should be asking, and we're just going to be working our way up through this model, is where does that structure come from?",
            "I just gave positive to tree and said, well, if that was the tree, you can define the smoothness prior over to look.",
            "It does great, but where does it come from?",
            "Well, here's where we start to see the generality of this approach or just want one very nice Journal aspect of it, which is basically the very same hierarchical Bayesian model we used to provide a prior for inducing this new extended.",
            "This new property helps to explain where history comes from, a kind of top down and bottom up Fusion of evidence.",
            "Essentially it's a it's a Bayesian tree learner, so here's the."
        ],
        [
            "Idea we have all these observed features which I was telling you about and we're going to assume that each of these features is generated over the same graph structure as our novel feature.",
            "In particular, each one is an independent sample on this, so they are conditionally independent.",
            "They're not, they're not a priority independent, they're highly Coke couple.",
            "That's the whole point of talking about similarity, but condition on the tree.",
            "Each one is an independent draw.",
            "From this this Gaussian, smoothing this process and then our job is to figure out which tree, or maybe to infer a distribution over trees.",
            "That best explains all these features as a whole, and then we're going to use that same tree structured model to give us a prior on a new property."
        ],
        [
            "So that's how it works.",
            "This is just an example of of learning a tree for this larger set of 50 mammals from roughly 100 features or so, and again, you can see all sorts of interesting structure that isn't really biologically correct like it's not biologically correct to put Dolphins, seals, and whales together.",
            "Dolphins and whales fine, but seals are rather different.",
            "They're more like dogs, Dolphins and whales, maybe more like cows.",
            "But intuitively, that's how we think of things."
        ],
        [
            "And and this is just what I said.",
            "We were sort of learning this tree here from the observed features and then using that to transfer to this new property.",
            "This is if you like our tree structured approach to semi supervised learning.",
            "Now you could use other kinds of structure and that's where again we start to see the power of being able to do Bayesian inference over different kinds of representation."
        ],
        [
            "So, for example, suppose we go back to the kind of representations that have long guided categorization research and that Tom was talking about basically flat clusters, right?",
            "So suppose we think that are the structure of our world is take these animals and cluster them into some higher level categories.",
            "Could that provide the basis for generalization here?",
            "Well, let's say we."
        ],
        [
            "Use the mysteriously process or Chinese restaurant process mixture so this is basically the same model that Tom was talking about and that John Anderson introduced into cognitive psychology now 20 years ago, and these are these are the five clusters that it finds for this set of 50 animals and you can see there pretty sensible, right?",
            "So you've got these rodents here.",
            "You've got the primates for the most part.",
            "You've got the aquatic mammals.",
            "You've got these sort of two kinds of basically herbiv, ricin, carnivorous animals, and suppose this was the underlying.",
            "Structure well."
        ],
        [
            "It turns out here we can compare that restructured model in the cluster structure.",
            "Would the cluster one actually does pretty well in terms of correlation, but if you look at what's going on here, you see there's something fundamentally wrong, right?",
            "What do you see when you look at this?",
            "You see these these bands?",
            "What does that mean?",
            "These are these are different stimuli for which the model all makes exactly the same prediction.",
            "But people make very different judgments, and you see that coming up.",
            "In each case Huawei is that anyone can explain.",
            "Huawei would, why would you see these bands here for this?",
            "For these kind of tasks with a flat cluster model as opposed to for a tree structure model, yeah.",
            "Raising your hand.",
            "Anyone else want to?",
            "In this case, that's true, yeah?",
            "Yeah, so so in our case we assume that that the properties are conditionally independent based on whatever the latent structure is.",
            "But but here the situation we have right is that you're only you're only capturing.",
            "If you like 1 grain of data.",
            "So either the premises the examples are in one.",
            "Either are in the same category as the test item, or they're not, and if you have, say, 2 examples, then you either have both of them in the same category, or one of them, or none of them.",
            "So there's really only a few different levels of prediction you could make, whereas in the tree you have a much more great in structure.",
            "It's a richer inductive bias.",
            "If you like this is this.",
            "Inductive bias is too strong, and that shows up in the fact that there's less that it can predict."
        ],
        [
            "That's just right now, but what about a different kind of representation?",
            "So again, you know.",
            "Just as Tom was showing for causal learning, the power of this kind of approaches, we can use it to evaluate different kinds of representations in terms of how they fit the same sort of data.",
            "So how about a low dimensional space?",
            "Again, a very popular model in categorization, research and perception.",
            "Like, for example, we talked about face faces.",
            "What about an animal space?",
            "Well."
        ],
        [
            "Here's here's a a very similar kind of way of building a 2 dimensional space.",
            "It's actually closely related, though, not identical, to work that Neil did, but the idea is just as we as we came up with, say, a tree structure with the, with the idea that we could define a smoothness prior over that tree and say well which tree best explains all the observed features.",
            "We could say which low dimensional space embedding of these animals best explains the observed features, where roughly two things are more likely to have the same property, or they covary more the closer they are in this space.",
            "And this is a 2D embedding of the very same animals, and again we can pick out just the ones that that are you."
        ],
        [
            "Use to simulate in any one of these tasks, and again it doesn't do badly.",
            "In particular, it does the best the best to the embedding does very well on generalizing where the target categories horses.",
            "That's this data set.",
            "But this is a slightly harder task here where you asked to generalize to all mammals, and so it's really more about its testing.",
            "More of the global geometry of this space.",
            "How well does your underlying structure underlying inductive bias get the global geometry right?",
            "In order to be able to infer basically how well does any set of a few examples cover the set of all mammals, that's what either of these approaches are getting at.",
            "And there that restructured model is much better, so it again suggests that here the 2D model has a pretty good inductive bias, but it's not the right inductive bias and it shows up at least in some cases.",
            "So it's a."
        ],
        [
            "Diagnosing that now, do we think that there's something intrinsically good about trees and not 2 dimensional spaces?",
            "Well, our general take on this is that.",
            "Just like in any other issue of inductive bias, what makes a good representation have?",
            "There's several things we need to think about, right?",
            "You need to think about degrees of freedom, and so I need to think about computational tractability.",
            "But you also need to think about the structure of the world Anna representation or prior is going to be a good one if it matches the actual structure of the world, at least two to enough of an extent to support the kinds of generalizations you need to do, you have to be able to get it right, and here we're seeing that.",
            "So this is a parallel experiment that Charles did.",
            "For a domain where we thought unlike all these classic studies of animal species in their properties, where we thought more of a low dimensional space, in particular, 2 dimensional space would be the right inductive bias for the world, and we wanted to see can we just show quantitative evidence in RCM psychological paradigm that people are using.",
            "That was something like a double dissociation here.",
            "And that's exactly what we got.",
            "So this is a visa geographic inference task where the objects are cities and people are asked about various properties that that could be expected to vary smoothly over the geographical space over the 2D geometry of the.",
            "United States in this case, and we can get from people.",
            "We can assess both the best 2 dimensional map which you know hopefully and indeed does roughly correspond to the actual 2 dimensional map of the US, or we can take from their judgments of similarity or proximity.",
            "We can take the best restructured bottle and see now how well do these do we're going to find the very same Gaussian smoothness process over these two different representations.",
            "And here we get the opposite pattern that restructured model.",
            "That's the one down here.",
            "The sexual doesn't do very well, but the two dimensional model.",
            "Does pretty well.",
            "Not not perfectly, but at least it does significantly better than that restructured model and we just take this as a way.",
            "I mean, it's I don't think it's all that surprising to say hey, people know about the two dimensional layout of big cities in the United States, and they use that to guide their inferences, but more as a way of validating this approach and showing that this is a way to work backwards from people's basic inductive judgments to make inferences about which which kinds of knowledge representations or structured priors are more appropriate in different cases, and to assess the fit of what's in the head with what's in the world."
        ],
        [
            "Now there are lots of other questions you could ask.",
            "How are we doing on time?",
            "15 minutes?",
            "OK, then I won't dwell on this.",
            "One of the many interesting questions, but.",
            "Charles and I.",
            "Well in general that the probabilistic models community and the.",
            "Has been engaged in interesting and productive debate with the connectionist or neural network modeling community, where there's really one of the big issues that we're debating is how much structure is in the head.",
            "You know, when you look at the brain as connections do for inspiration or neural network researchers, you don't see trees or necessarily these low dimensional spaces.",
            "You see high dimensional spaces of connectivity.",
            "Cinema question to ask, do we really need some kind of structure or low dimensional representations?",
            "Do we need to posit people are learning these explicit structures?",
            "Rogers and McClelland, for example, have proposed a connectionist network which can approximate these kinds of tree structures for the same domain, and they propose that as an alternative to what we've been doing, and it's an interesting question whether you can test these different accounts on the same sort of data.",
            "It turns out that this kind of connections approach, at least in our in our number of tests, doesn't seem to work very well on these inductive judgments.",
            "But there are some other approaches that a student in our group, Brandon Lake, has been exploring with her."
        ],
        [
            "Kind of interesting hybrids of these these structured probabilistic models and connectionist approaches their structured in the sense that they are learning one of these Gaussian graphical models, but they're less structured at the abstract level because they don't specify a an explicit form like a tree structure or low dimensional space.",
            "Rather, they just tried to use a more generic idea of sparsity, and for example he's able to just learn a sparse Gaussian graphical model for, say, the city data or the tree structure data, which does pretty well at predicting property induction judgments, and has another nice feature which is.",
            "We can scale up to two very large semantic domains like just all the things you know.",
            "Where we don't expect to have one single kind of structure like one single tree or low dimensional space to capture concepts as diverse as buildings, colleges, Penguins, fish and vegetables.",
            "This is just a little slice of applying this model to a set of 1000 semantic concepts and it's you know it's actually a way to learn something like a star semantic network.",
            "Now it has other other things that can't do.",
            "For example, it can't capture more abstract linguistic ways of learning about the world, like for example, if I tell you.",
            "That you know Dolphins look like fish, but they're not.",
            "They're actually mammals.",
            "Then that changes your underlying knowledge representation in a way that it seems like you need to have something like a tree structure to be able to say, OK, what do I mean by mammal?",
            "I mean this node, and when I'm saying Dolphin and what I mean by fish, I mean that note.",
            "And when I say that Dolphins aren't fish, they're mammals, and that means I have to somehow move this over here.",
            "And if I move that, maybe I'm going to move the whole subtree.",
            "So maybe that means other things that look like Dolphins, like, say, whales maybe should go with them.",
            "That kind of more explicit symbolic reasoning can't really be captured here, but it's an interesting question to see where these might meet up.",
            "Another thing that hasn't yet been captured here is going beyond what all of these approaches that I've shown you so far have been doing, which is basically trying to model fancy versions of similarity, just like in the kernel, both Bayesian and non Bayesian approach is at the heart of all of these things, is just the idea that there's some notion of similarity over some kind of structure, either a graphical structure or some higher level thing like a tree, and then that's what guides are generalization, but remember we have these other kinds of inductive reasoning like properties biting through."
        ],
        [
            "I or carrying bacteria which don't follow the form of similarity.",
            "So how can we capture this?",
            "Well, what what Charles did in this very nice paper is to give within the same general approach of describing abstract knowledge as some kind of a relational structure, some kind of a graph, and then some kind of stochastic process operating over that graph.",
            "Ways of specifying priors for all these different forms of inductive reasoning.",
            "So, for example, to model this these dimensional predicates, he gives a 1 dimensional chain structure and then some asymmetric drift process that operates over that graph.",
            "And it turns out that that does a very, very good job of modeling a number of different kinds of domains where they all have the same form.",
            "There's some underlying dimension that people are able to infer, and then there's some ordering along that dimension, some kind of dominance ordering that guides property."
        ],
        [
            "Duction and he's compared that with, you know, the same kind of 1 dimensional model, but with this more diffusion or smoothness process that doesn't do as well.",
            "Or a tree model, and so on.",
            "Or to take another example for doing this kind of food web reasoning.",
            "This is where that Charles did together with Patch.",
            "After Patch is really the lead lead on this project.",
            "We defined a Model 2, two kinds of models.",
            "There's that restructured model for taxonomic properties, and then over here there's this kind of directed.",
            "It's basically a Bayes net.",
            "It follows a noisy transmission process where if any prey species has some probability of transmitting any any disease property to a predator that eats it, and if a predator eats multiple things and it has a noisy or pooling.",
            "So it's very much the same kind of model that Tom was talking about in causal reasoning, and we showed again in a double disc."
        ],
        [
            "Sociation, where the food web does a good job of predicting peoples inferences about diseases where the tree structure model does a good job of predicting these more biological properties, but not the other way around.",
            "So the food web model does not do a good job.",
            "Does a very poor job of capturing biological properties, an likewise for that restructuring disease properties.",
            "So it suggests that people can flexibly use different representations of the same domain to reason about different properties.",
            "So to summarize."
        ],
        [
            "Where we got to hear it.",
            "This is this is most of the way through the talk, but I want to say a little about the deep questions that remain.",
            "So what we've shown is general Toolkit for thinking about inductive reasoning and using it to diagnose the underlying knowledge representations, right?",
            "We've shown that different kinds of priors are appropriate for different domains or different views of a domain, and that there seems to be some kind of a match between what's in peoples heads at least for these domains that we've studied.",
            "We don't claim that people are well calibrated for all domains.",
            "You might ask him to reason about, but for certain things that are important in the real world, like natural categories and their properties.",
            "It seems like we have evidence that people people are using something like the right models and using them in something like the right way to guide Bayesian inference, and we've given a fairly general framework for describing these different kinds of background knowledge or different kinds of intuitive theories in terms of something like a relational structure over the objects, and then some kind of stochastic process over that over that graph providing the prior and then we account for the differences in these domains by the different forms of structure or the different kinds of stochastic processes.",
            "But one of the big questions that remain well, how does a learner figure out what's the appropriate form of structure?",
            "And what about these cases where it seems like multiple models or multiple structures might capture different aspects of the main?",
            "How do you figure that out?",
            "In a sense, these are versions of the question that Tom was talking about this this year.",
            "How many different models to learn is sort of a version of the question of how much structure, but now take it to another level, and this is a question of what kind of structure, which is in a sense a generalization of the problem of how much structure and we're going to try to use the same kinds of tools that Tom was talking about.",
            "Least broadly, but over.",
            "But now over these more structured kinds of."
        ],
        [
            "Representations, so in the hierarchical base sense, we're going to be asking here say if we consider different forms of structure for the same observations, how can we figure out whether we should use a tree, a space, or a chain?",
            "That's one particular, or you can think of this as extending structure learning from the problem of finding a structure of a particular form to making inferences about the form of the structure now."
        ],
        [
            "This problem of learning the formal structure domain is a very important one, although it hasn't been studied until recently.",
            "Too much in either machine learning or in human learning, but it's, but it comes up at crucial moments in scientific discovery or cognitive development.",
            "Again, these are in some sense the most interesting cases of deep learning, like really deep learning.",
            "So take scientists for example.",
            "The idea that we should organize biological species in a tree is not something that all cultures have always had in their scientific knowledge in Western Europe.",
            "Trace it back to the work of Linnaeus and other naturalists.",
            "You know, in this sort of dawning of the scientific age.",
            "But prior to that, say, going back to even the early modern period of the Middle Ages, the dominant metaphor for the structure of the natural world was more of a chain, what's called the great chain of being, right?",
            "So you have a linear representation with rocks at one end, and God at the other end, and all the biological species.",
            "Sort of a radon levels of complexity from plants up through animals, and humans and Angels, of course.",
            "And then God at the at the other end.",
            "But then what happens when naturally start to go out and take data seriously?",
            "In particular, Linnaeus was really interested not just in animals, but in plants, and he had the idea that the same general principles should describe how all these animals are related to each other and plants, and he came up with this seven level hierarchy, which then went through a number of different revisions, right?",
            "Darwin's notion of trees?",
            "This is a sketch from an early notebook of Darwin's has something in common with Linnaeus.",
            "It has a hierarchical structure with the.",
            "The ultimate species at the leaf nodes, but unlike Linnaeus, is perfect seven level thing every.",
            "You know, a place for every species and everything in its place.",
            "Darwin Darwin had this much more sort of random structure which reflects in the intuitions about these castec nature of evolution.",
            "What's interesting here in particular, though from our point of view, is that this is from an early notebook of Darwin's prior to working out the theory of natural selection.",
            "This is he was just playing around with some ideas about the structure of natural species, and it reflects the fact that these moments of structural discovery we're talking about are not necessarily deeply informed by a causal theory, right?",
            "Darwin didn't have the causal.",
            "I think that gave rise of the tree.",
            "He was just making inferences from the data he can see and some some vague intuitions about processes that could give rise to this about the likely structure.",
            "You have a similar situation with Mendeleev, who say discovered the periodic structure of the chemical elements.",
            "He didn't know quantum mechanics, right?",
            "He didn't know the underlying causal mechanisms that give rise to this periodic table, but he could go out and look at the properties of his set of objects, elements, how they interact with each other, and just it was just a pure but abstract pattern discovery, right?",
            "He just said this is seems to be.",
            "If this if I organize the.",
            "Objects in this way I can explain the data that I observe.",
            "How do you do that?",
            "That's that's a deep challenge to us.",
            "An children go through similar kinds of discoveries of abstract structural form in their in their development.",
            "So just to give one example that connects with what we've been talking about, the idea of hierarchical structure of categories.",
            "So it is true that reasonably young children like, say, 5 year olds, will understand that objects can be organized into something like a tree structure hierarchy, and that they can learn words for different nodes in that tree at different levels.",
            "Like you have a dog, it's also a mammal.",
            "It's also an animal that's also a living thing, and so on.",
            "But at the very earliest stages when children are learning words, they have something they go through what's called a mutual exclusivity stage where they just actually partition objects into flat clusters.",
            "It's like every there's everything is just in one kind of cluster, so it's more of this traditional clustering idea, and then only later do they realize actually no, we need something richer, structure, something more like a tree structure.",
            "I'll talk a little bit more tomorrow in the lecture on cognitive development about that and how this kind of model might give some insight into that.",
            "But for now I just want to tackle the abstract problem without talking about, you know.",
            "Evidence that children actually do anything like this just because from a machine learning point of view, or just generally a computational point of view, this isn't a problem that our traditional approaches really give much insight into, and it's a way we can try to actually advance the state of the art machine learning.",
            "We have all these techniques, whether it's PCA or multidimensional scaling, hierarchical clustering, clustering, K means clustering, Chinese restaurant process mixture models which are always of finding structure in data.",
            "But they all find structure of a particular form and we don't have general ways of sort of specifying large classes of different kinds of structures and figuring out among them what's the right.",
            "Formal structure."
        ],
        [
            "In a domain.",
            "So to put this goal kind of modestly.",
            "Not really, our ultimate goal is something like a universal framework for unsupervised learning, where instead of having this kind of bag of heuristic algorithms or even a bag of principle generative models with a human user going in and trying to select which is the right one to use an also humans being responsible for generating new models, we'd like to have some kind of universal learner who has some general notion of structure which includes all of these and can be combined in other ways to make new forms of structure and be able to learn from the data.",
            "What is the?",
            "What?",
            "What is the right form of structure and construct repres?"
        ],
        [
            "Stations of the right form.",
            "So here's how Charles figured out how to do this, and I don't want to say that this is he solved this problem.",
            "It's a really hard problem, and this is just this is just one first attempt, which I hope will just inspire a lot more.",
            "Other approaches to do this better and more generally, but the idea is to capture all these different forms of structure that we have in our standard toolkit of unsupervised learning with graphs and then to describe the different classes of structures, different forms with simple grammars or rules for generating graphs.",
            "And we'll see a lot more about grammars tomorrow when we talk about language, but.",
            "Basically, these grammars are rules for taking a node abit of structure and node with it's in and out links and rewriting it, expanding it out into a little bit more structure.",
            "So for example, here's a here's."
        ],
        [
            "Grammar which which says take a note and it's in and out links and replace it like that with two nodes connected directly with a direct connection where the one on the at the start.",
            "The parent has the in links and the child has the out links and you can see that if you apply this rule successively to one node of a structure just starting out from from just a single node."
        ],
        [
            "It grows out a chain and now."
        ],
        [
            "Matter how you apply this rule, it always grows out of chain."
        ],
        [
            "Similarly, there are rules which we can apply that will grow out flat clusters or these full orders dominance structures.",
            "You can apply these things to know that starts off connected to itself to get a ring.",
            "We can get hierarchies, including ones with the objects at any node of the hierarchy, or just at the leaf nodes.",
            "And by taking these very simple rules and combining the structures they generate just by taking cross products, we can get more interesting kinds of structures like across of 2 Chainz gives us this 2 dimensional grid, so that that allows us to go in the direction of multidimensional spaces.",
            "We can even get topologically interesting structures like you cross a chain in the ring and you get this cylinder here, so there's some combinatorial power to this.",
            "This sort of universal.",
            "Grammar if you like."
        ],
        [
            "Or structures."
        ],
        [
            "And the idea is now."
        ],
        [
            "To use that framework to describe this hypothesis, space of possible form.",
            "So the forms of structure now are described with these these rules, each of which generates a hypothesis space of particular relational structures, each of which specifies a class of graphs that can be generated by that rule and with some extra math which I don't have time to tell you bout actually puts a prior.",
            "It's A kind of Occam's razor like simplicity prior, and it uses a similar ideas to the so called Bayesian Occam's Razor, which we've seen a little bit about.",
            "But it's really a whole topic in itself, but that provides basically the hypothesis basin priors for these graph structures.",
            "And then we're going to be doing posterior inference about both the form and the structure from observed data looking for the joint setting of the structure and the former.",
            "The best form of structure and the best structure of that form, which together explain the data will the bottom level of this is just the same smoothness model that I showed you before, and we're just putting together the smoothness and this sort of simplicity constraint hypothesis space to do discovery of form as a kind of grammar induction.",
            "And as you like, yes, and we're just going to happen again.",
            "It's taking this the general idea of nonparametric probabilistic models, which was developed originally in statistics and machine learning, for asking these questions of how much structure exists and growing this out, so to speak, to talk about more general questions of what kind of structure exists.",
            "But that includes, if you like things like, well, how much structure, but of a more abstract form like is it one dimension or two dimensions?",
            "That's just a choice of grammar here, or how many levels in a hierarchy."
        ],
        [
            "Just show you a couple of examples of this of this structural form learning system.",
            "So given a set of animals and features, we find this tree structure and this is just yet another tree.",
            "It's a little more interesting than before, 'cause it's not just mammals, it includes insects and fish and birds and so on.",
            "But the key new insight here is that it figures out which form of structure it finds a tree, as opposed to any other kind of structure.",
            "But say given a different data set where now if you like the objects aren't biological species, but judges on the US Supreme Court and the features instead of being, you know whether.",
            "Rehnquist has feet or choose hey or whatever is the features are how they voted on different cases.",
            "So it's a kind of a political data set.",
            "Now what the algorithm discovers is a different form of structure.",
            "It's a 1 dimensional chain with the Liberals on the left, conveniently enough and the Conservatives on the right.",
            "The left right, of course, is arbitrary.",
            "But the idea that we talk about political views as a spectrum from left to right, that turns out not to be arbitrary, that actually seems to be the best form of structure to describe this domain.",
            "For better or worse, that's just the way it is, and our minds seem to have collectively, in cultural sense, locked."
        ],
        [
            "After that just a couple of other illustrations to show something of the generality.",
            "So here's a set of faces where we generated these from a face synthesizer so they're not real Germans or anybody else.",
            "They are synthesized, but it's, but we did this because we wanted to generate a realistic set of data, which we knew to be 2 dimensional.",
            "And see, could our algorithm discover that so there's two knobs that are being tweaked here.",
            "There's a black white racial nob.",
            "And then there's a kind of a masculinity nob you could clearly see the black white one.",
            "I'm not sure if you can tell that these are the more masculine ones, and these are the.",
            "More gender neutral ones.",
            "They're all pretty male.",
            "It might help if we call these ones the tennis team and these guys the football team.",
            "Then you can probably see it and the algorithm discovered.",
            "It finds those dimensions, but even more importantly it right.",
            "It finds that kind of two dimensional structure is the right one.",
            "Or here's just maybe the.",
            "Particularly example because it's not something that people have previously a kind of structure people have previously looked for, at least to my knowledge and unsupervised learning, which is this cylindrical structure.",
            "So here the model takes distances between cities and find that the best form of structure to describe that is this cross between a chain or a cross product of a chain and a ring, which basically corresponds to latitude in lanja tude."
        ],
        [
            "Alright, the last thing I was going to talk about, which I won't talk about 'cause we want to move on, but in case some of you are interested there, there's one more basic question that is that still an issue.",
            "Well, there's many, many, but the one other one I wanna talk about with this topic of how given that real knowledge representations have often different models for different aspects of a domain, how could we figure that out?",
            "And we this is again work of Pat Shadows in Charles Kempson.",
            "Also, the Koshman Singa have developed some interesting kinds of nonparametric Bayesian techniques in particular."
        ],
        [
            "Something called cross Cap where we've."
        ],
        [
            "If you like a Chinese restaurant process as well as a prior partition on the different features in the domain which is able to figure out how, for example, there are different features which can be described by what's a taxonomic structure.",
            "So this is a taxonomic clustering of animals and other features, like whether an animal is ferocious or lives in water or eats fish which can be described more by this ecological clustering in terms of, say, animals living in air or water, or whether their predator prey and if it figures this out both both how many different forms of structure there are, that's the.",
            "Organization this way and also with the Chinese restaurant process going over the rows nested inside each of these partitions.",
            "How many categories there are in each of these different ways of viewing the domain?",
            "So that's pretty cool, and it's another way that I would say we were closing the loop from human and machine learning.",
            "So just to wrap up then."
        ],
        [
            "We use this as another case study of our general approach to understanding these problems of induction in cognition an our toolkit, which includes the idea of Bayesian inference over generative models.",
            "The idea of models defined over representations of various forms of structure, and in particular to capture some of these more high level kind of reasoning.",
            "We've had to put in more explicit kinds of symbolic structure, but we can say that doesn't stop us from using these other valuable ideas coming more from Bayesian statistics in conventional machine learning, the idea of hierarchical Bayesian nonparametric approaches.",
            "To try to explain basically where these abstract forms of knowledge come from and to try to understand what form of knowledge exists in the domain and it's.",
            "It's a nice example both both in in what I talked about here and also going back to some of the techniques that Tom talked about at the end, like his feature discovery work based on the ICP and finding the words and so on where.",
            "Lost cognitive scientist sort of machine learning oriented cognitive scientists have been able to make progress in connecting these fields and really advancing the state of the art in both which we can show.",
            "We can demonstrate in a concrete way by both good quantitative fits to human data and also new and exciting.",
            "At least we're excited.",
            "Algorithms that take machine learning in new and more human like ways.",
            "OK, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, I'm going to continue a theme that Tom talked about.",
                    "label": 0
                },
                {
                    "sent": "Finding structure in data and continuing a theme that he's illustrated.",
                    "label": 1
                },
                {
                    "sent": "Very nice section number of themes, but in particular the what we hope is a virtuous and productive cycle of work going back and forth between human human learning and machine learning and trying to simultaneously push the state of the art of fields, which is really what this summer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "School is all about to set all of this in the broader context that we introduced on the first day.",
                    "label": 0
                },
                {
                    "sent": "Remember the big question that we're trying to get out?",
                    "label": 1
                },
                {
                    "sent": "How does the mind get so much from so little or these problems of induction across perception cognition?",
                    "label": 1
                },
                {
                    "sent": "Many in many areas of cognition, language categorization, causal learning, social understanding, and so on.",
                    "label": 0
                },
                {
                    "sent": "And in all these cases, what we're looking for are these general reverse engineering principles.",
                    "label": 0
                },
                {
                    "sent": "As Nick laid out very nicely before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what we've we've been developing here on the sort of computational cognitive science part of this summer school is a set of sort of a toolkit of techniques for answering the basic questions about how this could work.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Basically, these questions of abstract knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "How does knowledge guide inference from sparse data?",
                    "label": 1
                },
                {
                    "sent": "What form does knowledge take in these different domains, tasks, and how might that knowledge itself be learned?",
                    "label": 0
                },
                {
                    "sent": "And Tom in both the causal learning lecture and in the last one has given a very nice.",
                    "label": 0
                },
                {
                    "sent": "Very nice examples of how to how to put these ideas together.",
                    "label": 0
                },
                {
                    "sent": "The basic idea of Bayesian inference for understanding how knowledge guides inference from data.",
                    "label": 0
                },
                {
                    "sent": "The idea of doing probabilistic inference over various kinds of representations, different forms of structure, and then the idea of trying to understand where some of these inductive biases come from.",
                    "label": 0
                },
                {
                    "sent": "These abstract knowledge with at least back in the causal learning lecture.",
                    "label": 0
                },
                {
                    "sent": "The idea of doing some kind of inference over hierarchy of representations and having knowledge of some more abstract form that gives you some kind of hypothesis space, hypothesis spaces, or priors on priors and so on.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to try to illustrate many of these same ideas in the context of finding structure in data building on the kind of categorization issues that Tom talked about and have going up to some higher level knowledge which will also lead us into tomorrow's topic of more generally high level cognition and language.",
                    "label": 1
                },
                {
                    "sent": "Another way to put some of the themes I'm going to be getting out here just to take a theme that Tom was putting out there, he said the fundamental question that you're getting at with these nonparametric methods is how much structure exists in the world or in some domain of the world.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to try to generalize that here to the question of what kind of structure exists.",
                    "label": 0
                },
                {
                    "sent": "How much structure of a certain kind, like how many clusters?",
                    "label": 0
                },
                {
                    "sent": "That's that's one version of this, but we might also ask questions of what's the right form of knowledge.",
                    "label": 0
                },
                {
                    "sent": "Like if suppose we don't know that our world is clustered.",
                    "label": 0
                },
                {
                    "sent": "How could we figure out that clusters is the right kind of structure be looking for, as opposed to some other kind of structure?",
                    "label": 0
                },
                {
                    "sent": "And I'll treat all of this without starting with a very concrete case study like Tom did in the causal learning lecture.",
                    "label": 0
                },
                {
                    "sent": "I think this is where we can make the most progress and really see the potential for progress.",
                    "label": 0
                },
                {
                    "sent": "In this intersection between human and machine learning, is starting with what might seem like a pretty simple, boring, almost like psycho, physically boring Felix here.",
                    "label": 0
                },
                {
                    "sent": "Kind of that was too boring for him physically oriented kind of task where we can really dig into the meat of it and then see what sorts of larger questions that opens up.",
                    "label": 0
                },
                {
                    "sent": "So here that task is what cognitive psychologist called property induction or sometimes called category based induction 'cause it interacts very closely with and builds on the study of.",
                    "label": 0
                },
                {
                    "sent": "Natural categories of objects and it's been studied at least since the work of rips in the 1970s.",
                    "label": 0
                },
                {
                    "sent": "Many other famous and important cognitive psychologists have contributed to this latest office in Medine.",
                    "label": 0
                },
                {
                    "sent": "Susan Carey.",
                    "label": 0
                },
                {
                    "sent": "Steven Sloman, just to name a few.",
                    "label": 0
                },
                {
                    "sent": "So here's the kind of task and the kind of phenomena that you might study here.",
                    "label": 0
                },
                {
                    "sent": "The stimuli in the taskbar posed looking something like a classic kind of syllogistic argument.",
                    "label": 0
                },
                {
                    "sent": "But these aren't deductive arguments are inductive arguments.",
                    "label": 0
                },
                {
                    "sent": "They're not supposed to be true or false, or give you an all or none sense of validity, but just give you some graded sense of, well, how likely is the conclusion of the statement under the line to be true given the information above the line, the premises?",
                    "label": 0
                },
                {
                    "sent": "I should probably use this point.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They're not just gesture, so here's here's an inductive argument that most people think is pretty strong given that gorillas have T9 hormones and seals have T9 hormones, how likely is it that horses also have T9 hormones?",
                    "label": 1
                },
                {
                    "sent": "Many people think horses are pretty similar to gorillas and seals.",
                    "label": 0
                },
                {
                    "sent": "Maybe you think about some higher level category like, well, they're all mammals or something like that.",
                    "label": 0
                },
                {
                    "sent": "And those those facts seem relevant biological similarity, higher level biological categories.",
                    "label": 0
                },
                {
                    "sent": "Because the property here, which is what your reasoning about say, having teen hormones, seems like the kind of thing that you're basic.",
                    "label": 0
                },
                {
                    "sent": "Knowledge about the anatomy, Physiology, and biological structure of organisms is relevant for.",
                    "label": 0
                },
                {
                    "sent": "Now here are some other inductive arguments.",
                    "label": 0
                },
                {
                    "sent": "Say, given that gorillas and seals have some property, how likely is it that anteaters have this property and most people consider this to be a weaker argument?",
                    "label": 0
                },
                {
                    "sent": "That one again, maybe because anteaters, well, you know they are also mammals, are left similar to gorillas and seals than horses.",
                    "label": 0
                },
                {
                    "sent": "Or how about this one over here?",
                    "label": 0
                },
                {
                    "sent": "Gorillas have teen hormones, chimps, monkeys, baboons.",
                    "label": 0
                },
                {
                    "sent": "Those are four examples of animals with teen hormones.",
                    "label": 0
                },
                {
                    "sent": "How likely is the horses have T9 hormones again here in at least in some cases, people consider this to be a weaker argument.",
                    "label": 0
                },
                {
                    "sent": "This one, even though they horses might be.",
                    "label": 0
                },
                {
                    "sent": "Fairly similar to any one of these on their own, they don't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to be a very diverse sample of mammals.",
                    "label": 0
                },
                {
                    "sent": "It seems like, well, maybe this is just a property that's true of primates, so these are the kinds of concepts that have been proposed in cognitive psychology, notions of similarity or typicality diversity that can be can be understood qualitatively, and one way to think about what we're trying to do is to understand these concepts formally in terms of the structure of the inference and the underlying knowledge representations that support these concepts.",
                    "label": 0
                },
                {
                    "sent": "So here is it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just one slide overview of the set of ideas how we're going to attack these problems, and I should say at the outset that this is work that Charles Kemp did as part of his thesis with me, and it's really all his work.",
                    "label": 0
                },
                {
                    "sent": "Basically, I'm very lucky to have been involved in to be able to talk to you about it.",
                    "label": 0
                },
                {
                    "sent": "There's two key papers, a psych review paper in the PNS paper that I'll talk about later on.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "It's a picture of how we take the idea of hierarchical Bayesian models over structured representations and also non parametric representations whose complexity can grow with the data similar to the ideas that Tom talked about but defines in a slightly different way to work with what might be more structured forms of knowledge.",
                    "label": 0
                },
                {
                    "sent": "In this hierarchical model there's three levels that we call the level of the data structure and form up here, and these higher level structures together, or we think of is trying to capture.",
                    "label": 0
                },
                {
                    "sent": "In some way, this notion of an intuitive theory that cognitive scientists have long thought about is a very general way to think about abstract knowledge that guides inference from data.",
                    "label": 0
                },
                {
                    "sent": "But I won't really say much more except that you know the word theory might pop up in the background, but that's what I mean.",
                    "label": 0
                },
                {
                    "sent": "Is this set of knowledge at various different kinds of levels?",
                    "label": 0
                },
                {
                    "sent": "Here?",
                    "label": 0
                },
                {
                    "sent": "It's only the bottom level of the data that's directly observable, and in this case that might consist of various observable properties of animals, like you know whether an elephant has tusks or.",
                    "label": 0
                },
                {
                    "sent": "What they eat?",
                    "label": 0
                },
                {
                    "sent": "You know, choose various vegetables.",
                    "label": 0
                },
                {
                    "sent": "Horse.",
                    "label": 0
                },
                {
                    "sent": "Also choose things doesn't have tusks, so there's sort of a matrix of data here consisting of rows, which are these objects or categories, and then columns which are various observable features.",
                    "label": 0
                },
                {
                    "sent": "And there's some other features which maybe aren't directly observable, like whether an animal has some biological property, like T9 hormones.",
                    "label": 0
                },
                {
                    "sent": "Now, if you think how interesting is it really whether an animal has T9 hormones, then I think about other kinds of very important, but not usually directly observable or not easier.",
                    "label": 1
                },
                {
                    "sent": "Cheap to observe properties like whether something is good to eat or is poisonous, or whether something wants to kill you or not.",
                    "label": 0
                },
                {
                    "sent": "You know that might be very costly to observe.",
                    "label": 0
                },
                {
                    "sent": "The examples there and in all these cases what we want to be able to do is to take sparse observations about some new feature and be able to infer how to generalize to other other instances.",
                    "label": 0
                },
                {
                    "sent": "Other categories in this domain, their value for that feature.",
                    "label": 0
                },
                {
                    "sent": "So it's our goal is essentially to take this observed data and fill in the missing data of this matrix, and we're going to do that by positing these two levels of knowledge.",
                    "label": 0
                },
                {
                    "sent": "Some kind of what we call a structure, some relations over the objects domain like for exam.",
                    "label": 0
                },
                {
                    "sent": "Apple and intuitive version of a taxonomic tree where the species correspond to these leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "Here and then.",
                    "label": 0
                },
                {
                    "sent": "Some way of defining a probabilistic model over the structure of this tree, which which puts priors on the features that we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Basically well.",
                    "label": 0
                },
                {
                    "sent": "In fact, this this this will have an interpretation in some cases, as actually like the kernel of a Bayesian version of kernel methods, Gaussian process and so you can think of this as kind of a structured way of smoothing your very sparsely observed data.",
                    "label": 0
                },
                {
                    "sent": "And this in turn also is something which can be learned from the rest of the day, to the much more densely observed part.",
                    "label": 0
                },
                {
                    "sent": "And that comes from the fact that we have a probabilistic model underlying this.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "What's the view this as the likelihood for learning?",
                    "label": 0
                },
                {
                    "sent": "Or you could view it as the prior for this novel property.",
                    "label": 0
                },
                {
                    "sent": "It's a probabilistic model of how all of these pieces of observed or potentially observable data depend on the structure, and then learning the structure would be doing inference up to this level with a model conditioned by a hypothesis basin prior that's provided by this higher level knowledge.",
                    "label": 0
                },
                {
                    "sent": "This higher level knowledge says.",
                    "label": 0
                },
                {
                    "sent": "Maybe we're looking to represent the domain, not just with any possible graph, but with in particular a tree structure with the basic objects at the leaf nodes, and then that provides some higher level inductive bias which might help guide our inference, and we can even try to learn at this level.",
                    "label": 0
                },
                {
                    "sent": "I mean, this could be something which is which is fixed or given to a learner.",
                    "label": 0
                },
                {
                    "sent": "Or we could even think about how we could learn the more abstract structure of domain at this level, so that's the basic picture of what will be trying to do here.",
                    "label": 0
                },
                {
                    "sent": "Now I want to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It off by just giving you a sense for the kind of data, and again a little bit more depth of how people have explained this in the traditional psychological literature.",
                    "label": 0
                },
                {
                    "sent": "So here's 2 classic experiments from Oscarson and colleagues.",
                    "label": 0
                },
                {
                    "sent": "Where the idea is you give in one experiment, you give a number of arguments that are pretty similar, but might just very.",
                    "label": 0
                },
                {
                    "sent": "For example, in which species of animal appear in the premises here.",
                    "label": 0
                },
                {
                    "sent": "So in one case you might have cows, horses and rhinos and another one IP seals, squirrels and rhinos for example.",
                    "label": 0
                },
                {
                    "sent": "And then the conclusion category might be a general one like all mammals.",
                    "label": 0
                },
                {
                    "sent": "So you're asked there to generalize from a few examples to a larger category, or it might be some other category at the same level, like how likely is it that cows and Ryan, if cows and rhinos have this property that horses have this property?",
                    "label": 0
                },
                {
                    "sent": "And the property itself, in at least the classic versions of these experiments is sometimes called a blank property, meaning that something you don't know anything about.",
                    "label": 0
                },
                {
                    "sent": "But, crucially, you know something about this?",
                    "label": 0
                },
                {
                    "sent": "You know something abstract, which is that these properties are biologically relevant and that will become important as we start to go out from this domain and see other kinds of inductive reasoning for which other kinds of knowledge structures might be relevant.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there's another kind of data that's typically collected here, which is important for providing the basis for modeling.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the basis is provided by similarity judgments, as we've seen in some of the other lectures.",
                    "label": 0
                },
                {
                    "sent": "That's a standard way to get insight into mental representation or here it's feature listings.",
                    "label": 0
                },
                {
                    "sent": "So in a different experiment, office and colleagues had subjects tell you bout typical features for various animals.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have a data set of 48 or 50 animals, all mammals, and.",
                    "label": 0
                },
                {
                    "sent": "85 features and here's a here's some examples of the features that people judge to be typically associated with one animal elephant, so they are Gray there, hairless.",
                    "label": 1
                },
                {
                    "sent": "They have tough skin there.",
                    "label": 0
                },
                {
                    "sent": "Big their boldness, body shape.",
                    "label": 0
                },
                {
                    "sent": "They have long legs.",
                    "label": 0
                },
                {
                    "sent": "They have a tail and so on.",
                    "label": 0
                },
                {
                    "sent": "There's also various kinds of ecological properties like the vegetation there, grazers, they live in the Bush, in the jungle and so on.",
                    "label": 0
                },
                {
                    "sent": "They travel on the ground, they're smart, they're timid, so it's the kind of things that a child might hear from their parents.",
                    "label": 0
                },
                {
                    "sent": "Read about in picture books.",
                    "label": 0
                },
                {
                    "sent": "Hear about or see when they go to the zoo, not genomic high throughput screens or something like that, but just everyday observable properties of animals.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can put these two pieces of data together to try to give a general framework in which many modeling approaches can be compared, and it looks kind of like this, so there's the task that you're trying to model.",
                    "label": 0
                },
                {
                    "sent": "Are these judgments about how strong one of these arguments is compared to the others, which you can think of as from?",
                    "label": 0
                },
                {
                    "sent": "This is kind of computational formulation that should look more familiar to machine learning.",
                    "label": 0
                },
                {
                    "sent": "There's this new property which you can think of as an mostly unobserved column of this matrix, and you observe its value in just a couple of points corresponding to the two premises here.",
                    "label": 0
                },
                {
                    "sent": "And your job is to tell me which other.",
                    "label": 0
                },
                {
                    "sent": "What are the values of this property at all the on observed entries, and we're going to do that by extracting some more general knowledge from all these other observable features here.",
                    "label": 0
                },
                {
                    "sent": "So these these columns here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Respond to these directly observable features on the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last slide.",
                    "label": 0
                },
                {
                    "sent": "Somehow we want to extract some knowledge about what these things are like in general and how properties tend to be distributed over these objects that will bridge from the old features to the new property.",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar with the idea of semi supervised learning where you're trying to learn some given label, let's say this here the new property.",
                    "label": 0
                },
                {
                    "sent": "With a lot of unlabeled data, so here's all the unlabeled data, which is just unlabeled with respect to this new property we're learning.",
                    "label": 1
                },
                {
                    "sent": "This is you could see this as a kind of semi supervised learning problem, and there's very close relations between the approaches will develop and those in machine learning, although at the same time they actually go beyond them in an interesting way that would be worthwhile for machine learning people to pursue.",
                    "label": 0
                },
                {
                    "sent": "In fact, we've done a little bit of that with Charles.",
                    "label": 0
                },
                {
                    "sent": "It's also related to even more general class of problems that you might call sparsely observed matrix completion.",
                    "label": 1
                },
                {
                    "sent": "I don't mean sparse matrices, but sparsely observed things so you know if you've seen the Netflix challenge or.",
                    "label": 0
                },
                {
                    "sent": "Many other kinds of problems where we have some matrix and we just observe some entries in some rows and sum columns and we want to somehow figure out the underlying structure that can support generalization from the observed parts of the matrix to the rest of it.",
                    "label": 0
                },
                {
                    "sent": "So here it's set up in this particular structured way where there's a big observable chunk of the matrix, corresponding to, say, very perceptually in culturali available features and other things that might matter a lot better.",
                    "label": 0
                },
                {
                    "sent": "Harder to observe, but more generally, if you're if you somehow represent the abstract structure of this domain, that could be useful for filling in all sorts of an observed entries in.",
                    "label": 0
                },
                {
                    "sent": "In your observable data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of a typical cognitive psychological approach.",
                    "label": 0
                },
                {
                    "sent": "This problem.",
                    "label": 0
                },
                {
                    "sent": "You take a notion like similarity, which you say we're just going to assume that's a basic thing that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can compute, you know, for example something like distance in this feature space, or correlation between objects in the feature space.",
                    "label": 0
                },
                {
                    "sent": "SO22 species are similar if the rows of features are similar.",
                    "label": 0
                },
                {
                    "sent": "Horses and cows here would be very similar horses and seals.",
                    "label": 0
                },
                {
                    "sent": "You know less similar 'cause they have fewer features in common.",
                    "label": 0
                },
                {
                    "sent": "And then of course you need to do.",
                    "label": 0
                },
                {
                    "sent": "You need to go beyond just pairwise similarity if you want to evaluate an argument like this in similarity terms, you have to say well how similar are cows?",
                    "label": 0
                },
                {
                    "sent": "Two horses and rhinos as a set of examples and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In literature, several several ideas have been proposed.",
                    "label": 0
                },
                {
                    "sent": "Here's here's a plot for two models, but you could call the Max similarity model and the some similarity model.",
                    "label": 0
                },
                {
                    "sent": "So in the some similarity model is maybe the more familiar one.",
                    "label": 0
                },
                {
                    "sent": "It's similar to kernel density estimates or exemplar models and categorization, where what you do is you take the similarity to each of your test item.",
                    "label": 0
                },
                {
                    "sent": "If you like to each example, or each category in the premises, and then you sum up.",
                    "label": 0
                },
                {
                    "sent": "Or you could Alternatively take the average, but you sum up the similarity to each one in the Mac similarity model, you take the Max similarity.",
                    "label": 0
                },
                {
                    "sent": "So you say you know.",
                    "label": 0
                },
                {
                    "sent": "How similar is each mammal to 2 cows, elephants and horses?",
                    "label": 0
                },
                {
                    "sent": "And then what contributes is only the most similar exemplar?",
                    "label": 0
                },
                {
                    "sent": "It's interesting here that the maximal remodel works pretty well, and what we mean by working well here is that over the course of one experiment you have a bunch of these data points represents one stimulus, one argument, which again people give you various graded measures of strength, like this one here is considered to be a very strong argument from gorillas.",
                    "label": 0
                },
                {
                    "sent": "Meissen seals all mammals.",
                    "label": 0
                },
                {
                    "sent": "This one is considered to be a very weak one, perhaps intuitively because cows, elephants and horses, while each one might be a typical.",
                    "label": 0
                },
                {
                    "sent": "Mammal there as a Holder.",
                    "label": 0
                },
                {
                    "sent": "Not a very diverse sample of mammals.",
                    "label": 0
                },
                {
                    "sent": "They're all kind of these four legged herbivore's.",
                    "label": 0
                },
                {
                    "sent": "So that gets a relatively weak judgment, and the ratings on on the vertical axis are peoples judgments and the model predictions are on the X axis here, so here's a pretty high correlation between the Mac similarity model and what people say, whereas for the some similarity model which is interesting, this is the more familiar model in terms of, say, generalizing categories from examples.",
                    "label": 0
                },
                {
                    "sent": "Indeed, of these example, our models are basically the gold standard in.",
                    "label": 0
                },
                {
                    "sent": "Categorization, but here it's not just sort of worst correlation.",
                    "label": 0
                },
                {
                    "sent": "It's actually significant negative correlation, which is not something that I don't think I would have anticipated.",
                    "label": 0
                },
                {
                    "sent": "It really suggests there's something fundamentally different going on here that we need to understand.",
                    "label": 0
                },
                {
                    "sent": "Why is?",
                    "label": 0
                },
                {
                    "sent": "Why should similarity work in this particular way?",
                    "label": 0
                },
                {
                    "sent": "In this taxonomic domain?",
                    "label": 0
                },
                {
                    "sent": "But there's more richness here to this to the study of inductive reasoning beyond similarity, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are these arguments here?",
                    "label": 0
                },
                {
                    "sent": "Which of these do you consider to be a stronger argument, given that poodles can bite through?",
                    "label": 0
                },
                {
                    "sent": "Why are there for German Shepherds can bite through wire?",
                    "label": 1
                },
                {
                    "sent": "Or, given that Dobermans can bite through wire.",
                    "label": 0
                },
                {
                    "sent": "Therefore German shepherds can bite through wire.",
                    "label": 1
                },
                {
                    "sent": "So how many people choose the first one?",
                    "label": 0
                },
                {
                    "sent": "How many people choose the second one?",
                    "label": 0
                },
                {
                    "sent": "That's that's typically right, it, usually it's about 2/3, one third here it was about roughly about that.",
                    "label": 0
                },
                {
                    "sent": "So those of you who chose the second one.",
                    "label": 0
                },
                {
                    "sent": "Very good reason.",
                    "label": 0
                },
                {
                    "sent": "Right Dobermans and German Shepherds seem more similar than Poodles and German Shepherds, and we've just been talking all about similarity for the last few minutes now.",
                    "label": 0
                },
                {
                    "sent": "Why, though?",
                    "label": 0
                },
                {
                    "sent": "Would you choose the first one and then want to say intuitively?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "German Shepherd can do it then because dogs can write it.",
                    "label": 0
                },
                {
                    "sent": "So it seems like intuitively there's some underlying dimension here that's relevant, like strength or size that you referred to, and you know human.",
                    "label": 0
                },
                {
                    "sent": "Right, so we're talking about the Alsatian dog, that's right.",
                    "label": 0
                },
                {
                    "sent": "I see, yeah, so that's that's that's even higher level cognition, and I was hoping to get out here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you did a different.",
                    "label": 0
                },
                {
                    "sent": "It seemed like you did a different kind of reasoning there, right?",
                    "label": 0
                },
                {
                    "sent": "So something that's going to be more asymmetric, right?",
                    "label": 0
                },
                {
                    "sent": "If we turned it around and we said, given that German Shepherds can bite through wire, how likely is it that poodles can bite through wire?",
                    "label": 1
                },
                {
                    "sent": "You'd say, well, that doesn't help you at all.",
                    "label": 0
                },
                {
                    "sent": "'cause German Shepherds are really strong, right?",
                    "label": 0
                },
                {
                    "sent": "So this looks very different than similarity Now, but this one here.",
                    "label": 0
                },
                {
                    "sent": "Here I'm directly testing if you like the symmetry of these kinds of inductions.",
                    "label": 0
                },
                {
                    "sent": "So given that salmon carry some bacteria, how likely is it that grizzly bears carry the same bacteria versus given that grizzly bears carry that bacteria, how likely is it that salmon carry that bacteria?",
                    "label": 0
                },
                {
                    "sent": "So how many people choose the first one?",
                    "label": 0
                },
                {
                    "sent": "How people choose the second one?",
                    "label": 0
                },
                {
                    "sent": "OK, well if you're not from North America, you may not know very much about grizzly bears, but grizzly bears like to eat salmon, and one intuition that many people have here is that if grizzly bears, well, it basically.",
                    "label": 0
                },
                {
                    "sent": "Grizzly bears could have gotten this from the salmon, and people prefer to reasoning from a direction of cops to effect than the other way around.",
                    "label": 0
                },
                {
                    "sent": "So you can see how this even this disparate leave.",
                    "label": 0
                },
                {
                    "sent": "Very simple task of inductive reasoning from properties of natural species can bring in all sorts of different kinds of knowledge that we want to understand.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is where the tools of this hierarchical Bayesian approach might be valuable.",
                    "label": 0
                },
                {
                    "sent": "So in this case this picture here with it with something like a taxonomic tree, might be a way to capture some of this sort of similarity based taxonomic reasoning, but we might need other forms of knowledge to capture these other kinds of inductive reasoning that go beyond similarities.",
                    "label": 0
                },
                {
                    "sent": "So let's try to see how this approach can play out in this domain and will start off developing the basic ideas for similarity based taxonomic reasoning, But then will also show a little bit about how it can work for other forms of knowledge and then come to the question of how do you figure out.",
                    "label": 0
                },
                {
                    "sent": "What's the right form of knowledge for a domain?",
                    "label": 0
                },
                {
                    "sent": "So first let's see how do we model the basic property induction task.",
                    "label": 0
                },
                {
                    "sent": "How do we go from some set of observed features to make intelligent inferences about a very sparsely observed feature?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And not, perhaps not too surprisingly, this is.",
                    "label": 0
                },
                {
                    "sent": "This is the Bayesian part or the most basic Bayesian part, so we can think of very concretely like this.",
                    "label": 0
                },
                {
                    "sent": "We can say this is what we want to infer one and further missing entries in this column, and we can consider a hypothesis space of all possible ways to complete this column.",
                    "label": 0
                },
                {
                    "sent": "All possible labelings?",
                    "label": 0
                },
                {
                    "sent": "Let's just say it's a binary feature that's either true or false for each object or category in the domain.",
                    "label": 0
                },
                {
                    "sent": "So we could write down a set of if we have N animals here that is 2 to the N possible binary vectors to the end ways to complete.",
                    "label": 0
                },
                {
                    "sent": "From any set of examples.",
                    "label": 0
                },
                {
                    "sent": "So here I'm just showing a few of them, and this includes very simple hypotheses like this one which says the property is true for all animals, or this one which says it's true for all but the aquatic mammals, Dolphins and seals.",
                    "label": 0
                },
                {
                    "sent": "And then you can have other things down here that look more arbitrary like this.",
                    "label": 0
                },
                {
                    "sent": "One is true for horses and shrimps.",
                    "label": 0
                },
                {
                    "sent": "Squirrels, seals and rhinos, but not the other animals.",
                    "label": 0
                },
                {
                    "sent": "The basic Bayesian analysis of this problem says look what examples we have in the premises that's represented here by X, and we want to compute the conditional probability that the conclusion is true.",
                    "label": 0
                },
                {
                    "sent": "So the basic Bayesian analysis says assign some probability to each of these hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's going to be a prior and go in and just look at this ratio here.",
                    "label": 0
                },
                {
                    "sent": "Look at how much prior probability there is on the hypothesis, which include both the premise categories and the conclusion category relative to how much prior probability probability there is an hypothesis which just include.",
                    "label": 0
                },
                {
                    "sent": "The premises, so if there's a category which is in many or most of the high probability hypothesis that the examples the premise categories are also in, then it's very likely to get how this property an otherwise maybe not so much.",
                    "label": 0
                },
                {
                    "sent": "So here I've just written down some arbitrary schematic assignment of priors, which gives this very simple hypothesis that the property is true for all mammals.",
                    "label": 0
                },
                {
                    "sent": "Pretty high probability.",
                    "label": 0
                },
                {
                    "sent": "These other ones here, you know that are sort of seem like they're neatly taxonomically organized.",
                    "label": 0
                },
                {
                    "sent": "Lower but still pretty high probability, and then these kind of arbitrary hypothesis here.",
                    "label": 0
                },
                {
                    "sent": "Very low probability and then.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Bayesian inference here is just.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple, what I've done here is I've just ruled out all the hypotheses.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are inconsistent with the examples, so I'm left with the hypothesis that are consistent and I now see how much of the prior probability on these ones, the ones that include both horses and rhinos, also applies to any one of these other animals.",
                    "label": 0
                },
                {
                    "sent": "And then I've just shown that in grade level here that conditional probability, so it's very high for cows because most of these high probability hypotheses, these ones over here, which include horses and rhinos, also include cows.",
                    "label": 0
                },
                {
                    "sent": "But let's say it's much lower for Dolphins, because only one of those includes Dolphins.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense again, so this should be very elementary, but doing this analysis just raises the interesting question that this this lecture is real.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About which is what is the real structure of this prior?",
                    "label": 0
                },
                {
                    "sent": "Is it just a enumerated list of numbers over all possible hypothesis, or if not, what is it and where does it come from?",
                    "label": 0
                },
                {
                    "sent": "So if we ask where does it come from?",
                    "label": 0
                },
                {
                    "sent": "Here's here's a simple empiricist answer.",
                    "label": 0
                },
                {
                    "sent": "Write an answer that says knowledge is grounded in our sensory experience or their experience of the world.",
                    "label": 0
                },
                {
                    "sent": "So why don't we do this?",
                    "label": 0
                },
                {
                    "sent": "We want to assign a prior probability to any possible way that feature could pattern over the objects in our domain.",
                    "label": 0
                },
                {
                    "sent": "Just remember how often in the past we saw features that pattern in some way.",
                    "label": 0
                },
                {
                    "sent": "You know how often in the past if we see a feature that was true of justice.",
                    "label": 0
                },
                {
                    "sent": "Just you know horses, cows, chimps, gorillas, my squirrels rise and elephants and not Dolphins, seals and then set these priors for a new feature in proportion to our frequency in our past experience.",
                    "label": 0
                },
                {
                    "sent": "Does that sound like a good idea?",
                    "label": 0
                },
                {
                    "sent": "I mean this very simple should work in some sense, right?",
                    "label": 0
                },
                {
                    "sent": "You have to have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how much?",
                    "label": 0
                },
                {
                    "sent": "Possibilities.",
                    "label": 0
                },
                {
                    "sent": "A lot, yeah, not just all the possibilities, but all the possibilities.",
                    "label": 0
                },
                {
                    "sent": "Lots of times.",
                    "label": 0
                },
                {
                    "sent": "I mean, this goes back to basic issues that these guys talked about right?",
                    "label": 0
                },
                {
                    "sent": "Or an also that Tom was talking basic issues of inductive bias.",
                    "label": 0
                },
                {
                    "sent": "Remember, I think both Neil and Bernard showed in the context of say, learning about binary digits.",
                    "label": 0
                },
                {
                    "sent": "How much data would we have to see to learn an intelligent prior overall binary vectors in a 60 by 60 image space, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's huge and you've got the same sort of problem here.",
                    "label": 0
                },
                {
                    "sent": "I'm just listing 10 animals, but there's lots and lots of animals we know about and animals are just a small fraction of the concepts that we know about.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to actually estimate a distribution over all possible ways of labeling all possible things that you know that would just take a ridiculous amount of data.",
                    "label": 0
                },
                {
                    "sent": "So basically sort of bias variance issues.",
                    "label": 0
                },
                {
                    "sent": "Just say that's never going to work, but also more interesting Lee, right?",
                    "label": 0
                },
                {
                    "sent": "We want to understand where these priors come from, even if it might be the case, you know the brain has lots of connections, maybe at some level in the brain.",
                    "label": 0
                },
                {
                    "sent": "The prior does look a lot like this.",
                    "label": 0
                },
                {
                    "sent": "Maybe it is just a billions and billions of numbers just laid on top of each other, but from a more cognitive level.",
                    "label": 0
                },
                {
                    "sent": "There's real knowledge here.",
                    "label": 0
                },
                {
                    "sent": "It's not just high dimensional distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So if we take.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just a generic illustration of inductive bias, but we've seen a lot of these examples so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip over this if we take these different kinds of reasoning phenomenon that we showed.",
                    "label": 0
                },
                {
                    "sent": "You know the basic kind of taxonomic similarity, but also these cases, like reasoning from poodles.",
                    "label": 0
                },
                {
                    "sent": "Dobermans where you appeal to some dimension like strength or these sort of food.",
                    "label": 0
                },
                {
                    "sent": "Web kinds of reasoning.",
                    "label": 0
                },
                {
                    "sent": "This more causal character, it seems like not just our different priors relevant, but qualitatively different forms of knowledge are being used in these different cases, and we want to understand what those forms of knowledge are and how they give rise to the numbers that Bayesian inference needs.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's try to see how this goes and I'll first show you how we can develop a prior for this taxonomic similarity and then branch out to other forms of knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the basic starting point here is the idea of some kind of a taxonomic tree, and there's a number of reasons to think about organizing the structure of biological species this way.",
                    "label": 0
                },
                {
                    "sent": "First of all, of course, because you know what we know about the actual structure of the world, evolutionary theory tells us that species are generated from some kind of a branching process with, so make sense to think of putting the basic categories at the leaf nodes of a tree.",
                    "label": 0
                },
                {
                    "sent": "There's a long history in cognitive psychology of positing something like tree structured semantic hierarchies, so this is a classic.",
                    "label": 0
                },
                {
                    "sent": "Example from the work of Collins and Quillian.",
                    "label": 0
                },
                {
                    "sent": "It's like the oldest data structure in the book literally.",
                    "label": 0
                },
                {
                    "sent": "Now here the actual trees were going to use look a little bit more like evolutionary trees and that they might be kind of have this more arbitrary branching structure.",
                    "label": 0
                },
                {
                    "sent": "But again, they're not going to be grounded in any real evolutionary science.",
                    "label": 0
                },
                {
                    "sent": "It's more intuitive version.",
                    "label": 0
                },
                {
                    "sent": "Imagine people sort of looking around in the world and coming to this kind of tree now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's going to be our basic structure, and then we need some way of defining a probability model over the properties of animals.",
                    "label": 0
                },
                {
                    "sent": "In terms of this tree.",
                    "label": 0
                },
                {
                    "sent": "And here we're going to use an idea which is which is borrowed from machine learning.",
                    "label": 0
                },
                {
                    "sent": "The basic idea that we can think of some notion of smoothness as the basis of similarity.",
                    "label": 0
                },
                {
                    "sent": "So intuitively we want to say properties which which which very smoothly over the tree should have higher probability than ones which vary in a more arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "And this is a nice notion because it's not just specific to tree structure.",
                    "label": 0
                },
                {
                    "sent": "You can define this on any graph and will use that too.",
                    "label": 0
                },
                {
                    "sent": "Make this a much broader model, so here is between.",
                    "label": 0
                },
                {
                    "sent": "These are two tree structured graphs.",
                    "label": 0
                },
                {
                    "sent": "This one is a special case of a tree that's a chain, and here are two cases of smoothly varying features, right where intuitively you only have to cut the graph in a small number of places to separate and say the positive and negative instances, whereas these are much less smooth ones and we want to say intuitively a feature that's distributed like this should have high probability.",
                    "label": 0
                },
                {
                    "sent": "If this is the underlying structure and a feature distributed in this way should have low probability, and there's some math behind this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to go into the details of the math, but I'm happy to talk about it.",
                    "label": 0
                },
                {
                    "sent": "Or really, I just point you to the work of Jerry Zhu, John Lafferty and Zubin Ghahremani, which we build on.",
                    "label": 0
                },
                {
                    "sent": "There's a number of related ideas, but this is the work that we most directly build on, and it has has a number of different interpretations.",
                    "label": 0
                },
                {
                    "sent": "You can interpret this in terms of diffusion like the idea.",
                    "label": 0
                },
                {
                    "sent": "Imagine a property kind of diffusing over a graph and and that's a way to capture smoothness.",
                    "label": 0
                },
                {
                    "sent": "Or you can.",
                    "label": 0
                },
                {
                    "sent": "There's also a connection to two kernels from a Gaussian process formalism where you can think of this as like a.",
                    "label": 0
                },
                {
                    "sent": "You know the covariance using the graph to capture the covariance structure of a Gaussian model, where each node each object is a random variable and you're trying to capture a high dimensional distribution, right?",
                    "label": 0
                },
                {
                    "sent": "What's the joint distribution of how properties can be distributed over all objects and intuitively want to say objects which are close in the graph should have high covariance.",
                    "label": 0
                },
                {
                    "sent": "They're likely to share the same values of features whatever they are.",
                    "label": 0
                },
                {
                    "sent": "This is a particularly simple way to write down the math where we say F, INF, J represent the feature value for objects I&J, and then there's an edge.",
                    "label": 0
                },
                {
                    "sent": "There's edges between some objects in the graph, and those edges can have a length DJ and intuitively want to say things which are connected by short edges.",
                    "label": 0
                },
                {
                    "sent": "That's high coupling, so they should.",
                    "label": 0
                },
                {
                    "sent": "We should penalize a distribution of features in which they are very different, and that's what this quadratic model does here.",
                    "label": 0
                },
                {
                    "sent": "And then we treat that as an energy which we exponentiate to get a probability model over the entire over the entire set of objects.",
                    "label": 0
                },
                {
                    "sent": "And you can interpret this.",
                    "label": 0
                },
                {
                    "sent": "The link between the idea of the Laplacian of a graph and the covariance matrix of this big high dimensional Gaussian which is has a lot of interesting math behind it, but I'm not going to dwell on it here, that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "Hopefully should be should be clear and just think intuitively objects which are close from the graph should should go very highly in terms of their feature values and that idea.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is very powerful, so if we took this tree structure that I put there, which hopefully you know I didn't dwell on it, but hopefully it seems fairly intuitive.",
                    "label": 0
                },
                {
                    "sent": "We have say the primates in one branch of the tree we have the rodents, we have the Dolphins and seals, the aquatic mammals.",
                    "label": 0
                },
                {
                    "sent": "They're in another branch.",
                    "label": 0
                },
                {
                    "sent": "We have the big four legged herbivores over there so on I'll tell you in a minute where we get that tree.",
                    "label": 0
                },
                {
                    "sent": "But let's say we're given that tree and we define the smoothness prior over it and then we use that to provide the predictions for novel properties.",
                    "label": 0
                },
                {
                    "sent": "Well, it provides a very very good model for a number of datasets, so I'm just showing these are five different datasets.",
                    "label": 0
                },
                {
                    "sent": "It's the same model in each case, but different judgments are being predicted along the Y axis and again dots down here are ones which the model says are very low, likely to generalize from the examples to the test item or the conclusion.",
                    "label": 0
                },
                {
                    "sent": "These ones are very high and people agree very much.",
                    "label": 0
                },
                {
                    "sent": "There's very high correlation, so again, it's not perfect, but for high level cognition, it's pretty compelling.",
                    "label": 0
                },
                {
                    "sent": "Impressive results here, so this is, these are just two different.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assets and here are three others that are a little bit smaller with different mammals.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next question you should be asking, and we're just going to be working our way up through this model, is where does that structure come from?",
                    "label": 0
                },
                {
                    "sent": "I just gave positive to tree and said, well, if that was the tree, you can define the smoothness prior over to look.",
                    "label": 0
                },
                {
                    "sent": "It does great, but where does it come from?",
                    "label": 0
                },
                {
                    "sent": "Well, here's where we start to see the generality of this approach or just want one very nice Journal aspect of it, which is basically the very same hierarchical Bayesian model we used to provide a prior for inducing this new extended.",
                    "label": 0
                },
                {
                    "sent": "This new property helps to explain where history comes from, a kind of top down and bottom up Fusion of evidence.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's a it's a Bayesian tree learner, so here's the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea we have all these observed features which I was telling you about and we're going to assume that each of these features is generated over the same graph structure as our novel feature.",
                    "label": 0
                },
                {
                    "sent": "In particular, each one is an independent sample on this, so they are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "They're not, they're not a priority independent, they're highly Coke couple.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point of talking about similarity, but condition on the tree.",
                    "label": 0
                },
                {
                    "sent": "Each one is an independent draw.",
                    "label": 0
                },
                {
                    "sent": "From this this Gaussian, smoothing this process and then our job is to figure out which tree, or maybe to infer a distribution over trees.",
                    "label": 0
                },
                {
                    "sent": "That best explains all these features as a whole, and then we're going to use that same tree structured model to give us a prior on a new property.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's how it works.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of of learning a tree for this larger set of 50 mammals from roughly 100 features or so, and again, you can see all sorts of interesting structure that isn't really biologically correct like it's not biologically correct to put Dolphins, seals, and whales together.",
                    "label": 0
                },
                {
                    "sent": "Dolphins and whales fine, but seals are rather different.",
                    "label": 0
                },
                {
                    "sent": "They're more like dogs, Dolphins and whales, maybe more like cows.",
                    "label": 0
                },
                {
                    "sent": "But intuitively, that's how we think of things.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and this is just what I said.",
                    "label": 0
                },
                {
                    "sent": "We were sort of learning this tree here from the observed features and then using that to transfer to this new property.",
                    "label": 0
                },
                {
                    "sent": "This is if you like our tree structured approach to semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Now you could use other kinds of structure and that's where again we start to see the power of being able to do Bayesian inference over different kinds of representation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, for example, suppose we go back to the kind of representations that have long guided categorization research and that Tom was talking about basically flat clusters, right?",
                    "label": 0
                },
                {
                    "sent": "So suppose we think that are the structure of our world is take these animals and cluster them into some higher level categories.",
                    "label": 0
                },
                {
                    "sent": "Could that provide the basis for generalization here?",
                    "label": 0
                },
                {
                    "sent": "Well, let's say we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the mysteriously process or Chinese restaurant process mixture so this is basically the same model that Tom was talking about and that John Anderson introduced into cognitive psychology now 20 years ago, and these are these are the five clusters that it finds for this set of 50 animals and you can see there pretty sensible, right?",
                    "label": 0
                },
                {
                    "sent": "So you've got these rodents here.",
                    "label": 0
                },
                {
                    "sent": "You've got the primates for the most part.",
                    "label": 0
                },
                {
                    "sent": "You've got the aquatic mammals.",
                    "label": 0
                },
                {
                    "sent": "You've got these sort of two kinds of basically herbiv, ricin, carnivorous animals, and suppose this was the underlying.",
                    "label": 0
                },
                {
                    "sent": "Structure well.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out here we can compare that restructured model in the cluster structure.",
                    "label": 0
                },
                {
                    "sent": "Would the cluster one actually does pretty well in terms of correlation, but if you look at what's going on here, you see there's something fundamentally wrong, right?",
                    "label": 0
                },
                {
                    "sent": "What do you see when you look at this?",
                    "label": 0
                },
                {
                    "sent": "You see these these bands?",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "These are these are different stimuli for which the model all makes exactly the same prediction.",
                    "label": 0
                },
                {
                    "sent": "But people make very different judgments, and you see that coming up.",
                    "label": 0
                },
                {
                    "sent": "In each case Huawei is that anyone can explain.",
                    "label": 0
                },
                {
                    "sent": "Huawei would, why would you see these bands here for this?",
                    "label": 0
                },
                {
                    "sent": "For these kind of tasks with a flat cluster model as opposed to for a tree structure model, yeah.",
                    "label": 0
                },
                {
                    "sent": "Raising your hand.",
                    "label": 0
                },
                {
                    "sent": "Anyone else want to?",
                    "label": 0
                },
                {
                    "sent": "In this case, that's true, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so in our case we assume that that the properties are conditionally independent based on whatever the latent structure is.",
                    "label": 0
                },
                {
                    "sent": "But but here the situation we have right is that you're only you're only capturing.",
                    "label": 0
                },
                {
                    "sent": "If you like 1 grain of data.",
                    "label": 0
                },
                {
                    "sent": "So either the premises the examples are in one.",
                    "label": 0
                },
                {
                    "sent": "Either are in the same category as the test item, or they're not, and if you have, say, 2 examples, then you either have both of them in the same category, or one of them, or none of them.",
                    "label": 0
                },
                {
                    "sent": "So there's really only a few different levels of prediction you could make, whereas in the tree you have a much more great in structure.",
                    "label": 0
                },
                {
                    "sent": "It's a richer inductive bias.",
                    "label": 0
                },
                {
                    "sent": "If you like this is this.",
                    "label": 0
                },
                {
                    "sent": "Inductive bias is too strong, and that shows up in the fact that there's less that it can predict.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's just right now, but what about a different kind of representation?",
                    "label": 0
                },
                {
                    "sent": "So again, you know.",
                    "label": 0
                },
                {
                    "sent": "Just as Tom was showing for causal learning, the power of this kind of approaches, we can use it to evaluate different kinds of representations in terms of how they fit the same sort of data.",
                    "label": 0
                },
                {
                    "sent": "So how about a low dimensional space?",
                    "label": 0
                },
                {
                    "sent": "Again, a very popular model in categorization, research and perception.",
                    "label": 0
                },
                {
                    "sent": "Like, for example, we talked about face faces.",
                    "label": 0
                },
                {
                    "sent": "What about an animal space?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's here's a a very similar kind of way of building a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's actually closely related, though, not identical, to work that Neil did, but the idea is just as we as we came up with, say, a tree structure with the, with the idea that we could define a smoothness prior over that tree and say well which tree best explains all the observed features.",
                    "label": 0
                },
                {
                    "sent": "We could say which low dimensional space embedding of these animals best explains the observed features, where roughly two things are more likely to have the same property, or they covary more the closer they are in this space.",
                    "label": 0
                },
                {
                    "sent": "And this is a 2D embedding of the very same animals, and again we can pick out just the ones that that are you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use to simulate in any one of these tasks, and again it doesn't do badly.",
                    "label": 0
                },
                {
                    "sent": "In particular, it does the best the best to the embedding does very well on generalizing where the target categories horses.",
                    "label": 0
                },
                {
                    "sent": "That's this data set.",
                    "label": 0
                },
                {
                    "sent": "But this is a slightly harder task here where you asked to generalize to all mammals, and so it's really more about its testing.",
                    "label": 0
                },
                {
                    "sent": "More of the global geometry of this space.",
                    "label": 0
                },
                {
                    "sent": "How well does your underlying structure underlying inductive bias get the global geometry right?",
                    "label": 0
                },
                {
                    "sent": "In order to be able to infer basically how well does any set of a few examples cover the set of all mammals, that's what either of these approaches are getting at.",
                    "label": 0
                },
                {
                    "sent": "And there that restructured model is much better, so it again suggests that here the 2D model has a pretty good inductive bias, but it's not the right inductive bias and it shows up at least in some cases.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diagnosing that now, do we think that there's something intrinsically good about trees and not 2 dimensional spaces?",
                    "label": 0
                },
                {
                    "sent": "Well, our general take on this is that.",
                    "label": 0
                },
                {
                    "sent": "Just like in any other issue of inductive bias, what makes a good representation have?",
                    "label": 0
                },
                {
                    "sent": "There's several things we need to think about, right?",
                    "label": 0
                },
                {
                    "sent": "You need to think about degrees of freedom, and so I need to think about computational tractability.",
                    "label": 0
                },
                {
                    "sent": "But you also need to think about the structure of the world Anna representation or prior is going to be a good one if it matches the actual structure of the world, at least two to enough of an extent to support the kinds of generalizations you need to do, you have to be able to get it right, and here we're seeing that.",
                    "label": 0
                },
                {
                    "sent": "So this is a parallel experiment that Charles did.",
                    "label": 0
                },
                {
                    "sent": "For a domain where we thought unlike all these classic studies of animal species in their properties, where we thought more of a low dimensional space, in particular, 2 dimensional space would be the right inductive bias for the world, and we wanted to see can we just show quantitative evidence in RCM psychological paradigm that people are using.",
                    "label": 0
                },
                {
                    "sent": "That was something like a double dissociation here.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what we got.",
                    "label": 0
                },
                {
                    "sent": "So this is a visa geographic inference task where the objects are cities and people are asked about various properties that that could be expected to vary smoothly over the geographical space over the 2D geometry of the.",
                    "label": 0
                },
                {
                    "sent": "United States in this case, and we can get from people.",
                    "label": 0
                },
                {
                    "sent": "We can assess both the best 2 dimensional map which you know hopefully and indeed does roughly correspond to the actual 2 dimensional map of the US, or we can take from their judgments of similarity or proximity.",
                    "label": 0
                },
                {
                    "sent": "We can take the best restructured bottle and see now how well do these do we're going to find the very same Gaussian smoothness process over these two different representations.",
                    "label": 0
                },
                {
                    "sent": "And here we get the opposite pattern that restructured model.",
                    "label": 0
                },
                {
                    "sent": "That's the one down here.",
                    "label": 0
                },
                {
                    "sent": "The sexual doesn't do very well, but the two dimensional model.",
                    "label": 0
                },
                {
                    "sent": "Does pretty well.",
                    "label": 0
                },
                {
                    "sent": "Not not perfectly, but at least it does significantly better than that restructured model and we just take this as a way.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's I don't think it's all that surprising to say hey, people know about the two dimensional layout of big cities in the United States, and they use that to guide their inferences, but more as a way of validating this approach and showing that this is a way to work backwards from people's basic inductive judgments to make inferences about which which kinds of knowledge representations or structured priors are more appropriate in different cases, and to assess the fit of what's in the head with what's in the world.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there are lots of other questions you could ask.",
                    "label": 0
                },
                {
                    "sent": "How are we doing on time?",
                    "label": 0
                },
                {
                    "sent": "15 minutes?",
                    "label": 0
                },
                {
                    "sent": "OK, then I won't dwell on this.",
                    "label": 0
                },
                {
                    "sent": "One of the many interesting questions, but.",
                    "label": 0
                },
                {
                    "sent": "Charles and I.",
                    "label": 0
                },
                {
                    "sent": "Well in general that the probabilistic models community and the.",
                    "label": 0
                },
                {
                    "sent": "Has been engaged in interesting and productive debate with the connectionist or neural network modeling community, where there's really one of the big issues that we're debating is how much structure is in the head.",
                    "label": 0
                },
                {
                    "sent": "You know, when you look at the brain as connections do for inspiration or neural network researchers, you don't see trees or necessarily these low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "You see high dimensional spaces of connectivity.",
                    "label": 0
                },
                {
                    "sent": "Cinema question to ask, do we really need some kind of structure or low dimensional representations?",
                    "label": 0
                },
                {
                    "sent": "Do we need to posit people are learning these explicit structures?",
                    "label": 0
                },
                {
                    "sent": "Rogers and McClelland, for example, have proposed a connectionist network which can approximate these kinds of tree structures for the same domain, and they propose that as an alternative to what we've been doing, and it's an interesting question whether you can test these different accounts on the same sort of data.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this kind of connections approach, at least in our in our number of tests, doesn't seem to work very well on these inductive judgments.",
                    "label": 0
                },
                {
                    "sent": "But there are some other approaches that a student in our group, Brandon Lake, has been exploring with her.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of interesting hybrids of these these structured probabilistic models and connectionist approaches their structured in the sense that they are learning one of these Gaussian graphical models, but they're less structured at the abstract level because they don't specify a an explicit form like a tree structure or low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Rather, they just tried to use a more generic idea of sparsity, and for example he's able to just learn a sparse Gaussian graphical model for, say, the city data or the tree structure data, which does pretty well at predicting property induction judgments, and has another nice feature which is.",
                    "label": 0
                },
                {
                    "sent": "We can scale up to two very large semantic domains like just all the things you know.",
                    "label": 0
                },
                {
                    "sent": "Where we don't expect to have one single kind of structure like one single tree or low dimensional space to capture concepts as diverse as buildings, colleges, Penguins, fish and vegetables.",
                    "label": 0
                },
                {
                    "sent": "This is just a little slice of applying this model to a set of 1000 semantic concepts and it's you know it's actually a way to learn something like a star semantic network.",
                    "label": 0
                },
                {
                    "sent": "Now it has other other things that can't do.",
                    "label": 0
                },
                {
                    "sent": "For example, it can't capture more abstract linguistic ways of learning about the world, like for example, if I tell you.",
                    "label": 0
                },
                {
                    "sent": "That you know Dolphins look like fish, but they're not.",
                    "label": 0
                },
                {
                    "sent": "They're actually mammals.",
                    "label": 0
                },
                {
                    "sent": "Then that changes your underlying knowledge representation in a way that it seems like you need to have something like a tree structure to be able to say, OK, what do I mean by mammal?",
                    "label": 0
                },
                {
                    "sent": "I mean this node, and when I'm saying Dolphin and what I mean by fish, I mean that note.",
                    "label": 0
                },
                {
                    "sent": "And when I say that Dolphins aren't fish, they're mammals, and that means I have to somehow move this over here.",
                    "label": 0
                },
                {
                    "sent": "And if I move that, maybe I'm going to move the whole subtree.",
                    "label": 0
                },
                {
                    "sent": "So maybe that means other things that look like Dolphins, like, say, whales maybe should go with them.",
                    "label": 0
                },
                {
                    "sent": "That kind of more explicit symbolic reasoning can't really be captured here, but it's an interesting question to see where these might meet up.",
                    "label": 0
                },
                {
                    "sent": "Another thing that hasn't yet been captured here is going beyond what all of these approaches that I've shown you so far have been doing, which is basically trying to model fancy versions of similarity, just like in the kernel, both Bayesian and non Bayesian approach is at the heart of all of these things, is just the idea that there's some notion of similarity over some kind of structure, either a graphical structure or some higher level thing like a tree, and then that's what guides are generalization, but remember we have these other kinds of inductive reasoning like properties biting through.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I or carrying bacteria which don't follow the form of similarity.",
                    "label": 0
                },
                {
                    "sent": "So how can we capture this?",
                    "label": 0
                },
                {
                    "sent": "Well, what what Charles did in this very nice paper is to give within the same general approach of describing abstract knowledge as some kind of a relational structure, some kind of a graph, and then some kind of stochastic process operating over that graph.",
                    "label": 0
                },
                {
                    "sent": "Ways of specifying priors for all these different forms of inductive reasoning.",
                    "label": 0
                },
                {
                    "sent": "So, for example, to model this these dimensional predicates, he gives a 1 dimensional chain structure and then some asymmetric drift process that operates over that graph.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that that does a very, very good job of modeling a number of different kinds of domains where they all have the same form.",
                    "label": 0
                },
                {
                    "sent": "There's some underlying dimension that people are able to infer, and then there's some ordering along that dimension, some kind of dominance ordering that guides property.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duction and he's compared that with, you know, the same kind of 1 dimensional model, but with this more diffusion or smoothness process that doesn't do as well.",
                    "label": 0
                },
                {
                    "sent": "Or a tree model, and so on.",
                    "label": 0
                },
                {
                    "sent": "Or to take another example for doing this kind of food web reasoning.",
                    "label": 0
                },
                {
                    "sent": "This is where that Charles did together with Patch.",
                    "label": 0
                },
                {
                    "sent": "After Patch is really the lead lead on this project.",
                    "label": 0
                },
                {
                    "sent": "We defined a Model 2, two kinds of models.",
                    "label": 0
                },
                {
                    "sent": "There's that restructured model for taxonomic properties, and then over here there's this kind of directed.",
                    "label": 0
                },
                {
                    "sent": "It's basically a Bayes net.",
                    "label": 0
                },
                {
                    "sent": "It follows a noisy transmission process where if any prey species has some probability of transmitting any any disease property to a predator that eats it, and if a predator eats multiple things and it has a noisy or pooling.",
                    "label": 0
                },
                {
                    "sent": "So it's very much the same kind of model that Tom was talking about in causal reasoning, and we showed again in a double disc.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sociation, where the food web does a good job of predicting peoples inferences about diseases where the tree structure model does a good job of predicting these more biological properties, but not the other way around.",
                    "label": 0
                },
                {
                    "sent": "So the food web model does not do a good job.",
                    "label": 0
                },
                {
                    "sent": "Does a very poor job of capturing biological properties, an likewise for that restructuring disease properties.",
                    "label": 0
                },
                {
                    "sent": "So it suggests that people can flexibly use different representations of the same domain to reason about different properties.",
                    "label": 0
                },
                {
                    "sent": "So to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we got to hear it.",
                    "label": 0
                },
                {
                    "sent": "This is this is most of the way through the talk, but I want to say a little about the deep questions that remain.",
                    "label": 0
                },
                {
                    "sent": "So what we've shown is general Toolkit for thinking about inductive reasoning and using it to diagnose the underlying knowledge representations, right?",
                    "label": 0
                },
                {
                    "sent": "We've shown that different kinds of priors are appropriate for different domains or different views of a domain, and that there seems to be some kind of a match between what's in peoples heads at least for these domains that we've studied.",
                    "label": 0
                },
                {
                    "sent": "We don't claim that people are well calibrated for all domains.",
                    "label": 0
                },
                {
                    "sent": "You might ask him to reason about, but for certain things that are important in the real world, like natural categories and their properties.",
                    "label": 0
                },
                {
                    "sent": "It seems like we have evidence that people people are using something like the right models and using them in something like the right way to guide Bayesian inference, and we've given a fairly general framework for describing these different kinds of background knowledge or different kinds of intuitive theories in terms of something like a relational structure over the objects, and then some kind of stochastic process over that over that graph providing the prior and then we account for the differences in these domains by the different forms of structure or the different kinds of stochastic processes.",
                    "label": 0
                },
                {
                    "sent": "But one of the big questions that remain well, how does a learner figure out what's the appropriate form of structure?",
                    "label": 0
                },
                {
                    "sent": "And what about these cases where it seems like multiple models or multiple structures might capture different aspects of the main?",
                    "label": 0
                },
                {
                    "sent": "How do you figure that out?",
                    "label": 0
                },
                {
                    "sent": "In a sense, these are versions of the question that Tom was talking about this this year.",
                    "label": 0
                },
                {
                    "sent": "How many different models to learn is sort of a version of the question of how much structure, but now take it to another level, and this is a question of what kind of structure, which is in a sense a generalization of the problem of how much structure and we're going to try to use the same kinds of tools that Tom was talking about.",
                    "label": 0
                },
                {
                    "sent": "Least broadly, but over.",
                    "label": 0
                },
                {
                    "sent": "But now over these more structured kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representations, so in the hierarchical base sense, we're going to be asking here say if we consider different forms of structure for the same observations, how can we figure out whether we should use a tree, a space, or a chain?",
                    "label": 0
                },
                {
                    "sent": "That's one particular, or you can think of this as extending structure learning from the problem of finding a structure of a particular form to making inferences about the form of the structure now.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem of learning the formal structure domain is a very important one, although it hasn't been studied until recently.",
                    "label": 0
                },
                {
                    "sent": "Too much in either machine learning or in human learning, but it's, but it comes up at crucial moments in scientific discovery or cognitive development.",
                    "label": 0
                },
                {
                    "sent": "Again, these are in some sense the most interesting cases of deep learning, like really deep learning.",
                    "label": 0
                },
                {
                    "sent": "So take scientists for example.",
                    "label": 0
                },
                {
                    "sent": "The idea that we should organize biological species in a tree is not something that all cultures have always had in their scientific knowledge in Western Europe.",
                    "label": 0
                },
                {
                    "sent": "Trace it back to the work of Linnaeus and other naturalists.",
                    "label": 0
                },
                {
                    "sent": "You know, in this sort of dawning of the scientific age.",
                    "label": 0
                },
                {
                    "sent": "But prior to that, say, going back to even the early modern period of the Middle Ages, the dominant metaphor for the structure of the natural world was more of a chain, what's called the great chain of being, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a linear representation with rocks at one end, and God at the other end, and all the biological species.",
                    "label": 0
                },
                {
                    "sent": "Sort of a radon levels of complexity from plants up through animals, and humans and Angels, of course.",
                    "label": 0
                },
                {
                    "sent": "And then God at the at the other end.",
                    "label": 0
                },
                {
                    "sent": "But then what happens when naturally start to go out and take data seriously?",
                    "label": 0
                },
                {
                    "sent": "In particular, Linnaeus was really interested not just in animals, but in plants, and he had the idea that the same general principles should describe how all these animals are related to each other and plants, and he came up with this seven level hierarchy, which then went through a number of different revisions, right?",
                    "label": 0
                },
                {
                    "sent": "Darwin's notion of trees?",
                    "label": 0
                },
                {
                    "sent": "This is a sketch from an early notebook of Darwin's has something in common with Linnaeus.",
                    "label": 0
                },
                {
                    "sent": "It has a hierarchical structure with the.",
                    "label": 0
                },
                {
                    "sent": "The ultimate species at the leaf nodes, but unlike Linnaeus, is perfect seven level thing every.",
                    "label": 0
                },
                {
                    "sent": "You know, a place for every species and everything in its place.",
                    "label": 0
                },
                {
                    "sent": "Darwin Darwin had this much more sort of random structure which reflects in the intuitions about these castec nature of evolution.",
                    "label": 0
                },
                {
                    "sent": "What's interesting here in particular, though from our point of view, is that this is from an early notebook of Darwin's prior to working out the theory of natural selection.",
                    "label": 0
                },
                {
                    "sent": "This is he was just playing around with some ideas about the structure of natural species, and it reflects the fact that these moments of structural discovery we're talking about are not necessarily deeply informed by a causal theory, right?",
                    "label": 0
                },
                {
                    "sent": "Darwin didn't have the causal.",
                    "label": 0
                },
                {
                    "sent": "I think that gave rise of the tree.",
                    "label": 0
                },
                {
                    "sent": "He was just making inferences from the data he can see and some some vague intuitions about processes that could give rise to this about the likely structure.",
                    "label": 0
                },
                {
                    "sent": "You have a similar situation with Mendeleev, who say discovered the periodic structure of the chemical elements.",
                    "label": 0
                },
                {
                    "sent": "He didn't know quantum mechanics, right?",
                    "label": 0
                },
                {
                    "sent": "He didn't know the underlying causal mechanisms that give rise to this periodic table, but he could go out and look at the properties of his set of objects, elements, how they interact with each other, and just it was just a pure but abstract pattern discovery, right?",
                    "label": 0
                },
                {
                    "sent": "He just said this is seems to be.",
                    "label": 0
                },
                {
                    "sent": "If this if I organize the.",
                    "label": 0
                },
                {
                    "sent": "Objects in this way I can explain the data that I observe.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "That's that's a deep challenge to us.",
                    "label": 0
                },
                {
                    "sent": "An children go through similar kinds of discoveries of abstract structural form in their in their development.",
                    "label": 0
                },
                {
                    "sent": "So just to give one example that connects with what we've been talking about, the idea of hierarchical structure of categories.",
                    "label": 0
                },
                {
                    "sent": "So it is true that reasonably young children like, say, 5 year olds, will understand that objects can be organized into something like a tree structure hierarchy, and that they can learn words for different nodes in that tree at different levels.",
                    "label": 0
                },
                {
                    "sent": "Like you have a dog, it's also a mammal.",
                    "label": 0
                },
                {
                    "sent": "It's also an animal that's also a living thing, and so on.",
                    "label": 0
                },
                {
                    "sent": "But at the very earliest stages when children are learning words, they have something they go through what's called a mutual exclusivity stage where they just actually partition objects into flat clusters.",
                    "label": 0
                },
                {
                    "sent": "It's like every there's everything is just in one kind of cluster, so it's more of this traditional clustering idea, and then only later do they realize actually no, we need something richer, structure, something more like a tree structure.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit more tomorrow in the lecture on cognitive development about that and how this kind of model might give some insight into that.",
                    "label": 0
                },
                {
                    "sent": "But for now I just want to tackle the abstract problem without talking about, you know.",
                    "label": 0
                },
                {
                    "sent": "Evidence that children actually do anything like this just because from a machine learning point of view, or just generally a computational point of view, this isn't a problem that our traditional approaches really give much insight into, and it's a way we can try to actually advance the state of the art machine learning.",
                    "label": 0
                },
                {
                    "sent": "We have all these techniques, whether it's PCA or multidimensional scaling, hierarchical clustering, clustering, K means clustering, Chinese restaurant process mixture models which are always of finding structure in data.",
                    "label": 0
                },
                {
                    "sent": "But they all find structure of a particular form and we don't have general ways of sort of specifying large classes of different kinds of structures and figuring out among them what's the right.",
                    "label": 0
                },
                {
                    "sent": "Formal structure.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a domain.",
                    "label": 0
                },
                {
                    "sent": "So to put this goal kind of modestly.",
                    "label": 0
                },
                {
                    "sent": "Not really, our ultimate goal is something like a universal framework for unsupervised learning, where instead of having this kind of bag of heuristic algorithms or even a bag of principle generative models with a human user going in and trying to select which is the right one to use an also humans being responsible for generating new models, we'd like to have some kind of universal learner who has some general notion of structure which includes all of these and can be combined in other ways to make new forms of structure and be able to learn from the data.",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What is the right form of structure and construct repres?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stations of the right form.",
                    "label": 0
                },
                {
                    "sent": "So here's how Charles figured out how to do this, and I don't want to say that this is he solved this problem.",
                    "label": 0
                },
                {
                    "sent": "It's a really hard problem, and this is just this is just one first attempt, which I hope will just inspire a lot more.",
                    "label": 0
                },
                {
                    "sent": "Other approaches to do this better and more generally, but the idea is to capture all these different forms of structure that we have in our standard toolkit of unsupervised learning with graphs and then to describe the different classes of structures, different forms with simple grammars or rules for generating graphs.",
                    "label": 0
                },
                {
                    "sent": "And we'll see a lot more about grammars tomorrow when we talk about language, but.",
                    "label": 0
                },
                {
                    "sent": "Basically, these grammars are rules for taking a node abit of structure and node with it's in and out links and rewriting it, expanding it out into a little bit more structure.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's a here's.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grammar which which says take a note and it's in and out links and replace it like that with two nodes connected directly with a direct connection where the one on the at the start.",
                    "label": 0
                },
                {
                    "sent": "The parent has the in links and the child has the out links and you can see that if you apply this rule successively to one node of a structure just starting out from from just a single node.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It grows out a chain and now.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matter how you apply this rule, it always grows out of chain.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, there are rules which we can apply that will grow out flat clusters or these full orders dominance structures.",
                    "label": 0
                },
                {
                    "sent": "You can apply these things to know that starts off connected to itself to get a ring.",
                    "label": 0
                },
                {
                    "sent": "We can get hierarchies, including ones with the objects at any node of the hierarchy, or just at the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "And by taking these very simple rules and combining the structures they generate just by taking cross products, we can get more interesting kinds of structures like across of 2 Chainz gives us this 2 dimensional grid, so that that allows us to go in the direction of multidimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "We can even get topologically interesting structures like you cross a chain in the ring and you get this cylinder here, so there's some combinatorial power to this.",
                    "label": 0
                },
                {
                    "sent": "This sort of universal.",
                    "label": 0
                },
                {
                    "sent": "Grammar if you like.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or structures.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea is now.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To use that framework to describe this hypothesis, space of possible form.",
                    "label": 0
                },
                {
                    "sent": "So the forms of structure now are described with these these rules, each of which generates a hypothesis space of particular relational structures, each of which specifies a class of graphs that can be generated by that rule and with some extra math which I don't have time to tell you bout actually puts a prior.",
                    "label": 0
                },
                {
                    "sent": "It's A kind of Occam's razor like simplicity prior, and it uses a similar ideas to the so called Bayesian Occam's Razor, which we've seen a little bit about.",
                    "label": 0
                },
                {
                    "sent": "But it's really a whole topic in itself, but that provides basically the hypothesis basin priors for these graph structures.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to be doing posterior inference about both the form and the structure from observed data looking for the joint setting of the structure and the former.",
                    "label": 0
                },
                {
                    "sent": "The best form of structure and the best structure of that form, which together explain the data will the bottom level of this is just the same smoothness model that I showed you before, and we're just putting together the smoothness and this sort of simplicity constraint hypothesis space to do discovery of form as a kind of grammar induction.",
                    "label": 0
                },
                {
                    "sent": "And as you like, yes, and we're just going to happen again.",
                    "label": 0
                },
                {
                    "sent": "It's taking this the general idea of nonparametric probabilistic models, which was developed originally in statistics and machine learning, for asking these questions of how much structure exists and growing this out, so to speak, to talk about more general questions of what kind of structure exists.",
                    "label": 0
                },
                {
                    "sent": "But that includes, if you like things like, well, how much structure, but of a more abstract form like is it one dimension or two dimensions?",
                    "label": 0
                },
                {
                    "sent": "That's just a choice of grammar here, or how many levels in a hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just show you a couple of examples of this of this structural form learning system.",
                    "label": 0
                },
                {
                    "sent": "So given a set of animals and features, we find this tree structure and this is just yet another tree.",
                    "label": 0
                },
                {
                    "sent": "It's a little more interesting than before, 'cause it's not just mammals, it includes insects and fish and birds and so on.",
                    "label": 0
                },
                {
                    "sent": "But the key new insight here is that it figures out which form of structure it finds a tree, as opposed to any other kind of structure.",
                    "label": 0
                },
                {
                    "sent": "But say given a different data set where now if you like the objects aren't biological species, but judges on the US Supreme Court and the features instead of being, you know whether.",
                    "label": 0
                },
                {
                    "sent": "Rehnquist has feet or choose hey or whatever is the features are how they voted on different cases.",
                    "label": 0
                },
                {
                    "sent": "So it's a kind of a political data set.",
                    "label": 0
                },
                {
                    "sent": "Now what the algorithm discovers is a different form of structure.",
                    "label": 0
                },
                {
                    "sent": "It's a 1 dimensional chain with the Liberals on the left, conveniently enough and the Conservatives on the right.",
                    "label": 0
                },
                {
                    "sent": "The left right, of course, is arbitrary.",
                    "label": 0
                },
                {
                    "sent": "But the idea that we talk about political views as a spectrum from left to right, that turns out not to be arbitrary, that actually seems to be the best form of structure to describe this domain.",
                    "label": 0
                },
                {
                    "sent": "For better or worse, that's just the way it is, and our minds seem to have collectively, in cultural sense, locked.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that just a couple of other illustrations to show something of the generality.",
                    "label": 0
                },
                {
                    "sent": "So here's a set of faces where we generated these from a face synthesizer so they're not real Germans or anybody else.",
                    "label": 0
                },
                {
                    "sent": "They are synthesized, but it's, but we did this because we wanted to generate a realistic set of data, which we knew to be 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "And see, could our algorithm discover that so there's two knobs that are being tweaked here.",
                    "label": 0
                },
                {
                    "sent": "There's a black white racial nob.",
                    "label": 0
                },
                {
                    "sent": "And then there's a kind of a masculinity nob you could clearly see the black white one.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you can tell that these are the more masculine ones, and these are the.",
                    "label": 0
                },
                {
                    "sent": "More gender neutral ones.",
                    "label": 0
                },
                {
                    "sent": "They're all pretty male.",
                    "label": 0
                },
                {
                    "sent": "It might help if we call these ones the tennis team and these guys the football team.",
                    "label": 0
                },
                {
                    "sent": "Then you can probably see it and the algorithm discovered.",
                    "label": 0
                },
                {
                    "sent": "It finds those dimensions, but even more importantly it right.",
                    "label": 0
                },
                {
                    "sent": "It finds that kind of two dimensional structure is the right one.",
                    "label": 0
                },
                {
                    "sent": "Or here's just maybe the.",
                    "label": 0
                },
                {
                    "sent": "Particularly example because it's not something that people have previously a kind of structure people have previously looked for, at least to my knowledge and unsupervised learning, which is this cylindrical structure.",
                    "label": 0
                },
                {
                    "sent": "So here the model takes distances between cities and find that the best form of structure to describe that is this cross between a chain or a cross product of a chain and a ring, which basically corresponds to latitude in lanja tude.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, the last thing I was going to talk about, which I won't talk about 'cause we want to move on, but in case some of you are interested there, there's one more basic question that is that still an issue.",
                    "label": 0
                },
                {
                    "sent": "Well, there's many, many, but the one other one I wanna talk about with this topic of how given that real knowledge representations have often different models for different aspects of a domain, how could we figure that out?",
                    "label": 0
                },
                {
                    "sent": "And we this is again work of Pat Shadows in Charles Kempson.",
                    "label": 0
                },
                {
                    "sent": "Also, the Koshman Singa have developed some interesting kinds of nonparametric Bayesian techniques in particular.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something called cross Cap where we've.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you like a Chinese restaurant process as well as a prior partition on the different features in the domain which is able to figure out how, for example, there are different features which can be described by what's a taxonomic structure.",
                    "label": 0
                },
                {
                    "sent": "So this is a taxonomic clustering of animals and other features, like whether an animal is ferocious or lives in water or eats fish which can be described more by this ecological clustering in terms of, say, animals living in air or water, or whether their predator prey and if it figures this out both both how many different forms of structure there are, that's the.",
                    "label": 0
                },
                {
                    "sent": "Organization this way and also with the Chinese restaurant process going over the rows nested inside each of these partitions.",
                    "label": 0
                },
                {
                    "sent": "How many categories there are in each of these different ways of viewing the domain?",
                    "label": 0
                },
                {
                    "sent": "So that's pretty cool, and it's another way that I would say we were closing the loop from human and machine learning.",
                    "label": 0
                },
                {
                    "sent": "So just to wrap up then.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use this as another case study of our general approach to understanding these problems of induction in cognition an our toolkit, which includes the idea of Bayesian inference over generative models.",
                    "label": 0
                },
                {
                    "sent": "The idea of models defined over representations of various forms of structure, and in particular to capture some of these more high level kind of reasoning.",
                    "label": 1
                },
                {
                    "sent": "We've had to put in more explicit kinds of symbolic structure, but we can say that doesn't stop us from using these other valuable ideas coming more from Bayesian statistics in conventional machine learning, the idea of hierarchical Bayesian nonparametric approaches.",
                    "label": 0
                },
                {
                    "sent": "To try to explain basically where these abstract forms of knowledge come from and to try to understand what form of knowledge exists in the domain and it's.",
                    "label": 0
                },
                {
                    "sent": "It's a nice example both both in in what I talked about here and also going back to some of the techniques that Tom talked about at the end, like his feature discovery work based on the ICP and finding the words and so on where.",
                    "label": 0
                },
                {
                    "sent": "Lost cognitive scientist sort of machine learning oriented cognitive scientists have been able to make progress in connecting these fields and really advancing the state of the art in both which we can show.",
                    "label": 0
                },
                {
                    "sent": "We can demonstrate in a concrete way by both good quantitative fits to human data and also new and exciting.",
                    "label": 0
                },
                {
                    "sent": "At least we're excited.",
                    "label": 0
                },
                {
                    "sent": "Algorithms that take machine learning in new and more human like ways.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                }
            ]
        }
    }
}