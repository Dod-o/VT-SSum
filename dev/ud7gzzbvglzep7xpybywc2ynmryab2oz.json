{
    "id": "ud7gzzbvglzep7xpybywc2ynmryab2oz",
    "title": "Gaussian Processes",
    "info": {
        "author": [
            "Carl Edward Rasmussen, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_rasmussen_gp/",
    "segmentation": [
        [
            "OK, so welcome back.",
            "I was originally scheduled to talk about Gaussian process, but as you can see I've changed my mind in the last minute.",
            "About what to talk about.",
            "So in the previous talks, we've all we've all had talks about.",
            "You know big areas of machine learning, and this talk is going to be a little bit different.",
            "It's going to be about 1 specific, one specific method, and there's a certain number of reasons why I would like to speak about this.",
            "This particular method, so one of the reasons is that that Gaussian distributions you can do amazing things with Gaussian distributions, right?",
            "And sometimes I think these are a little bit underappreciated.",
            "So in this talk there's going to be.",
            "Maybe no loopy messages.",
            "There's going to be no negative variances.",
            "There's going to be no nested plates or anything like that.",
            "There's just going to be a Gaussian distribution, right?",
            "And we'll see that there's a lot of mileage in a Gaussian distribution.",
            "So the second reason that I like to talk about this this model is that it's it's a nice model from if you're a proponent of the Bayesian way of doing things because you can actually compute everything and you can inside these models, although they are not trivial model, you can compute everything and it gives you a very nice window on what's actually going on in that methodology.",
            "And the third reason is that Gaussian process turn out actually a very practical tool for solving real problems.",
            "OK, so that's sort 3.",
            "Three justification, so I've talked to a lot of people over the last couple of days, and some of them have said, oh, you know, we've been.",
            "We've been trying your software.",
            "I've been looking at Gaussian process, so today things are going to be just the basics I'm going.",
            "I'm going to just talk about, you know what is a Gaussian process and and and how is it that we can use it in this in this amazing way.",
            "So so the idea in the talk here is that you should be going away in an hour and a half from now.",
            "Saying OK Gaussian Gaussian distributions are amazing things.",
            "They're cool, right?",
            "If you don't say that then then somehow having succeeded right?",
            "OK, so so let's see how that goes.",
            "So today I'm only talking about the basics right?",
            "So, but on the other hand, it's supposed to be sort of.",
            "What I'm saying is supposed to be fairly coherent, right?",
            "So if you have problems with what I'm saying then please please tell me so.",
            "The next talk I'm going to give, I'll probably start ramping a bit more there, but today it's supposed to be understandable what I'm saying, right?",
            "So do object if it's not so.",
            "So Gaussian process models are, so I've spent a number of years talking to people about them, and I've noticed that they are kind of hard to understand and actually I myself had a hard time understanding them when I first started looking at them, so I probably took about so two years to actually really figure out what was going on.",
            "So at the time I only had myself to ask and I didn't actually know the answer, so that make progress slow, right?",
            "But you are in a nice situation that you can ask me, right?",
            "So please do that.",
            "Right, OK, so there are a number of subtleties.",
            "I'm not sure exactly why it's difficult to understand, probably part of the reason is that I'm not explaining it very well, but part of the reason is also just that the concepts are being used in slightly unusual ways, right?",
            "So it takes a little bit of time to get around the idea, but please tell me when something is.",
            "Is unclear if you don't tell me then I can't do anything about it, right?",
            "So if you do tell me, let's see.",
            "OK, so I've actually written a book about Gaussian process with Chris Williams.",
            "It looks like this.",
            "Unfortunately I haven't brought a bookstore.",
            "So I guess my services Department should be told about this problem.",
            "But if you don't like books, it also exists for free download on the web, so you can just get it from there.",
            "Don't download it right now.",
            "And the stuff that I'm going to talk about is all in the book, and there's some code also that goes through the book, and so I'll be using that if not today, then in the next lecture and showing you how how you can use this.",
            "Alright, so let's get started so.",
            "What do we?",
            "What is the regression problem where the regression problem is a problem that you've got a bunch of data?",
            "So here I've got the measured concentration CO2."
        ],
        [
            "Centration atmospheric CO2 concentration measured over something like 40 years monthly measurement and you can see that things are rising here and there might be an interesting problem to try and predict.",
            "You know what's going to happen in the future, right?",
            "And our regression problem is to say, well, given this data, could I predict what's going to happen in the future and it's sort of clear that you should be able to predict something about what's happening in the future, right?",
            "It's also clear that I couldn't ask well what's it going to look like in 2050, for example, or or 2100, right?",
            "The data here doesn't really tell me very much about that, but it does tell me something about what's happening, you know?",
            "At least for short, for short intervals here.",
            "So this is this data up to 2004.",
            "So that's the regression problem.",
            "So now the title was something about solving regression problems.",
            "So before we can before we can really talk about that will have to agree on what a solution to a regression problem is.",
            "So here's an idea.",
            "Here's a solution to arresting problem well, so this is just a linear fit to the data.",
            "So now the question is, is this a good?",
            "Solution.",
            "I don't know whether you have opinions about this.",
            "It should be fairly obvious that this is bad for a number of reasons, so one of the reasons is that it is actually not fitting the data very well.",
            "OK, this is saying that the concentration looks higher over here than it does over here, right?",
            "So it is capturing something of the variability in the data, but there's clearly things in the data that is not capturing very well, right?",
            "So it's not fitting the data very well, so you might object."
        ],
        [
            "Two to the model on that grounds, which I would do too.",
            "OK, so let's get a little bit more serious.",
            "So now I've fitted a data model which is.",
            "It's a quadratic plus, a little sinusoidal term, right?",
            "OK, now things look a little bit better."
        ],
        [
            "Right, we are actually fitting a lot of the lot of variability, not everything, so there's things going on here that we're not capturing.",
            "Things that we're doing here that we're not capturing.",
            "But maybe this is a much more reasonable model.",
            "But there's actually a problem.",
            "There's a deep problem with this model.",
            "Autumn is illustrated nicely.",
            "If I instead of having a quadratic plus a sinusoidal, have a cubic plus sinusoidal.",
            "Then things look like this.",
            "OK, so things look at so maybe we shouldn't be too worried about what's happening, what's going to happen in the future, right?",
            "But how do I really know whether I should be using one model or the other, right?",
            "The problem is here that my assumptions are sort of a little bit a little bit on unspecified at the moment, and also the model just predicts a number, right?",
            "That's not very useful.",
            "A good guy."
        ],
        [
            "Models should tell us something about what the values are, but it should also tell us something about what the uncertainties are, right?",
            "So that's why what I mean by solving a nonlinear regression problems.",
            "OK, we should get some reasonable predictions, but we should also get information about what the uncertainties are.",
            "OK, so I'm going to use this example a little bit throughout the talk, not because I think this is a deeply serious way of actually modeling this data set, right?",
            "So if you are really going to model this data set, then presumably you would want to know about other things that are going on at the same time.",
            "For example, if you knew about, you know how the economy were doing, how was the world population doing?",
            "How many people had downloaded David Mackay's book and important things like that might have an impact on what's going on up here, right?",
            "So obviously you would use those those inputs as well, but unfortunately, if I did that, then I have a hard time actually plotting the function for you.",
            "OK, so that's really the only reason why I'm looking at this slightly stylized problem here.",
            "OK, but I'm still trying to model the real data here.",
            "Right, but the input here is just time, right?",
            "So it's kind of a weird weird model from that perspective.",
            "Alright, so how do we solve?",
            "How do we solve problems like that?",
            "Well, so one idea is to say, well, we can use a parametric model and we can fit that in various ways.",
            "So here just to introduce the notation I'm going to mention one of one of the ways you could do that."
        ],
        [
            "So if you have a data set here as a collection of inputs and outputs, I call them X&Y and you have a model which is somehow parameterized function which is parameterized by by some weights, right?",
            "It could be a linear function, or it could be you know any function you care to dream up or you think of as a good idea in this.",
            "And in this situation, and you have some noise here, which could have some distribution.",
            "Let's just assume that it's a Gaussian.",
            "If it's Gaussian, then you get your likelihood function.",
            "They likely functioned as the probability of the data given the parameters.",
            "So in this case we're actually only modeling the outputs, so the probability of the data is just the probability outputs given the input.",
            "We're not making a model explicitly of the inputs.",
            "So the likelihood function could look like this is the Gaussian, so it just looks at E to the minus half of the distance squared between the predicted output.",
            "So this is a model of the outcomes of the model and the actual observations divided by a noise term here.",
            "And one of the standard ways of going about fitting these things is to try and maximize the likelihood so you do the argmax or the likelihood function with respect to the parameters, and that is the maximum likelihood parameters.",
            "And now when you come to make predictions, you stick those maximum likelihood weights back into your likelihood model, and that will give you a prediction here.",
            "So I call a new a test input, the place where we want to make predictions.",
            "I call that X star, and we can compute the probability the probability distribution over why star, what the CO2 level would be.",
            "That new time extar given by conditioning on those maximum likelihood weights right there.",
            "Other ways of doing this as well?",
            "Let's not get hooked up on the details.",
            "OK, so you can also do other kinds of inference, so in particular Bayesian inference.",
            "In that case things looks a little bit different.",
            "You have the same setup here.",
            "You have the same likelihood function, but now you also have a prior prior on the parameters."
        ],
        [
            "And when you do inference, what you're supposed to do is you multiply the likelihood in the prior and renormalize to get the posterior distribution over the weights and the posterior is your friend.",
            "And once you've got the posterior you can compute the things that you want to compute, so in particular you want to compute two types of things you want to compute prediction."
        ],
        [
            "So you want to know well what does the function do in other places?",
            "In places where we didn't measure it, so to do predictions.",
            "Here again, we use the likelihood function.",
            "We take.",
            "What's the likelihood function say about those new predictions given particular weights, and then we average over the posterior distribution over weights, right?",
            "So if you have a high posterior somewhere then you get a get a lot, give a large weight to those predictions, and if you have a lower posterior than those predicted predictions coming out of that particular setting of the model won't influence the actual.",
            "Predictions camera so we've seen this before and another thing you might be able to might be interested in computing is the is the marginal likelihood margin, like it was important for model selection.",
            "I'll talk much more about that later on, and again, the marginal likelihood is a is a.",
            "Is a.",
            "You get that by marginalizing over the distribution over the weights right?",
            "So you can see the interesting things that you want to compute actually computed by doing integrals over the over the weights.",
            "So, so This is why this is always happens in parametric models.",
            "You can sort of see that.",
            "Something a little bit funny about that, right?",
            "We're not really interested in the weights like the weights are sometimes called nuisance parameters, right in these kinds of models, you're usually uninterested in the weights.",
            "You just want to know what are the predictions, right?",
            "So sometimes if you have a parametric model, you might actually be interested in what is the value of one of the parameters, but I'm not really talking about that case, right?",
            "I'm talking about, you know, complex nonlinear relationships, and you want to know what is the prediction, right?",
            "You're not interested in the weight, so the way Gaussian process models works is it explicitly says, well, the prior here.",
            "Notice that the posterior here is nothing but the likelihood times the prior or is proportional to the likelihood times the prior, right?",
            "So somehow the prior.",
            "Is here a prior on weights right?",
            "But the way it works is it induces a prior on the functions right?",
            "If you set the weights to something particular then you'll get a particular function, right?",
            "So if you have a distribution over those weights, then you'll get a distribution over functions.",
            "So what we're going to do differently here is we're going to instead of going wild by the weights which were not interested in, we're going to go directly for a distribution over the things that we're interested in, right over the predictions.",
            "OK, and in fact, this is going to turn out turn out to be much simpler than going by this two step procedure of introducing model weights and then getting rid of them again.",
            "So you might think that the way to actually actually get good results here is to look at complicated functions to dream up complicated nonlinear functions about of the weights.",
            "Here to be able to do very advanced things and dream up complicated priors on the weights and try to somehow do these integral."
        ],
        [
            "And what I'm going to try and persuade you today is that that's not the way to do it, right?",
            "The way to do it is different, and it turns out that it just involves a Gaussian distribution.",
            "So before I can tell you about that, let's make sure we're talking about the same Gaussians.",
            "So a Gaussian distribution has a location parameter.",
            "I mean, and it has a stack."
        ],
        [
            "Deviation or variance.",
            "Two samples here 02 Gaussian distributions in one dimension here.",
            "And over here I've drawn a picture of a Gaussian distribution, 2 dimensions in particular correlated strongly anticorrelated.",
            "So what I've drawn here, I have the two variables on these access here and this is equal probability contours, so there's a high probability inside this region and lower probability as you move out, and the probability goes down very rapidly in this direction, but it goes down much more slowly in this direction.",
            "So what I'm going to use in the talk here, I'm going to use Gaussian distributions that live in high dimensional spaces.",
            "OK, unfortunately I can only draw a 2 dimensional Gaussian right, but we have to try and think of what do these distributions look like in much much higher dimensions, right?",
            "And that's going to be part of the challenge.",
            "OK, so I can't draw those things.",
            "So what can you do with Gaussian distribution as well?",
            "You can you can can dish."
        ],
        [
            "And a Gaussian distribution.",
            "So a conditional distribution means that if I now know somehow that the value of this variable has this particular value of this variable.",
            "Sorry, has this particular value, then what does that tell me about the other variable, right?",
            "And of course, if the Gaussian is correlated like this, then it tells me quite a lot in particular tells me if I have this value value.",
            "Here it is very unlikely that my other variable would have have values over here.",
            "In fact, the conditional distribution of the other variable.",
            "Given that this variable is set to, this value is again Gaussian and it's centered over here.",
            "OK, so that's conditioning.",
            "Another thing we can do is marginalization, so marginalization just means summing out or integrating out some of the of the variables and looking at just the district.",
            "The marginal distribution of the other variables right in here again, in this 2 dimensional example here I just marginalized this joint Gaussian by summing in this direction and what I get back is again a Gaussian distribution, right?",
            "And this turns out to be crucial that we can do both of these conditioning and Gaussian in marginalization.",
            "And the results of those operations are again Gaussians.",
            "I've also given you the equations here, so if I have if I have distributed joint Gaussian distribution over X&Y here with mean mean vector which contains A&B and a covariance which can be schematically written."
        ],
        [
            "Like this, then I've given you here the equations for the marginal distribution of one of the variables or the conditional distribution of one of the variables given the other variables, right?",
            "But let's not look too much at the algebra right, but the slide is in there.",
            "If you want to look at it later on.",
            "OK, so now.",
            "What about what?"
        ],
        [
            "The Gaussian processes.",
            "So what is the Gaussian process?",
            "I've only been talking about Gaussian distribution so far, so Gaussian process is just a generalization of a Gaussian distribution to infinitely many many variables, and the reason why we want to do that is we want to do inference about functions right?",
            "So we have to be able to put probability distributions or functions.",
            "Now there's sort of an informal analogy here we can think of an infinitely long vector.",
            "We can think of that as being a function, right?",
            "If I, let's think of a 1 dimensional function.",
            "If I want to specify what that one dimensional function is.",
            "I should just specify what is the function value for each input.",
            "OK, then I've specified what the function is.",
            "The problem there might be that there are infinitely many possible inputs to the function, right?",
            "So the vector will become very very long.",
            "OK, but let's not worry about that for the time being.",
            "Mathematically, at least, the function is sort of like an infinitely long vector, right?",
            "This is not mathematically very precise, but it turns out to be precise enough for what I'm going to do.",
            "I'm going to use it so we simply think of functions as being infinitely long vectors OK.",
            "So of course you might start getting an uncomfortable feeling now because, well, I can't write down these things in my computer.",
            "Of course, right?",
            "'cause they only have finite memory, so I better not try to do that right?",
            "And I won't try to do that.",
            "OK, so somehow will be able to do what we need to do without writing down those vectors.",
            "OK.",
            "So now what is the definition of a Gaussian process?",
            "So here's the definition of Gaussian processes.",
            "A collection of random variables, any finite number of which have Gaussian distributions.",
            "OK, so this is a sort of a mathematic mathematicians definition here.",
            "So mathematicians always very cautious when they talk about infinite things here, so they don't actually say it's an infinite collection of random variables that have joint Gaussian distributions, because maybe that doesn't exist, whatever that might mean, right?",
            "But just say any finite number of them should have Gaussian distributions, and then there.",
            "Then there OK, right?",
            "So there there.",
            "They're always a bit fidgety when it comes to infinities, right?",
            "Yes.",
            "About comparable I don't think it matters.",
            "I think we don't need to be.",
            "I'm not going to look into the mathematical details.",
            "I'm trying to get give you a high level intuition here, right?",
            "I think it's uncountably infinite is probably the right right answer.",
            "OK, so a Gaussian distributed Gaussian process is this somewhat mysterious object, right?",
            "Just think of it as being a very large Gaussian.",
            "OK, that analogy would be good enough.",
            "OK, so let's see what happens.",
            "So whereas the Gaussian distribution is fully specified by a mean vector and covariance matrix, so it's like this.",
            "So you have a Gaussian, a joint Gaussian distribution over over D dimensional object.",
            "Or like this we have a mean vector and covariance matrix right in the mean vector will have the same length as the number of dimensions of F and the covariance matrix will be a matrix which has size D by D in that dimensional space.",
            "So what happens if we try to lift this up to the infinite dimensional thing?",
            "OK, so now what does the vector?",
            "What does it mean vector turn into?",
            "If you want to try to follow this analogy?",
            "Anyone?",
            "It turns into a function, right?",
            "So you need an infinitely long mean vector and we think of infinitely long vectors as being functions, right?",
            "So the mean of a Gaussian process is a mean function.",
            "And how about the covariance matrix?",
            "That's right, it's a function is going to be a function of two variables, right?",
            "So just like you normally think of this of covariance matrices as being indexed by the dimension Now, this is just doing exactly the same thing, But the index set here is are the values of X, which happens to be the inputs to the regression thing that we're trying to do.",
            "OK, so so so maybe I won't try to prove this that this thing actually really exists, but if it exists it should definitely have this kind of form right?",
            "It should be, you know, a very long mean component here in this 2 dimensional.",
            "A function of two arguments.",
            "OK, so now.",
            "Let's worry a little bit about this thing about it being infinitely big, so we can't write it down right?",
            "So so we might.",
            "This might be totally impractical, right?",
            "But it turns out that it's not totally impractical and the reason for that is because of the marginalization property, so."
        ],
        [
            "Number of that.",
            "If you have a joint Gaussian, then you find the marginal distribution for some of the variables by just integrating out the ones that you're not interested in.",
            "In particular, if I had a joint joint distribution between X is and why is here and the had mean for Forex would be a in the covariance of X would be capital A in the cross covariance between X&Y is B here, so if you think of this as being let's think of this as being being used in the case where the mean vector is infinitely long, right?",
            "If I now happen to be interested in only a finite number of these entries, then I could just rearrange this thing to say, well, let's put the finite number of indexes up here and call them X, and then the infinitely many other indexes that I'm not interested in.",
            "I'll put them down and be here OK, and they say, well, what's the distribution over the X is?",
            "Well, the distribution over the X is just this good old fashioned Gaussian distribution.",
            "OK, and it only depends on things that are that are finite.",
            "In particular, if you had the entries here, then this would be 1 by D vector in R&D by D covariance matrix.",
            "OK, so this is essentially why it's possible to work with these things.",
            "Without using infinitely many infinitely infinite amounts of memory.",
            "OK, so let's look at an example of one of these things.",
            "What these things actually look like.",
            "So here's a here's a 1 dimensional Gaussian process.",
            "So X is a 1 dimensional variable here, so I choose.",
            "I have a Gaussian process distribution over these functions, so I have a problem."
        ],
        [
            "Traditional functions now, which is a Gaussian process here and have a mean function, and I'm just going to use the function zero.",
            "I'm going to use that throughout, but there's no reason why you should always do that.",
            "But that's the simplest thing you can choose.",
            "And here's a covariance function an for the covariance function.",
            "I'm going to use the.",
            "This function here, so it's the exponential of the distance minus half the squared distance between X&X prime.",
            "OK, So what this intuition which is going on here is to say, well, if X&X prime are very close to each other.",
            "Then the entry of the entry in the covariance is going to be is going to be large, but it's going to be close to 1 right?",
            "If X&X prime are very close to each other, then the squared distance is close to 0 in each of the minus.",
            "Something small is close to 1.",
            "And as the distance between the X is grow, then the.",
            "The covariances will fall, right?",
            "So now how do we get?",
            "How do we get a picture of what this kind of what this kind of object actually does?",
            "What does it look like?",
            "Well, the only thing we can actually do is we can say, well, what are the what does it do on a finite subset of our data, right?",
            "Because we can't actually work with functions themselves, so let's just say, well, let's look at what's the distribution of the function.",
            "Evaluate it at a set of fixed points here, right?",
            "So I could choose a set of 10 points.",
            "Then I could say, well, what does the distribution?",
            "Over the function look like in that case.",
            "So what do I need to do?",
            "Well, I need to marginalized out all the other stuff right, and we know that's very simple.",
            "We know that you just do that by retaining the part of the covariance that you're interested in, right?",
            "So the the joint distribution of that vector of variables will just have a mean, which is the mean evaluated at those points, which is zero in this case, and the covariance will be the covariance matrix by just evaluating the covariance function at all the pairs of X&X prime.",
            "OK, so the entries of the covariance matrix will just be the covariance function evaluated at those pairs of points.",
            "OK, so now I've looked at.",
            "I now look at it just a finite subset of the of the function values and I get back to just the Gaussian distribution.",
            "OK, so how can we visualize what that Gaussian distribution looks like?",
            "Well, one way of visualizing that is to actually draw samples from it and see what those samples look like.",
            "OK, so I'll do that.",
            "So what do I need to do?",
            "I need to choose some values of X. OK, so I write down a bunch of values of X and then I need to evaluate the covariance so the covariance is just the function of the X is right.",
            "So I just plug in, evaluate all the entries here by just plugging into this formula here and that will give me the covariance matrix.",
            "And then I draw a random sample from that distribution right?",
            "And that random sample will be a vector which will have the length which we will be the number of points that I chose, the number of X is here right?",
            "And I just plop those values Y as a function of those axes.",
            "Here's an example of doing that, right?",
            "So I chose 20 points randomly chosen along the input here, and then I then I wrote down the covariance matrix."
        ],
        [
            "Drew a sample from that joint Gaussian distribution, and that was a 20 dimensional vector.",
            "And then I just plot the values that those 20 values as a function of the inputs here, right?",
            "Functions be continuously sample from the distribution.",
            "Will they turn out to be continuous function?",
            "Yes, that's a very good question.",
            "My question is, are those functions continuous right?",
            "We're stepping a little bit ahead of ahead of ourselves.",
            "In fact, they would be continuous in this case.",
            "But let's look at the samples.",
            "We already see that.",
            "Well, it looks as though there might be a function underneath this, right?",
            "It looks it looks plausible that this really did define a distribution over functions, right?",
            "Because I can actually show you pictures of samples drawn from that.",
            "Drawn from that distribution now, if I had chosen, I could draw another sample and I would get different different random function from this distribution.",
            "And also I could choose a different covariance function.",
            "Which is something that I'll talk more about, so I chose the covariance function E to the minus half of distance squared here.",
            "Incidentally, so this looks a little bit Gaussian, right?",
            "But this is not the reason why it's called a Gaussian process."
        ],
        [
            "This is just the covariance function of a Gaussian process.",
            "It happens to have something that looks like a Gaussian form, right?",
            "I tend to call it squared exponential instead of Gaussian to point out that it's not a distribution, right?",
            "OK, it is, although it has this sort of Gaussian Gaussian shape.",
            "OK, so here was a here's a.",
            "Here's a sample of that from that distribution over functions, right?",
            "So hopefully this persuades you that there is.",
            "Distribution over functions that we've actually divide.",
            "Now one question is, you know, how do you actually generate samples from from things like this?",
            "So one way of generating samples from a joint?"
        ],
        [
            "Nelson, if you have a covariance matrix K Anna mean vector M as a little bit of two lines of octave or Matlab here.",
            "So if you just draw standard random vector from the standard normal.",
            "Random number generator and if you multiply that by essentially the Cholesky factor of the covariance matrix and add them in here, that will generate Gaussian Gaussian random vector which has covariance K an mean M. And actually, I've actually written down here why that's the case of the covariance.",
            "Is the expectation of the difference between Y and its mean and multiplied by itself.",
            "And if you plug in, what happens?",
            "What happens here?",
            "Then we get when we subtract the mean, the mean goes away here, and the sample here is just the Cholesky factor times the underlying set variable.",
            "Here we want the expectation of this thing that ours are just constants, so they go outside and the expectation of said time said.",
            "Transpose because their standard normal variables is just the unit matrix, so the covariance is K, right?",
            "So that's why this construction works, and I think this afternoon the people who are taking part in the practicals you will actually be forced to look at what this.",
            "That's a very good idea, right?",
            "So if you uncertain about you know how this thing works, then actually try to go on your laptop.",
            "Actually try this construction and generate some functions, right?",
            "It makes you feel a lot more at ease with this thing.",
            "Let's try to explore this a little bit deeper, right?",
            "It's a bit mysterious that you just have this construction, and then you do this sort of mathematical thing with this Gillespie decomposition.",
            "And then this function pops out.",
            "Like can we?",
            "Can we get a little bit closer to that?",
            "Yes, this is the main function, is like putting higher and what you expect.",
            "Functions fly around.",
            "Yeah, so the Gaussian process prior is is a distribution of our functions right into specified in terms of amine function, right?",
            "So if you draw samples from that Gaussian process distribution, then on average they will have the mean function that the average over the functions will be the mean function.",
            "Alright, if you put the MX is the quadratic.",
            "That means you're assuming that it's like that's correct, so I was assuming here I was using a mean of zero.",
            "That means when if I would average together all my functions then I would get this error function would estimate that also right?",
            "You don't fix it.",
            "You could do that, but the simplest thing is not to do it.",
            "Actually.",
            "The simplest thing is actually 90% of all Gaussian process work is done with a mean of zero.",
            "I don't think that's necessarily the best thing to do, but you don't necessarily do so.",
            "Will come back to how that can be.",
            "It sounds mysterious, right?",
            "But it turns out that you can get all the modeling to take place in the covariance function, But this is a little bit surprising, so I'll come back to that.",
            "OK so I was going to look a little bit closer at this generation process.",
            "So, So what we're trying to do here is we're given a set of inputs we want to.",
            "We want to generate jointly from the joint distribution of all the outputs.",
            "So there's a little trick you can use.",
            "You can always decompose a joint distribution in terms of conditional distribution."
        ],
        [
            "In the following way you can say, well, let's choose, it's the product of the first marginal distribution of the first variable times the the conditional distribution of the second variable given the first one times their condition conditional distribution over the third one.",
            "Given the two first ones and so forth.",
            "But you can always decompose joint distribution like that.",
            "So that it doesn't have to be Gaussian in order for you to do that.",
            "So, so we can actually.",
            "And the nice thing about writing it down this way is that you now get it as a product of 1 dimensional distribution, right?",
            "And 1 dimensional distributions are much easier to get a good intuition for, and we can plot what they look like, what they look like, right?",
            "So?",
            "One way of generating jointly from this distribution is actually to generate this sequentially, right?",
            "So first we pick a value for the first variable and then we pick a value for the second one condition on the first one and the equation we would.",
            "We would need to use here would be.",
            "We now need to know what is the conditional distribution of one of some set of variables given another set of variables.",
            "And there's this rather large but closed form expression for what that distribution looks like.",
            "OK, and this is also a good thing to try at home.",
            "It's actually so good that I'll try it here.",
            "OK so I have a tiny demo here.",
            "Very very simple demo.",
            "So again we have a 1 dimensional function of 1.",
            "This is the output.",
            "This is the input and what I'm going to do here is I'm going to sequentially generate from that conditional distribution.",
            "OK, so first I need to generate my first sample.",
            "So my first sample would just be from the marginal distribution of one of the variables right?",
            "And let's say my covariance function in this case was E to the minus distance squared, so the so it doesn't depend on the absolute position.",
            "It's a translation invariant covariance function.",
            "So what I've depicted here is what the distribution is of of what the marginal distribution is for any particular of these of these.",
            "Infinitely many variables, of which I'll only look at a few.",
            "OK, so now I just pick an X at random.",
            "OK, just chosen XI.",
            "Picked it here, right?",
            "So now what is the marginal distribution of the this distribution here?",
            "So this is just the Gaussian, right?",
            "The marginal distribution is Gaussian, so I just pick a random number from that Gaussian.",
            "OK, I picked a number over there close to the mean.",
            "OK, so now we go on to the next to the next variable.",
            "So the next variable is picked over here.",
            "So now what does the marginal distribution look like in this case?",
            "Where the marginal distribution because I had the covariance function that said, where you have covariance between things that are close to you, right?",
            "If X&X prime are close, then they covary, right?",
            "And they can only call vary by having similar values, right?",
            "So it's saying that if my new point was very close to the old point then they would have to covary, right?",
            "If my point was over, here was very far away from the parents and the covariance between them would be very small.",
            "Alright, so my new marginal distributions look like this and I just pick another X at random.",
            "Pick it here and then I pick a random number.",
            "From that, I'm going to say, OK, it was a fairly negative here.",
            "And then I go on to the next one, OK?",
            "So OK, my next, my next Canada is over here, but now we can see the marginal distribution over these points.",
            "So now the inside this region, the predictions or the marginal distribution of outcomes here or condition on these two observations is going to is suddenly highly constrained right in saying, well the function cat do just whatever it likes in there because it has to covary with both of these guys right?",
            "And the only way IT can do that is by lying in this in this interval.",
            "And magically by this by this property that the conditional distribution of a Gaussian is again a Gaussian, those distributions are actually exactly Gaussian, right?",
            "So we can continue doing this.",
            "And you can sort of see that we are slowly uncovering this underlying function, right?",
            "An whenever I get to make a prediction in a place where they essentially no so in place in place like this, for example, they essentially no variability left, like when I picked the random number from that distribution, I'm going to get one that lies exactly on the on the function, right?",
            "So this of course is the same.",
            "It's equivalent procedure to the one where you generate jointly, right you generate jointly.",
            "Ann, you are jointly satisfying all the constraints.",
            "That's what the Cholesky decomposition is doing for you.",
            "OK. That's enough.",
            "OK, and again, I'm very recommend you know you can do this in two or three lines of Matlab, and it's very, very instructive to actually watch this process is doing and how the Gaussian process is inducing this distribution over functions.",
            "OK, here's another picture of a function that I generated by doing exactly the same thing.",
            "So in this case my my input space is a 2 dimensional space, right?",
            "So now I'm map from A2 dimensions onto this onto one dimension.",
            "So what did they actually do?",
            "Well, I wrote down it's exactly the same covariance."
        ],
        [
            "Now instead of X -- X prime, I just have the XNXX prime squared.",
            "I use the squared distance Euclidean distance squared between the two.",
            "So what I actually did is I wrote down.",
            "So this is 100 by 100 grid of input points.",
            "So that means I have 10,000 input points.",
            "OK, input points that lie on this grid.",
            "And then I need to construct the covariance matrix.",
            "So the covariance matrix has an entry for every pair of input points.",
            "OK, so this is a 10,000 by 10,000 covariance matrix.",
            "And then I pick a random number from that 10,000 dimensional distribution and that point from that distribution will be a 10,000 dimensional vector and I plot those 10,000 values as a function of those.",
            "100 by 100 and again you can see that this sort of gives rise to sort of interesting looking functions, and indeed it looks very much like these are are smooth functions or in fact for this covariance function you can show that these are infinitely differentiable functions.",
            "Yes, sorry.",
            "Gosha OK, so that was just this thing I said I wouldn't call it Gaussian, and then I called in Gaussian, so it's this squared exponential form.",
            "So E to the minus distance squared, right?",
            "But I don't like calling it a Gaussian because because it's not a distribution.",
            "Yeah, yeah, OK, very good.",
            "So if I were giving people points you would have a few.",
            "OK, so so.",
            "So far so good.",
            "So what I've done so far is I've sort of said, well, you know you can use Gaussian distributions to define distributions over functions, right?",
            "So now I've got a vocabulary where I can start thinking about how to do inference right.",
            "Up.",
            "Until now we've only drawn random functions, right?",
            "We're not interested in machine learning and random functions, right?",
            "We're actually interested in stuff that has something to do with our data.",
            "OK, so this is the last little step that we have to do.",
            "I'll have to make sure that my function somehow has something to do with the data that I'm interested in.",
            "So how do we do learning in these things?",
            "OK, so so now we have to do some inference.",
            "So we already looked at this slide.",
            "This is the how to do inference in a in a parametric model.",
            "And we also looked at the Bayesian inference here.",
            "So again, what?",
            "What normally happens in parametric models?",
            "Although our model is now is a nonparametric model will normally happens here is that you put a prior on the weights.",
            "Then you construct the posterior over the weights."
        ],
        [
            "And then you use that posterior to compute the things that you're interested in, right?",
            "And you're interested in making new predictions.",
            "So that's an integral over the posterior distribution.",
            "Or you might be introduced interested in the marginal likelihood, the marginal likelihood being the product of the prior and the likelihood.",
            "This is an unnormalized posterior, so again, it's it's an integral over the posterior distribution.",
            "We can also do.",
            "We can also compute the probability of the model if we have different different sets of models, and indeed this is what we're going to.",
            "We're going to do now.",
            "The problem here is that for most interesting models, these into."
        ],
        [
            "Both are are intractable, right?",
            "So we can't actually compute what's going on.",
            "But Interestingly, Gaussian process are tractable and interesting."
        ],
        [
            "Some models where you can exactly do those integrals right?",
            "So how does that work now?",
            "Firstly, there's a little bit of a problem here, right?",
            "Because where are the weights like?",
            "I'm just saying, well, we're just using a prior over the functions and then we have some data.",
            "The data also tells you something about what the function is, and then we're supposed to combine those those two.",
            "But how do we apply this methodology?",
            "When like where?",
            "Where are the parameters like where is the model here, right?",
            "So it turns out that.",
            "The right answer to that question is that the parameters in our model is the function itself.",
            "OK, so that sounds that sounds a little bit weird, right?",
            "But what we're doing inference over is just the function, right?",
            "And so if we plug into our usual framework, if we just plug in every time we saw the parameters before, we'll just stick in the function and let's see what happens right.",
            "After all, we want to do inference about the function.",
            "The likelihood tells you something about what the function is doing and the prior to tell you something about what the function is doing.",
            "OK, so we have so.",
            "So we have the same ingredients as before.",
            "We just turn the crank on what we're supposed to be doing.",
            "So what about the likelihood function so we have again a Gaussian likelihood function.",
            "So likelihood function tells you what's the probability of the observed data given the parameters.",
            "So now that will be the.",
            "We're not modeling the inputs here, it's the prob."
        ],
        [
            "See of the story I change notational it, so this is shorthand for P of the thing that says this is a conditional probability.",
            "I didn't write the piece anymore.",
            "Write everything.",
            "I write down these probabilities.",
            "So if there's not a piece missing.",
            "So, so the likelihood function is the probability of the data given the parameters.",
            "OK, so let's write that down so it's the probability of the data and we only interested in modeling the output here its probability of outputs given the data in the data is now the function.",
            "OK, so now I condition on the function I put in FF of X here right?",
            "And what is that thing?",
            "Well, the likelihood tells us something about if the true function is something, then what's the probability of observing the thing that you actually observed, right?",
            "And that's if the if the noise that depends on the noise assumption, right?",
            "The noise assumption is Gaussian.",
            "Then it says.",
            "Well, then that likelihood that probability is going to be proportional to E to the minus the distance between the actual function and that observed value.",
            "So that yes.",
            "So the question was.",
            "The parameters in our data data function.",
            "Sorry, So what I maybe maybe I said it wrong so the likelihood function is the probability of the data given the parameters OK, and now we've now substituting the function for the parameters and the probability of the data is only the output part that we're really modeling, right so?",
            "OK, sorry if I said that wrong.",
            "OK, so something interesting happens here, right?",
            "Because we actually only the likelihood function is only interested in what the function does at the locations where we have data, right?",
            "It doesn't say it doesn't actually depend on what the function does out here where there was no data right?",
            "So although although when we write it down, it looks as though the likelihood function cares about what the function does everywhere, that happens not to be the case, right?",
            "It actually only depends on what the function is doing at the places where we where we made observations.",
            "OK, so the likelihood function actually only depends on the values of X at the input location, so I use BF F. As a vector of those of those F values.",
            "OK. And this is just a shorthand notation for for a Gaussian distribution, right?",
            "So the Gaussian here is centered on F and it has whatever the observation noises.",
            "So that's the likelihood function, yes, so from my ex Now you mean the mini mineplex and the covariance of X.",
            "Right now here it just mean I just mean what the function is, so the function is unknown, just like the parameters are unknown when we're doing inference, so this is just, you can just think of this as being sort of an abstract notation for this stuff that we don't know.",
            "We don't know what F is, right?",
            "But we do know that F the places where we have observations F should be close to those observations.",
            "OK, so now we have to put a prior on our on our parameters right now promises in our function, but Luckily we know how to put distributions over functions, right?",
            "So let's put a Gaussian process prior on the functions.",
            "So our prior here is the probability of the parameters given the given the model class.",
            "Here if we want and we've got just going to use a Gaussian process prior and we can choose any any Gaussian process that we want.",
            "Of course, I'll have to tell you about ways of making smart choices about this, right?",
            "We'll come back to that right.",
            "But formally speaking, we just need to specify what that Gaussian process prior looks like in a parameterized in terms of mean function, which in this case I'll just set to 0 again and covariance function.",
            "Yes, there was a question about.",
            "That website and the function that it's using.",
            "Then set up your observing the value of X. Yeah, so I'm sure this was a little little shorthand, maybe a little bit too short for.",
            "Let's look at the at the likelihood function again, right?",
            "So the likelihood function basically says it's a product over terms and each of the terms as this form and it's interested in the squared difference between the observation and what the function is doing at that point.",
            "But notice that the total likelihood here only cares about what the function does at the locations where we made observations.",
            "OK, and this is just my very compact notation for the for the same thing, right?",
            "It's a joint Gaussian centered on the F values.",
            "With this amount of noise.",
            "OK, and then we need the we need the Gaussian.",
            "We've got some pro prior over function so that will give that Gaussian process prior and see what happens.",
            "Now what we need to do, we need to compute the posterior right.",
            "We do that by multiplying the prior with the likelihood.",
            "So now the likelihood is just a just a regular Gaussian here, and the prior is this funny kind of Gaussian is infinite dimensional Gaussian OK, But let's just go ahead right?",
            "So we multiply 2 Gaussians, we get another Gaussian.",
            "We got an unnormalized Gaussian right?",
            "But when you multiply 2 regular Gaussians, we get one of these, we gotta and normalized regular Gaussian if you multiply.",
            "A Gaussian with one of these infinitely large Gaussians, we just get an infinitely large Gaussian.",
            "OK. And formally, this is just using the standard standard way of multiplying together 2 Gaussians.",
            "But what does it actually mean intuitively?",
            "Now the posterior distribution is what it's a.",
            "It said it's an infinite dimensional Gaussian, so why does that make sense?",
            "Well, it makes sense because it's supposed to be a distribution of our functions, right?",
            "And a Gaussian process is a distribution or functions.",
            "Like so we started with with with a random distribution or functions and then we said, well, we now have some data that is telling us something about what the function is doing and that also had a Gaussian form right?",
            "We multiplied together those two and then we got this infinite dimensional Gaussian here.",
            "So now, so how do we actually use this thing right?",
            "We use this thing to make predictions.",
            "And how do we make predictions?",
            "Well, the predictions are just an integral over the posterior distribution.",
            "Or we just we just want to marginalized out the values of F that we're interested in here, right?",
            "So if we choose test point X star, we say, OK, let's just modernize out the rest of the of the infinitely many variables and just look at the one that we're interested in, right?",
            "And then we do that by just throwing away the parts of the covariance that we're not interested in.",
            "OK, I won't drag you through the actual algebra, but this is.",
            "This is the only thing which is going on right and when you get the predictions, the predictions here just comes down to this one dimensional Gaussian that has a mean which is just a number.",
            "In this case, it's a vector times matrix times vector here and again the variance is just a number.",
            "OK, so it's just a 1 dimensional Gaussian distribution, yes?",
            "Evaluated.",
            "That's correct, right?",
            "OK, so the question is, I've computed what the posterior distribution over the predicted value is at a particular point, but I was free to choose point anyway anyway I want right?",
            "So I just compute this, sorry.",
            "Up here, yeah.",
            "The function evaluated at no no this is.",
            "This is the general.",
            "This is the posterior distribution over the function.",
            "Right, this is not a particular X here.",
            "This is the X.",
            "Here is just just just a placeholder.",
            "Like this equation works for any value of X.",
            "Yes, you just said girls in process is in distribution or functions and I remember the definition goes in process collection of random variables in a number of which have goes in distributions.",
            "So I just can you remind please help why is ghosting, processing, distribution or function which functions?",
            "Well, the distribution of functions because you can choose to look at you know what is the function doing at any number of locations and any number of locations, the distribution or the function values are jointly Gaussian.",
            "That's what the Gaussian process says, right?",
            "Let me take a few more questions.",
            "Then I'm going to draw pictures of what this actually means, right?",
            "'cause algebra is not so easy to digest, so let's take a few more questions.",
            "Matrix and we have 10s or 100,000 data points into huge matrix.",
            "So there's a question about you know how?",
            "How big is the matrix that we have to compute right?",
            "And it's true that the matrix here is depends on the covariance matrix here, which has the dimension which is the number of points squared, right?",
            "So if you have 10,000 training points, then this matrix is a big matrix, right?",
            "That's true, but it's not an infinitely big matrix, right?",
            "It only scales well, scales quadratically with the number of data points, right?",
            "So if you want to use this for very very large data datasets, then you might not want to use this naive implementation, right?",
            "So that's a very good point.",
            "Very noisy, able becausw.",
            "Now there is you already have a prior over the functions which have some variance, so it seems like there is some duplicacy like there is a Sigma squared noise and there is this noise in the face, so it's a very good question.",
            "So the question is you know do we need the Sigma squared and and what is the what is the role of the Sigma squared?",
            "How would I specify the signal?",
            "So I'm going to get back to that right?",
            "That's an excellent question.",
            "Where do we get that from?",
            "Right obviously I'll have to tell you how to do that.",
            "OK, so now let's look at a pictorial representation of what this actually looks like.",
            "So on this panel over here I've drawn."
        ],
        [
            "Three independently drawn 3 random functions from this from this Gaussian process and in the blue.",
            "Here I've actually done things properly right.",
            "I've actually written down a set of X is an, then written down the covariance for those X is drawn a point from that from that Gaussian and plotted those values as a function of the X values, right?",
            "So here in the green and the right I've been a little bit more sloppy.",
            "I've just joined up the points with lines, right?",
            "Because I sort of know that there's an underlying smooth function there.",
            "OK, and what I've shown in this in the shaded area here I'm showing you know what is the mean, which is 0 here plus minus 2 standard deviations, right?",
            "And that's this area where the function should lie in with with 95% confidence.",
            "So these are these are the random functions.",
            "This is draw.",
            "These are draws from the prior Gaussian process.",
            "So then I observe I've made a number of observations.",
            "I made five observations here that I've plotted with the pluses here and now I update my Gaussian process by multiplying Gaussian process with the Windows 5 likelihood terms and that gives me a new Gaussian process.",
            "The posterior process and you can see that the posterior process, as indeed we've seen from sampling from the prior.",
            "Actually is forced to go through those data points right?",
            "Because we, because the likelihood enforces that.",
            "So in this case it's a likelihood with a very small noise variance, so it's forced to actually agree with the data points at those points.",
            "And again, I've drawn 3 random functions from that distribution in colors here, and I've also shown in Gray this what the error bars are.",
            "In order to do this plot, I just plug into this equation down here.",
            "In this case, I would have to draw.",
            "I'd have to.",
            "Compute here to compute this.",
            "This distribution here I would have to foreign X star, let's say 2.5 or something like that.",
            "I just plug in X dollar is equal to 2.5 and I plug in.",
            "This is just the covariance function evaluated at 25 and all the ex is there 5X is here.",
            "There's a 5 by 1 vector.",
            "This is a 5 by 5 matrix and these are the vector of the observed values here.",
            "OK, and similarly for the for the covariance.",
            "OK, so this is just a pictorial representation of how the prior distribution or functions which didn't tell us very much about what was going on, gets updated when we actually see the data.",
            "OK, any questions about that?",
            "This piece that's correct.",
            "Yes yeah.",
            "And in practice we tend not to believe our data that much right?",
            "So in practice you would probably have a likelihood function that said that the observation noise is not non 0 right, and then you would get a similar picture to this except that the distribution wouldn't go have a variance that exactly went to zero, but it would go down to the smallest.",
            "Yes, that's correct, yeah.",
            "Yeah, I said it's something very very small so you can't see it right?",
            "OK. Good, so let's try to relate this.",
            "This kind of model to some of the other models we've been looking at.",
            "So here's a here's a graphical model representation of the Gaussian process.",
            "So we have these."
        ],
        [
            "So the in this kind of notation, the function values here the true underlying function values are the latent variables like we don't get to absorb those right?",
            "And because the the Gaussian process prior they had their jointly have a joint Gaussian distribution.",
            "So that means that all of the latent variables are connected to each other, right?",
            "So if we're looking back at the graphical models framework, this is sort of telling us that that's bad, right?",
            "Because it says that there's no, there's going to be no computational speed up there, no?",
            "Conditional independence relationships here, right?",
            "Because you got a conditional independence when you had an absent absence of a link, right?",
            "And all these variables are linked together, and each of the each of the variables here is, well, the variable does depends on where it is.",
            "It depends on on X here and on the on this side I've shown the training examples, so we've actually observed what the observation is.",
            "So that somehow constrains what this variable can do, because the likelihood says or you better be close to my observation here.",
            "Right and then they are coupled with all the other variables, although training variables and they also coupled to the test variables and for the test variables.",
            "We haven't observed what the corresponding target is, right?",
            "So these are in in circular nodes here.",
            "And so that means that if we would actually end, we would put in other test points here if I would add another test point then that wouldn't change the distribution over all these things right?",
            "Because it would it wouldn't care about what the function was doing, so it's only when you actually add an observation of that.",
            "That of the corresponding value that you influence the distribution over the over the latent variable.",
            "And that's again just the same as the marginalization property, right?",
            "If you want want to write down the diagram for the whole Gaussian process, you would have to have an infinite number of these nodes, right?",
            "But because most of the nodes don't correspond to any observation, they don't change the distribution of the remaining number of nodes, so you don't have to put them in the graph.",
            "And let's examine so these are two 2."
        ],
        [
            "Components here the mean and the variance.",
            "The mean looks like this right?",
            "So the mean is a has this particular form and you can write that this is Lena in two particular ways, right?",
            "One way is that it's a linear in the observations, right?",
            "So if I multiply the vector here of covariances between the test input and the training inputs by this?",
            "Inverse covariance matrix here and I call those values beta here.",
            "Then it's just the dot product between the beta vector in the observation.",
            "So this is what the statisticians call called a linear method right?",
            "Because the prediction the mean prediction is going to be a linear function of the observations.",
            "OK, another way that is linear is if we now collect the Y times the matrix here and then we can see that it's a linear function of the covariance function evaluated at X and the training cases.",
            "And actually you might you might sort of recognize this from one of bound hearts.",
            "Last slides that in the support vector machine the prediction that you make or the outcome is actually a linear combination of the kernel evaluated at those points, and that turns out to be.",
            "An extremely tight relationship between every time bound says kernel.",
            "I say covariance function.",
            "It's the same thing, right?",
            "And and one can think of a Gaussian process as being also a method that works in an expanded space.",
            "In this feature space and the mean prediction comes out as the as the output of the support vector machine.",
            "But I have something extra here.",
            "I have a probabilistic model, right?",
            "So I also have accompanied by my mean prediction here.",
            "I also have a variance.",
            "And the variance also has an interesting form, so the variance look like this.",
            "It's the difference between two terms here, so this thing here is the covariance function evaluated between XR&X Tower itself, right?",
            "And this is just the prior variance like this doesn't refer to the data at all right?",
            "To the observations.",
            "This is the prior covariance.",
            "So this is the covariance you would have when you haven't seen any data.",
            "So if we look back at this kind of plot here, right?",
            "So this is the prior variance in this case, because the covariance function stationary.",
            "It doesn't depend on the location.",
            "So that's a prior variance, so that's the variance you would have if you hadn't seen any data, right?",
            "That's the variance you had before you start.",
            "Then you get to subtract something from that variance.",
            "You get to subtract something which is a quadratic form, so you get to subtract something which is always positive.",
            "And you get to subtract what the stuff you get to subtract here has to do with how much does the training cases tell you about what the function is at that value, right?",
            "The more the training cases tell you about what the function is doing there, the smaller the predicted variance is going to be right, and you can sort of see that that's happening here, so you get large values for the covariance, at least if your covariance function is a squared exponential, you get large values for these things here when the test inputs are close to the training inputs, right?",
            "So that's the places where it's telling you a lot about what the function is doing right, and then the predicted variance will be small in those areas.",
            "But if you're predicting far away from the data from your training data, then you just get back that you get the prior variance.",
            "Pression looks a bit late in Kernel Ridge regression, yes, so the point is, this looks a little bit like Kernel Ridge regression.",
            "This is Kernel Ridge regression, right?",
            "That's a different name for.",
            "For this method in Kernel Ridge regression, people usually concentrate on the on what the mean function is doing right, but there's no difference.",
            "OK.",
            "So actually Gaussian process regression has quite a few names because it's been invented over and over again, right?",
            "So it's also called creaking models.",
            "So if any of you have bumped into those before is exactly the same idea.",
            "Creating so in the geostatistics literature, it's sort of the standard model.",
            "For some reason, no other community knows about it, except maybe for the machine learning community these days, and it's called creaking.",
            "Yeah, so how come the rights doesn't depend on the.",
            "Yeah, so there's a.",
            "There's an interesting point here which is.",
            "If you look at this expression, this expression doesn't actually depend on the observations, right.",
            "The observations are the Y values, so how can it be that the predicted variance doesn't depend on the observations, right?",
            "And this is just a property of the way this thing works.",
            "You would certainly imagine that it would somehow depend on how closely observed targets were, and if it if the targets were far away, you would maybe assume that that would mean that the distribution would be wide or something like that.",
            "But because basically because.",
            "These things are Gaussian, right?",
            "If you have a neighbor which is Gaussian with a particular noise level, then the variance of your of your of the underlying function at that point will just be a product of those constraints.",
            "Right, so so it doesn't actually matter.",
            "You know what those values are, you just have to the width of a Gaussian.",
            "If you multiply 2 Gaussians, it doesn't matter whether you know they are far apart or they are on top of each other.",
            "The width is just this.",
            "Some of the provisions or the precision is the sum of the positions.",
            "So this is a consequence of this, gaussianity.",
            "It's not necessarily clear that this is a good thing.",
            "Something to be aware of, right?",
            "How much the data X as an expense?",
            "Well, this is my sort of informal way of explaining what this term is doing right, and we saw that on the pots that the more the closer you had, the more observations you had in the closer you are to the observation, the smaller there Bobby came right.",
            "And this is just my interpretation of what's going on.",
            "So maybe like how much excess coverage over.",
            "Jose are from the desert.",
            "That's right, right?",
            "And it's measured somehow in terms of what the covariance function is saying.",
            "OK, so we haven't.",
            "Actually, we haven't actually really.",
            "I'm not really told you how you turn this kind of idea into a practical framework told you so far as to say, well, we can have this distribution or functions.",
            "We can define distributions or function.",
            "We can also define distributions that are somehow conditional on making observations, but I didn't tell you anything about you know how do you come up with this covariance function?",
            "And what about those?",
            "All those other parameters that were floating around like somebody was saying?",
            "You know what about Sigma squared?",
            "How do you get that?",
            "How do you get the noise?",
            "Value so I have to tell you what to do about those.",
            "So now one of the other things that we could compute was the marginal likelihood.",
            "The marginal likelihood is just the probability of the data.",
            "Marginalized over the.",
            "So here we are.",
            "So again, it's a.",
            "It's a, it's an integral over.",
            "Now our posterior distribution will be over function value.",
            "So is this an integral over function values and we can do this integral because this is a Gaussian in our case, although it might be an infinite dimensional Gaussian, we can still do that particular integral.",
            "So if we look at what is the value of the marginal likelihood, then it has this particular form.",
            "So it contains 3 terms.",
            "The first term is the only term that depends."
        ],
        [
            "On the data.",
            "So I call this the data fit term.",
            "OK. Then there's a second term here, which depends on the half minus half the log of the determinant of the covariance matrix, and this is a complexity penalty term, and I'll show you afterwards why why it has that name and there's a third term here which is uninteresting.",
            "OK, so the marginal likelihood is the combination of these three terms.",
            "And when I choose a covariance function, I can then compute well what is the what is the value of the marginal likelihood, i.e.",
            "And this is.",
            "By using Bayes rule we can then swap around the model and the and the data and we can ask what's the probability of the model given the data right?",
            "And we can now try out different covariance functions.",
            "So that's essentially what we're going to do, so that would be to say, for example, if I have two or three different covariance functions, and I don't know which is good for my data set, so each covariance function implies a prior over functions, right?",
            "And it might be that this prior distribution or functions you know totally disagrees with your data right?",
            "In which case you know this is probably not a good model, and hopefully the marginal likelihood will tell.",
            "Tell us that this is not a good model.",
            "Optimize my prior then like you optimizing applied in this way will not buy some parameters for optimizing.",
            "That's right.",
            "So also what we're going to look at exactly now is what happens if we had parameters in our specifications."
        ],
        [
            "We already had the noise parameter, so the this Sigma N squared hear the noise variance.",
            "So we have to we have to do something about that will have to fit that parameter and also we had the.",
            "So before we I just looked at E to the minus the distance squared.",
            "But how do we actually define?",
            "You know when are two points close to each other and when are they were they deemed to be far apart right?",
            "That seemed a little arbitrary before, like if they were more than one or two apart then the exponential of distance squared would be fairly low.",
            "But how did I know that that was the right?",
            "That was the right distance measure?",
            "Actually, I didn't know that right?",
            "And this is a very relevant question.",
            "So there's a question here.",
            "How far do X&X prime have to be away from each other?",
            "Before I think the covariance should start falling right?",
            "If there is the X&X prime or further away from each other and some particular distance, then I deem them to be to the function values to be more or less independent.",
            "There's another way of phrasing it right?",
            "So generally this is something that I don't know right, and I can't guess that when I'm just when I just got my got my data set right?",
            "So this is something that you know I can put in a parameter, so I put in another parameter here L, which is called the characteristic length scale.",
            "So L just measures how close should X&X prime be before?",
            "I deem them to be close together.",
            "It's roughly speaking OK, so L is now a parameter of my of my model.",
            "Similarly, V is a parameter, so before we just said OK, covariances could go up to one right.",
            "But what happens if my data set lives on a much larger scale, right?",
            "Well, I should be able to cope with that as well, right?",
            "So there's a scale parameter outside everything, which I called V here.",
            "OK, so that's just the prior variance over the function that we're interested in.",
            "So I've got in this case I've got.",
            "I've got now three parameters that I don't know how to set, So what I'm going to do is I'm going to try and choose different values for those three parameters and see what happens.",
            "OK, so first I've plotted that in a very simple example down here.",
            "So I have 20 observations here, so those are the 20 observations.",
            "And then I choose in this plot, I'm going to focus on the link scale.",
            "And I'm going to choose three different length scales and see what happens what my.",
            "My prediction look like and I'm just going to look at the predictive mean in this case, right?",
            "The prediction predictions, of course have particular distributions associated with Airbus associated with them.",
            "Alright, so in red here I've chosen.",
            "Let's start with blue and blue.",
            "I've chosen a very long length scale, right?",
            "The very long length scale says that the we expect a priore that function values correlate a lot, even if the points are quite spaced quite far apart.",
            "If that's my assumption, if that's my prior assumption, then when I see the data then I get a fit that looks like this.",
            "OK, and obviously we can see by that that wasn't good in this case.",
            "OK, so my prior assumption didn't match the data very well, so I can try.",
            "I can try a very short length scale if I choose a very short link scale.",
            "Then basically it says that the covariance between the neighboring points drops off very quickly as a function of the input.",
            "OK, and what you can see?",
            "What happens here with the mean function is the mean function starts wobbling around a lot OK?",
            "Because it only has to move a slight amount away from the from the data point before the function can start doing new things basically right.",
            "And there's some nice aspects of this, because the fit here is an almost perfect fit to the data, right?",
            "I think it's it's not quite hitting those two data points, but the rest of the data points it almost got them right.",
            "If I reduced the length scale by a little bit more, I would be able to fit the data exactly right, but actually in terms of generalization, this doesn't look so good, right?",
            "It looks as though it's actually.",
            "It looks as though there's a smooth underlying function and there's some noise associated with that.",
            "OK, so and here in green I've chosen an intermediate.",
            "Length scale and that intermediate length scale gives the fit in green.",
            "Here an which looks like a reasonable fit right?",
            "And when I look at what is the marginal likelihood associated with this so I can compute that by just using the expression from before, so just plugging into this expression.",
            "It's only depends on the data and the evaluated covariance function.",
            "Here, when I plug into that, it chooses the green model, right?",
            "That's actually how I found the green model.",
            "I just plugged in the parameters and optimized with respect to the link scale right and it came up with this with this link scale.",
            "So notice that this is despite the fact that it's very easy for the model to fit the data exactly if it wants to.",
            "It doesn't want to write, and we heard already yesterday.",
            "Some of the reasons why what the mechanisms are that prevents the models from liking, fitting the data too much.",
            "OK.",
            "I found a question there.",
            "So this is doing an optimization.",
            "Could we put the prior on the parameters and it again without out?",
            "And could we do that instead of doing Monte Carlo?",
            "Doing one of those deterministic like he kind of right?",
            "So the question is now I'm optimizing over one of those parameters.",
            "Could I?",
            "Could I do things and we know that we should be careful if we're doing optimization?",
            "Could we do things a little bit better?",
            "Could we actually put a prior over the length scales and then find the posterior over the length scales?",
            "And indeed you can do that, but?",
            "It's not computationally.",
            "So easy to do that because you end up with a with a collection of different Gaussian processes, like you can't integrate them analytically, right?",
            "So you end up with, but you can use Monte Carlo for example to do these things, although the individual models under the Markov chain of course involves these matrices that depend on the size of data.",
            "So for big datasets you know this might get out of hand at some point, right?",
            "But ideally that's exactly what you would want to try to do right what we're trying to do here is to get away with just optimizing.",
            "Very few parameters right?",
            "So in this entire model I have only three parameters, right?",
            "So it's not like it's not like a parametric model where you wouldn't be able to get a fit like this if you only had three parameters, right?",
            "So it might be that we're a little bit in better shape in this case because we can manage to specify things in terms of very few parameters.",
            "Yep, sorry.",
            "Previous business model.",
            "It's not for mistresses.",
            "So, so the question is, you know why is the why is that the right form of the complexity term, right?",
            "So that's an extremely good question, and that's the next thing I'm going to talk about, right?",
            "So notice here that the marginal likelihood.",
            "So sometimes when you are in the sort of regularization framework, then you have a regularization parameter, right?",
            "You have a data fit terms plus Lambda times the regularizer, right?",
            "And you don't know what to do about Lambda.",
            "There's a neat thing here.",
            "There's no Lambda, right?",
            "This is automatically on the right scale.",
            "It's automatically doing exactly the right thing, so there's no free parameters anymore.",
            "OK.",
            "But you might be a bit.",
            "But might be a bit.",
            "Unsure about using this stuff that came came out of my my somewhat funky algebra here.",
            "So you do you believe that this is the case, right?",
            "So now I'll try to persuade you that indeed this is very reasonable.",
            "The result should look like this.",
            "Hey there logo Model T as in Tom industry.",
            "OK, so it's the determinant of the K matrix, which is a number, and it's the logarithm of that determinant, right?",
            "So the whole thing is a number.",
            "OK, so just before we go there, there's this plot again, which, which is a very important plot.",
            "So Chris Bishop also showed his."
        ],
        [
            "Version of this plot, so I'm just going to go through it again because I think it's very, very important and hopefully someone else will also show this plot so you'll have seen it 3 times and you'll know that it's true.",
            "So what I'm showing you here is the is the value of the marginal likelihood.",
            "And here I have an abstract representation of all possible datasets.",
            "Let's say you have a certain size OK, and I had different kinds of models.",
            "I have a very simple model, so a very simple model will only give high probability to data set.",
            "Data sets that look a very particular kind, right?",
            "So if I have a linear model for example, then it will only give me high probability if the if the data actually can be fitted by elite by a linear model.",
            "And I can have very complex models, so let's say a huge neural network or something like that that could fit almost any.",
            "Any data set, but because these things are probabilities, they have to have to integrate to one, right?",
            "So for the complex models, they can't have very large values of the marginal likelihood if they have to have, you know significantly non zero values for lots and lots of datasets right?",
            "Whereas the simple models can have very high values because they only account for.",
            "If you.",
            "A few different possibilities.",
            "So when you actually come with your data set then there will be a model complexity which is just right, right?",
            "And what I showed you before is actually the lines correspond to models of different complexity.",
            "Right now the complexity is not given in terms of."
        ],
        [
            "The number of parameters or something like that, but it's given you can see that the intuitively appealing thing that this is a simple model here, but the read function is a sort of a complex model, right?",
            "Not by counting parameters or anything like that, but by looking exactly at this complexity term that has exactly this form.",
            "I'm going to use the last five minutes to try to persuade you."
        ],
        [
            "So that this is actually exactly the right form.",
            "OK, in an intuitive way.",
            "OK, so let's play a little game.",
            "OK, I'm going to give you some data points.",
            "I'm going to give you some scalar data points, and they're going to have a mean of zero, and I'm not going to tell you what the variance is.",
            "They're going to have a Gaussian distribution, so I'm going to give you, let's say 20 data points, and I want you to tell me what is the variance of the Gaussian that I used to generate the data.",
            "OK, so that's a fairly simple modeling problem.",
            "Does anybody, would anybody like to venture?",
            "What would a good answer be so this case?",
            "So they're just sample from a Gaussian with mean of zero and an unknown variance like sample.",
            "Some scalars give you those scalars, and now I want you to tell me what was the variance of the generating Gaussian.",
            "Summer.",
            "Square root of sum squares divided by.",
            "OK, here's something about this summer squares.",
            "Yeah, so you're looking essentially at the property of the data.",
            "Yeah, yeah, looking at the second moment of the data and that's a good answer.",
            "Why is that a good answer?",
            "We agree that's a good answer, right?",
            "But what principle were you using?",
            "Maximum likelihood, alright?",
            "So let's try and do that.",
            "Let's try and write down the likelihood function and let's try and maximize it, OK?",
            "So the likelihood function look like, well, it's."
        ],
        [
            "Is just a Gaussian distribution, right?",
            "The data is generated by this Gaussian.",
            "OK, we don't know what the variances, that's that's a free parameter.",
            "So the probability of the data given that parameter written down the log of the likelihood here?",
            "OK, it's the.",
            "So the log of that so that log cancelled exponential, so was exponential of minus the distance between the observations and the mean.",
            "And I told you the mean was zero, so mu is zero in this case divided by Sigma squared.",
            "So that's where we get from.",
            "From that time there's a normalization term that looks like this, and then there's a multiple of 2\u03c0 as well.",
            "OK.",
            "So the way to solve the problem, a good solution to the problem is to say well, choose the Sigma squared which maximizes the likelihood.",
            "Alright, and this is the likelihood, and if you maximize this likelihood then you get exactly that.",
            "Sigma squared should be the empirical variance here, but notice that this is exactly the same equation as I showed you before.",
            "Right, I have a data fit term in blue.",
            "I had the log of the determinant, so now this is not written as a determinant.",
            "This is written as a matrix here because this is this is this is a 1 dimensional example of this right?",
            "So the so I have independent observations there so that my matrix K in this case is just Sigma squared times the.",
            "Times the unit matrix.",
            "So the determinant of a matrix.",
            "Here I just get an end that pops out.",
            "So if we look back so we have a data fit term, the only thing that depends on and have a complexity term and the complexity term depends on the variance of that matrix.",
            "So this is the one dimensional case where we had the general case before you go back.",
            "Sorry.",
            "We had a data fit term complexity term and we had this thing over here, right?",
            "So it's exactly the same, the same principle and you can see also the intuition about why this is the complexity term even in this simple case, right?",
            "So here's a here's a situation.",
            "What things would look like?",
            "I gave you these data points and you could try to fit it with the with the with the Gaussian, which has a very small variance, right?",
            "So I've drawn a Gaussian with a little bit too small variance, but what's the problem with this with this model?",
            "But the problem is that there are a few data points.",
            "That appeared in places where the model says that that was extremely unlikely.",
            "OK, so that's not a good model.",
            "Down here I now have a now have a very complex model.",
            "This model would allow for many more different configurations of the data points, so this is kind of corresponds to my complex model.",
            "What's wrong with the model?",
            "Well, the data fit term is fine, right?",
            "They all lie nicely under the curve.",
            "The problem is the complexity, right?",
            "We allowed four points appearing out here where they didn't appear right, so we were wasting our mass out there.",
            "So the data fit was fine.",
            "Term was not good, but the complete was good, but the complexity term was hitting us there, right?",
            "And that's why we wouldn't want to choose that big that big matrix.",
            "And otherwise so this is the same situation we have.",
            "This is this is just the right complexity, right?",
            "By choosing the variance to match the empirical variance.",
            "OK, so I hope that this persuades you that what's going on in this somewhat impenetrable algebra is actually is actually very reasonable, right?",
            "We're trying to match the covariance matrix now in the joint space, so we have to think about the distribution of all of all the points in this very large dimensional space, right?",
            "But we want the covariance somehow to not just allow any distribution of points, but be as tight as possible that the data.",
            "Allows us to be.",
            "That was a question.",
            "Watching process actually suffer from the first dimension, so the question, does the Gaussian process suffer from the curse of dimensionality?",
            "Well, the curse of dimensionality is something that that there isn't really any way around, right?",
            "It's saying that.",
            "Once you move to higher and higher dimensional spaces, they're basically more and more things that would seem to be to be reasonable, right?",
            "So when would you expect this kind of thing to actually do something about the curse of dimensionality?",
            "Only in the cases where somehow you could specify a prior distribution which will help you to look away from those from those areas, right?",
            "So that's what I'm going to talk about next time, right?",
            "So today we've only been talking about very simple forms of covariance functions, right?",
            "So maybe we can get some more mileage out of specifying more closely the properties of the covariance function, and maybe we can try to find out how to match those against our datasets using this methodology.",
            "Yes, in this formula, samples are actually independent, but indeed.",
            "No function case.",
            "They were going to be a little correlated, so that's exactly right.",
            "So the difference between my equation here is just that this is the independent version of that, right?",
            "So I don't have the correlation matrix and in the Gaussian process case I will have this situation that things that are highly correlated or things that are close by should have values that are highly correlated, and if they don't then this is being penalized by the by the data fit term.",
            "OK, so I think I should.",
            "I should stop here, but I hope.",
            "But I've managed to persuade you that you don't have to think about, you know complex parametric forms and complex prior distributions to be able to do a good job of regression, but actually just working with a."
        ],
        [
            "The joint Gaussian distribution over these things can be.",
            "Very effective in actually coming up with good answers to regression functions, right?",
            "Alright, that's it for today."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so welcome back.",
                    "label": 0
                },
                {
                    "sent": "I was originally scheduled to talk about Gaussian process, but as you can see I've changed my mind in the last minute.",
                    "label": 0
                },
                {
                    "sent": "About what to talk about.",
                    "label": 0
                },
                {
                    "sent": "So in the previous talks, we've all we've all had talks about.",
                    "label": 0
                },
                {
                    "sent": "You know big areas of machine learning, and this talk is going to be a little bit different.",
                    "label": 0
                },
                {
                    "sent": "It's going to be about 1 specific, one specific method, and there's a certain number of reasons why I would like to speak about this.",
                    "label": 0
                },
                {
                    "sent": "This particular method, so one of the reasons is that that Gaussian distributions you can do amazing things with Gaussian distributions, right?",
                    "label": 0
                },
                {
                    "sent": "And sometimes I think these are a little bit underappreciated.",
                    "label": 0
                },
                {
                    "sent": "So in this talk there's going to be.",
                    "label": 0
                },
                {
                    "sent": "Maybe no loopy messages.",
                    "label": 0
                },
                {
                    "sent": "There's going to be no negative variances.",
                    "label": 0
                },
                {
                    "sent": "There's going to be no nested plates or anything like that.",
                    "label": 0
                },
                {
                    "sent": "There's just going to be a Gaussian distribution, right?",
                    "label": 1
                },
                {
                    "sent": "And we'll see that there's a lot of mileage in a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So the second reason that I like to talk about this this model is that it's it's a nice model from if you're a proponent of the Bayesian way of doing things because you can actually compute everything and you can inside these models, although they are not trivial model, you can compute everything and it gives you a very nice window on what's actually going on in that methodology.",
                    "label": 0
                },
                {
                    "sent": "And the third reason is that Gaussian process turn out actually a very practical tool for solving real problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort 3.",
                    "label": 0
                },
                {
                    "sent": "Three justification, so I've talked to a lot of people over the last couple of days, and some of them have said, oh, you know, we've been.",
                    "label": 0
                },
                {
                    "sent": "We've been trying your software.",
                    "label": 0
                },
                {
                    "sent": "I've been looking at Gaussian process, so today things are going to be just the basics I'm going.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just talk about, you know what is a Gaussian process and and and how is it that we can use it in this in this amazing way.",
                    "label": 0
                },
                {
                    "sent": "So so the idea in the talk here is that you should be going away in an hour and a half from now.",
                    "label": 0
                },
                {
                    "sent": "Saying OK Gaussian Gaussian distributions are amazing things.",
                    "label": 0
                },
                {
                    "sent": "They're cool, right?",
                    "label": 0
                },
                {
                    "sent": "If you don't say that then then somehow having succeeded right?",
                    "label": 0
                },
                {
                    "sent": "OK, so so let's see how that goes.",
                    "label": 0
                },
                {
                    "sent": "So today I'm only talking about the basics right?",
                    "label": 0
                },
                {
                    "sent": "So, but on the other hand, it's supposed to be sort of.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is supposed to be fairly coherent, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have problems with what I'm saying then please please tell me so.",
                    "label": 0
                },
                {
                    "sent": "The next talk I'm going to give, I'll probably start ramping a bit more there, but today it's supposed to be understandable what I'm saying, right?",
                    "label": 0
                },
                {
                    "sent": "So do object if it's not so.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process models are, so I've spent a number of years talking to people about them, and I've noticed that they are kind of hard to understand and actually I myself had a hard time understanding them when I first started looking at them, so I probably took about so two years to actually really figure out what was going on.",
                    "label": 0
                },
                {
                    "sent": "So at the time I only had myself to ask and I didn't actually know the answer, so that make progress slow, right?",
                    "label": 0
                },
                {
                    "sent": "But you are in a nice situation that you can ask me, right?",
                    "label": 0
                },
                {
                    "sent": "So please do that.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so there are a number of subtleties.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure exactly why it's difficult to understand, probably part of the reason is that I'm not explaining it very well, but part of the reason is also just that the concepts are being used in slightly unusual ways, right?",
                    "label": 0
                },
                {
                    "sent": "So it takes a little bit of time to get around the idea, but please tell me when something is.",
                    "label": 0
                },
                {
                    "sent": "Is unclear if you don't tell me then I can't do anything about it, right?",
                    "label": 0
                },
                {
                    "sent": "So if you do tell me, let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've actually written a book about Gaussian process with Chris Williams.",
                    "label": 0
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately I haven't brought a bookstore.",
                    "label": 0
                },
                {
                    "sent": "So I guess my services Department should be told about this problem.",
                    "label": 0
                },
                {
                    "sent": "But if you don't like books, it also exists for free download on the web, so you can just get it from there.",
                    "label": 0
                },
                {
                    "sent": "Don't download it right now.",
                    "label": 0
                },
                {
                    "sent": "And the stuff that I'm going to talk about is all in the book, and there's some code also that goes through the book, and so I'll be using that if not today, then in the next lecture and showing you how how you can use this.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's get started so.",
                    "label": 0
                },
                {
                    "sent": "What do we?",
                    "label": 0
                },
                {
                    "sent": "What is the regression problem where the regression problem is a problem that you've got a bunch of data?",
                    "label": 0
                },
                {
                    "sent": "So here I've got the measured concentration CO2.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Centration atmospheric CO2 concentration measured over something like 40 years monthly measurement and you can see that things are rising here and there might be an interesting problem to try and predict.",
                    "label": 0
                },
                {
                    "sent": "You know what's going to happen in the future, right?",
                    "label": 0
                },
                {
                    "sent": "And our regression problem is to say, well, given this data, could I predict what's going to happen in the future and it's sort of clear that you should be able to predict something about what's happening in the future, right?",
                    "label": 0
                },
                {
                    "sent": "It's also clear that I couldn't ask well what's it going to look like in 2050, for example, or or 2100, right?",
                    "label": 0
                },
                {
                    "sent": "The data here doesn't really tell me very much about that, but it does tell me something about what's happening, you know?",
                    "label": 0
                },
                {
                    "sent": "At least for short, for short intervals here.",
                    "label": 0
                },
                {
                    "sent": "So this is this data up to 2004.",
                    "label": 0
                },
                {
                    "sent": "So that's the regression problem.",
                    "label": 0
                },
                {
                    "sent": "So now the title was something about solving regression problems.",
                    "label": 0
                },
                {
                    "sent": "So before we can before we can really talk about that will have to agree on what a solution to a regression problem is.",
                    "label": 0
                },
                {
                    "sent": "So here's an idea.",
                    "label": 0
                },
                {
                    "sent": "Here's a solution to arresting problem well, so this is just a linear fit to the data.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, is this a good?",
                    "label": 0
                },
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you have opinions about this.",
                    "label": 0
                },
                {
                    "sent": "It should be fairly obvious that this is bad for a number of reasons, so one of the reasons is that it is actually not fitting the data very well.",
                    "label": 0
                },
                {
                    "sent": "OK, this is saying that the concentration looks higher over here than it does over here, right?",
                    "label": 0
                },
                {
                    "sent": "So it is capturing something of the variability in the data, but there's clearly things in the data that is not capturing very well, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not fitting the data very well, so you might object.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two to the model on that grounds, which I would do too.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's get a little bit more serious.",
                    "label": 0
                },
                {
                    "sent": "So now I've fitted a data model which is.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic plus, a little sinusoidal term, right?",
                    "label": 0
                },
                {
                    "sent": "OK, now things look a little bit better.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, we are actually fitting a lot of the lot of variability, not everything, so there's things going on here that we're not capturing.",
                    "label": 0
                },
                {
                    "sent": "Things that we're doing here that we're not capturing.",
                    "label": 0
                },
                {
                    "sent": "But maybe this is a much more reasonable model.",
                    "label": 0
                },
                {
                    "sent": "But there's actually a problem.",
                    "label": 0
                },
                {
                    "sent": "There's a deep problem with this model.",
                    "label": 0
                },
                {
                    "sent": "Autumn is illustrated nicely.",
                    "label": 0
                },
                {
                    "sent": "If I instead of having a quadratic plus a sinusoidal, have a cubic plus sinusoidal.",
                    "label": 0
                },
                {
                    "sent": "Then things look like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so things look at so maybe we shouldn't be too worried about what's happening, what's going to happen in the future, right?",
                    "label": 0
                },
                {
                    "sent": "But how do I really know whether I should be using one model or the other, right?",
                    "label": 0
                },
                {
                    "sent": "The problem is here that my assumptions are sort of a little bit a little bit on unspecified at the moment, and also the model just predicts a number, right?",
                    "label": 0
                },
                {
                    "sent": "That's not very useful.",
                    "label": 0
                },
                {
                    "sent": "A good guy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models should tell us something about what the values are, but it should also tell us something about what the uncertainties are, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why what I mean by solving a nonlinear regression problems.",
                    "label": 0
                },
                {
                    "sent": "OK, we should get some reasonable predictions, but we should also get information about what the uncertainties are.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to use this example a little bit throughout the talk, not because I think this is a deeply serious way of actually modeling this data set, right?",
                    "label": 0
                },
                {
                    "sent": "So if you are really going to model this data set, then presumably you would want to know about other things that are going on at the same time.",
                    "label": 0
                },
                {
                    "sent": "For example, if you knew about, you know how the economy were doing, how was the world population doing?",
                    "label": 0
                },
                {
                    "sent": "How many people had downloaded David Mackay's book and important things like that might have an impact on what's going on up here, right?",
                    "label": 0
                },
                {
                    "sent": "So obviously you would use those those inputs as well, but unfortunately, if I did that, then I have a hard time actually plotting the function for you.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's really the only reason why I'm looking at this slightly stylized problem here.",
                    "label": 0
                },
                {
                    "sent": "OK, but I'm still trying to model the real data here.",
                    "label": 0
                },
                {
                    "sent": "Right, but the input here is just time, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a weird weird model from that perspective.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how do we solve?",
                    "label": 0
                },
                {
                    "sent": "How do we solve problems like that?",
                    "label": 0
                },
                {
                    "sent": "Well, so one idea is to say, well, we can use a parametric model and we can fit that in various ways.",
                    "label": 0
                },
                {
                    "sent": "So here just to introduce the notation I'm going to mention one of one of the ways you could do that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you have a data set here as a collection of inputs and outputs, I call them X&Y and you have a model which is somehow parameterized function which is parameterized by by some weights, right?",
                    "label": 0
                },
                {
                    "sent": "It could be a linear function, or it could be you know any function you care to dream up or you think of as a good idea in this.",
                    "label": 0
                },
                {
                    "sent": "And in this situation, and you have some noise here, which could have some distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume that it's a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If it's Gaussian, then you get your likelihood function.",
                    "label": 0
                },
                {
                    "sent": "They likely functioned as the probability of the data given the parameters.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're actually only modeling the outputs, so the probability of the data is just the probability outputs given the input.",
                    "label": 0
                },
                {
                    "sent": "We're not making a model explicitly of the inputs.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood function could look like this is the Gaussian, so it just looks at E to the minus half of the distance squared between the predicted output.",
                    "label": 0
                },
                {
                    "sent": "So this is a model of the outcomes of the model and the actual observations divided by a noise term here.",
                    "label": 0
                },
                {
                    "sent": "And one of the standard ways of going about fitting these things is to try and maximize the likelihood so you do the argmax or the likelihood function with respect to the parameters, and that is the maximum likelihood parameters.",
                    "label": 0
                },
                {
                    "sent": "And now when you come to make predictions, you stick those maximum likelihood weights back into your likelihood model, and that will give you a prediction here.",
                    "label": 0
                },
                {
                    "sent": "So I call a new a test input, the place where we want to make predictions.",
                    "label": 0
                },
                {
                    "sent": "I call that X star, and we can compute the probability the probability distribution over why star, what the CO2 level would be.",
                    "label": 0
                },
                {
                    "sent": "That new time extar given by conditioning on those maximum likelihood weights right there.",
                    "label": 0
                },
                {
                    "sent": "Other ways of doing this as well?",
                    "label": 0
                },
                {
                    "sent": "Let's not get hooked up on the details.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can also do other kinds of inference, so in particular Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "In that case things looks a little bit different.",
                    "label": 0
                },
                {
                    "sent": "You have the same setup here.",
                    "label": 0
                },
                {
                    "sent": "You have the same likelihood function, but now you also have a prior prior on the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you do inference, what you're supposed to do is you multiply the likelihood in the prior and renormalize to get the posterior distribution over the weights and the posterior is your friend.",
                    "label": 0
                },
                {
                    "sent": "And once you've got the posterior you can compute the things that you want to compute, so in particular you want to compute two types of things you want to compute prediction.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you want to know well what does the function do in other places?",
                    "label": 0
                },
                {
                    "sent": "In places where we didn't measure it, so to do predictions.",
                    "label": 0
                },
                {
                    "sent": "Here again, we use the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "We take.",
                    "label": 0
                },
                {
                    "sent": "What's the likelihood function say about those new predictions given particular weights, and then we average over the posterior distribution over weights, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a high posterior somewhere then you get a get a lot, give a large weight to those predictions, and if you have a lower posterior than those predicted predictions coming out of that particular setting of the model won't influence the actual.",
                    "label": 0
                },
                {
                    "sent": "Predictions camera so we've seen this before and another thing you might be able to might be interested in computing is the is the marginal likelihood margin, like it was important for model selection.",
                    "label": 0
                },
                {
                    "sent": "I'll talk much more about that later on, and again, the marginal likelihood is a is a.",
                    "label": 0
                },
                {
                    "sent": "Is a.",
                    "label": 0
                },
                {
                    "sent": "You get that by marginalizing over the distribution over the weights right?",
                    "label": 0
                },
                {
                    "sent": "So you can see the interesting things that you want to compute actually computed by doing integrals over the over the weights.",
                    "label": 0
                },
                {
                    "sent": "So, so This is why this is always happens in parametric models.",
                    "label": 0
                },
                {
                    "sent": "You can sort of see that.",
                    "label": 0
                },
                {
                    "sent": "Something a little bit funny about that, right?",
                    "label": 0
                },
                {
                    "sent": "We're not really interested in the weights like the weights are sometimes called nuisance parameters, right in these kinds of models, you're usually uninterested in the weights.",
                    "label": 0
                },
                {
                    "sent": "You just want to know what are the predictions, right?",
                    "label": 0
                },
                {
                    "sent": "So sometimes if you have a parametric model, you might actually be interested in what is the value of one of the parameters, but I'm not really talking about that case, right?",
                    "label": 0
                },
                {
                    "sent": "I'm talking about, you know, complex nonlinear relationships, and you want to know what is the prediction, right?",
                    "label": 0
                },
                {
                    "sent": "You're not interested in the weight, so the way Gaussian process models works is it explicitly says, well, the prior here.",
                    "label": 0
                },
                {
                    "sent": "Notice that the posterior here is nothing but the likelihood times the prior or is proportional to the likelihood times the prior, right?",
                    "label": 0
                },
                {
                    "sent": "So somehow the prior.",
                    "label": 0
                },
                {
                    "sent": "Is here a prior on weights right?",
                    "label": 0
                },
                {
                    "sent": "But the way it works is it induces a prior on the functions right?",
                    "label": 0
                },
                {
                    "sent": "If you set the weights to something particular then you'll get a particular function, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a distribution over those weights, then you'll get a distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do differently here is we're going to instead of going wild by the weights which were not interested in, we're going to go directly for a distribution over the things that we're interested in, right over the predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact, this is going to turn out turn out to be much simpler than going by this two step procedure of introducing model weights and then getting rid of them again.",
                    "label": 0
                },
                {
                    "sent": "So you might think that the way to actually actually get good results here is to look at complicated functions to dream up complicated nonlinear functions about of the weights.",
                    "label": 0
                },
                {
                    "sent": "Here to be able to do very advanced things and dream up complicated priors on the weights and try to somehow do these integral.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'm going to try and persuade you today is that that's not the way to do it, right?",
                    "label": 0
                },
                {
                    "sent": "The way to do it is different, and it turns out that it just involves a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So before I can tell you about that, let's make sure we're talking about the same Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So a Gaussian distribution has a location parameter.",
                    "label": 0
                },
                {
                    "sent": "I mean, and it has a stack.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deviation or variance.",
                    "label": 0
                },
                {
                    "sent": "Two samples here 02 Gaussian distributions in one dimension here.",
                    "label": 0
                },
                {
                    "sent": "And over here I've drawn a picture of a Gaussian distribution, 2 dimensions in particular correlated strongly anticorrelated.",
                    "label": 0
                },
                {
                    "sent": "So what I've drawn here, I have the two variables on these access here and this is equal probability contours, so there's a high probability inside this region and lower probability as you move out, and the probability goes down very rapidly in this direction, but it goes down much more slowly in this direction.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to use in the talk here, I'm going to use Gaussian distributions that live in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, unfortunately I can only draw a 2 dimensional Gaussian right, but we have to try and think of what do these distributions look like in much much higher dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "And that's going to be part of the challenge.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can't draw those things.",
                    "label": 0
                },
                {
                    "sent": "So what can you do with Gaussian distribution as well?",
                    "label": 0
                },
                {
                    "sent": "You can you can can dish.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So a conditional distribution means that if I now know somehow that the value of this variable has this particular value of this variable.",
                    "label": 0
                },
                {
                    "sent": "Sorry, has this particular value, then what does that tell me about the other variable, right?",
                    "label": 0
                },
                {
                    "sent": "And of course, if the Gaussian is correlated like this, then it tells me quite a lot in particular tells me if I have this value value.",
                    "label": 0
                },
                {
                    "sent": "Here it is very unlikely that my other variable would have have values over here.",
                    "label": 0
                },
                {
                    "sent": "In fact, the conditional distribution of the other variable.",
                    "label": 0
                },
                {
                    "sent": "Given that this variable is set to, this value is again Gaussian and it's centered over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's conditioning.",
                    "label": 0
                },
                {
                    "sent": "Another thing we can do is marginalization, so marginalization just means summing out or integrating out some of the of the variables and looking at just the district.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution of the other variables right in here again, in this 2 dimensional example here I just marginalized this joint Gaussian by summing in this direction and what I get back is again a Gaussian distribution, right?",
                    "label": 0
                },
                {
                    "sent": "And this turns out to be crucial that we can do both of these conditioning and Gaussian in marginalization.",
                    "label": 0
                },
                {
                    "sent": "And the results of those operations are again Gaussians.",
                    "label": 0
                },
                {
                    "sent": "I've also given you the equations here, so if I have if I have distributed joint Gaussian distribution over X&Y here with mean mean vector which contains A&B and a covariance which can be schematically written.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, then I've given you here the equations for the marginal distribution of one of the variables or the conditional distribution of one of the variables given the other variables, right?",
                    "label": 0
                },
                {
                    "sent": "But let's not look too much at the algebra right, but the slide is in there.",
                    "label": 0
                },
                {
                    "sent": "If you want to look at it later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "What about what?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So what is the Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "I've only been talking about Gaussian distribution so far, so Gaussian process is just a generalization of a Gaussian distribution to infinitely many many variables, and the reason why we want to do that is we want to do inference about functions right?",
                    "label": 1
                },
                {
                    "sent": "So we have to be able to put probability distributions or functions.",
                    "label": 0
                },
                {
                    "sent": "Now there's sort of an informal analogy here we can think of an infinitely long vector.",
                    "label": 0
                },
                {
                    "sent": "We can think of that as being a function, right?",
                    "label": 0
                },
                {
                    "sent": "If I, let's think of a 1 dimensional function.",
                    "label": 0
                },
                {
                    "sent": "If I want to specify what that one dimensional function is.",
                    "label": 0
                },
                {
                    "sent": "I should just specify what is the function value for each input.",
                    "label": 0
                },
                {
                    "sent": "OK, then I've specified what the function is.",
                    "label": 0
                },
                {
                    "sent": "The problem there might be that there are infinitely many possible inputs to the function, right?",
                    "label": 0
                },
                {
                    "sent": "So the vector will become very very long.",
                    "label": 0
                },
                {
                    "sent": "OK, but let's not worry about that for the time being.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, at least, the function is sort of like an infinitely long vector, right?",
                    "label": 0
                },
                {
                    "sent": "This is not mathematically very precise, but it turns out to be precise enough for what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use it so we simply think of functions as being infinitely long vectors OK.",
                    "label": 0
                },
                {
                    "sent": "So of course you might start getting an uncomfortable feeling now because, well, I can't write down these things in my computer.",
                    "label": 0
                },
                {
                    "sent": "Of course, right?",
                    "label": 0
                },
                {
                    "sent": "'cause they only have finite memory, so I better not try to do that right?",
                    "label": 0
                },
                {
                    "sent": "And I won't try to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so somehow will be able to do what we need to do without writing down those vectors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now what is the definition of a Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "So here's the definition of Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "A collection of random variables, any finite number of which have Gaussian distributions.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a sort of a mathematic mathematicians definition here.",
                    "label": 0
                },
                {
                    "sent": "So mathematicians always very cautious when they talk about infinite things here, so they don't actually say it's an infinite collection of random variables that have joint Gaussian distributions, because maybe that doesn't exist, whatever that might mean, right?",
                    "label": 0
                },
                {
                    "sent": "But just say any finite number of them should have Gaussian distributions, and then there.",
                    "label": 0
                },
                {
                    "sent": "Then there OK, right?",
                    "label": 0
                },
                {
                    "sent": "So there there.",
                    "label": 0
                },
                {
                    "sent": "They're always a bit fidgety when it comes to infinities, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "About comparable I don't think it matters.",
                    "label": 0
                },
                {
                    "sent": "I think we don't need to be.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to look into the mathematical details.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to get give you a high level intuition here, right?",
                    "label": 0
                },
                {
                    "sent": "I think it's uncountably infinite is probably the right right answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so a Gaussian distributed Gaussian process is this somewhat mysterious object, right?",
                    "label": 0
                },
                {
                    "sent": "Just think of it as being a very large Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, that analogy would be good enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what happens.",
                    "label": 1
                },
                {
                    "sent": "So whereas the Gaussian distribution is fully specified by a mean vector and covariance matrix, so it's like this.",
                    "label": 0
                },
                {
                    "sent": "So you have a Gaussian, a joint Gaussian distribution over over D dimensional object.",
                    "label": 0
                },
                {
                    "sent": "Or like this we have a mean vector and covariance matrix right in the mean vector will have the same length as the number of dimensions of F and the covariance matrix will be a matrix which has size D by D in that dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So what happens if we try to lift this up to the infinite dimensional thing?",
                    "label": 0
                },
                {
                    "sent": "OK, so now what does the vector?",
                    "label": 0
                },
                {
                    "sent": "What does it mean vector turn into?",
                    "label": 0
                },
                {
                    "sent": "If you want to try to follow this analogy?",
                    "label": 0
                },
                {
                    "sent": "Anyone?",
                    "label": 0
                },
                {
                    "sent": "It turns into a function, right?",
                    "label": 1
                },
                {
                    "sent": "So you need an infinitely long mean vector and we think of infinitely long vectors as being functions, right?",
                    "label": 0
                },
                {
                    "sent": "So the mean of a Gaussian process is a mean function.",
                    "label": 0
                },
                {
                    "sent": "And how about the covariance matrix?",
                    "label": 0
                },
                {
                    "sent": "That's right, it's a function is going to be a function of two variables, right?",
                    "label": 0
                },
                {
                    "sent": "So just like you normally think of this of covariance matrices as being indexed by the dimension Now, this is just doing exactly the same thing, But the index set here is are the values of X, which happens to be the inputs to the regression thing that we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so maybe I won't try to prove this that this thing actually really exists, but if it exists it should definitely have this kind of form right?",
                    "label": 0
                },
                {
                    "sent": "It should be, you know, a very long mean component here in this 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "A function of two arguments.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "Let's worry a little bit about this thing about it being infinitely big, so we can't write it down right?",
                    "label": 0
                },
                {
                    "sent": "So so we might.",
                    "label": 0
                },
                {
                    "sent": "This might be totally impractical, right?",
                    "label": 0
                },
                {
                    "sent": "But it turns out that it's not totally impractical and the reason for that is because of the marginalization property, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Number of that.",
                    "label": 0
                },
                {
                    "sent": "If you have a joint Gaussian, then you find the marginal distribution for some of the variables by just integrating out the ones that you're not interested in.",
                    "label": 0
                },
                {
                    "sent": "In particular, if I had a joint joint distribution between X is and why is here and the had mean for Forex would be a in the covariance of X would be capital A in the cross covariance between X&Y is B here, so if you think of this as being let's think of this as being being used in the case where the mean vector is infinitely long, right?",
                    "label": 0
                },
                {
                    "sent": "If I now happen to be interested in only a finite number of these entries, then I could just rearrange this thing to say, well, let's put the finite number of indexes up here and call them X, and then the infinitely many other indexes that I'm not interested in.",
                    "label": 0
                },
                {
                    "sent": "I'll put them down and be here OK, and they say, well, what's the distribution over the X is?",
                    "label": 0
                },
                {
                    "sent": "Well, the distribution over the X is just this good old fashioned Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, and it only depends on things that are that are finite.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you had the entries here, then this would be 1 by D vector in R&D by D covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is essentially why it's possible to work with these things.",
                    "label": 0
                },
                {
                    "sent": "Without using infinitely many infinitely infinite amounts of memory.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at an example of one of these things.",
                    "label": 0
                },
                {
                    "sent": "What these things actually look like.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a 1 dimensional Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So X is a 1 dimensional variable here, so I choose.",
                    "label": 0
                },
                {
                    "sent": "I have a Gaussian process distribution over these functions, so I have a problem.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Traditional functions now, which is a Gaussian process here and have a mean function, and I'm just going to use the function zero.",
                    "label": 1
                },
                {
                    "sent": "I'm going to use that throughout, but there's no reason why you should always do that.",
                    "label": 0
                },
                {
                    "sent": "But that's the simplest thing you can choose.",
                    "label": 0
                },
                {
                    "sent": "And here's a covariance function an for the covariance function.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use the.",
                    "label": 0
                },
                {
                    "sent": "This function here, so it's the exponential of the distance minus half the squared distance between X&X prime.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this intuition which is going on here is to say, well, if X&X prime are very close to each other.",
                    "label": 0
                },
                {
                    "sent": "Then the entry of the entry in the covariance is going to be is going to be large, but it's going to be close to 1 right?",
                    "label": 0
                },
                {
                    "sent": "If X&X prime are very close to each other, then the squared distance is close to 0 in each of the minus.",
                    "label": 0
                },
                {
                    "sent": "Something small is close to 1.",
                    "label": 0
                },
                {
                    "sent": "And as the distance between the X is grow, then the.",
                    "label": 0
                },
                {
                    "sent": "The covariances will fall, right?",
                    "label": 0
                },
                {
                    "sent": "So now how do we get?",
                    "label": 0
                },
                {
                    "sent": "How do we get a picture of what this kind of what this kind of object actually does?",
                    "label": 1
                },
                {
                    "sent": "What does it look like?",
                    "label": 0
                },
                {
                    "sent": "Well, the only thing we can actually do is we can say, well, what are the what does it do on a finite subset of our data, right?",
                    "label": 0
                },
                {
                    "sent": "Because we can't actually work with functions themselves, so let's just say, well, let's look at what's the distribution of the function.",
                    "label": 0
                },
                {
                    "sent": "Evaluate it at a set of fixed points here, right?",
                    "label": 0
                },
                {
                    "sent": "So I could choose a set of 10 points.",
                    "label": 0
                },
                {
                    "sent": "Then I could say, well, what does the distribution?",
                    "label": 0
                },
                {
                    "sent": "Over the function look like in that case.",
                    "label": 0
                },
                {
                    "sent": "So what do I need to do?",
                    "label": 0
                },
                {
                    "sent": "Well, I need to marginalized out all the other stuff right, and we know that's very simple.",
                    "label": 0
                },
                {
                    "sent": "We know that you just do that by retaining the part of the covariance that you're interested in, right?",
                    "label": 0
                },
                {
                    "sent": "So the the joint distribution of that vector of variables will just have a mean, which is the mean evaluated at those points, which is zero in this case, and the covariance will be the covariance matrix by just evaluating the covariance function at all the pairs of X&X prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so the entries of the covariance matrix will just be the covariance function evaluated at those pairs of points.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I've looked at.",
                    "label": 0
                },
                {
                    "sent": "I now look at it just a finite subset of the of the function values and I get back to just the Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, so how can we visualize what that Gaussian distribution looks like?",
                    "label": 0
                },
                {
                    "sent": "Well, one way of visualizing that is to actually draw samples from it and see what those samples look like.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll do that.",
                    "label": 0
                },
                {
                    "sent": "So what do I need to do?",
                    "label": 0
                },
                {
                    "sent": "I need to choose some values of X. OK, so I write down a bunch of values of X and then I need to evaluate the covariance so the covariance is just the function of the X is right.",
                    "label": 0
                },
                {
                    "sent": "So I just plug in, evaluate all the entries here by just plugging into this formula here and that will give me the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And then I draw a random sample from that distribution right?",
                    "label": 0
                },
                {
                    "sent": "And that random sample will be a vector which will have the length which we will be the number of points that I chose, the number of X is here right?",
                    "label": 1
                },
                {
                    "sent": "And I just plop those values Y as a function of those axes.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of doing that, right?",
                    "label": 0
                },
                {
                    "sent": "So I chose 20 points randomly chosen along the input here, and then I then I wrote down the covariance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drew a sample from that joint Gaussian distribution, and that was a 20 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And then I just plot the values that those 20 values as a function of the inputs here, right?",
                    "label": 0
                },
                {
                    "sent": "Functions be continuously sample from the distribution.",
                    "label": 0
                },
                {
                    "sent": "Will they turn out to be continuous function?",
                    "label": 0
                },
                {
                    "sent": "Yes, that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "My question is, are those functions continuous right?",
                    "label": 0
                },
                {
                    "sent": "We're stepping a little bit ahead of ahead of ourselves.",
                    "label": 0
                },
                {
                    "sent": "In fact, they would be continuous in this case.",
                    "label": 0
                },
                {
                    "sent": "But let's look at the samples.",
                    "label": 0
                },
                {
                    "sent": "We already see that.",
                    "label": 0
                },
                {
                    "sent": "Well, it looks as though there might be a function underneath this, right?",
                    "label": 0
                },
                {
                    "sent": "It looks it looks plausible that this really did define a distribution over functions, right?",
                    "label": 0
                },
                {
                    "sent": "Because I can actually show you pictures of samples drawn from that.",
                    "label": 0
                },
                {
                    "sent": "Drawn from that distribution now, if I had chosen, I could draw another sample and I would get different different random function from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And also I could choose a different covariance function.",
                    "label": 0
                },
                {
                    "sent": "Which is something that I'll talk more about, so I chose the covariance function E to the minus half of distance squared here.",
                    "label": 0
                },
                {
                    "sent": "Incidentally, so this looks a little bit Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "But this is not the reason why it's called a Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just the covariance function of a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "It happens to have something that looks like a Gaussian form, right?",
                    "label": 0
                },
                {
                    "sent": "I tend to call it squared exponential instead of Gaussian to point out that it's not a distribution, right?",
                    "label": 0
                },
                {
                    "sent": "OK, it is, although it has this sort of Gaussian Gaussian shape.",
                    "label": 0
                },
                {
                    "sent": "OK, so here was a here's a.",
                    "label": 0
                },
                {
                    "sent": "Here's a sample of that from that distribution over functions, right?",
                    "label": 0
                },
                {
                    "sent": "So hopefully this persuades you that there is.",
                    "label": 0
                },
                {
                    "sent": "Distribution over functions that we've actually divide.",
                    "label": 0
                },
                {
                    "sent": "Now one question is, you know, how do you actually generate samples from from things like this?",
                    "label": 0
                },
                {
                    "sent": "So one way of generating samples from a joint?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nelson, if you have a covariance matrix K Anna mean vector M as a little bit of two lines of octave or Matlab here.",
                    "label": 1
                },
                {
                    "sent": "So if you just draw standard random vector from the standard normal.",
                    "label": 0
                },
                {
                    "sent": "Random number generator and if you multiply that by essentially the Cholesky factor of the covariance matrix and add them in here, that will generate Gaussian Gaussian random vector which has covariance K an mean M. And actually, I've actually written down here why that's the case of the covariance.",
                    "label": 0
                },
                {
                    "sent": "Is the expectation of the difference between Y and its mean and multiplied by itself.",
                    "label": 0
                },
                {
                    "sent": "And if you plug in, what happens?",
                    "label": 0
                },
                {
                    "sent": "What happens here?",
                    "label": 0
                },
                {
                    "sent": "Then we get when we subtract the mean, the mean goes away here, and the sample here is just the Cholesky factor times the underlying set variable.",
                    "label": 0
                },
                {
                    "sent": "Here we want the expectation of this thing that ours are just constants, so they go outside and the expectation of said time said.",
                    "label": 0
                },
                {
                    "sent": "Transpose because their standard normal variables is just the unit matrix, so the covariance is K, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why this construction works, and I think this afternoon the people who are taking part in the practicals you will actually be forced to look at what this.",
                    "label": 0
                },
                {
                    "sent": "That's a very good idea, right?",
                    "label": 0
                },
                {
                    "sent": "So if you uncertain about you know how this thing works, then actually try to go on your laptop.",
                    "label": 0
                },
                {
                    "sent": "Actually try this construction and generate some functions, right?",
                    "label": 0
                },
                {
                    "sent": "It makes you feel a lot more at ease with this thing.",
                    "label": 0
                },
                {
                    "sent": "Let's try to explore this a little bit deeper, right?",
                    "label": 0
                },
                {
                    "sent": "It's a bit mysterious that you just have this construction, and then you do this sort of mathematical thing with this Gillespie decomposition.",
                    "label": 0
                },
                {
                    "sent": "And then this function pops out.",
                    "label": 0
                },
                {
                    "sent": "Like can we?",
                    "label": 0
                },
                {
                    "sent": "Can we get a little bit closer to that?",
                    "label": 0
                },
                {
                    "sent": "Yes, this is the main function, is like putting higher and what you expect.",
                    "label": 0
                },
                {
                    "sent": "Functions fly around.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the Gaussian process prior is is a distribution of our functions right into specified in terms of amine function, right?",
                    "label": 0
                },
                {
                    "sent": "So if you draw samples from that Gaussian process distribution, then on average they will have the mean function that the average over the functions will be the mean function.",
                    "label": 0
                },
                {
                    "sent": "Alright, if you put the MX is the quadratic.",
                    "label": 0
                },
                {
                    "sent": "That means you're assuming that it's like that's correct, so I was assuming here I was using a mean of zero.",
                    "label": 0
                },
                {
                    "sent": "That means when if I would average together all my functions then I would get this error function would estimate that also right?",
                    "label": 0
                },
                {
                    "sent": "You don't fix it.",
                    "label": 0
                },
                {
                    "sent": "You could do that, but the simplest thing is not to do it.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "The simplest thing is actually 90% of all Gaussian process work is done with a mean of zero.",
                    "label": 0
                },
                {
                    "sent": "I don't think that's necessarily the best thing to do, but you don't necessarily do so.",
                    "label": 0
                },
                {
                    "sent": "Will come back to how that can be.",
                    "label": 0
                },
                {
                    "sent": "It sounds mysterious, right?",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you can get all the modeling to take place in the covariance function, But this is a little bit surprising, so I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "OK so I was going to look a little bit closer at this generation process.",
                    "label": 0
                },
                {
                    "sent": "So, So what we're trying to do here is we're given a set of inputs we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to generate jointly from the joint distribution of all the outputs.",
                    "label": 0
                },
                {
                    "sent": "So there's a little trick you can use.",
                    "label": 0
                },
                {
                    "sent": "You can always decompose a joint distribution in terms of conditional distribution.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the following way you can say, well, let's choose, it's the product of the first marginal distribution of the first variable times the the conditional distribution of the second variable given the first one times their condition conditional distribution over the third one.",
                    "label": 0
                },
                {
                    "sent": "Given the two first ones and so forth.",
                    "label": 0
                },
                {
                    "sent": "But you can always decompose joint distribution like that.",
                    "label": 0
                },
                {
                    "sent": "So that it doesn't have to be Gaussian in order for you to do that.",
                    "label": 0
                },
                {
                    "sent": "So, so we can actually.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about writing it down this way is that you now get it as a product of 1 dimensional distribution, right?",
                    "label": 0
                },
                {
                    "sent": "And 1 dimensional distributions are much easier to get a good intuition for, and we can plot what they look like, what they look like, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "One way of generating jointly from this distribution is actually to generate this sequentially, right?",
                    "label": 0
                },
                {
                    "sent": "So first we pick a value for the first variable and then we pick a value for the second one condition on the first one and the equation we would.",
                    "label": 0
                },
                {
                    "sent": "We would need to use here would be.",
                    "label": 0
                },
                {
                    "sent": "We now need to know what is the conditional distribution of one of some set of variables given another set of variables.",
                    "label": 0
                },
                {
                    "sent": "And there's this rather large but closed form expression for what that distribution looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is also a good thing to try at home.",
                    "label": 0
                },
                {
                    "sent": "It's actually so good that I'll try it here.",
                    "label": 0
                },
                {
                    "sent": "OK so I have a tiny demo here.",
                    "label": 0
                },
                {
                    "sent": "Very very simple demo.",
                    "label": 0
                },
                {
                    "sent": "So again we have a 1 dimensional function of 1.",
                    "label": 0
                },
                {
                    "sent": "This is the output.",
                    "label": 0
                },
                {
                    "sent": "This is the input and what I'm going to do here is I'm going to sequentially generate from that conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so first I need to generate my first sample.",
                    "label": 0
                },
                {
                    "sent": "So my first sample would just be from the marginal distribution of one of the variables right?",
                    "label": 0
                },
                {
                    "sent": "And let's say my covariance function in this case was E to the minus distance squared, so the so it doesn't depend on the absolute position.",
                    "label": 0
                },
                {
                    "sent": "It's a translation invariant covariance function.",
                    "label": 0
                },
                {
                    "sent": "So what I've depicted here is what the distribution is of of what the marginal distribution is for any particular of these of these.",
                    "label": 0
                },
                {
                    "sent": "Infinitely many variables, of which I'll only look at a few.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I just pick an X at random.",
                    "label": 0
                },
                {
                    "sent": "OK, just chosen XI.",
                    "label": 0
                },
                {
                    "sent": "Picked it here, right?",
                    "label": 0
                },
                {
                    "sent": "So now what is the marginal distribution of the this distribution here?",
                    "label": 0
                },
                {
                    "sent": "So this is just the Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution is Gaussian, so I just pick a random number from that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, I picked a number over there close to the mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we go on to the next to the next variable.",
                    "label": 0
                },
                {
                    "sent": "So the next variable is picked over here.",
                    "label": 0
                },
                {
                    "sent": "So now what does the marginal distribution look like in this case?",
                    "label": 0
                },
                {
                    "sent": "Where the marginal distribution because I had the covariance function that said, where you have covariance between things that are close to you, right?",
                    "label": 0
                },
                {
                    "sent": "If X&X prime are close, then they covary, right?",
                    "label": 0
                },
                {
                    "sent": "And they can only call vary by having similar values, right?",
                    "label": 0
                },
                {
                    "sent": "So it's saying that if my new point was very close to the old point then they would have to covary, right?",
                    "label": 0
                },
                {
                    "sent": "If my point was over, here was very far away from the parents and the covariance between them would be very small.",
                    "label": 0
                },
                {
                    "sent": "Alright, so my new marginal distributions look like this and I just pick another X at random.",
                    "label": 0
                },
                {
                    "sent": "Pick it here and then I pick a random number.",
                    "label": 0
                },
                {
                    "sent": "From that, I'm going to say, OK, it was a fairly negative here.",
                    "label": 0
                },
                {
                    "sent": "And then I go on to the next one, OK?",
                    "label": 0
                },
                {
                    "sent": "So OK, my next, my next Canada is over here, but now we can see the marginal distribution over these points.",
                    "label": 0
                },
                {
                    "sent": "So now the inside this region, the predictions or the marginal distribution of outcomes here or condition on these two observations is going to is suddenly highly constrained right in saying, well the function cat do just whatever it likes in there because it has to covary with both of these guys right?",
                    "label": 0
                },
                {
                    "sent": "And the only way IT can do that is by lying in this in this interval.",
                    "label": 0
                },
                {
                    "sent": "And magically by this by this property that the conditional distribution of a Gaussian is again a Gaussian, those distributions are actually exactly Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "So we can continue doing this.",
                    "label": 0
                },
                {
                    "sent": "And you can sort of see that we are slowly uncovering this underlying function, right?",
                    "label": 0
                },
                {
                    "sent": "An whenever I get to make a prediction in a place where they essentially no so in place in place like this, for example, they essentially no variability left, like when I picked the random number from that distribution, I'm going to get one that lies exactly on the on the function, right?",
                    "label": 0
                },
                {
                    "sent": "So this of course is the same.",
                    "label": 0
                },
                {
                    "sent": "It's equivalent procedure to the one where you generate jointly, right you generate jointly.",
                    "label": 0
                },
                {
                    "sent": "Ann, you are jointly satisfying all the constraints.",
                    "label": 0
                },
                {
                    "sent": "That's what the Cholesky decomposition is doing for you.",
                    "label": 0
                },
                {
                    "sent": "OK. That's enough.",
                    "label": 0
                },
                {
                    "sent": "OK, and again, I'm very recommend you know you can do this in two or three lines of Matlab, and it's very, very instructive to actually watch this process is doing and how the Gaussian process is inducing this distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "OK, here's another picture of a function that I generated by doing exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "So in this case my my input space is a 2 dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "So now I'm map from A2 dimensions onto this onto one dimension.",
                    "label": 0
                },
                {
                    "sent": "So what did they actually do?",
                    "label": 0
                },
                {
                    "sent": "Well, I wrote down it's exactly the same covariance.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now instead of X -- X prime, I just have the XNXX prime squared.",
                    "label": 0
                },
                {
                    "sent": "I use the squared distance Euclidean distance squared between the two.",
                    "label": 0
                },
                {
                    "sent": "So what I actually did is I wrote down.",
                    "label": 0
                },
                {
                    "sent": "So this is 100 by 100 grid of input points.",
                    "label": 0
                },
                {
                    "sent": "So that means I have 10,000 input points.",
                    "label": 0
                },
                {
                    "sent": "OK, input points that lie on this grid.",
                    "label": 0
                },
                {
                    "sent": "And then I need to construct the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So the covariance matrix has an entry for every pair of input points.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a 10,000 by 10,000 covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And then I pick a random number from that 10,000 dimensional distribution and that point from that distribution will be a 10,000 dimensional vector and I plot those 10,000 values as a function of those.",
                    "label": 0
                },
                {
                    "sent": "100 by 100 and again you can see that this sort of gives rise to sort of interesting looking functions, and indeed it looks very much like these are are smooth functions or in fact for this covariance function you can show that these are infinitely differentiable functions.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "Gosha OK, so that was just this thing I said I wouldn't call it Gaussian, and then I called in Gaussian, so it's this squared exponential form.",
                    "label": 0
                },
                {
                    "sent": "So E to the minus distance squared, right?",
                    "label": 0
                },
                {
                    "sent": "But I don't like calling it a Gaussian because because it's not a distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, OK, very good.",
                    "label": 0
                },
                {
                    "sent": "So if I were giving people points you would have a few.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "So far so good.",
                    "label": 0
                },
                {
                    "sent": "So what I've done so far is I've sort of said, well, you know you can use Gaussian distributions to define distributions over functions, right?",
                    "label": 0
                },
                {
                    "sent": "So now I've got a vocabulary where I can start thinking about how to do inference right.",
                    "label": 0
                },
                {
                    "sent": "Up.",
                    "label": 0
                },
                {
                    "sent": "Until now we've only drawn random functions, right?",
                    "label": 0
                },
                {
                    "sent": "We're not interested in machine learning and random functions, right?",
                    "label": 0
                },
                {
                    "sent": "We're actually interested in stuff that has something to do with our data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the last little step that we have to do.",
                    "label": 0
                },
                {
                    "sent": "I'll have to make sure that my function somehow has something to do with the data that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "So how do we do learning in these things?",
                    "label": 0
                },
                {
                    "sent": "OK, so so now we have to do some inference.",
                    "label": 0
                },
                {
                    "sent": "So we already looked at this slide.",
                    "label": 0
                },
                {
                    "sent": "This is the how to do inference in a in a parametric model.",
                    "label": 0
                },
                {
                    "sent": "And we also looked at the Bayesian inference here.",
                    "label": 0
                },
                {
                    "sent": "So again, what?",
                    "label": 0
                },
                {
                    "sent": "What normally happens in parametric models?",
                    "label": 0
                },
                {
                    "sent": "Although our model is now is a nonparametric model will normally happens here is that you put a prior on the weights.",
                    "label": 0
                },
                {
                    "sent": "Then you construct the posterior over the weights.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you use that posterior to compute the things that you're interested in, right?",
                    "label": 0
                },
                {
                    "sent": "And you're interested in making new predictions.",
                    "label": 0
                },
                {
                    "sent": "So that's an integral over the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Or you might be introduced interested in the marginal likelihood, the marginal likelihood being the product of the prior and the likelihood.",
                    "label": 0
                },
                {
                    "sent": "This is an unnormalized posterior, so again, it's it's an integral over the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "We can also do.",
                    "label": 0
                },
                {
                    "sent": "We can also compute the probability of the model if we have different different sets of models, and indeed this is what we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to do now.",
                    "label": 0
                },
                {
                    "sent": "The problem here is that for most interesting models, these into.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both are are intractable, right?",
                    "label": 0
                },
                {
                    "sent": "So we can't actually compute what's going on.",
                    "label": 0
                },
                {
                    "sent": "But Interestingly, Gaussian process are tractable and interesting.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some models where you can exactly do those integrals right?",
                    "label": 0
                },
                {
                    "sent": "So how does that work now?",
                    "label": 0
                },
                {
                    "sent": "Firstly, there's a little bit of a problem here, right?",
                    "label": 0
                },
                {
                    "sent": "Because where are the weights like?",
                    "label": 0
                },
                {
                    "sent": "I'm just saying, well, we're just using a prior over the functions and then we have some data.",
                    "label": 0
                },
                {
                    "sent": "The data also tells you something about what the function is, and then we're supposed to combine those those two.",
                    "label": 0
                },
                {
                    "sent": "But how do we apply this methodology?",
                    "label": 0
                },
                {
                    "sent": "When like where?",
                    "label": 0
                },
                {
                    "sent": "Where are the parameters like where is the model here, right?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The right answer to that question is that the parameters in our model is the function itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so that sounds that sounds a little bit weird, right?",
                    "label": 0
                },
                {
                    "sent": "But what we're doing inference over is just the function, right?",
                    "label": 0
                },
                {
                    "sent": "And so if we plug into our usual framework, if we just plug in every time we saw the parameters before, we'll just stick in the function and let's see what happens right.",
                    "label": 0
                },
                {
                    "sent": "After all, we want to do inference about the function.",
                    "label": 0
                },
                {
                    "sent": "The likelihood tells you something about what the function is doing and the prior to tell you something about what the function is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have so.",
                    "label": 0
                },
                {
                    "sent": "So we have the same ingredients as before.",
                    "label": 0
                },
                {
                    "sent": "We just turn the crank on what we're supposed to be doing.",
                    "label": 0
                },
                {
                    "sent": "So what about the likelihood function so we have again a Gaussian likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So likelihood function tells you what's the probability of the observed data given the parameters.",
                    "label": 0
                },
                {
                    "sent": "So now that will be the.",
                    "label": 0
                },
                {
                    "sent": "We're not modeling the inputs here, it's the prob.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See of the story I change notational it, so this is shorthand for P of the thing that says this is a conditional probability.",
                    "label": 0
                },
                {
                    "sent": "I didn't write the piece anymore.",
                    "label": 0
                },
                {
                    "sent": "Write everything.",
                    "label": 0
                },
                {
                    "sent": "I write down these probabilities.",
                    "label": 0
                },
                {
                    "sent": "So if there's not a piece missing.",
                    "label": 0
                },
                {
                    "sent": "So, so the likelihood function is the probability of the data given the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's write that down so it's the probability of the data and we only interested in modeling the output here its probability of outputs given the data in the data is now the function.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I condition on the function I put in FF of X here right?",
                    "label": 0
                },
                {
                    "sent": "And what is that thing?",
                    "label": 0
                },
                {
                    "sent": "Well, the likelihood tells us something about if the true function is something, then what's the probability of observing the thing that you actually observed, right?",
                    "label": 0
                },
                {
                    "sent": "And that's if the if the noise that depends on the noise assumption, right?",
                    "label": 0
                },
                {
                    "sent": "The noise assumption is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then it says.",
                    "label": 0
                },
                {
                    "sent": "Well, then that likelihood that probability is going to be proportional to E to the minus the distance between the actual function and that observed value.",
                    "label": 0
                },
                {
                    "sent": "So that yes.",
                    "label": 0
                },
                {
                    "sent": "So the question was.",
                    "label": 0
                },
                {
                    "sent": "The parameters in our data data function.",
                    "label": 0
                },
                {
                    "sent": "Sorry, So what I maybe maybe I said it wrong so the likelihood function is the probability of the data given the parameters OK, and now we've now substituting the function for the parameters and the probability of the data is only the output part that we're really modeling, right so?",
                    "label": 0
                },
                {
                    "sent": "OK, sorry if I said that wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so something interesting happens here, right?",
                    "label": 0
                },
                {
                    "sent": "Because we actually only the likelihood function is only interested in what the function does at the locations where we have data, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't say it doesn't actually depend on what the function does out here where there was no data right?",
                    "label": 0
                },
                {
                    "sent": "So although although when we write it down, it looks as though the likelihood function cares about what the function does everywhere, that happens not to be the case, right?",
                    "label": 0
                },
                {
                    "sent": "It actually only depends on what the function is doing at the places where we where we made observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so the likelihood function actually only depends on the values of X at the input location, so I use BF F. As a vector of those of those F values.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is just a shorthand notation for for a Gaussian distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So the Gaussian here is centered on F and it has whatever the observation noises.",
                    "label": 0
                },
                {
                    "sent": "So that's the likelihood function, yes, so from my ex Now you mean the mini mineplex and the covariance of X.",
                    "label": 0
                },
                {
                    "sent": "Right now here it just mean I just mean what the function is, so the function is unknown, just like the parameters are unknown when we're doing inference, so this is just, you can just think of this as being sort of an abstract notation for this stuff that we don't know.",
                    "label": 0
                },
                {
                    "sent": "We don't know what F is, right?",
                    "label": 0
                },
                {
                    "sent": "But we do know that F the places where we have observations F should be close to those observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have to put a prior on our on our parameters right now promises in our function, but Luckily we know how to put distributions over functions, right?",
                    "label": 0
                },
                {
                    "sent": "So let's put a Gaussian process prior on the functions.",
                    "label": 0
                },
                {
                    "sent": "So our prior here is the probability of the parameters given the given the model class.",
                    "label": 0
                },
                {
                    "sent": "Here if we want and we've got just going to use a Gaussian process prior and we can choose any any Gaussian process that we want.",
                    "label": 1
                },
                {
                    "sent": "Of course, I'll have to tell you about ways of making smart choices about this, right?",
                    "label": 0
                },
                {
                    "sent": "We'll come back to that right.",
                    "label": 0
                },
                {
                    "sent": "But formally speaking, we just need to specify what that Gaussian process prior looks like in a parameterized in terms of mean function, which in this case I'll just set to 0 again and covariance function.",
                    "label": 0
                },
                {
                    "sent": "Yes, there was a question about.",
                    "label": 0
                },
                {
                    "sent": "That website and the function that it's using.",
                    "label": 0
                },
                {
                    "sent": "Then set up your observing the value of X. Yeah, so I'm sure this was a little little shorthand, maybe a little bit too short for.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the at the likelihood function again, right?",
                    "label": 0
                },
                {
                    "sent": "So the likelihood function basically says it's a product over terms and each of the terms as this form and it's interested in the squared difference between the observation and what the function is doing at that point.",
                    "label": 0
                },
                {
                    "sent": "But notice that the total likelihood here only cares about what the function does at the locations where we made observations.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is just my very compact notation for the for the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "It's a joint Gaussian centered on the F values.",
                    "label": 0
                },
                {
                    "sent": "With this amount of noise.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we need the we need the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We've got some pro prior over function so that will give that Gaussian process prior and see what happens.",
                    "label": 0
                },
                {
                    "sent": "Now what we need to do, we need to compute the posterior right.",
                    "label": 0
                },
                {
                    "sent": "We do that by multiplying the prior with the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So now the likelihood is just a just a regular Gaussian here, and the prior is this funny kind of Gaussian is infinite dimensional Gaussian OK, But let's just go ahead right?",
                    "label": 0
                },
                {
                    "sent": "So we multiply 2 Gaussians, we get another Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We got an unnormalized Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "But when you multiply 2 regular Gaussians, we get one of these, we gotta and normalized regular Gaussian if you multiply.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian with one of these infinitely large Gaussians, we just get an infinitely large Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK. And formally, this is just using the standard standard way of multiplying together 2 Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But what does it actually mean intuitively?",
                    "label": 0
                },
                {
                    "sent": "Now the posterior distribution is what it's a.",
                    "label": 0
                },
                {
                    "sent": "It said it's an infinite dimensional Gaussian, so why does that make sense?",
                    "label": 0
                },
                {
                    "sent": "Well, it makes sense because it's supposed to be a distribution of our functions, right?",
                    "label": 0
                },
                {
                    "sent": "And a Gaussian process is a distribution or functions.",
                    "label": 0
                },
                {
                    "sent": "Like so we started with with with a random distribution or functions and then we said, well, we now have some data that is telling us something about what the function is doing and that also had a Gaussian form right?",
                    "label": 0
                },
                {
                    "sent": "We multiplied together those two and then we got this infinite dimensional Gaussian here.",
                    "label": 0
                },
                {
                    "sent": "So now, so how do we actually use this thing right?",
                    "label": 0
                },
                {
                    "sent": "We use this thing to make predictions.",
                    "label": 0
                },
                {
                    "sent": "And how do we make predictions?",
                    "label": 0
                },
                {
                    "sent": "Well, the predictions are just an integral over the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Or we just we just want to marginalized out the values of F that we're interested in here, right?",
                    "label": 0
                },
                {
                    "sent": "So if we choose test point X star, we say, OK, let's just modernize out the rest of the of the infinitely many variables and just look at the one that we're interested in, right?",
                    "label": 0
                },
                {
                    "sent": "And then we do that by just throwing away the parts of the covariance that we're not interested in.",
                    "label": 0
                },
                {
                    "sent": "OK, I won't drag you through the actual algebra, but this is.",
                    "label": 0
                },
                {
                    "sent": "This is the only thing which is going on right and when you get the predictions, the predictions here just comes down to this one dimensional Gaussian that has a mean which is just a number.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a vector times matrix times vector here and again the variance is just a number.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just a 1 dimensional Gaussian distribution, yes?",
                    "label": 0
                },
                {
                    "sent": "Evaluated.",
                    "label": 0
                },
                {
                    "sent": "That's correct, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is, I've computed what the posterior distribution over the predicted value is at a particular point, but I was free to choose point anyway anyway I want right?",
                    "label": 0
                },
                {
                    "sent": "So I just compute this, sorry.",
                    "label": 0
                },
                {
                    "sent": "Up here, yeah.",
                    "label": 0
                },
                {
                    "sent": "The function evaluated at no no this is.",
                    "label": 0
                },
                {
                    "sent": "This is the general.",
                    "label": 0
                },
                {
                    "sent": "This is the posterior distribution over the function.",
                    "label": 0
                },
                {
                    "sent": "Right, this is not a particular X here.",
                    "label": 0
                },
                {
                    "sent": "This is the X.",
                    "label": 0
                },
                {
                    "sent": "Here is just just just a placeholder.",
                    "label": 0
                },
                {
                    "sent": "Like this equation works for any value of X.",
                    "label": 0
                },
                {
                    "sent": "Yes, you just said girls in process is in distribution or functions and I remember the definition goes in process collection of random variables in a number of which have goes in distributions.",
                    "label": 0
                },
                {
                    "sent": "So I just can you remind please help why is ghosting, processing, distribution or function which functions?",
                    "label": 0
                },
                {
                    "sent": "Well, the distribution of functions because you can choose to look at you know what is the function doing at any number of locations and any number of locations, the distribution or the function values are jointly Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's what the Gaussian process says, right?",
                    "label": 0
                },
                {
                    "sent": "Let me take a few more questions.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to draw pictures of what this actually means, right?",
                    "label": 0
                },
                {
                    "sent": "'cause algebra is not so easy to digest, so let's take a few more questions.",
                    "label": 0
                },
                {
                    "sent": "Matrix and we have 10s or 100,000 data points into huge matrix.",
                    "label": 0
                },
                {
                    "sent": "So there's a question about you know how?",
                    "label": 0
                },
                {
                    "sent": "How big is the matrix that we have to compute right?",
                    "label": 0
                },
                {
                    "sent": "And it's true that the matrix here is depends on the covariance matrix here, which has the dimension which is the number of points squared, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have 10,000 training points, then this matrix is a big matrix, right?",
                    "label": 0
                },
                {
                    "sent": "That's true, but it's not an infinitely big matrix, right?",
                    "label": 0
                },
                {
                    "sent": "It only scales well, scales quadratically with the number of data points, right?",
                    "label": 0
                },
                {
                    "sent": "So if you want to use this for very very large data datasets, then you might not want to use this naive implementation, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Very noisy, able becausw.",
                    "label": 0
                },
                {
                    "sent": "Now there is you already have a prior over the functions which have some variance, so it seems like there is some duplicacy like there is a Sigma squared noise and there is this noise in the face, so it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "So the question is you know do we need the Sigma squared and and what is the what is the role of the Sigma squared?",
                    "label": 0
                },
                {
                    "sent": "How would I specify the signal?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to get back to that right?",
                    "label": 0
                },
                {
                    "sent": "That's an excellent question.",
                    "label": 0
                },
                {
                    "sent": "Where do we get that from?",
                    "label": 0
                },
                {
                    "sent": "Right obviously I'll have to tell you how to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at a pictorial representation of what this actually looks like.",
                    "label": 0
                },
                {
                    "sent": "So on this panel over here I've drawn.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three independently drawn 3 random functions from this from this Gaussian process and in the blue.",
                    "label": 0
                },
                {
                    "sent": "Here I've actually done things properly right.",
                    "label": 0
                },
                {
                    "sent": "I've actually written down a set of X is an, then written down the covariance for those X is drawn a point from that from that Gaussian and plotted those values as a function of the X values, right?",
                    "label": 0
                },
                {
                    "sent": "So here in the green and the right I've been a little bit more sloppy.",
                    "label": 0
                },
                {
                    "sent": "I've just joined up the points with lines, right?",
                    "label": 0
                },
                {
                    "sent": "Because I sort of know that there's an underlying smooth function there.",
                    "label": 0
                },
                {
                    "sent": "OK, and what I've shown in this in the shaded area here I'm showing you know what is the mean, which is 0 here plus minus 2 standard deviations, right?",
                    "label": 0
                },
                {
                    "sent": "And that's this area where the function should lie in with with 95% confidence.",
                    "label": 0
                },
                {
                    "sent": "So these are these are the random functions.",
                    "label": 0
                },
                {
                    "sent": "This is draw.",
                    "label": 0
                },
                {
                    "sent": "These are draws from the prior Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So then I observe I've made a number of observations.",
                    "label": 0
                },
                {
                    "sent": "I made five observations here that I've plotted with the pluses here and now I update my Gaussian process by multiplying Gaussian process with the Windows 5 likelihood terms and that gives me a new Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "The posterior process and you can see that the posterior process, as indeed we've seen from sampling from the prior.",
                    "label": 0
                },
                {
                    "sent": "Actually is forced to go through those data points right?",
                    "label": 0
                },
                {
                    "sent": "Because we, because the likelihood enforces that.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's a likelihood with a very small noise variance, so it's forced to actually agree with the data points at those points.",
                    "label": 0
                },
                {
                    "sent": "And again, I've drawn 3 random functions from that distribution in colors here, and I've also shown in Gray this what the error bars are.",
                    "label": 0
                },
                {
                    "sent": "In order to do this plot, I just plug into this equation down here.",
                    "label": 0
                },
                {
                    "sent": "In this case, I would have to draw.",
                    "label": 0
                },
                {
                    "sent": "I'd have to.",
                    "label": 0
                },
                {
                    "sent": "Compute here to compute this.",
                    "label": 0
                },
                {
                    "sent": "This distribution here I would have to foreign X star, let's say 2.5 or something like that.",
                    "label": 0
                },
                {
                    "sent": "I just plug in X dollar is equal to 2.5 and I plug in.",
                    "label": 0
                },
                {
                    "sent": "This is just the covariance function evaluated at 25 and all the ex is there 5X is here.",
                    "label": 0
                },
                {
                    "sent": "There's a 5 by 1 vector.",
                    "label": 0
                },
                {
                    "sent": "This is a 5 by 5 matrix and these are the vector of the observed values here.",
                    "label": 0
                },
                {
                    "sent": "OK, and similarly for the for the covariance.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a pictorial representation of how the prior distribution or functions which didn't tell us very much about what was going on, gets updated when we actually see the data.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about that?",
                    "label": 0
                },
                {
                    "sent": "This piece that's correct.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah.",
                    "label": 0
                },
                {
                    "sent": "And in practice we tend not to believe our data that much right?",
                    "label": 0
                },
                {
                    "sent": "So in practice you would probably have a likelihood function that said that the observation noise is not non 0 right, and then you would get a similar picture to this except that the distribution wouldn't go have a variance that exactly went to zero, but it would go down to the smallest.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's correct, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I said it's something very very small so you can't see it right?",
                    "label": 0
                },
                {
                    "sent": "OK. Good, so let's try to relate this.",
                    "label": 0
                },
                {
                    "sent": "This kind of model to some of the other models we've been looking at.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a graphical model representation of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So we have these.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the in this kind of notation, the function values here the true underlying function values are the latent variables like we don't get to absorb those right?",
                    "label": 0
                },
                {
                    "sent": "And because the the Gaussian process prior they had their jointly have a joint Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "So that means that all of the latent variables are connected to each other, right?",
                    "label": 1
                },
                {
                    "sent": "So if we're looking back at the graphical models framework, this is sort of telling us that that's bad, right?",
                    "label": 0
                },
                {
                    "sent": "Because it says that there's no, there's going to be no computational speed up there, no?",
                    "label": 0
                },
                {
                    "sent": "Conditional independence relationships here, right?",
                    "label": 0
                },
                {
                    "sent": "Because you got a conditional independence when you had an absent absence of a link, right?",
                    "label": 0
                },
                {
                    "sent": "And all these variables are linked together, and each of the each of the variables here is, well, the variable does depends on where it is.",
                    "label": 0
                },
                {
                    "sent": "It depends on on X here and on the on this side I've shown the training examples, so we've actually observed what the observation is.",
                    "label": 0
                },
                {
                    "sent": "So that somehow constrains what this variable can do, because the likelihood says or you better be close to my observation here.",
                    "label": 0
                },
                {
                    "sent": "Right and then they are coupled with all the other variables, although training variables and they also coupled to the test variables and for the test variables.",
                    "label": 0
                },
                {
                    "sent": "We haven't observed what the corresponding target is, right?",
                    "label": 0
                },
                {
                    "sent": "So these are in in circular nodes here.",
                    "label": 0
                },
                {
                    "sent": "And so that means that if we would actually end, we would put in other test points here if I would add another test point then that wouldn't change the distribution over all these things right?",
                    "label": 0
                },
                {
                    "sent": "Because it would it wouldn't care about what the function was doing, so it's only when you actually add an observation of that.",
                    "label": 1
                },
                {
                    "sent": "That of the corresponding value that you influence the distribution over the over the latent variable.",
                    "label": 1
                },
                {
                    "sent": "And that's again just the same as the marginalization property, right?",
                    "label": 0
                },
                {
                    "sent": "If you want want to write down the diagram for the whole Gaussian process, you would have to have an infinite number of these nodes, right?",
                    "label": 0
                },
                {
                    "sent": "But because most of the nodes don't correspond to any observation, they don't change the distribution of the remaining number of nodes, so you don't have to put them in the graph.",
                    "label": 0
                },
                {
                    "sent": "And let's examine so these are two 2.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Components here the mean and the variance.",
                    "label": 1
                },
                {
                    "sent": "The mean looks like this right?",
                    "label": 0
                },
                {
                    "sent": "So the mean is a has this particular form and you can write that this is Lena in two particular ways, right?",
                    "label": 1
                },
                {
                    "sent": "One way is that it's a linear in the observations, right?",
                    "label": 0
                },
                {
                    "sent": "So if I multiply the vector here of covariances between the test input and the training inputs by this?",
                    "label": 0
                },
                {
                    "sent": "Inverse covariance matrix here and I call those values beta here.",
                    "label": 0
                },
                {
                    "sent": "Then it's just the dot product between the beta vector in the observation.",
                    "label": 0
                },
                {
                    "sent": "So this is what the statisticians call called a linear method right?",
                    "label": 0
                },
                {
                    "sent": "Because the prediction the mean prediction is going to be a linear function of the observations.",
                    "label": 0
                },
                {
                    "sent": "OK, another way that is linear is if we now collect the Y times the matrix here and then we can see that it's a linear function of the covariance function evaluated at X and the training cases.",
                    "label": 0
                },
                {
                    "sent": "And actually you might you might sort of recognize this from one of bound hearts.",
                    "label": 0
                },
                {
                    "sent": "Last slides that in the support vector machine the prediction that you make or the outcome is actually a linear combination of the kernel evaluated at those points, and that turns out to be.",
                    "label": 0
                },
                {
                    "sent": "An extremely tight relationship between every time bound says kernel.",
                    "label": 0
                },
                {
                    "sent": "I say covariance function.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "And and one can think of a Gaussian process as being also a method that works in an expanded space.",
                    "label": 0
                },
                {
                    "sent": "In this feature space and the mean prediction comes out as the as the output of the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "But I have something extra here.",
                    "label": 0
                },
                {
                    "sent": "I have a probabilistic model, right?",
                    "label": 0
                },
                {
                    "sent": "So I also have accompanied by my mean prediction here.",
                    "label": 0
                },
                {
                    "sent": "I also have a variance.",
                    "label": 0
                },
                {
                    "sent": "And the variance also has an interesting form, so the variance look like this.",
                    "label": 0
                },
                {
                    "sent": "It's the difference between two terms here, so this thing here is the covariance function evaluated between XR&X Tower itself, right?",
                    "label": 1
                },
                {
                    "sent": "And this is just the prior variance like this doesn't refer to the data at all right?",
                    "label": 0
                },
                {
                    "sent": "To the observations.",
                    "label": 0
                },
                {
                    "sent": "This is the prior covariance.",
                    "label": 1
                },
                {
                    "sent": "So this is the covariance you would have when you haven't seen any data.",
                    "label": 0
                },
                {
                    "sent": "So if we look back at this kind of plot here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the prior variance in this case, because the covariance function stationary.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on the location.",
                    "label": 0
                },
                {
                    "sent": "So that's a prior variance, so that's the variance you would have if you hadn't seen any data, right?",
                    "label": 0
                },
                {
                    "sent": "That's the variance you had before you start.",
                    "label": 0
                },
                {
                    "sent": "Then you get to subtract something from that variance.",
                    "label": 0
                },
                {
                    "sent": "You get to subtract something which is a quadratic form, so you get to subtract something which is always positive.",
                    "label": 0
                },
                {
                    "sent": "And you get to subtract what the stuff you get to subtract here has to do with how much does the training cases tell you about what the function is at that value, right?",
                    "label": 0
                },
                {
                    "sent": "The more the training cases tell you about what the function is doing there, the smaller the predicted variance is going to be right, and you can sort of see that that's happening here, so you get large values for the covariance, at least if your covariance function is a squared exponential, you get large values for these things here when the test inputs are close to the training inputs, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the places where it's telling you a lot about what the function is doing right, and then the predicted variance will be small in those areas.",
                    "label": 0
                },
                {
                    "sent": "But if you're predicting far away from the data from your training data, then you just get back that you get the prior variance.",
                    "label": 0
                },
                {
                    "sent": "Pression looks a bit late in Kernel Ridge regression, yes, so the point is, this looks a little bit like Kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "This is Kernel Ridge regression, right?",
                    "label": 1
                },
                {
                    "sent": "That's a different name for.",
                    "label": 0
                },
                {
                    "sent": "For this method in Kernel Ridge regression, people usually concentrate on the on what the mean function is doing right, but there's no difference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So actually Gaussian process regression has quite a few names because it's been invented over and over again, right?",
                    "label": 0
                },
                {
                    "sent": "So it's also called creaking models.",
                    "label": 0
                },
                {
                    "sent": "So if any of you have bumped into those before is exactly the same idea.",
                    "label": 0
                },
                {
                    "sent": "Creating so in the geostatistics literature, it's sort of the standard model.",
                    "label": 0
                },
                {
                    "sent": "For some reason, no other community knows about it, except maybe for the machine learning community these days, and it's called creaking.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so how come the rights doesn't depend on the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's a.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting point here which is.",
                    "label": 0
                },
                {
                    "sent": "If you look at this expression, this expression doesn't actually depend on the observations, right.",
                    "label": 0
                },
                {
                    "sent": "The observations are the Y values, so how can it be that the predicted variance doesn't depend on the observations, right?",
                    "label": 0
                },
                {
                    "sent": "And this is just a property of the way this thing works.",
                    "label": 0
                },
                {
                    "sent": "You would certainly imagine that it would somehow depend on how closely observed targets were, and if it if the targets were far away, you would maybe assume that that would mean that the distribution would be wide or something like that.",
                    "label": 0
                },
                {
                    "sent": "But because basically because.",
                    "label": 0
                },
                {
                    "sent": "These things are Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "If you have a neighbor which is Gaussian with a particular noise level, then the variance of your of your of the underlying function at that point will just be a product of those constraints.",
                    "label": 0
                },
                {
                    "sent": "Right, so so it doesn't actually matter.",
                    "label": 0
                },
                {
                    "sent": "You know what those values are, you just have to the width of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you multiply 2 Gaussians, it doesn't matter whether you know they are far apart or they are on top of each other.",
                    "label": 0
                },
                {
                    "sent": "The width is just this.",
                    "label": 0
                },
                {
                    "sent": "Some of the provisions or the precision is the sum of the positions.",
                    "label": 0
                },
                {
                    "sent": "So this is a consequence of this, gaussianity.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily clear that this is a good thing.",
                    "label": 0
                },
                {
                    "sent": "Something to be aware of, right?",
                    "label": 1
                },
                {
                    "sent": "How much the data X as an expense?",
                    "label": 0
                },
                {
                    "sent": "Well, this is my sort of informal way of explaining what this term is doing right, and we saw that on the pots that the more the closer you had, the more observations you had in the closer you are to the observation, the smaller there Bobby came right.",
                    "label": 0
                },
                {
                    "sent": "And this is just my interpretation of what's going on.",
                    "label": 0
                },
                {
                    "sent": "So maybe like how much excess coverage over.",
                    "label": 0
                },
                {
                    "sent": "Jose are from the desert.",
                    "label": 0
                },
                {
                    "sent": "That's right, right?",
                    "label": 0
                },
                {
                    "sent": "And it's measured somehow in terms of what the covariance function is saying.",
                    "label": 0
                },
                {
                    "sent": "OK, so we haven't.",
                    "label": 0
                },
                {
                    "sent": "Actually, we haven't actually really.",
                    "label": 0
                },
                {
                    "sent": "I'm not really told you how you turn this kind of idea into a practical framework told you so far as to say, well, we can have this distribution or functions.",
                    "label": 0
                },
                {
                    "sent": "We can define distributions or function.",
                    "label": 0
                },
                {
                    "sent": "We can also define distributions that are somehow conditional on making observations, but I didn't tell you anything about you know how do you come up with this covariance function?",
                    "label": 0
                },
                {
                    "sent": "And what about those?",
                    "label": 0
                },
                {
                    "sent": "All those other parameters that were floating around like somebody was saying?",
                    "label": 0
                },
                {
                    "sent": "You know what about Sigma squared?",
                    "label": 0
                },
                {
                    "sent": "How do you get that?",
                    "label": 0
                },
                {
                    "sent": "How do you get the noise?",
                    "label": 0
                },
                {
                    "sent": "Value so I have to tell you what to do about those.",
                    "label": 0
                },
                {
                    "sent": "So now one of the other things that we could compute was the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood is just the probability of the data.",
                    "label": 0
                },
                {
                    "sent": "Marginalized over the.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "So again, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's an integral over.",
                    "label": 0
                },
                {
                    "sent": "Now our posterior distribution will be over function value.",
                    "label": 0
                },
                {
                    "sent": "So is this an integral over function values and we can do this integral because this is a Gaussian in our case, although it might be an infinite dimensional Gaussian, we can still do that particular integral.",
                    "label": 0
                },
                {
                    "sent": "So if we look at what is the value of the marginal likelihood, then it has this particular form.",
                    "label": 1
                },
                {
                    "sent": "So it contains 3 terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is the only term that depends.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the data.",
                    "label": 0
                },
                {
                    "sent": "So I call this the data fit term.",
                    "label": 1
                },
                {
                    "sent": "OK. Then there's a second term here, which depends on the half minus half the log of the determinant of the covariance matrix, and this is a complexity penalty term, and I'll show you afterwards why why it has that name and there's a third term here which is uninteresting.",
                    "label": 0
                },
                {
                    "sent": "OK, so the marginal likelihood is the combination of these three terms.",
                    "label": 1
                },
                {
                    "sent": "And when I choose a covariance function, I can then compute well what is the what is the value of the marginal likelihood, i.e.",
                    "label": 1
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "By using Bayes rule we can then swap around the model and the and the data and we can ask what's the probability of the model given the data right?",
                    "label": 0
                },
                {
                    "sent": "And we can now try out different covariance functions.",
                    "label": 0
                },
                {
                    "sent": "So that's essentially what we're going to do, so that would be to say, for example, if I have two or three different covariance functions, and I don't know which is good for my data set, so each covariance function implies a prior over functions, right?",
                    "label": 0
                },
                {
                    "sent": "And it might be that this prior distribution or functions you know totally disagrees with your data right?",
                    "label": 0
                },
                {
                    "sent": "In which case you know this is probably not a good model, and hopefully the marginal likelihood will tell.",
                    "label": 0
                },
                {
                    "sent": "Tell us that this is not a good model.",
                    "label": 0
                },
                {
                    "sent": "Optimize my prior then like you optimizing applied in this way will not buy some parameters for optimizing.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "So also what we're going to look at exactly now is what happens if we had parameters in our specifications.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We already had the noise parameter, so the this Sigma N squared hear the noise variance.",
                    "label": 0
                },
                {
                    "sent": "So we have to we have to do something about that will have to fit that parameter and also we had the.",
                    "label": 0
                },
                {
                    "sent": "So before we I just looked at E to the minus the distance squared.",
                    "label": 0
                },
                {
                    "sent": "But how do we actually define?",
                    "label": 0
                },
                {
                    "sent": "You know when are two points close to each other and when are they were they deemed to be far apart right?",
                    "label": 0
                },
                {
                    "sent": "That seemed a little arbitrary before, like if they were more than one or two apart then the exponential of distance squared would be fairly low.",
                    "label": 0
                },
                {
                    "sent": "But how did I know that that was the right?",
                    "label": 0
                },
                {
                    "sent": "That was the right distance measure?",
                    "label": 0
                },
                {
                    "sent": "Actually, I didn't know that right?",
                    "label": 0
                },
                {
                    "sent": "And this is a very relevant question.",
                    "label": 0
                },
                {
                    "sent": "So there's a question here.",
                    "label": 0
                },
                {
                    "sent": "How far do X&X prime have to be away from each other?",
                    "label": 0
                },
                {
                    "sent": "Before I think the covariance should start falling right?",
                    "label": 0
                },
                {
                    "sent": "If there is the X&X prime or further away from each other and some particular distance, then I deem them to be to the function values to be more or less independent.",
                    "label": 0
                },
                {
                    "sent": "There's another way of phrasing it right?",
                    "label": 0
                },
                {
                    "sent": "So generally this is something that I don't know right, and I can't guess that when I'm just when I just got my got my data set right?",
                    "label": 0
                },
                {
                    "sent": "So this is something that you know I can put in a parameter, so I put in another parameter here L, which is called the characteristic length scale.",
                    "label": 0
                },
                {
                    "sent": "So L just measures how close should X&X prime be before?",
                    "label": 0
                },
                {
                    "sent": "I deem them to be close together.",
                    "label": 0
                },
                {
                    "sent": "It's roughly speaking OK, so L is now a parameter of my of my model.",
                    "label": 0
                },
                {
                    "sent": "Similarly, V is a parameter, so before we just said OK, covariances could go up to one right.",
                    "label": 0
                },
                {
                    "sent": "But what happens if my data set lives on a much larger scale, right?",
                    "label": 0
                },
                {
                    "sent": "Well, I should be able to cope with that as well, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a scale parameter outside everything, which I called V here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just the prior variance over the function that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So I've got in this case I've got.",
                    "label": 0
                },
                {
                    "sent": "I've got now three parameters that I don't know how to set, So what I'm going to do is I'm going to try and choose different values for those three parameters and see what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so first I've plotted that in a very simple example down here.",
                    "label": 0
                },
                {
                    "sent": "So I have 20 observations here, so those are the 20 observations.",
                    "label": 0
                },
                {
                    "sent": "And then I choose in this plot, I'm going to focus on the link scale.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to choose three different length scales and see what happens what my.",
                    "label": 0
                },
                {
                    "sent": "My prediction look like and I'm just going to look at the predictive mean in this case, right?",
                    "label": 0
                },
                {
                    "sent": "The prediction predictions, of course have particular distributions associated with Airbus associated with them.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in red here I've chosen.",
                    "label": 0
                },
                {
                    "sent": "Let's start with blue and blue.",
                    "label": 0
                },
                {
                    "sent": "I've chosen a very long length scale, right?",
                    "label": 0
                },
                {
                    "sent": "The very long length scale says that the we expect a priore that function values correlate a lot, even if the points are quite spaced quite far apart.",
                    "label": 0
                },
                {
                    "sent": "If that's my assumption, if that's my prior assumption, then when I see the data then I get a fit that looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, and obviously we can see by that that wasn't good in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, so my prior assumption didn't match the data very well, so I can try.",
                    "label": 0
                },
                {
                    "sent": "I can try a very short length scale if I choose a very short link scale.",
                    "label": 0
                },
                {
                    "sent": "Then basically it says that the covariance between the neighboring points drops off very quickly as a function of the input.",
                    "label": 0
                },
                {
                    "sent": "OK, and what you can see?",
                    "label": 0
                },
                {
                    "sent": "What happens here with the mean function is the mean function starts wobbling around a lot OK?",
                    "label": 0
                },
                {
                    "sent": "Because it only has to move a slight amount away from the from the data point before the function can start doing new things basically right.",
                    "label": 0
                },
                {
                    "sent": "And there's some nice aspects of this, because the fit here is an almost perfect fit to the data, right?",
                    "label": 1
                },
                {
                    "sent": "I think it's it's not quite hitting those two data points, but the rest of the data points it almost got them right.",
                    "label": 0
                },
                {
                    "sent": "If I reduced the length scale by a little bit more, I would be able to fit the data exactly right, but actually in terms of generalization, this doesn't look so good, right?",
                    "label": 0
                },
                {
                    "sent": "It looks as though it's actually.",
                    "label": 0
                },
                {
                    "sent": "It looks as though there's a smooth underlying function and there's some noise associated with that.",
                    "label": 0
                },
                {
                    "sent": "OK, so and here in green I've chosen an intermediate.",
                    "label": 1
                },
                {
                    "sent": "Length scale and that intermediate length scale gives the fit in green.",
                    "label": 0
                },
                {
                    "sent": "Here an which looks like a reasonable fit right?",
                    "label": 0
                },
                {
                    "sent": "And when I look at what is the marginal likelihood associated with this so I can compute that by just using the expression from before, so just plugging into this expression.",
                    "label": 0
                },
                {
                    "sent": "It's only depends on the data and the evaluated covariance function.",
                    "label": 0
                },
                {
                    "sent": "Here, when I plug into that, it chooses the green model, right?",
                    "label": 0
                },
                {
                    "sent": "That's actually how I found the green model.",
                    "label": 0
                },
                {
                    "sent": "I just plugged in the parameters and optimized with respect to the link scale right and it came up with this with this link scale.",
                    "label": 0
                },
                {
                    "sent": "So notice that this is despite the fact that it's very easy for the model to fit the data exactly if it wants to.",
                    "label": 0
                },
                {
                    "sent": "It doesn't want to write, and we heard already yesterday.",
                    "label": 0
                },
                {
                    "sent": "Some of the reasons why what the mechanisms are that prevents the models from liking, fitting the data too much.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I found a question there.",
                    "label": 0
                },
                {
                    "sent": "So this is doing an optimization.",
                    "label": 0
                },
                {
                    "sent": "Could we put the prior on the parameters and it again without out?",
                    "label": 0
                },
                {
                    "sent": "And could we do that instead of doing Monte Carlo?",
                    "label": 0
                },
                {
                    "sent": "Doing one of those deterministic like he kind of right?",
                    "label": 0
                },
                {
                    "sent": "So the question is now I'm optimizing over one of those parameters.",
                    "label": 0
                },
                {
                    "sent": "Could I?",
                    "label": 0
                },
                {
                    "sent": "Could I do things and we know that we should be careful if we're doing optimization?",
                    "label": 0
                },
                {
                    "sent": "Could we do things a little bit better?",
                    "label": 1
                },
                {
                    "sent": "Could we actually put a prior over the length scales and then find the posterior over the length scales?",
                    "label": 0
                },
                {
                    "sent": "And indeed you can do that, but?",
                    "label": 0
                },
                {
                    "sent": "It's not computationally.",
                    "label": 0
                },
                {
                    "sent": "So easy to do that because you end up with a with a collection of different Gaussian processes, like you can't integrate them analytically, right?",
                    "label": 0
                },
                {
                    "sent": "So you end up with, but you can use Monte Carlo for example to do these things, although the individual models under the Markov chain of course involves these matrices that depend on the size of data.",
                    "label": 0
                },
                {
                    "sent": "So for big datasets you know this might get out of hand at some point, right?",
                    "label": 0
                },
                {
                    "sent": "But ideally that's exactly what you would want to try to do right what we're trying to do here is to get away with just optimizing.",
                    "label": 0
                },
                {
                    "sent": "Very few parameters right?",
                    "label": 0
                },
                {
                    "sent": "So in this entire model I have only three parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not like it's not like a parametric model where you wouldn't be able to get a fit like this if you only had three parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So it might be that we're a little bit in better shape in this case because we can manage to specify things in terms of very few parameters.",
                    "label": 0
                },
                {
                    "sent": "Yep, sorry.",
                    "label": 0
                },
                {
                    "sent": "Previous business model.",
                    "label": 0
                },
                {
                    "sent": "It's not for mistresses.",
                    "label": 0
                },
                {
                    "sent": "So, so the question is, you know why is the why is that the right form of the complexity term, right?",
                    "label": 1
                },
                {
                    "sent": "So that's an extremely good question, and that's the next thing I'm going to talk about, right?",
                    "label": 0
                },
                {
                    "sent": "So notice here that the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So sometimes when you are in the sort of regularization framework, then you have a regularization parameter, right?",
                    "label": 0
                },
                {
                    "sent": "You have a data fit terms plus Lambda times the regularizer, right?",
                    "label": 0
                },
                {
                    "sent": "And you don't know what to do about Lambda.",
                    "label": 0
                },
                {
                    "sent": "There's a neat thing here.",
                    "label": 0
                },
                {
                    "sent": "There's no Lambda, right?",
                    "label": 0
                },
                {
                    "sent": "This is automatically on the right scale.",
                    "label": 0
                },
                {
                    "sent": "It's automatically doing exactly the right thing, so there's no free parameters anymore.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But you might be a bit.",
                    "label": 0
                },
                {
                    "sent": "But might be a bit.",
                    "label": 0
                },
                {
                    "sent": "Unsure about using this stuff that came came out of my my somewhat funky algebra here.",
                    "label": 0
                },
                {
                    "sent": "So you do you believe that this is the case, right?",
                    "label": 0
                },
                {
                    "sent": "So now I'll try to persuade you that indeed this is very reasonable.",
                    "label": 0
                },
                {
                    "sent": "The result should look like this.",
                    "label": 0
                },
                {
                    "sent": "Hey there logo Model T as in Tom industry.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the determinant of the K matrix, which is a number, and it's the logarithm of that determinant, right?",
                    "label": 0
                },
                {
                    "sent": "So the whole thing is a number.",
                    "label": 0
                },
                {
                    "sent": "OK, so just before we go there, there's this plot again, which, which is a very important plot.",
                    "label": 0
                },
                {
                    "sent": "So Chris Bishop also showed his.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version of this plot, so I'm just going to go through it again because I think it's very, very important and hopefully someone else will also show this plot so you'll have seen it 3 times and you'll know that it's true.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you here is the is the value of the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "And here I have an abstract representation of all possible datasets.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a certain size OK, and I had different kinds of models.",
                    "label": 0
                },
                {
                    "sent": "I have a very simple model, so a very simple model will only give high probability to data set.",
                    "label": 0
                },
                {
                    "sent": "Data sets that look a very particular kind, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have a linear model for example, then it will only give me high probability if the if the data actually can be fitted by elite by a linear model.",
                    "label": 0
                },
                {
                    "sent": "And I can have very complex models, so let's say a huge neural network or something like that that could fit almost any.",
                    "label": 0
                },
                {
                    "sent": "Any data set, but because these things are probabilities, they have to have to integrate to one, right?",
                    "label": 0
                },
                {
                    "sent": "So for the complex models, they can't have very large values of the marginal likelihood if they have to have, you know significantly non zero values for lots and lots of datasets right?",
                    "label": 0
                },
                {
                    "sent": "Whereas the simple models can have very high values because they only account for.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "A few different possibilities.",
                    "label": 0
                },
                {
                    "sent": "So when you actually come with your data set then there will be a model complexity which is just right, right?",
                    "label": 0
                },
                {
                    "sent": "And what I showed you before is actually the lines correspond to models of different complexity.",
                    "label": 0
                },
                {
                    "sent": "Right now the complexity is not given in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of parameters or something like that, but it's given you can see that the intuitively appealing thing that this is a simple model here, but the read function is a sort of a complex model, right?",
                    "label": 0
                },
                {
                    "sent": "Not by counting parameters or anything like that, but by looking exactly at this complexity term that has exactly this form.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use the last five minutes to try to persuade you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that this is actually exactly the right form.",
                    "label": 0
                },
                {
                    "sent": "OK, in an intuitive way.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's play a little game.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to give you some data points.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you some scalar data points, and they're going to have a mean of zero, and I'm not going to tell you what the variance is.",
                    "label": 0
                },
                {
                    "sent": "They're going to have a Gaussian distribution, so I'm going to give you, let's say 20 data points, and I want you to tell me what is the variance of the Gaussian that I used to generate the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a fairly simple modeling problem.",
                    "label": 0
                },
                {
                    "sent": "Does anybody, would anybody like to venture?",
                    "label": 0
                },
                {
                    "sent": "What would a good answer be so this case?",
                    "label": 0
                },
                {
                    "sent": "So they're just sample from a Gaussian with mean of zero and an unknown variance like sample.",
                    "label": 0
                },
                {
                    "sent": "Some scalars give you those scalars, and now I want you to tell me what was the variance of the generating Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Summer.",
                    "label": 0
                },
                {
                    "sent": "Square root of sum squares divided by.",
                    "label": 0
                },
                {
                    "sent": "OK, here's something about this summer squares.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you're looking essentially at the property of the data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, looking at the second moment of the data and that's a good answer.",
                    "label": 0
                },
                {
                    "sent": "Why is that a good answer?",
                    "label": 0
                },
                {
                    "sent": "We agree that's a good answer, right?",
                    "label": 0
                },
                {
                    "sent": "But what principle were you using?",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood, alright?",
                    "label": 0
                },
                {
                    "sent": "So let's try and do that.",
                    "label": 0
                },
                {
                    "sent": "Let's try and write down the likelihood function and let's try and maximize it, OK?",
                    "label": 0
                },
                {
                    "sent": "So the likelihood function look like, well, it's.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is just a Gaussian distribution, right?",
                    "label": 0
                },
                {
                    "sent": "The data is generated by this Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't know what the variances, that's that's a free parameter.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the data given that parameter written down the log of the likelihood here?",
                    "label": 0
                },
                {
                    "sent": "OK, it's the.",
                    "label": 0
                },
                {
                    "sent": "So the log of that so that log cancelled exponential, so was exponential of minus the distance between the observations and the mean.",
                    "label": 0
                },
                {
                    "sent": "And I told you the mean was zero, so mu is zero in this case divided by Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So that's where we get from.",
                    "label": 0
                },
                {
                    "sent": "From that time there's a normalization term that looks like this, and then there's a multiple of 2\u03c0 as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the way to solve the problem, a good solution to the problem is to say well, choose the Sigma squared which maximizes the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Alright, and this is the likelihood, and if you maximize this likelihood then you get exactly that.",
                    "label": 0
                },
                {
                    "sent": "Sigma squared should be the empirical variance here, but notice that this is exactly the same equation as I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Right, I have a data fit term in blue.",
                    "label": 0
                },
                {
                    "sent": "I had the log of the determinant, so now this is not written as a determinant.",
                    "label": 1
                },
                {
                    "sent": "This is written as a matrix here because this is this is this is a 1 dimensional example of this right?",
                    "label": 0
                },
                {
                    "sent": "So the so I have independent observations there so that my matrix K in this case is just Sigma squared times the.",
                    "label": 0
                },
                {
                    "sent": "Times the unit matrix.",
                    "label": 0
                },
                {
                    "sent": "So the determinant of a matrix.",
                    "label": 1
                },
                {
                    "sent": "Here I just get an end that pops out.",
                    "label": 0
                },
                {
                    "sent": "So if we look back so we have a data fit term, the only thing that depends on and have a complexity term and the complexity term depends on the variance of that matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is the one dimensional case where we had the general case before you go back.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "We had a data fit term complexity term and we had this thing over here, right?",
                    "label": 0
                },
                {
                    "sent": "So it's exactly the same, the same principle and you can see also the intuition about why this is the complexity term even in this simple case, right?",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a situation.",
                    "label": 0
                },
                {
                    "sent": "What things would look like?",
                    "label": 0
                },
                {
                    "sent": "I gave you these data points and you could try to fit it with the with the with the Gaussian, which has a very small variance, right?",
                    "label": 0
                },
                {
                    "sent": "So I've drawn a Gaussian with a little bit too small variance, but what's the problem with this with this model?",
                    "label": 0
                },
                {
                    "sent": "But the problem is that there are a few data points.",
                    "label": 0
                },
                {
                    "sent": "That appeared in places where the model says that that was extremely unlikely.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's not a good model.",
                    "label": 0
                },
                {
                    "sent": "Down here I now have a now have a very complex model.",
                    "label": 0
                },
                {
                    "sent": "This model would allow for many more different configurations of the data points, so this is kind of corresponds to my complex model.",
                    "label": 0
                },
                {
                    "sent": "What's wrong with the model?",
                    "label": 0
                },
                {
                    "sent": "Well, the data fit term is fine, right?",
                    "label": 0
                },
                {
                    "sent": "They all lie nicely under the curve.",
                    "label": 0
                },
                {
                    "sent": "The problem is the complexity, right?",
                    "label": 0
                },
                {
                    "sent": "We allowed four points appearing out here where they didn't appear right, so we were wasting our mass out there.",
                    "label": 0
                },
                {
                    "sent": "So the data fit was fine.",
                    "label": 0
                },
                {
                    "sent": "Term was not good, but the complete was good, but the complexity term was hitting us there, right?",
                    "label": 0
                },
                {
                    "sent": "And that's why we wouldn't want to choose that big that big matrix.",
                    "label": 0
                },
                {
                    "sent": "And otherwise so this is the same situation we have.",
                    "label": 0
                },
                {
                    "sent": "This is this is just the right complexity, right?",
                    "label": 1
                },
                {
                    "sent": "By choosing the variance to match the empirical variance.",
                    "label": 0
                },
                {
                    "sent": "OK, so I hope that this persuades you that what's going on in this somewhat impenetrable algebra is actually is actually very reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "We're trying to match the covariance matrix now in the joint space, so we have to think about the distribution of all of all the points in this very large dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "But we want the covariance somehow to not just allow any distribution of points, but be as tight as possible that the data.",
                    "label": 1
                },
                {
                    "sent": "Allows us to be.",
                    "label": 0
                },
                {
                    "sent": "That was a question.",
                    "label": 0
                },
                {
                    "sent": "Watching process actually suffer from the first dimension, so the question, does the Gaussian process suffer from the curse of dimensionality?",
                    "label": 0
                },
                {
                    "sent": "Well, the curse of dimensionality is something that that there isn't really any way around, right?",
                    "label": 0
                },
                {
                    "sent": "It's saying that.",
                    "label": 0
                },
                {
                    "sent": "Once you move to higher and higher dimensional spaces, they're basically more and more things that would seem to be to be reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "So when would you expect this kind of thing to actually do something about the curse of dimensionality?",
                    "label": 0
                },
                {
                    "sent": "Only in the cases where somehow you could specify a prior distribution which will help you to look away from those from those areas, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm going to talk about next time, right?",
                    "label": 0
                },
                {
                    "sent": "So today we've only been talking about very simple forms of covariance functions, right?",
                    "label": 0
                },
                {
                    "sent": "So maybe we can get some more mileage out of specifying more closely the properties of the covariance function, and maybe we can try to find out how to match those against our datasets using this methodology.",
                    "label": 0
                },
                {
                    "sent": "Yes, in this formula, samples are actually independent, but indeed.",
                    "label": 0
                },
                {
                    "sent": "No function case.",
                    "label": 0
                },
                {
                    "sent": "They were going to be a little correlated, so that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "So the difference between my equation here is just that this is the independent version of that, right?",
                    "label": 0
                },
                {
                    "sent": "So I don't have the correlation matrix and in the Gaussian process case I will have this situation that things that are highly correlated or things that are close by should have values that are highly correlated, and if they don't then this is being penalized by the by the data fit term.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I should.",
                    "label": 0
                },
                {
                    "sent": "I should stop here, but I hope.",
                    "label": 0
                },
                {
                    "sent": "But I've managed to persuade you that you don't have to think about, you know complex parametric forms and complex prior distributions to be able to do a good job of regression, but actually just working with a.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The joint Gaussian distribution over these things can be.",
                    "label": 0
                },
                {
                    "sent": "Very effective in actually coming up with good answers to regression functions, right?",
                    "label": 0
                },
                {
                    "sent": "Alright, that's it for today.",
                    "label": 0
                }
            ]
        }
    }
}