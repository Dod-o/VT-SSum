{
    "id": "nl6lynndzhuyb2e632epjeq7xleqz3qg",
    "title": "Sparsity: algorithms, approximations, and analysis",
    "info": {
        "author": [
            "Anna Gilbert, University of Michigan"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_gilbert_analysis/",
    "segmentation": [
        [
            "So I'm going to start with a story."
        ],
        [
            "Electrical engineering and this is really sort of mathematicians view of electrical engineering, so it's a bit of a cartoon.",
            "But imagine that you've got an image or some data, or a signal, and you want to compress that data, or you want to represent that data very concisely, and so your data is this little snippet of a famous image called Barbara and you transform this image or this data into your favorite.",
            "Orthonormal basis, in this case.",
            "One of my favourites is a 2 dimensional wavelet basis an you observe that in that basis a small number of these basis coefficients are large, IE a small number of them are very bright, alot of them are dark and so you think, well, you know if I want to represent this image sparsely or concisely, maybe I should threshold these coefficients.",
            "I should throw away.",
            "A bunch of them and I should just keep a few bright ones.",
            "Set all of the rest to zero, and then when I do, my inverse 2 dimensional wavelet transform, I should get a pretty decent approximation or a pretty good version of my original image.",
            "So here is the original data.",
            "Here is the synthesized or reconstructed version of my data, and here is my compression or my sparse representation of the original data.",
            "If you're a statistician, you might even argue.",
            "That this image is better than that image because you've somehow removed what you might think of as noise in your data.",
            "But now this is.",
            "This is really a very simple cartoon of transform coding.",
            "You pick an orthonormal basis, you apply a transform and then you threshold the coefficients you set to zero whole bunch of very small ones and then you do an inverse transform.",
            "You know this.",
            "You then measure how close is this to the original."
        ],
        [
            "OK, so I'm going to talk about much more sophisticated ideas than than this simple cartoon.",
            "Not not all that much more sophisticated, so this is about how to compress images and signals or data.",
            "How to do it accurately.",
            "So this is where mathematics comes in.",
            "How to do it concisely so you think of conciseness, sparsity, and parsimony as all being belonging to statistics?",
            "And also how to do it efficiently?",
            "So how to compute things very efficiently so this sort of belongs to the realm of algorithms."
        ],
        [
            "And so this one picture I showed you with a 2 dimensional wavelet transform really is a very simple cartoon because roundabout, maybe the early 1990s.",
            "The mid 90s signal processors, image coders, people in electrical engineering, decided that they could spend their time trying to develop a really good orthonormal basis for images, one that would represent.",
            "Images really concisely that would really allow you to compress images, and it turns out that that's very difficult.",
            "There's some actually some good technical mathematical reasons for why that's very difficult, and instead what they decided that if one orthonormal basis is good for representing my data representing my image, then surely I should get a sparser or a more concise or a more accurate representation.",
            "If I use more than one orthonormal basis, and so.",
            "I should put my energy into designing good compression and decompression algorithms and not so much of my energy into designing."
        ],
        [
            "Orthonormal basis.",
            "This is especially true for images."
        ],
        [
            "And so in this is captured by fantastic poster.",
            "Which doesn't display so well on this on this projector.",
            "This poster from 1998, not mathematics awareness poster, so the original data or the original image is this picture of a mandrill.",
            "And the subsequent pictures below it are the reconstructed representation of the mandrill in different orthonormal bases done sequentially.",
            "So the first picture is what you would get of the mandrill after you had converted to a certain 2 dimensional wavelet transform and then thresholded the coefficients an reconstructed and up close on the screen.",
            "You can see there is some difference between the original mandrill.",
            "And the reconstituted wavelet compression version an.",
            "In fact, the difference between these two is captured by this picture, and in order to get this difference picture.",
            "This is also been encoded by a very large collection of orthonormal bases, called wavelet packets, and then the bottom picture is the difference between this representation plus this representation and the original and So what electrical engineers, people and image coding and applied mathematicians discovered.",
            "Is that different orthonormal bases pull out different features.",
            "This is especially true of images, or you can you can use language that is especially evocative for features for images.",
            "Orthonormal basis that consists of wavelets picks out certain types of features, those of wavelet packets pulls out, sort of feathery texture.",
            "And the bottom one, which is very hard to see.",
            "Pulls out almost like point list type type textures, type features and so the idea is that instead of using one orthonormal basis to try and capture all of these different features, you should use a whole collection of orthonormal bases, each one sort of tuned or tailored for different features that you have in."
        ],
        [
            "Our data and so to make that more mathematically precise, we're going to define what we mean by a collection of features or a collection of atoms, and that is a dictionary D in in our end.",
            "So we're in dimension N, and we have D different atoms.",
            "Let's normalize them all to have unit Norm.",
            "An unit L2 norm one and we call each of these atoms we call each of these elements atoms, and if they span all of our end, we say the dictionary is complete and if they are linearly dependent, we say the dictionary is redundant.",
            "So you really want to think of D as being much larger than N. You have a whole giant collection of of vectors of features and you want to.",
            "Use a parsimonious collection of them to represent your data."
        ],
        [
            "Concisely.",
            "So how do we talk about representing your data?",
            "Will we do this in a very simple linear model so we form a matrix fee?",
            "Its columns consists of elements from the dictionary, atoms from the dictionary, and when we say we form a linear combination literally, what we mean is the matrix vector product fee times C. So in case you are familiar with statistical literature and nomenclature, I have tried to suppress that and justice kept the pictures the relative sizes.",
            "So you should think of a short and very wide matrix fee is multiplying a very long vector C and I wish to have just a few non zero entries in C so that I have a very small linear combination feedtime see.",
            "To represent my my data, my Image X.",
            "Similarly I also want to look at what are the dot products between my input data X and Adams in the dictionary fee, and so that's easy to represent with linear algebra.",
            "Just take the transpose or the add joint of the dictionary matrix Times X.",
            "So just so that you keep in mind the dimensions X is short, feed transposes to."
        ],
        [
            "OK, So what are some examples of redundant or over complete dictionaries?",
            "Well, one of my favorite examples is the 448 Dirac or Fourier spikes and signs basis.",
            "This is the union of two orthonormal bases.",
            "I have complex exponentials.",
            "These are these guys that oscillate and then I have spikes or impulses.",
            "These guys like this.",
            "So in dimension N. There are two end of these vectors and this is what they look like in pictures.",
            "This is what they look like."
        ],
        [
            "Math.",
            "OK, so now that we've got an idea of the players in our story, let's try and formulate some rigorous mathematical problems.",
            "You know what do we mean by come up with a concise representation of our data, and so there is 3 different.",
            "Rigorous, precise problems that we could formulate.",
            "So the first one is we refer to as the exact sparse approximation problem, and this is the idea that you're given an input vector X piece of data X or an image X in RN and a complete dictionary feet, and you would like to find the sparsest coefficient vector, sparsest vector C, such that X is exactly equal to the linear combination feed times C. OK, I use the L0 norm.",
            "Quasi norm here to count the number of non zero entries in the vector see this literally says find the sparsest representation for X over this dictionary fee so.",
            "Representing your data exactly is frequently too much to ask for, and in fact it may not even be desirable.",
            "If you remember this picture of the Patch of Barbara, I actually thought that the reconstructed, the approximate version, actually looked better than the original it somehow denoised, and so maybe what you want to ask for is instead of representing X exactly that you'll be very close to the input vector.",
            "And close in the L2 norm.",
            "So you can say given an input vector XA dictionary fee and some error tolerance epsilon, I would like to find the sparsest linear representation over my dictionary fee that comes within Euclidean distance epsilon of the input vector.",
            "Now you could also say, well, I'm not so concerned with my error tolerance.",
            "I'm really much more concerned with my memory budget for my compression and what I really care about is how many atoms I use in my sparse linear combination and so that goes by the name problem named sparse.",
            "Given an input vector X and redundant dictionary fee, and a sparsity parameter K you want to find.",
            "The linear combination of at most K atoms that comes as close to X as possible, so minimize the L2 distance between the input vector X and the linear combination feedtime.",
            "See subject to using no more than K atoms in your in your representation.",
            "So these are three very rigorously precisely defined sparse approximation problems.",
            "Now we're in a setting where we can ask how difficult is it to come up with feasible.",
            "Algorithms to solve these problems?",
            "Not not super fast, just a plain old feasible, how?"
        ],
        [
            "How efficient are they an?",
            "Unfortunately, there's a whole slew of bad news, so the basically the bad news is that all of these problems are NP hard, so they were also NP complete, so there in NP and their NP hard so.",
            "The bunch of very depressing results given an arbitrary redundant dictionary and an arbitrary input vector X, it's NP hard to solve any of these problems.",
            "Worse, at least for the sparse approximation problem.",
            "It's NP hard to determine whether or not your optimal error is 0.",
            "For a given sparsity level K. So in other words, it's NP hard to distinguish between error zero and error error that's not equal to zero.",
            "That means it's very hard to even approximate solutions to these problems and.",
            "This is."
        ],
        [
            "Sort of.",
            "Depressing news and yet also somewhat helpful news in that it means that there's room for approximation algorithms for to solve them.",
            "So rather than go through the proof of the hardness of these problems, I just sort of want to give you a flavor of the proof of the hardness of these, and also.",
            "Tell you how this is related to seduco.",
            "So the reduction is from a for my 2, two a standard, a textbook NP complete problem which is known as exact set cover by three sets and I say it's a textbook problem.",
            "If you go and pull off the shelf, your copy of Gary and Johnson.",
            "Yep, it's one of the 20 NP complete problems in there, so it's a constraint satisfaction problem.",
            "It's related to vertex cover, so these are these algorithms are all over.",
            "These problems are all of the same.",
            "Same flavor, and so I'm going to try and cast sparse approximation as one of these constraint satisfaction problems.",
            "So.",
            "X3 C exact set cover by three sets is the following problem, so you're given a finite universe.",
            "You OK and I really want you to sort of think in pictures to think of analogies to the sparse approximation problem.",
            "I'm going to have my universe you can run from one to N down here, and I'm given a collection X of subsets X, One X2.",
            "There D of them and.",
            "X each subset contains exactly 3 elements from the universe, and so X 3C problem asks does my collection of subsets contain a disjoint collection whose union is exactly equal to you, and if you start drawing pictures of your collection of subsets where you place you in the Roseanne, you make column vectors out of these subsets X One X2 up to XD.",
            "You put a ones in the rows of X which correspond to the elements of the universe.",
            "Pretty soon this starts to look like your dictionary matrix.",
            "And the X 3C if you were to formulate this as a sparse approximation problem would say form this overcomplete dictionary and you would like to know is there a coefficient vector that you could multiply here such that this matrix feed times your coefficient vector is exactly equal to the all ones vector and such that I have in my coefficient vector.",
            "I make it so that I.",
            "Have subsets that don't overlap at all, so this is a constraint satisfaction problem and you can show that if you could solve a sparse approximation problem, you could if you could solve this problem you could solve sparse approximation so you can also frame sudoku as a constraint satisfaction problem.",
            "You can generate a.",
            "You can also frame it as a sparse approximation problem and you can create a dictionary matrix out of all the constraints for Sudoku.",
            "And you can investigate how well sparse approximation algorithms are, how good heuristic they are for solving Sudoku problems.",
            "It's kind of a fun exercise."
        ],
        [
            "To do.",
            "OK, so that's a sketch of the theoretical results.",
            "I think it's probably more useful to understand the implications of the theoretical result, so you might think that this is bad news, right given.",
            "Any polynomial time algorithm for solving any of these three problems?",
            "There is some redundant dictionary fee and some signal X such that the algorithm is just flat out wrong.",
            "So when when people come to you and say I have a polynomial time algorithm for solving sparse approximation problems, you should go, uh, huh?",
            "I would be very interested in this, but probably there is some pair of dictionary on signal which will foil your algorithm.",
            "So this is an incredibly pessimistic setting.",
            "You know it captures the worst case behavior, and it says there's there.",
            "There is some pathology out there that will probably foil the polynomial time algorithm, and even worse, probably there are examples out there for, you know, for which you can't even hope to approximate the solutions."
        ],
        [
            "But you know, by my nature, I'm an optimist, not a pessimist.",
            "And you know this this horribly pessimistic result has not stopped electrical engineers from developing state of the art image coders for image compression.",
            "And they certainly are not doing an exponential time algorithm for encoding and decoding images.",
            "So, so what's going on?",
            "Why?",
            "Why do these things seem to work?",
            "So well, there are a couple of reasons, so one of them is that natural dictionaries are very far from arbitrary, so this union of spikes and signs my favorite dictionary is not an arbitrary dictionary by any means.",
            "It does not look."
        ],
        [
            "At all, like this dictionary matrix that you would get from an instance of X3, C, it has all sorts of beautiful structure."
        ],
        [
            "But and it's entirely possible that all of this beautiful structure you can exploit in polynomial time.",
            "And, um.",
            "The other sort of hope that one would have for solving at least parts of these sparse approximation problems is that we know just fine how to solve sparse approximation problems when our dictionary is not redundant, but it is an orthogonal basis, or an orthonormal basis, so they're very simple algorithms for solving all of these problems in that case."
        ],
        [
            "So I want to emphasize the sort of peculiarity of in somewhat uselessness of those hardness results, and emphasize that the hardness depends on both the dictionary fee and the input signal X being arbitrary, not random, but arbitrary, and if you change either one of those instance types, you change the nature of the problem so.",
            "Frequently you'll hear people say I have a polynomial time algorithm for solving an NP hard problem.",
            "That's not really what they mean.",
            "Most likely what they mean is there dictionary fee is perhaps random.",
            "It comes from some sort of distribution and their input signal is arbitrary.",
            "You know, in which case the problem begins to have the flavor more of compressive sensing, or they have a redundant and arbitrary redundant dictionary, but they have some sort of random signal model.",
            "They impose some sort of prior knowledge, some sort of parameterized signal model.",
            "All of those are really different from the from both of these cases being being arbitrary, and so you know if I tell you that I'm going to build my image coder based on wavelet packets and a 2D wavelet transform, that is a fixed redundant dictionary fee and I can design efficient algorithms for that one particular fixed dictionary.",
            "So I just want to point out to some extent the uselessness of these hardness results.",
            "An to be able to be aware of what sort of type of input that input problem that you."
        ],
        [
            "Have OK so let us.",
            "Try and figure out what is it about the orthonormal basis case.",
            "The transform coding case that permits us to have we hope feasible algorithms and you know, discuss what might be possible.",
            "We've discussed what is impossible.",
            "Let's let's do what is possible.",
            "OK, so I showed you the picture of Barbara at the beginning where we have an orthonormal basis.",
            "So there's A and we.",
            "Computed all of the coefficients in the basis.",
            "Similarly set to 0.",
            "A bunch of the really small ones, so you could.",
            "Do a slightly more sophisticated algorithm and you could pull off the atoms or the the vectors in the basis one at a time in decreasing size.",
            "I should say you want to pull them off in an order that is commensurate with with the sizes of the dot product of your signal.",
            "In that basis there we go OK, and then if you say, stop after you've pulled off the K biggest.",
            "That gives you the best approximation in the L2 cents to the input vector using at most K atoms.",
            "So since the basis is orthonormal, you can use the Pythagorean theorem or the Plancherel theorem.",
            "If you want to be fancy.",
            "And pulling off the K biggest at the atoms that correspond to the K biggest dot product in magnitude will give you the best K term representation, so that's a very simple algorithm, just guaranteed to work.",
            "Runs in polynomial time.",
            "So what is it about the orthonormal basis case that makes this work?"
        ],
        [
            "Other than the Pythagorean theorem, well, it is exactly the Pythagorean theorem.",
            "The inner products between the atoms that are in the basis are small, which is to say it's zero.",
            "So all of these, all of these atoms are orthogonal to one another.",
            "And so when you take a dot product of your signal with this Atom versus this Atom, can you all see this?",
            "It's very hard to confuse an Atom that points in this direction with an Atom that points in this direction.",
            "When you're when you're trying to figure out which which Adam should I choose so the converse of that is that when the atoms are nearly parallel.",
            "And you take a dot product with your signal.",
            "It's really hard to tell which dot product is bigger and hence which Atom you should choose.",
            "Is it this one that's making the dot product bigger or this one so?",
            "There's something important about the geometry of these redundant dictionaries that is going to allow us to have feasible algorithms.",
            "I should say that if you haven't done the little mental trick of the naive algorithm is to do brute force search.",
            "Which is exponential time to find like the best K term representation now would be a good time to convince yourself that brute force search works, but it's not particularly efficient."
        ],
        [
            "OK.",
            "So.",
            "I said there's something very important about the geometry of the dictionaries that is going to allow us to come up with efficient or at least feasible algorithms, and I've been pointing my fingers like this to say that, you know, it really matters what is the angle between atoms?",
            "Is the angle 90 degrees or is it something much, much smaller so statisticians actually defined the notion of coherence of a dictionary?",
            "Which is the biggest dot product between two distinct elements in the dictionary?",
            "Two distinct atoms in the dictionary, and it's somewhat of an unfortunate name because small coherence is good and large coherence is bad.",
            "Incoherence here is the resource, so be it.",
            "This is the definition.",
            "So these two atoms are very hard to tell apart.",
            "It's very hard to figure out which atoms should you choose this one or this one when they point in almost the same direction.",
            "Whereas these guys are all fairly well spread out, so it's easier to figure out which Atom you should pick to make up your."
        ],
        [
            "Representation.",
            "OK, so this is where.",
            "We're missing something.",
            "OK, so.",
            "You might ask is it possible to construct very large?",
            "Redundant dictionaries that are also very incoherent, so the first example explicit example that we gave was the union of two orthonormal bases, spikes and signs.",
            "They in dimension N There are two N atoms in that dictionary, which really isn't very big, it has coherence 1 / sqrt N. If you make sure to normalize your complex exponentials so they all have norm one.",
            "Then the dot product between any spike in any complex exponential is 1 / sqrt N. So you could ask can I get can I get bigger dictionaries with that type of incoherence?",
            "And the answer is yes.",
            "So here are here's an example where.",
            "Um?",
            "Actually, here's an example where the answer is unfortunately no.",
            "I have a very large dictionary.",
            "The number of atoms in the dictionary is N log.",
            "NO log N times the dimension, but the coherence is 1 / sqrt 2, which is pretty big.",
            "There certainly are very nice, very large dictionaries where the coherence is small.",
            "It's like 1 / sqrt N and there are N squared different vectors.",
            "So at this point I should allude to the construction of spherical codes.",
            "So you have.",
            "Redundant dictionary is equivalent to a spherical code.",
            "You have atoms that all have Euclidean length one.",
            "These are points on the unit sphere in dimension N. And you can ask, how many vectors can I stuff into end dimensions?",
            "How many points can I put on the sphere in end dimensions so that they are all relatively far apart from one another?",
            "So the angles that they all have with one another is pretty small.",
            "This is a questions that people have looked at for a long time in coding theory and one of the beautiful constructions of a very large incoherent dictionary is occur dot code.",
            "It is a union of end different.",
            "Orthonormal basis, so there are N of these orthonormal bases, each of size N, which is how you get N ^2.",
            "Adams was fantastic."
        ],
        [
            "Apple.",
            "OK, so now that we've got in place.",
            "A good intuition about redundant dictionaries, and now that we've got in place where three different sparse approximation problems, we can ask.",
            "OK, how do we now come up with algorithms for computing sparse approximations of our data?",
            "And can we use geometric constraints on our redundant dictionaries to guarantee that our algorithms are both correct and fairly efficient?",
            "So in order to draw inspiration for creating our algorithms, we're going to look at building our approximation one step at a time, just like we did for the orthonormal basis, where we pull off in in order of descending dot products, we pull off atoms one at a time.",
            "We're going to pick at each step.",
            "We're going to pick the best Atom at that at that step.",
            "This is a very basic.",
            "So incredibly simple greedy algorithm IK."
        ],
        [
            "Formulation.",
            "And one of the first I should say the best well studied algorithm in this area is called orthogonal matching pursuit.",
            "Oh for orthogonal and MP for matching pursuit.",
            "So it was.",
            "Sort of most promoted in the signal processing community in the early 90s.",
            "Bimala Anjang, although it it you know if you go back and look in the literature arose in many forms much earlier than that, and this is really the overall architecture of greedy algorithms.",
            "So the input is, whoops, the input is the dictionary fee.",
            "Signal X An let's say K steps and you initialize your coefficient vector to be all zeros, and then there are two parts to the iteration.",
            "The first part is the selection step you want to find the vector in the dictionary that maximizes the dot product with the residual.",
            "The residual being the input minus feet time.",
            "See your current approximation.",
            "So there's a selection step or a maximization step, and then there's an update step you want to.",
            "Update The coefficient C in your current approximation in the case of OMP, you want to orthogonalize.",
            "You want to let the coefficient C be the orthogonal projection onto the current set of atoms that make up your representation, i.e.",
            "You want to solve least squares problem at each iteration, and then you iterate."
        ],
        [
            "OK, now I know that there are many experts in the audience on these types of algorithms, because I've seen a number of the posters outside that are all that are variations and much more sophisticated versions of this very basic greedy architecture.",
            "So there's matching pursuit.",
            "There's where you don't do an orthogonal projection to update the coefficients.",
            "There's thresholding where you take the, not the one biggest Atom, not the Atom that has the one biggest product with the residual.",
            "But maybe you take a group of them.",
            "Maybe you take all of those that are above a certain threshold, and if you take all of those that conform to certain sparsity pattern, you can also have alternative stopping rules.",
            "You don't just run it for K steps, you have all different sorts of error criteria, and there are many many variations, including a whole bunch in the in the posters outside."
        ],
        [
            "So you might ask, well, OK, how good a job does OMP do?",
            "Is it provably good and is it a provable approximation algorithm for an NP hard problem?",
            "And so initially some applied mathematicians analyzed this algorithm and they came up with.",
            "An answer to this question that at least I found personally unsatisfying, so the result basically says if you have a complete dictionary and an arbitrary input vector X, after T steps, I know that my error from OMP is no more than some constant times 1 / sqrt T. So I found this an unsatisfying result because it doesn't tell me if X has a sparse representation.",
            "That's, you know, consists of two atoms.",
            "It doesn't tell me anything about stopping after 2 steps.",
            "It just says by the time I have run for N steps my error is zero essentially so this."
        ],
        [
            "Not particularly helpful.",
            "So you'd like to know when does this algorithm actually identify sparse?"
        ],
        [
            "Presentations and so.",
            "For this simple algorithm you can formulate a sufficient condition on the geometry of the dictionary for for.",
            "For recovering exactly sparse signals, or for identifying the support set Lambda of these sparse signals.",
            "So for the sake of time I'm just going to say that there is an exact recovery condition which is phrased in terms of the geometry of the dictionary.",
            "Unfortunately, this particular phrasing of the theorem you need to know the support set or the sparsity pattern Lambda, and a more practically useful, although perhaps less geometrically interesting, result says that this exact recovery condition holds whenever you do not ask for too many vectors in your sparsity pattern, and too many is a function of one over the coherence.",
            "So here is where coherence is used as a resource.",
            "And as a way of saying, as long as you ask for a fairly sparse signal sparser than one over the coherence, then this algorithm is probably good, and you can also analyze its running time.",
            "You might ask well.",
            "Roughly speaking, what is one over the coherence?",
            "For most, you know, sort of large dictionaries for most redundant dictionaries, the coherence is about 1 / sqrt N, and so you can ask for getting good approximations to signals with sparsity less than roughly square root of the dimension.",
            "Which is a.",
            "You know, it's a this is something checkable and this is something that you know.",
            "It gives you a reasonable practical rule of thumb."
        ],
        [
            "OK so um.",
            "Not going to discuss how to prove these just to prove these results, just give you a flavor of the analysis of the algorithm and why this exact recovery condition might come in and analyzing OMP an that is OMP is always going to pick good atoms if the good atoms have bigger dot product with the residual than the bad atoms, and so you always want to make sure that this ratio is less than one.",
            "So remember, Lambda is the support set and this is talking about Dot product with your residual and so you can transform this ratio.",
            "This greedy selection ratio into a condition that looks like the exact recovery condition.",
            "So this is a flavor of the analysis of the."
        ],
        [
            "Algorithms?",
            "More flavors tastes of the results are of this form.",
            "So OMP is kind of a simple minded algorithm in kind of.",
            "It is incredibly simple minded.",
            "And so while it does run in polynomial time, and while there are fairly are very easily checkable conditions, the output of the algorithm is pretty good, but it doesn't recover the spokes.",
            "Exactly sparse representation exactly.",
            "So what you can get is an approximation algorithm for a sparse approximation.",
            "So you can show that as long as you're not asking for too many vectors, too many as a function of one over the coherence, the approximation that you return the L2 error of that approximation is guaranteed to be within some factor of the optimal representation for X OK, and this factor is not a constant.",
            "Unfortunately, it's like you know the square root of 1 + 6 * K. You can do better with more sophisticated analysis.",
            "And by cranking down on on how sparse so signal you're asking for sparsity versus one over the square root of the coherence.",
            "And if you make your algorithm more sophisticated, you do a two phase greedy pursuit algorithm where you run something that looks like OMP for some number of steps, and then you do a big cleanup pass.",
            "Then you can get an honest to God.",
            "Constant factor approximation in the optimal error, and you can even if you let up on that coherence a little bit, you can actually figure out exactly what that constant is, how it depends on the coherence and K the number of terms in your representation.",
            "So it's a little embarrassing is that these types of sophisticated results came before these simple minded results, so sometimes sometimes writing the 2nd paper is better than writing the 1st paper."
        ],
        [
            "OK, so this is all about greedy algorithms.",
            "I don't want to give you the impression that this is the end all be all in algorithmic formulation, because many of you are actually more familiar with an alternative algorithm approach which is really more suitable for two of the three problems SO2A."
        ],
        [
            "The three problems are non convex optimization problems.",
            "You want to minimize the L0 norm of a coefficient vector subject 2, an equality constraint or.",
            "I have an L2 error constraint and you so these are both non convex."
        ],
        [
            "Asian problems astandard algorithmic formulation or or trick is to relax these optimization problems to convex problems and so here are the various convex relaxations you can relate this parameter Delta to this parameter epsilon.",
            "And then."
        ],
        [
            "Once you have a convex optimization problem who have a whole slew of algorithms at your disposal.",
            "And so some of the names associated with people who have studied these and many, many, many others are listed here.",
            "There are a couple of things that I want to point out.",
            "One of them is that just because you formulated the problem as a linear program doesn't mean you're done so for practical purposes, you need to have a good LP solver and good LP solvers that work at scale that work for your specific type of problems.",
            "This is really a very very high art form, and as a subject matter all to itself, so that is a comment about practicality.",
            "Comment about theory is that.",
            "You still have to show that the solution to your convex relaxation is related to the solution to the nonconvex problem, and so there are all sorts of interesting questions that have to be done even after you formulate your problem as a convex Optima."
        ],
        [
            "Patient problem, so it's sort of interesting is that the sufficient geometric conditions for solving these convex optimization problems and the relationship of their solutions to the nonconvex ones are exactly the same as the ones for the greedy."
        ],
        [
            "Rhythms.",
            "OK, there are other alternate optimization formulations.",
            "Constrained minimization, unconstrained minimization.",
            "This one goes by the name of L1 regularization.",
            "Almost certainly some of you have seen this many."
        ],
        [
            "If you are experts on this, so I sort of wanted to point out that two of these three sparse approximation problems are amenable to convex relaxation and the other one is not, so it is much more well suited to greedy algorithms so sparse.",
            "Is a perfectly fine problem with which to throw out greedy algorithms and these other two are make more sense to study convex relaxation.",
            "OK, so I'm going to skip the connection between sparse approximation and statistical learning because."
        ],
        [
            "It's hinted at by many of these solutions."
        ],
        [
            "They come up in."
        ],
        [
            "So."
        ],
        [
            "Mystical learning theory.",
            "The algorithms have different names, but they're very similar flavor.",
            "The greedy algorithms things like forward selection, forward stagewise regression, least angle regression, constrained optimization, things like quadratic programming, lasso, and unconstrained optimization.",
            "All of these regularization techniques, and I would be remiss to not point out this beautiful very early paper that shows its process.",
            "A sparse approximation formulation is equivalent to support vector machines.",
            "So beautiful early work, long before most of the work was done in sparse approximation."
        ],
        [
            "So you might also ask what is the connection between sparse approximation and compressed sensing, which also there are several talks right after mine.",
            "On this topic in many post."
        ],
        [
            "So there are two 2 main points I want to make quickly.",
            "One of them is that you want to interchange the role of data and coefficients or measurements and image and this is what you can't now and this is what you wish to."
        ],
        [
            "Reconstruct."
        ],
        [
            "You can think of it more as a design problem.",
            "I want to design a good matrix Phi such that if I take linear combinations of a sparse vector an collect these measurements from these measurements and this matrix I wish to recover a few important pieces of information about."
        ],
        [
            "X."
        ],
        [
            "OK, and quickly the difference between sparse approximation and compressed sensing is like the difference in route finding between am I close to my route.",
            "Where am I close to 0?"
        ],
        [
            "So am I doing a good job in representing why or am I doing a good job in representing X?"
        ],
        [
            "So this."
        ],
        [
            "Is the two main differences many more parameters that you could play with since?"
        ],
        [
            "Design problem and lots of applications aside from statistical learning theory and more."
        ],
        [
            "Teen learning"
        ],
        [
            "And so I think this is a fantastic area of mathematics and statistics and computer science all coming together, and hopefully I've given you some idea of the flavor of the computer science and the applied math results an I encourage you to go look at the papers in your field that talk about sparsity as well.",
            "There there really excellent thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start with a story.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Electrical engineering and this is really sort of mathematicians view of electrical engineering, so it's a bit of a cartoon.",
                    "label": 0
                },
                {
                    "sent": "But imagine that you've got an image or some data, or a signal, and you want to compress that data, or you want to represent that data very concisely, and so your data is this little snippet of a famous image called Barbara and you transform this image or this data into your favorite.",
                    "label": 0
                },
                {
                    "sent": "Orthonormal basis, in this case.",
                    "label": 0
                },
                {
                    "sent": "One of my favourites is a 2 dimensional wavelet basis an you observe that in that basis a small number of these basis coefficients are large, IE a small number of them are very bright, alot of them are dark and so you think, well, you know if I want to represent this image sparsely or concisely, maybe I should threshold these coefficients.",
                    "label": 0
                },
                {
                    "sent": "I should throw away.",
                    "label": 0
                },
                {
                    "sent": "A bunch of them and I should just keep a few bright ones.",
                    "label": 0
                },
                {
                    "sent": "Set all of the rest to zero, and then when I do, my inverse 2 dimensional wavelet transform, I should get a pretty decent approximation or a pretty good version of my original image.",
                    "label": 0
                },
                {
                    "sent": "So here is the original data.",
                    "label": 0
                },
                {
                    "sent": "Here is the synthesized or reconstructed version of my data, and here is my compression or my sparse representation of the original data.",
                    "label": 0
                },
                {
                    "sent": "If you're a statistician, you might even argue.",
                    "label": 0
                },
                {
                    "sent": "That this image is better than that image because you've somehow removed what you might think of as noise in your data.",
                    "label": 0
                },
                {
                    "sent": "But now this is.",
                    "label": 0
                },
                {
                    "sent": "This is really a very simple cartoon of transform coding.",
                    "label": 1
                },
                {
                    "sent": "You pick an orthonormal basis, you apply a transform and then you threshold the coefficients you set to zero whole bunch of very small ones and then you do an inverse transform.",
                    "label": 0
                },
                {
                    "sent": "You know this.",
                    "label": 0
                },
                {
                    "sent": "You then measure how close is this to the original.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about much more sophisticated ideas than than this simple cartoon.",
                    "label": 0
                },
                {
                    "sent": "Not not all that much more sophisticated, so this is about how to compress images and signals or data.",
                    "label": 1
                },
                {
                    "sent": "How to do it accurately.",
                    "label": 0
                },
                {
                    "sent": "So this is where mathematics comes in.",
                    "label": 0
                },
                {
                    "sent": "How to do it concisely so you think of conciseness, sparsity, and parsimony as all being belonging to statistics?",
                    "label": 0
                },
                {
                    "sent": "And also how to do it efficiently?",
                    "label": 0
                },
                {
                    "sent": "So how to compute things very efficiently so this sort of belongs to the realm of algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this one picture I showed you with a 2 dimensional wavelet transform really is a very simple cartoon because roundabout, maybe the early 1990s.",
                    "label": 0
                },
                {
                    "sent": "The mid 90s signal processors, image coders, people in electrical engineering, decided that they could spend their time trying to develop a really good orthonormal basis for images, one that would represent.",
                    "label": 0
                },
                {
                    "sent": "Images really concisely that would really allow you to compress images, and it turns out that that's very difficult.",
                    "label": 0
                },
                {
                    "sent": "There's some actually some good technical mathematical reasons for why that's very difficult, and instead what they decided that if one orthonormal basis is good for representing my data representing my image, then surely I should get a sparser or a more concise or a more accurate representation.",
                    "label": 0
                },
                {
                    "sent": "If I use more than one orthonormal basis, and so.",
                    "label": 0
                },
                {
                    "sent": "I should put my energy into designing good compression and decompression algorithms and not so much of my energy into designing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Orthonormal basis.",
                    "label": 0
                },
                {
                    "sent": "This is especially true for images.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in this is captured by fantastic poster.",
                    "label": 0
                },
                {
                    "sent": "Which doesn't display so well on this on this projector.",
                    "label": 0
                },
                {
                    "sent": "This poster from 1998, not mathematics awareness poster, so the original data or the original image is this picture of a mandrill.",
                    "label": 0
                },
                {
                    "sent": "And the subsequent pictures below it are the reconstructed representation of the mandrill in different orthonormal bases done sequentially.",
                    "label": 0
                },
                {
                    "sent": "So the first picture is what you would get of the mandrill after you had converted to a certain 2 dimensional wavelet transform and then thresholded the coefficients an reconstructed and up close on the screen.",
                    "label": 0
                },
                {
                    "sent": "You can see there is some difference between the original mandrill.",
                    "label": 0
                },
                {
                    "sent": "And the reconstituted wavelet compression version an.",
                    "label": 0
                },
                {
                    "sent": "In fact, the difference between these two is captured by this picture, and in order to get this difference picture.",
                    "label": 0
                },
                {
                    "sent": "This is also been encoded by a very large collection of orthonormal bases, called wavelet packets, and then the bottom picture is the difference between this representation plus this representation and the original and So what electrical engineers, people and image coding and applied mathematicians discovered.",
                    "label": 0
                },
                {
                    "sent": "Is that different orthonormal bases pull out different features.",
                    "label": 0
                },
                {
                    "sent": "This is especially true of images, or you can you can use language that is especially evocative for features for images.",
                    "label": 0
                },
                {
                    "sent": "Orthonormal basis that consists of wavelets picks out certain types of features, those of wavelet packets pulls out, sort of feathery texture.",
                    "label": 0
                },
                {
                    "sent": "And the bottom one, which is very hard to see.",
                    "label": 0
                },
                {
                    "sent": "Pulls out almost like point list type type textures, type features and so the idea is that instead of using one orthonormal basis to try and capture all of these different features, you should use a whole collection of orthonormal bases, each one sort of tuned or tailored for different features that you have in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our data and so to make that more mathematically precise, we're going to define what we mean by a collection of features or a collection of atoms, and that is a dictionary D in in our end.",
                    "label": 1
                },
                {
                    "sent": "So we're in dimension N, and we have D different atoms.",
                    "label": 0
                },
                {
                    "sent": "Let's normalize them all to have unit Norm.",
                    "label": 0
                },
                {
                    "sent": "An unit L2 norm one and we call each of these atoms we call each of these elements atoms, and if they span all of our end, we say the dictionary is complete and if they are linearly dependent, we say the dictionary is redundant.",
                    "label": 1
                },
                {
                    "sent": "So you really want to think of D as being much larger than N. You have a whole giant collection of of vectors of features and you want to.",
                    "label": 0
                },
                {
                    "sent": "Use a parsimonious collection of them to represent your data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Concisely.",
                    "label": 0
                },
                {
                    "sent": "So how do we talk about representing your data?",
                    "label": 0
                },
                {
                    "sent": "Will we do this in a very simple linear model so we form a matrix fee?",
                    "label": 1
                },
                {
                    "sent": "Its columns consists of elements from the dictionary, atoms from the dictionary, and when we say we form a linear combination literally, what we mean is the matrix vector product fee times C. So in case you are familiar with statistical literature and nomenclature, I have tried to suppress that and justice kept the pictures the relative sizes.",
                    "label": 0
                },
                {
                    "sent": "So you should think of a short and very wide matrix fee is multiplying a very long vector C and I wish to have just a few non zero entries in C so that I have a very small linear combination feedtime see.",
                    "label": 0
                },
                {
                    "sent": "To represent my my data, my Image X.",
                    "label": 0
                },
                {
                    "sent": "Similarly I also want to look at what are the dot products between my input data X and Adams in the dictionary fee, and so that's easy to represent with linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Just take the transpose or the add joint of the dictionary matrix Times X.",
                    "label": 1
                },
                {
                    "sent": "So just so that you keep in mind the dimensions X is short, feed transposes to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what are some examples of redundant or over complete dictionaries?",
                    "label": 0
                },
                {
                    "sent": "Well, one of my favorite examples is the 448 Dirac or Fourier spikes and signs basis.",
                    "label": 0
                },
                {
                    "sent": "This is the union of two orthonormal bases.",
                    "label": 0
                },
                {
                    "sent": "I have complex exponentials.",
                    "label": 0
                },
                {
                    "sent": "These are these guys that oscillate and then I have spikes or impulses.",
                    "label": 0
                },
                {
                    "sent": "These guys like this.",
                    "label": 0
                },
                {
                    "sent": "So in dimension N. There are two end of these vectors and this is what they look like in pictures.",
                    "label": 0
                },
                {
                    "sent": "This is what they look like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Math.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we've got an idea of the players in our story, let's try and formulate some rigorous mathematical problems.",
                    "label": 0
                },
                {
                    "sent": "You know what do we mean by come up with a concise representation of our data, and so there is 3 different.",
                    "label": 0
                },
                {
                    "sent": "Rigorous, precise problems that we could formulate.",
                    "label": 0
                },
                {
                    "sent": "So the first one is we refer to as the exact sparse approximation problem, and this is the idea that you're given an input vector X piece of data X or an image X in RN and a complete dictionary feet, and you would like to find the sparsest coefficient vector, sparsest vector C, such that X is exactly equal to the linear combination feed times C. OK, I use the L0 norm.",
                    "label": 1
                },
                {
                    "sent": "Quasi norm here to count the number of non zero entries in the vector see this literally says find the sparsest representation for X over this dictionary fee so.",
                    "label": 0
                },
                {
                    "sent": "Representing your data exactly is frequently too much to ask for, and in fact it may not even be desirable.",
                    "label": 0
                },
                {
                    "sent": "If you remember this picture of the Patch of Barbara, I actually thought that the reconstructed, the approximate version, actually looked better than the original it somehow denoised, and so maybe what you want to ask for is instead of representing X exactly that you'll be very close to the input vector.",
                    "label": 0
                },
                {
                    "sent": "And close in the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So you can say given an input vector XA dictionary fee and some error tolerance epsilon, I would like to find the sparsest linear representation over my dictionary fee that comes within Euclidean distance epsilon of the input vector.",
                    "label": 0
                },
                {
                    "sent": "Now you could also say, well, I'm not so concerned with my error tolerance.",
                    "label": 0
                },
                {
                    "sent": "I'm really much more concerned with my memory budget for my compression and what I really care about is how many atoms I use in my sparse linear combination and so that goes by the name problem named sparse.",
                    "label": 0
                },
                {
                    "sent": "Given an input vector X and redundant dictionary fee, and a sparsity parameter K you want to find.",
                    "label": 0
                },
                {
                    "sent": "The linear combination of at most K atoms that comes as close to X as possible, so minimize the L2 distance between the input vector X and the linear combination feedtime.",
                    "label": 0
                },
                {
                    "sent": "See subject to using no more than K atoms in your in your representation.",
                    "label": 0
                },
                {
                    "sent": "So these are three very rigorously precisely defined sparse approximation problems.",
                    "label": 0
                },
                {
                    "sent": "Now we're in a setting where we can ask how difficult is it to come up with feasible.",
                    "label": 0
                },
                {
                    "sent": "Algorithms to solve these problems?",
                    "label": 0
                },
                {
                    "sent": "Not not super fast, just a plain old feasible, how?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How efficient are they an?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's a whole slew of bad news, so the basically the bad news is that all of these problems are NP hard, so they were also NP complete, so there in NP and their NP hard so.",
                    "label": 0
                },
                {
                    "sent": "The bunch of very depressing results given an arbitrary redundant dictionary and an arbitrary input vector X, it's NP hard to solve any of these problems.",
                    "label": 1
                },
                {
                    "sent": "Worse, at least for the sparse approximation problem.",
                    "label": 0
                },
                {
                    "sent": "It's NP hard to determine whether or not your optimal error is 0.",
                    "label": 1
                },
                {
                    "sent": "For a given sparsity level K. So in other words, it's NP hard to distinguish between error zero and error error that's not equal to zero.",
                    "label": 0
                },
                {
                    "sent": "That means it's very hard to even approximate solutions to these problems and.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Depressing news and yet also somewhat helpful news in that it means that there's room for approximation algorithms for to solve them.",
                    "label": 0
                },
                {
                    "sent": "So rather than go through the proof of the hardness of these problems, I just sort of want to give you a flavor of the proof of the hardness of these, and also.",
                    "label": 0
                },
                {
                    "sent": "Tell you how this is related to seduco.",
                    "label": 0
                },
                {
                    "sent": "So the reduction is from a for my 2, two a standard, a textbook NP complete problem which is known as exact set cover by three sets and I say it's a textbook problem.",
                    "label": 0
                },
                {
                    "sent": "If you go and pull off the shelf, your copy of Gary and Johnson.",
                    "label": 0
                },
                {
                    "sent": "Yep, it's one of the 20 NP complete problems in there, so it's a constraint satisfaction problem.",
                    "label": 0
                },
                {
                    "sent": "It's related to vertex cover, so these are these algorithms are all over.",
                    "label": 0
                },
                {
                    "sent": "These problems are all of the same.",
                    "label": 0
                },
                {
                    "sent": "Same flavor, and so I'm going to try and cast sparse approximation as one of these constraint satisfaction problems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "X3 C exact set cover by three sets is the following problem, so you're given a finite universe.",
                    "label": 0
                },
                {
                    "sent": "You OK and I really want you to sort of think in pictures to think of analogies to the sparse approximation problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have my universe you can run from one to N down here, and I'm given a collection X of subsets X, One X2.",
                    "label": 1
                },
                {
                    "sent": "There D of them and.",
                    "label": 0
                },
                {
                    "sent": "X each subset contains exactly 3 elements from the universe, and so X 3C problem asks does my collection of subsets contain a disjoint collection whose union is exactly equal to you, and if you start drawing pictures of your collection of subsets where you place you in the Roseanne, you make column vectors out of these subsets X One X2 up to XD.",
                    "label": 0
                },
                {
                    "sent": "You put a ones in the rows of X which correspond to the elements of the universe.",
                    "label": 0
                },
                {
                    "sent": "Pretty soon this starts to look like your dictionary matrix.",
                    "label": 0
                },
                {
                    "sent": "And the X 3C if you were to formulate this as a sparse approximation problem would say form this overcomplete dictionary and you would like to know is there a coefficient vector that you could multiply here such that this matrix feed times your coefficient vector is exactly equal to the all ones vector and such that I have in my coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "I make it so that I.",
                    "label": 0
                },
                {
                    "sent": "Have subsets that don't overlap at all, so this is a constraint satisfaction problem and you can show that if you could solve a sparse approximation problem, you could if you could solve this problem you could solve sparse approximation so you can also frame sudoku as a constraint satisfaction problem.",
                    "label": 0
                },
                {
                    "sent": "You can generate a.",
                    "label": 0
                },
                {
                    "sent": "You can also frame it as a sparse approximation problem and you can create a dictionary matrix out of all the constraints for Sudoku.",
                    "label": 0
                },
                {
                    "sent": "And you can investigate how well sparse approximation algorithms are, how good heuristic they are for solving Sudoku problems.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a fun exercise.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a sketch of the theoretical results.",
                    "label": 0
                },
                {
                    "sent": "I think it's probably more useful to understand the implications of the theoretical result, so you might think that this is bad news, right given.",
                    "label": 0
                },
                {
                    "sent": "Any polynomial time algorithm for solving any of these three problems?",
                    "label": 1
                },
                {
                    "sent": "There is some redundant dictionary fee and some signal X such that the algorithm is just flat out wrong.",
                    "label": 0
                },
                {
                    "sent": "So when when people come to you and say I have a polynomial time algorithm for solving sparse approximation problems, you should go, uh, huh?",
                    "label": 0
                },
                {
                    "sent": "I would be very interested in this, but probably there is some pair of dictionary on signal which will foil your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is an incredibly pessimistic setting.",
                    "label": 0
                },
                {
                    "sent": "You know it captures the worst case behavior, and it says there's there.",
                    "label": 0
                },
                {
                    "sent": "There is some pathology out there that will probably foil the polynomial time algorithm, and even worse, probably there are examples out there for, you know, for which you can't even hope to approximate the solutions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you know, by my nature, I'm an optimist, not a pessimist.",
                    "label": 0
                },
                {
                    "sent": "And you know this this horribly pessimistic result has not stopped electrical engineers from developing state of the art image coders for image compression.",
                    "label": 0
                },
                {
                    "sent": "And they certainly are not doing an exponential time algorithm for encoding and decoding images.",
                    "label": 1
                },
                {
                    "sent": "So, so what's going on?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why do these things seem to work?",
                    "label": 0
                },
                {
                    "sent": "So well, there are a couple of reasons, so one of them is that natural dictionaries are very far from arbitrary, so this union of spikes and signs my favorite dictionary is not an arbitrary dictionary by any means.",
                    "label": 1
                },
                {
                    "sent": "It does not look.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At all, like this dictionary matrix that you would get from an instance of X3, C, it has all sorts of beautiful structure.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But and it's entirely possible that all of this beautiful structure you can exploit in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "The other sort of hope that one would have for solving at least parts of these sparse approximation problems is that we know just fine how to solve sparse approximation problems when our dictionary is not redundant, but it is an orthogonal basis, or an orthonormal basis, so they're very simple algorithms for solving all of these problems in that case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to emphasize the sort of peculiarity of in somewhat uselessness of those hardness results, and emphasize that the hardness depends on both the dictionary fee and the input signal X being arbitrary, not random, but arbitrary, and if you change either one of those instance types, you change the nature of the problem so.",
                    "label": 1
                },
                {
                    "sent": "Frequently you'll hear people say I have a polynomial time algorithm for solving an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "That's not really what they mean.",
                    "label": 0
                },
                {
                    "sent": "Most likely what they mean is there dictionary fee is perhaps random.",
                    "label": 0
                },
                {
                    "sent": "It comes from some sort of distribution and their input signal is arbitrary.",
                    "label": 0
                },
                {
                    "sent": "You know, in which case the problem begins to have the flavor more of compressive sensing, or they have a redundant and arbitrary redundant dictionary, but they have some sort of random signal model.",
                    "label": 1
                },
                {
                    "sent": "They impose some sort of prior knowledge, some sort of parameterized signal model.",
                    "label": 0
                },
                {
                    "sent": "All of those are really different from the from both of these cases being being arbitrary, and so you know if I tell you that I'm going to build my image coder based on wavelet packets and a 2D wavelet transform, that is a fixed redundant dictionary fee and I can design efficient algorithms for that one particular fixed dictionary.",
                    "label": 0
                },
                {
                    "sent": "So I just want to point out to some extent the uselessness of these hardness results.",
                    "label": 0
                },
                {
                    "sent": "An to be able to be aware of what sort of type of input that input problem that you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have OK so let us.",
                    "label": 0
                },
                {
                    "sent": "Try and figure out what is it about the orthonormal basis case.",
                    "label": 0
                },
                {
                    "sent": "The transform coding case that permits us to have we hope feasible algorithms and you know, discuss what might be possible.",
                    "label": 0
                },
                {
                    "sent": "We've discussed what is impossible.",
                    "label": 0
                },
                {
                    "sent": "Let's let's do what is possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so I showed you the picture of Barbara at the beginning where we have an orthonormal basis.",
                    "label": 0
                },
                {
                    "sent": "So there's A and we.",
                    "label": 0
                },
                {
                    "sent": "Computed all of the coefficients in the basis.",
                    "label": 0
                },
                {
                    "sent": "Similarly set to 0.",
                    "label": 0
                },
                {
                    "sent": "A bunch of the really small ones, so you could.",
                    "label": 0
                },
                {
                    "sent": "Do a slightly more sophisticated algorithm and you could pull off the atoms or the the vectors in the basis one at a time in decreasing size.",
                    "label": 1
                },
                {
                    "sent": "I should say you want to pull them off in an order that is commensurate with with the sizes of the dot product of your signal.",
                    "label": 0
                },
                {
                    "sent": "In that basis there we go OK, and then if you say, stop after you've pulled off the K biggest.",
                    "label": 0
                },
                {
                    "sent": "That gives you the best approximation in the L2 cents to the input vector using at most K atoms.",
                    "label": 0
                },
                {
                    "sent": "So since the basis is orthonormal, you can use the Pythagorean theorem or the Plancherel theorem.",
                    "label": 0
                },
                {
                    "sent": "If you want to be fancy.",
                    "label": 0
                },
                {
                    "sent": "And pulling off the K biggest at the atoms that correspond to the K biggest dot product in magnitude will give you the best K term representation, so that's a very simple algorithm, just guaranteed to work.",
                    "label": 0
                },
                {
                    "sent": "Runs in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "So what is it about the orthonormal basis case that makes this work?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other than the Pythagorean theorem, well, it is exactly the Pythagorean theorem.",
                    "label": 0
                },
                {
                    "sent": "The inner products between the atoms that are in the basis are small, which is to say it's zero.",
                    "label": 1
                },
                {
                    "sent": "So all of these, all of these atoms are orthogonal to one another.",
                    "label": 0
                },
                {
                    "sent": "And so when you take a dot product of your signal with this Atom versus this Atom, can you all see this?",
                    "label": 0
                },
                {
                    "sent": "It's very hard to confuse an Atom that points in this direction with an Atom that points in this direction.",
                    "label": 1
                },
                {
                    "sent": "When you're when you're trying to figure out which which Adam should I choose so the converse of that is that when the atoms are nearly parallel.",
                    "label": 1
                },
                {
                    "sent": "And you take a dot product with your signal.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to tell which dot product is bigger and hence which Atom you should choose.",
                    "label": 0
                },
                {
                    "sent": "Is it this one that's making the dot product bigger or this one so?",
                    "label": 0
                },
                {
                    "sent": "There's something important about the geometry of these redundant dictionaries that is going to allow us to have feasible algorithms.",
                    "label": 0
                },
                {
                    "sent": "I should say that if you haven't done the little mental trick of the naive algorithm is to do brute force search.",
                    "label": 0
                },
                {
                    "sent": "Which is exponential time to find like the best K term representation now would be a good time to convince yourself that brute force search works, but it's not particularly efficient.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I said there's something very important about the geometry of the dictionaries that is going to allow us to come up with efficient or at least feasible algorithms, and I've been pointing my fingers like this to say that, you know, it really matters what is the angle between atoms?",
                    "label": 0
                },
                {
                    "sent": "Is the angle 90 degrees or is it something much, much smaller so statisticians actually defined the notion of coherence of a dictionary?",
                    "label": 1
                },
                {
                    "sent": "Which is the biggest dot product between two distinct elements in the dictionary?",
                    "label": 1
                },
                {
                    "sent": "Two distinct atoms in the dictionary, and it's somewhat of an unfortunate name because small coherence is good and large coherence is bad.",
                    "label": 0
                },
                {
                    "sent": "Incoherence here is the resource, so be it.",
                    "label": 0
                },
                {
                    "sent": "This is the definition.",
                    "label": 0
                },
                {
                    "sent": "So these two atoms are very hard to tell apart.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to figure out which atoms should you choose this one or this one when they point in almost the same direction.",
                    "label": 0
                },
                {
                    "sent": "Whereas these guys are all fairly well spread out, so it's easier to figure out which Atom you should pick to make up your.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is where.",
                    "label": 0
                },
                {
                    "sent": "We're missing something.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You might ask is it possible to construct very large?",
                    "label": 0
                },
                {
                    "sent": "Redundant dictionaries that are also very incoherent, so the first example explicit example that we gave was the union of two orthonormal bases, spikes and signs.",
                    "label": 0
                },
                {
                    "sent": "They in dimension N There are two N atoms in that dictionary, which really isn't very big, it has coherence 1 / sqrt N. If you make sure to normalize your complex exponentials so they all have norm one.",
                    "label": 0
                },
                {
                    "sent": "Then the dot product between any spike in any complex exponential is 1 / sqrt N. So you could ask can I get can I get bigger dictionaries with that type of incoherence?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So here are here's an example where.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Actually, here's an example where the answer is unfortunately no.",
                    "label": 0
                },
                {
                    "sent": "I have a very large dictionary.",
                    "label": 0
                },
                {
                    "sent": "The number of atoms in the dictionary is N log.",
                    "label": 0
                },
                {
                    "sent": "NO log N times the dimension, but the coherence is 1 / sqrt 2, which is pretty big.",
                    "label": 0
                },
                {
                    "sent": "There certainly are very nice, very large dictionaries where the coherence is small.",
                    "label": 0
                },
                {
                    "sent": "It's like 1 / sqrt N and there are N squared different vectors.",
                    "label": 0
                },
                {
                    "sent": "So at this point I should allude to the construction of spherical codes.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "Redundant dictionary is equivalent to a spherical code.",
                    "label": 0
                },
                {
                    "sent": "You have atoms that all have Euclidean length one.",
                    "label": 0
                },
                {
                    "sent": "These are points on the unit sphere in dimension N. And you can ask, how many vectors can I stuff into end dimensions?",
                    "label": 0
                },
                {
                    "sent": "How many points can I put on the sphere in end dimensions so that they are all relatively far apart from one another?",
                    "label": 0
                },
                {
                    "sent": "So the angles that they all have with one another is pretty small.",
                    "label": 0
                },
                {
                    "sent": "This is a questions that people have looked at for a long time in coding theory and one of the beautiful constructions of a very large incoherent dictionary is occur dot code.",
                    "label": 0
                },
                {
                    "sent": "It is a union of end different.",
                    "label": 0
                },
                {
                    "sent": "Orthonormal basis, so there are N of these orthonormal bases, each of size N, which is how you get N ^2.",
                    "label": 0
                },
                {
                    "sent": "Adams was fantastic.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we've got in place.",
                    "label": 0
                },
                {
                    "sent": "A good intuition about redundant dictionaries, and now that we've got in place where three different sparse approximation problems, we can ask.",
                    "label": 0
                },
                {
                    "sent": "OK, how do we now come up with algorithms for computing sparse approximations of our data?",
                    "label": 0
                },
                {
                    "sent": "And can we use geometric constraints on our redundant dictionaries to guarantee that our algorithms are both correct and fairly efficient?",
                    "label": 0
                },
                {
                    "sent": "So in order to draw inspiration for creating our algorithms, we're going to look at building our approximation one step at a time, just like we did for the orthonormal basis, where we pull off in in order of descending dot products, we pull off atoms one at a time.",
                    "label": 0
                },
                {
                    "sent": "We're going to pick at each step.",
                    "label": 0
                },
                {
                    "sent": "We're going to pick the best Atom at that at that step.",
                    "label": 1
                },
                {
                    "sent": "This is a very basic.",
                    "label": 0
                },
                {
                    "sent": "So incredibly simple greedy algorithm IK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formulation.",
                    "label": 0
                },
                {
                    "sent": "And one of the first I should say the best well studied algorithm in this area is called orthogonal matching pursuit.",
                    "label": 1
                },
                {
                    "sent": "Oh for orthogonal and MP for matching pursuit.",
                    "label": 0
                },
                {
                    "sent": "So it was.",
                    "label": 0
                },
                {
                    "sent": "Sort of most promoted in the signal processing community in the early 90s.",
                    "label": 0
                },
                {
                    "sent": "Bimala Anjang, although it it you know if you go back and look in the literature arose in many forms much earlier than that, and this is really the overall architecture of greedy algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the input is, whoops, the input is the dictionary fee.",
                    "label": 1
                },
                {
                    "sent": "Signal X An let's say K steps and you initialize your coefficient vector to be all zeros, and then there are two parts to the iteration.",
                    "label": 0
                },
                {
                    "sent": "The first part is the selection step you want to find the vector in the dictionary that maximizes the dot product with the residual.",
                    "label": 0
                },
                {
                    "sent": "The residual being the input minus feet time.",
                    "label": 0
                },
                {
                    "sent": "See your current approximation.",
                    "label": 0
                },
                {
                    "sent": "So there's a selection step or a maximization step, and then there's an update step you want to.",
                    "label": 0
                },
                {
                    "sent": "Update The coefficient C in your current approximation in the case of OMP, you want to orthogonalize.",
                    "label": 0
                },
                {
                    "sent": "You want to let the coefficient C be the orthogonal projection onto the current set of atoms that make up your representation, i.e.",
                    "label": 0
                },
                {
                    "sent": "You want to solve least squares problem at each iteration, and then you iterate.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I know that there are many experts in the audience on these types of algorithms, because I've seen a number of the posters outside that are all that are variations and much more sophisticated versions of this very basic greedy architecture.",
                    "label": 0
                },
                {
                    "sent": "So there's matching pursuit.",
                    "label": 0
                },
                {
                    "sent": "There's where you don't do an orthogonal projection to update the coefficients.",
                    "label": 0
                },
                {
                    "sent": "There's thresholding where you take the, not the one biggest Atom, not the Atom that has the one biggest product with the residual.",
                    "label": 0
                },
                {
                    "sent": "But maybe you take a group of them.",
                    "label": 0
                },
                {
                    "sent": "Maybe you take all of those that are above a certain threshold, and if you take all of those that conform to certain sparsity pattern, you can also have alternative stopping rules.",
                    "label": 0
                },
                {
                    "sent": "You don't just run it for K steps, you have all different sorts of error criteria, and there are many many variations, including a whole bunch in the in the posters outside.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you might ask, well, OK, how good a job does OMP do?",
                    "label": 0
                },
                {
                    "sent": "Is it provably good and is it a provable approximation algorithm for an NP hard problem?",
                    "label": 0
                },
                {
                    "sent": "And so initially some applied mathematicians analyzed this algorithm and they came up with.",
                    "label": 0
                },
                {
                    "sent": "An answer to this question that at least I found personally unsatisfying, so the result basically says if you have a complete dictionary and an arbitrary input vector X, after T steps, I know that my error from OMP is no more than some constant times 1 / sqrt T. So I found this an unsatisfying result because it doesn't tell me if X has a sparse representation.",
                    "label": 1
                },
                {
                    "sent": "That's, you know, consists of two atoms.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell me anything about stopping after 2 steps.",
                    "label": 0
                },
                {
                    "sent": "It just says by the time I have run for N steps my error is zero essentially so this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not particularly helpful.",
                    "label": 0
                },
                {
                    "sent": "So you'd like to know when does this algorithm actually identify sparse?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Presentations and so.",
                    "label": 0
                },
                {
                    "sent": "For this simple algorithm you can formulate a sufficient condition on the geometry of the dictionary for for.",
                    "label": 1
                },
                {
                    "sent": "For recovering exactly sparse signals, or for identifying the support set Lambda of these sparse signals.",
                    "label": 1
                },
                {
                    "sent": "So for the sake of time I'm just going to say that there is an exact recovery condition which is phrased in terms of the geometry of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this particular phrasing of the theorem you need to know the support set or the sparsity pattern Lambda, and a more practically useful, although perhaps less geometrically interesting, result says that this exact recovery condition holds whenever you do not ask for too many vectors in your sparsity pattern, and too many is a function of one over the coherence.",
                    "label": 0
                },
                {
                    "sent": "So here is where coherence is used as a resource.",
                    "label": 0
                },
                {
                    "sent": "And as a way of saying, as long as you ask for a fairly sparse signal sparser than one over the coherence, then this algorithm is probably good, and you can also analyze its running time.",
                    "label": 0
                },
                {
                    "sent": "You might ask well.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, what is one over the coherence?",
                    "label": 0
                },
                {
                    "sent": "For most, you know, sort of large dictionaries for most redundant dictionaries, the coherence is about 1 / sqrt N, and so you can ask for getting good approximations to signals with sparsity less than roughly square root of the dimension.",
                    "label": 1
                },
                {
                    "sent": "Which is a.",
                    "label": 0
                },
                {
                    "sent": "You know, it's a this is something checkable and this is something that you know.",
                    "label": 0
                },
                {
                    "sent": "It gives you a reasonable practical rule of thumb.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "Not going to discuss how to prove these just to prove these results, just give you a flavor of the analysis of the algorithm and why this exact recovery condition might come in and analyzing OMP an that is OMP is always going to pick good atoms if the good atoms have bigger dot product with the residual than the bad atoms, and so you always want to make sure that this ratio is less than one.",
                    "label": 1
                },
                {
                    "sent": "So remember, Lambda is the support set and this is talking about Dot product with your residual and so you can transform this ratio.",
                    "label": 0
                },
                {
                    "sent": "This greedy selection ratio into a condition that looks like the exact recovery condition.",
                    "label": 1
                },
                {
                    "sent": "So this is a flavor of the analysis of the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "More flavors tastes of the results are of this form.",
                    "label": 0
                },
                {
                    "sent": "So OMP is kind of a simple minded algorithm in kind of.",
                    "label": 0
                },
                {
                    "sent": "It is incredibly simple minded.",
                    "label": 0
                },
                {
                    "sent": "And so while it does run in polynomial time, and while there are fairly are very easily checkable conditions, the output of the algorithm is pretty good, but it doesn't recover the spokes.",
                    "label": 0
                },
                {
                    "sent": "Exactly sparse representation exactly.",
                    "label": 0
                },
                {
                    "sent": "So what you can get is an approximation algorithm for a sparse approximation.",
                    "label": 0
                },
                {
                    "sent": "So you can show that as long as you're not asking for too many vectors, too many as a function of one over the coherence, the approximation that you return the L2 error of that approximation is guaranteed to be within some factor of the optimal representation for X OK, and this factor is not a constant.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's like you know the square root of 1 + 6 * K. You can do better with more sophisticated analysis.",
                    "label": 0
                },
                {
                    "sent": "And by cranking down on on how sparse so signal you're asking for sparsity versus one over the square root of the coherence.",
                    "label": 0
                },
                {
                    "sent": "And if you make your algorithm more sophisticated, you do a two phase greedy pursuit algorithm where you run something that looks like OMP for some number of steps, and then you do a big cleanup pass.",
                    "label": 0
                },
                {
                    "sent": "Then you can get an honest to God.",
                    "label": 0
                },
                {
                    "sent": "Constant factor approximation in the optimal error, and you can even if you let up on that coherence a little bit, you can actually figure out exactly what that constant is, how it depends on the coherence and K the number of terms in your representation.",
                    "label": 0
                },
                {
                    "sent": "So it's a little embarrassing is that these types of sophisticated results came before these simple minded results, so sometimes sometimes writing the 2nd paper is better than writing the 1st paper.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is all about greedy algorithms.",
                    "label": 0
                },
                {
                    "sent": "I don't want to give you the impression that this is the end all be all in algorithmic formulation, because many of you are actually more familiar with an alternative algorithm approach which is really more suitable for two of the three problems SO2A.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The three problems are non convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize the L0 norm of a coefficient vector subject 2, an equality constraint or.",
                    "label": 0
                },
                {
                    "sent": "I have an L2 error constraint and you so these are both non convex.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian problems astandard algorithmic formulation or or trick is to relax these optimization problems to convex problems and so here are the various convex relaxations you can relate this parameter Delta to this parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once you have a convex optimization problem who have a whole slew of algorithms at your disposal.",
                    "label": 1
                },
                {
                    "sent": "And so some of the names associated with people who have studied these and many, many, many others are listed here.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of things that I want to point out.",
                    "label": 0
                },
                {
                    "sent": "One of them is that just because you formulated the problem as a linear program doesn't mean you're done so for practical purposes, you need to have a good LP solver and good LP solvers that work at scale that work for your specific type of problems.",
                    "label": 0
                },
                {
                    "sent": "This is really a very very high art form, and as a subject matter all to itself, so that is a comment about practicality.",
                    "label": 0
                },
                {
                    "sent": "Comment about theory is that.",
                    "label": 0
                },
                {
                    "sent": "You still have to show that the solution to your convex relaxation is related to the solution to the nonconvex problem, and so there are all sorts of interesting questions that have to be done even after you formulate your problem as a convex Optima.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient problem, so it's sort of interesting is that the sufficient geometric conditions for solving these convex optimization problems and the relationship of their solutions to the nonconvex ones are exactly the same as the ones for the greedy.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythms.",
                    "label": 0
                },
                {
                    "sent": "OK, there are other alternate optimization formulations.",
                    "label": 0
                },
                {
                    "sent": "Constrained minimization, unconstrained minimization.",
                    "label": 0
                },
                {
                    "sent": "This one goes by the name of L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Almost certainly some of you have seen this many.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you are experts on this, so I sort of wanted to point out that two of these three sparse approximation problems are amenable to convex relaxation and the other one is not, so it is much more well suited to greedy algorithms so sparse.",
                    "label": 0
                },
                {
                    "sent": "Is a perfectly fine problem with which to throw out greedy algorithms and these other two are make more sense to study convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to skip the connection between sparse approximation and statistical learning because.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's hinted at by many of these solutions.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They come up in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mystical learning theory.",
                    "label": 0
                },
                {
                    "sent": "The algorithms have different names, but they're very similar flavor.",
                    "label": 0
                },
                {
                    "sent": "The greedy algorithms things like forward selection, forward stagewise regression, least angle regression, constrained optimization, things like quadratic programming, lasso, and unconstrained optimization.",
                    "label": 0
                },
                {
                    "sent": "All of these regularization techniques, and I would be remiss to not point out this beautiful very early paper that shows its process.",
                    "label": 0
                },
                {
                    "sent": "A sparse approximation formulation is equivalent to support vector machines.",
                    "label": 1
                },
                {
                    "sent": "So beautiful early work, long before most of the work was done in sparse approximation.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you might also ask what is the connection between sparse approximation and compressed sensing, which also there are several talks right after mine.",
                    "label": 0
                },
                {
                    "sent": "On this topic in many post.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are two 2 main points I want to make quickly.",
                    "label": 0
                },
                {
                    "sent": "One of them is that you want to interchange the role of data and coefficients or measurements and image and this is what you can't now and this is what you wish to.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reconstruct.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can think of it more as a design problem.",
                    "label": 0
                },
                {
                    "sent": "I want to design a good matrix Phi such that if I take linear combinations of a sparse vector an collect these measurements from these measurements and this matrix I wish to recover a few important pieces of information about.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and quickly the difference between sparse approximation and compressed sensing is like the difference in route finding between am I close to my route.",
                    "label": 0
                },
                {
                    "sent": "Where am I close to 0?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So am I doing a good job in representing why or am I doing a good job in representing X?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the two main differences many more parameters that you could play with since?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Design problem and lots of applications aside from statistical learning theory and more.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teen learning",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I think this is a fantastic area of mathematics and statistics and computer science all coming together, and hopefully I've given you some idea of the flavor of the computer science and the applied math results an I encourage you to go look at the papers in your field that talk about sparsity as well.",
                    "label": 0
                },
                {
                    "sent": "There there really excellent thank you.",
                    "label": 0
                }
            ]
        }
    }
}