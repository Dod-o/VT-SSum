{
    "id": "blyso242bfsqcznin52ukmhuq4fwxxqc",
    "title": "Multilingual Document Retrieval Through Hub Languages",
    "info": {
        "author": [
            "Primo\u017e \u0160kraba, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 16, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Multilingual Information Access"
        ]
    },
    "url": "http://videolectures.net/is2012_skraba_hub_languages/",
    "segmentation": [
        [
            "Marco gave a nice introduction about basically what I'm going to be talking about.",
            "And so it's quite a long title that we have here.",
            "It's multilingual document retrieval and hub languages, so.",
            "1st, I'm just going to mention a bit, what's well, kind of the problem of document retrieval, which I think everyone is pretty familiar with.",
            "So the idea is that we have collections of documents and just like the name says, what we really want is given some example document.",
            "How can we find documents that are similar?",
            "This has been a fairly old problem, been very well studied, so now we come into situations that are.",
            "The methods that have been developed don't really work."
        ],
        [
            "For so the first one, I would say is this multilingual part, right?",
            "So we have the web is no longer just in English.",
            "The general motivating example in all the experiments we're going to have our from Wikipedia.",
            "Here I have the Wikipedia page for data mining and of course Wikipedia comes in different languages, so we might want to have the German one or the Spanish one.",
            "And if we look at Wikipedia as just one big.",
            "Giant document collection.",
            "Well, you know the certainly these documents should be related, right?",
            "So if I'm interested, if I have an English document and I want to ask for a specific language or maybe just a list of all documents in other languages, kind of the links that exist in Wikipedia should certainly give the right information.",
            "So the way we're going to cast this problem is a little bit different, though.",
            "We're going to use the links that we know, that is, you know, in Wikipedia itself, there are links between the different languages, so we're going to use that information to try to extend it to places where places where these links may not exist.",
            "So essentially really find.",
            "All reasonably similar documents across all languages.",
            "OK."
        ],
        [
            "So before we get to the really multilingual part and the hub languages and all the other complications, I just wanted to go through an kind of say what the general setting is.",
            "Let's say we just have two languages, so we're going to use a very simple representation bag of words, which essentially just means that all our languages so you know if we have a language, XML, language, Y, each language is going to be represented by just a vector space, so just a bunch of vectors.",
            "Where the individual dimensions are the words that occur first.",
            "The first thing we're going to do is we're going to find the basis for each language independently.",
            "So in principle we don't really know.",
            "We shouldn't really know what's 1 document and in one language looks like as document in another language because we're not really going to use machine translation or anything heavy like that.",
            "But we're just going to find some reasonable representation of these languages independently."
        ],
        [
            "And then what we're going to want to do is, well, we're going to assume that these representations are actually accurate and represent the documents in the topics well.",
            "And then we're going to somehow map them together into one space.",
            "The problem is if we were just working in one language, well, you know, I give you another query document and it's fine.",
            "I can just map it into this vector space.",
            "The words that occur in each language are the same.",
            "It's fine, everything works, but hear the words that happen in one language obviously don't happen in another language.",
            "So what we need is some way to make these things come parable.",
            "And since we already dealing with vector spaces, the natural thing to do is to just say, well, OK, I have my basis in XI, have my basis in Y.",
            "Let's align these bases somehow, and then essentially we get some new space here where we can actually compare these different things.",
            "Yeah, so this is something we've called cross lingual similarity and essentially the whole point is that through this this alignment we actually reduce the multilingual or bilingual problem into just the one language case and we can use the same methods that we have been developed before."
        ],
        [
            "So just one word about kind of representations.",
            "Well, we have a lot of different languages, and you know these are very high dimensional sparse spaces.",
            "So one of the first things we do is we always compute a low rank approximation.",
            "This is because alignment in general is a very difficult problem, and we're going to be learning from relatively small alignments, so we want low dimensional spaces.",
            "So the way we do this is quite a classical technique.",
            "It's the single value decomposition, and in principle what it really gives us is this topic space, because the dimensions the directions in these high dimensional spaces that are going to be most.",
            "Presented are going to be exactly the topics that are most prevalent and.",
            "The hope is, and actually what ends up happening is that what we do is we are taking this large space of.",
            "You know of very language specific documents and mapping it into some independent topic space.",
            "So it just.",
            "I mean, you're just splitting each vector into these dimensions, right?",
            "So I mean, it's just a direct sum that you're doing."
        ],
        [
            "OK, so now we have our representation.",
            "So now the second problem is of course, how do we actually map these things together so and align them so that we can actually make a comparison?",
            "Well, there's many different ways to do it.",
            "We did experiments on three different approaches.",
            "The first one is called Cross Lingual, LSI, which is essentially just in some sense the most agnostic way to do things where we just group things together and hope that decomposition takes care of things.",
            "The next is least squares, where essentially we're looking for a map W from the SpaceX to Y.",
            "So essentially we're mapping any document in X into the space of Y and comparing it there.",
            "So when we're learning W, we obviously need some optimization, some cost function, and here we just use the Frobenius norm and then the last method is regression Canonical analysis.",
            "And here I don't want to really concentrate too much on the math, but the one thing to notice here if we look at just the formulas very briefly, is that here what you're doing is you're taking a document.",
            "In X and you're mapping it first through a pseudo inverse of this basis that you found, whereas here the order is different, right?",
            "So here first you apply something from the basis in Y and it's not really an inverse, but it's the transpose.",
            "These things are very deeply related, but we're not exactly sure the exact relationships between all the inverses and transpose is an what these actually represent, and I'll get back to that in the experiments."
        ],
        [
            "So that's just kind of the two language case.",
            "So what do we have now?",
            "We have our representations, we have our.",
            "We have our optimization techniques to learn Maps, but now what's the whole problem that we're trying to solve?",
            "Well, in Wikipedia, we all know that the distribution of languages isn't Even so if we have a large intersection, obviously we have more correspondences.",
            "Things are easier to solve.",
            "It's easier to optimize things that are.",
            "That are more aligned so English in German have 4 million and 1 million articles.",
            "I'm sure we could find a good intersection, but what about Selena in Hindi which only have 100,000 language, 100,000 articles each?",
            "Well, So what I'm going to say is that, well, let's say I'm going to say that these big languages, English and German, are going to be hub languages.",
            "And what that means is that.",
            "If I take Slovene well, most of these articles are also going to exist in English.",
            "Likewise, Hindi is also going to have most of this common articles in with English.",
            "But the common articles between Slovene and Hindi are not that big, so the goal is now, well.",
            "I'm going to take advantage of the fact that this intersection is big, and this intersection is big and make up for the fact that this intersection is not particularly big."
        ],
        [
            "So here is kind of the general idea.",
            "Well, you know I'm going to assume that.",
            "I can learn this very well.",
            "I can learn this very well, but you know this is quite sparse and quite small, so I'm going to go through English to actually learn this, and that's where the hub languages part of it comes in.",
            "So one of the first things we could think about doing is we can forget all about the representation stuff and say, well, you know.",
            "If I have a link here and I have a link here, I can just compose the two and I get a correspondence there, but.",
            "Part of the problem with that is this is a very common tutorial representation, making it quite hard to generalize, so that's actually why we go to the vector space model where we actually don't use the combinatorial information to actually learn Maps between the representations, and these actually extend across the language space is much better."
        ],
        [
            "OK, so bit about the experiments as the example shows, I.",
            "We used Salinan Hindi with English as a hub.",
            "There's been.",
            "A lot of quite a bit of preprocessing, but the most important one is that stub documents are removed because they don't really contain any information and to give you an idea about the numbers, well, Celine and English has about 50,000 correspondences, Hindi and English has 15,000, but Celine and Hindi only have 4000.",
            "And of those almost all of them exist also in English."
        ],
        [
            "So in principle this is quite a simple idea, but then the devil is always in the details, so there's actually when we're thinking about document retrieval.",
            "There's five different ways to do this.",
            "Well, we can map Slovenian directly to Hindi.",
            "We can map Hindi directly to Slovenian.",
            "We can map Slovenian to English and Hindi to English, so that we do the comparison in our Hub language.",
            "Or we could go through English from Slovenian into English and Hindi or likewise.",
            "The other way around, and the key difference here is what language or what space we're actually doing.",
            "Our document retrieval in.",
            "As it turns out, this becomes actually an important and important part of this."
        ],
        [
            "So.",
            "We used actually three different subsets, so either we used all or most of the alignment information.",
            "We use only alignment information that's consistent through the Hub language.",
            "So in this case we would only use things that exist in English Hindi anseline.",
            "Or we remove all of those common things and only use the information that's in the hubs and in some sense this is really the best test to understand what's going on, because here there's already.",
            "We've already kind of halfway reduced it to a bilingual setting."
        ],
        [
            "So here are some results.",
            "I'm just going to highlight some of the important things.",
            "So you know we have our different ways of doing these things, and the interesting thing is that a lot of the times, well, you know if we do things directly and we have all the first of all, if we use all the information, it's the results are generally the best.",
            "Common information tends to be very compareable, the empty ones, that is where we don't actually have where we go exclusively through the hub language.",
            "The results are much, much worse.",
            "That's somehow unsurprising.",
            "On the other hand, this kind of very agnostic method.",
            "This baseline is also tends to work quite reasonably well.",
            "In this case.",
            "One of the things that I'll point out in the next slide though, is."
        ],
        [
            "If we look at this baseline a little more carefully is, well, OK.",
            "Yes, the common anol is always going to be the best, because we've thrown the most information in common is reasonable.",
            "But if we look at empty, well, you know it's quite bad.",
            "This this kind of agnostic decomposition doesn't work very well if we don't already have some relatively large."
        ],
        [
            "Overlap whereas yeah so."
        ],
        [
            "Let's see, we have.",
            "Here for the empty."
        ],
        [
            "As if we look for least squares, well, you know it's a little bit better and actually we can even make it a bit better if we go the other way around.",
            "I don't think the numbers themselves are that particularly interesting here, but what is interesting is that."
        ],
        [
            "These numbers aren't symmetric.",
            "It really does matter which way you map, and because we're learning linear Maps and there's a lot of different choices and how we can learn these Maps, understanding this asymmetry is kind of what we're currently trying to do, and I think it's very important to kind of optimize.",
            "To get better performance because obviously.",
            "In One Direction, we're utilizing different information for the other direction.",
            "OK, so."
        ],
        [
            "We did one more test, which is this kind of empty for this empty representation, and to say, well, you know what does that empty representation means.",
            "It means that, well, all the alignments I have here map to something here and in the other language and Maps to here and there is no common topical intersection."
        ],
        [
            "And So what we did was, well, let's say we knew the alignment, so we just learned our representations independently in English, and we just took documents that were in one language and saw how well we could do retrieval if we already knew the right correspondence.",
            "So essentially, this all this is really a test of how well these things are represented in English, and the idea to have here is that, well, if the representations are kind of orthogonal.",
            "Then anything we map into one space is going to map into zero in the other, and so we've essentially lost all the information.",
            "The good news is, is that you know the mean retrieval rate was the mean.",
            "Precision was very good, which tells us essentially that the representations are good, that the information is there and essentially in order to get those numbers higher that I showed before, what we need is we need to better ways to learn the Maps.",
            "OK, so."
        ],
        [
            "What's kind of the whole take home message here is that well, we can take advantage of the distribution of languages.",
            "The use of representations rather than combinatorial information means that we can, actually.",
            "I mean we can actually work in cases where there's no.",
            "Direct combinatorial information.",
            "Because we've spread it out over the entire space through these linear representations and through kind of.",
            "A select experiment.",
            "We can also show that the linear representations do have.",
            "They do still contain the relevant information we need.",
            "As I mentioned, the kind of.",
            "The kind of atypical things we don't really expect, or that the space where you do the retrieval is important is very important.",
            "These non symmetry Maps I think.",
            "Is something that needs to be investigated further and in response to this.",
            "Also you know right now we're doing we're learning these Maps and kind of a relatively simple way through least squares or some simple optimization technique, But what we'd really like to do is we'd like to formulate The thing is just one big giant optimization problem so that we have some idea how to best utilize all the information, and I think that's it for now, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Marco gave a nice introduction about basically what I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "And so it's quite a long title that we have here.",
                    "label": 0
                },
                {
                    "sent": "It's multilingual document retrieval and hub languages, so.",
                    "label": 1
                },
                {
                    "sent": "1st, I'm just going to mention a bit, what's well, kind of the problem of document retrieval, which I think everyone is pretty familiar with.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we have collections of documents and just like the name says, what we really want is given some example document.",
                    "label": 0
                },
                {
                    "sent": "How can we find documents that are similar?",
                    "label": 0
                },
                {
                    "sent": "This has been a fairly old problem, been very well studied, so now we come into situations that are.",
                    "label": 0
                },
                {
                    "sent": "The methods that have been developed don't really work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For so the first one, I would say is this multilingual part, right?",
                    "label": 0
                },
                {
                    "sent": "So we have the web is no longer just in English.",
                    "label": 0
                },
                {
                    "sent": "The general motivating example in all the experiments we're going to have our from Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "Here I have the Wikipedia page for data mining and of course Wikipedia comes in different languages, so we might want to have the German one or the Spanish one.",
                    "label": 0
                },
                {
                    "sent": "And if we look at Wikipedia as just one big.",
                    "label": 0
                },
                {
                    "sent": "Giant document collection.",
                    "label": 0
                },
                {
                    "sent": "Well, you know the certainly these documents should be related, right?",
                    "label": 0
                },
                {
                    "sent": "So if I'm interested, if I have an English document and I want to ask for a specific language or maybe just a list of all documents in other languages, kind of the links that exist in Wikipedia should certainly give the right information.",
                    "label": 0
                },
                {
                    "sent": "So the way we're going to cast this problem is a little bit different, though.",
                    "label": 0
                },
                {
                    "sent": "We're going to use the links that we know, that is, you know, in Wikipedia itself, there are links between the different languages, so we're going to use that information to try to extend it to places where places where these links may not exist.",
                    "label": 0
                },
                {
                    "sent": "So essentially really find.",
                    "label": 0
                },
                {
                    "sent": "All reasonably similar documents across all languages.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we get to the really multilingual part and the hub languages and all the other complications, I just wanted to go through an kind of say what the general setting is.",
                    "label": 0
                },
                {
                    "sent": "Let's say we just have two languages, so we're going to use a very simple representation bag of words, which essentially just means that all our languages so you know if we have a language, XML, language, Y, each language is going to be represented by just a vector space, so just a bunch of vectors.",
                    "label": 0
                },
                {
                    "sent": "Where the individual dimensions are the words that occur first.",
                    "label": 0
                },
                {
                    "sent": "The first thing we're going to do is we're going to find the basis for each language independently.",
                    "label": 0
                },
                {
                    "sent": "So in principle we don't really know.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't really know what's 1 document and in one language looks like as document in another language because we're not really going to use machine translation or anything heavy like that.",
                    "label": 0
                },
                {
                    "sent": "But we're just going to find some reasonable representation of these languages independently.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then what we're going to want to do is, well, we're going to assume that these representations are actually accurate and represent the documents in the topics well.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to somehow map them together into one space.",
                    "label": 0
                },
                {
                    "sent": "The problem is if we were just working in one language, well, you know, I give you another query document and it's fine.",
                    "label": 0
                },
                {
                    "sent": "I can just map it into this vector space.",
                    "label": 1
                },
                {
                    "sent": "The words that occur in each language are the same.",
                    "label": 0
                },
                {
                    "sent": "It's fine, everything works, but hear the words that happen in one language obviously don't happen in another language.",
                    "label": 0
                },
                {
                    "sent": "So what we need is some way to make these things come parable.",
                    "label": 0
                },
                {
                    "sent": "And since we already dealing with vector spaces, the natural thing to do is to just say, well, OK, I have my basis in XI, have my basis in Y.",
                    "label": 0
                },
                {
                    "sent": "Let's align these bases somehow, and then essentially we get some new space here where we can actually compare these different things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is something we've called cross lingual similarity and essentially the whole point is that through this this alignment we actually reduce the multilingual or bilingual problem into just the one language case and we can use the same methods that we have been developed before.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just one word about kind of representations.",
                    "label": 0
                },
                {
                    "sent": "Well, we have a lot of different languages, and you know these are very high dimensional sparse spaces.",
                    "label": 0
                },
                {
                    "sent": "So one of the first things we do is we always compute a low rank approximation.",
                    "label": 1
                },
                {
                    "sent": "This is because alignment in general is a very difficult problem, and we're going to be learning from relatively small alignments, so we want low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "So the way we do this is quite a classical technique.",
                    "label": 0
                },
                {
                    "sent": "It's the single value decomposition, and in principle what it really gives us is this topic space, because the dimensions the directions in these high dimensional spaces that are going to be most.",
                    "label": 0
                },
                {
                    "sent": "Presented are going to be exactly the topics that are most prevalent and.",
                    "label": 0
                },
                {
                    "sent": "The hope is, and actually what ends up happening is that what we do is we are taking this large space of.",
                    "label": 0
                },
                {
                    "sent": "You know of very language specific documents and mapping it into some independent topic space.",
                    "label": 1
                },
                {
                    "sent": "So it just.",
                    "label": 0
                },
                {
                    "sent": "I mean, you're just splitting each vector into these dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's just a direct sum that you're doing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have our representation.",
                    "label": 0
                },
                {
                    "sent": "So now the second problem is of course, how do we actually map these things together so and align them so that we can actually make a comparison?",
                    "label": 0
                },
                {
                    "sent": "Well, there's many different ways to do it.",
                    "label": 0
                },
                {
                    "sent": "We did experiments on three different approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one is called Cross Lingual, LSI, which is essentially just in some sense the most agnostic way to do things where we just group things together and hope that decomposition takes care of things.",
                    "label": 0
                },
                {
                    "sent": "The next is least squares, where essentially we're looking for a map W from the SpaceX to Y.",
                    "label": 0
                },
                {
                    "sent": "So essentially we're mapping any document in X into the space of Y and comparing it there.",
                    "label": 0
                },
                {
                    "sent": "So when we're learning W, we obviously need some optimization, some cost function, and here we just use the Frobenius norm and then the last method is regression Canonical analysis.",
                    "label": 0
                },
                {
                    "sent": "And here I don't want to really concentrate too much on the math, but the one thing to notice here if we look at just the formulas very briefly, is that here what you're doing is you're taking a document.",
                    "label": 0
                },
                {
                    "sent": "In X and you're mapping it first through a pseudo inverse of this basis that you found, whereas here the order is different, right?",
                    "label": 0
                },
                {
                    "sent": "So here first you apply something from the basis in Y and it's not really an inverse, but it's the transpose.",
                    "label": 0
                },
                {
                    "sent": "These things are very deeply related, but we're not exactly sure the exact relationships between all the inverses and transpose is an what these actually represent, and I'll get back to that in the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's just kind of the two language case.",
                    "label": 0
                },
                {
                    "sent": "So what do we have now?",
                    "label": 0
                },
                {
                    "sent": "We have our representations, we have our.",
                    "label": 0
                },
                {
                    "sent": "We have our optimization techniques to learn Maps, but now what's the whole problem that we're trying to solve?",
                    "label": 0
                },
                {
                    "sent": "Well, in Wikipedia, we all know that the distribution of languages isn't Even so if we have a large intersection, obviously we have more correspondences.",
                    "label": 1
                },
                {
                    "sent": "Things are easier to solve.",
                    "label": 0
                },
                {
                    "sent": "It's easier to optimize things that are.",
                    "label": 0
                },
                {
                    "sent": "That are more aligned so English in German have 4 million and 1 million articles.",
                    "label": 0
                },
                {
                    "sent": "I'm sure we could find a good intersection, but what about Selena in Hindi which only have 100,000 language, 100,000 articles each?",
                    "label": 0
                },
                {
                    "sent": "Well, So what I'm going to say is that, well, let's say I'm going to say that these big languages, English and German, are going to be hub languages.",
                    "label": 0
                },
                {
                    "sent": "And what that means is that.",
                    "label": 0
                },
                {
                    "sent": "If I take Slovene well, most of these articles are also going to exist in English.",
                    "label": 0
                },
                {
                    "sent": "Likewise, Hindi is also going to have most of this common articles in with English.",
                    "label": 1
                },
                {
                    "sent": "But the common articles between Slovene and Hindi are not that big, so the goal is now, well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take advantage of the fact that this intersection is big, and this intersection is big and make up for the fact that this intersection is not particularly big.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is kind of the general idea.",
                    "label": 0
                },
                {
                    "sent": "Well, you know I'm going to assume that.",
                    "label": 0
                },
                {
                    "sent": "I can learn this very well.",
                    "label": 0
                },
                {
                    "sent": "I can learn this very well, but you know this is quite sparse and quite small, so I'm going to go through English to actually learn this, and that's where the hub languages part of it comes in.",
                    "label": 0
                },
                {
                    "sent": "So one of the first things we could think about doing is we can forget all about the representation stuff and say, well, you know.",
                    "label": 0
                },
                {
                    "sent": "If I have a link here and I have a link here, I can just compose the two and I get a correspondence there, but.",
                    "label": 0
                },
                {
                    "sent": "Part of the problem with that is this is a very common tutorial representation, making it quite hard to generalize, so that's actually why we go to the vector space model where we actually don't use the combinatorial information to actually learn Maps between the representations, and these actually extend across the language space is much better.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so bit about the experiments as the example shows, I.",
                    "label": 0
                },
                {
                    "sent": "We used Salinan Hindi with English as a hub.",
                    "label": 1
                },
                {
                    "sent": "There's been.",
                    "label": 0
                },
                {
                    "sent": "A lot of quite a bit of preprocessing, but the most important one is that stub documents are removed because they don't really contain any information and to give you an idea about the numbers, well, Celine and English has about 50,000 correspondences, Hindi and English has 15,000, but Celine and Hindi only have 4000.",
                    "label": 0
                },
                {
                    "sent": "And of those almost all of them exist also in English.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in principle this is quite a simple idea, but then the devil is always in the details, so there's actually when we're thinking about document retrieval.",
                    "label": 0
                },
                {
                    "sent": "There's five different ways to do this.",
                    "label": 1
                },
                {
                    "sent": "Well, we can map Slovenian directly to Hindi.",
                    "label": 0
                },
                {
                    "sent": "We can map Hindi directly to Slovenian.",
                    "label": 0
                },
                {
                    "sent": "We can map Slovenian to English and Hindi to English, so that we do the comparison in our Hub language.",
                    "label": 0
                },
                {
                    "sent": "Or we could go through English from Slovenian into English and Hindi or likewise.",
                    "label": 0
                },
                {
                    "sent": "The other way around, and the key difference here is what language or what space we're actually doing.",
                    "label": 0
                },
                {
                    "sent": "Our document retrieval in.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, this becomes actually an important and important part of this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We used actually three different subsets, so either we used all or most of the alignment information.",
                    "label": 1
                },
                {
                    "sent": "We use only alignment information that's consistent through the Hub language.",
                    "label": 1
                },
                {
                    "sent": "So in this case we would only use things that exist in English Hindi anseline.",
                    "label": 0
                },
                {
                    "sent": "Or we remove all of those common things and only use the information that's in the hubs and in some sense this is really the best test to understand what's going on, because here there's already.",
                    "label": 0
                },
                {
                    "sent": "We've already kind of halfway reduced it to a bilingual setting.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some results.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to highlight some of the important things.",
                    "label": 0
                },
                {
                    "sent": "So you know we have our different ways of doing these things, and the interesting thing is that a lot of the times, well, you know if we do things directly and we have all the first of all, if we use all the information, it's the results are generally the best.",
                    "label": 0
                },
                {
                    "sent": "Common information tends to be very compareable, the empty ones, that is where we don't actually have where we go exclusively through the hub language.",
                    "label": 0
                },
                {
                    "sent": "The results are much, much worse.",
                    "label": 0
                },
                {
                    "sent": "That's somehow unsurprising.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this kind of very agnostic method.",
                    "label": 0
                },
                {
                    "sent": "This baseline is also tends to work quite reasonably well.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I'll point out in the next slide though, is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at this baseline a little more carefully is, well, OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, the common anol is always going to be the best, because we've thrown the most information in common is reasonable.",
                    "label": 0
                },
                {
                    "sent": "But if we look at empty, well, you know it's quite bad.",
                    "label": 0
                },
                {
                    "sent": "This this kind of agnostic decomposition doesn't work very well if we don't already have some relatively large.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overlap whereas yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see, we have.",
                    "label": 0
                },
                {
                    "sent": "Here for the empty.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As if we look for least squares, well, you know it's a little bit better and actually we can even make it a bit better if we go the other way around.",
                    "label": 0
                },
                {
                    "sent": "I don't think the numbers themselves are that particularly interesting here, but what is interesting is that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These numbers aren't symmetric.",
                    "label": 0
                },
                {
                    "sent": "It really does matter which way you map, and because we're learning linear Maps and there's a lot of different choices and how we can learn these Maps, understanding this asymmetry is kind of what we're currently trying to do, and I think it's very important to kind of optimize.",
                    "label": 0
                },
                {
                    "sent": "To get better performance because obviously.",
                    "label": 0
                },
                {
                    "sent": "In One Direction, we're utilizing different information for the other direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did one more test, which is this kind of empty for this empty representation, and to say, well, you know what does that empty representation means.",
                    "label": 0
                },
                {
                    "sent": "It means that, well, all the alignments I have here map to something here and in the other language and Maps to here and there is no common topical intersection.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we did was, well, let's say we knew the alignment, so we just learned our representations independently in English, and we just took documents that were in one language and saw how well we could do retrieval if we already knew the right correspondence.",
                    "label": 0
                },
                {
                    "sent": "So essentially, this all this is really a test of how well these things are represented in English, and the idea to have here is that, well, if the representations are kind of orthogonal.",
                    "label": 0
                },
                {
                    "sent": "Then anything we map into one space is going to map into zero in the other, and so we've essentially lost all the information.",
                    "label": 0
                },
                {
                    "sent": "The good news is, is that you know the mean retrieval rate was the mean.",
                    "label": 0
                },
                {
                    "sent": "Precision was very good, which tells us essentially that the representations are good, that the information is there and essentially in order to get those numbers higher that I showed before, what we need is we need to better ways to learn the Maps.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's kind of the whole take home message here is that well, we can take advantage of the distribution of languages.",
                    "label": 1
                },
                {
                    "sent": "The use of representations rather than combinatorial information means that we can, actually.",
                    "label": 1
                },
                {
                    "sent": "I mean we can actually work in cases where there's no.",
                    "label": 0
                },
                {
                    "sent": "Direct combinatorial information.",
                    "label": 0
                },
                {
                    "sent": "Because we've spread it out over the entire space through these linear representations and through kind of.",
                    "label": 0
                },
                {
                    "sent": "A select experiment.",
                    "label": 0
                },
                {
                    "sent": "We can also show that the linear representations do have.",
                    "label": 1
                },
                {
                    "sent": "They do still contain the relevant information we need.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, the kind of.",
                    "label": 0
                },
                {
                    "sent": "The kind of atypical things we don't really expect, or that the space where you do the retrieval is important is very important.",
                    "label": 0
                },
                {
                    "sent": "These non symmetry Maps I think.",
                    "label": 0
                },
                {
                    "sent": "Is something that needs to be investigated further and in response to this.",
                    "label": 0
                },
                {
                    "sent": "Also you know right now we're doing we're learning these Maps and kind of a relatively simple way through least squares or some simple optimization technique, But what we'd really like to do is we'd like to formulate The thing is just one big giant optimization problem so that we have some idea how to best utilize all the information, and I think that's it for now, thanks.",
                    "label": 0
                }
            ]
        }
    }
}