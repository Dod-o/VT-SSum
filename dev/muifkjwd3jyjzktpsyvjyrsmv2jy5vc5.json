{
    "id": "muifkjwd3jyjzktpsyvjyrsmv2jy5vc5",
    "title": "Latent Topic Models for Hypertext",
    "info": {
        "author": [
            "Amit Gruber, The Hebrew University of Jerusalem"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/uai08_gruber_ltm/",
    "segmentation": [
        [
            "OK, so I'm going to talk about latent topic models for hypertext documents and the work I'm presenting here is a joint work with Malazan Smith from the IBM Research Labs and with my advisor, you're wise."
        ],
        [
            "Topic models aimed at capturing the semantic of tax documents by using a small number of hidden factors, Andrew just gave us a great introduction for topic models an let's take, for example, this document.",
            "It is the CNN web page and it discusses several topics.",
            "There's a section about sports, there's section about crime.",
            "There's a section about politics and topic models.",
            "Want to find these three different topics, and Moreover they also want to find the proportion between these topics.",
            "So there is been there.",
            "There's been a lot of work and a lot of research about topic models, permanent approaches, the bag of words approach in which the documents are a Representatives Term document Co.",
            "Occurrence matrix.",
            "Each row of these metrics is the histogram of words in one document.",
            "And then usually this metrics is projected into a lower dimensional factor space.",
            "The algebraic methods like latent semantic analysis or a probabilistic methods like the probabilistic latent semantic analysis or deleting the reallocation.",
            "And this.",
            "These methods have been extending many various ways and we just told us both some of them even shows some of these models.",
            "People model the dynamics of topics overtime.",
            "The correlation between topics, sparsity of correlation adjustment mentioned a few extensions.",
            "And so on and so on.",
            "So in this work, we focus on a special kind of documents.",
            "We focus on hypertext documents.",
            "These are documents that have links between."
        ],
        [
            "So here is a bunch of documents.",
            "These are the Wikipedia web web pages and we can see that there are many, many words in them.",
            "We can apply a topic models on them and we can get very nice and interesting results."
        ],
        [
            "But taking a closer look, we see that there is much more than it to it.",
            "We see that there are many, many links.",
            "Actually I drew only some of the links, not to get too cluttered.",
            "And then and we would like actually to take advantage of this link links because they tell us a lot about the contents of the documents."
        ],
        [
            "If we look around, we see that hypertext is just everywhere in the Internet, just keeps growing each and every day in a very base, and I put taxes not only web pages we can find also in scientific publications with citations.",
            "One might think that links all sparse with respect to the words in the documents, and why bother modeling them at all.",
            "But it turns out that links carry a lot of information.",
            "Take for example Google.",
            "They made billions out of links, their Pagerank algorithm for ranking documents is based on the connectivity of the web.",
            "But when we consider working with the web, we must keep in mind that the topology of the web is very complicated and.",
            "And no one actually knows what does it look like.",
            "It's dynamic changing every day and it's.",
            "It's just it's."
        ],
        [
            "Just a jungle.",
            "Now let's take our goals.",
            "Our input is a set of documents and then.",
            "And between these documents we have links, links go out of certain words in the document and they point to other documents.",
            "And we want first to do what our topic models do.",
            "We want to estimate the mixture of topics within each one of these documents.",
            "And we also want to probabilistically associate each word with these topics that used to compute the probability of each word to be drawn for a certain topic.",
            "But in this case we want to do more than this.",
            "We also want to learn important the relative importance of the documents in the corpus.",
            "And when and when I'm saying importance, I mean the probability to link to that document.",
            "An of course or a task is completely."
        ],
        [
            "Unsupervised.",
            "We are not the first one to work on on a topic.",
            "Models for a hypertext documents Kononov man suggested to model links between documents in a very similar fashion to how they how words are usually modeled.",
            "They suggested the user bag of links representation so they have two matrices.",
            "One of them is the term Term document Co occurrence matrix and the other one is the Link Document Co occurrence matrix, and they factorize both metrics is to have one common factor that.",
            "Describes the topics of of the documents, and another another factor, one that relates to links and the other one relates, works.",
            "But with the bag of links or presentation, we no longer retain the Association between link in Word.",
            "We just know how many links are there in a certain document.",
            "There are two recent works.",
            "The first one way deeds that Andrew showed in his presentation and the other one vinyl patian cone and they try to learn the influence of citations and they distinguish between two kinds of documents.",
            "They have documents that point to other ones.",
            "They citing documents, and they also have the cited documents, but a document can be both at the same time, so it's a bipartite graph at the citation graph is bipartite graph, and links may go only in One Direction."
        ],
        [
            "And now we turn to describe our work.",
            "We present another novel topic model called Latent Topic Hypertext model."
        ],
        [
            "And in the latent topic hypertext model, the documents are created in 2 steps.",
            "In the first step we create the text of all the documents we create the words by using latent virtual allocation.",
            "So that's something we are familiar with."
        ],
        [
            "Any of the second step after we have finished creating all the text of all the documents only turn we turn only then will turn into creating the links between documents.",
            "So we go over all the words in the corpus and for each word we decide whether there should be a link out from it and if yes, where should it point."
        ],
        [
            "Notice that by having this 2 steps.",
            "I am generative process.",
            "We allow for arbitrary topology of the graph.",
            "There is no problem with loops.",
            "In the graph there is no document, may even point to itself.",
            "And a link is an is an entity that goes out of word and it points to a document.",
            "It's a symmetric.",
            "And there can be at most one link outgoing from word, and it is very easy to incorporate in this framework.",
            "A links that are shared between multiple words or links that are not associated to a single word, like links that go from from an image.",
            "It's really it's really easy to do it with the same."
        ],
        [
            "Work.",
            "So how does the latent topic hyper model?",
            "How does the latent topic hypertext model journals links?",
            "We just activate over all the words in the corpus and for each word we have to decide we have to decide if there will be a link going out of it or not.",
            "This decision of course depends on the topical match between the word and target document, namely, that is the topic of the word and the topics mixture in the target document.",
            "Again, it's a symmetric.",
            "And it also depends on the relative importance of the target document.",
            "So the way we model it explicitly is we're saying that if we have a word with topics Z, the probability to point to a document D given that topic.",
            "Is the product of two factors, the first of them is the relative importance of the target document D. Without this problem, this parameter by Lambda D. And the other term is the topical match between the word and the target documents.",
            "That is Theta D. The topic mixture in the target document.",
            "Of the topics the the topic of the word."
        ],
        [
            "The graphical description of the model is as follows.",
            "First, in the first step we create the text of the documents.",
            "So here in this diagram I have a single document.",
            "And it's an it is generated by LDA.",
            "It's only a single document.",
            "That's why you don't see the additional plate usually.",
            "See in laytonville allocation.",
            "So I will briefly review the rotations in this slide, 'cause that's that's not our work.",
            "We have the topic mixer Theta D. Then for each one of the words in the document the we have first to draw a topic that's denoted by ZW.",
            "The topic of the word.",
            "And after we have chosen the topic of the word CW, we need to choose a word that corresponds to the topic and this word is drawn from a.",
            "The distribution beta of the relevant topics Z.",
            "And of course, both data and beta averagely hyperparameters alphanate.",
            "These are actually notations very similar to the ones under used.",
            "The only difference is that used fine instead of beta in this diagram."
        ],
        [
            "Now that we know how to create a single document, let's take another one and put it aside.",
            "So I ever just two documents created by LDA."
        ],
        [
            "And now when we want to decide whether to create links or not, let's focus for simplicity on a special case where links are only allowed between document D, prime to document D, we have created both documents already using the LDA.",
            "And now for each one of the words in the document deprime, we need to decide whether there should be a link pointing today or not.",
            "This decision, as we said before, depends on the topical match between different between the word in the prime and the topic mixture in Indy, that's ZW.",
            "The topic of the word, and Thirdly the topic mixture in the document T and it also depends on the importance of the document D, which is the parameter Lambda we see in this slide.",
            "This Lambda is in the general case where we have many documents, is multinomial and it and it also has a hyperparameter gamma.",
            "So in this in this drawing.",
            "The the observation link all their great circles or observation, then a link means a.",
            "But there either can bailing or not.",
            "It's not necessarily means that there is a link, it's just the existence or non existence of that link."
        ],
        [
            "In order to model the creation of links, we just added the parameters.",
            "If this is the size of the corpus, this is a linear number of additional parameters, and that's versus AD times KE parameters.",
            "In previous models, such as the model by content offline or the model of Russia Vitale.",
            "In this model, we not only links give us information but also non links.",
            "When you see that link does not exist, it suggests something about either the lack of importance of the target document or about the mismatch between the topics of word and the target document.",
            "Links are associated with words and they actually share the same topic with that word.",
            "And when we do inference which I'm going to talk about in the next slide, we still at links affect the topic estimation in both the target document and in the source."
        ],
        [
            "So unfortunately exact inference is intractable in this model like in other Oracle models such as LDA.",
            "But when we try to devise an approximate inference algorithm, it's even more challenging in this case because.",
            "While there can be in not that many links, there are many many non links.",
            "There is a quadratic number of non links because.",
            "Whenever you have a link then there D -- 1 non links.",
            "So it's very challenging to devise the even approximate inference algorithm because it's hard to take into account all these observations.",
            "However, it is due to the special form of the probability of link given topic, it's.",
            "To take advantage of symmetries in the model and two and two computer, only aggregations of probability distributions on latent variables to devise an EM algorithm for inference."
        ],
        [
            "Now let's see how good, how good it work.",
            "So we experimented with two datasets.",
            "The first one is the web data set that has been used before.",
            "It's been used in the Conan often work and also in other works, and it's a large data set.",
            "It consists of more than 8000 documents and we extracted about 13,000 links between them.",
            "The other.",
            "The other data set we use is a small data set of Wikipedia pages that we downloaded from the web.",
            "We downloaded them by starting crawling from the NIPS webpage an you might be asking yourself why did we start calling from the NIPS web page?",
            "Not from the UI one.",
            "So the reason is that there is no UI web page and that's all work for you."
        ],
        [
            "So the first, the first picture is, is an eye candy.",
            "Here we see one of the I trained both in all the experiments.",
            "They use 20 even factors.",
            "And here we see one of the topics that we that we got from the Wikipedia data set.",
            "So on the left side there is a a list of words of the most probable words to come from that topic.",
            "That's something very standard.",
            "Many many topic models will show you very nice and very nice topics with clear interpretation.",
            "There is nothing new about it.",
            "On the right side of this diagram you see the documents that are more probable that are most probable to be linked to from that topic, and we see that they actually match the words the topic is about neural networks and the most probable documents to blink from are the artificial neural network document and the newer."
        ],
        [
            "Network documents.",
            "This is another example.",
            "It's about speech recognition pattern recognition."
        ],
        [
            "The third one was a rather surprising one.",
            "It was still names of cities Vancouver in Denver and we see we see a geographic words.",
            "So the reason that we have these documents and we have these words are because this city is hosted in NIPS conference."
        ],
        [
            "And the 4th, the 4th topic is about cognitive science."
        ],
        [
            "And actually all 20 topics look very nice.",
            "A the Wikipedia page of the Journal of Machine Learning Research is a very short webpage, and if we try to do topic analysis based only on the words in that document, then it's quite difficult.",
            "When I, when I applied LDA then the most probable topic in that document.",
            "Actually related to the Wikipedia stuff to the to the to the stuff on the bottles of this page, like search article invocation.",
            "It's not related to the JLR, whereas when we use the link information by the Lt HMM, we got a topics that better describe the actual contents of the documents we got learning machine engineering and look not only on the words look only on their proportions.",
            "In the."
        ],
        [
            "Comment.",
            "And let's take a look at quantitative results.",
            "So we tested our algorithm on the task of.",
            "Link recommendation and this graph shows you recall the percentage of true links.",
            "If we suggest to link to any.",
            "To end documents.",
            "So we have photographs here.",
            "The blue one is the Lt hmm and we want to be as high as possible.",
            "That's the recall.",
            "The black graph is a sanity check where we compared ourselves to a method based on the in degree of document.",
            "I mean we just forget about the topic and we suggest always to link to the most probably the most frequently linked document.",
            "And we see that we actually gain from the topic analysis.",
            "And the sign in right graphs are the algorithms of cotton, often aneiro Shiva.",
            "It was very disappointing and very surprising to see that they actually worked worse than the."
        ],
        [
            "Degree based method and we decided to take a closer look on that so when we checked what happened with trade set, we saw that in the trend set both algorithms work much better.",
            "They work much better than the industry based method.",
            "And and this suggests that they do overfitting due to the large number of parameters they have to model links."
        ],
        [
            "And we have the same experiments on the web data set.",
            "A larger data set."
        ],
        [
            "And now let us conclude.",
            "So the latent topic hypertext model is is a model that explicitly models generational Flinx, and it's where they associate them towards.",
            "In an LDA like model, although approximate inference is challenging, it is still possible to do that in an efficient time.",
            "And we see that it performs much better in previous topic models.",
            "In the task of Link recommendation.",
            "So if you want to try it out, there is code and data available on the web and you're welcome to download and use it.",
            "Thank you.",
            "Yes.",
            "We haven't really tested the word prediction, so I can't give you an answer on that.",
            "Yeah, could use a little something about the how you managed to make the inference, yes, so I'll go back to the inference slide."
        ],
        [
            "So the main problem here, you'll see only two documents, but if you have many of them then the observation link is we have it quadratic a number of times, and when you write we drive the name algorithm.",
            "When you do them then it is straightforward to get the hamster formulas and the main difference between M for this model and the light and reallocation is that when you estimate the topic, make sure you have to take into account.",
            "Both the times that links have been created and have not been created, so for that we have auxiliary variables, discrete auxiliary variables.",
            "So we first draw a latent topic from the top mixture, Theta in the target document and we test if it is equal to the topic of the word or not.",
            "This is quadratic number of latent variables and you don't want to explicitly to compute the distribution over all these variables.",
            "But if you write carefully.",
            "The aggregations that you need for the M step.",
            "Then it turns out that you can just have some actions that factorized to terms that depend only on the.",
            "Topic of the words in the corpus.",
            "You can aggregate over.",
            "The main problem is when you don't have links, but when you don't have a link then you can just aggregate the topics the topic distribution and of all the words in the corpus.",
            "And then you have a different that relates to the topic in the target documents and you can somehow summon multiply these documents and at these terms and you can actually do that in time linear in the size of corpus times the number of topics.",
            "Expected counts exactly exactly.",
            "The documents that actually are linked is that right?",
            "So we have expected counts for both their links that that exist in links don't exist because you have to take into account both of them.",
            "But we have only aggregation of them.",
            "You don't never compute explicit representation for these auxiliary variables, it's only aggregations of them.",
            "Another question.",
            "Sequences of words.",
            "I don't think you really link words in those cases.",
            "OK, so so if I understand the question correctly then it's quite easy to do that because.",
            "In in a previous work fast, so we showed how to out to force consecutive words to have the same topic.",
            "So you can just force consecutive words to have exactly the same topic an view them all as the source of their link.",
            "So the way to do that is to look at the topics of the of the document as hidden Markov chain and then you can just tweak the transition matrix between the topics an.",
            "And for them to be identical."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about latent topic models for hypertext documents and the work I'm presenting here is a joint work with Malazan Smith from the IBM Research Labs and with my advisor, you're wise.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic models aimed at capturing the semantic of tax documents by using a small number of hidden factors, Andrew just gave us a great introduction for topic models an let's take, for example, this document.",
                    "label": 0
                },
                {
                    "sent": "It is the CNN web page and it discusses several topics.",
                    "label": 0
                },
                {
                    "sent": "There's a section about sports, there's section about crime.",
                    "label": 0
                },
                {
                    "sent": "There's a section about politics and topic models.",
                    "label": 0
                },
                {
                    "sent": "Want to find these three different topics, and Moreover they also want to find the proportion between these topics.",
                    "label": 0
                },
                {
                    "sent": "So there is been there.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work and a lot of research about topic models, permanent approaches, the bag of words approach in which the documents are a Representatives Term document Co.",
                    "label": 0
                },
                {
                    "sent": "Occurrence matrix.",
                    "label": 0
                },
                {
                    "sent": "Each row of these metrics is the histogram of words in one document.",
                    "label": 0
                },
                {
                    "sent": "And then usually this metrics is projected into a lower dimensional factor space.",
                    "label": 0
                },
                {
                    "sent": "The algebraic methods like latent semantic analysis or a probabilistic methods like the probabilistic latent semantic analysis or deleting the reallocation.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "These methods have been extending many various ways and we just told us both some of them even shows some of these models.",
                    "label": 0
                },
                {
                    "sent": "People model the dynamics of topics overtime.",
                    "label": 0
                },
                {
                    "sent": "The correlation between topics, sparsity of correlation adjustment mentioned a few extensions.",
                    "label": 0
                },
                {
                    "sent": "And so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So in this work, we focus on a special kind of documents.",
                    "label": 1
                },
                {
                    "sent": "We focus on hypertext documents.",
                    "label": 1
                },
                {
                    "sent": "These are documents that have links between.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a bunch of documents.",
                    "label": 0
                },
                {
                    "sent": "These are the Wikipedia web web pages and we can see that there are many, many words in them.",
                    "label": 0
                },
                {
                    "sent": "We can apply a topic models on them and we can get very nice and interesting results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But taking a closer look, we see that there is much more than it to it.",
                    "label": 0
                },
                {
                    "sent": "We see that there are many, many links.",
                    "label": 0
                },
                {
                    "sent": "Actually I drew only some of the links, not to get too cluttered.",
                    "label": 0
                },
                {
                    "sent": "And then and we would like actually to take advantage of this link links because they tell us a lot about the contents of the documents.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we look around, we see that hypertext is just everywhere in the Internet, just keeps growing each and every day in a very base, and I put taxes not only web pages we can find also in scientific publications with citations.",
                    "label": 1
                },
                {
                    "sent": "One might think that links all sparse with respect to the words in the documents, and why bother modeling them at all.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that links carry a lot of information.",
                    "label": 0
                },
                {
                    "sent": "Take for example Google.",
                    "label": 0
                },
                {
                    "sent": "They made billions out of links, their Pagerank algorithm for ranking documents is based on the connectivity of the web.",
                    "label": 1
                },
                {
                    "sent": "But when we consider working with the web, we must keep in mind that the topology of the web is very complicated and.",
                    "label": 0
                },
                {
                    "sent": "And no one actually knows what does it look like.",
                    "label": 0
                },
                {
                    "sent": "It's dynamic changing every day and it's.",
                    "label": 0
                },
                {
                    "sent": "It's just it's.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a jungle.",
                    "label": 0
                },
                {
                    "sent": "Now let's take our goals.",
                    "label": 0
                },
                {
                    "sent": "Our input is a set of documents and then.",
                    "label": 1
                },
                {
                    "sent": "And between these documents we have links, links go out of certain words in the document and they point to other documents.",
                    "label": 0
                },
                {
                    "sent": "And we want first to do what our topic models do.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate the mixture of topics within each one of these documents.",
                    "label": 0
                },
                {
                    "sent": "And we also want to probabilistically associate each word with these topics that used to compute the probability of each word to be drawn for a certain topic.",
                    "label": 0
                },
                {
                    "sent": "But in this case we want to do more than this.",
                    "label": 0
                },
                {
                    "sent": "We also want to learn important the relative importance of the documents in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And when and when I'm saying importance, I mean the probability to link to that document.",
                    "label": 0
                },
                {
                    "sent": "An of course or a task is completely.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unsupervised.",
                    "label": 0
                },
                {
                    "sent": "We are not the first one to work on on a topic.",
                    "label": 1
                },
                {
                    "sent": "Models for a hypertext documents Kononov man suggested to model links between documents in a very similar fashion to how they how words are usually modeled.",
                    "label": 0
                },
                {
                    "sent": "They suggested the user bag of links representation so they have two matrices.",
                    "label": 0
                },
                {
                    "sent": "One of them is the term Term document Co occurrence matrix and the other one is the Link Document Co occurrence matrix, and they factorize both metrics is to have one common factor that.",
                    "label": 0
                },
                {
                    "sent": "Describes the topics of of the documents, and another another factor, one that relates to links and the other one relates, works.",
                    "label": 0
                },
                {
                    "sent": "But with the bag of links or presentation, we no longer retain the Association between link in Word.",
                    "label": 0
                },
                {
                    "sent": "We just know how many links are there in a certain document.",
                    "label": 0
                },
                {
                    "sent": "There are two recent works.",
                    "label": 0
                },
                {
                    "sent": "The first one way deeds that Andrew showed in his presentation and the other one vinyl patian cone and they try to learn the influence of citations and they distinguish between two kinds of documents.",
                    "label": 0
                },
                {
                    "sent": "They have documents that point to other ones.",
                    "label": 0
                },
                {
                    "sent": "They citing documents, and they also have the cited documents, but a document can be both at the same time, so it's a bipartite graph at the citation graph is bipartite graph, and links may go only in One Direction.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we turn to describe our work.",
                    "label": 0
                },
                {
                    "sent": "We present another novel topic model called Latent Topic Hypertext model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the latent topic hypertext model, the documents are created in 2 steps.",
                    "label": 1
                },
                {
                    "sent": "In the first step we create the text of all the documents we create the words by using latent virtual allocation.",
                    "label": 0
                },
                {
                    "sent": "So that's something we are familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any of the second step after we have finished creating all the text of all the documents only turn we turn only then will turn into creating the links between documents.",
                    "label": 0
                },
                {
                    "sent": "So we go over all the words in the corpus and for each word we decide whether there should be a link out from it and if yes, where should it point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notice that by having this 2 steps.",
                    "label": 0
                },
                {
                    "sent": "I am generative process.",
                    "label": 0
                },
                {
                    "sent": "We allow for arbitrary topology of the graph.",
                    "label": 1
                },
                {
                    "sent": "There is no problem with loops.",
                    "label": 0
                },
                {
                    "sent": "In the graph there is no document, may even point to itself.",
                    "label": 1
                },
                {
                    "sent": "And a link is an is an entity that goes out of word and it points to a document.",
                    "label": 0
                },
                {
                    "sent": "It's a symmetric.",
                    "label": 0
                },
                {
                    "sent": "And there can be at most one link outgoing from word, and it is very easy to incorporate in this framework.",
                    "label": 0
                },
                {
                    "sent": "A links that are shared between multiple words or links that are not associated to a single word, like links that go from from an image.",
                    "label": 0
                },
                {
                    "sent": "It's really it's really easy to do it with the same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So how does the latent topic hyper model?",
                    "label": 0
                },
                {
                    "sent": "How does the latent topic hypertext model journals links?",
                    "label": 0
                },
                {
                    "sent": "We just activate over all the words in the corpus and for each word we have to decide we have to decide if there will be a link going out of it or not.",
                    "label": 0
                },
                {
                    "sent": "This decision of course depends on the topical match between the word and target document, namely, that is the topic of the word and the topics mixture in the target document.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a symmetric.",
                    "label": 0
                },
                {
                    "sent": "And it also depends on the relative importance of the target document.",
                    "label": 1
                },
                {
                    "sent": "So the way we model it explicitly is we're saying that if we have a word with topics Z, the probability to point to a document D given that topic.",
                    "label": 0
                },
                {
                    "sent": "Is the product of two factors, the first of them is the relative importance of the target document D. Without this problem, this parameter by Lambda D. And the other term is the topical match between the word and the target documents.",
                    "label": 1
                },
                {
                    "sent": "That is Theta D. The topic mixture in the target document.",
                    "label": 1
                },
                {
                    "sent": "Of the topics the the topic of the word.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The graphical description of the model is as follows.",
                    "label": 0
                },
                {
                    "sent": "First, in the first step we create the text of the documents.",
                    "label": 0
                },
                {
                    "sent": "So here in this diagram I have a single document.",
                    "label": 0
                },
                {
                    "sent": "And it's an it is generated by LDA.",
                    "label": 1
                },
                {
                    "sent": "It's only a single document.",
                    "label": 1
                },
                {
                    "sent": "That's why you don't see the additional plate usually.",
                    "label": 0
                },
                {
                    "sent": "See in laytonville allocation.",
                    "label": 0
                },
                {
                    "sent": "So I will briefly review the rotations in this slide, 'cause that's that's not our work.",
                    "label": 0
                },
                {
                    "sent": "We have the topic mixer Theta D. Then for each one of the words in the document the we have first to draw a topic that's denoted by ZW.",
                    "label": 0
                },
                {
                    "sent": "The topic of the word.",
                    "label": 0
                },
                {
                    "sent": "And after we have chosen the topic of the word CW, we need to choose a word that corresponds to the topic and this word is drawn from a.",
                    "label": 0
                },
                {
                    "sent": "The distribution beta of the relevant topics Z.",
                    "label": 0
                },
                {
                    "sent": "And of course, both data and beta averagely hyperparameters alphanate.",
                    "label": 0
                },
                {
                    "sent": "These are actually notations very similar to the ones under used.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that used fine instead of beta in this diagram.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we know how to create a single document, let's take another one and put it aside.",
                    "label": 0
                },
                {
                    "sent": "So I ever just two documents created by LDA.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now when we want to decide whether to create links or not, let's focus for simplicity on a special case where links are only allowed between document D, prime to document D, we have created both documents already using the LDA.",
                    "label": 0
                },
                {
                    "sent": "And now for each one of the words in the document deprime, we need to decide whether there should be a link pointing today or not.",
                    "label": 0
                },
                {
                    "sent": "This decision, as we said before, depends on the topical match between different between the word in the prime and the topic mixture in Indy, that's ZW.",
                    "label": 0
                },
                {
                    "sent": "The topic of the word, and Thirdly the topic mixture in the document T and it also depends on the importance of the document D, which is the parameter Lambda we see in this slide.",
                    "label": 0
                },
                {
                    "sent": "This Lambda is in the general case where we have many documents, is multinomial and it and it also has a hyperparameter gamma.",
                    "label": 0
                },
                {
                    "sent": "So in this in this drawing.",
                    "label": 0
                },
                {
                    "sent": "The the observation link all their great circles or observation, then a link means a.",
                    "label": 0
                },
                {
                    "sent": "But there either can bailing or not.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily means that there is a link, it's just the existence or non existence of that link.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to model the creation of links, we just added the parameters.",
                    "label": 0
                },
                {
                    "sent": "If this is the size of the corpus, this is a linear number of additional parameters, and that's versus AD times KE parameters.",
                    "label": 0
                },
                {
                    "sent": "In previous models, such as the model by content offline or the model of Russia Vitale.",
                    "label": 1
                },
                {
                    "sent": "In this model, we not only links give us information but also non links.",
                    "label": 0
                },
                {
                    "sent": "When you see that link does not exist, it suggests something about either the lack of importance of the target document or about the mismatch between the topics of word and the target document.",
                    "label": 0
                },
                {
                    "sent": "Links are associated with words and they actually share the same topic with that word.",
                    "label": 1
                },
                {
                    "sent": "And when we do inference which I'm going to talk about in the next slide, we still at links affect the topic estimation in both the target document and in the source.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So unfortunately exact inference is intractable in this model like in other Oracle models such as LDA.",
                    "label": 1
                },
                {
                    "sent": "But when we try to devise an approximate inference algorithm, it's even more challenging in this case because.",
                    "label": 0
                },
                {
                    "sent": "While there can be in not that many links, there are many many non links.",
                    "label": 0
                },
                {
                    "sent": "There is a quadratic number of non links because.",
                    "label": 0
                },
                {
                    "sent": "Whenever you have a link then there D -- 1 non links.",
                    "label": 0
                },
                {
                    "sent": "So it's very challenging to devise the even approximate inference algorithm because it's hard to take into account all these observations.",
                    "label": 0
                },
                {
                    "sent": "However, it is due to the special form of the probability of link given topic, it's.",
                    "label": 0
                },
                {
                    "sent": "To take advantage of symmetries in the model and two and two computer, only aggregations of probability distributions on latent variables to devise an EM algorithm for inference.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see how good, how good it work.",
                    "label": 0
                },
                {
                    "sent": "So we experimented with two datasets.",
                    "label": 0
                },
                {
                    "sent": "The first one is the web data set that has been used before.",
                    "label": 0
                },
                {
                    "sent": "It's been used in the Conan often work and also in other works, and it's a large data set.",
                    "label": 0
                },
                {
                    "sent": "It consists of more than 8000 documents and we extracted about 13,000 links between them.",
                    "label": 0
                },
                {
                    "sent": "The other.",
                    "label": 0
                },
                {
                    "sent": "The other data set we use is a small data set of Wikipedia pages that we downloaded from the web.",
                    "label": 0
                },
                {
                    "sent": "We downloaded them by starting crawling from the NIPS webpage an you might be asking yourself why did we start calling from the NIPS web page?",
                    "label": 1
                },
                {
                    "sent": "Not from the UI one.",
                    "label": 0
                },
                {
                    "sent": "So the reason is that there is no UI web page and that's all work for you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first, the first picture is, is an eye candy.",
                    "label": 0
                },
                {
                    "sent": "Here we see one of the I trained both in all the experiments.",
                    "label": 0
                },
                {
                    "sent": "They use 20 even factors.",
                    "label": 0
                },
                {
                    "sent": "And here we see one of the topics that we that we got from the Wikipedia data set.",
                    "label": 0
                },
                {
                    "sent": "So on the left side there is a a list of words of the most probable words to come from that topic.",
                    "label": 0
                },
                {
                    "sent": "That's something very standard.",
                    "label": 0
                },
                {
                    "sent": "Many many topic models will show you very nice and very nice topics with clear interpretation.",
                    "label": 0
                },
                {
                    "sent": "There is nothing new about it.",
                    "label": 0
                },
                {
                    "sent": "On the right side of this diagram you see the documents that are more probable that are most probable to be linked to from that topic, and we see that they actually match the words the topic is about neural networks and the most probable documents to blink from are the artificial neural network document and the newer.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network documents.",
                    "label": 0
                },
                {
                    "sent": "This is another example.",
                    "label": 0
                },
                {
                    "sent": "It's about speech recognition pattern recognition.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third one was a rather surprising one.",
                    "label": 0
                },
                {
                    "sent": "It was still names of cities Vancouver in Denver and we see we see a geographic words.",
                    "label": 0
                },
                {
                    "sent": "So the reason that we have these documents and we have these words are because this city is hosted in NIPS conference.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the 4th, the 4th topic is about cognitive science.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually all 20 topics look very nice.",
                    "label": 0
                },
                {
                    "sent": "A the Wikipedia page of the Journal of Machine Learning Research is a very short webpage, and if we try to do topic analysis based only on the words in that document, then it's quite difficult.",
                    "label": 1
                },
                {
                    "sent": "When I, when I applied LDA then the most probable topic in that document.",
                    "label": 0
                },
                {
                    "sent": "Actually related to the Wikipedia stuff to the to the to the stuff on the bottles of this page, like search article invocation.",
                    "label": 0
                },
                {
                    "sent": "It's not related to the JLR, whereas when we use the link information by the Lt HMM, we got a topics that better describe the actual contents of the documents we got learning machine engineering and look not only on the words look only on their proportions.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comment.",
                    "label": 0
                },
                {
                    "sent": "And let's take a look at quantitative results.",
                    "label": 0
                },
                {
                    "sent": "So we tested our algorithm on the task of.",
                    "label": 0
                },
                {
                    "sent": "Link recommendation and this graph shows you recall the percentage of true links.",
                    "label": 0
                },
                {
                    "sent": "If we suggest to link to any.",
                    "label": 0
                },
                {
                    "sent": "To end documents.",
                    "label": 0
                },
                {
                    "sent": "So we have photographs here.",
                    "label": 0
                },
                {
                    "sent": "The blue one is the Lt hmm and we want to be as high as possible.",
                    "label": 0
                },
                {
                    "sent": "That's the recall.",
                    "label": 0
                },
                {
                    "sent": "The black graph is a sanity check where we compared ourselves to a method based on the in degree of document.",
                    "label": 0
                },
                {
                    "sent": "I mean we just forget about the topic and we suggest always to link to the most probably the most frequently linked document.",
                    "label": 0
                },
                {
                    "sent": "And we see that we actually gain from the topic analysis.",
                    "label": 0
                },
                {
                    "sent": "And the sign in right graphs are the algorithms of cotton, often aneiro Shiva.",
                    "label": 0
                },
                {
                    "sent": "It was very disappointing and very surprising to see that they actually worked worse than the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Degree based method and we decided to take a closer look on that so when we checked what happened with trade set, we saw that in the trend set both algorithms work much better.",
                    "label": 0
                },
                {
                    "sent": "They work much better than the industry based method.",
                    "label": 0
                },
                {
                    "sent": "And and this suggests that they do overfitting due to the large number of parameters they have to model links.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have the same experiments on the web data set.",
                    "label": 0
                },
                {
                    "sent": "A larger data set.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now let us conclude.",
                    "label": 0
                },
                {
                    "sent": "So the latent topic hypertext model is is a model that explicitly models generational Flinx, and it's where they associate them towards.",
                    "label": 0
                },
                {
                    "sent": "In an LDA like model, although approximate inference is challenging, it is still possible to do that in an efficient time.",
                    "label": 0
                },
                {
                    "sent": "And we see that it performs much better in previous topic models.",
                    "label": 0
                },
                {
                    "sent": "In the task of Link recommendation.",
                    "label": 0
                },
                {
                    "sent": "So if you want to try it out, there is code and data available on the web and you're welcome to download and use it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We haven't really tested the word prediction, so I can't give you an answer on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, could use a little something about the how you managed to make the inference, yes, so I'll go back to the inference slide.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main problem here, you'll see only two documents, but if you have many of them then the observation link is we have it quadratic a number of times, and when you write we drive the name algorithm.",
                    "label": 0
                },
                {
                    "sent": "When you do them then it is straightforward to get the hamster formulas and the main difference between M for this model and the light and reallocation is that when you estimate the topic, make sure you have to take into account.",
                    "label": 0
                },
                {
                    "sent": "Both the times that links have been created and have not been created, so for that we have auxiliary variables, discrete auxiliary variables.",
                    "label": 0
                },
                {
                    "sent": "So we first draw a latent topic from the top mixture, Theta in the target document and we test if it is equal to the topic of the word or not.",
                    "label": 0
                },
                {
                    "sent": "This is quadratic number of latent variables and you don't want to explicitly to compute the distribution over all these variables.",
                    "label": 0
                },
                {
                    "sent": "But if you write carefully.",
                    "label": 0
                },
                {
                    "sent": "The aggregations that you need for the M step.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that you can just have some actions that factorized to terms that depend only on the.",
                    "label": 0
                },
                {
                    "sent": "Topic of the words in the corpus.",
                    "label": 0
                },
                {
                    "sent": "You can aggregate over.",
                    "label": 0
                },
                {
                    "sent": "The main problem is when you don't have links, but when you don't have a link then you can just aggregate the topics the topic distribution and of all the words in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And then you have a different that relates to the topic in the target documents and you can somehow summon multiply these documents and at these terms and you can actually do that in time linear in the size of corpus times the number of topics.",
                    "label": 0
                },
                {
                    "sent": "Expected counts exactly exactly.",
                    "label": 0
                },
                {
                    "sent": "The documents that actually are linked is that right?",
                    "label": 0
                },
                {
                    "sent": "So we have expected counts for both their links that that exist in links don't exist because you have to take into account both of them.",
                    "label": 0
                },
                {
                    "sent": "But we have only aggregation of them.",
                    "label": 0
                },
                {
                    "sent": "You don't never compute explicit representation for these auxiliary variables, it's only aggregations of them.",
                    "label": 0
                },
                {
                    "sent": "Another question.",
                    "label": 0
                },
                {
                    "sent": "Sequences of words.",
                    "label": 0
                },
                {
                    "sent": "I don't think you really link words in those cases.",
                    "label": 0
                },
                {
                    "sent": "OK, so so if I understand the question correctly then it's quite easy to do that because.",
                    "label": 0
                },
                {
                    "sent": "In in a previous work fast, so we showed how to out to force consecutive words to have the same topic.",
                    "label": 0
                },
                {
                    "sent": "So you can just force consecutive words to have exactly the same topic an view them all as the source of their link.",
                    "label": 0
                },
                {
                    "sent": "So the way to do that is to look at the topics of the of the document as hidden Markov chain and then you can just tweak the transition matrix between the topics an.",
                    "label": 0
                },
                {
                    "sent": "And for them to be identical.",
                    "label": 0
                }
            ]
        }
    }
}