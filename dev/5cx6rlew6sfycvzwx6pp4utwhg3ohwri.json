{
    "id": "5cx6rlew6sfycvzwx6pp4utwhg3ohwri",
    "title": "Learning discriminative space-time actions from weakly labelled videos",
    "info": {
        "author": [
            "Michael Sapienza, Oxford Brookes University"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_sapienza_labelled_videos/",
    "segmentation": [
        [
            "And I'm going to talk about.",
            "Learning discriminative space time actions from weakly labeled videos."
        ],
        [
            "So I will start buying."
        ],
        [
            "Reducing problem definition and the datasets that we've used and then move on to describe the baseline method.",
            "Our in my contributions."
        ],
        [
            "The details of our proposed approach.",
            "Finally, I will talk about the experiments we used to value."
        ],
        [
            "Way to work and conclude."
        ],
        [
            "So in human action classification from video, the task is the following.",
            "You've got some training data which consists of video clips of predefined length.",
            "An known action class where it is assumed that there is 1 action class per video."
        ],
        [
            "The task is to learn a model which assigns a query video clip to the correct action label."
        ],
        [
            "So to evaluate the work we've used for challenging datasets, which I will show in order of increasing difficulty.",
            "The first data set is the KTH data set, which has six action classes and it contains.",
            "Actions staged human actions which are performed against homogeneous backgrounds, which makes this datasets relatively easy."
        ],
        [
            "We also use the YouTube data set which has 11 action classes.",
            "These videos are relatively low resolution and they contain unconstrained camera motions and actions."
        ],
        [
            "So the next data set is the Hollywood Two data set.",
            "It's got 12 action classes and notice the significant variations in illumination by the occlusions pose, making it quite difficult.",
            "Data set and also the video clips are of different length."
        ],
        [
            "And finally, we've used HMDB 51 datasets, which has 51 action classes and then over 6800 videos, and these are taken from online user generated content and also movies making it probably the most challenging data set out."
        ],
        [
            "At the moment.",
            "So let's start by describing the state of the art baseline.",
            "So initially features our spacetime.",
            "Features are extracted from blocks in the video."
        ],
        [
            "And then.",
            "And visual vocabulary is generated by clustering these features with."
        ],
        [
            "Gaming zarger them.",
            "So each feature is quantized to the closest class."
        ],
        [
            "The center and each video is represented by the frequency of occurrence of these visual way."
        ],
        [
            "So an action model is created with an SVM in a one versus own approach."
        ],
        [
            "And finally, a query action clip is classified based on the length model."
        ],
        [
            "So there are some problems with this approach that we would like to tackle."
        ],
        [
            "I'm.",
            "So firstly.",
            "The history, so we represent each video with a single histogram, including the actions and the background, and this includes the relevant information and motion and seeing patterns which appear in multiple action classes.",
            "So for example, in the KTH data set you can see that the background is not discriminative of the action."
        ],
        [
            "And in the YouTube data set.",
            "The actions trampoline, jumping and volleyball spiking.",
            "They both share similar backgrounds.",
            "And they contain.",
            "They both contain jumping actions.",
            "However, there are some differences which you would like."
        ],
        [
            "Model and capture and so this motivates us to localize discriminative action parts."
        ],
        [
            "So Secondly, in the standard bag of features approach and it does not give us look location information and this is useful because usually actions only occur in a subset of the video.",
            "For example, this is a cycling video.",
            "And actions don't usually start and stop at the beginning and end of each video, so this location information is useful and it can also help us get better classification results."
        ],
        [
            "OK, so we need to get better action clip classification and to simultaneously localize discriminative action part."
        ],
        [
            "So we would like to assign each query video clip to the correct action label and at the same time get an idea of where the action is happening in the video without having any annotation data or bounding box information, and this makes the problem weakly labeled.",
            "And the advantage of having weak labels is that they are much easier to find than together than to manually annotate these thousands of videos.",
            "So for example, you can imagine getting information from the caption or title of the videos on YouTube, and so these are so these ideas have had some success in object detection and image classification, and so we would also like to apply them to videos.",
            "So."
        ],
        [
            "Our contributions are as follows.",
            "We represent video clips as bags of bag of feature instances with latent flaws."
        ],
        [
            "Variables we use multiple instance learning into the space time volumes to learn an action model and the latent class variable simultaneous."
        ],
        [
            "And we're also proposing mapping from instance level scores to a final classification score."
        ],
        [
            "So consider an action video sequence.",
            "As you can see on the left it is usually represented by a single histogram.",
            "And notice that the label of the video is known an.",
            "So instead of representing it in that way, we've broken up the video into multiple sub volumes using a sliding window approach.",
            "And now only the label of the bag is known and not of its instances.",
            "And so we would like to find out which subvolumes are particularly discriminative of the action and which are not.",
            "So, so you can see the boxes with the solid line would be considered as positive examples for the classifier and the ones with dotted dashed lines would be costing the negative set."
        ],
        [
            "OK, so we've reviewed the heuristic algorithm by Andrews ET al to recover the latent variables, which represent the unknown class of each instance, and then SVM model represented by AW vector and bias B.",
            "So consider the following example, so initial.",
            "The deposit if bags are shown in blue and initially the instances in the positive bag have an unknown label and are shown in Gray.",
            "The negative instances are shown as red circles and they're remain strictly negative."
        ],
        [
            "So to start this this procedure we will assume that all the instances in the positive bag also have a positive label."
        ],
        [
            "And next we'll compute the SVM solution."
        ],
        [
            "So.",
            "In the next step we will assign a positive label to those instances which fall on the positive side of the hyperplane.",
            "A negative label to those instances which fall on the negative side of the hyperplane.",
            "However, if all the instances inside a positive bag switch to a negative label, then the least negative example is."
        ],
        [
            "Spectral positive label.",
            "So this process is this process is each."
        ],
        [
            "Rated.",
            "Until the labels do not change."
        ],
        [
            "And finally, the labels and the modular output."
        ],
        [
            "So for example.",
            "This is the result we achieve for one video in the KTH data set.",
            "The black dots in the video denote the dense features.",
            "The cubes do noticeable yems, which are expected density using sliding window approach and."
        ],
        [
            "This is the location of the person in the video and these are."
        ],
        [
            "Examples of positive subvolumes.",
            "So here is an example of a negative subvolume.",
            "So note that only the positive sub volumes are being displayed in the image."
        ],
        [
            "And so the result of this process is that."
        ],
        [
            "And only subvolumes around the person were selected as positive instances to be.",
            "So we will learn an action model only from those instances."
        ],
        [
            "So finally, at this time the final task is to.",
            "Give a final classification result so we would like to.",
            "Give a particular label through a whole video.",
            "In this case, it's equivalent to a bag and so know that videos vary in length considerably, and therefore we will have a variable number of instances per bag, so one."
        ],
        [
            "To do this is to take the sign of the maximum instance in Japan: some threshold on the quantities of the scores in each bag.",
            "However."
        ],
        [
            "And we learned a mapping from the training data instead of two 2 using one of those dishes decision scores.",
            "So we considered 6 features of the instance scores for."
        ],
        [
            "The number of positive examples.",
            "Number of negatives than the maximum score, the minimum score and the mean score, and."
        ],
        [
            "We learned an Austrian model."
        ],
        [
            "In this conference."
        ],
        [
            "Dimensional space."
        ],
        [
            "So in the experiments, like I said, we've used for challenging datasets which I've shown previously, and we downsample them through a common."
        ],
        [
            "Solution.",
            "An in the baseline approach we use the same parameters as that of langata."
        ],
        [
            "And for our approach, we use the same codebook as that of the baseline.",
            "We extracted subvolumes on the regular grid with the spacing of 20 pixels.",
            "An now since the learning problem is much much larger, so we've got approximately 200,000 instances on the KTH datasets which each have the same dimensionality as the histograms in the baseline, we use a linear approximation to the Chinese with Colonel proposed by vitality and cinnamon and.",
            "As detection windows we've we've used fixed size subvolumes of cube cuboid and cuboid which extends to the entire duration of."
        ],
        [
            "Video.",
            "So let's have a look at some quantitative results on the KTH data set, or three subvolumes surpassed the baseline method."
        ],
        [
            "On the YouTube data set also surpassed the baseline considerably."
        ],
        [
            "However, on the Hollywood data set, we get better performance on the.",
            "Mean accuracy and F1 school and less."
        ],
        [
            "The average precision.",
            "Finally on the HMD data set we only get better results on the F1 score, so in addition to the classification performance."
        ],
        [
            "Then we get localization information, so this is an example from the Hollywood today to fit.",
            "This is an action clip which was classified correctly as a drive car action sequence.",
            "And inside the video volume you can see the detected subvolumes.",
            "So notice so this is the video played out, and notice that the subvolumes are are cubes in space and time.",
            "And finally, in the last frames on the steering wheel is visible.",
            "We have the maximum confidence of the action."
        ],
        [
            "So this is another example of from the Hollywood 2 datasets.",
            "This time the action is get out of car.",
            "And similarly we also have localization information, so you can see that faster woman gets out of the car.",
            "And then a man gets out of the car and they walk off."
        ],
        [
            "OK, so in conclusion, our approach captures salient action patches at a higher computational cost."
        ],
        [
            "Reactive compatible and better classification performance as compared to the bag of features baseline."
        ],
        [
            "And in addition to classification, will also have localization information."
        ],
        [
            "So I'm in ongoing work, can we currently, and extending it by using general mixture models of subvolumes tailored for each action and we would also like to incorporate structure into the background features approach like for example by using pictorial style models."
        ],
        [
            "Thank you very much.",
            "Honey."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Learning discriminative space time actions from weakly labeled videos.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will start buying.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reducing problem definition and the datasets that we've used and then move on to describe the baseline method.",
                    "label": 0
                },
                {
                    "sent": "Our in my contributions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The details of our proposed approach.",
                    "label": 0
                },
                {
                    "sent": "Finally, I will talk about the experiments we used to value.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way to work and conclude.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in human action classification from video, the task is the following.",
                    "label": 1
                },
                {
                    "sent": "You've got some training data which consists of video clips of predefined length.",
                    "label": 0
                },
                {
                    "sent": "An known action class where it is assumed that there is 1 action class per video.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The task is to learn a model which assigns a query video clip to the correct action label.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to evaluate the work we've used for challenging datasets, which I will show in order of increasing difficulty.",
                    "label": 1
                },
                {
                    "sent": "The first data set is the KTH data set, which has six action classes and it contains.",
                    "label": 1
                },
                {
                    "sent": "Actions staged human actions which are performed against homogeneous backgrounds, which makes this datasets relatively easy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also use the YouTube data set which has 11 action classes.",
                    "label": 0
                },
                {
                    "sent": "These videos are relatively low resolution and they contain unconstrained camera motions and actions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next data set is the Hollywood Two data set.",
                    "label": 0
                },
                {
                    "sent": "It's got 12 action classes and notice the significant variations in illumination by the occlusions pose, making it quite difficult.",
                    "label": 1
                },
                {
                    "sent": "Data set and also the video clips are of different length.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we've used HMDB 51 datasets, which has 51 action classes and then over 6800 videos, and these are taken from online user generated content and also movies making it probably the most challenging data set out.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the moment.",
                    "label": 0
                },
                {
                    "sent": "So let's start by describing the state of the art baseline.",
                    "label": 0
                },
                {
                    "sent": "So initially features our spacetime.",
                    "label": 0
                },
                {
                    "sent": "Features are extracted from blocks in the video.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And visual vocabulary is generated by clustering these features with.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaming zarger them.",
                    "label": 0
                },
                {
                    "sent": "So each feature is quantized to the closest class.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The center and each video is represented by the frequency of occurrence of these visual way.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an action model is created with an SVM in a one versus own approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, a query action clip is classified based on the length model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some problems with this approach that we would like to tackle.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So firstly.",
                    "label": 0
                },
                {
                    "sent": "The history, so we represent each video with a single histogram, including the actions and the background, and this includes the relevant information and motion and seeing patterns which appear in multiple action classes.",
                    "label": 1
                },
                {
                    "sent": "So for example, in the KTH data set you can see that the background is not discriminative of the action.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the YouTube data set.",
                    "label": 0
                },
                {
                    "sent": "The actions trampoline, jumping and volleyball spiking.",
                    "label": 0
                },
                {
                    "sent": "They both share similar backgrounds.",
                    "label": 0
                },
                {
                    "sent": "And they contain.",
                    "label": 0
                },
                {
                    "sent": "They both contain jumping actions.",
                    "label": 0
                },
                {
                    "sent": "However, there are some differences which you would like.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model and capture and so this motivates us to localize discriminative action parts.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Secondly, in the standard bag of features approach and it does not give us look location information and this is useful because usually actions only occur in a subset of the video.",
                    "label": 1
                },
                {
                    "sent": "For example, this is a cycling video.",
                    "label": 0
                },
                {
                    "sent": "And actions don't usually start and stop at the beginning and end of each video, so this location information is useful and it can also help us get better classification results.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we need to get better action clip classification and to simultaneously localize discriminative action part.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we would like to assign each query video clip to the correct action label and at the same time get an idea of where the action is happening in the video without having any annotation data or bounding box information, and this makes the problem weakly labeled.",
                    "label": 0
                },
                {
                    "sent": "And the advantage of having weak labels is that they are much easier to find than together than to manually annotate these thousands of videos.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can imagine getting information from the caption or title of the videos on YouTube, and so these are so these ideas have had some success in object detection and image classification, and so we would also like to apply them to videos.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our contributions are as follows.",
                    "label": 0
                },
                {
                    "sent": "We represent video clips as bags of bag of feature instances with latent flaws.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variables we use multiple instance learning into the space time volumes to learn an action model and the latent class variable simultaneous.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're also proposing mapping from instance level scores to a final classification score.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So consider an action video sequence.",
                    "label": 1
                },
                {
                    "sent": "As you can see on the left it is usually represented by a single histogram.",
                    "label": 0
                },
                {
                    "sent": "And notice that the label of the video is known an.",
                    "label": 0
                },
                {
                    "sent": "So instead of representing it in that way, we've broken up the video into multiple sub volumes using a sliding window approach.",
                    "label": 0
                },
                {
                    "sent": "And now only the label of the bag is known and not of its instances.",
                    "label": 0
                },
                {
                    "sent": "And so we would like to find out which subvolumes are particularly discriminative of the action and which are not.",
                    "label": 0
                },
                {
                    "sent": "So, so you can see the boxes with the solid line would be considered as positive examples for the classifier and the ones with dotted dashed lines would be costing the negative set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've reviewed the heuristic algorithm by Andrews ET al to recover the latent variables, which represent the unknown class of each instance, and then SVM model represented by AW vector and bias B.",
                    "label": 1
                },
                {
                    "sent": "So consider the following example, so initial.",
                    "label": 0
                },
                {
                    "sent": "The deposit if bags are shown in blue and initially the instances in the positive bag have an unknown label and are shown in Gray.",
                    "label": 0
                },
                {
                    "sent": "The negative instances are shown as red circles and they're remain strictly negative.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to start this this procedure we will assume that all the instances in the positive bag also have a positive label.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And next we'll compute the SVM solution.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the next step we will assign a positive label to those instances which fall on the positive side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "A negative label to those instances which fall on the negative side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "However, if all the instances inside a positive bag switch to a negative label, then the least negative example is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spectral positive label.",
                    "label": 0
                },
                {
                    "sent": "So this process is this process is each.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rated.",
                    "label": 0
                },
                {
                    "sent": "Until the labels do not change.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, the labels and the modular output.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "This is the result we achieve for one video in the KTH data set.",
                    "label": 0
                },
                {
                    "sent": "The black dots in the video denote the dense features.",
                    "label": 1
                },
                {
                    "sent": "The cubes do noticeable yems, which are expected density using sliding window approach and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the location of the person in the video and these are.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples of positive subvolumes.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of a negative subvolume.",
                    "label": 0
                },
                {
                    "sent": "So note that only the positive sub volumes are being displayed in the image.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the result of this process is that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And only subvolumes around the person were selected as positive instances to be.",
                    "label": 0
                },
                {
                    "sent": "So we will learn an action model only from those instances.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally, at this time the final task is to.",
                    "label": 0
                },
                {
                    "sent": "Give a final classification result so we would like to.",
                    "label": 0
                },
                {
                    "sent": "Give a particular label through a whole video.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's equivalent to a bag and so know that videos vary in length considerably, and therefore we will have a variable number of instances per bag, so one.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this is to take the sign of the maximum instance in Japan: some threshold on the quantities of the scores in each bag.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we learned a mapping from the training data instead of two 2 using one of those dishes decision scores.",
                    "label": 0
                },
                {
                    "sent": "So we considered 6 features of the instance scores for.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of positive examples.",
                    "label": 0
                },
                {
                    "sent": "Number of negatives than the maximum score, the minimum score and the mean score, and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learned an Austrian model.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this conference.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the experiments, like I said, we've used for challenging datasets which I've shown previously, and we downsample them through a common.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "An in the baseline approach we use the same parameters as that of langata.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for our approach, we use the same codebook as that of the baseline.",
                    "label": 1
                },
                {
                    "sent": "We extracted subvolumes on the regular grid with the spacing of 20 pixels.",
                    "label": 1
                },
                {
                    "sent": "An now since the learning problem is much much larger, so we've got approximately 200,000 instances on the KTH datasets which each have the same dimensionality as the histograms in the baseline, we use a linear approximation to the Chinese with Colonel proposed by vitality and cinnamon and.",
                    "label": 0
                },
                {
                    "sent": "As detection windows we've we've used fixed size subvolumes of cube cuboid and cuboid which extends to the entire duration of.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look at some quantitative results on the KTH data set, or three subvolumes surpassed the baseline method.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the YouTube data set also surpassed the baseline considerably.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, on the Hollywood data set, we get better performance on the.",
                    "label": 0
                },
                {
                    "sent": "Mean accuracy and F1 school and less.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The average precision.",
                    "label": 0
                },
                {
                    "sent": "Finally on the HMD data set we only get better results on the F1 score, so in addition to the classification performance.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we get localization information, so this is an example from the Hollywood today to fit.",
                    "label": 0
                },
                {
                    "sent": "This is an action clip which was classified correctly as a drive car action sequence.",
                    "label": 0
                },
                {
                    "sent": "And inside the video volume you can see the detected subvolumes.",
                    "label": 0
                },
                {
                    "sent": "So notice so this is the video played out, and notice that the subvolumes are are cubes in space and time.",
                    "label": 0
                },
                {
                    "sent": "And finally, in the last frames on the steering wheel is visible.",
                    "label": 0
                },
                {
                    "sent": "We have the maximum confidence of the action.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is another example of from the Hollywood 2 datasets.",
                    "label": 0
                },
                {
                    "sent": "This time the action is get out of car.",
                    "label": 0
                },
                {
                    "sent": "And similarly we also have localization information, so you can see that faster woman gets out of the car.",
                    "label": 0
                },
                {
                    "sent": "And then a man gets out of the car and they walk off.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion, our approach captures salient action patches at a higher computational cost.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reactive compatible and better classification performance as compared to the bag of features baseline.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in addition to classification, will also have localization information.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm in ongoing work, can we currently, and extending it by using general mixture models of subvolumes tailored for each action and we would also like to incorporate structure into the background features approach like for example by using pictorial style models.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Honey.",
                    "label": 0
                }
            ]
        }
    }
}