{
    "id": "i2edzfxyj366u2dhoyorixwvqagfi4ai",
    "title": "Learning from Distributions via Support Measure Machines",
    "info": {
        "author": [
            "Krikamol Muandet, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_muandet_machines/",
    "segmentation": [
        [
            "So in standard kernel bad learning technique, so dataport often map into a high dimensional feature space by means of a positive definite kernel.",
            "So this is shown in the left figure, and Interestingly, this is equivalently the mapping of the direct measure.",
            "Divide each data point to the expectation of the kernel under the distribution as shown in the middle finger.",
            "However, this distribution of you doesn't give us any new information about the observation.",
            "Therefore, we are interested in the generalization of this idea to a more general probability measure as shown in the right figure, so this will shed light on many real world application.",
            "So first of all, the distribution may be used to represent the noisy example.",
            "Secondly, model modeling group of sample rather than individual sample using a distribution.",
            "Allow it to incorporate the higher order statistic.",
            "This also will be useful in domain adaptation and generalization.",
            "Last but not least, one may also use distribution to sort of scale scaled out the data set instead of scaling up the learning algorithm in large scale machine learning and then apply the learning algorithm on a set of smaller group up distribution instead of on the original data set.",
            "So."
        ],
        [
            "To achieve this, we consider the following regularization framework.",
            "So, given the sample pair of distribution and target output, our loss function depends on the expectation of the function in the arcade.",
            "Shifts under is trending distribution.",
            "So we show that in this case the representative in Poe and the solution can be written as a linear combination of the kernel mean embedding of the training distribution.",
            "Of course, our framework is different from the situation when you learn from an infinitely many sample door from distribution.",
            "This is one we want to achieve, but clearly this is intractable.",
            "So another extreme is when we look only at the mean of the distribution and.",
            "Minimize the loss using just the mean of the distribution.",
            "This is also clearly not good because there's a information loss in this case, so in a sense we are doing something.",
            "Slightly different.",
            "But it's not only allow us to preserve necessary information, but also allow an efficient computation.",
            "So based on the proposed framework."
        ],
        [
            "We propose the support, measure machine or SMM.",
            "So first we defy the embedding kernel which map the distribution into the Dark Ages.",
            "So then the linear kernel on distribution is defined as the inner product between two embedding.",
            "So we can also get the standard feature map of the SVM as a special case when we have the direct measure as a training sample and then to allow for a nonlinear learning algorithm.",
            "We defy the second kernel, called the Level 2 kernel.",
            "On top of this embedding.",
            "For example, you can use standard Gaussian RBF kernel to divide this, so you can think of it as a sort of having another layer of abstraction."
        ],
        [
            "Last but not least, we also showed that for some distribution, the support measure machine reduced to a more flexible version of SVM.",
            "So in this case as opposed to the standard search we have shown in the left.",
            "The flexible SVM allows to place different kernel on training sample.",
            "For example, we can use the Gaussian RBF kernel with different bandwidth.",
            "To model to model the data set that has a substantial an heterogeneous uncertainty structure.",
            "For example, in astronomical data, and so there's more to this project.",
            "So if you are interested, please feel free to visit that as poster.",
            "TS 54 Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in standard kernel bad learning technique, so dataport often map into a high dimensional feature space by means of a positive definite kernel.",
                    "label": 0
                },
                {
                    "sent": "So this is shown in the left figure, and Interestingly, this is equivalently the mapping of the direct measure.",
                    "label": 0
                },
                {
                    "sent": "Divide each data point to the expectation of the kernel under the distribution as shown in the middle finger.",
                    "label": 0
                },
                {
                    "sent": "However, this distribution of you doesn't give us any new information about the observation.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we are interested in the generalization of this idea to a more general probability measure as shown in the right figure, so this will shed light on many real world application.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the distribution may be used to represent the noisy example.",
                    "label": 0
                },
                {
                    "sent": "Secondly, model modeling group of sample rather than individual sample using a distribution.",
                    "label": 0
                },
                {
                    "sent": "Allow it to incorporate the higher order statistic.",
                    "label": 0
                },
                {
                    "sent": "This also will be useful in domain adaptation and generalization.",
                    "label": 0
                },
                {
                    "sent": "Last but not least, one may also use distribution to sort of scale scaled out the data set instead of scaling up the learning algorithm in large scale machine learning and then apply the learning algorithm on a set of smaller group up distribution instead of on the original data set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To achieve this, we consider the following regularization framework.",
                    "label": 0
                },
                {
                    "sent": "So, given the sample pair of distribution and target output, our loss function depends on the expectation of the function in the arcade.",
                    "label": 1
                },
                {
                    "sent": "Shifts under is trending distribution.",
                    "label": 0
                },
                {
                    "sent": "So we show that in this case the representative in Poe and the solution can be written as a linear combination of the kernel mean embedding of the training distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course, our framework is different from the situation when you learn from an infinitely many sample door from distribution.",
                    "label": 1
                },
                {
                    "sent": "This is one we want to achieve, but clearly this is intractable.",
                    "label": 0
                },
                {
                    "sent": "So another extreme is when we look only at the mean of the distribution and.",
                    "label": 0
                },
                {
                    "sent": "Minimize the loss using just the mean of the distribution.",
                    "label": 1
                },
                {
                    "sent": "This is also clearly not good because there's a information loss in this case, so in a sense we are doing something.",
                    "label": 0
                },
                {
                    "sent": "Slightly different.",
                    "label": 0
                },
                {
                    "sent": "But it's not only allow us to preserve necessary information, but also allow an efficient computation.",
                    "label": 0
                },
                {
                    "sent": "So based on the proposed framework.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We propose the support, measure machine or SMM.",
                    "label": 0
                },
                {
                    "sent": "So first we defy the embedding kernel which map the distribution into the Dark Ages.",
                    "label": 0
                },
                {
                    "sent": "So then the linear kernel on distribution is defined as the inner product between two embedding.",
                    "label": 0
                },
                {
                    "sent": "So we can also get the standard feature map of the SVM as a special case when we have the direct measure as a training sample and then to allow for a nonlinear learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "We defy the second kernel, called the Level 2 kernel.",
                    "label": 0
                },
                {
                    "sent": "On top of this embedding.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use standard Gaussian RBF kernel to divide this, so you can think of it as a sort of having another layer of abstraction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Last but not least, we also showed that for some distribution, the support measure machine reduced to a more flexible version of SVM.",
                    "label": 0
                },
                {
                    "sent": "So in this case as opposed to the standard search we have shown in the left.",
                    "label": 0
                },
                {
                    "sent": "The flexible SVM allows to place different kernel on training sample.",
                    "label": 1
                },
                {
                    "sent": "For example, we can use the Gaussian RBF kernel with different bandwidth.",
                    "label": 0
                },
                {
                    "sent": "To model to model the data set that has a substantial an heterogeneous uncertainty structure.",
                    "label": 0
                },
                {
                    "sent": "For example, in astronomical data, and so there's more to this project.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested, please feel free to visit that as poster.",
                    "label": 0
                },
                {
                    "sent": "TS 54 Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}