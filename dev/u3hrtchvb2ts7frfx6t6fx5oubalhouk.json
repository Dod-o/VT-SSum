{
    "id": "u3hrtchvb2ts7frfx6t6fx5oubalhouk",
    "title": "Multiple Kernel Testing for SVM-based System Identification",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_taylor_mkt/",
    "segmentation": [
        [
            "It seems it's strange the noise model, but anyway it seems to be.",
            "You know, this is sort of like a proof of principle, this isn't."
        ],
        [
            "Uh, totally stupid."
        ],
        [
            "Yeah, OK, so that's that's clearly only in real applications we're interested in."
        ],
        [
            "Actor valued at.",
            "Data, so this is an extension of the SVM to a vector valued, so the new SVM to vector valued function.",
            "So it's a fairly straightforward.",
            "Crank the handle.",
            "Basically the loss function is the epsilon insensitive with the P norm.",
            "And you can extend to multiple kernel learning with optimizing this.",
            "As you would expect, these cameras are now vectors.",
            "OK, so just fair."
        ],
        [
            "Straight forward.",
            "Um?",
            "So now the question is how would we apply this to the approximation of nonlinear systems with vector valued?",
            "Paths and so.",
            "The idea here is to actually combine kernels.",
            "With a different kernels that we might consider, but with they might be, combination might vary overtime, so one period some kernel might be more relevant and another period of a different kernel, and so the combination is controlled by these FI Jays which are basically combining a set of Gaussian RBF localities around certain times.",
            "OK, so the idea is you would adapt.",
            "This is just by the way we haven't got results for this is just a proposal.",
            "And so you would need to optimize this in the in the multiple kernel learning, and hopefully this could.",
            "This could be done and fine."
        ],
        [
            "This slide is to show that actually, this very naturally translates into an application of a PAC Bayes bound that we developed for the variational approximation approach since this.",
            "Combo."
        ],
        [
            "Nation here can be viewed as a linear differential time varying linear differential equation.",
            "I mean where the?",
            "The parameters of the differential equation will be controlled by the varying of these Phi JS.",
            "We can actually write that down."
        ],
        [
            "Is this FL?",
            "And so then we can apply the PAC Bayes approach where we know the.",
            "The prize given by the nonlinear stochastic differential equation and ized TI there, the KL divergent's can be computed OK.",
            "It may be a little hard to do but principle it can be done and then we can apply the empirical error, the true error and bound the new ekl between the two in terms of this KL divergences here.",
            "So this applies shows that you know in principle this thing should.",
            "We should be able to bound it's performance and this could be used to drive algorithms to optimize the fit or to optimize the generalization performance as measured by the expected.",
            "So the loss here is just the indicator function.",
            "Whenever an observation is outside the Epsilon star tube around the observations.",
            "Sorry around the path when the observation is outside that share that part OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It seems it's strange the noise model, but anyway it seems to be.",
                    "label": 0
                },
                {
                    "sent": "You know, this is sort of like a proof of principle, this isn't.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, totally stupid.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, so that's that's clearly only in real applications we're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actor valued at.",
                    "label": 0
                },
                {
                    "sent": "Data, so this is an extension of the SVM to a vector valued, so the new SVM to vector valued function.",
                    "label": 0
                },
                {
                    "sent": "So it's a fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "Crank the handle.",
                    "label": 0
                },
                {
                    "sent": "Basically the loss function is the epsilon insensitive with the P norm.",
                    "label": 0
                },
                {
                    "sent": "And you can extend to multiple kernel learning with optimizing this.",
                    "label": 0
                },
                {
                    "sent": "As you would expect, these cameras are now vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so just fair.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Straight forward.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So now the question is how would we apply this to the approximation of nonlinear systems with vector valued?",
                    "label": 1
                },
                {
                    "sent": "Paths and so.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to actually combine kernels.",
                    "label": 0
                },
                {
                    "sent": "With a different kernels that we might consider, but with they might be, combination might vary overtime, so one period some kernel might be more relevant and another period of a different kernel, and so the combination is controlled by these FI Jays which are basically combining a set of Gaussian RBF localities around certain times.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is you would adapt.",
                    "label": 0
                },
                {
                    "sent": "This is just by the way we haven't got results for this is just a proposal.",
                    "label": 0
                },
                {
                    "sent": "And so you would need to optimize this in the in the multiple kernel learning, and hopefully this could.",
                    "label": 0
                },
                {
                    "sent": "This could be done and fine.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slide is to show that actually, this very naturally translates into an application of a PAC Bayes bound that we developed for the variational approximation approach since this.",
                    "label": 0
                },
                {
                    "sent": "Combo.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation here can be viewed as a linear differential time varying linear differential equation.",
                    "label": 0
                },
                {
                    "sent": "I mean where the?",
                    "label": 0
                },
                {
                    "sent": "The parameters of the differential equation will be controlled by the varying of these Phi JS.",
                    "label": 0
                },
                {
                    "sent": "We can actually write that down.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this FL?",
                    "label": 0
                },
                {
                    "sent": "And so then we can apply the PAC Bayes approach where we know the.",
                    "label": 0
                },
                {
                    "sent": "The prize given by the nonlinear stochastic differential equation and ized TI there, the KL divergent's can be computed OK.",
                    "label": 0
                },
                {
                    "sent": "It may be a little hard to do but principle it can be done and then we can apply the empirical error, the true error and bound the new ekl between the two in terms of this KL divergences here.",
                    "label": 0
                },
                {
                    "sent": "So this applies shows that you know in principle this thing should.",
                    "label": 0
                },
                {
                    "sent": "We should be able to bound it's performance and this could be used to drive algorithms to optimize the fit or to optimize the generalization performance as measured by the expected.",
                    "label": 0
                },
                {
                    "sent": "So the loss here is just the indicator function.",
                    "label": 0
                },
                {
                    "sent": "Whenever an observation is outside the Epsilon star tube around the observations.",
                    "label": 0
                },
                {
                    "sent": "Sorry around the path when the observation is outside that share that part OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}