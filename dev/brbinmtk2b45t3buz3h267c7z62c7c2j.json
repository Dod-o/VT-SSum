{
    "id": "brbinmtk2b45t3buz3h267c7z62c7c2j",
    "title": "Cooperative Visual Dialogue with Deep RL",
    "info": {
        "author": [
            "Dhruv Batra, School of Interactive Computing, College of Computing, Georgia Institute of Technology",
            "Devi Parikh, School of Interactive Computing, College of Computing, Georgia Institute of Technology"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_parikh_batra_deep_rl/",
    "segmentation": [
        [
            "Thank you.",
            "Alright, so I'm going to talk about I'm going to start by talking a little bit about Visual question answering and just giving you guys an update on some of the more recent things we've been doing.",
            "In particular looking at different slices of the task, and then I'll move on to introducing visual dialogue.",
            "I'll give you the background on that and then Drew is going to follow up on some of the URL aspects relevant to conversational agents.",
            "Alright, alright, so let me let me get started."
        ],
        [
            "So my background isn't computer vision and so we've traditionally been interested in being able to take images like this and trying to detect all the objects, figure out where the people are.",
            "Is this an indoor images at an outdoor image?",
            "What is the activity that's going on?",
            "Is this a college campus or not?",
            "And things like that, so it's trying to extract semantics out of the pixels in the image, and more recently there's been more and more interest in trying to take this semantic information from the image and translating it into natural language so that it can be communicated to a human.",
            "Right, so be a tasks like describing the image in the sentence or via tasks like telling.",
            "Maybe a short story about what's going on in the scene, maybe answering a question about this image and some."
        ],
        [
            "They even holding a natural language dialogue grounded in this image.",
            "And so there's a variety of applications for which these visually grounded dialogue agents would be useful.",
            "So one example is aiding visually impaired users so it can be assistive technology that someone is using in their homes or even on social media.",
            "For example, on Facebook, for instance, you can now upload an image, and there's an automatic approach that converts this image into sort of a caption if you will.",
            "That tells you that this image may contain pizza, people, food, and so on, and so you can imagine that if this information is not enough, if someone wants to know more about it.",
            "They can follow up on this by having a dialogue with an agent that where they can find out more about this image, right?",
            "So for example, maybe for any first."
        ],
        [
            "An image the agent describes this image in a sentence and then you can ask follow up questions like great is he at the beach and it might respond to an amount in and so on.",
            "Another application is where the user may not be biologically impaired, but might be situationally impaired in the sense that if you think of an analyst that's looking at large quantities."
        ],
        [
            "Surveillance video feed.",
            "It's not feasible for the person to go through sift through all of this data to extract the relevant information that may care about that they might care about, and so a very natural interface to elicit the information that's relevant to whatever decision that they're trying to make would be through natural language, right?",
            "So, for example, someone might ask, did anyone enter this room last week?",
            "It would be nice if there was an agent that could look through all of this video and automatically respond with yes 127 instances logged on camera, but any of them carrying a black backpack.",
            "And so on.",
            "And here's here's another example of."
        ],
        [
            "Be similar to the situational impairment in the sense that if there's a certain environment where it's not safe or it's not feasible for a human to go in, but it's OK to send an agent in, for example, this robot that you see right here, you would want to be able to interact with this robot and extract the relevant information again through natural language, right?",
            "So you might want to be able to have a person that says is there smoke in any room around you.",
            "The agent responds with yes, in one room.",
            "Go there and look for people.",
            "Right?",
            "So these are.",
            "These are sort of applications where having a conversation grounded in visual data could be useful, right?",
            "So what I'm going to do today is I'm going to start by talk."
        ],
        [
            "About one step that we took in this direction, but it was just visual question answering just one short of a question and the response as opposed to a back and forth dialogue.",
            "And like I said, I'll give you some background on that and tell you some of the recent tasks that we've set up in this space.",
            "And then after that I'll introduce this task of visual dialogue which extend just this one short interaction to a more sequential conversation, or a sequential dialogue.",
            "And just a quick shout out before."
        ],
        [
            "Get to that that there's been what I'll talk about, and then whatever through will talk about, there's been parallel work from a lot of people here in Montreal.",
            "This this guess what?",
            "Which is sort of a task oriented, goal driven dialogue that agents might be having."
        ],
        [
            "And then there's been follow-up work on that, which will, which will be more relevant to work through talks about?",
            "Alright, so let me let me get started with."
        ],
        [
            "The question on city so visual question answering the task is you're given an image.",
            "You're given a free form natural language question about this image and we want to be able to build AI systems that can answer this question accurately, right?",
            "And so just to ground this task, let me show you a demo.",
            "So much enough so this demo is hosted off of Cloud CV, which is a project out of the roof slab and what you can do is you can drag any image here.",
            "And then start asking questions about it, right?",
            "So the task is you can ask any questions.",
            "So what is in this image?",
            "So it's food.",
            "What kind of food is it sandwich?",
            "How many sandwiches are there?",
            "Two, what are the sandwiches on?",
            "Wait, what color is the plate white?",
            "Is there any wine so this is a demo account demo straight so I know this works, but you're welcome, but you're welcome.",
            "You're welcome to try this out.",
            "It's on line you're going to load any image, ask any questions, and at the end of my talk if there's time.",
            "I'm also happy to take questions from the audience.",
            "Wine in the image.",
            "Yes, what color is the wine?",
            "Read that song right?",
            "So this is mainly to give you a sense for what this task is.",
            "Given any image you want to be able to ask any question and you're hoping to build systems that can answer them in a sensible way.",
            "Alright."
        ],
        [
            "So that's the task.",
            "And so because you can ask any questions, you see a wide variety of questions that the system would have to deal with.",
            "There might be questions that require you to reason about attributes of of small objects in the image.",
            "You might have to recognize objects out of context, like this banana.",
            "You might need to count.",
            "You might need to have access to external knowledge that if there's meat on something then it might not be vegetarian.",
            "Social context, scene attributes and so on."
        ],
        [
            "And so one of the first things that we did in this space, and this was about two years ago, now was we collected this week.",
            "Your data set, which had over 1/4 of a million images.",
            "So these were images from the Coco data set.",
            "Here's a random sample of."
        ],
        [
            "These images look like and we also had 50,000."
        ],
        [
            "Abstract scenes which were images made from clip art where you already know where the objects are, what these objects are, what their poses, sort of the low level vision tasks have already been solved, and you can focus on the higher level reasoning to answer questions about these images.",
            "And then we had over 3/4 of."
        ],
        [
            "A million questions where we collected three questions for every image and the way we collected these was we went to Mechanical Turk and we asked.",
            "We showed people an image and we asked them to ask a question about this image that they think."
        ],
        [
            "Wood stump, a smart robot, and so this is our attempt at trying to get somewhat interesting questions about these images, and then we had 13 different people answer every question about the image, so that's your ground truth, and they gave us about 10,000,000."
        ],
        [
            "Unsers and so this was the data that we introduced at ICC via couple of years ago.",
            "So since since in these last couple of years there's been a lot of."
        ],
        [
            "It can be QA trying to look at different attention mechanisms to figure out which image which regions in the image are relevant to answering the question.",
            "Looking at attention mechanisms on the question, trying to reason about which words and phrases are relevant in the question.",
            "Looking at different ways to combine information from both modalities, images and questions, and looking at ways in which you can induce a program based on a question such that the execution of that program gives you the answer to that question and a wide variety of directions."
        ],
        [
            "And so that's been very exciting.",
            "We had organized a challenge at CPR last year.",
            "Some 30 teams participated just to give you a sense for the state of the art from last year.",
            "The winning team had this."
        ],
        [
            "Multimodal compact bilinear pooling approach from Berkeley and the state of the art on the data set at the time was 66% and it's gone up a few percent since.",
            "Alright, so that was all great.",
            "It's been an exciting space, but there are a few issues that I want to talk about when you try and think of visual question answering as a way of benchmarking progress on image understanding or in computer vision, right?",
            "And so one of those issues is that language priors can be extremely strong, right?",
            "And we saw this even in captioning.",
            "So for example, back in 2014 when we first saw results where a system could automatically take this image in and spit out a description that set a giraffe is standing in grass next to a tree.",
            "Some of us were really excited.",
            "I don't know how many of you were doing computer vision before that, but it was exciting to think that all this model recognize the giraffe.",
            "It figured out that it's standing standing in grass.",
            "It realizes there are trees around it, and so on."
        ],
        [
            "But turns out that if you just take a random sample of images from the Coco data set that have a giraffe in it.",
            "This description fits quite well to many of these images.",
            "Right, and so it turned out that all the system really had to do was recognized giraffe, which isn't all that hard and images like this and then just hallucinate the rest.",
            "Just hallucinate the standing.",
            "The standing being in the grass.",
            "The fact that there are trees around just make all of that up and the description will match quite well to most images that have a giraffe in this data set.",
            "And so that was not satisfying, right?",
            "That's not why image capturing was interesting from a computer vision standpoint."
        ],
        [
            "The same thing happens in the QA.",
            "So for example in the week your data set, if there's a question that says is there a Clock.",
            "Don't even listen to the rest of the question.",
            "Don't even look at the image, just say yes and you'll be right 98% of the time.",
            "Right is the man wearing glasses.",
            "Again, don't look at the image and listen to the rest of the question.",
            "Just say yes, you'll be right 94% of the time.",
            "Are the lights on?",
            "Yes, do you see and just say yes?",
            "And and and.",
            "The reason for this was the way in which these questions had been collected.",
            "I told you that we showed people an image and we asked them to ask a question about this image.",
            "And you're not going to think about asking a question about a Clock tower unless you see your Clock tower in the image.",
            "And so when you ask, is there a Clock tower this probably a Clock tower, which is why you're asking about it in the 1st place, right?",
            "And so there's a heavy prior towards yes in this case.",
            "And it's not just about you."
        ],
        [
            "There are other questions like is the man standing nose?",
            "The more popular answer?",
            "What sport is just a tennis?",
            "Because Coco has a lot of tennis images.",
            "How many do with your safe bet?",
            "What animals, even an abstract scenes we should go with dog right?",
            "And so that again, is problematic that yes, you can use these priors and do well on the Wikia benchmark, but that doesn't really help us benchmark the extent to which these these algorithms are understanding the image.",
            "And so one attempt that we've recently made."
        ],
        [
            "Trying to address this and trying to balance these language browsers out while still keeping the richness and compositionality and complexities that language bring that language brings is to introduce this balance.",
            "Tweak your data set, and So what we've done is the following for every image question pair from the weaker data set.",
            "What we did was we showed people 24 nearest neighbors of that image and we asked them to pick an image where the answer to that question is different from what the answer to that on that original image was right?",
            "So for example, there was an image of attention."
        ],
        [
            "Image and the question is what game is this?",
            "So the answer to that image was tennis and now we've shown you 24 nearest neighbors to that image in FC-7 space and we've asked use workers to click on an image where the answer to the same question is not tennis.",
            "Right, and So what?",
            "This gives us, and so they might, for example, click on this.",
            "And So what this gives us is for every question there are now two images that are semantically quite similar because they are near neighbors.",
            "In this FC-7 representation.",
            "But where the answer to the same question is different so."
        ],
        [
            "Is the TV on for this image?",
            "The answer is yes and for this image the answer is no.",
            "And so now just from a language prior you can't just take the question with the TV on and guess yes and get away with it, right?",
            "You do have to look at the image and understand whether the TV is on or not and then respond how many."
        ],
        [
            "It's our present again.",
            "These are two similar images where the answer for one image is 2 and the answer to the other is yes."
        ],
        [
            "What sign is this handicap?",
            "In one case, one way and the other?",
            "Where is the child?"
        ],
        [
            "Eating fridge and arms, what is?"
        ],
        [
            "Cat doing on the rug.",
            "In one case sleeping the other is sitting."
        ],
        [
            "Color depends one case, it's orange, the other is proud.",
            "And so I hope with this data set is that this will now require VK models to focus on the image at least a little bit more than that.",
            "Then they had to in the previous version of the data set, right?",
            "So this is the week over."
        ],
        [
            "And 2.0 it's more balanced than the first version of this data set in terms of language priors, so you can look at the entropy of answers conditioned on certain question engrams, and that entropy goes up by a good amount.",
            "It's also bigger than the original, so that should be bigger than weaker version 1.0 because of this very nature, where for every image we've now found a second second image, so it's about 1.8 times larger, which is nice.",
            "And so if you."
        ],
        [
            "Benchmark certain existing state of the art VK models on this new data set.",
            "Sort of trends across the board are that most of these models if you train them on weaker version 1.0 and then test them on this balanced set, accuracy goes down by about 7 to 8% and then if you retrain them on this balance that you gain about one or two percent back.",
            "One interesting thing is that if you break up, if you analyze this based on answer types, what we found is that the biggest drop in performance around the questions where the answer was yes or no.",
            "And I gave you hints of this earlier, but there was a prior towards yes, so these models had a hard time struggling on this new data set where now yes or no are both equally likely for the same question, right?",
            "The other interesting thing was like once you retrain on this balance data set, which means the model has to focus on the image a little bit more.",
            "You get the biggest improvement back.",
            "Or these yes, no questions.",
            "And for the number questions where the question was how many of something?",
            "And what's interesting to note here is if you look at the top four approaches on the weaker version 1.0 from the challenge last year there was a healthy improvement on the accuracy overall.",
            "But what was disappointing was that there was barely any difference on the yes, no questions, and there wasn't as much of a difference on the number questions either.",
            "Most of the improvements were coming from this other questions and our hope is that with these tendencies that we're seeing with the balance data set, there might be more room for algorithms to actually make progress on these binary questions.",
            "And on the counting questions.",
            "And so that's what we're organizing a child of equal challenge at CPR in a few weeks.",
            "It's on this version 2.0 of the data set and the paper that in."
        ],
        [
            "Reduce the status that is also being presented in CPO.",
            "Alright, so are there?",
            "Are we taking questions as we go along or are there any questions so far?",
            "Yes.",
            "It was the 24 nearest neighbors in FC-7 representation of fighting.",
            "It was VGG net or something, some standard CNN.",
            "There's a sense here that you had to identify which of the factors you wanted to balance for, right?",
            "So how do you?",
            "You know, take that into account that you know as you want to get richer and richer representations or captions, you're going to have to balance from anymore.",
            "Now you're balancing for like 2 things you know, like the object and maybe some characteristics you're going to have to balance more and more things to get more complex description, right, right?",
            "And So what we're balancing for is in a question, question, condition, manner that for every question we want to have two images where there are similar, but the answers are different and there are, and I'm going to talk about a couple of other ways in which we've tried to split this task up to get to this image grounding.",
            "And I think there's no one good answer to what the right way of balancing is.",
            "It's sort of more of a post hoc thing that when you see behaviors of algorithms and you realize that that's not what you meant for this benchmark to capture you, try and add that in and then you go from there.",
            "Get feedback from the community and move, move forward.",
            "Another sort of more balancing that is more natural from a classification perspective is to say that all the answers should be equally likely, but that I don't think makes a lot of sense, or becuase so for example, bananas are.",
            "Usually yellow and then sometimes green, but they're rarely purple and trying to insist on finding images where bananas are powerful might be going too far, and so this is taking a step in that direction, but we don't force it all the way, and so it's not clear at what point data set biases are useful priors from our world versus just idiosyncrasy's that crept in in the way that the data set was collected, and it's very difficult to tease those apart.",
            "Yes.",
            "It seems like one way to see or evaluate these sort of things is to see if the algorithm like recognizes if there is actually enough information to answer the question in the video, or like graphics in factual question as to avoid this article image, you know if anybody is looking into this concern.",
            "So we've done some work on trying to get these models to reason about whether this question is even relevant for this image or not, and I think that's sort of what you're getting at where if the question is what color is the banana?",
            "And there are no bananas.",
            "We've trained some approaches that try to detect that that I shouldn't even be answering this question, because this question doesn't make sense for this image, and an even more extreme version of it is if the question is what's the capital of India.",
            "And there's a picture of a cat, right?",
            "But it's not the ecosystems job to be figuring out what capitals of countries are, so it's recognizing that these questions are just not what I'm trained for.",
            "And so I'm not even going to bother answering them.",
            "So we've done some work on that.",
            "If interested, there's a paper at EML P. 2016 on that, and if you send me an email, I can send you a pointer to that.",
            "Yes.",
            "Isolated events you Sir.",
            "Actually.",
            "Yes.",
            "If I pass something by the water.",
            "Yes, and so that exactly gets to dialogue right where we QA is very much A1.",
            "Short response to every question.",
            "But then when you have things like what color is it, the fact that it is referring back to the board that we talked about in an earlier question.",
            "That is exactly one of the challenges that comes up when we move to dialogue and I'll talk about that in a little bit.",
            "There's also an issue of consistency where you might ask one question where it gives you a response.",
            "That is, the person standing.",
            "It would say yes, and then you say is the person sitting, and it might still say yes because it doesn't realize that these both can't be true at the same time.",
            "Any other questions?",
            "Alright.",
            "Oh wow.",
            "So another thing that we've been looking at is trying to deal with the fact that there are certain strong priors on the training set that these models try and exploit.",
            "So for example, for."
        ],
        [
            "Color white is one of the most popular questions, and So what happens is that at Test time, if there's now a question whose answer is black, these models are predisposed to just saying white, because that's what that's the product they picked up on, which in itself would not be an issue.",
            "But the problem is that these priors are the same in the train and the test, and so even if these models are just exporting the product, they can do quite well on the test set in the process of doing that.",
            "And that again gets in the way of evaluating whether these models are actually understanding the image better or not.",
            "Right?"
        ],
        [
            "And so we have another split of the week.",
            "Your data set that tries to tries to change these priors from train to tests.",
            "So during training the most popular sport was tennis.",
            "But then at Test time the more popular sport might be skiing, for example for reading training, the more popular number was one at Test time, the more popular number might be too, and so this is again a way of stress stressing these stress testing these models to see if they're actually looking at the image to answer it or just going based on priors.",
            "Alright, so in the interest of time I'm going to skip ahead to get to visual dialogue."
        ],
        [
            "And so, like I said, visual dialogue is now trying to push this one round of question answering to have sequential questions and answers where you have to reason about the history that we've talked about so far.",
            "So just to give you an example of what we ideally like these agents to do, if there's an image like this, it would be nice if this agent maybe started out by describing this image in a sentence.",
            "So man and a woman are holding umbrellas and then a human might have a follow up question.",
            "What color is his umbrella?",
            "And so in this very first question, you need the system to be able to realize that his is referring to this man that we just talked about and this man was this sort of region in the image that we had talked about and so then his umbrella is referring to this umbrella and it's the color of this umbrella that I need to respond with, right?",
            "So ideally the agent would respond by saying his umbrella is black.",
            "There might be follow question that says what about hers.",
            "This is now even more complicated code reference because one you need to figure out that her is referring to this woman that we talked about here, which was this region in the image.",
            "And we need to realize that hers is referring to her umbrella, which also we had talked about earlier, and that's this umbrella right here, right?",
            "And ideally it would respond with something like hers is multi colored.",
            "How many other people are in the image?",
            "So now other people you need to realize that it is about the people in this image that we haven't already talked about.",
            "We already talked about the man and the woman and these are the other people that's being that we are asking.",
            "We're getting a question for I think 3 there include and how many are men.",
            "And now again we need to reason that how many are men is amongst these other three people that we just talked about and not all the people in the image right?",
            "So there's this very complicated consistency, reasoning, coreference resolution back to the image.",
            "That the agent would have to deal with to be able to do well on this task, right?",
            "And so this is sort of the Holy Grail of what we're going after.",
            "And so there's sort of a nice visual dialogue."
        ],
        [
            "The task sort of formally just stating it is, you're given an image.",
            "You're given a history of the conversation so far, and then you're given a question at round T and the system's task is to answer this question.",
            "The way we set up."
        ],
        [
            "Evaluation because of freeform dialogue, is hard to evaluate, so we set it up as a retrieval task where you're given these hundred options that the system has to sort based on it's based on what's most appropriate given the image given the history.",
            "And given the question that's being asked.",
            "Alright, and so the first thing that we did in this space again was to collect visual dialogue."
        ],
        [
            "Data set and what we did was we paired two people on Mechanical Turk through a live chat interface.",
            "One person was given the role of a question.",
            "Are the other person was given the role of an answer and the question."
        ],
        [
            "Sure was not shown the image.",
            "The questioner was shown a caption that describes the image and the question was told to ask questions so that they can build a mental model of the image that's being described.",
            "The answer was shown the capture and also the image, and when they get a question from the questioner, their job was to respond, right was to answer the question and so this."
        ],
        [
            "Is just a snapshot of what this data collection looks like.",
            "It's two people chatting on Mechanical Turk having a dialogue that's grounded in this image.",
            "And this just this infrastructure of setting up these two people having a live conversation was a good amount of engineering effort that we put into this, and so this interface is publicly available and a couple of dialogue projects have already used that, so it's a valuable resource if you're interested in this space.",
            "And so this data set we have."
        ],
        [
            "More than 120,000 images so far from the Coco data set we've collected one dialogue for every image, and each dialogue consisted of this 10 rounds of back and forth.",
            "So 10 question answers that went back and forth, and so that's this data set.",
            "It has 1.2 million dialogue QA pairs.",
            "So this data set is also publicly available, so we experimented with a variety of sort of these in."
        ],
        [
            "Order decoder style neural dialogue models.",
            "We experimented with a few different encoders.",
            "I won't I won't get into the details of those, and on the decoder side we had a generative decoder where it's trained to sort of maximize the likelihood of whatever the human response was, and we also had a discriminative decoder that takes in as input, not just the image.",
            "The history and the question, but also the hundred options that it needs to sort, and so this decoder is explicitly learning to rank those hundred options to optimize for a metric.",
            "And so keep in mind that this is likely to work more accurately, but this is not a chat board that you can deploy because it needs as input 100 possible options that it can then sort.",
            "And so this was just a visual."
        ],
        [
            "Ocean of memory network encoder.",
            "So these are sort of an exhaustive evaluation of the different generative discriminative encoder decoders that I talked about.",
            "The main takeaway is that we found that of the three encoders that we experimented with, we found that the memory network outperformed the other two questions.",
            "So the way we, the way we specify those options was 100.",
            "Options had obviously one was the ground truth response from the human.",
            "There were a few random options from the data set.",
            "There were a few popular answers from the data set and then there were some options that were nearest neighbors too that were responses to nearest neighbor questions to the question that was asked so they would keep the language the language would be happy picking them up and you really have to reason about the history and the image to be able to rule them out and identify what the right option is.",
            "So it was a different set of 100 options for every question.",
            "It wasn't a fixed set of under options across the board.",
            "So instead of generating the census, basically the answer should be what answer you should should should fit, right?",
            "So that's how the evaluation is set up.",
            "The evaluation is set up where these are the hundred options and the model has to rank them.",
            "You can choose to train a model that is generating a response, but then the way it sorts this is by computing the likelihood of each one, or it can be a discriminative model that explicitly learns to write.",
            "Any other questions?",
            "Alright.",
            "And so code for all of these models is available.",
            "It's nice, modular course, you're I know Abhishek Das, who wrote this code is quite excited about how clean it is.",
            "You're welcome to check it out alright.",
            "And So what?",
            "This gives us is you can sort of put together a visual chat bot, right?",
            "So you can upload an image, starts with the caption and ask a question where is this?",
            "Looks like a city or their other buildings, yes?",
            "Or they told yes great.",
            "How's the weather?",
            "Cloudy?",
            "Anything else?",
            "There's a building in the background, but I can't tell what it is.",
            "Right, so let me let me show you a few examples of the kinds of things that does.",
            "So here's one."
        ],
        [
            "Samples of question answer dialogue that's being held about this image.",
            "You see a lot of short responses.",
            "The responses are fairly accurate in this case, and so this is one of the good examples from our results.",
            "Here's another one that's also not bad.",
            "Somewhat short responses, but fairly accurate.",
            "Here are.",
            "Here's maybe a more interesting."
        ],
        [
            "So if you look at this image in this dialogue, if you look at these pairs of question answers, is the cat on the floor?",
            "Yes, what is the cat on?",
            "I can't tell, right?",
            "So there's still this issue with it not really being very good at being consistent with what it just said in the past of reasoning about history.",
            "Here's another one."
        ],
        [
            "You see anything in the middle?",
            "No, just the cat and the cat.",
            "I mean, there are.",
            "It's had enough.",
            "Here's another one, so this is 1 sort of."
        ],
        [
            "Standard failure case that might not be surprising to people who work in dialogue that it picks this very safe generic response that it can use almost anytime, so it will keep saying I can't tell for most for a good number of questions.",
            "So this was this visual dialogue paper is also coming up at that CPR.",
            "Alright so I will."
        ],
        [
            "We had just one sentence on this.",
            "We have new dialogue model where what we try and do is re Conseil this issue where a discriminative dialogue model performs better because it's explicitly learning to rank the responses.",
            "But you can't deploy that in practice, but as a generative model is what you can deploy in practice.",
            "But it has these issues of using generic safe responses and so we have a model that tries to sort of get the best of both worlds where it very first trains a discriminative model.",
            "It freezes that and then uses that as a perceptual loss or an Oracle.",
            "To optimize the generator so that the generative model is trying to produce responses that the discriminator is happy with, and so that gives us qualitatively much better responses."
        ],
        [
            "Alright, and so with that, some interesting directions in dialogue.",
            "I think one is once you've trained these agents.",
            "If you just like I've talked about training and answer that answers questions, but you can also think about training a questionnaire that asks questions about images and if you do that, it's interesting to think about whether you can get them to talk to each other, sort of in a self play like setting to then become even better at being able to respond to human questions.",
            "Andrew will talk a little bit about that and so what's nice is that these agents can get better without needing any more human intervention.",
            "Um?",
            "And I think in general things like trying to predict where the conversation is going, especially if you're trying to accomplish a downstream task sort of theory of mind or trying to reason about what the other agent that you're talking to knows or does not know, or what is it that they care about.",
            "I think those are all interesting open directions in this space, so with that I'll."
        ],
        [
            "Conclude, I think there's this natural progression of tasks in the vision language space, starting from captioning to visual question answering of answering one question to visual dialogue in V QA.",
            "I talked about some efforts that we are making are trying to elevate the role of image understanding in visual Question, answering as a benchmark and then finally introduced visual dialogue, which is this task that not only requires understanding the image and the question, but also reasoning about history having some memory, being consistent with what you said in the past, and I think that's.",
            "That's an exciting space.",
            "Alright, so I'll end there, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to talk about I'm going to start by talking a little bit about Visual question answering and just giving you guys an update on some of the more recent things we've been doing.",
                    "label": 0
                },
                {
                    "sent": "In particular looking at different slices of the task, and then I'll move on to introducing visual dialogue.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the background on that and then Drew is going to follow up on some of the URL aspects relevant to conversational agents.",
                    "label": 0
                },
                {
                    "sent": "Alright, alright, so let me let me get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my background isn't computer vision and so we've traditionally been interested in being able to take images like this and trying to detect all the objects, figure out where the people are.",
                    "label": 0
                },
                {
                    "sent": "Is this an indoor images at an outdoor image?",
                    "label": 0
                },
                {
                    "sent": "What is the activity that's going on?",
                    "label": 0
                },
                {
                    "sent": "Is this a college campus or not?",
                    "label": 0
                },
                {
                    "sent": "And things like that, so it's trying to extract semantics out of the pixels in the image, and more recently there's been more and more interest in trying to take this semantic information from the image and translating it into natural language so that it can be communicated to a human.",
                    "label": 0
                },
                {
                    "sent": "Right, so be a tasks like describing the image in the sentence or via tasks like telling.",
                    "label": 0
                },
                {
                    "sent": "Maybe a short story about what's going on in the scene, maybe answering a question about this image and some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They even holding a natural language dialogue grounded in this image.",
                    "label": 0
                },
                {
                    "sent": "And so there's a variety of applications for which these visually grounded dialogue agents would be useful.",
                    "label": 0
                },
                {
                    "sent": "So one example is aiding visually impaired users so it can be assistive technology that someone is using in their homes or even on social media.",
                    "label": 0
                },
                {
                    "sent": "For example, on Facebook, for instance, you can now upload an image, and there's an automatic approach that converts this image into sort of a caption if you will.",
                    "label": 0
                },
                {
                    "sent": "That tells you that this image may contain pizza, people, food, and so on, and so you can imagine that if this information is not enough, if someone wants to know more about it.",
                    "label": 0
                },
                {
                    "sent": "They can follow up on this by having a dialogue with an agent that where they can find out more about this image, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, maybe for any first.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An image the agent describes this image in a sentence and then you can ask follow up questions like great is he at the beach and it might respond to an amount in and so on.",
                    "label": 0
                },
                {
                    "sent": "Another application is where the user may not be biologically impaired, but might be situationally impaired in the sense that if you think of an analyst that's looking at large quantities.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Surveillance video feed.",
                    "label": 0
                },
                {
                    "sent": "It's not feasible for the person to go through sift through all of this data to extract the relevant information that may care about that they might care about, and so a very natural interface to elicit the information that's relevant to whatever decision that they're trying to make would be through natural language, right?",
                    "label": 0
                },
                {
                    "sent": "So, for example, someone might ask, did anyone enter this room last week?",
                    "label": 1
                },
                {
                    "sent": "It would be nice if there was an agent that could look through all of this video and automatically respond with yes 127 instances logged on camera, but any of them carrying a black backpack.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And here's here's another example of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be similar to the situational impairment in the sense that if there's a certain environment where it's not safe or it's not feasible for a human to go in, but it's OK to send an agent in, for example, this robot that you see right here, you would want to be able to interact with this robot and extract the relevant information again through natural language, right?",
                    "label": 0
                },
                {
                    "sent": "So you might want to be able to have a person that says is there smoke in any room around you.",
                    "label": 1
                },
                {
                    "sent": "The agent responds with yes, in one room.",
                    "label": 1
                },
                {
                    "sent": "Go there and look for people.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "These are sort of applications where having a conversation grounded in visual data could be useful, right?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do today is I'm going to start by talk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About one step that we took in this direction, but it was just visual question answering just one short of a question and the response as opposed to a back and forth dialogue.",
                    "label": 1
                },
                {
                    "sent": "And like I said, I'll give you some background on that and tell you some of the recent tasks that we've set up in this space.",
                    "label": 0
                },
                {
                    "sent": "And then after that I'll introduce this task of visual dialogue which extend just this one short interaction to a more sequential conversation, or a sequential dialogue.",
                    "label": 0
                },
                {
                    "sent": "And just a quick shout out before.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get to that that there's been what I'll talk about, and then whatever through will talk about, there's been parallel work from a lot of people here in Montreal.",
                    "label": 0
                },
                {
                    "sent": "This this guess what?",
                    "label": 0
                },
                {
                    "sent": "Which is sort of a task oriented, goal driven dialogue that agents might be having.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's been follow-up work on that, which will, which will be more relevant to work through talks about?",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me get started with.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question on city so visual question answering the task is you're given an image.",
                    "label": 1
                },
                {
                    "sent": "You're given a free form natural language question about this image and we want to be able to build AI systems that can answer this question accurately, right?",
                    "label": 0
                },
                {
                    "sent": "And so just to ground this task, let me show you a demo.",
                    "label": 0
                },
                {
                    "sent": "So much enough so this demo is hosted off of Cloud CV, which is a project out of the roof slab and what you can do is you can drag any image here.",
                    "label": 0
                },
                {
                    "sent": "And then start asking questions about it, right?",
                    "label": 0
                },
                {
                    "sent": "So the task is you can ask any questions.",
                    "label": 1
                },
                {
                    "sent": "So what is in this image?",
                    "label": 0
                },
                {
                    "sent": "So it's food.",
                    "label": 0
                },
                {
                    "sent": "What kind of food is it sandwich?",
                    "label": 0
                },
                {
                    "sent": "How many sandwiches are there?",
                    "label": 0
                },
                {
                    "sent": "Two, what are the sandwiches on?",
                    "label": 0
                },
                {
                    "sent": "Wait, what color is the plate white?",
                    "label": 0
                },
                {
                    "sent": "Is there any wine so this is a demo account demo straight so I know this works, but you're welcome, but you're welcome.",
                    "label": 0
                },
                {
                    "sent": "You're welcome to try this out.",
                    "label": 0
                },
                {
                    "sent": "It's on line you're going to load any image, ask any questions, and at the end of my talk if there's time.",
                    "label": 0
                },
                {
                    "sent": "I'm also happy to take questions from the audience.",
                    "label": 1
                },
                {
                    "sent": "Wine in the image.",
                    "label": 0
                },
                {
                    "sent": "Yes, what color is the wine?",
                    "label": 0
                },
                {
                    "sent": "Read that song right?",
                    "label": 0
                },
                {
                    "sent": "So this is mainly to give you a sense for what this task is.",
                    "label": 0
                },
                {
                    "sent": "Given any image you want to be able to ask any question and you're hoping to build systems that can answer them in a sensible way.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the task.",
                    "label": 0
                },
                {
                    "sent": "And so because you can ask any questions, you see a wide variety of questions that the system would have to deal with.",
                    "label": 0
                },
                {
                    "sent": "There might be questions that require you to reason about attributes of of small objects in the image.",
                    "label": 0
                },
                {
                    "sent": "You might have to recognize objects out of context, like this banana.",
                    "label": 0
                },
                {
                    "sent": "You might need to count.",
                    "label": 0
                },
                {
                    "sent": "You might need to have access to external knowledge that if there's meat on something then it might not be vegetarian.",
                    "label": 0
                },
                {
                    "sent": "Social context, scene attributes and so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so one of the first things that we did in this space, and this was about two years ago, now was we collected this week.",
                    "label": 0
                },
                {
                    "sent": "Your data set, which had over 1/4 of a million images.",
                    "label": 1
                },
                {
                    "sent": "So these were images from the Coco data set.",
                    "label": 0
                },
                {
                    "sent": "Here's a random sample of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These images look like and we also had 50,000.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abstract scenes which were images made from clip art where you already know where the objects are, what these objects are, what their poses, sort of the low level vision tasks have already been solved, and you can focus on the higher level reasoning to answer questions about these images.",
                    "label": 0
                },
                {
                    "sent": "And then we had over 3/4 of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A million questions where we collected three questions for every image and the way we collected these was we went to Mechanical Turk and we asked.",
                    "label": 0
                },
                {
                    "sent": "We showed people an image and we asked them to ask a question about this image that they think.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wood stump, a smart robot, and so this is our attempt at trying to get somewhat interesting questions about these images, and then we had 13 different people answer every question about the image, so that's your ground truth, and they gave us about 10,000,000.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unsers and so this was the data that we introduced at ICC via couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "So since since in these last couple of years there's been a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can be QA trying to look at different attention mechanisms to figure out which image which regions in the image are relevant to answering the question.",
                    "label": 0
                },
                {
                    "sent": "Looking at attention mechanisms on the question, trying to reason about which words and phrases are relevant in the question.",
                    "label": 0
                },
                {
                    "sent": "Looking at different ways to combine information from both modalities, images and questions, and looking at ways in which you can induce a program based on a question such that the execution of that program gives you the answer to that question and a wide variety of directions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that's been very exciting.",
                    "label": 0
                },
                {
                    "sent": "We had organized a challenge at CPR last year.",
                    "label": 0
                },
                {
                    "sent": "Some 30 teams participated just to give you a sense for the state of the art from last year.",
                    "label": 0
                },
                {
                    "sent": "The winning team had this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multimodal compact bilinear pooling approach from Berkeley and the state of the art on the data set at the time was 66% and it's gone up a few percent since.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that was all great.",
                    "label": 0
                },
                {
                    "sent": "It's been an exciting space, but there are a few issues that I want to talk about when you try and think of visual question answering as a way of benchmarking progress on image understanding or in computer vision, right?",
                    "label": 0
                },
                {
                    "sent": "And so one of those issues is that language priors can be extremely strong, right?",
                    "label": 0
                },
                {
                    "sent": "And we saw this even in captioning.",
                    "label": 0
                },
                {
                    "sent": "So for example, back in 2014 when we first saw results where a system could automatically take this image in and spit out a description that set a giraffe is standing in grass next to a tree.",
                    "label": 0
                },
                {
                    "sent": "Some of us were really excited.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many of you were doing computer vision before that, but it was exciting to think that all this model recognize the giraffe.",
                    "label": 0
                },
                {
                    "sent": "It figured out that it's standing standing in grass.",
                    "label": 0
                },
                {
                    "sent": "It realizes there are trees around it, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But turns out that if you just take a random sample of images from the Coco data set that have a giraffe in it.",
                    "label": 1
                },
                {
                    "sent": "This description fits quite well to many of these images.",
                    "label": 0
                },
                {
                    "sent": "Right, and so it turned out that all the system really had to do was recognized giraffe, which isn't all that hard and images like this and then just hallucinate the rest.",
                    "label": 0
                },
                {
                    "sent": "Just hallucinate the standing.",
                    "label": 0
                },
                {
                    "sent": "The standing being in the grass.",
                    "label": 0
                },
                {
                    "sent": "The fact that there are trees around just make all of that up and the description will match quite well to most images that have a giraffe in this data set.",
                    "label": 0
                },
                {
                    "sent": "And so that was not satisfying, right?",
                    "label": 0
                },
                {
                    "sent": "That's not why image capturing was interesting from a computer vision standpoint.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same thing happens in the QA.",
                    "label": 0
                },
                {
                    "sent": "So for example in the week your data set, if there's a question that says is there a Clock.",
                    "label": 1
                },
                {
                    "sent": "Don't even listen to the rest of the question.",
                    "label": 0
                },
                {
                    "sent": "Don't even look at the image, just say yes and you'll be right 98% of the time.",
                    "label": 0
                },
                {
                    "sent": "Right is the man wearing glasses.",
                    "label": 1
                },
                {
                    "sent": "Again, don't look at the image and listen to the rest of the question.",
                    "label": 0
                },
                {
                    "sent": "Just say yes, you'll be right 94% of the time.",
                    "label": 1
                },
                {
                    "sent": "Are the lights on?",
                    "label": 0
                },
                {
                    "sent": "Yes, do you see and just say yes?",
                    "label": 0
                },
                {
                    "sent": "And and and.",
                    "label": 0
                },
                {
                    "sent": "The reason for this was the way in which these questions had been collected.",
                    "label": 0
                },
                {
                    "sent": "I told you that we showed people an image and we asked them to ask a question about this image.",
                    "label": 0
                },
                {
                    "sent": "And you're not going to think about asking a question about a Clock tower unless you see your Clock tower in the image.",
                    "label": 0
                },
                {
                    "sent": "And so when you ask, is there a Clock tower this probably a Clock tower, which is why you're asking about it in the 1st place, right?",
                    "label": 0
                },
                {
                    "sent": "And so there's a heavy prior towards yes in this case.",
                    "label": 0
                },
                {
                    "sent": "And it's not just about you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are other questions like is the man standing nose?",
                    "label": 1
                },
                {
                    "sent": "The more popular answer?",
                    "label": 1
                },
                {
                    "sent": "What sport is just a tennis?",
                    "label": 1
                },
                {
                    "sent": "Because Coco has a lot of tennis images.",
                    "label": 0
                },
                {
                    "sent": "How many do with your safe bet?",
                    "label": 0
                },
                {
                    "sent": "What animals, even an abstract scenes we should go with dog right?",
                    "label": 0
                },
                {
                    "sent": "And so that again, is problematic that yes, you can use these priors and do well on the Wikia benchmark, but that doesn't really help us benchmark the extent to which these these algorithms are understanding the image.",
                    "label": 0
                },
                {
                    "sent": "And so one attempt that we've recently made.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trying to address this and trying to balance these language browsers out while still keeping the richness and compositionality and complexities that language bring that language brings is to introduce this balance.",
                    "label": 0
                },
                {
                    "sent": "Tweak your data set, and So what we've done is the following for every image question pair from the weaker data set.",
                    "label": 0
                },
                {
                    "sent": "What we did was we showed people 24 nearest neighbors of that image and we asked them to pick an image where the answer to that question is different from what the answer to that on that original image was right?",
                    "label": 0
                },
                {
                    "sent": "So for example, there was an image of attention.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image and the question is what game is this?",
                    "label": 0
                },
                {
                    "sent": "So the answer to that image was tennis and now we've shown you 24 nearest neighbors to that image in FC-7 space and we've asked use workers to click on an image where the answer to the same question is not tennis.",
                    "label": 0
                },
                {
                    "sent": "Right, and So what?",
                    "label": 0
                },
                {
                    "sent": "This gives us, and so they might, for example, click on this.",
                    "label": 0
                },
                {
                    "sent": "And So what this gives us is for every question there are now two images that are semantically quite similar because they are near neighbors.",
                    "label": 0
                },
                {
                    "sent": "In this FC-7 representation.",
                    "label": 0
                },
                {
                    "sent": "But where the answer to the same question is different so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the TV on for this image?",
                    "label": 0
                },
                {
                    "sent": "The answer is yes and for this image the answer is no.",
                    "label": 0
                },
                {
                    "sent": "And so now just from a language prior you can't just take the question with the TV on and guess yes and get away with it, right?",
                    "label": 0
                },
                {
                    "sent": "You do have to look at the image and understand whether the TV is on or not and then respond how many.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's our present again.",
                    "label": 0
                },
                {
                    "sent": "These are two similar images where the answer for one image is 2 and the answer to the other is yes.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What sign is this handicap?",
                    "label": 0
                },
                {
                    "sent": "In one case, one way and the other?",
                    "label": 0
                },
                {
                    "sent": "Where is the child?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eating fridge and arms, what is?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cat doing on the rug.",
                    "label": 0
                },
                {
                    "sent": "In one case sleeping the other is sitting.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Color depends one case, it's orange, the other is proud.",
                    "label": 0
                },
                {
                    "sent": "And so I hope with this data set is that this will now require VK models to focus on the image at least a little bit more than that.",
                    "label": 0
                },
                {
                    "sent": "Then they had to in the previous version of the data set, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the week over.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And 2.0 it's more balanced than the first version of this data set in terms of language priors, so you can look at the entropy of answers conditioned on certain question engrams, and that entropy goes up by a good amount.",
                    "label": 1
                },
                {
                    "sent": "It's also bigger than the original, so that should be bigger than weaker version 1.0 because of this very nature, where for every image we've now found a second second image, so it's about 1.8 times larger, which is nice.",
                    "label": 0
                },
                {
                    "sent": "And so if you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Benchmark certain existing state of the art VK models on this new data set.",
                    "label": 1
                },
                {
                    "sent": "Sort of trends across the board are that most of these models if you train them on weaker version 1.0 and then test them on this balanced set, accuracy goes down by about 7 to 8% and then if you retrain them on this balance that you gain about one or two percent back.",
                    "label": 0
                },
                {
                    "sent": "One interesting thing is that if you break up, if you analyze this based on answer types, what we found is that the biggest drop in performance around the questions where the answer was yes or no.",
                    "label": 1
                },
                {
                    "sent": "And I gave you hints of this earlier, but there was a prior towards yes, so these models had a hard time struggling on this new data set where now yes or no are both equally likely for the same question, right?",
                    "label": 0
                },
                {
                    "sent": "The other interesting thing was like once you retrain on this balance data set, which means the model has to focus on the image a little bit more.",
                    "label": 1
                },
                {
                    "sent": "You get the biggest improvement back.",
                    "label": 0
                },
                {
                    "sent": "Or these yes, no questions.",
                    "label": 0
                },
                {
                    "sent": "And for the number questions where the question was how many of something?",
                    "label": 0
                },
                {
                    "sent": "And what's interesting to note here is if you look at the top four approaches on the weaker version 1.0 from the challenge last year there was a healthy improvement on the accuracy overall.",
                    "label": 0
                },
                {
                    "sent": "But what was disappointing was that there was barely any difference on the yes, no questions, and there wasn't as much of a difference on the number questions either.",
                    "label": 0
                },
                {
                    "sent": "Most of the improvements were coming from this other questions and our hope is that with these tendencies that we're seeing with the balance data set, there might be more room for algorithms to actually make progress on these binary questions.",
                    "label": 0
                },
                {
                    "sent": "And on the counting questions.",
                    "label": 0
                },
                {
                    "sent": "And so that's what we're organizing a child of equal challenge at CPR in a few weeks.",
                    "label": 0
                },
                {
                    "sent": "It's on this version 2.0 of the data set and the paper that in.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reduce the status that is also being presented in CPO.",
                    "label": 0
                },
                {
                    "sent": "Alright, so are there?",
                    "label": 0
                },
                {
                    "sent": "Are we taking questions as we go along or are there any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It was the 24 nearest neighbors in FC-7 representation of fighting.",
                    "label": 0
                },
                {
                    "sent": "It was VGG net or something, some standard CNN.",
                    "label": 0
                },
                {
                    "sent": "There's a sense here that you had to identify which of the factors you wanted to balance for, right?",
                    "label": 0
                },
                {
                    "sent": "So how do you?",
                    "label": 0
                },
                {
                    "sent": "You know, take that into account that you know as you want to get richer and richer representations or captions, you're going to have to balance from anymore.",
                    "label": 0
                },
                {
                    "sent": "Now you're balancing for like 2 things you know, like the object and maybe some characteristics you're going to have to balance more and more things to get more complex description, right, right?",
                    "label": 0
                },
                {
                    "sent": "And So what we're balancing for is in a question, question, condition, manner that for every question we want to have two images where there are similar, but the answers are different and there are, and I'm going to talk about a couple of other ways in which we've tried to split this task up to get to this image grounding.",
                    "label": 0
                },
                {
                    "sent": "And I think there's no one good answer to what the right way of balancing is.",
                    "label": 0
                },
                {
                    "sent": "It's sort of more of a post hoc thing that when you see behaviors of algorithms and you realize that that's not what you meant for this benchmark to capture you, try and add that in and then you go from there.",
                    "label": 0
                },
                {
                    "sent": "Get feedback from the community and move, move forward.",
                    "label": 0
                },
                {
                    "sent": "Another sort of more balancing that is more natural from a classification perspective is to say that all the answers should be equally likely, but that I don't think makes a lot of sense, or becuase so for example, bananas are.",
                    "label": 0
                },
                {
                    "sent": "Usually yellow and then sometimes green, but they're rarely purple and trying to insist on finding images where bananas are powerful might be going too far, and so this is taking a step in that direction, but we don't force it all the way, and so it's not clear at what point data set biases are useful priors from our world versus just idiosyncrasy's that crept in in the way that the data set was collected, and it's very difficult to tease those apart.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It seems like one way to see or evaluate these sort of things is to see if the algorithm like recognizes if there is actually enough information to answer the question in the video, or like graphics in factual question as to avoid this article image, you know if anybody is looking into this concern.",
                    "label": 0
                },
                {
                    "sent": "So we've done some work on trying to get these models to reason about whether this question is even relevant for this image or not, and I think that's sort of what you're getting at where if the question is what color is the banana?",
                    "label": 0
                },
                {
                    "sent": "And there are no bananas.",
                    "label": 0
                },
                {
                    "sent": "We've trained some approaches that try to detect that that I shouldn't even be answering this question, because this question doesn't make sense for this image, and an even more extreme version of it is if the question is what's the capital of India.",
                    "label": 0
                },
                {
                    "sent": "And there's a picture of a cat, right?",
                    "label": 0
                },
                {
                    "sent": "But it's not the ecosystems job to be figuring out what capitals of countries are, so it's recognizing that these questions are just not what I'm trained for.",
                    "label": 0
                },
                {
                    "sent": "And so I'm not even going to bother answering them.",
                    "label": 0
                },
                {
                    "sent": "So we've done some work on that.",
                    "label": 0
                },
                {
                    "sent": "If interested, there's a paper at EML P. 2016 on that, and if you send me an email, I can send you a pointer to that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Isolated events you Sir.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "If I pass something by the water.",
                    "label": 0
                },
                {
                    "sent": "Yes, and so that exactly gets to dialogue right where we QA is very much A1.",
                    "label": 0
                },
                {
                    "sent": "Short response to every question.",
                    "label": 0
                },
                {
                    "sent": "But then when you have things like what color is it, the fact that it is referring back to the board that we talked about in an earlier question.",
                    "label": 0
                },
                {
                    "sent": "That is exactly one of the challenges that comes up when we move to dialogue and I'll talk about that in a little bit.",
                    "label": 0
                },
                {
                    "sent": "There's also an issue of consistency where you might ask one question where it gives you a response.",
                    "label": 0
                },
                {
                    "sent": "That is, the person standing.",
                    "label": 0
                },
                {
                    "sent": "It would say yes, and then you say is the person sitting, and it might still say yes because it doesn't realize that these both can't be true at the same time.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Oh wow.",
                    "label": 0
                },
                {
                    "sent": "So another thing that we've been looking at is trying to deal with the fact that there are certain strong priors on the training set that these models try and exploit.",
                    "label": 0
                },
                {
                    "sent": "So for example, for.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Color white is one of the most popular questions, and So what happens is that at Test time, if there's now a question whose answer is black, these models are predisposed to just saying white, because that's what that's the product they picked up on, which in itself would not be an issue.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that these priors are the same in the train and the test, and so even if these models are just exporting the product, they can do quite well on the test set in the process of doing that.",
                    "label": 0
                },
                {
                    "sent": "And that again gets in the way of evaluating whether these models are actually understanding the image better or not.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we have another split of the week.",
                    "label": 0
                },
                {
                    "sent": "Your data set that tries to tries to change these priors from train to tests.",
                    "label": 0
                },
                {
                    "sent": "So during training the most popular sport was tennis.",
                    "label": 0
                },
                {
                    "sent": "But then at Test time the more popular sport might be skiing, for example for reading training, the more popular number was one at Test time, the more popular number might be too, and so this is again a way of stress stressing these stress testing these models to see if they're actually looking at the image to answer it or just going based on priors.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in the interest of time I'm going to skip ahead to get to visual dialogue.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, like I said, visual dialogue is now trying to push this one round of question answering to have sequential questions and answers where you have to reason about the history that we've talked about so far.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an example of what we ideally like these agents to do, if there's an image like this, it would be nice if this agent maybe started out by describing this image in a sentence.",
                    "label": 0
                },
                {
                    "sent": "So man and a woman are holding umbrellas and then a human might have a follow up question.",
                    "label": 1
                },
                {
                    "sent": "What color is his umbrella?",
                    "label": 0
                },
                {
                    "sent": "And so in this very first question, you need the system to be able to realize that his is referring to this man that we just talked about and this man was this sort of region in the image that we had talked about and so then his umbrella is referring to this umbrella and it's the color of this umbrella that I need to respond with, right?",
                    "label": 0
                },
                {
                    "sent": "So ideally the agent would respond by saying his umbrella is black.",
                    "label": 0
                },
                {
                    "sent": "There might be follow question that says what about hers.",
                    "label": 0
                },
                {
                    "sent": "This is now even more complicated code reference because one you need to figure out that her is referring to this woman that we talked about here, which was this region in the image.",
                    "label": 0
                },
                {
                    "sent": "And we need to realize that hers is referring to her umbrella, which also we had talked about earlier, and that's this umbrella right here, right?",
                    "label": 0
                },
                {
                    "sent": "And ideally it would respond with something like hers is multi colored.",
                    "label": 0
                },
                {
                    "sent": "How many other people are in the image?",
                    "label": 0
                },
                {
                    "sent": "So now other people you need to realize that it is about the people in this image that we haven't already talked about.",
                    "label": 0
                },
                {
                    "sent": "We already talked about the man and the woman and these are the other people that's being that we are asking.",
                    "label": 0
                },
                {
                    "sent": "We're getting a question for I think 3 there include and how many are men.",
                    "label": 0
                },
                {
                    "sent": "And now again we need to reason that how many are men is amongst these other three people that we just talked about and not all the people in the image right?",
                    "label": 0
                },
                {
                    "sent": "So there's this very complicated consistency, reasoning, coreference resolution back to the image.",
                    "label": 0
                },
                {
                    "sent": "That the agent would have to deal with to be able to do well on this task, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is sort of the Holy Grail of what we're going after.",
                    "label": 0
                },
                {
                    "sent": "And so there's sort of a nice visual dialogue.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The task sort of formally just stating it is, you're given an image.",
                    "label": 0
                },
                {
                    "sent": "You're given a history of the conversation so far, and then you're given a question at round T and the system's task is to answer this question.",
                    "label": 0
                },
                {
                    "sent": "The way we set up.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluation because of freeform dialogue, is hard to evaluate, so we set it up as a retrieval task where you're given these hundred options that the system has to sort based on it's based on what's most appropriate given the image given the history.",
                    "label": 0
                },
                {
                    "sent": "And given the question that's being asked.",
                    "label": 0
                },
                {
                    "sent": "Alright, and so the first thing that we did in this space again was to collect visual dialogue.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data set and what we did was we paired two people on Mechanical Turk through a live chat interface.",
                    "label": 1
                },
                {
                    "sent": "One person was given the role of a question.",
                    "label": 0
                },
                {
                    "sent": "Are the other person was given the role of an answer and the question.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure was not shown the image.",
                    "label": 0
                },
                {
                    "sent": "The questioner was shown a caption that describes the image and the question was told to ask questions so that they can build a mental model of the image that's being described.",
                    "label": 0
                },
                {
                    "sent": "The answer was shown the capture and also the image, and when they get a question from the questioner, their job was to respond, right was to answer the question and so this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is just a snapshot of what this data collection looks like.",
                    "label": 0
                },
                {
                    "sent": "It's two people chatting on Mechanical Turk having a dialogue that's grounded in this image.",
                    "label": 0
                },
                {
                    "sent": "And this just this infrastructure of setting up these two people having a live conversation was a good amount of engineering effort that we put into this, and so this interface is publicly available and a couple of dialogue projects have already used that, so it's a valuable resource if you're interested in this space.",
                    "label": 0
                },
                {
                    "sent": "And so this data set we have.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More than 120,000 images so far from the Coco data set we've collected one dialogue for every image, and each dialogue consisted of this 10 rounds of back and forth.",
                    "label": 0
                },
                {
                    "sent": "So 10 question answers that went back and forth, and so that's this data set.",
                    "label": 0
                },
                {
                    "sent": "It has 1.2 million dialogue QA pairs.",
                    "label": 1
                },
                {
                    "sent": "So this data set is also publicly available, so we experimented with a variety of sort of these in.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Order decoder style neural dialogue models.",
                    "label": 0
                },
                {
                    "sent": "We experimented with a few different encoders.",
                    "label": 1
                },
                {
                    "sent": "I won't I won't get into the details of those, and on the decoder side we had a generative decoder where it's trained to sort of maximize the likelihood of whatever the human response was, and we also had a discriminative decoder that takes in as input, not just the image.",
                    "label": 1
                },
                {
                    "sent": "The history and the question, but also the hundred options that it needs to sort, and so this decoder is explicitly learning to rank those hundred options to optimize for a metric.",
                    "label": 0
                },
                {
                    "sent": "And so keep in mind that this is likely to work more accurately, but this is not a chat board that you can deploy because it needs as input 100 possible options that it can then sort.",
                    "label": 0
                },
                {
                    "sent": "And so this was just a visual.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean of memory network encoder.",
                    "label": 1
                },
                {
                    "sent": "So these are sort of an exhaustive evaluation of the different generative discriminative encoder decoders that I talked about.",
                    "label": 0
                },
                {
                    "sent": "The main takeaway is that we found that of the three encoders that we experimented with, we found that the memory network outperformed the other two questions.",
                    "label": 0
                },
                {
                    "sent": "So the way we, the way we specify those options was 100.",
                    "label": 0
                },
                {
                    "sent": "Options had obviously one was the ground truth response from the human.",
                    "label": 0
                },
                {
                    "sent": "There were a few random options from the data set.",
                    "label": 0
                },
                {
                    "sent": "There were a few popular answers from the data set and then there were some options that were nearest neighbors too that were responses to nearest neighbor questions to the question that was asked so they would keep the language the language would be happy picking them up and you really have to reason about the history and the image to be able to rule them out and identify what the right option is.",
                    "label": 0
                },
                {
                    "sent": "So it was a different set of 100 options for every question.",
                    "label": 0
                },
                {
                    "sent": "It wasn't a fixed set of under options across the board.",
                    "label": 0
                },
                {
                    "sent": "So instead of generating the census, basically the answer should be what answer you should should should fit, right?",
                    "label": 0
                },
                {
                    "sent": "So that's how the evaluation is set up.",
                    "label": 0
                },
                {
                    "sent": "The evaluation is set up where these are the hundred options and the model has to rank them.",
                    "label": 0
                },
                {
                    "sent": "You can choose to train a model that is generating a response, but then the way it sorts this is by computing the likelihood of each one, or it can be a discriminative model that explicitly learns to write.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And so code for all of these models is available.",
                    "label": 0
                },
                {
                    "sent": "It's nice, modular course, you're I know Abhishek Das, who wrote this code is quite excited about how clean it is.",
                    "label": 0
                },
                {
                    "sent": "You're welcome to check it out alright.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "This gives us is you can sort of put together a visual chat bot, right?",
                    "label": 0
                },
                {
                    "sent": "So you can upload an image, starts with the caption and ask a question where is this?",
                    "label": 0
                },
                {
                    "sent": "Looks like a city or their other buildings, yes?",
                    "label": 0
                },
                {
                    "sent": "Or they told yes great.",
                    "label": 0
                },
                {
                    "sent": "How's the weather?",
                    "label": 0
                },
                {
                    "sent": "Cloudy?",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "There's a building in the background, but I can't tell what it is.",
                    "label": 0
                },
                {
                    "sent": "Right, so let me let me show you a few examples of the kinds of things that does.",
                    "label": 0
                },
                {
                    "sent": "So here's one.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples of question answer dialogue that's being held about this image.",
                    "label": 0
                },
                {
                    "sent": "You see a lot of short responses.",
                    "label": 0
                },
                {
                    "sent": "The responses are fairly accurate in this case, and so this is one of the good examples from our results.",
                    "label": 0
                },
                {
                    "sent": "Here's another one that's also not bad.",
                    "label": 0
                },
                {
                    "sent": "Somewhat short responses, but fairly accurate.",
                    "label": 0
                },
                {
                    "sent": "Here are.",
                    "label": 0
                },
                {
                    "sent": "Here's maybe a more interesting.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you look at this image in this dialogue, if you look at these pairs of question answers, is the cat on the floor?",
                    "label": 1
                },
                {
                    "sent": "Yes, what is the cat on?",
                    "label": 0
                },
                {
                    "sent": "I can't tell, right?",
                    "label": 0
                },
                {
                    "sent": "So there's still this issue with it not really being very good at being consistent with what it just said in the past of reasoning about history.",
                    "label": 0
                },
                {
                    "sent": "Here's another one.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see anything in the middle?",
                    "label": 1
                },
                {
                    "sent": "No, just the cat and the cat.",
                    "label": 1
                },
                {
                    "sent": "I mean, there are.",
                    "label": 0
                },
                {
                    "sent": "It's had enough.",
                    "label": 0
                },
                {
                    "sent": "Here's another one, so this is 1 sort of.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard failure case that might not be surprising to people who work in dialogue that it picks this very safe generic response that it can use almost anytime, so it will keep saying I can't tell for most for a good number of questions.",
                    "label": 0
                },
                {
                    "sent": "So this was this visual dialogue paper is also coming up at that CPR.",
                    "label": 0
                },
                {
                    "sent": "Alright so I will.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We had just one sentence on this.",
                    "label": 0
                },
                {
                    "sent": "We have new dialogue model where what we try and do is re Conseil this issue where a discriminative dialogue model performs better because it's explicitly learning to rank the responses.",
                    "label": 0
                },
                {
                    "sent": "But you can't deploy that in practice, but as a generative model is what you can deploy in practice.",
                    "label": 1
                },
                {
                    "sent": "But it has these issues of using generic safe responses and so we have a model that tries to sort of get the best of both worlds where it very first trains a discriminative model.",
                    "label": 1
                },
                {
                    "sent": "It freezes that and then uses that as a perceptual loss or an Oracle.",
                    "label": 0
                },
                {
                    "sent": "To optimize the generator so that the generative model is trying to produce responses that the discriminator is happy with, and so that gives us qualitatively much better responses.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and so with that, some interesting directions in dialogue.",
                    "label": 0
                },
                {
                    "sent": "I think one is once you've trained these agents.",
                    "label": 0
                },
                {
                    "sent": "If you just like I've talked about training and answer that answers questions, but you can also think about training a questionnaire that asks questions about images and if you do that, it's interesting to think about whether you can get them to talk to each other, sort of in a self play like setting to then become even better at being able to respond to human questions.",
                    "label": 0
                },
                {
                    "sent": "Andrew will talk a little bit about that and so what's nice is that these agents can get better without needing any more human intervention.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And I think in general things like trying to predict where the conversation is going, especially if you're trying to accomplish a downstream task sort of theory of mind or trying to reason about what the other agent that you're talking to knows or does not know, or what is it that they care about.",
                    "label": 1
                },
                {
                    "sent": "I think those are all interesting open directions in this space, so with that I'll.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclude, I think there's this natural progression of tasks in the vision language space, starting from captioning to visual question answering of answering one question to visual dialogue in V QA.",
                    "label": 1
                },
                {
                    "sent": "I talked about some efforts that we are making are trying to elevate the role of image understanding in visual Question, answering as a benchmark and then finally introduced visual dialogue, which is this task that not only requires understanding the image and the question, but also reasoning about history having some memory, being consistent with what you said in the past, and I think that's.",
                    "label": 1
                },
                {
                    "sent": "That's an exciting space.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll end there, thank you.",
                    "label": 0
                }
            ]
        }
    }
}