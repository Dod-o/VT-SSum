{
    "id": "bhltkroxs5imbxo4qajbv6wow2jrglql",
    "title": "Dynamic Factor Graphs for Time Series Modeling",
    "info": {
        "author": [
            "Piotr Mirowski, Courant Institute of Mathematical Sciences, New York University (NYU)"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Data Mining->Temporal Data",
            "Top->Computer Science->Data Mining->Time Series Analysis"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_mirowski_dfg/",
    "segmentation": [
        [
            "Hello everybody, thank you much for coming to the first talk of the office afternoon session.",
            "My name is Piotr Mirowski and I am currently close to the completion of my PhD in computer science at the Courant Institute of Mathematical Sciences of New York University, and my advisor is young local who is one of the early founders of modern known networks such as convolutional networks, widely used.",
            "And this research is a more theoretical part of my PhD, focused on the modeling of time series using highly nonlinear dynamics, explaining the time series.",
            "So I called this dynamical factor graph."
        ],
        [
            "The reason why I resorted to performance factor graphs instead of more commonly used graphical models or Damico Bayesian networks is that you actually were going to sacrifice most of a probabilistic part of the state space models.",
            "So is this based model is simply a way to link unobserved variables Y?"
        ],
        [
            "To some hidden latent.",
            "And observed variables Z.",
            "Which themselves are governed by dynamics.",
            "And of course we did not know initially, neither of a configuration of a latent variables, which is a pure hypothesis.",
            "We don't even know the number of dimensions and we do not know.",
            "Of course we dynamical model and the observation model which enables to generate observed variables.",
            "Why from an absurd variables Z?",
            "So we are facing lots of unknowns."
        ],
        [
            "However, contrary to a hidden Markov model where the sequence of latent states is discrete, we want to have continuously continuously value with latent states.",
            "And also with as many dimensions as possible as required by our problem.",
            "Um?",
            "So we can see that.",
            "The boat circles representing the state state variables are.",
            "Highly dimensional and continuous values.",
            "Also, we want to enable nonlinear dynamics.",
            "Linking the latent States and possibly also explaining the generative model going from latent states to upset variables.",
            "Yes, so just I just forgot to maybe add that in a factor graph the.",
            "The circles are present variables and Viva little squares presented factors which model the conditional dependencies between subsets of variables.",
            "Yes.",
            "So and only a linear dynamic is for instance a simple matrix that encodes a rotation.",
            "You have an M dimensional vector.",
            "And you try you multiple, you multiply this vector by a matrix.",
            "And what you obtain most probably is a rotation with some scaling.",
            "If you have another dynamic that can suppose pretty much anything, for instance, that coupling between two elements of your of your.",
            "Input vector or maybe a sequence of consecutive transformations with some nonlinearities and?",
            "Maybe a blessing destruction?",
            "Actually I'm skipping ahead or account."
        ],
        [
            "Take time series so this little butterfly represents a model developed by Lawrence in 1963.",
            "A free free variant model which is supposed to be a very crude approximation of some property of the atmosphere, and it's Celtic because a little perturbation grows exponentially in time as you iterate, which is not.",
            "Of course it doesn't happen in linear linear models.",
            "So yes, coming back to real world data.",
            "The reason why we were trying to develop a new algorithm is simply because I notice it working on Chaotic Time series.",
            "That's very often the latent states that explain the observed variables or have more dimensions.",
            "So typically if allowance data set comes from free variot data set with some nonlinear couplings between components, so it's a free variant system of three equations nonlinear.",
            "But you observe only one variable.",
            "Or, in the case of human motion, capture the markers that are recorded by cameras actually derive from many more joint angles or physiological properties of a body that is move."
        ],
        [
            "So coming back to the factor graph formalism.",
            "So as I said, we have an N dimensional observed variable and dimensional latent variable Z.",
            "And.",
            "The observation model is a sum of a deterministic part, the function G. A function of a latent state and some noise.",
            "I work here in Africa and most common approximation is space models.",
            "Is that the noise is Gaussian and I will even more crude approximation?",
            "Is that at the covariance matrix is fixed and the same goes for the latent States and with such a dynamic function F. So here I'm just showing a first order Markov dynamic.",
            "Corresponding to that factor graph.",
            "However, the second innovation of our research consistent allowing any kind of.",
            "D folder P folder Markovian dynamics.",
            "With a latent state, depending on a full sequence of previous latent States and also possibly on the previous sequence of latent observations.",
            "And I'm finally going closing this introduction with third innovation, which consists in the nature of the non non linear factors that you're using.",
            "So I talked about hidden Markov models, but also so called linear dynamical systems such as common filters or nonlinear dynamical systems such as extended common filters, particle filters.",
            "More recently Eric one Zoom in Germany introduced Nonny Nonny identical systems with radial basis functions modeling the nonlinearity or single hidden layer perceptrons.",
            "This is the work by Ilene.",
            "However, those dynamics do not necessarily enable.",
            "Complex nonlinearities that can be enabled by deep networks, where there are several consecutive layers, each with its own linearity and which unable to in some way to factorize, to give a hierarchical structure to the nonlinearity."
        ],
        [
            "So I found this reason.",
            "I use convolutional networks, which are the typical embodiment of a deep architecture.",
            "Multi layer and which also has property of having convolutional kernels.",
            "That means that in the snow network will many less many fewer, far fewer real free parameters than connections, because the connections are replicated, the weights or shared.",
            "This has been inspired by the way the human cortex visual cortex works.",
            "So typically so if we consider the conversion network using in our research, if we have a pattern of 11 time points of latent States and end dimension and components.",
            "The first layer consists simply in 12 finite impulse response filters of three points.",
            "That's but learn to do very simple 3 point operations, 3 point statistics, something like a gradient or.",
            "Such an and then the second layer.",
            "I also has three point operators, but it looks also atovi filters and all the components and the third layer is fully interconnected, apologize."
        ],
        [
            "Um?",
            "So now.",
            "Since we have explained the motivation and what's knew, I'm going to give explanations on how everything works, so I said that we forgo the probabilistic framework and we simply rely on energy based framework, which is just another fancy word for error based optimization.",
            "So we have you can see the observed variables Y and observe variable Z in time.",
            "So just two time steps, T -- 1 and T. V latent state generates we observe variable through a two component, two component factor.",
            "The first one is deterministic.",
            "It's this function generated function G and the second one is the error between the approximated of variable Y star and the actual observed variable wine.",
            "And it's the same factor for every single time step, and we call this error the observation energy.",
            "Then another factor models with dynamics.",
            "So again, we have a deterministic function F of past latent States and past latent observation observed variables.",
            "And we have again an approximation Z star of the current value Z of a latent variable.",
            "So have we can basically imagine this as two as two different sets of Springs linking that butter webling linking some other wobbling latent States and all we look for is simply to stabilize everything."
        ],
        [
            "So there's that over this debacle.",
            "Spring represents with dental energy.",
            "And we have of course two different sets of parameters for observations and dynamics.",
            "So how do we infer?",
            "Assuming that we clump the values of the parameters?",
            "We simply try to minimize the dynamical energy and the observation energy at each factor.",
            "However, since we use factor graphs factor graphs, I'm coming back."
        ],
        [
            "Factor graphs are a way to express the joint probability of latent and observed variables in a graph.",
            "As a product of probabilities at each factor.",
            "If we go to the negative, even if you go to the lock space of negative log space, this product becomes a sum.",
            "And so instead of maximizing the product of abilities, we Max minimize the sum of energies."
        ],
        [
            "So here we have energy at two different kinds of factors in."
        ],
        [
            "Factors.",
            "And we try to minimize this with respect to the latent state.",
            "And the total energy assuming that we have clamped the parameters is basically the sum of those energies.",
            "And so this also basically shows the inference algorithm once we have trained the model.",
            "And once we and when we were testing it on some testing data.",
            "Now, if you want to train the model."
        ],
        [
            "We have to look at the full sequence of latent states.",
            "And we have to, we want to clamp the values of little states, treat them as observed variables and minimize the error respect to V parameters.",
            "We are and so we have this loss function which is there some of us damico and observation errors plus a regularization term.",
            "We want as usual, and also some additional constraints on the latent states.",
            "Actually well, this constraints intervene in the inference not in V loss function.",
            "But anyway.",
            "The way our algorithm works is by an approximate deterministic approximation of a grade of gradient based of expectation maximization by gradient based techniques we iteratively alternate.",
            "An expectation step.",
            "Where we infer latent variables and maximization step, where we learn parameters."
        ],
        [
            "I mentioned that we are using also some constraints on valid states, so one of them.",
            "This is the smoothness penalty, so I said that we may have more variable latent variables than observed variables, which makes the inference of latent variables and under constraint problem.",
            "Voter ways of many approaches to solve it with the two ones.",
            "What we tried is regular raising with parameters using an L1 norm to enforce sparsity in the parameters or to enforce a smoothness between consecutive values of the same component of latent states.",
            "So of course this is a bit counterintuitive to the notion of dynamics.",
            "However, since we have a heavy under constraint problem, we have a huge search space and if we at the same time try to learn a dynamic.",
            "On village and States and have the considered in states to be as continues as possible, we will end up with latent states which are which have some dynamical evolution but without high frequency noise.",
            "So perfect list rationes that were first thought."
        ],
        [
            "Data problem.",
            "With asynchronous sine waves.",
            "So here we have a mixture of different sources which are five different sinusoids with frequencies which are not multiple of each of each other.",
            "And we only observe some.",
            "And by using five small, independent final final Impulse response filters, I managed to obtain on each latent state.",
            "A perfect sinusoid.",
            "Which is actually the best possible approximation of the problem.",
            "The best representation in a way.",
            "I did a blind source decomposition by blind source separation.",
            "Um?"
        ],
        [
            "Coming back to the Lorentz attractor.",
            "So this is this is an example of this nonlinear dynamical system with and you have this product of Y1Y3 which is nonlinear.",
            "Again, I just instead of using free variables as my observation, just use a projection of a free of them.",
            "Set bash observation.",
            "But using convolutional networks and enforcing smoothness, I reconstruct on my free latent states.",
            "So it's true that I chose because I had some hint side to use free Latin states, but I reconsidered my threaten states, something that actually looks like the butterfly attractor of valence states loans, dynamical system, and with a very similar correlation dimension.",
            "And Moreover, I obtain a smaller one step prediction error than the best solution obtained using super vect."
        ],
        [
            "Regression.",
            "And the cats benchmarked.",
            "I'm serious competition.",
            "I just took the data from the 2004 competition which consisted in predicting 100 values.",
            "Miss 100 missing values out of a 5000 point time series.",
            "So here we actually we could use not only the past but also the future of a time series to make a prediction.",
            "And again I managed to obtain the best to obtain better results.",
            "Vendor best results at the time of competition."
        ],
        [
            "But maybe what's the most nice to look at is what we did on human motion capture.",
            "On markers, so I took a data set used in the.",
            "Byron Taylor and Jeff Hinton from Toronto using restricted Boltzmann machines so very similar method, but just with a different philosophy.",
            "Where they had 49 dimensional human motion map markers.",
            "Who motion capture markers?",
            "Free Time Series 2 for training, one for testing and on the testing time series of 260 time points.",
            "Just like them, I deleted on half of a sequence.",
            "Ivar, the entire upper body, or just the entire left leg.",
            "And the point of the goal was to reconstruct the upper body or the left leg.",
            "From the other limbs and from the dynamics which were learned on the training datasets."
        ],
        [
            "So.",
            "I want going.",
            "I'm going to show you the video.",
            "Oops.",
            "So let's look 1st at the testing data set.",
            "And in Red now you can read.",
            "You can see what's going on, what is going to be deleted.",
            "So I show actually both the upper upper body and the left leg.",
            "So you see how they?",
            "The skeleton moves.",
            "OK. And let's look at the reconstruction.",
            "With a missing left leg.",
            "So let's look at live so you see that the left leg seems almost normal.",
            "Envy upper building.",
            "Again, the arms are moving and it's smooth and continuous.",
            "So the interest of this technique is that currently when we are missing, motion can often hear from 5 minutes perfect when we are missing markers in motion.",
            "Capture data.",
            "Will people working on motion capture do?",
            "Is that we try to do a inference of missing markers just by looking at nearest most similar looking at nearest neighbors, which means they try to find.",
            "In Nona data, where there are no missing markers and just based on the comparison with the non missing markers, the most similar pattern.",
            "But it doesn't take into account the dynamics.",
            "And this is how it would look like.",
            "So you see, that's the dynamics, or in a way important.",
            "Because otherwise we tend to obtain a.",
            "Very hockey.",
            "Animation.",
            "So I think I can skip the like I can still show what the upper body would look like.",
            "So I don't know if.",
            "Yes, so in the boy.",
            "In terms of a row estimate of the error, our normalized mean squared error is better.",
            "Of course smaller than if we just use nearest neighbors.",
            "But in addition, we've seen that the motion is more realistic.",
            "Why should those slides in case the video didn't work?",
            "Which is always useful."
        ],
        [
            "But we.",
            "It's relatively hard to spot the difference, but we can see that of course I introduced some error in the in the way the skeleton moves because of accumulated smart united errors.",
            "However, the the left leg and the upper body still look like the original."
        ],
        [
            "Everything goes for the second subsequence.",
            "Original.",
            "Left upper body.",
            "And left leg and there is a little bit of a Michael Jackson like move left leg.",
            "It's still OK. Well, I won't say that the leg doesn't always touch for the ground the feeble.",
            "It's not my problem."
        ],
        [
            "So I think I can say thank you much and."
        ],
        [
            "You can check the list of references also in the paper.",
            "This post has actually already been shown yesterday, but I will be very happy to talk, talk, talk with you about it after this presentation or tomorrow morning.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everybody, thank you much for coming to the first talk of the office afternoon session.",
                    "label": 0
                },
                {
                    "sent": "My name is Piotr Mirowski and I am currently close to the completion of my PhD in computer science at the Courant Institute of Mathematical Sciences of New York University, and my advisor is young local who is one of the early founders of modern known networks such as convolutional networks, widely used.",
                    "label": 1
                },
                {
                    "sent": "And this research is a more theoretical part of my PhD, focused on the modeling of time series using highly nonlinear dynamics, explaining the time series.",
                    "label": 0
                },
                {
                    "sent": "So I called this dynamical factor graph.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reason why I resorted to performance factor graphs instead of more commonly used graphical models or Damico Bayesian networks is that you actually were going to sacrifice most of a probabilistic part of the state space models.",
                    "label": 0
                },
                {
                    "sent": "So is this based model is simply a way to link unobserved variables Y?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To some hidden latent.",
                    "label": 0
                },
                {
                    "sent": "And observed variables Z.",
                    "label": 0
                },
                {
                    "sent": "Which themselves are governed by dynamics.",
                    "label": 0
                },
                {
                    "sent": "And of course we did not know initially, neither of a configuration of a latent variables, which is a pure hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We don't even know the number of dimensions and we do not know.",
                    "label": 0
                },
                {
                    "sent": "Of course we dynamical model and the observation model which enables to generate observed variables.",
                    "label": 1
                },
                {
                    "sent": "Why from an absurd variables Z?",
                    "label": 0
                },
                {
                    "sent": "So we are facing lots of unknowns.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, contrary to a hidden Markov model where the sequence of latent states is discrete, we want to have continuously continuously value with latent states.",
                    "label": 0
                },
                {
                    "sent": "And also with as many dimensions as possible as required by our problem.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we can see that.",
                    "label": 0
                },
                {
                    "sent": "The boat circles representing the state state variables are.",
                    "label": 0
                },
                {
                    "sent": "Highly dimensional and continuous values.",
                    "label": 0
                },
                {
                    "sent": "Also, we want to enable nonlinear dynamics.",
                    "label": 0
                },
                {
                    "sent": "Linking the latent States and possibly also explaining the generative model going from latent states to upset variables.",
                    "label": 0
                },
                {
                    "sent": "Yes, so just I just forgot to maybe add that in a factor graph the.",
                    "label": 1
                },
                {
                    "sent": "The circles are present variables and Viva little squares presented factors which model the conditional dependencies between subsets of variables.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So and only a linear dynamic is for instance a simple matrix that encodes a rotation.",
                    "label": 0
                },
                {
                    "sent": "You have an M dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And you try you multiple, you multiply this vector by a matrix.",
                    "label": 0
                },
                {
                    "sent": "And what you obtain most probably is a rotation with some scaling.",
                    "label": 0
                },
                {
                    "sent": "If you have another dynamic that can suppose pretty much anything, for instance, that coupling between two elements of your of your.",
                    "label": 0
                },
                {
                    "sent": "Input vector or maybe a sequence of consecutive transformations with some nonlinearities and?",
                    "label": 1
                },
                {
                    "sent": "Maybe a blessing destruction?",
                    "label": 0
                },
                {
                    "sent": "Actually I'm skipping ahead or account.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take time series so this little butterfly represents a model developed by Lawrence in 1963.",
                    "label": 0
                },
                {
                    "sent": "A free free variant model which is supposed to be a very crude approximation of some property of the atmosphere, and it's Celtic because a little perturbation grows exponentially in time as you iterate, which is not.",
                    "label": 0
                },
                {
                    "sent": "Of course it doesn't happen in linear linear models.",
                    "label": 0
                },
                {
                    "sent": "So yes, coming back to real world data.",
                    "label": 0
                },
                {
                    "sent": "The reason why we were trying to develop a new algorithm is simply because I notice it working on Chaotic Time series.",
                    "label": 0
                },
                {
                    "sent": "That's very often the latent states that explain the observed variables or have more dimensions.",
                    "label": 0
                },
                {
                    "sent": "So typically if allowance data set comes from free variot data set with some nonlinear couplings between components, so it's a free variant system of three equations nonlinear.",
                    "label": 0
                },
                {
                    "sent": "But you observe only one variable.",
                    "label": 0
                },
                {
                    "sent": "Or, in the case of human motion, capture the markers that are recorded by cameras actually derive from many more joint angles or physiological properties of a body that is move.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So coming back to the factor graph formalism.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we have an N dimensional observed variable and dimensional latent variable Z.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The observation model is a sum of a deterministic part, the function G. A function of a latent state and some noise.",
                    "label": 0
                },
                {
                    "sent": "I work here in Africa and most common approximation is space models.",
                    "label": 0
                },
                {
                    "sent": "Is that the noise is Gaussian and I will even more crude approximation?",
                    "label": 0
                },
                {
                    "sent": "Is that at the covariance matrix is fixed and the same goes for the latent States and with such a dynamic function F. So here I'm just showing a first order Markov dynamic.",
                    "label": 1
                },
                {
                    "sent": "Corresponding to that factor graph.",
                    "label": 0
                },
                {
                    "sent": "However, the second innovation of our research consistent allowing any kind of.",
                    "label": 0
                },
                {
                    "sent": "D folder P folder Markovian dynamics.",
                    "label": 0
                },
                {
                    "sent": "With a latent state, depending on a full sequence of previous latent States and also possibly on the previous sequence of latent observations.",
                    "label": 0
                },
                {
                    "sent": "And I'm finally going closing this introduction with third innovation, which consists in the nature of the non non linear factors that you're using.",
                    "label": 0
                },
                {
                    "sent": "So I talked about hidden Markov models, but also so called linear dynamical systems such as common filters or nonlinear dynamical systems such as extended common filters, particle filters.",
                    "label": 0
                },
                {
                    "sent": "More recently Eric one Zoom in Germany introduced Nonny Nonny identical systems with radial basis functions modeling the nonlinearity or single hidden layer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "This is the work by Ilene.",
                    "label": 0
                },
                {
                    "sent": "However, those dynamics do not necessarily enable.",
                    "label": 0
                },
                {
                    "sent": "Complex nonlinearities that can be enabled by deep networks, where there are several consecutive layers, each with its own linearity and which unable to in some way to factorize, to give a hierarchical structure to the nonlinearity.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I found this reason.",
                    "label": 0
                },
                {
                    "sent": "I use convolutional networks, which are the typical embodiment of a deep architecture.",
                    "label": 0
                },
                {
                    "sent": "Multi layer and which also has property of having convolutional kernels.",
                    "label": 0
                },
                {
                    "sent": "That means that in the snow network will many less many fewer, far fewer real free parameters than connections, because the connections are replicated, the weights or shared.",
                    "label": 0
                },
                {
                    "sent": "This has been inspired by the way the human cortex visual cortex works.",
                    "label": 0
                },
                {
                    "sent": "So typically so if we consider the conversion network using in our research, if we have a pattern of 11 time points of latent States and end dimension and components.",
                    "label": 0
                },
                {
                    "sent": "The first layer consists simply in 12 finite impulse response filters of three points.",
                    "label": 0
                },
                {
                    "sent": "That's but learn to do very simple 3 point operations, 3 point statistics, something like a gradient or.",
                    "label": 0
                },
                {
                    "sent": "Such an and then the second layer.",
                    "label": 0
                },
                {
                    "sent": "I also has three point operators, but it looks also atovi filters and all the components and the third layer is fully interconnected, apologize.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Since we have explained the motivation and what's knew, I'm going to give explanations on how everything works, so I said that we forgo the probabilistic framework and we simply rely on energy based framework, which is just another fancy word for error based optimization.",
                    "label": 0
                },
                {
                    "sent": "So we have you can see the observed variables Y and observe variable Z in time.",
                    "label": 0
                },
                {
                    "sent": "So just two time steps, T -- 1 and T. V latent state generates we observe variable through a two component, two component factor.",
                    "label": 0
                },
                {
                    "sent": "The first one is deterministic.",
                    "label": 0
                },
                {
                    "sent": "It's this function generated function G and the second one is the error between the approximated of variable Y star and the actual observed variable wine.",
                    "label": 0
                },
                {
                    "sent": "And it's the same factor for every single time step, and we call this error the observation energy.",
                    "label": 1
                },
                {
                    "sent": "Then another factor models with dynamics.",
                    "label": 0
                },
                {
                    "sent": "So again, we have a deterministic function F of past latent States and past latent observation observed variables.",
                    "label": 0
                },
                {
                    "sent": "And we have again an approximation Z star of the current value Z of a latent variable.",
                    "label": 1
                },
                {
                    "sent": "So have we can basically imagine this as two as two different sets of Springs linking that butter webling linking some other wobbling latent States and all we look for is simply to stabilize everything.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's that over this debacle.",
                    "label": 0
                },
                {
                    "sent": "Spring represents with dental energy.",
                    "label": 0
                },
                {
                    "sent": "And we have of course two different sets of parameters for observations and dynamics.",
                    "label": 0
                },
                {
                    "sent": "So how do we infer?",
                    "label": 0
                },
                {
                    "sent": "Assuming that we clump the values of the parameters?",
                    "label": 1
                },
                {
                    "sent": "We simply try to minimize the dynamical energy and the observation energy at each factor.",
                    "label": 1
                },
                {
                    "sent": "However, since we use factor graphs factor graphs, I'm coming back.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factor graphs are a way to express the joint probability of latent and observed variables in a graph.",
                    "label": 0
                },
                {
                    "sent": "As a product of probabilities at each factor.",
                    "label": 0
                },
                {
                    "sent": "If we go to the negative, even if you go to the lock space of negative log space, this product becomes a sum.",
                    "label": 0
                },
                {
                    "sent": "And so instead of maximizing the product of abilities, we Max minimize the sum of energies.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have energy at two different kinds of factors in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factors.",
                    "label": 0
                },
                {
                    "sent": "And we try to minimize this with respect to the latent state.",
                    "label": 0
                },
                {
                    "sent": "And the total energy assuming that we have clamped the parameters is basically the sum of those energies.",
                    "label": 0
                },
                {
                    "sent": "And so this also basically shows the inference algorithm once we have trained the model.",
                    "label": 0
                },
                {
                    "sent": "And once we and when we were testing it on some testing data.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to train the model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have to look at the full sequence of latent states.",
                    "label": 0
                },
                {
                    "sent": "And we have to, we want to clamp the values of little states, treat them as observed variables and minimize the error respect to V parameters.",
                    "label": 0
                },
                {
                    "sent": "We are and so we have this loss function which is there some of us damico and observation errors plus a regularization term.",
                    "label": 0
                },
                {
                    "sent": "We want as usual, and also some additional constraints on the latent states.",
                    "label": 0
                },
                {
                    "sent": "Actually well, this constraints intervene in the inference not in V loss function.",
                    "label": 0
                },
                {
                    "sent": "But anyway.",
                    "label": 0
                },
                {
                    "sent": "The way our algorithm works is by an approximate deterministic approximation of a grade of gradient based of expectation maximization by gradient based techniques we iteratively alternate.",
                    "label": 0
                },
                {
                    "sent": "An expectation step.",
                    "label": 0
                },
                {
                    "sent": "Where we infer latent variables and maximization step, where we learn parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mentioned that we are using also some constraints on valid states, so one of them.",
                    "label": 0
                },
                {
                    "sent": "This is the smoothness penalty, so I said that we may have more variable latent variables than observed variables, which makes the inference of latent variables and under constraint problem.",
                    "label": 1
                },
                {
                    "sent": "Voter ways of many approaches to solve it with the two ones.",
                    "label": 0
                },
                {
                    "sent": "What we tried is regular raising with parameters using an L1 norm to enforce sparsity in the parameters or to enforce a smoothness between consecutive values of the same component of latent states.",
                    "label": 0
                },
                {
                    "sent": "So of course this is a bit counterintuitive to the notion of dynamics.",
                    "label": 0
                },
                {
                    "sent": "However, since we have a heavy under constraint problem, we have a huge search space and if we at the same time try to learn a dynamic.",
                    "label": 0
                },
                {
                    "sent": "On village and States and have the considered in states to be as continues as possible, we will end up with latent states which are which have some dynamical evolution but without high frequency noise.",
                    "label": 0
                },
                {
                    "sent": "So perfect list rationes that were first thought.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data problem.",
                    "label": 0
                },
                {
                    "sent": "With asynchronous sine waves.",
                    "label": 0
                },
                {
                    "sent": "So here we have a mixture of different sources which are five different sinusoids with frequencies which are not multiple of each of each other.",
                    "label": 0
                },
                {
                    "sent": "And we only observe some.",
                    "label": 0
                },
                {
                    "sent": "And by using five small, independent final final Impulse response filters, I managed to obtain on each latent state.",
                    "label": 0
                },
                {
                    "sent": "A perfect sinusoid.",
                    "label": 0
                },
                {
                    "sent": "Which is actually the best possible approximation of the problem.",
                    "label": 0
                },
                {
                    "sent": "The best representation in a way.",
                    "label": 0
                },
                {
                    "sent": "I did a blind source decomposition by blind source separation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming back to the Lorentz attractor.",
                    "label": 0
                },
                {
                    "sent": "So this is this is an example of this nonlinear dynamical system with and you have this product of Y1Y3 which is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Again, I just instead of using free variables as my observation, just use a projection of a free of them.",
                    "label": 0
                },
                {
                    "sent": "Set bash observation.",
                    "label": 0
                },
                {
                    "sent": "But using convolutional networks and enforcing smoothness, I reconstruct on my free latent states.",
                    "label": 0
                },
                {
                    "sent": "So it's true that I chose because I had some hint side to use free Latin states, but I reconsidered my threaten states, something that actually looks like the butterfly attractor of valence states loans, dynamical system, and with a very similar correlation dimension.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, I obtain a smaller one step prediction error than the best solution obtained using super vect.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regression.",
                    "label": 0
                },
                {
                    "sent": "And the cats benchmarked.",
                    "label": 0
                },
                {
                    "sent": "I'm serious competition.",
                    "label": 0
                },
                {
                    "sent": "I just took the data from the 2004 competition which consisted in predicting 100 values.",
                    "label": 0
                },
                {
                    "sent": "Miss 100 missing values out of a 5000 point time series.",
                    "label": 1
                },
                {
                    "sent": "So here we actually we could use not only the past but also the future of a time series to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "And again I managed to obtain the best to obtain better results.",
                    "label": 0
                },
                {
                    "sent": "Vendor best results at the time of competition.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But maybe what's the most nice to look at is what we did on human motion capture.",
                    "label": 1
                },
                {
                    "sent": "On markers, so I took a data set used in the.",
                    "label": 0
                },
                {
                    "sent": "Byron Taylor and Jeff Hinton from Toronto using restricted Boltzmann machines so very similar method, but just with a different philosophy.",
                    "label": 0
                },
                {
                    "sent": "Where they had 49 dimensional human motion map markers.",
                    "label": 0
                },
                {
                    "sent": "Who motion capture markers?",
                    "label": 0
                },
                {
                    "sent": "Free Time Series 2 for training, one for testing and on the testing time series of 260 time points.",
                    "label": 0
                },
                {
                    "sent": "Just like them, I deleted on half of a sequence.",
                    "label": 0
                },
                {
                    "sent": "Ivar, the entire upper body, or just the entire left leg.",
                    "label": 1
                },
                {
                    "sent": "And the point of the goal was to reconstruct the upper body or the left leg.",
                    "label": 0
                },
                {
                    "sent": "From the other limbs and from the dynamics which were learned on the training datasets.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I want going.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you the video.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "So let's look 1st at the testing data set.",
                    "label": 0
                },
                {
                    "sent": "And in Red now you can read.",
                    "label": 0
                },
                {
                    "sent": "You can see what's going on, what is going to be deleted.",
                    "label": 0
                },
                {
                    "sent": "So I show actually both the upper upper body and the left leg.",
                    "label": 1
                },
                {
                    "sent": "So you see how they?",
                    "label": 0
                },
                {
                    "sent": "The skeleton moves.",
                    "label": 0
                },
                {
                    "sent": "OK. And let's look at the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "With a missing left leg.",
                    "label": 1
                },
                {
                    "sent": "So let's look at live so you see that the left leg seems almost normal.",
                    "label": 0
                },
                {
                    "sent": "Envy upper building.",
                    "label": 0
                },
                {
                    "sent": "Again, the arms are moving and it's smooth and continuous.",
                    "label": 0
                },
                {
                    "sent": "So the interest of this technique is that currently when we are missing, motion can often hear from 5 minutes perfect when we are missing markers in motion.",
                    "label": 0
                },
                {
                    "sent": "Capture data.",
                    "label": 0
                },
                {
                    "sent": "Will people working on motion capture do?",
                    "label": 0
                },
                {
                    "sent": "Is that we try to do a inference of missing markers just by looking at nearest most similar looking at nearest neighbors, which means they try to find.",
                    "label": 0
                },
                {
                    "sent": "In Nona data, where there are no missing markers and just based on the comparison with the non missing markers, the most similar pattern.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't take into account the dynamics.",
                    "label": 0
                },
                {
                    "sent": "And this is how it would look like.",
                    "label": 0
                },
                {
                    "sent": "So you see, that's the dynamics, or in a way important.",
                    "label": 0
                },
                {
                    "sent": "Because otherwise we tend to obtain a.",
                    "label": 0
                },
                {
                    "sent": "Very hockey.",
                    "label": 0
                },
                {
                    "sent": "Animation.",
                    "label": 0
                },
                {
                    "sent": "So I think I can skip the like I can still show what the upper body would look like.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if.",
                    "label": 0
                },
                {
                    "sent": "Yes, so in the boy.",
                    "label": 0
                },
                {
                    "sent": "In terms of a row estimate of the error, our normalized mean squared error is better.",
                    "label": 1
                },
                {
                    "sent": "Of course smaller than if we just use nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "But in addition, we've seen that the motion is more realistic.",
                    "label": 0
                },
                {
                    "sent": "Why should those slides in case the video didn't work?",
                    "label": 0
                },
                {
                    "sent": "Which is always useful.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we.",
                    "label": 0
                },
                {
                    "sent": "It's relatively hard to spot the difference, but we can see that of course I introduced some error in the in the way the skeleton moves because of accumulated smart united errors.",
                    "label": 0
                },
                {
                    "sent": "However, the the left leg and the upper body still look like the original.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything goes for the second subsequence.",
                    "label": 0
                },
                {
                    "sent": "Original.",
                    "label": 0
                },
                {
                    "sent": "Left upper body.",
                    "label": 0
                },
                {
                    "sent": "And left leg and there is a little bit of a Michael Jackson like move left leg.",
                    "label": 0
                },
                {
                    "sent": "It's still OK. Well, I won't say that the leg doesn't always touch for the ground the feeble.",
                    "label": 0
                },
                {
                    "sent": "It's not my problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I can say thank you much and.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can check the list of references also in the paper.",
                    "label": 0
                },
                {
                    "sent": "This post has actually already been shown yesterday, but I will be very happy to talk, talk, talk with you about it after this presentation or tomorrow morning.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}