{
    "id": "kxwlmmlkr5h6jn32zhwu4fglpmj4cnah",
    "title": "Deep Learning",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Department of Statistical Sciences, University of Toronto"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_salakhutdinov_deep_learning/",
    "segmentation": [
        [
            "So thanks for coming.",
            "Coming here on Saturday.",
            "I just want to point out also that for students here who want to do particular people who want to do PhD, I'm.",
            "Going to be moving to CMU and building my group there so you know, talk to me if you're if you're interested.",
            "OK so I was thinking about what I should be talking about in this summer school and what I'm going to do is the first part of the lecture.",
            "I'll talk about some of the older work that my lab has been doing, but a little bit more technical, and then the second part you will see some of the recent work that's been happening in my lab as well as in other labs, so you'll get to see a little bit of recent exciting projects, OK?"
        ],
        [
            "So first of all, I would like to think a lot of my students a lot of work that I'm going to be talking about here is really the work of my fantastic colleagues, some of whom are already here.",
            "And I'll point out different pieces of what students are doing.",
            "OK, so let me start by sort of you've probably seen this before."
        ],
        [
            "You know, if you've seen the datasets that we have right, it's it's.",
            "It's amazing and I think that what we're trying to do is when I'm going to be talking about mostly in this tutorial is I'm going to be talking about unsupervised learning.",
            "So this is the case where you know, for images of for taxes, very easy to get these datasets, but it's somehow it's often very hard to get labels right.",
            "It's very hard to get people to annotate what's going on in the images, so how can we develop models?",
            "That I capable of discovering, structuring, how we can do it in unsupervised or semi supervised way.",
            "And as well as how can we apply these models in a lot of different application domains as being in the machine learning field?",
            "It's very exciting to see how we can develop systems that work across multiple domains.",
            "And what I would argue is that one particular framework for doing this."
        ],
        [
            "Is is learning these hierarchical models these deep learning models or models that discover structured multiple at multiple levels and I'm going to make it a little bit more precise as I go through the talk."
        ],
        [
            "Here's one example.",
            "This is an older example.",
            "It goes back to 2006, where if you take bag of words representation something very simple.",
            "Can you take a Reuters data set which is about 800,000 news stories?",
            "These are.",
            "Web pages on the Reuters news feeds an nobody tells you about what this story is about, right?",
            "Nobody tells you that what the label for this story is, and if you just project these bag of words representation.",
            "So these stories into 2 dimensional space.",
            "This is what the model is discovering.",
            "Right, so it's sort of finds interesting structure in the data, so you know you can visualize what's going on in the data.",
            "You can find similar things this was done.",
            "This data set was collected back in 96 and it's kind of interesting that it puts European economic policies next to disasters and accidents, right?",
            "So?",
            "So I guess in today's in today's world it's you know, these two spikes are going to be even closer, right?",
            "So here is an A."
        ],
        [
            "Example, right?",
            "So what you can do?",
            "You know these are the systems where you can take these images and you can say can you tag these images?",
            "Can you see what's going on in those images or the other way around?",
            "Given the word, can you find similar images right?",
            "And then you know you can go a little."
        ],
        [
            "Beyond that, you can say can you build a system that actually has some understanding of what's going on in the images?",
            "So here's a here's a picture of Antonio Torralba.",
            "Do you know who Antonio Torralba is?",
            "He's a Prophet.",
            "MIT is a vision guy.",
            "You can say, well, can you tag this person?",
            "And this is a picture.",
            "Took him on my cell phone at Nips.",
            "Which was in Montreal, which is actually here, so you know if you ask the system to tag it.",
            "This is what.",
            "No, the tags you provide, which is sort of reasonable, right?",
            "Sort of sees a lot of people, but then you can say, well, can I now build a system that actually describes in natural language what's going on in this image, right?",
            "And one way of doing this?",
            "I think the cheating way of doing this would be to, say find a similar image in your training set and copy the caption or copy the description.",
            "And this is what the system does.",
            "People taking pictures of a crazy person.",
            "So this is true.",
            "I we have an app on the phone, you can do it and I'm telling you really liked it.",
            "Actually, you know that that description could probably go along with a lot of different pictures, right?",
            "But then you can also build the model and say, can you actually build a model, generates the sentences and this is what the system does.",
            "A group of people in a crowded area, group of people that are working in talking a group of people that are in the outside.",
            "So obviously here you can see syntactically it's kind of a weird sentence, but you can see that you know these systems are doing reasonably well in these samples from the model.",
            "And I'll talk about that particular model in the second half of the tutorial, but I just wanted to give you a little bit of a preview."
        ],
        [
            "Here's just wanted to show you.",
            "Here's what these kinds of systems can do, which is quite.",
            "Remarkable, you know, like a car is parked in the middle of nowhere or a little boy with a bunch of friends on the street where there's a cat sitting on the shelf.",
            "You know these look a little bit too good, right?",
            "If you look at these things just way too good, then second half of this tutorial I'm going to show you some failure examples and you will see that you know these systems are not on par with human performance yet.",
            "OK, so."
        ],
        [
            "So in the first part of the tutorial, I'm going to give you some basics on Boltzmann machines.",
            "In particular, I'm going to talk about restricted.",
            "Both machines did both machines, and then I'm going to focus on couple of topics.",
            "In particular, I'm going to talk about multimodal, debossed machine is 1 particular application of these models.",
            "An in the last part of the tutorial I'm going to talk about how we can evaluate these models.",
            "And that will get a little bit more technical, but I'm hoping that I'll give you some flavor of what we can do there, yeah.",
            "Perfect.",
            "Perfect, that's that's great, so I'm going to go fast on that, but let me first start with the following.",
            "You've probably seen this, but I can just go very quickly."
        ],
        [
            "About the notion of learning representations right, let's say I'm trying to classify that image.",
            "If I take these two pixels and I say, well, based on these two pixels, can I classify what's going on here?",
            "It's very hard right?",
            "So a lot of us in particular application domains we try to come up with the right representations, right?",
            "If you can, basically."
        ],
        [
            "Figure out there is a will and there is a handle and that's the representation of features.",
            "Then you can build a reasonable classifier, right?",
            "So if you look at a lot of applications that make speech vision, that box really matters the most.",
            "How do you find these representations right and you know?"
        ],
        [
            "If you look at sort of traditional approaches data, you do some feature extraction.",
            "You apply your favorite learning algorithm right, and that's a paradigm.",
            "Probably some of you have worked with.",
            "You know, in object detection these hog features then you recognizing what that is and classification these MFC features and then you can recognize who that is right and the interest."
        ],
        [
            "Pieces that you know in the vision domain computer vision domain.",
            "If you look at the space of features, space is pretty big.",
            "You know people are trying to figure out where trying to figure out what's the right representations to use.",
            "Sift was a very influential work by David Lowe at UBC, which basically moved that field ahead for about a decade.",
            "Everybody were using Sift for doing recognition and it."
        ],
        [
            "Without that, you actually don't need that.",
            "Given enough data, you can learn what the right representations are.",
            "In fact, now in most of the computer vision domain people are using convolutional neural Nets to find what the right representations are."
        ],
        [
            "If you look at the speech, the same thing happens."
        ],
        [
            "There you basically don't need that.",
            "Seems that these approaches are able to learn what the right representations should be."
        ],
        [
            "So let me quickly go over restricted Boltzmann machines.",
            "You've seen this before, so I'm going to go very quickly.",
            "You can specify just just for notational convenience.",
            "V's here are the observed variables.",
            "These stochastic binary variables H is are the stochastic hidden variables.",
            "You can think of HSS detecting certain features, certain patterns that you see in the data, right?",
            "The conditionals here are given by the product of these marginals and effectively what this is saying is that if you tell me what features you see in the data, I can generate the image for you right?",
            "Because that defines that distribution here."
        ],
        [
            "So here is what you can learn with these things.",
            "If I give you these handwritten characters then I can learn these little.",
            "Our edge is in the way to think about these models.",
            "Is that if I give you a new image and these are sort of the right features that the model speaking up and these numbers here are given by the conditional probabilities of these hidden variables being active right?",
            "So it's just an intuition you should keep in the background.",
            "You know these models are probabilistic models, but if you know bout sparse coding, I see a lot of different models.",
            "They roughly trying to do the same thing."
        ],
        [
            "So how do we do learning in these models?",
            "I can specify the model, the probability which is given by this expression, and you've seen that from the previous classes.",
            "I assume you can maximize the likelihood objective.",
            "It's a natural thing to do, and if you take the derivative of the log likelihood, then you have you do a little bit of math.",
            "It's not that difficult, but it comes down to being the difference between these two expected sufficient statistics.",
            "So this is expected.",
            "Sufficient statistics driven by the data.",
            "And it turns out that this thing you can compute exactly, and the reason why I can compute exactly is because.",
            "Of particular structure of these models.",
            "Turns out that you know you can compute this conditional probability, right?",
            "So believe me, you know this term can be computed exactly.",
            "This is a more difficult term.",
            "Unfortunately, computing these term requires exponentially exponential sum, right?",
            "And the way you should think about this is, imagine the space of all possible handwritten characters you can generate, or the space of all possible images you can generate, right?",
            "You sort of have to.",
            "Do this sum of all possibilities, and that's an exponential sum.",
            "So a lot of work in the machine learning community in the statistics communities is trying to figure out how we can approximate these expectations efficiently."
        ],
        [
            "So this term is easy to compute, right?",
            "We can do that this term you have to sum over all possible configurations and that cannot.",
            "We cannot compute it and then typically there are approaches.",
            "Most of the successful approaches are basically using.",
            "MCMC methods methods like contrastive divergent persistent contrastive divergent and such, so you've seen you've seen those.",
            "In fact, I should point out here is that we've tried looking at some variational approximations here and unfortunately they don't work, or at least we couldn't get them to work.",
            "So MCMC is somehow is the most.",
            "You know empirically best way of fitting."
        ],
        [
            "These models.",
            "OK, but now the interesting thing about these models is they can extend them.",
            "You can extend them to dealing with real valued data, right?",
            "So if you change, you know the definition of the model A little bit.",
            "There's nothing intrinsically complex here.",
            "These are pairwise these unary connections, but in this case the conditional is given by the product of Gaussians, right?",
            "And that's useful when you're modeling real valued data.",
            "Um?",
            "So you still have stochastic binary hidden variables.",
            "You have stochastic real valued visible variables, and you can define the model."
        ],
        [
            "So what does it learn if you fit this model in 4 million unlabeled images?",
            "This is what the learn features look like.",
            "This is without using any convolutional type of architecture, but these are sort of filters that the model is learning."
        ],
        [
            "And when you think about this again, you can say, well, this is a new image and this new image is given by some combination of these filters.",
            "Right, yeah, there's a question.",
            "Note these are these are 32 by 32 tiny images, so these fit on the full full 32 by 32 images.",
            "Um?"
        ],
        [
            "You can also extend these models to dealing with count data.",
            "Right, so in this case, imagine that you have stochastic one of K variables here, so these are useful for modeling.",
            "For example, bag of words representation.",
            "Or if you want to deal with any type of bag of words representation, typically you know dealing with some kind of text.",
            "Idata so these are stochastic.",
            "One of the visible variables you can think of these variables as being vocabulary size.",
            "So in English you know maybe you're dealing with 50,100 thousand.",
            "OK here would be 100,000 dies.",
            "The number of words that's appearing in a particular document.",
            "In this case the conditional probability is given by this softmax distribution, right?",
            "So you can think of these models as undirected versions of topic models like latent judicial allocation.",
            "Very sort of similar type of argument.",
            "You can actually think of these as being multi normal random variables.",
            "Um?"
        ],
        [
            "And now you know if you look at what these models are doing.",
            "If you fit them to the Reuters data set and you look at these activations of hidden variables, they also discovering sort of pseudo topics.",
            "Right and the way you can think about what's happening here intuitively is that for every single document.",
            "Every single document is made up by some combination of these topics, right?",
            "So it's interesting because for images you say here's an image, and it's made up of these filters or these sort of edges.",
            "For documents it sort of finds topics as sort of a first level representation, so it's kind of interesting that in different domains you finding these interesting you finding this interesting structure."
        ],
        [
            "And so you can deal with binary with Gaussian with count type of data softmax.",
            "All of these models will have binary random variables, but they can use for modeling different kinds of data.",
            "In fact, there's a work by I think it was back in 2006 or 2007 by Max Swelling in his group, where they generalize these models to other members within exponential family.",
            "So if you're dealing with.",
            "Different data modalities.",
            "You can adjust these models.",
            "Yeah, there's a question.",
            "Yeah.",
            "Yeah, I'll come back to that.",
            "So the question is how many you know?",
            "We think about the number of hidden units you know.",
            "If you typically depends on the application.",
            "If you're interested in visualization or trying to visualize what's going on, or some compression, and you have smaller.",
            "In other cases, if you want to do retrieval or classification, you typically have overcomplete representation, so the number of hidden units is actually bigger than.",
            "The original size of the data.",
            "Yeah.",
            "Yeah, I'll get to that.",
            "I'll get to that.",
            "So in these models, one of the advantage of these models is that it's very easy to infer the states of the hidden variables, right can be done in closed form, so this is effectively telling you that if I give you a document, I can tell you which what topics you see in a document, right?",
            "Or if you give me an image, I can tell you what features you see in the image, right?",
            "And this is unlike some of the other kinds of models like.",
            "Directed graphical models.",
            "Bayesian models where given the data, we have to do a little bit of work to figure out what's the distribution over the latent variables, right?"
        ],
        [
            "Now sometimes these models are called product models called product of experts and the reason for that is if you look at the joint distribution of observed and hidden variables, has this form right?",
            "It's a log linear model.",
            "But if you moduli's over the hidden variables and you can do it explicitly in these models, you have this interesting form, so we have a something of the form product of bunch of things, right?",
            "So what does this mean?",
            "Let me give you an intuition as to what what, what is happening here.",
            "Suppose I'm dealing with the.",
            "Text data.",
            "And I'm finding these topics.",
            "That's what I'm discovering and suppose I tell you that in my document I'm seeing the topic government corruption and oil.",
            "Then the word Putin will have very high probability.",
            "Right?",
            "And this is, and that's what makes it different.",
            "It makes a big difference when you, when you're looking at models like for example topic models like Late English location models, right?",
            "So here you taking these distributions over words, you have three topics.",
            "And these are sort of broad distributions, but then you multiplying them together and re normalizing this is this exactly what you're doing here.",
            "So if you have three things in this case 3 topics and you multiplying them together right, you can imagine that the distribution is going to be spiky.",
            "And that distribution can model very precisely what kind of words you expecting to see in a document, right?",
            "Like you know, if I tell you that I'm seeing these three topics, you know, maybe Hugo Chavez will show up in here right?",
            "Compared to LDA type of models, because in traditional topic models, the way you generating the data is you picking a topic, you generating a word, you picking another topic, you generating a word right?",
            "So these admixture models you can think of them as mixture models, just which is very different from these models.",
            "So, so the claim here is that these models can more precisely model.",
            "Model the data an in fact, if you're using these models for doing retrieval, can you find similar documents?",
            "You know these models work much better than traditional topic models."
        ],
        [
            "OK, so now let me step back a little bit.",
            "Yep, there's a question.",
            "Yeah, it depends.",
            "It depends on the implementation.",
            "Depends on the implementation.",
            "These models, the GBM models.",
            "Maybe training at training time.",
            "They little bit tougher to train.",
            "That's true, but at the same time if you look at, you know other models like latent semantic analysis, just sort of or.",
            "Yeah.",
            "I guess it depends on the implementation.",
            "Can make the very fast implementation of these models.",
            "You can also make very fast implementation of LDA models and such.",
            "One advantage of LDA models is that sometimes it gives you more interpretable topics and people like that.",
            "Sometimes the topics that you get in these undirected models don't carry the interpretability.",
            "Right, because you can imagine that each single hidden unit sort of defines a broad distribution of awards, and it's the intersection of these distributions that define a particular topic.",
            "Right?",
            "OK."
        ],
        [
            "So let me.",
            "Let me jump into a divorce machine model.",
            "And maybe give you a little bit of intuition how we can learn these models OK?",
            "So."
        ],
        [
            "I've shown you that if we actually learning, you know these PBM type of models.",
            "They can capture certain low level structure, right?",
            "You can capture these little edges, or if we're looking at bag of words, we typically capturing some correlations between words, right?",
            "But the results obviously need to go beyond that right?",
            "And the hope is that, you know, we can try to learn some high level representation.",
            "By combining these, these low level features.",
            "Right and we can learn these simple representation and then maybe we can learn something that a little bit more complex, right?",
            "That's that's the hope behind these models and then we can learn these systems in completely unsupervised way.",
            "So how can we do?"
        ],
        [
            "Do that well.",
            "I can write down the probability distribution over the entire system, so here you can think of this model as just a Markov random field with hidden variables and you have multiple levels of hidden variables.",
            "Right and you introducing dependencies between hidden variables, right?",
            "That's the key.",
            "And all connections here are undirected and this is very different from models like deep belief networks, for example.",
            "Which hybrid models that are combining directed and undirected models?",
            "I think that they probably somebody was talking about deep belief networks in this in the summer school.",
            "So what changed from before?",
            "Well, this is what we had when we were defining these simple GBM models.",
            "And now you have these two terms, two additional terms and these two additional terms are modeling H1 the dependencies within H1 and H2 here, and H2 and H3 here.",
            "Right and all connections are undirected.",
            "Um?",
            "You can also combine bottom up and top down right so you can say what's the probability of this hidden variable.",
            "The probability of this hidden variable is given by the logistic function, and this is what's coming from above and what's coming from below, right?",
            "So we have this natural notion of things going in both directions.",
            "And this is and obviously in this case the hidden variables.",
            "The problem with this model is that the hidden variables become dependent even when you condition on the input right?",
            "So conditional on the input you have this complicated structure here, so it's one of those kinds of problems that you will see a lot in machine learning.",
            "You can you can deal with a similar model, or you can extend this.",
            "You know you can extend the model and make it more flexible.",
            "More powerful, but often you have to sort of.",
            "No computing or doing inference in these models become problematic, so we have to do something something there.",
            "And how does it different from other models will compare to?"
        ],
        [
            "Models like neural networks, for example, right?",
            "The way you can think about neural networks is that given the input, you have an output in the entire system is deterministic.",
            "Right, so if you think about convolutional neural networks or other kinds of neural networks, the way you're doing it is that given the data, everything is deterministic, and then you're predicting the class, whereas these systems are stochastic.",
            "Right?",
            "If you look at the belief network."
        ],
        [
            "You know this is an RBM sitting on top, followed down by the sigmoid belief network.",
            "That's the definition of the belief network.",
            "The problem with deep belief networks in principle is that if you want to do proper inference in those models, it's very hard.",
            "It's very hard because of explaining away.",
            "Right, in order to figure out what's the probability of this hidden unit.",
            "For example, it depends on what's below what's above, as well as on the neighbors, so it's generally difficult to do inference in these models.",
            "So what happens in the belief network type of models is that you just converting them into a neural net model, and that's how your inference is done.",
            "So."
        ],
        [
            "Let's look at the learning algorithm.",
            "Maybe I can give you some intuitions of how we can do learning in these systems.",
            "If you look at the maximum likelihood learning in the system, you basically matching these two expected sufficient statistics.",
            "And that's basically any type you do anytime you're dealing with undirected graphical models.",
            "Conditional random fields factor graphs.",
            "You basically have in one way or another you're coming down to this equation.",
            "Right, so this equation is again saying maybe the intuition here is that you have."
        ],
        [
            "Spectra sufficient statistics driven by the data and expected sufficient cystic driven by the model, and we can think about the first term is you basically looking at correlations driven by the data.",
            "So you have a date and you look at what's the correlations you see in the data and this is what the model believes.",
            "Those correlations should be right and you're trying to match the two.",
            "That's the intuition.",
            "And the problem obviously here is that both expectations are intractable, so we have to do something about that.",
            "Um?",
            "So both expectations are intractable, but maybe I can give you a reason for that is again, if we look at this."
        ],
        [
            "Additional probability we say, given the data, what's the probability of this hidden variable being active?",
            "We cannot.",
            "We cannot longer compute it, because this probability of this hidden unit being active depends."
        ],
        [
            "On all of these guys.",
            "Right?",
            "So inference becomes becomes problematic, but maybe I can give you an intuition of what these two expectations are doing.",
            "So let's say you have the data and let's say you get to see these handwritten characters.",
            "Right?",
            "So one thing that you can do is you should do is you should say these things should be more probable, right?",
            "You've seen them, they're real data, so you want to raise up the probability of those observing these data points right?",
            "On the other hand.",
            "If you look at this image.",
            "Right, you want to say the probability of this image should be very very small, right?",
            "And that's what's happening with these models effectively.",
            "Is that this term here is effectively trying to make these guys be more probable.",
            "And this term.",
            "Is trying to basically look at this exponential space and basically push down everywhere.",
            "Right, and that's where that's the reason why computing this term exactly is problematic in this case.",
            "Let's say this is 28 by 28 image, it's binary image.",
            "Some pixels are on, some pixels are off, but that means that there are two to the 784 possible configurations, right?",
            "And two to the 784 is a huge number.",
            "Right?",
            "I think so.",
            "I think somebody was mentioning that it's more than the number of particles in the universe by orders of magnitude.",
            "So that basically means that you cannot compute these expectations by brute force.",
            "Um so."
        ],
        [
            "So.",
            "Alright, so that's the intuition.",
            "So what we're going to do is we're going to do the following.",
            "We're going to be using variational inference to approximate these expectations, and I'm going to tell you why it makes sense, and we're going to be using stochastic approximation, which is Markov chain Monte Carlo based inference that approximate these expect."
        ],
        [
            "Patience.",
            "Now there's been a lot of work on dealing with these models.",
            "You know there's much more literature in that space, but the problem is that now when we're dealing with real world applications, we have to deal with thousands of variables.",
            "It's it's very hard to deal with that system and then if you look at a lot of previous approaches, a lot of previous previous approaches, it's very hard to make them work whenever you have hidden variables or layers of hidden variables.",
            "So for example, contrastive divergent.",
            "If you naively applied contrastive divergent to these systems, it just doesn't work, or if you look at score matching there is something called pseudo likelihood models composite likelihood MCMC.",
            "Emily is a lot of work done in a statistical communities.",
            "Alot of these approaches unfortunately.",
            "It's fairly hard to get them to get them working.",
            "So let me show you what what you can do.",
            "It's by far not the most perfect algorithm, it's."
        ],
        [
            "Offers I'll show you where it fails, but it's a step.",
            "So what do you need to do?",
            "Well, the first thing you need to do is you need to do inference, right?",
            "You need to say given the data, what's the distribution over these hidden variables, right?",
            "Or what is what are the features that I'm seeing in the data?",
            "And you may want to do that.",
            "For example, for you know if you're interested in doing classification or so forth.",
            "So we need to solve that problem.",
            "On the other hand, what I can do is I can try to simulate from the model.",
            "Right, I have the probabilistic model.",
            "There is an efficient alternating Gibbs sampling right?",
            "And then maybe I can generate the data from."
        ],
        [
            "Smaller suppose if I generate the data from the model looks like this.",
            "Right?",
            "Well, I can given the approximate conditional, I can estimate data dependent expectations and if I sample from the model I can approximate data independent expectations.",
            "Right, and then I'm trying to match the two.",
            "Right, so the intuition you should have here is that if I simulate from my model and it looks like this.",
            "Maybe I should be changing the parameters of the model such that the distribution of what I'm seeing here matches.",
            "The true data distribution or the empirical distribution that I'm seeing, right?",
            "That's roughly the intuition you should have.",
            "Yeah, it's a question.",
            "Yeah, sort of.",
            "I mean, both of these systems and I'm going to show you the sort of have to do.",
            "You know, the way you have to do inference in this model, you still propagate information up and down and this you generate.",
            "But the way to think about this is that you know, given the data you have to do some inference and then you also have to approximate this joint distribution and the way you can do it just by simulating from the model.",
            "Yeah.",
            "And so if you look at this space, that's where the problem comes in.",
            "In my view, the reason why is because.",
            "This space is a very complicated space.",
            "You have a lot of different modes, right?",
            "Imagine again, the space of all possible characters you can generate, right?",
            "It's a very complex space, that's why building generative models is really, really hard task, like building a generative model of natural images is very hard.",
            "On the other hand.",
            "If I look at this problem right, if I take a particular input and I slice through that space here, the posterior distribution you expecting this posterior distribution to not be as complex as this one.",
            "In fact, you know in most of the cases you expecting the posterior distribution to be unimodal.",
            "Maybe by model.",
            "Or maybe you have a few modes.",
            "If I show you the image like this you could say, yeah, it could be this, or it could be this we have to different explanations about what you seeing in the data.",
            "You have two modes right?",
            "We have 10 different explanations, completely different explanation.",
            "You have 10 modes, but in general you know."
        ],
        [
            "We expect this to be unimodal, right?",
            "So what can we do here?",
            "Well, what we can do is we can use variational inference.",
            "In particular, we can use mean field approximation to approximate to do this inference here, and I'm going to show you that the variational inference is actually pretty good at capturing a single mode in the posterior distribution, and it's very efficient and we need efficiency at the test time, right?",
            "And we're going to be using stochastic approximation MCMC based method to actually try to explore this space.",
            "Right this joint space?"
        ],
        [
            "OK, let me give you an intuition behind stochastic approximation.",
            "What is this?",
            "What is the system do?",
            "I should point out that this actually the static approximation was developed by.",
            "Well, one of The Pioneers were Robinson Monroe back in 57.",
            "And in fact, the interesting thing is that this particular algorithm, which now people call persistent contrastive divergent.",
            "If you've seen it.",
            "Is actually due to Laurent Yonas.",
            "So he published it back in 89.",
            "An interesting thing, I think he published in like French Journal something like that so people didn't know about this algorithm.",
            "Up until you know, five years ago where people rediscovered what he has done back in the 80s.",
            "But the algorithm goes as follows.",
            "You have a state.",
            "This is the state of the system.",
            "This is state of the observed pixels or visible variables and hidden variables and you have parameters in your model.",
            "So you're going to be updating the state in the parameter sequentially.",
            "So you're going to be generating a state from some transition kernel, like for example doing a Gibbs sampler or Metropolis Hasting, running interesting operator.",
            "You essentially simulating a Markov chain that leads be Theta septien variant, so again, a Gibbs sampler will do and then what you're doing is that you replacing this intractable expectation with the point estimate that you get out of your Markov chain.",
            "OK, now in practice you typically simulating several Markov chains in parallel and just replacing expectation by just averaging over those Markov chains.",
            "So it's a very sort of simple system.",
            "You can think of it as simulated from the Markov chain.",
            "Approximate the expectation, simulate update the parameters simulator Markov chain, update the parameters so it's a non homogeneous Markov chain because the stationary distribution changes as you updating the parameters of the model.",
            "But in the end you can show certain convergence guarantees right?",
            "So the way."
        ],
        [
            "The update rule works is that you're replacing these intractable expectations, but what you're getting out of your Markov chain?",
            "Right, and what you can do is you can do the following.",
            "You can say well if I add and subtract the true expectations right I have.",
            "This is what I'm doing right?",
            "And you can think of this as being a true gradient.",
            "This is what you want to estimate.",
            "If you could estimate that, I wouldn't be talking about the algorithm right, and this is the difference between the truth and what you're getting out of your Markov chain.",
            "Right?",
            "The problem with this formulation, and that's where a lot of sort of a lot of people were trying to investigate this, is that whatever you're getting out of your Markov chain is not necessarily an unbiased estimator of these expectations.",
            "So you have to do a little bit more work in order to guarantee convergence of this algorithm, But you can have almost sure convergence guarantees, so you will converge to some stable points as the learning rate goes to zero.",
            "And if you think about the theory, the theory is kind of makes a lot of sense, but it's also a very theoretical right.",
            "It's effectively telling you the following it says.",
            "If my learning rate is small compared to the mixing rate of the Markov chain, my Markov chain eventually will come to the station in distribution and it will be an unbiased estimator of these expectations.",
            "Think about this follows.",
            "If I set the learning rate to zero and I just keep running my Markov chain, eventually it will reach stationary distribution once it reaches station distribution after maybe 100 years, you'll get an unbiased estimator here, right?",
            "So at least theoretically, you can say that.",
            "Yes, ultimately I will have fun.",
            "This, you know, certain conditions that will have convergence guarantees, which is good, at least in theoretical standpoint.",
            "The problem is that in high dimensional spaces you have a very complicated thing that you're trying to model, so then you have to be a little bit smart about how you doing this right and then becomes a little bit more empirical question.",
            "An one of the insights is that these transition operators can be any valid transition operators like temporal transitions.",
            "You know there is parallel simulated tempering.",
            "There's a lot of work.",
            "In, then, in a statistical community that where people are trying to figure out how you can define these Markov chains that can explore these these distributions."
        ],
        [
            "More efficiently, OK?",
            "Now let's come back to the first part.",
            "So the first part remember when we're trying to approximate doing this inference here.",
            "And to do this inference we have to be a little bit smart about how we do it.",
            "But we also have to keep in mind that we have to do it efficiently.",
            "Yet there's a question.",
            "Yes, yes they can obviously.",
            "So for example, things like if you're working with both machines, the fact that they're structured in layers, you can define, you know a little bit more efficient like a Gibbs sampler becomes a little bit more efficient.",
            "Yeah, yeah, I think the general there is some work.",
            "There is some theoretical work that can show on the certain conditions.",
            "Things like parallel tempering with simulated tampering with pronunciations can improve things, but it's a way of of playing with the temperature parameter and to be able to move between the different modes.",
            "But you know, under certain conditions you can show that they can give you improvements, but in general for these kinds of models I don't think there are.",
            "OK, so let me quickly tell you bout variational inference and maybe give you an intuition F as to us to what we can do here, right?",
            "And this is the last part that's going to be very technical.",
            "And then I'm going to show you some results, so bear bear with me.",
            "So the idea behind variational inference is actually very very simple.",
            "And a lot of you know a lot of models that we see in machine learning.",
            "People are using inference variational inference.",
            "The key idea is quite simple and there's just extensions on this idea.",
            "It's the following.",
            "Suppose you have an intractable distribution.",
            "Is conditional distribution right?",
            "You cannot compute this distribution exactly is exponential computations that come in.",
            "But what if you want approximated with a simple tractable distribution queue?",
            "Right think of Q as being a Gaussian distribution or fully factorized distribution, or something simple, something that you can specify exactly.",
            "Here's what you do.",
            "You can say, well, the look of the marginal.",
            "Here is the log of this joint.",
            "You have to sum over hidden variables.",
            "Right and then what I'm going to do is I'm going to multiply and divide by Q distribution.",
            "I have to be careful that I don't divide by zero, so there are some technical conditions, but let's leave those out.",
            "And here is the key.",
            "What you doing is using Jensen's inequality.",
            "You just taking this log and pushing it inside.",
            "Right, you see log here goes inside.",
            "So the log of the expectation is always greater than expectation of the log, because the log function is concave function.",
            "And now what happens for Boltzmann machines?",
            "Well, what happens for both machines?",
            "If you take this particular formulation and just write it up?",
            "Right, you have this term here and this is a normalized probability.",
            "So this is this entire log is just linear in the parameters.",
            "This term is called the entropy term of the Q distribution.",
            "But notice what happens here.",
            "This is the term that requires exponential computation.",
            "It's the log of the normalizing constant, right?",
            "This is something that we cannot compute.",
            "This is where the problems come.",
            "In fact, it's very hard to even estimate this thing.",
            "On the other hand, this thing, and this is known as a variation about this thing is easy to compute.",
            "This thing is easy to compute.",
            "Right, so things that depend on the Q distribution are easy to compute and we don't really care about this term here.",
            "Right, because it's just a constant, it doesn't really doesn't really play.",
            "You know if we're trying to find what's the best queue, we only need these two terms.",
            "We don't really care about this one.",
            "The other thing that you can think about this formulation is you can say, well, it's the same thing as the log of the probability right, which is what we're trying to optimize minus the KL divergent was a cool backliner divergance between Q&P.",
            "An you can think of KL divergences measuring distance between the two distributions.",
            "So obviously if Q is the same as P, you can show that this is going to be 0.",
            "Otherwise it's a non negative quantity it so it can be 0 or positive.",
            "So that's why it's about.",
            "But it has sort of intuitive interpretation.",
            "You basically saying I want to push on this variational bound.",
            "I want this term to be 0 fine McHugh.",
            "That's as close as possible to P according to the KL Divergent according to this particular.",
            "Measure of distance between the two distributions.",
            "And we can maximize the KL between the approximate and the true right?"
        ],
        [
            "So what we can do here is now you can say well, what's the form of the Q distribution, and that's where a lot of different formulations come in, right?",
            "How do we define what the Q is?",
            "Can you?",
            "Can you make some small approximations about Q and so forth?",
            "In our case, we can just choose a fully factorized distribution, essentially breaking the links in the model.",
            "We can say our approximation is just product of bunch of marginals or these conditionals.",
            "And then we can just maximize the low bond with respect to these variational parameters, and essentially what's happening here is that if this is your posterior, then the mean field will find a single mode.",
            "We can find this mode, unfortunately, nonconvex problems.",
            "So whatever mode you find you find.",
            "And it turns out that if you just derive what the updates are, it's a very simple, simple set of updates.",
            "You just cycling through these signal of nonlinear equations until you converge.",
            "Typically, after five iterations you know you do converge.",
            "Um?",
            "Right, so that's that's the.",
            "That's the approximation that we do."
        ],
        [
            "So you doing variational inference you maximizing the lower bound with respective variational parameters.",
            "You doing MCMC.",
            "To update the model parameters and then you can sort of play this theory game by saying that, well, asymptotically you guaranteed to again come to the stable point of the variational lower bound, right?",
            "The whole theory theory goes through.",
            "So we have fast inference.",
            "Learning can scale to millions of examples.",
            "The big question is that."
        ],
        [
            "Does it actually work?",
            "Right, so so that's $1,000,000 question, but let me."
        ],
        [
            "Show you.",
            "Let me let me.",
            "Let you judge whether this is a good model or not.",
            "So what I'm going to show you is I'm going to show you two panels on one panel.",
            "You will see the real data on the other panel.",
            "You will see simulated data and some of you have seen that.",
            "So if you've seen that.",
            "Don't raise your hands.",
            "Right and you basically have to judge.",
            "So in one case you will see the real data on another one you'll see simulated data data simulated from the model.",
            "You have to tell me which one is which, so these are handwritten characters coming from 50 different alphabets around the world.",
            "How?"
        ],
        [
            "If you thought this was simulated, this was real, honestly.",
            "Good, what about the other way around?",
            "Perfect 5050 is great.",
            "Now, if you actually look at the data a little bit more, you will start seeing the difference.",
            "Right?"
        ],
        [
            "This is simulated.",
            "This is real.",
            "So for example if you look at this character right.",
            "It's just not quite right.",
            "There's certain things that are missing also."
        ],
        [
            "If you look at the real data.",
            "The characters that you see there's much more diversity in the real data, right?"
        ],
        [
            "And that has to do with inability of the Markov chain to explore this space.",
            "Right, but it does capture a lot of interesting structure that you see in the data.",
            "Right, that's the trick I've learned from neuro scientists.",
            "If I show you things really fast enough, you won't tell the difference, right?",
            "So, but but it does learn you know, interesting."
        ],
        [
            "Structure that you see in the data here is.",
            "Yep."
        ],
        [
            "Ocean.",
            "No.",
            "It's a local optimum.",
            "So what what it finds like?",
            "Obviously you know these ones?",
            "Yeah, it's a good question.",
            "You look at characters like.",
            "You know these ones.",
            "These are less complicated or these ones.",
            "Here is."
        ],
        [
            "Other data.",
            "This is the M news data set.",
            "Probably most of you have seen this.",
            "How many?",
            "How many of you think this is simulated?",
            "This is real.",
            "What about the other way around?",
            "Oh wow, you guys are bad.",
            "Most of the audience that I that I talked to about they really can't tell the difference.",
            "'cause a lot of the lot of us are working on these data set which is sort of a toy data set.",
            "This is real.",
            "This is simulated.",
            "Is the zero throwing you off?",
            "Some people told me this zero throws you off, but I promise I did not put the zero in there.",
            "Just took a random subset and then.",
            "But yeah, so here.",
            "For example, if you look at this three, if you look at this five right, it's just not quite right.",
            "You you will be able to tell the difference."
        ],
        [
            "But in terms of, you know if you now do the proper thing, you can say well, given the new test data and you find the representation, can you actually figure out?",
            "Can you do?",
            "Can written character recognition can you can do optical character recognition and so forth so these models do work reasonably well.",
            "So for these types of tasks, what we effectively doing is we are inferring the distribution of the hidden variables and then you fitting logistic on top.",
            "Right and you can back propagate through entire system, but the details they don't matter that."
        ],
        [
            "Here is another example of.",
            "Obviously you can see that this is simulated.",
            "This is real, so these are 3D objects.",
            "It does sort of find interesting things.",
            "So for example, if you look at this guy here, kind of looks like a car with wings, right?",
            "You can sort of see that, So what happens here is that when it transitions from one mode to another mode simulates these things so.",
            "Of course you can also see that they're a little bit imbalanced, so these are just generates one donkey, one elephant, but generates too many people with guns, right?",
            "Like if you look at here here, here.",
            "So it's just has a little bit of puts the mass disproportionally on some classes, and again it has to do with the fact that it's very hard to calibrate these.",
            "These"
        ],
        [
            "Now you can also do fun things like padding completion, right?",
            "So if I show you these images, half of these images, this is what the remaining ones look like.",
            "And the interesting, and this is the truth and the interesting thing here is that these objects here are the test objects, so you never see, for example, you never see cowboy.",
            "At the training time, right, you only see cowboy the test time, but the model is actually, you know, kind of figures out that there is a lag in an arm and a some completion.",
            "I should have also put what the nearest neighbors looks like and there are some examples where I think for.",
            "This example no for this example thinks it's a heap on just completes the remaining fine as a hippo.",
            "So if you look at the nearest neighbor type of approaches, they work a little bit worse than these systems."
        ],
        [
            "OK, so let me step back and talk about how we can a little bit more of a let me just.",
            "Time wise, how we can apply these models and this is work with new tissue estava."
        ],
        [
            "OK, let me just step back a little bit and say, well, if we looking at.",
            "You know multimedia content on the web.",
            "It's we always deal with not a single modality, not just images or not just tax.",
            "It's typically combination of the two, or if you look at robotics applications, a lot of things coming in.",
            "So how can we apply or build models that can take advantage of these different data modalities, right?",
            "And maybe what we can do is we can try."
        ],
        [
            "To build a system that goes from images and tags into some some representation of both, right?",
            "Are and why is this useful?",
            "Well, you can do multiple things right."
        ],
        [
            "So for example, given the image and given some tags, we can improve classification.",
            "People have shown that that's generally true even if the tags that you see are noisy.",
            "You can fill in missing modalities given the image.",
            "Maybe you can generate tags associated with the with the image or the other way around given some tags, can you retrieve images?",
            "Right, so if you're building a probabilistic model that models both of these data modalities, you can do all of these things.",
            "What is?"
        ],
        [
            "Some challenges, well, one of the big challenges that typically images in text.",
            "They have very different representations, right?",
            "If you look at images, you have dense representation.",
            "We look at pixels, or even if you look at some features that you constructing from images.",
            "If you look at text, text is much more.",
            "Complex words mean things, right?",
            "But in terms of representation, typically sparse.",
            "It's very difficult to learn these cross model features just based on those representations.",
            "What's happening in what was happening in our communities that people would just concatenate these features and trying to learn something on top.",
            "That's the general approach."
        ],
        [
            "The second challenge is that typically.",
            "We have very noisy or missing data, right?",
            "So in this case, sometimes the modality is missing completely.",
            "I show you this picture with no tags.",
            "Sometimes it's very noisy.",
            "And we have to build.",
            "I believe we have to build some form of generative model that allows us to deal with these.",
            "Deal with these inconsistencies.",
            "And."
        ],
        [
            "These attacks, this is the text generated by the model, right?",
            "So it's sort of.",
            "Does reasonably reasonably well.",
            "So."
        ],
        [
            "What can we do here?",
            "Well, we can build a model.",
            "We know how to deal with real valid data.",
            "We know how to deal with count data.",
            "We can just build a single model here.",
            "But again, because of this, data modalities data inputs have very different statistical properties.",
            "It's very difficult to learn a small.",
            "I think this model was already proposed by a few people back in 2000.",
            "Four 2005 and such, and it's sort of kind of work, but never really worked."
        ],
        [
            "So what we can do is we can do the following, we can."
        ],
        [
            "Is build a hierarchy right so we can say take these word representation, push them up, take the images, push them up and then try to model some?",
            "Um, correlation between these two different data modalities, right?"
        ],
        [
            "It is very natural notion of bottom up and top down in these models like an abortion machine models because.",
            "It's effectively what is saying is that these words can essentially affect the features the low level features of images, and the other way around.",
            "So the information flows up and down in this model.",
            "During inference, when you running mean field approximation, you basically propagating information up and down up until you settle to some to some joint state.",
            "And you can obviously deal with the missing modality, because if the modality is missing and we're building the probabilistic model of both of these data distributions, one of them is missing.",
            "We just integrating it out.",
            "Um?"
        ],
        [
            "And you can write down the proper question.",
            "Just confirm what?",
            "That's right.",
            "That's right, so the entire model is being trained jointly.",
            "All layers are being trained jointly, not just one layer at a time.",
            "And then you know fine tuning.",
            "So we actually building the joint distribution jointly, yes?",
            "So you can specify the probability distribution.",
            "The equation looks a little bit scary, but it's very simple actually.",
            "You have a replicated softmax put pathway.",
            "You have a Gaussian which pathway and this is just joints both of them together which defines."
        ],
        [
            "Defines a proper proper model, yes.",
            "Yeah, I think it's sort of so that becomes a question of like how do you choose the architecture becomes a little bit of exploratory analysis, right?",
            "I mean, in this case we just said two layers worked OK.",
            "It's fine.",
            "I think that in most of the cases you know the architecture can give you the right architecture, can give you some gains, but it's not like you know you're building a model with three layers versus.",
            "Four layers in the Fuller model gives you huge advantages of a two layer model and so forth.",
            "I think these architectures are more or less stable, so in this case I think we were using two layers in the third layer, which was sufficient.",
            "For us up sorry, so let me show you some examples.",
            "These are sort of examples were given an image right?",
            "You're looking at the distribution of a words and this is what these look like.",
            "Sort of finds interesting things.",
            "These are good examples.",
            "It's always fun to look at.",
            "Failed example."
        ],
        [
            "It's even more fun to look at the failed examples.",
            "These are some failure examples, so it was an image like portrait.",
            "Women soldier.",
            "I like this once.",
            "He thinks it's Barack Obama.",
            "Election politics, right?",
            "So this happened to, so we tried to look at why is why this is happening.",
            "In fact, that was done three years ago, and at that time we didn't have quite as good features.",
            "One on one hand, but then on the other hand, we actually look at this was done on the Flickr data set, and unfortunately, Flickr data set.",
            "There aren't that many images of animals, but there are lot of Obama signs, right?",
            "So there is so there is a little bit of because of his signs are like blue and white.",
            "So then the model just correlates that.",
            "But obviously these are these are failed cases."
        ],
        [
            "Here also the other way around, you can give in tax.",
            "You can find images sort of does reasonably well for a lot of things.",
            "Here's a failed case chocolate cake.",
            "Right, so if I say, can I look like chocolate cakes, right?",
            "Like if you.",
            "You know, if you if you squint your eyes a little bit here you'll find these two bugs right, but it does.",
            "It does fail."
        ],
        [
            "This is what the data set looks like.",
            "You have some images like for example.",
            "For this image it's a very good description of what's going on for this image just tells us what kind of camera was used to take this image right?",
            "So you can see there's a lot of noise in the data and you have to deal with the noise."
        ],
        [
            "This is the architecture that we used has about 12 million parameters.",
            "You have just been using 200 most frequently used text.",
            "You have about 25,000 labeled examples.",
            "Split and then what's interesting about the data is that we actually have 1 million additional unlabeled data, which is.",
            "Actually helped us.",
            "Improve the results, that was that was interesting.",
            "And if you look at, you know when I talk to my statistician friends.",
            "They say 12 million parameters.",
            "That's just crazy.",
            "What are you doing when I talk to my friends and industry like friends at Google, they say 12 million parameters.",
            "You don't have a bigger model.",
            "We're using 12 billion parameters, right?",
            "So it depends on the scope.",
            "Depends on who you're talking to."
        ],
        [
            "In terms of looking at the results, this is just some.",
            "Some results has been some work on using LDA and SVM type of models just using labeled examples.",
            "But what's interesting about these models is that if we actually add a mill."
        ],
        [
            "Unlabeled examples we do see some gains and we do see substantial gains in improvement.",
            "So that was giving us a little bit of hope by saying that if you actually have unlabeled data because you're building the generative model of both of these modalities, it is helping you, at least for this particular data set.",
            "It was helping us."
        ],
        [
            "And now you can step back and say, OK, Now you can generate decks.",
            "How about a more challenging problem?",
            "How about a problem of generating complete sentences?",
            "And this is what I've shown you and this is going to be second half tutorial.",
            "I'm going to show you how how you can actually extend this kind of model and do that.",
            "Now in the last part of this tutorial, what I'd like to do is I'd like to just, yeah, question.",
            "That's right, so yeah, should be.",
            "I should be a little bit more careful, so this is retrieval.",
            "So the way that you do it in this system is that you generating the features.",
            "Right, and then once you generated the features you just looking at the training data, in which case a million images and finding the one that's closest in terms of cosine similarity to the training image.",
            "Yes, yes, it's true.",
            "We're not at this stage where we can generate these images.",
            "No way where if my team was able to generate those images.",
            "That would be the most amazing thing.",
            "So this work was done three years ago, and at that time we were using bunch of sort of computer vision features so that was sift.",
            "It was.",
            "Basically the same set of features as were used by other people because we wanted to compare on par.",
            "I think that if you're going to be using image Net features now.",
            "You know all the numbers will go up, maybe by 10%?",
            "It's a general trend, like if you just replace one set of features with another set of features you know everything goes by 10% or 15% up right so?"
        ],
        [
            "OK, now in the last part of the tutorial what I'd like what I thought I would do is I would give you a little bit more technical so it is going to be probably the most technical part of the tutorial where I'm going to try to tell you about an interesting method for for trying to evaluate these models.",
            "I'm going to show you that evaluation is a very hard problem because you know we're doing approximate maximum likelihood.",
            "We can compute these so called normalizing constant partition functions, so trying to see is your model better than somebody else's model is actually very challenging.",
            "And this is joint work with durable Diane Roger Gross and I think you're is here, yeah, so you can ask him more questions if you if you'd like to."
        ],
        [
            "So let's just come back to the graphical models.",
            "And generally, where you specify your graphical model, you can say, well, the probability of an ensemble of random variables.",
            "You can either specify the energy and say the probability is one is E to the negative energy in this normalizing constant.",
            "Or you can just say it's some function and normalized function.",
            "A normalized probability of X divided by the normalizing constant and the normalizing constant is you have to sum over all possible configurations so that you actually have a valid probability distribution, right?",
            "And this is problem.",
            "Right, this is a problem because it's difficult to compute because it requires exponential sums.",
            "So then it's very hard to evaluate generatively Markov random fields, conditional random fields, and so forth.",
            "So most of the time people use these models to see how well you can classify things or how well you can do sort of auxiliary task.",
            "Test your system on an exhilarate task because computing these probabilities is really really hard."
        ],
        [
            "OK, so for example, if we go back to restricted Boltzmann machines for example.",
            "I can define the energy as before you've seen that, then I can say the probability of the marginal should be the joint of this marginal is given by the Boltzmann distribution, and so you're summing over the states of the hidden variable.",
            "So you can write it in this way.",
            "This term you can compute exactly 4 restricted balls machines.",
            "You can approximate it for both Diebold machines, at least for BMS.",
            "You can compute them exactly, so we have this this ratio.",
            "This you can compute, right?",
            "This is just a function of F of V. This you cannot compute right because, again, the normalizing constant you have to sum over this exponential space, you have to sum over all possible configurations, so you cannot do this.",
            "You have to approximate fine.",
            "We know how to approximate."
        ],
        [
            "These things, so let's look at the following questions.",
            "Like how do you do model selection?",
            "How do you do complexity control?",
            "So suppose you have two MRF's or two PBM's with two parameters, each MRF for each PBM has different number of hidden variables, was trained using different learning algorithms and so forth.",
            "And what you'd like to do is you'd like on the validation set to look at the ratio of the two right there.",
            "Just compare the tool.",
            "Which one is better and typically the way you do it is maybe you know you look at reconstruction error, which is the proxy is a bad proxy, or you try to see which one model the features that you're learning abetted, classifying things.",
            "That's another proxy.",
            "But in order to sort of do the proper way of evaluation, you actually have to compute the ratio of partition functions.",
            "Right, we have to compute some estimate of the thing and it's."
        ],
        [
            "It's very hard to do it, so let me give you an example.",
            "Which one is a better generative model have to models model and Model B?",
            "How many of you think that model is a better generative model?",
            "Great, how many of you think Baeza better generative model?",
            "Nobody thinks that way.",
            "That's why you should never trust people who show you samples from the model.",
            "So I'll tell you what this model is.",
            "This model is the following generative model.",
            "I pick a training sample at random and I'm going to show it to you.",
            "This is, this is what I'm doing here.",
            "So this is saying that this model is has no generalization, can't get kicked capabilities right?",
            "All I'm doing is I'm just showing you the data.",
            "And this is a mixture model is a mixture of Bernoulli model, so there's actually much better model in this.",
            "Right, so you have to be careful when I show you pretty pictures of generated things.",
            "You have to be careful and maybe just I overfeed it on the training set and all of these things are exactly coming from the training set."
        ],
        [
            "Right?",
            "So for example, these are samples from GBM.",
            "These samples from mixture of Bernoulli sense.",
            "Let's say I want to compare this on the validation set, right?",
            "I need some estimate of the partition function.",
            "I need to know is this model better than this model?",
            "Right?"
        ],
        [
            "And in fact, you know we some samples with some approximations that I'm going to talk about.",
            "You can show that PBM's.",
            "I actually much much better than mixtures in this case.",
            "Make sure we notice we can evaluate these models.",
            "At least approximately.",
            "I'm going to tell you how, but we can.",
            "In this case, I'm pretty confident that I can tell you this is a better model in this."
        ],
        [
            "OK, so how do we do that?",
            "There is one very beautiful technique called annealed importance sampling.",
            "Um was developed by Radford Neal and it's I think it's sort of considered one of the Golden standards for estimating partition functions or estimating exponential sums.",
            "So let me maybe give you an intuition of what this algorithm is actually doing.",
            "Suppose you have two distributions.",
            "Define on some SpaceX and you have these functions, so you have these probabilities.",
            "I'm going to call the initial distribution and target distribution, and suppose that this probability the initial distribution is very easy to sample from.",
            "OK.",
            "So think again, is this distribution is maybe being uniform distribution?",
            "Right, if it's a uniform distribution is very easy to get samples from and you have a target distribution and this is intractable target distribution.",
            "This is what you want to get your hands on.",
            "Right, so under some conditions you can do the following the partition function.",
            "You have to sum over all possible configurations of X, right?",
            "That's what you want to do, and that's an exponential sum.",
            "You can compute exactly.",
            "And what you can do is you can multiply and divide by the simple distribution.",
            "And again, you have to be careful that you don't, you know, dividing by zero and so forth, but let's just assume that both of these distributions assign nonzero probability to the entire SpaceX.",
            "Right, I can do that.",
            "Nothing changes and then I can do the following.",
            "I can say how about using a Monte Carlo approximation.",
            "This partition function can be approximated by draw samples from your initial distribution.",
            "Right, which you can do.",
            "Let's say it's a uniform distribution and just look at the ratio of the two.",
            "If you look at the ratio of the two and do the average, that's called Monte Carlo approximation.",
            "Right?",
            "Um?",
            "And this W here is called the importance weight.",
            "It looks at the ratio of F divided by the initial distribution.",
            "OK, that's it.",
            "It's called Simple Monte Carlo simple important sampling, important sampling.",
            "It's very widely used, used idea very simple.",
            "The problem is that in high dimensional spaces.",
            "The variance of this estimator will be high.",
            "You can show it an unbiased estimator.",
            "Asymptotically.",
            "Everything is fine, you'll get the right answer, but the variance can be very high, or in fact can be infinite, in which case you will never get the right.",
            "Value yes, there is a question.",
            "Sorry what.",
            "No, the key thing is that you cannot draw samples from the target distribution.",
            "Right, so for example, you cannot draw samples from exact samples.",
            "You cannot draw exact samples from GBM, so MRF.",
            "So any type of undirected graphical model, unless it has a very specific structure like a load tree with structure, in which case you can compute everything more or less exactly.",
            "So the idea here is that for any given XI can compute this thing.",
            "If you tell me XI can compute F. But I cannot draw samples from this model.",
            "Yeah, good question.",
            "And then there is the idea of annealed importance sampling of a very beautiful idea which.",
            "Little bit of a crazy idea.",
            "It says, well, if you're working in important sampling, how about instead of working in this space, we've been going through higher space, high dimensional space in do important sampling in high dimensional space?",
            "So here's here's the idea.",
            "Suppose you have."
        ],
        [
            "The sequence of intermediate distributions they go.",
            "You know this is your initial distribution.",
            "This is your target distribution and you have a sequence of these distributions and one way when general way is to use geometric averages, there are other ways of doing it, but one way is to look at the geometric averages just easiest way, so you can say all of your intermediate distributions.",
            "I'm going to be defining that way right.",
            "F beta divided by set of data.",
            "Which is going to be take the initial distribution, raise it to the power of 1 minus beta.",
            "Take the target distribution raised to the power of beta and you have a normalizing constant and these betas have to be chosen by the user.",
            "This is a scheduling that you defining.",
            "If the initial distribution is uniform, then you can say that all of your intermediate distributions have this form, which is why it's called annealing, right?",
            "The reason why it's called annealing is that when beta is 0.",
            "You have uniform distribution.",
            "When Beta is one, you have your target distribution, so you're going from uniform to the target distribution, right?",
            "And this is called the inverse temperature.",
            "So depending how you define it.",
            "Um?",
            "So.",
            "You can do that and the idea here, yes.",
            "Yes.",
            "Yeah, it's a good question.",
            "How do you choose the spacing of betas?",
            "There's no good answer for that.",
            "It has to be chosen by the user as being a lot of tons of papers.",
            "If you look at the statistical literature is tons of papers people are trying to optimize for what the right betas are and so forth are some techniques of doing that.",
            "But in terms of practical.",
            "Things.",
            "I don't know of any practical technique that can actually give you the right the right spacing, so you sort of have to play with it.",
            "Yeah, there's a question.",
            "So in our case, I think it's mostly uniform.",
            "We just we just.",
            "It's the easiest thing to say from zero to 1.",
            "Define maybe 10,000 million distributions.",
            "All of them are spaced uniform and the certain conditions maybe have geometric spacing and so forth, but it's just, it's just an empirical question, yeah?",
            "Why don't you use?",
            "Well, it becomes a little bit expensive to do it for learning.",
            "There's been some work in trying to use these these ideas for learning, but it's just too expensive.",
            "It's too expensive.",
            "Like you know, if you want to get good estimates and you have 10,000 intermediate distributions, you can't afford doing 10,000 things.",
            "For every single update, but let me let me sort of the goal here.",
            "I guess for me wasn't really to focus on on this particular matter.",
            "I'm going to talk about the different one."
        ],
        [
            "But just to give you an intuition, here is that you start with the uniform distribution.",
            "An you go at the sequence.",
            "You sort of changing it and changing it and so forth and you go to the target distribution.",
            "So have a sample here running around and then sort of falls between the two modes and so forth.",
            "So you also need to define a transition operator for every one of these intermediate distributions, but that's very easy to do.",
            "Turns out that geometric averages.",
            "It's a trivial change to the code, so believe me when I say that defining these intermediate transition distributions is really, really easy.",
            "You just multiplying your energy function by scalar."
        ],
        [
            "So how does annealing point sampling works?",
            "You sample from your initial distribution, you sample the next state, and so forth, and then you go all the way down until you sample from.",
            "When beta equals to one.",
            "Right and then there is this thing where you basically look at the ratio of all of these normalized distributions as you go along.",
            "And it turns out magically that that gives you an unbiased estimate of the partition function.",
            "Right?",
            "And the key idea here is that whenever you go from one temperature to another temperature, you look at the ratio.",
            "Right of these guys.",
            "The hope is that the ratios are going to be very, very small, so it's like, you know, in some case you dividing into finer distributions and the difference between 2 nearby distribution is going to be small.",
            "Right, so the hope is that by that you're going to be reducing variance."
        ],
        [
            "And so you're getting an unbiased estimate of your partition function, which is nice.",
            "Yeah.",
            "Right so well.",
            "So the thing about as I mentioned in the straight important sampling, the variance is high because if the difference between the initial and target distribution is too big.",
            "You know, samples from the initial distribution are not going to be matching the samples from, you know they they from the target distribution.",
            "And when you look at the ratio of the two, you're going to have huge variance.",
            "Here when you defining a sequence of these distributions right when you're looking at the ratio between intermediate steps, that ratio is hopefully going to be close to one, so you actually going to be.",
            "The variance shrinks and you can show that.",
            "I think that asymptotically, you can show that the variance is going to be roughly shrinking as you increase the number of intermediate distributions.",
            "That's that's well.",
            "Yeah, so this is it's true.",
            "I mean when you multiply, think so.",
            "For example what will happen in some cases?",
            "What might happen is that there are sort of these.",
            "Phase transitions where even small changes, intervening distributions or small changes in beta can heat like huge changes in the distributions, so that can happen, but it's just an empirical question.",
            "I think it's as there is some.",
            "I think there is some theoretical results shown by Radford Neal that the variance, roughly speaking, is going to be reducing in terms of the number of intermediate steps.",
            "There's sometimes some constant, so we can actually show that asymptotically you will be reducing the variance.",
            "In fact, if you take the number of intermediate distributions and you let it go to Infinity.",
            "You'll get zero variance because you can write this whole thing down as the integral, right?",
            "Samples.",
            "But yeah, to some extent so that maybe it may be a better way of thinking about it is the following.",
            "If you look at this ratio here right?",
            "Or the ratio of intermediate things, what you can do is you can say, let's say I have two distributions.",
            "And these two distributions are very close to each other.",
            "In KL space, if they are close to each other, I can use important sampling to estimate the ratio.",
            "If I could sample from 1.",
            "If I could get ID samples from one I can look at the ratio and it's going to be pretty good estimator.",
            "So imagine that your Markov chain had perfect your Markov chain.",
            "Actually, here was had a perfect transition operator.",
            "It would just draw ID sample at every single temperature, right?",
            "In this case you can actually show that for the intermediate distributions because they are so close you can actually get very precise estimate.",
            "Right, so that's that's the idea.",
            "Sort of.",
            "This argument fails because your Markov chain doesn't is not a perfect transition operator, so you can construct cases where this will have also huge variance.",
            "Right, but the key again because you're looking at the nearby.",
            "If you nearby distributions are very close to each other, you can use important sampling, so you can basically break this thing down and say if you could use important sampling for each one of those ratios, you would reduce the variance.",
            "Right, but again, there are some cases when you can construct specific things where the variance can explode, right?",
            "So there's no guarantees there is no formal guarantee by saying that it will succeed at the certain conditions.",
            "You can show that it will succeed, but.",
            "OK, so.",
            "You have a forward chain, right?",
            "So how do you actually show what The thing is?",
            "Here's the intuition.",
            "You have a forward chain.",
            "You start at the data and you March through this transition operator.",
            "You can think of this is just conditional probabilities.",
            "Right, but then what you can do is you can say, well, what if I start at the target and go all the way back?",
            "To the data using sort of reverse sequence of transition operators.",
            "Right, and it's just a theoretical construct and you can say, well, the reverse is that you actually, if you could draw exact samples from the target and then move backwards all the way to the data, that's going to be the reverse sequence, and then you can say, well, annual important sampling.",
            "It turns out there's just a simple important sampling.",
            "It's the same as important sampling."
        ],
        [
            "But on extended state space, right?",
            "You using forward chain as a sampler and then you basically estimating this ratio.",
            "You sampling from the forward, you start at the uniform go and then then you look."
        ],
        [
            "Duration.",
            "And here's what it looks like to estimate the partition function.",
            "So this is what annealing point sampling with geometric averages look like, so you know, as beta goes up.",
            "Nothing happens and then.",
            "Digits appear right, so you know if it's, it's just sort of visually.",
            "What happens here, right?",
            "You start it sort of base rates, almost uniform distribution, and as you go through different phases.",
            "This is what the samples look like and then as beta heats one, you get the digits right."
        ],
        [
            "Now, problems with these models.",
            "That was done a few years ago, but what's the big problem?",
            "So areas provides an unbiased estimate of the partition function that was shown, just like important sampling.",
            "But in general, we're going to be interested in estimating log.",
            "We never actually get the partition function we're interested in.",
            "We're interested in log probabilities, so by answers inequality, the actin expectation.",
            "The log of the estimate is always going to be less than the load of expected value, which is the true partition function.",
            "So what this is saying is that.",
            "Roughly speaking, an average, we're going to be underestimating the partition function by Markov inequality.",
            "It's also very unlikely to overestimate the log of the partition function, so this is saying the probability, the probability that the log of your estimate is bigger than the truth by some.",
            "Value B is going to be less than each of them.",
            "Mine is B, so estimating by a few nuts it's very improbable.",
            "Right, so what's happening?",
            "What's happening in practice is the following.",
            "If I look at the log of the probability, it's this thing I can compute, but then it's minus log of the partition function.",
            "Right, and that's just acoustic bound.",
            "So if we underestimate this, we're going to be over estimating the probabilities.",
            "Right, so why is this bad?",
            "This is bad by the following reason.",
            "You can take your model.",
            "Do a really bad job and running your important sampler.",
            "And get really good results and this is fantastic, right?",
            "The worst the you know if you're doing a bad job at estimating your partition functions, your result look good, right?",
            "And you know you code it up.",
            "You get some value of the partition functions and your numbers look really good.",
            "You say, wow, this is amazing.",
            "Right, and that's exactly what's been happening here.",
            "Like sometimes I see papers that are reporting on amnesty numbers like 50 or 40 knots, which is obviously wrong, and I know it's wrong because it cannot be right, and I think a lot of it has to do with, you know, and that's a problem.",
            "You know, like you're running bad code and you're getting good numbers, and the worst your code is, the better numbers you get, right?",
            "So that's what's happening.",
            "What's been happening with these models?",
            "So if you don't know what's going behind the doors, it's very hard for you to use this method.",
            "OK."
        ],
        [
            "But let's look at the motivation.",
            "For for sampling, let's say."
        ],
        [
            "We're looking at carbs and how do we do sampling?",
            "We started random configurations we."
        ],
        [
            "Update The hidden swear."
        ],
        [
            "Data visible and so."
        ],
        [
            "Choice of running this."
        ],
        [
            "Of chain and the theory tells you that if you run it to Infinity you get samples.",
            "Yes, you get, you get the quality distribution right."
        ],
        [
            "Suppose that what we actually do in practice is that we're running this chain 4000 steps, and then we pretend that this is an equilibrium distribution.",
            "So if I show you samples, this is what I'm actually doing, right?",
            "So why not treat this model as a generative model?",
            "Right?"
        ],
        [
            "So what we can do is we can do the following.",
            "We can say start at Rand."
        ],
        [
            "You go on."
        ],
        [
            "The way down."
        ],
        [
            "And what you actually get is the observed data, and that's your model.",
            "Active model.",
            "It's not that the RBM is your model, but this is your generative model."
        ],
        [
            "So if we use infinite number of layers here, then the genitive is going to be the same as GBM.",
            "Other ways this this generative models just in approximation to an RBM.",
            "But we're going to be treating this as a generative model."
        ],
        [
            "Alright, so let's consider X to be the joint space.",
            "Each is an observed and then we can define the following generative process, which is just a sequence of IIS distributions.",
            "So we start at random and then go all the way down to the data and this is the generative model that we call the annealing model.",
            "That's what we're interested in, because in practice that's how you're going to be using this model, right?",
            "If we want to draw samples, this is how I'm going to be doing samples.",
            "So instead of thinking, this is in GBM model and thinking of this is a generative model with multiple.",
            "With me"
        ],
        [
            "Layers.",
            "Now we would like to estimate the probability of test right?",
            "How do we do that?",
            "Well, here's the trick.",
            "The trick is starting at the data and doing the reverse annealing.",
            "Right, so you can say this is reverse annealing.",
            "Is you started that data?",
            "You have to be able to compute this conditional probability of hedons given visibles which you can do for our BMS.",
            "And then you can extend it to non tractable posteriors like the both machines in deep belief Nets.",
            "So you can do that and you have the sequence of reverse operators and now what you're going to do is we're going to be treating that as an importance as a proposal for important sampling."
        ],
        [
            "Right, so now you have a generative model, which is just a theoretical construct.",
            "And then the proposal starts at the data and melts the distribution.",
            "So we go the other way around.",
            "Right, and it turns out that for this model we can use this reverse AES as a proposal and then we look at the ratio of the two.",
            "Right and we can show that we get an unbiased estimator of this quantity.",
            "We actually get an unbiased estimate of the probability of the test data and then what's nice about this particular estimate is that it tends to underestimate rather than overestimate low probabilities, which is good.",
            "Which basically means that if you're writing a bad quote code.",
            "You'll get bad numbers here, which is good.",
            "Right?"
        ],
        [
            "So here's just one example, and I'm almost done.",
            "This is double nahmias.",
            "Look what happens here.",
            "Here this this shows the number of intermediate distributions that you're using in IIS, so if you just estimating partition function and use 10 intermediate distributions, you get a really bad estimate.",
            "You really underestimate the partition function, in which case you really overestimating log probabilities, so the higher in terms of log drops, the better right?",
            "And you can see as you increase the number of intermediate distributions as sort of goes here.",
            "And I call this the new state of the art.",
            "So you know, this is done really bad job.",
            "It's just a few intermediate distributions, but my numbers look fantastic, so I could claim this is a new state of the art."
        ],
        [
            "Whereas if you look at the other way of estimating it, you sitting over all the way here, right?",
            "So it sort of provides abound allow stochastic lower bound, so you cannot fool yourself into believing that these models are better than what they really are."
        ],
        [
            "If you changing different if you're doing different uniform other from uniform other initial distributions, you can sort of get this property in.",
            "For omniglot datasets you again see the same thing, right?",
            "The EIS goes up and race you really getting the bound which is."
        ],
        [
            "Which is important and finally just the last slide.",
            "I just wanted to show you that you know this is exact.",
            "Let's say this is the Yoshi's Island.",
            "Work on on contrastive sampling based log likelihood estimators.",
            "You can see that it's.",
            "It's doing much worse.",
            "That's the only estimated that we found.",
            "It also provides a decent lower bound 'cause all the other estimators like you know, coming from variational bounds like 3 weighted belief propagation for example."
        ],
        [
            "Away off man.",
            "Can see that the difference is quite small, so you actually get two of these estimators and you can judge how good how good your estimator is, which is which is quite quite important and you can do the same thing for Diebold's machines for deep belief Nets and we always see this kind of behavior that you cannot fool yourself too much into believing that your model is actually better than what what it really is, and this is important for for evaluation, and it also brings us to the notion of Helmholtz machines."
        ],
        [
            "Yes, and that's going to be that's going to be this afternoon, and now I'll stop here.",
            "Which is to say that you know this, these kind of ideas come to the notion of you have these directed models.",
            "You have a generative model.",
            "We have a recognition model, so how can you do the learning in these kinds of models?",
            "And this is work.",
            "Bye bye.",
            "You're an I'm done for the first part of the tutorial."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "Coming here on Saturday.",
                    "label": 0
                },
                {
                    "sent": "I just want to point out also that for students here who want to do particular people who want to do PhD, I'm.",
                    "label": 0
                },
                {
                    "sent": "Going to be moving to CMU and building my group there so you know, talk to me if you're if you're interested.",
                    "label": 0
                },
                {
                    "sent": "OK so I was thinking about what I should be talking about in this summer school and what I'm going to do is the first part of the lecture.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about some of the older work that my lab has been doing, but a little bit more technical, and then the second part you will see some of the recent work that's been happening in my lab as well as in other labs, so you'll get to see a little bit of recent exciting projects, OK?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, I would like to think a lot of my students a lot of work that I'm going to be talking about here is really the work of my fantastic colleagues, some of whom are already here.",
                    "label": 0
                },
                {
                    "sent": "And I'll point out different pieces of what students are doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start by sort of you've probably seen this before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, if you've seen the datasets that we have right, it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's amazing and I think that what we're trying to do is when I'm going to be talking about mostly in this tutorial is I'm going to be talking about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where you know, for images of for taxes, very easy to get these datasets, but it's somehow it's often very hard to get labels right.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to get people to annotate what's going on in the images, so how can we develop models?",
                    "label": 0
                },
                {
                    "sent": "That I capable of discovering, structuring, how we can do it in unsupervised or semi supervised way.",
                    "label": 1
                },
                {
                    "sent": "And as well as how can we apply these models in a lot of different application domains as being in the machine learning field?",
                    "label": 0
                },
                {
                    "sent": "It's very exciting to see how we can develop systems that work across multiple domains.",
                    "label": 0
                },
                {
                    "sent": "And what I would argue is that one particular framework for doing this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is is learning these hierarchical models these deep learning models or models that discover structured multiple at multiple levels and I'm going to make it a little bit more precise as I go through the talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's one example.",
                    "label": 0
                },
                {
                    "sent": "This is an older example.",
                    "label": 0
                },
                {
                    "sent": "It goes back to 2006, where if you take bag of words representation something very simple.",
                    "label": 1
                },
                {
                    "sent": "Can you take a Reuters data set which is about 800,000 news stories?",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "Web pages on the Reuters news feeds an nobody tells you about what this story is about, right?",
                    "label": 0
                },
                {
                    "sent": "Nobody tells you that what the label for this story is, and if you just project these bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "So these stories into 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is what the model is discovering.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's sort of finds interesting structure in the data, so you know you can visualize what's going on in the data.",
                    "label": 0
                },
                {
                    "sent": "You can find similar things this was done.",
                    "label": 1
                },
                {
                    "sent": "This data set was collected back in 96 and it's kind of interesting that it puts European economic policies next to disasters and accidents, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So I guess in today's in today's world it's you know, these two spikes are going to be even closer, right?",
                    "label": 0
                },
                {
                    "sent": "So here is an A.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, right?",
                    "label": 0
                },
                {
                    "sent": "So what you can do?",
                    "label": 0
                },
                {
                    "sent": "You know these are the systems where you can take these images and you can say can you tag these images?",
                    "label": 0
                },
                {
                    "sent": "Can you see what's going on in those images or the other way around?",
                    "label": 0
                },
                {
                    "sent": "Given the word, can you find similar images right?",
                    "label": 0
                },
                {
                    "sent": "And then you know you can go a little.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Beyond that, you can say can you build a system that actually has some understanding of what's going on in the images?",
                    "label": 0
                },
                {
                    "sent": "So here's a here's a picture of Antonio Torralba.",
                    "label": 0
                },
                {
                    "sent": "Do you know who Antonio Torralba is?",
                    "label": 0
                },
                {
                    "sent": "He's a Prophet.",
                    "label": 0
                },
                {
                    "sent": "MIT is a vision guy.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, can you tag this person?",
                    "label": 0
                },
                {
                    "sent": "And this is a picture.",
                    "label": 0
                },
                {
                    "sent": "Took him on my cell phone at Nips.",
                    "label": 0
                },
                {
                    "sent": "Which was in Montreal, which is actually here, so you know if you ask the system to tag it.",
                    "label": 0
                },
                {
                    "sent": "This is what.",
                    "label": 0
                },
                {
                    "sent": "No, the tags you provide, which is sort of reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "Sort of sees a lot of people, but then you can say, well, can I now build a system that actually describes in natural language what's going on in this image, right?",
                    "label": 0
                },
                {
                    "sent": "And one way of doing this?",
                    "label": 0
                },
                {
                    "sent": "I think the cheating way of doing this would be to, say find a similar image in your training set and copy the caption or copy the description.",
                    "label": 0
                },
                {
                    "sent": "And this is what the system does.",
                    "label": 0
                },
                {
                    "sent": "People taking pictures of a crazy person.",
                    "label": 1
                },
                {
                    "sent": "So this is true.",
                    "label": 0
                },
                {
                    "sent": "I we have an app on the phone, you can do it and I'm telling you really liked it.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know that that description could probably go along with a lot of different pictures, right?",
                    "label": 0
                },
                {
                    "sent": "But then you can also build the model and say, can you actually build a model, generates the sentences and this is what the system does.",
                    "label": 0
                },
                {
                    "sent": "A group of people in a crowded area, group of people that are working in talking a group of people that are in the outside.",
                    "label": 1
                },
                {
                    "sent": "So obviously here you can see syntactically it's kind of a weird sentence, but you can see that you know these systems are doing reasonably well in these samples from the model.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about that particular model in the second half of the tutorial, but I just wanted to give you a little bit of a preview.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's just wanted to show you.",
                    "label": 0
                },
                {
                    "sent": "Here's what these kinds of systems can do, which is quite.",
                    "label": 0
                },
                {
                    "sent": "Remarkable, you know, like a car is parked in the middle of nowhere or a little boy with a bunch of friends on the street where there's a cat sitting on the shelf.",
                    "label": 0
                },
                {
                    "sent": "You know these look a little bit too good, right?",
                    "label": 0
                },
                {
                    "sent": "If you look at these things just way too good, then second half of this tutorial I'm going to show you some failure examples and you will see that you know these systems are not on par with human performance yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the first part of the tutorial, I'm going to give you some basics on Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "In particular, I'm going to talk about restricted.",
                    "label": 0
                },
                {
                    "sent": "Both machines did both machines, and then I'm going to focus on couple of topics.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm going to talk about multimodal, debossed machine is 1 particular application of these models.",
                    "label": 0
                },
                {
                    "sent": "An in the last part of the tutorial I'm going to talk about how we can evaluate these models.",
                    "label": 0
                },
                {
                    "sent": "And that will get a little bit more technical, but I'm hoping that I'll give you some flavor of what we can do there, yeah.",
                    "label": 0
                },
                {
                    "sent": "Perfect.",
                    "label": 0
                },
                {
                    "sent": "Perfect, that's that's great, so I'm going to go fast on that, but let me first start with the following.",
                    "label": 0
                },
                {
                    "sent": "You've probably seen this, but I can just go very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the notion of learning representations right, let's say I'm trying to classify that image.",
                    "label": 0
                },
                {
                    "sent": "If I take these two pixels and I say, well, based on these two pixels, can I classify what's going on here?",
                    "label": 0
                },
                {
                    "sent": "It's very hard right?",
                    "label": 0
                },
                {
                    "sent": "So a lot of us in particular application domains we try to come up with the right representations, right?",
                    "label": 0
                },
                {
                    "sent": "If you can, basically.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figure out there is a will and there is a handle and that's the representation of features.",
                    "label": 0
                },
                {
                    "sent": "Then you can build a reasonable classifier, right?",
                    "label": 0
                },
                {
                    "sent": "So if you look at a lot of applications that make speech vision, that box really matters the most.",
                    "label": 0
                },
                {
                    "sent": "How do you find these representations right and you know?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at sort of traditional approaches data, you do some feature extraction.",
                    "label": 1
                },
                {
                    "sent": "You apply your favorite learning algorithm right, and that's a paradigm.",
                    "label": 1
                },
                {
                    "sent": "Probably some of you have worked with.",
                    "label": 0
                },
                {
                    "sent": "You know, in object detection these hog features then you recognizing what that is and classification these MFC features and then you can recognize who that is right and the interest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pieces that you know in the vision domain computer vision domain.",
                    "label": 1
                },
                {
                    "sent": "If you look at the space of features, space is pretty big.",
                    "label": 0
                },
                {
                    "sent": "You know people are trying to figure out where trying to figure out what's the right representations to use.",
                    "label": 0
                },
                {
                    "sent": "Sift was a very influential work by David Lowe at UBC, which basically moved that field ahead for about a decade.",
                    "label": 0
                },
                {
                    "sent": "Everybody were using Sift for doing recognition and it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Without that, you actually don't need that.",
                    "label": 0
                },
                {
                    "sent": "Given enough data, you can learn what the right representations are.",
                    "label": 0
                },
                {
                    "sent": "In fact, now in most of the computer vision domain people are using convolutional neural Nets to find what the right representations are.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the speech, the same thing happens.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There you basically don't need that.",
                    "label": 0
                },
                {
                    "sent": "Seems that these approaches are able to learn what the right representations should be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly go over restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "You've seen this before, so I'm going to go very quickly.",
                    "label": 0
                },
                {
                    "sent": "You can specify just just for notational convenience.",
                    "label": 0
                },
                {
                    "sent": "V's here are the observed variables.",
                    "label": 0
                },
                {
                    "sent": "These stochastic binary variables H is are the stochastic hidden variables.",
                    "label": 0
                },
                {
                    "sent": "You can think of HSS detecting certain features, certain patterns that you see in the data, right?",
                    "label": 0
                },
                {
                    "sent": "The conditionals here are given by the product of these marginals and effectively what this is saying is that if you tell me what features you see in the data, I can generate the image for you right?",
                    "label": 0
                },
                {
                    "sent": "Because that defines that distribution here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is what you can learn with these things.",
                    "label": 0
                },
                {
                    "sent": "If I give you these handwritten characters then I can learn these little.",
                    "label": 0
                },
                {
                    "sent": "Our edge is in the way to think about these models.",
                    "label": 0
                },
                {
                    "sent": "Is that if I give you a new image and these are sort of the right features that the model speaking up and these numbers here are given by the conditional probabilities of these hidden variables being active right?",
                    "label": 0
                },
                {
                    "sent": "So it's just an intuition you should keep in the background.",
                    "label": 0
                },
                {
                    "sent": "You know these models are probabilistic models, but if you know bout sparse coding, I see a lot of different models.",
                    "label": 0
                },
                {
                    "sent": "They roughly trying to do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we do learning in these models?",
                    "label": 0
                },
                {
                    "sent": "I can specify the model, the probability which is given by this expression, and you've seen that from the previous classes.",
                    "label": 0
                },
                {
                    "sent": "I assume you can maximize the likelihood objective.",
                    "label": 0
                },
                {
                    "sent": "It's a natural thing to do, and if you take the derivative of the log likelihood, then you have you do a little bit of math.",
                    "label": 0
                },
                {
                    "sent": "It's not that difficult, but it comes down to being the difference between these two expected sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "So this is expected.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics driven by the data.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this thing you can compute exactly, and the reason why I can compute exactly is because.",
                    "label": 0
                },
                {
                    "sent": "Of particular structure of these models.",
                    "label": 0
                },
                {
                    "sent": "Turns out that you know you can compute this conditional probability, right?",
                    "label": 0
                },
                {
                    "sent": "So believe me, you know this term can be computed exactly.",
                    "label": 0
                },
                {
                    "sent": "This is a more difficult term.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, computing these term requires exponentially exponential sum, right?",
                    "label": 0
                },
                {
                    "sent": "And the way you should think about this is, imagine the space of all possible handwritten characters you can generate, or the space of all possible images you can generate, right?",
                    "label": 0
                },
                {
                    "sent": "You sort of have to.",
                    "label": 0
                },
                {
                    "sent": "Do this sum of all possibilities, and that's an exponential sum.",
                    "label": 0
                },
                {
                    "sent": "So a lot of work in the machine learning community in the statistics communities is trying to figure out how we can approximate these expectations efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this term is easy to compute, right?",
                    "label": 1
                },
                {
                    "sent": "We can do that this term you have to sum over all possible configurations and that cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot compute it and then typically there are approaches.",
                    "label": 1
                },
                {
                    "sent": "Most of the successful approaches are basically using.",
                    "label": 0
                },
                {
                    "sent": "MCMC methods methods like contrastive divergent persistent contrastive divergent and such, so you've seen you've seen those.",
                    "label": 0
                },
                {
                    "sent": "In fact, I should point out here is that we've tried looking at some variational approximations here and unfortunately they don't work, or at least we couldn't get them to work.",
                    "label": 0
                },
                {
                    "sent": "So MCMC is somehow is the most.",
                    "label": 0
                },
                {
                    "sent": "You know empirically best way of fitting.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These models.",
                    "label": 0
                },
                {
                    "sent": "OK, but now the interesting thing about these models is they can extend them.",
                    "label": 0
                },
                {
                    "sent": "You can extend them to dealing with real valued data, right?",
                    "label": 0
                },
                {
                    "sent": "So if you change, you know the definition of the model A little bit.",
                    "label": 0
                },
                {
                    "sent": "There's nothing intrinsically complex here.",
                    "label": 0
                },
                {
                    "sent": "These are pairwise these unary connections, but in this case the conditional is given by the product of Gaussians, right?",
                    "label": 0
                },
                {
                    "sent": "And that's useful when you're modeling real valued data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So you still have stochastic binary hidden variables.",
                    "label": 1
                },
                {
                    "sent": "You have stochastic real valued visible variables, and you can define the model.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does it learn if you fit this model in 4 million unlabeled images?",
                    "label": 1
                },
                {
                    "sent": "This is what the learn features look like.",
                    "label": 0
                },
                {
                    "sent": "This is without using any convolutional type of architecture, but these are sort of filters that the model is learning.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you think about this again, you can say, well, this is a new image and this new image is given by some combination of these filters.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, there's a question.",
                    "label": 0
                },
                {
                    "sent": "Note these are these are 32 by 32 tiny images, so these fit on the full full 32 by 32 images.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also extend these models to dealing with count data.",
                    "label": 0
                },
                {
                    "sent": "Right, so in this case, imagine that you have stochastic one of K variables here, so these are useful for modeling.",
                    "label": 0
                },
                {
                    "sent": "For example, bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to deal with any type of bag of words representation, typically you know dealing with some kind of text.",
                    "label": 0
                },
                {
                    "sent": "Idata so these are stochastic.",
                    "label": 0
                },
                {
                    "sent": "One of the visible variables you can think of these variables as being vocabulary size.",
                    "label": 0
                },
                {
                    "sent": "So in English you know maybe you're dealing with 50,100 thousand.",
                    "label": 0
                },
                {
                    "sent": "OK here would be 100,000 dies.",
                    "label": 0
                },
                {
                    "sent": "The number of words that's appearing in a particular document.",
                    "label": 0
                },
                {
                    "sent": "In this case the conditional probability is given by this softmax distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So you can think of these models as undirected versions of topic models like latent judicial allocation.",
                    "label": 0
                },
                {
                    "sent": "Very sort of similar type of argument.",
                    "label": 0
                },
                {
                    "sent": "You can actually think of these as being multi normal random variables.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now you know if you look at what these models are doing.",
                    "label": 0
                },
                {
                    "sent": "If you fit them to the Reuters data set and you look at these activations of hidden variables, they also discovering sort of pseudo topics.",
                    "label": 0
                },
                {
                    "sent": "Right and the way you can think about what's happening here intuitively is that for every single document.",
                    "label": 0
                },
                {
                    "sent": "Every single document is made up by some combination of these topics, right?",
                    "label": 0
                },
                {
                    "sent": "So it's interesting because for images you say here's an image, and it's made up of these filters or these sort of edges.",
                    "label": 0
                },
                {
                    "sent": "For documents it sort of finds topics as sort of a first level representation, so it's kind of interesting that in different domains you finding these interesting you finding this interesting structure.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so you can deal with binary with Gaussian with count type of data softmax.",
                    "label": 0
                },
                {
                    "sent": "All of these models will have binary random variables, but they can use for modeling different kinds of data.",
                    "label": 1
                },
                {
                    "sent": "In fact, there's a work by I think it was back in 2006 or 2007 by Max Swelling in his group, where they generalize these models to other members within exponential family.",
                    "label": 0
                },
                {
                    "sent": "So if you're dealing with.",
                    "label": 0
                },
                {
                    "sent": "Different data modalities.",
                    "label": 0
                },
                {
                    "sent": "You can adjust these models.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "So the question is how many you know?",
                    "label": 0
                },
                {
                    "sent": "We think about the number of hidden units you know.",
                    "label": 0
                },
                {
                    "sent": "If you typically depends on the application.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in visualization or trying to visualize what's going on, or some compression, and you have smaller.",
                    "label": 0
                },
                {
                    "sent": "In other cases, if you want to do retrieval or classification, you typically have overcomplete representation, so the number of hidden units is actually bigger than.",
                    "label": 0
                },
                {
                    "sent": "The original size of the data.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll get to that.",
                    "label": 0
                },
                {
                    "sent": "I'll get to that.",
                    "label": 1
                },
                {
                    "sent": "So in these models, one of the advantage of these models is that it's very easy to infer the states of the hidden variables, right can be done in closed form, so this is effectively telling you that if I give you a document, I can tell you which what topics you see in a document, right?",
                    "label": 0
                },
                {
                    "sent": "Or if you give me an image, I can tell you what features you see in the image, right?",
                    "label": 0
                },
                {
                    "sent": "And this is unlike some of the other kinds of models like.",
                    "label": 0
                },
                {
                    "sent": "Directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "Bayesian models where given the data, we have to do a little bit of work to figure out what's the distribution over the latent variables, right?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now sometimes these models are called product models called product of experts and the reason for that is if you look at the joint distribution of observed and hidden variables, has this form right?",
                    "label": 1
                },
                {
                    "sent": "It's a log linear model.",
                    "label": 0
                },
                {
                    "sent": "But if you moduli's over the hidden variables and you can do it explicitly in these models, you have this interesting form, so we have a something of the form product of bunch of things, right?",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Let me give you an intuition as to what what, what is happening here.",
                    "label": 0
                },
                {
                    "sent": "Suppose I'm dealing with the.",
                    "label": 0
                },
                {
                    "sent": "Text data.",
                    "label": 0
                },
                {
                    "sent": "And I'm finding these topics.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm discovering and suppose I tell you that in my document I'm seeing the topic government corruption and oil.",
                    "label": 1
                },
                {
                    "sent": "Then the word Putin will have very high probability.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And this is, and that's what makes it different.",
                    "label": 0
                },
                {
                    "sent": "It makes a big difference when you, when you're looking at models like for example topic models like Late English location models, right?",
                    "label": 0
                },
                {
                    "sent": "So here you taking these distributions over words, you have three topics.",
                    "label": 0
                },
                {
                    "sent": "And these are sort of broad distributions, but then you multiplying them together and re normalizing this is this exactly what you're doing here.",
                    "label": 0
                },
                {
                    "sent": "So if you have three things in this case 3 topics and you multiplying them together right, you can imagine that the distribution is going to be spiky.",
                    "label": 0
                },
                {
                    "sent": "And that distribution can model very precisely what kind of words you expecting to see in a document, right?",
                    "label": 0
                },
                {
                    "sent": "Like you know, if I tell you that I'm seeing these three topics, you know, maybe Hugo Chavez will show up in here right?",
                    "label": 0
                },
                {
                    "sent": "Compared to LDA type of models, because in traditional topic models, the way you generating the data is you picking a topic, you generating a word, you picking another topic, you generating a word right?",
                    "label": 0
                },
                {
                    "sent": "So these admixture models you can think of them as mixture models, just which is very different from these models.",
                    "label": 0
                },
                {
                    "sent": "So, so the claim here is that these models can more precisely model.",
                    "label": 0
                },
                {
                    "sent": "Model the data an in fact, if you're using these models for doing retrieval, can you find similar documents?",
                    "label": 0
                },
                {
                    "sent": "You know these models work much better than traditional topic models.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let me step back a little bit.",
                    "label": 0
                },
                {
                    "sent": "Yep, there's a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it depends.",
                    "label": 0
                },
                {
                    "sent": "It depends on the implementation.",
                    "label": 0
                },
                {
                    "sent": "Depends on the implementation.",
                    "label": 0
                },
                {
                    "sent": "These models, the GBM models.",
                    "label": 0
                },
                {
                    "sent": "Maybe training at training time.",
                    "label": 0
                },
                {
                    "sent": "They little bit tougher to train.",
                    "label": 0
                },
                {
                    "sent": "That's true, but at the same time if you look at, you know other models like latent semantic analysis, just sort of or.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I guess it depends on the implementation.",
                    "label": 0
                },
                {
                    "sent": "Can make the very fast implementation of these models.",
                    "label": 0
                },
                {
                    "sent": "You can also make very fast implementation of LDA models and such.",
                    "label": 0
                },
                {
                    "sent": "One advantage of LDA models is that sometimes it gives you more interpretable topics and people like that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the topics that you get in these undirected models don't carry the interpretability.",
                    "label": 0
                },
                {
                    "sent": "Right, because you can imagine that each single hidden unit sort of defines a broad distribution of awards, and it's the intersection of these distributions that define a particular topic.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me.",
                    "label": 0
                },
                {
                    "sent": "Let me jump into a divorce machine model.",
                    "label": 0
                },
                {
                    "sent": "And maybe give you a little bit of intuition how we can learn these models OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've shown you that if we actually learning, you know these PBM type of models.",
                    "label": 0
                },
                {
                    "sent": "They can capture certain low level structure, right?",
                    "label": 0
                },
                {
                    "sent": "You can capture these little edges, or if we're looking at bag of words, we typically capturing some correlations between words, right?",
                    "label": 0
                },
                {
                    "sent": "But the results obviously need to go beyond that right?",
                    "label": 0
                },
                {
                    "sent": "And the hope is that, you know, we can try to learn some high level representation.",
                    "label": 0
                },
                {
                    "sent": "By combining these, these low level features.",
                    "label": 0
                },
                {
                    "sent": "Right and we can learn these simple representation and then maybe we can learn something that a little bit more complex, right?",
                    "label": 0
                },
                {
                    "sent": "That's that's the hope behind these models and then we can learn these systems in completely unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "So how can we do?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do that well.",
                    "label": 0
                },
                {
                    "sent": "I can write down the probability distribution over the entire system, so here you can think of this model as just a Markov random field with hidden variables and you have multiple levels of hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Right and you introducing dependencies between hidden variables, right?",
                    "label": 1
                },
                {
                    "sent": "That's the key.",
                    "label": 0
                },
                {
                    "sent": "And all connections here are undirected and this is very different from models like deep belief networks, for example.",
                    "label": 0
                },
                {
                    "sent": "Which hybrid models that are combining directed and undirected models?",
                    "label": 0
                },
                {
                    "sent": "I think that they probably somebody was talking about deep belief networks in this in the summer school.",
                    "label": 0
                },
                {
                    "sent": "So what changed from before?",
                    "label": 0
                },
                {
                    "sent": "Well, this is what we had when we were defining these simple GBM models.",
                    "label": 0
                },
                {
                    "sent": "And now you have these two terms, two additional terms and these two additional terms are modeling H1 the dependencies within H1 and H2 here, and H2 and H3 here.",
                    "label": 1
                },
                {
                    "sent": "Right and all connections are undirected.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can also combine bottom up and top down right so you can say what's the probability of this hidden variable.",
                    "label": 0
                },
                {
                    "sent": "The probability of this hidden variable is given by the logistic function, and this is what's coming from above and what's coming from below, right?",
                    "label": 0
                },
                {
                    "sent": "So we have this natural notion of things going in both directions.",
                    "label": 0
                },
                {
                    "sent": "And this is and obviously in this case the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "The problem with this model is that the hidden variables become dependent even when you condition on the input right?",
                    "label": 1
                },
                {
                    "sent": "So conditional on the input you have this complicated structure here, so it's one of those kinds of problems that you will see a lot in machine learning.",
                    "label": 0
                },
                {
                    "sent": "You can you can deal with a similar model, or you can extend this.",
                    "label": 0
                },
                {
                    "sent": "You know you can extend the model and make it more flexible.",
                    "label": 0
                },
                {
                    "sent": "More powerful, but often you have to sort of.",
                    "label": 0
                },
                {
                    "sent": "No computing or doing inference in these models become problematic, so we have to do something something there.",
                    "label": 0
                },
                {
                    "sent": "And how does it different from other models will compare to?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models like neural networks, for example, right?",
                    "label": 0
                },
                {
                    "sent": "The way you can think about neural networks is that given the input, you have an output in the entire system is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you think about convolutional neural networks or other kinds of neural networks, the way you're doing it is that given the data, everything is deterministic, and then you're predicting the class, whereas these systems are stochastic.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "If you look at the belief network.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know this is an RBM sitting on top, followed down by the sigmoid belief network.",
                    "label": 0
                },
                {
                    "sent": "That's the definition of the belief network.",
                    "label": 1
                },
                {
                    "sent": "The problem with deep belief networks in principle is that if you want to do proper inference in those models, it's very hard.",
                    "label": 0
                },
                {
                    "sent": "It's very hard because of explaining away.",
                    "label": 0
                },
                {
                    "sent": "Right, in order to figure out what's the probability of this hidden unit.",
                    "label": 0
                },
                {
                    "sent": "For example, it depends on what's below what's above, as well as on the neighbors, so it's generally difficult to do inference in these models.",
                    "label": 0
                },
                {
                    "sent": "So what happens in the belief network type of models is that you just converting them into a neural net model, and that's how your inference is done.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can give you some intuitions of how we can do learning in these systems.",
                    "label": 0
                },
                {
                    "sent": "If you look at the maximum likelihood learning in the system, you basically matching these two expected sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "And that's basically any type you do anytime you're dealing with undirected graphical models.",
                    "label": 1
                },
                {
                    "sent": "Conditional random fields factor graphs.",
                    "label": 0
                },
                {
                    "sent": "You basically have in one way or another you're coming down to this equation.",
                    "label": 0
                },
                {
                    "sent": "Right, so this equation is again saying maybe the intuition here is that you have.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spectra sufficient statistics driven by the data and expected sufficient cystic driven by the model, and we can think about the first term is you basically looking at correlations driven by the data.",
                    "label": 0
                },
                {
                    "sent": "So you have a date and you look at what's the correlations you see in the data and this is what the model believes.",
                    "label": 0
                },
                {
                    "sent": "Those correlations should be right and you're trying to match the two.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition.",
                    "label": 0
                },
                {
                    "sent": "And the problem obviously here is that both expectations are intractable, so we have to do something about that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So both expectations are intractable, but maybe I can give you a reason for that is again, if we look at this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Additional probability we say, given the data, what's the probability of this hidden variable being active?",
                    "label": 0
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot longer compute it, because this probability of this hidden unit being active depends.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On all of these guys.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So inference becomes becomes problematic, but maybe I can give you an intuition of what these two expectations are doing.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have the data and let's say you get to see these handwritten characters.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So one thing that you can do is you should do is you should say these things should be more probable, right?",
                    "label": 0
                },
                {
                    "sent": "You've seen them, they're real data, so you want to raise up the probability of those observing these data points right?",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "If you look at this image.",
                    "label": 0
                },
                {
                    "sent": "Right, you want to say the probability of this image should be very very small, right?",
                    "label": 0
                },
                {
                    "sent": "And that's what's happening with these models effectively.",
                    "label": 0
                },
                {
                    "sent": "Is that this term here is effectively trying to make these guys be more probable.",
                    "label": 0
                },
                {
                    "sent": "And this term.",
                    "label": 0
                },
                {
                    "sent": "Is trying to basically look at this exponential space and basically push down everywhere.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's where that's the reason why computing this term exactly is problematic in this case.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is 28 by 28 image, it's binary image.",
                    "label": 0
                },
                {
                    "sent": "Some pixels are on, some pixels are off, but that means that there are two to the 784 possible configurations, right?",
                    "label": 0
                },
                {
                    "sent": "And two to the 784 is a huge number.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "I think somebody was mentioning that it's more than the number of particles in the universe by orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "So that basically means that you cannot compute these expectations by brute force.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the intuition.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to do the following.",
                    "label": 0
                },
                {
                    "sent": "We're going to be using variational inference to approximate these expectations, and I'm going to tell you why it makes sense, and we're going to be using stochastic approximation, which is Markov chain Monte Carlo based inference that approximate these expect.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patience.",
                    "label": 0
                },
                {
                    "sent": "Now there's been a lot of work on dealing with these models.",
                    "label": 0
                },
                {
                    "sent": "You know there's much more literature in that space, but the problem is that now when we're dealing with real world applications, we have to deal with thousands of variables.",
                    "label": 0
                },
                {
                    "sent": "It's it's very hard to deal with that system and then if you look at a lot of previous approaches, a lot of previous previous approaches, it's very hard to make them work whenever you have hidden variables or layers of hidden variables.",
                    "label": 1
                },
                {
                    "sent": "So for example, contrastive divergent.",
                    "label": 1
                },
                {
                    "sent": "If you naively applied contrastive divergent to these systems, it just doesn't work, or if you look at score matching there is something called pseudo likelihood models composite likelihood MCMC.",
                    "label": 0
                },
                {
                    "sent": "Emily is a lot of work done in a statistical communities.",
                    "label": 0
                },
                {
                    "sent": "Alot of these approaches unfortunately.",
                    "label": 0
                },
                {
                    "sent": "It's fairly hard to get them to get them working.",
                    "label": 0
                },
                {
                    "sent": "So let me show you what what you can do.",
                    "label": 0
                },
                {
                    "sent": "It's by far not the most perfect algorithm, it's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Offers I'll show you where it fails, but it's a step.",
                    "label": 0
                },
                {
                    "sent": "So what do you need to do?",
                    "label": 0
                },
                {
                    "sent": "Well, the first thing you need to do is you need to do inference, right?",
                    "label": 0
                },
                {
                    "sent": "You need to say given the data, what's the distribution over these hidden variables, right?",
                    "label": 0
                },
                {
                    "sent": "Or what is what are the features that I'm seeing in the data?",
                    "label": 0
                },
                {
                    "sent": "And you may want to do that.",
                    "label": 0
                },
                {
                    "sent": "For example, for you know if you're interested in doing classification or so forth.",
                    "label": 0
                },
                {
                    "sent": "So we need to solve that problem.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, what I can do is I can try to simulate from the model.",
                    "label": 1
                },
                {
                    "sent": "Right, I have the probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "There is an efficient alternating Gibbs sampling right?",
                    "label": 0
                },
                {
                    "sent": "And then maybe I can generate the data from.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Smaller suppose if I generate the data from the model looks like this.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well, I can given the approximate conditional, I can estimate data dependent expectations and if I sample from the model I can approximate data independent expectations.",
                    "label": 0
                },
                {
                    "sent": "Right, and then I'm trying to match the two.",
                    "label": 0
                },
                {
                    "sent": "Right, so the intuition you should have here is that if I simulate from my model and it looks like this.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should be changing the parameters of the model such that the distribution of what I'm seeing here matches.",
                    "label": 0
                },
                {
                    "sent": "The true data distribution or the empirical distribution that I'm seeing, right?",
                    "label": 0
                },
                {
                    "sent": "That's roughly the intuition you should have.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sort of.",
                    "label": 0
                },
                {
                    "sent": "I mean, both of these systems and I'm going to show you the sort of have to do.",
                    "label": 0
                },
                {
                    "sent": "You know, the way you have to do inference in this model, you still propagate information up and down and this you generate.",
                    "label": 0
                },
                {
                    "sent": "But the way to think about this is that you know, given the data you have to do some inference and then you also have to approximate this joint distribution and the way you can do it just by simulating from the model.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And so if you look at this space, that's where the problem comes in.",
                    "label": 0
                },
                {
                    "sent": "In my view, the reason why is because.",
                    "label": 0
                },
                {
                    "sent": "This space is a very complicated space.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of different modes, right?",
                    "label": 0
                },
                {
                    "sent": "Imagine again, the space of all possible characters you can generate, right?",
                    "label": 0
                },
                {
                    "sent": "It's a very complex space, that's why building generative models is really, really hard task, like building a generative model of natural images is very hard.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "If I look at this problem right, if I take a particular input and I slice through that space here, the posterior distribution you expecting this posterior distribution to not be as complex as this one.",
                    "label": 0
                },
                {
                    "sent": "In fact, you know in most of the cases you expecting the posterior distribution to be unimodal.",
                    "label": 0
                },
                {
                    "sent": "Maybe by model.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have a few modes.",
                    "label": 0
                },
                {
                    "sent": "If I show you the image like this you could say, yeah, it could be this, or it could be this we have to different explanations about what you seeing in the data.",
                    "label": 0
                },
                {
                    "sent": "You have two modes right?",
                    "label": 0
                },
                {
                    "sent": "We have 10 different explanations, completely different explanation.",
                    "label": 0
                },
                {
                    "sent": "You have 10 modes, but in general you know.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We expect this to be unimodal, right?",
                    "label": 0
                },
                {
                    "sent": "So what can we do here?",
                    "label": 0
                },
                {
                    "sent": "Well, what we can do is we can use variational inference.",
                    "label": 0
                },
                {
                    "sent": "In particular, we can use mean field approximation to approximate to do this inference here, and I'm going to show you that the variational inference is actually pretty good at capturing a single mode in the posterior distribution, and it's very efficient and we need efficiency at the test time, right?",
                    "label": 0
                },
                {
                    "sent": "And we're going to be using stochastic approximation MCMC based method to actually try to explore this space.",
                    "label": 0
                },
                {
                    "sent": "Right this joint space?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me give you an intuition behind stochastic approximation.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "What is the system do?",
                    "label": 0
                },
                {
                    "sent": "I should point out that this actually the static approximation was developed by.",
                    "label": 0
                },
                {
                    "sent": "Well, one of The Pioneers were Robinson Monroe back in 57.",
                    "label": 0
                },
                {
                    "sent": "And in fact, the interesting thing is that this particular algorithm, which now people call persistent contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "If you've seen it.",
                    "label": 0
                },
                {
                    "sent": "Is actually due to Laurent Yonas.",
                    "label": 0
                },
                {
                    "sent": "So he published it back in 89.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing, I think he published in like French Journal something like that so people didn't know about this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Up until you know, five years ago where people rediscovered what he has done back in the 80s.",
                    "label": 0
                },
                {
                    "sent": "But the algorithm goes as follows.",
                    "label": 0
                },
                {
                    "sent": "You have a state.",
                    "label": 0
                },
                {
                    "sent": "This is the state of the system.",
                    "label": 0
                },
                {
                    "sent": "This is state of the observed pixels or visible variables and hidden variables and you have parameters in your model.",
                    "label": 0
                },
                {
                    "sent": "So you're going to be updating the state in the parameter sequentially.",
                    "label": 0
                },
                {
                    "sent": "So you're going to be generating a state from some transition kernel, like for example doing a Gibbs sampler or Metropolis Hasting, running interesting operator.",
                    "label": 0
                },
                {
                    "sent": "You essentially simulating a Markov chain that leads be Theta septien variant, so again, a Gibbs sampler will do and then what you're doing is that you replacing this intractable expectation with the point estimate that you get out of your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "OK, now in practice you typically simulating several Markov chains in parallel and just replacing expectation by just averaging over those Markov chains.",
                    "label": 1
                },
                {
                    "sent": "So it's a very sort of simple system.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as simulated from the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Approximate the expectation, simulate update the parameters simulator Markov chain, update the parameters so it's a non homogeneous Markov chain because the stationary distribution changes as you updating the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "But in the end you can show certain convergence guarantees right?",
                    "label": 0
                },
                {
                    "sent": "So the way.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The update rule works is that you're replacing these intractable expectations, but what you're getting out of your Markov chain?",
                    "label": 1
                },
                {
                    "sent": "Right, and what you can do is you can do the following.",
                    "label": 0
                },
                {
                    "sent": "You can say well if I add and subtract the true expectations right I have.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm doing right?",
                    "label": 0
                },
                {
                    "sent": "And you can think of this as being a true gradient.",
                    "label": 0
                },
                {
                    "sent": "This is what you want to estimate.",
                    "label": 0
                },
                {
                    "sent": "If you could estimate that, I wouldn't be talking about the algorithm right, and this is the difference between the truth and what you're getting out of your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The problem with this formulation, and that's where a lot of sort of a lot of people were trying to investigate this, is that whatever you're getting out of your Markov chain is not necessarily an unbiased estimator of these expectations.",
                    "label": 0
                },
                {
                    "sent": "So you have to do a little bit more work in order to guarantee convergence of this algorithm, But you can have almost sure convergence guarantees, so you will converge to some stable points as the learning rate goes to zero.",
                    "label": 1
                },
                {
                    "sent": "And if you think about the theory, the theory is kind of makes a lot of sense, but it's also a very theoretical right.",
                    "label": 0
                },
                {
                    "sent": "It's effectively telling you the following it says.",
                    "label": 0
                },
                {
                    "sent": "If my learning rate is small compared to the mixing rate of the Markov chain, my Markov chain eventually will come to the station in distribution and it will be an unbiased estimator of these expectations.",
                    "label": 0
                },
                {
                    "sent": "Think about this follows.",
                    "label": 0
                },
                {
                    "sent": "If I set the learning rate to zero and I just keep running my Markov chain, eventually it will reach stationary distribution once it reaches station distribution after maybe 100 years, you'll get an unbiased estimator here, right?",
                    "label": 0
                },
                {
                    "sent": "So at least theoretically, you can say that.",
                    "label": 0
                },
                {
                    "sent": "Yes, ultimately I will have fun.",
                    "label": 0
                },
                {
                    "sent": "This, you know, certain conditions that will have convergence guarantees, which is good, at least in theoretical standpoint.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in high dimensional spaces you have a very complicated thing that you're trying to model, so then you have to be a little bit smart about how you doing this right and then becomes a little bit more empirical question.",
                    "label": 1
                },
                {
                    "sent": "An one of the insights is that these transition operators can be any valid transition operators like temporal transitions.",
                    "label": 0
                },
                {
                    "sent": "You know there is parallel simulated tempering.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work.",
                    "label": 0
                },
                {
                    "sent": "In, then, in a statistical community that where people are trying to figure out how you can define these Markov chains that can explore these these distributions.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More efficiently, OK?",
                    "label": 0
                },
                {
                    "sent": "Now let's come back to the first part.",
                    "label": 0
                },
                {
                    "sent": "So the first part remember when we're trying to approximate doing this inference here.",
                    "label": 0
                },
                {
                    "sent": "And to do this inference we have to be a little bit smart about how we do it.",
                    "label": 0
                },
                {
                    "sent": "But we also have to keep in mind that we have to do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "Yet there's a question.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes they can obviously.",
                    "label": 0
                },
                {
                    "sent": "So for example, things like if you're working with both machines, the fact that they're structured in layers, you can define, you know a little bit more efficient like a Gibbs sampler becomes a little bit more efficient.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I think the general there is some work.",
                    "label": 0
                },
                {
                    "sent": "There is some theoretical work that can show on the certain conditions.",
                    "label": 0
                },
                {
                    "sent": "Things like parallel tempering with simulated tampering with pronunciations can improve things, but it's a way of of playing with the temperature parameter and to be able to move between the different modes.",
                    "label": 0
                },
                {
                    "sent": "But you know, under certain conditions you can show that they can give you improvements, but in general for these kinds of models I don't think there are.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me quickly tell you bout variational inference and maybe give you an intuition F as to us to what we can do here, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the last part that's going to be very technical.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to show you some results, so bear bear with me.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind variational inference is actually very very simple.",
                    "label": 0
                },
                {
                    "sent": "And a lot of you know a lot of models that we see in machine learning.",
                    "label": 0
                },
                {
                    "sent": "People are using inference variational inference.",
                    "label": 0
                },
                {
                    "sent": "The key idea is quite simple and there's just extensions on this idea.",
                    "label": 0
                },
                {
                    "sent": "It's the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have an intractable distribution.",
                    "label": 0
                },
                {
                    "sent": "Is conditional distribution right?",
                    "label": 0
                },
                {
                    "sent": "You cannot compute this distribution exactly is exponential computations that come in.",
                    "label": 0
                },
                {
                    "sent": "But what if you want approximated with a simple tractable distribution queue?",
                    "label": 0
                },
                {
                    "sent": "Right think of Q as being a Gaussian distribution or fully factorized distribution, or something simple, something that you can specify exactly.",
                    "label": 0
                },
                {
                    "sent": "Here's what you do.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, the look of the marginal.",
                    "label": 0
                },
                {
                    "sent": "Here is the log of this joint.",
                    "label": 0
                },
                {
                    "sent": "You have to sum over hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Right and then what I'm going to do is I'm going to multiply and divide by Q distribution.",
                    "label": 0
                },
                {
                    "sent": "I have to be careful that I don't divide by zero, so there are some technical conditions, but let's leave those out.",
                    "label": 0
                },
                {
                    "sent": "And here is the key.",
                    "label": 0
                },
                {
                    "sent": "What you doing is using Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "You just taking this log and pushing it inside.",
                    "label": 0
                },
                {
                    "sent": "Right, you see log here goes inside.",
                    "label": 0
                },
                {
                    "sent": "So the log of the expectation is always greater than expectation of the log, because the log function is concave function.",
                    "label": 0
                },
                {
                    "sent": "And now what happens for Boltzmann machines?",
                    "label": 0
                },
                {
                    "sent": "Well, what happens for both machines?",
                    "label": 0
                },
                {
                    "sent": "If you take this particular formulation and just write it up?",
                    "label": 0
                },
                {
                    "sent": "Right, you have this term here and this is a normalized probability.",
                    "label": 0
                },
                {
                    "sent": "So this is this entire log is just linear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "This term is called the entropy term of the Q distribution.",
                    "label": 0
                },
                {
                    "sent": "But notice what happens here.",
                    "label": 0
                },
                {
                    "sent": "This is the term that requires exponential computation.",
                    "label": 0
                },
                {
                    "sent": "It's the log of the normalizing constant, right?",
                    "label": 0
                },
                {
                    "sent": "This is something that we cannot compute.",
                    "label": 0
                },
                {
                    "sent": "This is where the problems come.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's very hard to even estimate this thing.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, this thing, and this is known as a variation about this thing is easy to compute.",
                    "label": 0
                },
                {
                    "sent": "This thing is easy to compute.",
                    "label": 0
                },
                {
                    "sent": "Right, so things that depend on the Q distribution are easy to compute and we don't really care about this term here.",
                    "label": 0
                },
                {
                    "sent": "Right, because it's just a constant, it doesn't really doesn't really play.",
                    "label": 0
                },
                {
                    "sent": "You know if we're trying to find what's the best queue, we only need these two terms.",
                    "label": 0
                },
                {
                    "sent": "We don't really care about this one.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you can think about this formulation is you can say, well, it's the same thing as the log of the probability right, which is what we're trying to optimize minus the KL divergent was a cool backliner divergance between Q&P.",
                    "label": 0
                },
                {
                    "sent": "An you can think of KL divergences measuring distance between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "So obviously if Q is the same as P, you can show that this is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's a non negative quantity it so it can be 0 or positive.",
                    "label": 0
                },
                {
                    "sent": "So that's why it's about.",
                    "label": 0
                },
                {
                    "sent": "But it has sort of intuitive interpretation.",
                    "label": 0
                },
                {
                    "sent": "You basically saying I want to push on this variational bound.",
                    "label": 0
                },
                {
                    "sent": "I want this term to be 0 fine McHugh.",
                    "label": 0
                },
                {
                    "sent": "That's as close as possible to P according to the KL Divergent according to this particular.",
                    "label": 0
                },
                {
                    "sent": "Measure of distance between the two distributions.",
                    "label": 0
                },
                {
                    "sent": "And we can maximize the KL between the approximate and the true right?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we can do here is now you can say well, what's the form of the Q distribution, and that's where a lot of different formulations come in, right?",
                    "label": 0
                },
                {
                    "sent": "How do we define what the Q is?",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Can you make some small approximations about Q and so forth?",
                    "label": 0
                },
                {
                    "sent": "In our case, we can just choose a fully factorized distribution, essentially breaking the links in the model.",
                    "label": 1
                },
                {
                    "sent": "We can say our approximation is just product of bunch of marginals or these conditionals.",
                    "label": 0
                },
                {
                    "sent": "And then we can just maximize the low bond with respect to these variational parameters, and essentially what's happening here is that if this is your posterior, then the mean field will find a single mode.",
                    "label": 0
                },
                {
                    "sent": "We can find this mode, unfortunately, nonconvex problems.",
                    "label": 0
                },
                {
                    "sent": "So whatever mode you find you find.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you just derive what the updates are, it's a very simple, simple set of updates.",
                    "label": 0
                },
                {
                    "sent": "You just cycling through these signal of nonlinear equations until you converge.",
                    "label": 0
                },
                {
                    "sent": "Typically, after five iterations you know you do converge.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the approximation that we do.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you doing variational inference you maximizing the lower bound with respective variational parameters.",
                    "label": 1
                },
                {
                    "sent": "You doing MCMC.",
                    "label": 0
                },
                {
                    "sent": "To update the model parameters and then you can sort of play this theory game by saying that, well, asymptotically you guaranteed to again come to the stable point of the variational lower bound, right?",
                    "label": 1
                },
                {
                    "sent": "The whole theory theory goes through.",
                    "label": 0
                },
                {
                    "sent": "So we have fast inference.",
                    "label": 0
                },
                {
                    "sent": "Learning can scale to millions of examples.",
                    "label": 0
                },
                {
                    "sent": "The big question is that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does it actually work?",
                    "label": 0
                },
                {
                    "sent": "Right, so so that's $1,000,000 question, but let me.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you.",
                    "label": 0
                },
                {
                    "sent": "Let me let me.",
                    "label": 0
                },
                {
                    "sent": "Let you judge whether this is a good model or not.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to show you is I'm going to show you two panels on one panel.",
                    "label": 0
                },
                {
                    "sent": "You will see the real data on the other panel.",
                    "label": 0
                },
                {
                    "sent": "You will see simulated data and some of you have seen that.",
                    "label": 0
                },
                {
                    "sent": "So if you've seen that.",
                    "label": 0
                },
                {
                    "sent": "Don't raise your hands.",
                    "label": 0
                },
                {
                    "sent": "Right and you basically have to judge.",
                    "label": 0
                },
                {
                    "sent": "So in one case you will see the real data on another one you'll see simulated data data simulated from the model.",
                    "label": 0
                },
                {
                    "sent": "You have to tell me which one is which, so these are handwritten characters coming from 50 different alphabets around the world.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you thought this was simulated, this was real, honestly.",
                    "label": 0
                },
                {
                    "sent": "Good, what about the other way around?",
                    "label": 0
                },
                {
                    "sent": "Perfect 5050 is great.",
                    "label": 0
                },
                {
                    "sent": "Now, if you actually look at the data a little bit more, you will start seeing the difference.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is simulated.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "So for example if you look at this character right.",
                    "label": 0
                },
                {
                    "sent": "It's just not quite right.",
                    "label": 0
                },
                {
                    "sent": "There's certain things that are missing also.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the real data.",
                    "label": 0
                },
                {
                    "sent": "The characters that you see there's much more diversity in the real data, right?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that has to do with inability of the Markov chain to explore this space.",
                    "label": 0
                },
                {
                    "sent": "Right, but it does capture a lot of interesting structure that you see in the data.",
                    "label": 0
                },
                {
                    "sent": "Right, that's the trick I've learned from neuro scientists.",
                    "label": 0
                },
                {
                    "sent": "If I show you things really fast enough, you won't tell the difference, right?",
                    "label": 0
                },
                {
                    "sent": "So, but but it does learn you know, interesting.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure that you see in the data here is.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "It's a local optimum.",
                    "label": 0
                },
                {
                    "sent": "So what what it finds like?",
                    "label": 0
                },
                {
                    "sent": "Obviously you know these ones?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "You look at characters like.",
                    "label": 0
                },
                {
                    "sent": "You know these ones.",
                    "label": 0
                },
                {
                    "sent": "These are less complicated or these ones.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other data.",
                    "label": 0
                },
                {
                    "sent": "This is the M news data set.",
                    "label": 0
                },
                {
                    "sent": "Probably most of you have seen this.",
                    "label": 0
                },
                {
                    "sent": "How many?",
                    "label": 0
                },
                {
                    "sent": "How many of you think this is simulated?",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "What about the other way around?",
                    "label": 0
                },
                {
                    "sent": "Oh wow, you guys are bad.",
                    "label": 0
                },
                {
                    "sent": "Most of the audience that I that I talked to about they really can't tell the difference.",
                    "label": 0
                },
                {
                    "sent": "'cause a lot of the lot of us are working on these data set which is sort of a toy data set.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "This is simulated.",
                    "label": 0
                },
                {
                    "sent": "Is the zero throwing you off?",
                    "label": 0
                },
                {
                    "sent": "Some people told me this zero throws you off, but I promise I did not put the zero in there.",
                    "label": 0
                },
                {
                    "sent": "Just took a random subset and then.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so here.",
                    "label": 0
                },
                {
                    "sent": "For example, if you look at this three, if you look at this five right, it's just not quite right.",
                    "label": 0
                },
                {
                    "sent": "You you will be able to tell the difference.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in terms of, you know if you now do the proper thing, you can say well, given the new test data and you find the representation, can you actually figure out?",
                    "label": 0
                },
                {
                    "sent": "Can you do?",
                    "label": 0
                },
                {
                    "sent": "Can written character recognition can you can do optical character recognition and so forth so these models do work reasonably well.",
                    "label": 0
                },
                {
                    "sent": "So for these types of tasks, what we effectively doing is we are inferring the distribution of the hidden variables and then you fitting logistic on top.",
                    "label": 0
                },
                {
                    "sent": "Right and you can back propagate through entire system, but the details they don't matter that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is another example of.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can see that this is simulated.",
                    "label": 0
                },
                {
                    "sent": "This is real, so these are 3D objects.",
                    "label": 1
                },
                {
                    "sent": "It does sort of find interesting things.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at this guy here, kind of looks like a car with wings, right?",
                    "label": 0
                },
                {
                    "sent": "You can sort of see that, So what happens here is that when it transitions from one mode to another mode simulates these things so.",
                    "label": 0
                },
                {
                    "sent": "Of course you can also see that they're a little bit imbalanced, so these are just generates one donkey, one elephant, but generates too many people with guns, right?",
                    "label": 0
                },
                {
                    "sent": "Like if you look at here here, here.",
                    "label": 0
                },
                {
                    "sent": "So it's just has a little bit of puts the mass disproportionally on some classes, and again it has to do with the fact that it's very hard to calibrate these.",
                    "label": 0
                },
                {
                    "sent": "These",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can also do fun things like padding completion, right?",
                    "label": 0
                },
                {
                    "sent": "So if I show you these images, half of these images, this is what the remaining ones look like.",
                    "label": 0
                },
                {
                    "sent": "And the interesting, and this is the truth and the interesting thing here is that these objects here are the test objects, so you never see, for example, you never see cowboy.",
                    "label": 0
                },
                {
                    "sent": "At the training time, right, you only see cowboy the test time, but the model is actually, you know, kind of figures out that there is a lag in an arm and a some completion.",
                    "label": 0
                },
                {
                    "sent": "I should have also put what the nearest neighbors looks like and there are some examples where I think for.",
                    "label": 0
                },
                {
                    "sent": "This example no for this example thinks it's a heap on just completes the remaining fine as a hippo.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the nearest neighbor type of approaches, they work a little bit worse than these systems.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me step back and talk about how we can a little bit more of a let me just.",
                    "label": 0
                },
                {
                    "sent": "Time wise, how we can apply these models and this is work with new tissue estava.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me just step back a little bit and say, well, if we looking at.",
                    "label": 0
                },
                {
                    "sent": "You know multimedia content on the web.",
                    "label": 1
                },
                {
                    "sent": "It's we always deal with not a single modality, not just images or not just tax.",
                    "label": 0
                },
                {
                    "sent": "It's typically combination of the two, or if you look at robotics applications, a lot of things coming in.",
                    "label": 0
                },
                {
                    "sent": "So how can we apply or build models that can take advantage of these different data modalities, right?",
                    "label": 0
                },
                {
                    "sent": "And maybe what we can do is we can try.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To build a system that goes from images and tags into some some representation of both, right?",
                    "label": 0
                },
                {
                    "sent": "Are and why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Well, you can do multiple things right.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, given the image and given some tags, we can improve classification.",
                    "label": 0
                },
                {
                    "sent": "People have shown that that's generally true even if the tags that you see are noisy.",
                    "label": 0
                },
                {
                    "sent": "You can fill in missing modalities given the image.",
                    "label": 1
                },
                {
                    "sent": "Maybe you can generate tags associated with the with the image or the other way around given some tags, can you retrieve images?",
                    "label": 0
                },
                {
                    "sent": "Right, so if you're building a probabilistic model that models both of these data modalities, you can do all of these things.",
                    "label": 0
                },
                {
                    "sent": "What is?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some challenges, well, one of the big challenges that typically images in text.",
                    "label": 0
                },
                {
                    "sent": "They have very different representations, right?",
                    "label": 1
                },
                {
                    "sent": "If you look at images, you have dense representation.",
                    "label": 0
                },
                {
                    "sent": "We look at pixels, or even if you look at some features that you constructing from images.",
                    "label": 0
                },
                {
                    "sent": "If you look at text, text is much more.",
                    "label": 0
                },
                {
                    "sent": "Complex words mean things, right?",
                    "label": 0
                },
                {
                    "sent": "But in terms of representation, typically sparse.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult to learn these cross model features just based on those representations.",
                    "label": 1
                },
                {
                    "sent": "What's happening in what was happening in our communities that people would just concatenate these features and trying to learn something on top.",
                    "label": 0
                },
                {
                    "sent": "That's the general approach.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second challenge is that typically.",
                    "label": 0
                },
                {
                    "sent": "We have very noisy or missing data, right?",
                    "label": 1
                },
                {
                    "sent": "So in this case, sometimes the modality is missing completely.",
                    "label": 0
                },
                {
                    "sent": "I show you this picture with no tags.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's very noisy.",
                    "label": 0
                },
                {
                    "sent": "And we have to build.",
                    "label": 0
                },
                {
                    "sent": "I believe we have to build some form of generative model that allows us to deal with these.",
                    "label": 0
                },
                {
                    "sent": "Deal with these inconsistencies.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These attacks, this is the text generated by the model, right?",
                    "label": 1
                },
                {
                    "sent": "So it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Does reasonably reasonably well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What can we do here?",
                    "label": 0
                },
                {
                    "sent": "Well, we can build a model.",
                    "label": 0
                },
                {
                    "sent": "We know how to deal with real valid data.",
                    "label": 0
                },
                {
                    "sent": "We know how to deal with count data.",
                    "label": 0
                },
                {
                    "sent": "We can just build a single model here.",
                    "label": 0
                },
                {
                    "sent": "But again, because of this, data modalities data inputs have very different statistical properties.",
                    "label": 1
                },
                {
                    "sent": "It's very difficult to learn a small.",
                    "label": 0
                },
                {
                    "sent": "I think this model was already proposed by a few people back in 2000.",
                    "label": 0
                },
                {
                    "sent": "Four 2005 and such, and it's sort of kind of work, but never really worked.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we can do is we can do the following, we can.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is build a hierarchy right so we can say take these word representation, push them up, take the images, push them up and then try to model some?",
                    "label": 0
                },
                {
                    "sent": "Um, correlation between these two different data modalities, right?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is very natural notion of bottom up and top down in these models like an abortion machine models because.",
                    "label": 0
                },
                {
                    "sent": "It's effectively what is saying is that these words can essentially affect the features the low level features of images, and the other way around.",
                    "label": 0
                },
                {
                    "sent": "So the information flows up and down in this model.",
                    "label": 0
                },
                {
                    "sent": "During inference, when you running mean field approximation, you basically propagating information up and down up until you settle to some to some joint state.",
                    "label": 0
                },
                {
                    "sent": "And you can obviously deal with the missing modality, because if the modality is missing and we're building the probabilistic model of both of these data distributions, one of them is missing.",
                    "label": 0
                },
                {
                    "sent": "We just integrating it out.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can write down the proper question.",
                    "label": 0
                },
                {
                    "sent": "Just confirm what?",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "That's right, so the entire model is being trained jointly.",
                    "label": 0
                },
                {
                    "sent": "All layers are being trained jointly, not just one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "And then you know fine tuning.",
                    "label": 0
                },
                {
                    "sent": "So we actually building the joint distribution jointly, yes?",
                    "label": 0
                },
                {
                    "sent": "So you can specify the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The equation looks a little bit scary, but it's very simple actually.",
                    "label": 0
                },
                {
                    "sent": "You have a replicated softmax put pathway.",
                    "label": 0
                },
                {
                    "sent": "You have a Gaussian which pathway and this is just joints both of them together which defines.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Defines a proper proper model, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it's sort of so that becomes a question of like how do you choose the architecture becomes a little bit of exploratory analysis, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, in this case we just said two layers worked OK.",
                    "label": 0
                },
                {
                    "sent": "It's fine.",
                    "label": 0
                },
                {
                    "sent": "I think that in most of the cases you know the architecture can give you the right architecture, can give you some gains, but it's not like you know you're building a model with three layers versus.",
                    "label": 0
                },
                {
                    "sent": "Four layers in the Fuller model gives you huge advantages of a two layer model and so forth.",
                    "label": 0
                },
                {
                    "sent": "I think these architectures are more or less stable, so in this case I think we were using two layers in the third layer, which was sufficient.",
                    "label": 0
                },
                {
                    "sent": "For us up sorry, so let me show you some examples.",
                    "label": 0
                },
                {
                    "sent": "These are sort of examples were given an image right?",
                    "label": 0
                },
                {
                    "sent": "You're looking at the distribution of a words and this is what these look like.",
                    "label": 0
                },
                {
                    "sent": "Sort of finds interesting things.",
                    "label": 0
                },
                {
                    "sent": "These are good examples.",
                    "label": 0
                },
                {
                    "sent": "It's always fun to look at.",
                    "label": 0
                },
                {
                    "sent": "Failed example.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's even more fun to look at the failed examples.",
                    "label": 0
                },
                {
                    "sent": "These are some failure examples, so it was an image like portrait.",
                    "label": 0
                },
                {
                    "sent": "Women soldier.",
                    "label": 0
                },
                {
                    "sent": "I like this once.",
                    "label": 0
                },
                {
                    "sent": "He thinks it's Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "Election politics, right?",
                    "label": 0
                },
                {
                    "sent": "So this happened to, so we tried to look at why is why this is happening.",
                    "label": 0
                },
                {
                    "sent": "In fact, that was done three years ago, and at that time we didn't have quite as good features.",
                    "label": 0
                },
                {
                    "sent": "One on one hand, but then on the other hand, we actually look at this was done on the Flickr data set, and unfortunately, Flickr data set.",
                    "label": 0
                },
                {
                    "sent": "There aren't that many images of animals, but there are lot of Obama signs, right?",
                    "label": 0
                },
                {
                    "sent": "So there is so there is a little bit of because of his signs are like blue and white.",
                    "label": 0
                },
                {
                    "sent": "So then the model just correlates that.",
                    "label": 0
                },
                {
                    "sent": "But obviously these are these are failed cases.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here also the other way around, you can give in tax.",
                    "label": 0
                },
                {
                    "sent": "You can find images sort of does reasonably well for a lot of things.",
                    "label": 0
                },
                {
                    "sent": "Here's a failed case chocolate cake.",
                    "label": 0
                },
                {
                    "sent": "Right, so if I say, can I look like chocolate cakes, right?",
                    "label": 0
                },
                {
                    "sent": "Like if you.",
                    "label": 0
                },
                {
                    "sent": "You know, if you if you squint your eyes a little bit here you'll find these two bugs right, but it does.",
                    "label": 0
                },
                {
                    "sent": "It does fail.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what the data set looks like.",
                    "label": 0
                },
                {
                    "sent": "You have some images like for example.",
                    "label": 0
                },
                {
                    "sent": "For this image it's a very good description of what's going on for this image just tells us what kind of camera was used to take this image right?",
                    "label": 0
                },
                {
                    "sent": "So you can see there's a lot of noise in the data and you have to deal with the noise.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the architecture that we used has about 12 million parameters.",
                    "label": 0
                },
                {
                    "sent": "You have just been using 200 most frequently used text.",
                    "label": 0
                },
                {
                    "sent": "You have about 25,000 labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Split and then what's interesting about the data is that we actually have 1 million additional unlabeled data, which is.",
                    "label": 0
                },
                {
                    "sent": "Actually helped us.",
                    "label": 0
                },
                {
                    "sent": "Improve the results, that was that was interesting.",
                    "label": 0
                },
                {
                    "sent": "And if you look at, you know when I talk to my statistician friends.",
                    "label": 0
                },
                {
                    "sent": "They say 12 million parameters.",
                    "label": 0
                },
                {
                    "sent": "That's just crazy.",
                    "label": 0
                },
                {
                    "sent": "What are you doing when I talk to my friends and industry like friends at Google, they say 12 million parameters.",
                    "label": 0
                },
                {
                    "sent": "You don't have a bigger model.",
                    "label": 0
                },
                {
                    "sent": "We're using 12 billion parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the scope.",
                    "label": 0
                },
                {
                    "sent": "Depends on who you're talking to.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of looking at the results, this is just some.",
                    "label": 0
                },
                {
                    "sent": "Some results has been some work on using LDA and SVM type of models just using labeled examples.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting about these models is that if we actually add a mill.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unlabeled examples we do see some gains and we do see substantial gains in improvement.",
                    "label": 0
                },
                {
                    "sent": "So that was giving us a little bit of hope by saying that if you actually have unlabeled data because you're building the generative model of both of these modalities, it is helping you, at least for this particular data set.",
                    "label": 0
                },
                {
                    "sent": "It was helping us.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now you can step back and say, OK, Now you can generate decks.",
                    "label": 0
                },
                {
                    "sent": "How about a more challenging problem?",
                    "label": 0
                },
                {
                    "sent": "How about a problem of generating complete sentences?",
                    "label": 0
                },
                {
                    "sent": "And this is what I've shown you and this is going to be second half tutorial.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you how how you can actually extend this kind of model and do that.",
                    "label": 0
                },
                {
                    "sent": "Now in the last part of this tutorial, what I'd like to do is I'd like to just, yeah, question.",
                    "label": 0
                },
                {
                    "sent": "That's right, so yeah, should be.",
                    "label": 0
                },
                {
                    "sent": "I should be a little bit more careful, so this is retrieval.",
                    "label": 0
                },
                {
                    "sent": "So the way that you do it in this system is that you generating the features.",
                    "label": 0
                },
                {
                    "sent": "Right, and then once you generated the features you just looking at the training data, in which case a million images and finding the one that's closest in terms of cosine similarity to the training image.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, it's true.",
                    "label": 0
                },
                {
                    "sent": "We're not at this stage where we can generate these images.",
                    "label": 0
                },
                {
                    "sent": "No way where if my team was able to generate those images.",
                    "label": 0
                },
                {
                    "sent": "That would be the most amazing thing.",
                    "label": 0
                },
                {
                    "sent": "So this work was done three years ago, and at that time we were using bunch of sort of computer vision features so that was sift.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                },
                {
                    "sent": "Basically the same set of features as were used by other people because we wanted to compare on par.",
                    "label": 0
                },
                {
                    "sent": "I think that if you're going to be using image Net features now.",
                    "label": 0
                },
                {
                    "sent": "You know all the numbers will go up, maybe by 10%?",
                    "label": 0
                },
                {
                    "sent": "It's a general trend, like if you just replace one set of features with another set of features you know everything goes by 10% or 15% up right so?",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now in the last part of the tutorial what I'd like what I thought I would do is I would give you a little bit more technical so it is going to be probably the most technical part of the tutorial where I'm going to try to tell you about an interesting method for for trying to evaluate these models.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you that evaluation is a very hard problem because you know we're doing approximate maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "We can compute these so called normalizing constant partition functions, so trying to see is your model better than somebody else's model is actually very challenging.",
                    "label": 0
                },
                {
                    "sent": "And this is joint work with durable Diane Roger Gross and I think you're is here, yeah, so you can ask him more questions if you if you'd like to.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just come back to the graphical models.",
                    "label": 0
                },
                {
                    "sent": "And generally, where you specify your graphical model, you can say, well, the probability of an ensemble of random variables.",
                    "label": 0
                },
                {
                    "sent": "You can either specify the energy and say the probability is one is E to the negative energy in this normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "Or you can just say it's some function and normalized function.",
                    "label": 0
                },
                {
                    "sent": "A normalized probability of X divided by the normalizing constant and the normalizing constant is you have to sum over all possible configurations so that you actually have a valid probability distribution, right?",
                    "label": 0
                },
                {
                    "sent": "And this is problem.",
                    "label": 0
                },
                {
                    "sent": "Right, this is a problem because it's difficult to compute because it requires exponential sums.",
                    "label": 0
                },
                {
                    "sent": "So then it's very hard to evaluate generatively Markov random fields, conditional random fields, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So most of the time people use these models to see how well you can classify things or how well you can do sort of auxiliary task.",
                    "label": 0
                },
                {
                    "sent": "Test your system on an exhilarate task because computing these probabilities is really really hard.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for example, if we go back to restricted Boltzmann machines for example.",
                    "label": 0
                },
                {
                    "sent": "I can define the energy as before you've seen that, then I can say the probability of the marginal should be the joint of this marginal is given by the Boltzmann distribution, and so you're summing over the states of the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "So you can write it in this way.",
                    "label": 0
                },
                {
                    "sent": "This term you can compute exactly 4 restricted balls machines.",
                    "label": 0
                },
                {
                    "sent": "You can approximate it for both Diebold machines, at least for BMS.",
                    "label": 0
                },
                {
                    "sent": "You can compute them exactly, so we have this this ratio.",
                    "label": 0
                },
                {
                    "sent": "This you can compute, right?",
                    "label": 0
                },
                {
                    "sent": "This is just a function of F of V. This you cannot compute right because, again, the normalizing constant you have to sum over this exponential space, you have to sum over all possible configurations, so you cannot do this.",
                    "label": 0
                },
                {
                    "sent": "You have to approximate fine.",
                    "label": 0
                },
                {
                    "sent": "We know how to approximate.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These things, so let's look at the following questions.",
                    "label": 0
                },
                {
                    "sent": "Like how do you do model selection?",
                    "label": 0
                },
                {
                    "sent": "How do you do complexity control?",
                    "label": 0
                },
                {
                    "sent": "So suppose you have two MRF's or two PBM's with two parameters, each MRF for each PBM has different number of hidden variables, was trained using different learning algorithms and so forth.",
                    "label": 1
                },
                {
                    "sent": "And what you'd like to do is you'd like on the validation set to look at the ratio of the two right there.",
                    "label": 1
                },
                {
                    "sent": "Just compare the tool.",
                    "label": 0
                },
                {
                    "sent": "Which one is better and typically the way you do it is maybe you know you look at reconstruction error, which is the proxy is a bad proxy, or you try to see which one model the features that you're learning abetted, classifying things.",
                    "label": 0
                },
                {
                    "sent": "That's another proxy.",
                    "label": 0
                },
                {
                    "sent": "But in order to sort of do the proper way of evaluation, you actually have to compute the ratio of partition functions.",
                    "label": 0
                },
                {
                    "sent": "Right, we have to compute some estimate of the thing and it's.",
                    "label": 1
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very hard to do it, so let me give you an example.",
                    "label": 0
                },
                {
                    "sent": "Which one is a better generative model have to models model and Model B?",
                    "label": 0
                },
                {
                    "sent": "How many of you think that model is a better generative model?",
                    "label": 0
                },
                {
                    "sent": "Great, how many of you think Baeza better generative model?",
                    "label": 0
                },
                {
                    "sent": "Nobody thinks that way.",
                    "label": 0
                },
                {
                    "sent": "That's why you should never trust people who show you samples from the model.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you what this model is.",
                    "label": 0
                },
                {
                    "sent": "This model is the following generative model.",
                    "label": 0
                },
                {
                    "sent": "I pick a training sample at random and I'm going to show it to you.",
                    "label": 0
                },
                {
                    "sent": "This is, this is what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "So this is saying that this model is has no generalization, can't get kicked capabilities right?",
                    "label": 0
                },
                {
                    "sent": "All I'm doing is I'm just showing you the data.",
                    "label": 0
                },
                {
                    "sent": "And this is a mixture model is a mixture of Bernoulli model, so there's actually much better model in this.",
                    "label": 0
                },
                {
                    "sent": "Right, so you have to be careful when I show you pretty pictures of generated things.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful and maybe just I overfeed it on the training set and all of these things are exactly coming from the training set.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So for example, these are samples from GBM.",
                    "label": 0
                },
                {
                    "sent": "These samples from mixture of Bernoulli sense.",
                    "label": 0
                },
                {
                    "sent": "Let's say I want to compare this on the validation set, right?",
                    "label": 0
                },
                {
                    "sent": "I need some estimate of the partition function.",
                    "label": 0
                },
                {
                    "sent": "I need to know is this model better than this model?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, you know we some samples with some approximations that I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "You can show that PBM's.",
                    "label": 0
                },
                {
                    "sent": "I actually much much better than mixtures in this case.",
                    "label": 0
                },
                {
                    "sent": "Make sure we notice we can evaluate these models.",
                    "label": 0
                },
                {
                    "sent": "At least approximately.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you how, but we can.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm pretty confident that I can tell you this is a better model in this.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do that?",
                    "label": 0
                },
                {
                    "sent": "There is one very beautiful technique called annealed importance sampling.",
                    "label": 0
                },
                {
                    "sent": "Um was developed by Radford Neal and it's I think it's sort of considered one of the Golden standards for estimating partition functions or estimating exponential sums.",
                    "label": 0
                },
                {
                    "sent": "So let me maybe give you an intuition of what this algorithm is actually doing.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have two distributions.",
                    "label": 0
                },
                {
                    "sent": "Define on some SpaceX and you have these functions, so you have these probabilities.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call the initial distribution and target distribution, and suppose that this probability the initial distribution is very easy to sample from.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So think again, is this distribution is maybe being uniform distribution?",
                    "label": 0
                },
                {
                    "sent": "Right, if it's a uniform distribution is very easy to get samples from and you have a target distribution and this is intractable target distribution.",
                    "label": 0
                },
                {
                    "sent": "This is what you want to get your hands on.",
                    "label": 0
                },
                {
                    "sent": "Right, so under some conditions you can do the following the partition function.",
                    "label": 0
                },
                {
                    "sent": "You have to sum over all possible configurations of X, right?",
                    "label": 0
                },
                {
                    "sent": "That's what you want to do, and that's an exponential sum.",
                    "label": 0
                },
                {
                    "sent": "You can compute exactly.",
                    "label": 0
                },
                {
                    "sent": "And what you can do is you can multiply and divide by the simple distribution.",
                    "label": 0
                },
                {
                    "sent": "And again, you have to be careful that you don't, you know, dividing by zero and so forth, but let's just assume that both of these distributions assign nonzero probability to the entire SpaceX.",
                    "label": 0
                },
                {
                    "sent": "Right, I can do that.",
                    "label": 0
                },
                {
                    "sent": "Nothing changes and then I can do the following.",
                    "label": 0
                },
                {
                    "sent": "I can say how about using a Monte Carlo approximation.",
                    "label": 0
                },
                {
                    "sent": "This partition function can be approximated by draw samples from your initial distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, which you can do.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's a uniform distribution and just look at the ratio of the two.",
                    "label": 0
                },
                {
                    "sent": "If you look at the ratio of the two and do the average, that's called Monte Carlo approximation.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this W here is called the importance weight.",
                    "label": 0
                },
                {
                    "sent": "It looks at the ratio of F divided by the initial distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "It's called Simple Monte Carlo simple important sampling, important sampling.",
                    "label": 0
                },
                {
                    "sent": "It's very widely used, used idea very simple.",
                    "label": 0
                },
                {
                    "sent": "The problem is that in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "The variance of this estimator will be high.",
                    "label": 0
                },
                {
                    "sent": "You can show it an unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "Asymptotically.",
                    "label": 0
                },
                {
                    "sent": "Everything is fine, you'll get the right answer, but the variance can be very high, or in fact can be infinite, in which case you will never get the right.",
                    "label": 0
                },
                {
                    "sent": "Value yes, there is a question.",
                    "label": 0
                },
                {
                    "sent": "Sorry what.",
                    "label": 0
                },
                {
                    "sent": "No, the key thing is that you cannot draw samples from the target distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example, you cannot draw samples from exact samples.",
                    "label": 0
                },
                {
                    "sent": "You cannot draw exact samples from GBM, so MRF.",
                    "label": 0
                },
                {
                    "sent": "So any type of undirected graphical model, unless it has a very specific structure like a load tree with structure, in which case you can compute everything more or less exactly.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that for any given XI can compute this thing.",
                    "label": 0
                },
                {
                    "sent": "If you tell me XI can compute F. But I cannot draw samples from this model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good question.",
                    "label": 0
                },
                {
                    "sent": "And then there is the idea of annealed importance sampling of a very beautiful idea which.",
                    "label": 0
                },
                {
                    "sent": "Little bit of a crazy idea.",
                    "label": 0
                },
                {
                    "sent": "It says, well, if you're working in important sampling, how about instead of working in this space, we've been going through higher space, high dimensional space in do important sampling in high dimensional space?",
                    "label": 0
                },
                {
                    "sent": "So here's here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sequence of intermediate distributions they go.",
                    "label": 0
                },
                {
                    "sent": "You know this is your initial distribution.",
                    "label": 0
                },
                {
                    "sent": "This is your target distribution and you have a sequence of these distributions and one way when general way is to use geometric averages, there are other ways of doing it, but one way is to look at the geometric averages just easiest way, so you can say all of your intermediate distributions.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be defining that way right.",
                    "label": 0
                },
                {
                    "sent": "F beta divided by set of data.",
                    "label": 0
                },
                {
                    "sent": "Which is going to be take the initial distribution, raise it to the power of 1 minus beta.",
                    "label": 0
                },
                {
                    "sent": "Take the target distribution raised to the power of beta and you have a normalizing constant and these betas have to be chosen by the user.",
                    "label": 0
                },
                {
                    "sent": "This is a scheduling that you defining.",
                    "label": 0
                },
                {
                    "sent": "If the initial distribution is uniform, then you can say that all of your intermediate distributions have this form, which is why it's called annealing, right?",
                    "label": 0
                },
                {
                    "sent": "The reason why it's called annealing is that when beta is 0.",
                    "label": 0
                },
                {
                    "sent": "You have uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "When Beta is one, you have your target distribution, so you're going from uniform to the target distribution, right?",
                    "label": 0
                },
                {
                    "sent": "And this is called the inverse temperature.",
                    "label": 0
                },
                {
                    "sent": "So depending how you define it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can do that and the idea here, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "How do you choose the spacing of betas?",
                    "label": 0
                },
                {
                    "sent": "There's no good answer for that.",
                    "label": 0
                },
                {
                    "sent": "It has to be chosen by the user as being a lot of tons of papers.",
                    "label": 0
                },
                {
                    "sent": "If you look at the statistical literature is tons of papers people are trying to optimize for what the right betas are and so forth are some techniques of doing that.",
                    "label": 0
                },
                {
                    "sent": "But in terms of practical.",
                    "label": 0
                },
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "I don't know of any practical technique that can actually give you the right the right spacing, so you sort of have to play with it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a question.",
                    "label": 0
                },
                {
                    "sent": "So in our case, I think it's mostly uniform.",
                    "label": 0
                },
                {
                    "sent": "We just we just.",
                    "label": 0
                },
                {
                    "sent": "It's the easiest thing to say from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Define maybe 10,000 million distributions.",
                    "label": 0
                },
                {
                    "sent": "All of them are spaced uniform and the certain conditions maybe have geometric spacing and so forth, but it's just, it's just an empirical question, yeah?",
                    "label": 0
                },
                {
                    "sent": "Why don't you use?",
                    "label": 0
                },
                {
                    "sent": "Well, it becomes a little bit expensive to do it for learning.",
                    "label": 0
                },
                {
                    "sent": "There's been some work in trying to use these these ideas for learning, but it's just too expensive.",
                    "label": 0
                },
                {
                    "sent": "It's too expensive.",
                    "label": 0
                },
                {
                    "sent": "Like you know, if you want to get good estimates and you have 10,000 intermediate distributions, you can't afford doing 10,000 things.",
                    "label": 0
                },
                {
                    "sent": "For every single update, but let me let me sort of the goal here.",
                    "label": 0
                },
                {
                    "sent": "I guess for me wasn't really to focus on on this particular matter.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the different one.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But just to give you an intuition, here is that you start with the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "An you go at the sequence.",
                    "label": 0
                },
                {
                    "sent": "You sort of changing it and changing it and so forth and you go to the target distribution.",
                    "label": 0
                },
                {
                    "sent": "So have a sample here running around and then sort of falls between the two modes and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you also need to define a transition operator for every one of these intermediate distributions, but that's very easy to do.",
                    "label": 0
                },
                {
                    "sent": "Turns out that geometric averages.",
                    "label": 1
                },
                {
                    "sent": "It's a trivial change to the code, so believe me when I say that defining these intermediate transition distributions is really, really easy.",
                    "label": 0
                },
                {
                    "sent": "You just multiplying your energy function by scalar.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does annealing point sampling works?",
                    "label": 0
                },
                {
                    "sent": "You sample from your initial distribution, you sample the next state, and so forth, and then you go all the way down until you sample from.",
                    "label": 0
                },
                {
                    "sent": "When beta equals to one.",
                    "label": 0
                },
                {
                    "sent": "Right and then there is this thing where you basically look at the ratio of all of these normalized distributions as you go along.",
                    "label": 0
                },
                {
                    "sent": "And it turns out magically that that gives you an unbiased estimate of the partition function.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And the key idea here is that whenever you go from one temperature to another temperature, you look at the ratio.",
                    "label": 0
                },
                {
                    "sent": "Right of these guys.",
                    "label": 0
                },
                {
                    "sent": "The hope is that the ratios are going to be very, very small, so it's like, you know, in some case you dividing into finer distributions and the difference between 2 nearby distribution is going to be small.",
                    "label": 0
                },
                {
                    "sent": "Right, so the hope is that by that you're going to be reducing variance.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so you're getting an unbiased estimate of your partition function, which is nice.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right so well.",
                    "label": 0
                },
                {
                    "sent": "So the thing about as I mentioned in the straight important sampling, the variance is high because if the difference between the initial and target distribution is too big.",
                    "label": 0
                },
                {
                    "sent": "You know, samples from the initial distribution are not going to be matching the samples from, you know they they from the target distribution.",
                    "label": 0
                },
                {
                    "sent": "And when you look at the ratio of the two, you're going to have huge variance.",
                    "label": 0
                },
                {
                    "sent": "Here when you defining a sequence of these distributions right when you're looking at the ratio between intermediate steps, that ratio is hopefully going to be close to one, so you actually going to be.",
                    "label": 0
                },
                {
                    "sent": "The variance shrinks and you can show that.",
                    "label": 0
                },
                {
                    "sent": "I think that asymptotically, you can show that the variance is going to be roughly shrinking as you increase the number of intermediate distributions.",
                    "label": 0
                },
                {
                    "sent": "That's that's well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is it's true.",
                    "label": 0
                },
                {
                    "sent": "I mean when you multiply, think so.",
                    "label": 0
                },
                {
                    "sent": "For example what will happen in some cases?",
                    "label": 0
                },
                {
                    "sent": "What might happen is that there are sort of these.",
                    "label": 0
                },
                {
                    "sent": "Phase transitions where even small changes, intervening distributions or small changes in beta can heat like huge changes in the distributions, so that can happen, but it's just an empirical question.",
                    "label": 0
                },
                {
                    "sent": "I think it's as there is some.",
                    "label": 0
                },
                {
                    "sent": "I think there is some theoretical results shown by Radford Neal that the variance, roughly speaking, is going to be reducing in terms of the number of intermediate steps.",
                    "label": 0
                },
                {
                    "sent": "There's sometimes some constant, so we can actually show that asymptotically you will be reducing the variance.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you take the number of intermediate distributions and you let it go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You'll get zero variance because you can write this whole thing down as the integral, right?",
                    "label": 0
                },
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "But yeah, to some extent so that maybe it may be a better way of thinking about it is the following.",
                    "label": 0
                },
                {
                    "sent": "If you look at this ratio here right?",
                    "label": 0
                },
                {
                    "sent": "Or the ratio of intermediate things, what you can do is you can say, let's say I have two distributions.",
                    "label": 0
                },
                {
                    "sent": "And these two distributions are very close to each other.",
                    "label": 0
                },
                {
                    "sent": "In KL space, if they are close to each other, I can use important sampling to estimate the ratio.",
                    "label": 0
                },
                {
                    "sent": "If I could sample from 1.",
                    "label": 0
                },
                {
                    "sent": "If I could get ID samples from one I can look at the ratio and it's going to be pretty good estimator.",
                    "label": 0
                },
                {
                    "sent": "So imagine that your Markov chain had perfect your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Actually, here was had a perfect transition operator.",
                    "label": 0
                },
                {
                    "sent": "It would just draw ID sample at every single temperature, right?",
                    "label": 0
                },
                {
                    "sent": "In this case you can actually show that for the intermediate distributions because they are so close you can actually get very precise estimate.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "This argument fails because your Markov chain doesn't is not a perfect transition operator, so you can construct cases where this will have also huge variance.",
                    "label": 0
                },
                {
                    "sent": "Right, but the key again because you're looking at the nearby.",
                    "label": 0
                },
                {
                    "sent": "If you nearby distributions are very close to each other, you can use important sampling, so you can basically break this thing down and say if you could use important sampling for each one of those ratios, you would reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "Right, but again, there are some cases when you can construct specific things where the variance can explode, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no guarantees there is no formal guarantee by saying that it will succeed at the certain conditions.",
                    "label": 0
                },
                {
                    "sent": "You can show that it will succeed, but.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You have a forward chain, right?",
                    "label": 0
                },
                {
                    "sent": "So how do you actually show what The thing is?",
                    "label": 0
                },
                {
                    "sent": "Here's the intuition.",
                    "label": 0
                },
                {
                    "sent": "You have a forward chain.",
                    "label": 0
                },
                {
                    "sent": "You start at the data and you March through this transition operator.",
                    "label": 0
                },
                {
                    "sent": "You can think of this is just conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "Right, but then what you can do is you can say, well, what if I start at the target and go all the way back?",
                    "label": 0
                },
                {
                    "sent": "To the data using sort of reverse sequence of transition operators.",
                    "label": 0
                },
                {
                    "sent": "Right, and it's just a theoretical construct and you can say, well, the reverse is that you actually, if you could draw exact samples from the target and then move backwards all the way to the data, that's going to be the reverse sequence, and then you can say, well, annual important sampling.",
                    "label": 0
                },
                {
                    "sent": "It turns out there's just a simple important sampling.",
                    "label": 0
                },
                {
                    "sent": "It's the same as important sampling.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But on extended state space, right?",
                    "label": 0
                },
                {
                    "sent": "You using forward chain as a sampler and then you basically estimating this ratio.",
                    "label": 0
                },
                {
                    "sent": "You sampling from the forward, you start at the uniform go and then then you look.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duration.",
                    "label": 0
                },
                {
                    "sent": "And here's what it looks like to estimate the partition function.",
                    "label": 0
                },
                {
                    "sent": "So this is what annealing point sampling with geometric averages look like, so you know, as beta goes up.",
                    "label": 0
                },
                {
                    "sent": "Nothing happens and then.",
                    "label": 0
                },
                {
                    "sent": "Digits appear right, so you know if it's, it's just sort of visually.",
                    "label": 0
                },
                {
                    "sent": "What happens here, right?",
                    "label": 0
                },
                {
                    "sent": "You start it sort of base rates, almost uniform distribution, and as you go through different phases.",
                    "label": 0
                },
                {
                    "sent": "This is what the samples look like and then as beta heats one, you get the digits right.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, problems with these models.",
                    "label": 0
                },
                {
                    "sent": "That was done a few years ago, but what's the big problem?",
                    "label": 0
                },
                {
                    "sent": "So areas provides an unbiased estimate of the partition function that was shown, just like important sampling.",
                    "label": 0
                },
                {
                    "sent": "But in general, we're going to be interested in estimating log.",
                    "label": 0
                },
                {
                    "sent": "We never actually get the partition function we're interested in.",
                    "label": 0
                },
                {
                    "sent": "We're interested in log probabilities, so by answers inequality, the actin expectation.",
                    "label": 0
                },
                {
                    "sent": "The log of the estimate is always going to be less than the load of expected value, which is the true partition function.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying is that.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, an average, we're going to be underestimating the partition function by Markov inequality.",
                    "label": 0
                },
                {
                    "sent": "It's also very unlikely to overestimate the log of the partition function, so this is saying the probability, the probability that the log of your estimate is bigger than the truth by some.",
                    "label": 0
                },
                {
                    "sent": "Value B is going to be less than each of them.",
                    "label": 0
                },
                {
                    "sent": "Mine is B, so estimating by a few nuts it's very improbable.",
                    "label": 0
                },
                {
                    "sent": "Right, so what's happening?",
                    "label": 0
                },
                {
                    "sent": "What's happening in practice is the following.",
                    "label": 0
                },
                {
                    "sent": "If I look at the log of the probability, it's this thing I can compute, but then it's minus log of the partition function.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's just acoustic bound.",
                    "label": 0
                },
                {
                    "sent": "So if we underestimate this, we're going to be over estimating the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Right, so why is this bad?",
                    "label": 0
                },
                {
                    "sent": "This is bad by the following reason.",
                    "label": 0
                },
                {
                    "sent": "You can take your model.",
                    "label": 0
                },
                {
                    "sent": "Do a really bad job and running your important sampler.",
                    "label": 0
                },
                {
                    "sent": "And get really good results and this is fantastic, right?",
                    "label": 0
                },
                {
                    "sent": "The worst the you know if you're doing a bad job at estimating your partition functions, your result look good, right?",
                    "label": 0
                },
                {
                    "sent": "And you know you code it up.",
                    "label": 0
                },
                {
                    "sent": "You get some value of the partition functions and your numbers look really good.",
                    "label": 0
                },
                {
                    "sent": "You say, wow, this is amazing.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's exactly what's been happening here.",
                    "label": 0
                },
                {
                    "sent": "Like sometimes I see papers that are reporting on amnesty numbers like 50 or 40 knots, which is obviously wrong, and I know it's wrong because it cannot be right, and I think a lot of it has to do with, you know, and that's a problem.",
                    "label": 0
                },
                {
                    "sent": "You know, like you're running bad code and you're getting good numbers, and the worst your code is, the better numbers you get, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what's happening.",
                    "label": 0
                },
                {
                    "sent": "What's been happening with these models?",
                    "label": 0
                },
                {
                    "sent": "So if you don't know what's going behind the doors, it's very hard for you to use this method.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's look at the motivation.",
                    "label": 0
                },
                {
                    "sent": "For for sampling, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're looking at carbs and how do we do sampling?",
                    "label": 0
                },
                {
                    "sent": "We started random configurations we.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update The hidden swear.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data visible and so.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choice of running this.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of chain and the theory tells you that if you run it to Infinity you get samples.",
                    "label": 0
                },
                {
                    "sent": "Yes, you get, you get the quality distribution right.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose that what we actually do in practice is that we're running this chain 4000 steps, and then we pretend that this is an equilibrium distribution.",
                    "label": 0
                },
                {
                    "sent": "So if I show you samples, this is what I'm actually doing, right?",
                    "label": 0
                },
                {
                    "sent": "So why not treat this model as a generative model?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we can do is we can do the following.",
                    "label": 0
                },
                {
                    "sent": "We can say start at Rand.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go on.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way down.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you actually get is the observed data, and that's your model.",
                    "label": 0
                },
                {
                    "sent": "Active model.",
                    "label": 0
                },
                {
                    "sent": "It's not that the RBM is your model, but this is your generative model.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we use infinite number of layers here, then the genitive is going to be the same as GBM.",
                    "label": 0
                },
                {
                    "sent": "Other ways this this generative models just in approximation to an RBM.",
                    "label": 0
                },
                {
                    "sent": "But we're going to be treating this as a generative model.",
                    "label": 1
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's consider X to be the joint space.",
                    "label": 0
                },
                {
                    "sent": "Each is an observed and then we can define the following generative process, which is just a sequence of IIS distributions.",
                    "label": 0
                },
                {
                    "sent": "So we start at random and then go all the way down to the data and this is the generative model that we call the annealing model.",
                    "label": 0
                },
                {
                    "sent": "That's what we're interested in, because in practice that's how you're going to be using this model, right?",
                    "label": 0
                },
                {
                    "sent": "If we want to draw samples, this is how I'm going to be doing samples.",
                    "label": 0
                },
                {
                    "sent": "So instead of thinking, this is in GBM model and thinking of this is a generative model with multiple.",
                    "label": 0
                },
                {
                    "sent": "With me",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Layers.",
                    "label": 0
                },
                {
                    "sent": "Now we would like to estimate the probability of test right?",
                    "label": 0
                },
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the trick.",
                    "label": 0
                },
                {
                    "sent": "The trick is starting at the data and doing the reverse annealing.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can say this is reverse annealing.",
                    "label": 0
                },
                {
                    "sent": "Is you started that data?",
                    "label": 0
                },
                {
                    "sent": "You have to be able to compute this conditional probability of hedons given visibles which you can do for our BMS.",
                    "label": 0
                },
                {
                    "sent": "And then you can extend it to non tractable posteriors like the both machines in deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "So you can do that and you have the sequence of reverse operators and now what you're going to do is we're going to be treating that as an importance as a proposal for important sampling.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so now you have a generative model, which is just a theoretical construct.",
                    "label": 0
                },
                {
                    "sent": "And then the proposal starts at the data and melts the distribution.",
                    "label": 0
                },
                {
                    "sent": "So we go the other way around.",
                    "label": 0
                },
                {
                    "sent": "Right, and it turns out that for this model we can use this reverse AES as a proposal and then we look at the ratio of the two.",
                    "label": 0
                },
                {
                    "sent": "Right and we can show that we get an unbiased estimator of this quantity.",
                    "label": 0
                },
                {
                    "sent": "We actually get an unbiased estimate of the probability of the test data and then what's nice about this particular estimate is that it tends to underestimate rather than overestimate low probabilities, which is good.",
                    "label": 0
                },
                {
                    "sent": "Which basically means that if you're writing a bad quote code.",
                    "label": 0
                },
                {
                    "sent": "You'll get bad numbers here, which is good.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's just one example, and I'm almost done.",
                    "label": 0
                },
                {
                    "sent": "This is double nahmias.",
                    "label": 0
                },
                {
                    "sent": "Look what happens here.",
                    "label": 0
                },
                {
                    "sent": "Here this this shows the number of intermediate distributions that you're using in IIS, so if you just estimating partition function and use 10 intermediate distributions, you get a really bad estimate.",
                    "label": 0
                },
                {
                    "sent": "You really underestimate the partition function, in which case you really overestimating log probabilities, so the higher in terms of log drops, the better right?",
                    "label": 0
                },
                {
                    "sent": "And you can see as you increase the number of intermediate distributions as sort of goes here.",
                    "label": 0
                },
                {
                    "sent": "And I call this the new state of the art.",
                    "label": 0
                },
                {
                    "sent": "So you know, this is done really bad job.",
                    "label": 0
                },
                {
                    "sent": "It's just a few intermediate distributions, but my numbers look fantastic, so I could claim this is a new state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas if you look at the other way of estimating it, you sitting over all the way here, right?",
                    "label": 0
                },
                {
                    "sent": "So it sort of provides abound allow stochastic lower bound, so you cannot fool yourself into believing that these models are better than what they really are.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you changing different if you're doing different uniform other from uniform other initial distributions, you can sort of get this property in.",
                    "label": 0
                },
                {
                    "sent": "For omniglot datasets you again see the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "The EIS goes up and race you really getting the bound which is.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is important and finally just the last slide.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to show you that you know this is exact.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is the Yoshi's Island.",
                    "label": 1
                },
                {
                    "sent": "Work on on contrastive sampling based log likelihood estimators.",
                    "label": 0
                },
                {
                    "sent": "You can see that it's.",
                    "label": 0
                },
                {
                    "sent": "It's doing much worse.",
                    "label": 1
                },
                {
                    "sent": "That's the only estimated that we found.",
                    "label": 0
                },
                {
                    "sent": "It also provides a decent lower bound 'cause all the other estimators like you know, coming from variational bounds like 3 weighted belief propagation for example.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Away off man.",
                    "label": 0
                },
                {
                    "sent": "Can see that the difference is quite small, so you actually get two of these estimators and you can judge how good how good your estimator is, which is which is quite quite important and you can do the same thing for Diebold's machines for deep belief Nets and we always see this kind of behavior that you cannot fool yourself too much into believing that your model is actually better than what what it really is, and this is important for for evaluation, and it also brings us to the notion of Helmholtz machines.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, and that's going to be that's going to be this afternoon, and now I'll stop here.",
                    "label": 0
                },
                {
                    "sent": "Which is to say that you know this, these kind of ideas come to the notion of you have these directed models.",
                    "label": 0
                },
                {
                    "sent": "You have a generative model.",
                    "label": 0
                },
                {
                    "sent": "We have a recognition model, so how can you do the learning in these kinds of models?",
                    "label": 0
                },
                {
                    "sent": "And this is work.",
                    "label": 0
                },
                {
                    "sent": "Bye bye.",
                    "label": 0
                },
                {
                    "sent": "You're an I'm done for the first part of the tutorial.",
                    "label": 0
                }
            ]
        }
    }
}