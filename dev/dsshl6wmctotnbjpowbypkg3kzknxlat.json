{
    "id": "dsshl6wmctotnbjpowbypkg3kzknxlat",
    "title": "Dependency Tree Kernels for Relation Extraction from Natural Language Text",
    "info": {
        "author": [
            "Frank Reichartz, Fraunhofer IAIS"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_reichartz_dtkrenlt/",
    "segmentation": [
        [
            "Next, papers on dependency tree, Cornus correlation inspection from natural language fix is going to be presented by Frank Rijkaard.",
            "So hello, I welcome you all to my talk today about dependency tree kernels for relation extraction from natural language text, which is joint work with Hannah Scott and got pass."
        ],
        [
            "So let's start with introducing the task of relation extraction.",
            "Relation extraction is the task of identifying semantic relations between entities within the same sentence.",
            "So most approaches in this for the task consider only binary target relation in this world.",
            "In this talk I will focus on binary tag relations.",
            "2 examples for these are recently Obama became president of the use A.",
            "It's a positive example for the relation.",
            "Which state is the entity Obama is in?",
            "The role relation have some role in the USA.",
            "So also another example is Bama is still the CEO of Microsoft, which is another positive example for the so called role relation.",
            "Bama has some role at the entity Microsoft."
        ],
        [
            "So.",
            "How to get from this natural language sentence Sue trees so the entropy guys have developed the techniques of syntactic parsers which transforms natural language sentences into a tree structure.",
            "I've some examples later, so these provide an extensive information of the syntactic structures of the sentence.",
            "Condensed into a tree form.",
            "So the focus and our work here are dependency past trees.",
            "However, there are some other types of past trees also like transformer, past trees or so-called type dependency pasteries.",
            "However, we consider only untyped dependently past trees here, so a dependency tree is structures representation of the grammatical dependencies between the words of a sentence by a vertex labeled director tree.",
            "These so called dependency pass please have some nice properties like there's a by active mapping between the words and vertices in the tree and the words in the sentence the note this applies the ordering of the children's of each note.",
            "So basically we have vertex labeled order directed trees.",
            "So these dependency path trees are good for language with free word order like German, and they can be generated with a high quality by freely available."
        ],
        [
            "Father, so let's have it look at the example sentence into resulting trees.",
            "So what we see here, the notes, the words, entities which are in relation are bold here and here.",
            "So this is my running example.",
            "I will come back to that later, so how?"
        ],
        [
            "So you can define corners on this, but first recall butter kernels.",
            "Kernels are the generalized dot product for linear classifiers, which implicitly Maps observation into a high dimensional feature space.",
            "So a dot product and future space can be expressed by kernel working on the input space.",
            "So it is called chronic trick.",
            "Like many of you know already.",
            "But the nice part is that the input space can consist of structured data like trees, or in our case pass trees.",
            "In our case, or in general, the embedding into the higher dimensional space can only be implicit, so the usage of kernels, overpass trees are structured information for relation extraction has been shown to be useful.",
            "There have been some work by Kalata Bonus, Cohen Mooney and by Sung Coolatta Anthonys community have staggered dependency past trees while sung at all worked on Frostmourne pass trees.",
            "So."
        ],
        [
            "Let's recall the work of Bunescu Mooney for the shortest path I will.",
            "Do not talk about what the shortest path kernel proposed by Bunescu Mooney do so.",
            "What we can do to improve it or what we have done to improve it so?",
            "We see here.",
            "This is the shortest path between the two nodes in each tree and bonus community proposed to simply compare each node.",
            "Count the matching attributes.",
            "Here we used by active mapping between words and vertices.",
            "Each node is associated with some features like the entity type, the word type, the part of speech tag, and the string.",
            "The word it says.",
            "So they propose to count the matching.",
            "Attributes this is an exact match they proposed, so like we see here 3 attributes match and NP the part of speech Tech matches for both nodes.",
            "The general Public Speech text, which is a growth category, matches and entity Pie type match sewists.",
            "Three, the similarity would be 3 and they propose to multiply each similarity in each comparison on the path to get the kernel value for these two instances, yes.",
            "Example that."
        ],
        [
            "President President, why you're saying?",
            "So.",
            "So this is an annotated data so.",
            "For example.",
            "President, not Obama between Obama and US.",
            "8 is.",
            "If you remember, the relation is technically task list between entities.",
            "So."
        ],
        [
            "President no use as entity.",
            "This is why it's picked depicted bold and so this is the shortest path.",
            "And this is the shortest path so.",
            "If you plot this this way."
        ],
        [
            "They propose to multiply it to get an incredible you.",
            "Of course they normalized it by length.",
            "So however, there are some restrictions if it's in computation is of course possible, but however there are some limitations.",
            "Becausw they proposed.",
            "If only one you see, we gotta directed.",
            "Rough three, so you see the edge direction is important if one at direction does not match in this path so well we would be 0.",
            "If one note does not match, good.",
            "The similarity would be 0.",
            "This is a harsh restriction.",
            "However, this kernel would use quite good performance."
        ],
        [
            "So let's have a look at another kernel who have not this shortcoming.",
            "It's a dependency tree kernel bike.",
            "A lot ahead I.",
            "So what does this corner do?",
            "Right?",
            "Prepare denies enemy nation to show.",
            "It computes the matching attributes between these two nodes.",
            "Then it's traverse to the children, computes similarity between these notes.",
            "You see there is no match.",
            "OK, you cannot see it.",
            "Of course, it's so small, but I tell you there's no match, so but he will continue to compare each possible pairs of the other top sequences of the children's of each nodes.",
            "If there's a match, he will traverse down and continue computation.",
            "So no match again."
        ],
        [
            "So, however, it does not only compare one.",
            "The subsequences of length one, so not up to our length of the children, so also this subsequence would be compared.",
            "And it will be the sum of the similarities would be the similarity between these two instances.",
            "So this would be another.",
            "Comparison made during computation.",
            "So, however, this night is this current is.",
            "That there is a note similarity function, which is rather nice when we exploit this nice property of dependency pasteries suggested by active mapping between words and vertices is really nice.",
            "However, when there's no match, it will not reverse down everything on a level below will be cut off, will be not contribute to the similarity.",
            "This is not good I think, and our results have shown that this is indeed not good."
        ],
        [
            "Could be improved.",
            "So let's come to our first approach so the decay only considers the root nodes of the subtrees and their matching children's of course.",
            "This discuss possible similar structure and similarities at lower levels of the trees.",
            "So our first approach was to apply the DTK to every combination of notes in the subtrees implied by the relation nodes.",
            "So."
        ],
        [
            "So let's have a look what our so called alpass dependency tree kernel news so.",
            "We have here our kernel.",
            "It considers all pairs of the subtrees reacts is this subtree.",
            "This is a subtree implied by the relation nodes, where this is what this is this and subtree is simply the implied subtrees implied complete subtree.",
            "So for this it's also the same.",
            "So what have we done?",
            "We simply propose to call the dependency tree kernel of Carlotta.",
            "When every possible pair of combination.",
            "Between.",
            "Each subtree so required across the corner on this pair and on this pair, and on despair, and simply takes exam.",
            "So, however, it's quite computational, more complex, but however we have also proposed a way to tackle this.",
            "To do this efficient so.",
            "This was SSL main idea of our or past dependency tree corner so.",
            "Basically it's the.",
            "Idk some of every possible pair."
        ],
        [
            "So.",
            "The motivation of our second approach is the so called possibilities by business community shown by there.",
            "Results that the most valuable information on is on the path between the two entities.",
            "This is why they proposed the shortest path kernel.",
            "So, but also valuable information is close to this part or below, so our second approach is to consider the information below and on the path in parallel.",
            "So we propose to apply the TTK to the path note sequences instead of only both root notes.",
            "You know the TTK was only called on the root notes and.",
            "Cut off this cuts.",
            "Valuable informations below.",
            "If just one note does not match.",
            "However, if you called it IDK on every possible combination of notes on the.",
            "Pass will tackle this shortcomings.",
            "So.",
            "We also propose you know the.",
            "This restriction of the shortest path kernel that passed need to have the same length as the similarity would be 0.",
            "So we relaxing this by.",
            "Using all possible subsequences of notes on the path and calling the TTK on all possible subsequences.",
            "Which can also be."
        ],
        [
            "Non efficient, so let's have a look.",
            "What does our kernel deuce in detail?",
            "This is the shortest path in light Gray.",
            "So.",
            "We are calling basically the TTK on every possible combinations of.",
            "Notes only on the path in the matching subsequences up to a length of Q.",
            "This one parameter we experimented with so.",
            "Sorry.",
            "Here is the farmer, basically this.",
            "Is an enumeration of all possible subsequences?",
            "Of the same length up to length.",
            "Q Odessa typo I missed that.",
            "So this is a penalty for the length of the gaps, so this is nice to introduce.",
            "We have experimented also on this parameter and this is basically the clutter kernel.",
            "However broken up some details are in the paper.",
            "We have broken this up because this allows for efficient computation, but I cannot go into details through technical I think.",
            "More details on the paper if you have any questions, just ask me after."
        ],
        [
            "So.",
            "How can?"
        ],
        [
            "You see, we're doing here a lot of computations.",
            "A lot of computations are the same.",
            "Just remember that and also remember that the degree is not that high."
        ],
        [
            "Industries so.",
            "What's about the efficient computation of TDK?",
            "Because we called the TTK so much and use it so often, we thought about yeah, OK, so it was rather slow at first, but cause clotted all applied the ideas of the string subsequence kernel from Moody at all to the computation of the tight subsequences.",
            "This yield and algorithms with run in the order of M * N in the power of three.",
            "This is rather slow.",
            "Especially for kernel computation, when there when we call this TTK is that often, so we adopted a later optimization by Christiani and short Taylor which runs in M times N .2 to the power of 2.",
            "This was this was proposed for the for string subsequent problem, but we adapted this for this child computation problems with this dealing factor, however.",
            "A close look into data that reveals that independent re nodes have very very few children's."
        ],
        [
            "If you look into detail.",
            "Of the distribution.",
            "So let's have a look South degree of the North has the following distribution most notes.",
            "About 658% of the nodes have just one children in the resulting forest, often benchmark data set.",
            "Only.",
            "If you go here up to five nodes covers 99% of the data, so maybe all these optimization which only holds for large values of M&N is not good for this computation in our case because we have this distribution, so we have sought what can we do else because there are not that many computational.",
            "Stuff we don't need.",
            "Maybe this they have proposed dynamic programming nation.",
            "This metrics need to be analyzed.",
            "It's rather fast.",
            "This bounce holds.",
            "We have implemented this.",
            "However, if you just do an efficient caching.",
            "You are faster, very faster.",
            "We have cut the runtime biasing."
        ],
        [
            "86% about so let's have a look why this is so.",
            "If you go to the exact runtime and don't forget all this nasty constants in their own notation.",
            "So if we're really count all these constants, also we see here, this is the line one is our proposed solution by caching also by also called index cache.",
            "So.",
            "You see the exact runtime of the short Taylor solution with running M * N to the power of two.",
            "You see for 96% of the data hour.",
            "In this case, strategy is faster.",
            "Actually.",
            "Or this also hold for the colossal ecosolutions, which is also faster.",
            "All this translate to in runtime improvement of 66% is rather nice, and this is only by catching and exploiting the properties of these dependency trees.",
            "So."
        ],
        [
            "Cool.",
            "Let's look at our empirical evaluation.",
            "We have experimental conducted on ACA 2003 data set.",
            "This is a public available benchmark data set for the task of.",
            "Relation extraction this is by the so-called AC program.",
            "Automatic content extraction program started in the 90s.",
            "Now it's called Tuck Technologies conferences and their proposed new task this year.",
            "However, we have only experimented on the ACA 2003 data set which consists of newspaper texts, annotated buoys, entities and relations between gems there about 10,000 trees, 500 documents and yes.",
            "Trees fell sentence are 10 thousands sentence.",
            "So there are annotated so-called five top level relations.",
            "The relation remember the examples Bama and Microsoft and Obama, and you say so.",
            "Social relation is somewhat hard to define near at a clear.",
            "An part is also clear.",
            "However, these are their relations which are annotated.",
            "We have conducted some experiments.",
            "We have run a five times repeated 5 fold cross validation to compare the different approaches.",
            "We also use the pre specified split.",
            "You know the status head comes within test training data pre specified.",
            "Split, so let's have a look.",
            "We have some relations with.",
            "A lot of training instances like role.",
            "And part.",
            "And at and.",
            "However, there are some was only very few training examples."
        ],
        [
            "So let's look into real light in more detail.",
            "What we see here is we have reimplemented the shortest path Colonel DDK kernel, of course, but cause we are needed it so we have run them all on the same data on the same set and compare the results.",
            "What we see here?",
            "Is that the shortest path which quite a high F micro value for this simple approach of 58.6%, However, we can reach F micro by using the pass TTC of up to nearly 70%.",
            "This is a rather big improvement even over the.",
            "TTK, but we also see here is the F macro value, you know, recall macros, overall category, and micros overall instances so.",
            "Off 5070.7 this is even better than our past TTC, but however we look into the results in more detail to see why this happens.",
            "Basically, one can say capacity carries a good kernel for relation extraction, so we also run this on the pre specified training test.",
            "Split what we see here is also a big improvement over the previous approaches of our proposed approach.",
            "However, what's nice to see is that the precision of the alpines TTK is quite higher.",
            "In general."
        ],
        [
            "One can say that for some relations will reach quite high results which make maybe this approach a pliable for real world problems like semantic search for information retrieval, something like that or or knowledge based population versus what we see here is.",
            "That the authors depends if we can perform best.",
            "On the small relations where there are not many training data available, you remember there were only like 40 or 50 training data available there.",
            "For the other relations there, like 405 hundred training instances available, what we see here is that on all pick relations where there is a lot of training data available, the party TK outperforms or other kernels.",
            "Of course we have run a significance test and there's very significant difference and.",
            "It's only outperformed on the relations.",
            "It's not that much training data by the opacity K, so it seems as the positive case a good color for relation extraction when there is a lot of.",
            "Training data available will, whereas a lot is not that much and you think about it's only what 506 hundred sentence one can annotate this in about one day today so.",
            "We're running you tests on this right now, so yes, this was already told.",
            "Party care better for relations."
        ],
        [
            "It's more later this is a conclusion, so now I will conclude my talk that performance what we've seen here is the performance depends on the type of relations into training size.",
            "Because some relations are reservoir, so the social relation is Reza unclear.",
            "What exactly means?",
            "So there I think there would be one human annotators.",
            "There would be a low Inter annotator agreement for this, so for some.",
            "Relations, we have sufficient performance for real applications like knowledge based population or semantic search or something like that.",
            "So however we only considered.",
            "Dependency past trees in this work.",
            "However, I combination with other kernels with kernels for other types of past researchers also reasonable at sea.",
            "2009 we've shown does the combination with the state of the art fast, past three kernel outperforms all other approaches.",
            "So what are what is future work we can do with this is for once make our.",
            "SV M3 Kernel toolkit freely available.",
            "It's rather nice.",
            "We're working on this.",
            "Another part would be considered more sophisticated, not similarity functions, you know.",
            "And we can exploit this.",
            "Be active mappings even more because words can have subtypes or something like that.",
            "You can determine the sense of the word is all.",
            "This should improve this note similarity functions.",
            "There's a lot of.",
            "Work to be done there.",
            "We're looking into this so as nice would be a combination of different machine learning approaches.",
            "For example pattern recognition or pattern mining approaches and.",
            "Last but not least, the application to other language would be nice.",
            "We are working on this right now.",
            "The problem example, an example for drama set down is no publicly available data set, so we need to work on annotated data to make a data set so that we can compare the different approaches."
        ],
        [
            "So.",
            "Thank you all.",
            "Earlier slides that.",
            "First ones that are structured in input.",
            "You only have the option of the implicit inner product.",
            "Uh.",
            "Say stop.",
            "Here.",
            "It's one of the first OK."
        ],
        [
            "It was the shortest path corner.",
            "Yeah, so I guess I'm asking that is.",
            "Concerned with with performance and I was just wondering if you had looked at at explicit propositional isation as an option for speeding things up.",
            "If that works for your data set, that's not possible.",
            "So so in other words, explicitly representing the feature space.",
            "Yes, it is sometimes possible.",
            "Yes, yes, it's it's for this kernel would be possible to have the mapping in the feature space, but it's rather hard to define them."
        ],
        [
            "King of the collateral to feature space.",
            "This girl is so park we don't know the expected mapping, so.",
            "Because this with all this.",
            "Copper is made there.",
            "We don't know this mapping, but wherever nice for kernels is that we don't need to know this mapping and still computers efficiently.",
            "Yes, yes we have thrown off course that these are kernels.",
            "This but is rather clear.",
            "Becausw could have shown that this is a kernel and we make only sums and product of valid kernels so.",
            "Our.",
            "Also Colonel so.",
            "You say?",
            "For relationships.",
            "Or Russia.",
            "First, then the more restrictive approach is more real."
        ],
        [
            "They won't make more, oh.",
            "Oh no, you you got me wrong.",
            "Actually this is our second approach and actually this is a more general approach.",
            "This is to all pairs, kernel, the all pairs kind of computes the similarity between every pair, so of course it's more general than only comparing notes on the path.",
            "And the substructure below them.",
            "Better when you accept yes yes.",
            "Yes, when there is more data available, the more restricted one of their power.",
            "Yes.",
            "OK. Any other questions?",
            "Optimizing the.",
            "Which came back with caching, yes."
        ],
        [
            "About it and send it to me, but how does this response work better with the find the bus space?",
            "Oh oh, the past the passing times of course very high.",
            "It's about one sentence, the second.",
            "However, this can be done in parallel like we have like 10 machines and can run this and it can be stored.",
            "Of course we have run this one time it have take sometimes have take about 9000 seconds.",
            "You can break that down.",
            "Then we have saved the trees and use it for more.",
            "However, this however, all these current computation are quite take quite some time, so this efficient.",
            "Computation of the TTK is for the DDK alone.",
            "It was not important to optimize it that much, however, because we used the TTK so often the optimization was needed to reduce the runtime to realistic levels so that we can apply it on real datasets.",
            "Which we may face in our work.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, papers on dependency tree, Cornus correlation inspection from natural language fix is going to be presented by Frank Rijkaard.",
                    "label": 0
                },
                {
                    "sent": "So hello, I welcome you all to my talk today about dependency tree kernels for relation extraction from natural language text, which is joint work with Hannah Scott and got pass.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with introducing the task of relation extraction.",
                    "label": 0
                },
                {
                    "sent": "Relation extraction is the task of identifying semantic relations between entities within the same sentence.",
                    "label": 1
                },
                {
                    "sent": "So most approaches in this for the task consider only binary target relation in this world.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will focus on binary tag relations.",
                    "label": 0
                },
                {
                    "sent": "2 examples for these are recently Obama became president of the use A.",
                    "label": 0
                },
                {
                    "sent": "It's a positive example for the relation.",
                    "label": 0
                },
                {
                    "sent": "Which state is the entity Obama is in?",
                    "label": 0
                },
                {
                    "sent": "The role relation have some role in the USA.",
                    "label": 0
                },
                {
                    "sent": "So also another example is Bama is still the CEO of Microsoft, which is another positive example for the so called role relation.",
                    "label": 0
                },
                {
                    "sent": "Bama has some role at the entity Microsoft.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How to get from this natural language sentence Sue trees so the entropy guys have developed the techniques of syntactic parsers which transforms natural language sentences into a tree structure.",
                    "label": 0
                },
                {
                    "sent": "I've some examples later, so these provide an extensive information of the syntactic structures of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Condensed into a tree form.",
                    "label": 0
                },
                {
                    "sent": "So the focus and our work here are dependency past trees.",
                    "label": 0
                },
                {
                    "sent": "However, there are some other types of past trees also like transformer, past trees or so-called type dependency pasteries.",
                    "label": 0
                },
                {
                    "sent": "However, we consider only untyped dependently past trees here, so a dependency tree is structures representation of the grammatical dependencies between the words of a sentence by a vertex labeled director tree.",
                    "label": 1
                },
                {
                    "sent": "These so called dependency pass please have some nice properties like there's a by active mapping between the words and vertices in the tree and the words in the sentence the note this applies the ordering of the children's of each note.",
                    "label": 0
                },
                {
                    "sent": "So basically we have vertex labeled order directed trees.",
                    "label": 0
                },
                {
                    "sent": "So these dependency path trees are good for language with free word order like German, and they can be generated with a high quality by freely available.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Father, so let's have it look at the example sentence into resulting trees.",
                    "label": 0
                },
                {
                    "sent": "So what we see here, the notes, the words, entities which are in relation are bold here and here.",
                    "label": 0
                },
                {
                    "sent": "So this is my running example.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that later, so how?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can define corners on this, but first recall butter kernels.",
                    "label": 0
                },
                {
                    "sent": "Kernels are the generalized dot product for linear classifiers, which implicitly Maps observation into a high dimensional feature space.",
                    "label": 1
                },
                {
                    "sent": "So a dot product and future space can be expressed by kernel working on the input space.",
                    "label": 1
                },
                {
                    "sent": "So it is called chronic trick.",
                    "label": 1
                },
                {
                    "sent": "Like many of you know already.",
                    "label": 0
                },
                {
                    "sent": "But the nice part is that the input space can consist of structured data like trees, or in our case pass trees.",
                    "label": 0
                },
                {
                    "sent": "In our case, or in general, the embedding into the higher dimensional space can only be implicit, so the usage of kernels, overpass trees are structured information for relation extraction has been shown to be useful.",
                    "label": 1
                },
                {
                    "sent": "There have been some work by Kalata Bonus, Cohen Mooney and by Sung Coolatta Anthonys community have staggered dependency past trees while sung at all worked on Frostmourne pass trees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's recall the work of Bunescu Mooney for the shortest path I will.",
                    "label": 0
                },
                {
                    "sent": "Do not talk about what the shortest path kernel proposed by Bunescu Mooney do so.",
                    "label": 1
                },
                {
                    "sent": "What we can do to improve it or what we have done to improve it so?",
                    "label": 0
                },
                {
                    "sent": "We see here.",
                    "label": 0
                },
                {
                    "sent": "This is the shortest path between the two nodes in each tree and bonus community proposed to simply compare each node.",
                    "label": 0
                },
                {
                    "sent": "Count the matching attributes.",
                    "label": 0
                },
                {
                    "sent": "Here we used by active mapping between words and vertices.",
                    "label": 0
                },
                {
                    "sent": "Each node is associated with some features like the entity type, the word type, the part of speech tag, and the string.",
                    "label": 0
                },
                {
                    "sent": "The word it says.",
                    "label": 0
                },
                {
                    "sent": "So they propose to count the matching.",
                    "label": 0
                },
                {
                    "sent": "Attributes this is an exact match they proposed, so like we see here 3 attributes match and NP the part of speech Tech matches for both nodes.",
                    "label": 0
                },
                {
                    "sent": "The general Public Speech text, which is a growth category, matches and entity Pie type match sewists.",
                    "label": 0
                },
                {
                    "sent": "Three, the similarity would be 3 and they propose to multiply each similarity in each comparison on the path to get the kernel value for these two instances, yes.",
                    "label": 0
                },
                {
                    "sent": "Example that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "President President, why you're saying?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is an annotated data so.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "President, not Obama between Obama and US.",
                    "label": 0
                },
                {
                    "sent": "8 is.",
                    "label": 0
                },
                {
                    "sent": "If you remember, the relation is technically task list between entities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "President no use as entity.",
                    "label": 0
                },
                {
                    "sent": "This is why it's picked depicted bold and so this is the shortest path.",
                    "label": 0
                },
                {
                    "sent": "And this is the shortest path so.",
                    "label": 0
                },
                {
                    "sent": "If you plot this this way.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They propose to multiply it to get an incredible you.",
                    "label": 0
                },
                {
                    "sent": "Of course they normalized it by length.",
                    "label": 0
                },
                {
                    "sent": "So however, there are some restrictions if it's in computation is of course possible, but however there are some limitations.",
                    "label": 0
                },
                {
                    "sent": "Becausw they proposed.",
                    "label": 0
                },
                {
                    "sent": "If only one you see, we gotta directed.",
                    "label": 0
                },
                {
                    "sent": "Rough three, so you see the edge direction is important if one at direction does not match in this path so well we would be 0.",
                    "label": 0
                },
                {
                    "sent": "If one note does not match, good.",
                    "label": 0
                },
                {
                    "sent": "The similarity would be 0.",
                    "label": 0
                },
                {
                    "sent": "This is a harsh restriction.",
                    "label": 0
                },
                {
                    "sent": "However, this kernel would use quite good performance.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's have a look at another kernel who have not this shortcoming.",
                    "label": 0
                },
                {
                    "sent": "It's a dependency tree kernel bike.",
                    "label": 1
                },
                {
                    "sent": "A lot ahead I.",
                    "label": 0
                },
                {
                    "sent": "So what does this corner do?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Prepare denies enemy nation to show.",
                    "label": 0
                },
                {
                    "sent": "It computes the matching attributes between these two nodes.",
                    "label": 0
                },
                {
                    "sent": "Then it's traverse to the children, computes similarity between these notes.",
                    "label": 1
                },
                {
                    "sent": "You see there is no match.",
                    "label": 0
                },
                {
                    "sent": "OK, you cannot see it.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's so small, but I tell you there's no match, so but he will continue to compare each possible pairs of the other top sequences of the children's of each nodes.",
                    "label": 0
                },
                {
                    "sent": "If there's a match, he will traverse down and continue computation.",
                    "label": 0
                },
                {
                    "sent": "So no match again.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, however, it does not only compare one.",
                    "label": 0
                },
                {
                    "sent": "The subsequences of length one, so not up to our length of the children, so also this subsequence would be compared.",
                    "label": 0
                },
                {
                    "sent": "And it will be the sum of the similarities would be the similarity between these two instances.",
                    "label": 0
                },
                {
                    "sent": "So this would be another.",
                    "label": 0
                },
                {
                    "sent": "Comparison made during computation.",
                    "label": 0
                },
                {
                    "sent": "So, however, this night is this current is.",
                    "label": 0
                },
                {
                    "sent": "That there is a note similarity function, which is rather nice when we exploit this nice property of dependency pasteries suggested by active mapping between words and vertices is really nice.",
                    "label": 0
                },
                {
                    "sent": "However, when there's no match, it will not reverse down everything on a level below will be cut off, will be not contribute to the similarity.",
                    "label": 0
                },
                {
                    "sent": "This is not good I think, and our results have shown that this is indeed not good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could be improved.",
                    "label": 0
                },
                {
                    "sent": "So let's come to our first approach so the decay only considers the root nodes of the subtrees and their matching children's of course.",
                    "label": 1
                },
                {
                    "sent": "This discuss possible similar structure and similarities at lower levels of the trees.",
                    "label": 0
                },
                {
                    "sent": "So our first approach was to apply the DTK to every combination of notes in the subtrees implied by the relation nodes.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's have a look what our so called alpass dependency tree kernel news so.",
                    "label": 0
                },
                {
                    "sent": "We have here our kernel.",
                    "label": 0
                },
                {
                    "sent": "It considers all pairs of the subtrees reacts is this subtree.",
                    "label": 0
                },
                {
                    "sent": "This is a subtree implied by the relation nodes, where this is what this is this and subtree is simply the implied subtrees implied complete subtree.",
                    "label": 0
                },
                {
                    "sent": "So for this it's also the same.",
                    "label": 0
                },
                {
                    "sent": "So what have we done?",
                    "label": 0
                },
                {
                    "sent": "We simply propose to call the dependency tree kernel of Carlotta.",
                    "label": 0
                },
                {
                    "sent": "When every possible pair of combination.",
                    "label": 0
                },
                {
                    "sent": "Between.",
                    "label": 0
                },
                {
                    "sent": "Each subtree so required across the corner on this pair and on this pair, and on despair, and simply takes exam.",
                    "label": 0
                },
                {
                    "sent": "So, however, it's quite computational, more complex, but however we have also proposed a way to tackle this.",
                    "label": 0
                },
                {
                    "sent": "To do this efficient so.",
                    "label": 0
                },
                {
                    "sent": "This was SSL main idea of our or past dependency tree corner so.",
                    "label": 0
                },
                {
                    "sent": "Basically it's the.",
                    "label": 0
                },
                {
                    "sent": "Idk some of every possible pair.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The motivation of our second approach is the so called possibilities by business community shown by there.",
                    "label": 0
                },
                {
                    "sent": "Results that the most valuable information on is on the path between the two entities.",
                    "label": 1
                },
                {
                    "sent": "This is why they proposed the shortest path kernel.",
                    "label": 1
                },
                {
                    "sent": "So, but also valuable information is close to this part or below, so our second approach is to consider the information below and on the path in parallel.",
                    "label": 1
                },
                {
                    "sent": "So we propose to apply the TTK to the path note sequences instead of only both root notes.",
                    "label": 0
                },
                {
                    "sent": "You know the TTK was only called on the root notes and.",
                    "label": 0
                },
                {
                    "sent": "Cut off this cuts.",
                    "label": 0
                },
                {
                    "sent": "Valuable informations below.",
                    "label": 0
                },
                {
                    "sent": "If just one note does not match.",
                    "label": 0
                },
                {
                    "sent": "However, if you called it IDK on every possible combination of notes on the.",
                    "label": 0
                },
                {
                    "sent": "Pass will tackle this shortcomings.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We also propose you know the.",
                    "label": 1
                },
                {
                    "sent": "This restriction of the shortest path kernel that passed need to have the same length as the similarity would be 0.",
                    "label": 0
                },
                {
                    "sent": "So we relaxing this by.",
                    "label": 0
                },
                {
                    "sent": "Using all possible subsequences of notes on the path and calling the TTK on all possible subsequences.",
                    "label": 0
                },
                {
                    "sent": "Which can also be.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Non efficient, so let's have a look.",
                    "label": 0
                },
                {
                    "sent": "What does our kernel deuce in detail?",
                    "label": 0
                },
                {
                    "sent": "This is the shortest path in light Gray.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We are calling basically the TTK on every possible combinations of.",
                    "label": 0
                },
                {
                    "sent": "Notes only on the path in the matching subsequences up to a length of Q.",
                    "label": 1
                },
                {
                    "sent": "This one parameter we experimented with so.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Here is the farmer, basically this.",
                    "label": 0
                },
                {
                    "sent": "Is an enumeration of all possible subsequences?",
                    "label": 0
                },
                {
                    "sent": "Of the same length up to length.",
                    "label": 0
                },
                {
                    "sent": "Q Odessa typo I missed that.",
                    "label": 1
                },
                {
                    "sent": "So this is a penalty for the length of the gaps, so this is nice to introduce.",
                    "label": 0
                },
                {
                    "sent": "We have experimented also on this parameter and this is basically the clutter kernel.",
                    "label": 1
                },
                {
                    "sent": "However broken up some details are in the paper.",
                    "label": 0
                },
                {
                    "sent": "We have broken this up because this allows for efficient computation, but I cannot go into details through technical I think.",
                    "label": 0
                },
                {
                    "sent": "More details on the paper if you have any questions, just ask me after.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How can?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see, we're doing here a lot of computations.",
                    "label": 0
                },
                {
                    "sent": "A lot of computations are the same.",
                    "label": 0
                },
                {
                    "sent": "Just remember that and also remember that the degree is not that high.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Industries so.",
                    "label": 0
                },
                {
                    "sent": "What's about the efficient computation of TDK?",
                    "label": 1
                },
                {
                    "sent": "Because we called the TTK so much and use it so often, we thought about yeah, OK, so it was rather slow at first, but cause clotted all applied the ideas of the string subsequence kernel from Moody at all to the computation of the tight subsequences.",
                    "label": 1
                },
                {
                    "sent": "This yield and algorithms with run in the order of M * N in the power of three.",
                    "label": 0
                },
                {
                    "sent": "This is rather slow.",
                    "label": 0
                },
                {
                    "sent": "Especially for kernel computation, when there when we call this TTK is that often, so we adopted a later optimization by Christiani and short Taylor which runs in M times N .2 to the power of 2.",
                    "label": 0
                },
                {
                    "sent": "This was this was proposed for the for string subsequent problem, but we adapted this for this child computation problems with this dealing factor, however.",
                    "label": 1
                },
                {
                    "sent": "A close look into data that reveals that independent re nodes have very very few children's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look into detail.",
                    "label": 0
                },
                {
                    "sent": "Of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look South degree of the North has the following distribution most notes.",
                    "label": 1
                },
                {
                    "sent": "About 658% of the nodes have just one children in the resulting forest, often benchmark data set.",
                    "label": 0
                },
                {
                    "sent": "Only.",
                    "label": 0
                },
                {
                    "sent": "If you go here up to five nodes covers 99% of the data, so maybe all these optimization which only holds for large values of M&N is not good for this computation in our case because we have this distribution, so we have sought what can we do else because there are not that many computational.",
                    "label": 0
                },
                {
                    "sent": "Stuff we don't need.",
                    "label": 0
                },
                {
                    "sent": "Maybe this they have proposed dynamic programming nation.",
                    "label": 0
                },
                {
                    "sent": "This metrics need to be analyzed.",
                    "label": 0
                },
                {
                    "sent": "It's rather fast.",
                    "label": 0
                },
                {
                    "sent": "This bounce holds.",
                    "label": 1
                },
                {
                    "sent": "We have implemented this.",
                    "label": 0
                },
                {
                    "sent": "However, if you just do an efficient caching.",
                    "label": 0
                },
                {
                    "sent": "You are faster, very faster.",
                    "label": 0
                },
                {
                    "sent": "We have cut the runtime biasing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "86% about so let's have a look why this is so.",
                    "label": 0
                },
                {
                    "sent": "If you go to the exact runtime and don't forget all this nasty constants in their own notation.",
                    "label": 0
                },
                {
                    "sent": "So if we're really count all these constants, also we see here, this is the line one is our proposed solution by caching also by also called index cache.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You see the exact runtime of the short Taylor solution with running M * N to the power of two.",
                    "label": 0
                },
                {
                    "sent": "You see for 96% of the data hour.",
                    "label": 1
                },
                {
                    "sent": "In this case, strategy is faster.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Or this also hold for the colossal ecosolutions, which is also faster.",
                    "label": 0
                },
                {
                    "sent": "All this translate to in runtime improvement of 66% is rather nice, and this is only by catching and exploiting the properties of these dependency trees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool.",
                    "label": 0
                },
                {
                    "sent": "Let's look at our empirical evaluation.",
                    "label": 1
                },
                {
                    "sent": "We have experimental conducted on ACA 2003 data set.",
                    "label": 1
                },
                {
                    "sent": "This is a public available benchmark data set for the task of.",
                    "label": 0
                },
                {
                    "sent": "Relation extraction this is by the so-called AC program.",
                    "label": 0
                },
                {
                    "sent": "Automatic content extraction program started in the 90s.",
                    "label": 0
                },
                {
                    "sent": "Now it's called Tuck Technologies conferences and their proposed new task this year.",
                    "label": 0
                },
                {
                    "sent": "However, we have only experimented on the ACA 2003 data set which consists of newspaper texts, annotated buoys, entities and relations between gems there about 10,000 trees, 500 documents and yes.",
                    "label": 1
                },
                {
                    "sent": "Trees fell sentence are 10 thousands sentence.",
                    "label": 0
                },
                {
                    "sent": "So there are annotated so-called five top level relations.",
                    "label": 0
                },
                {
                    "sent": "The relation remember the examples Bama and Microsoft and Obama, and you say so.",
                    "label": 0
                },
                {
                    "sent": "Social relation is somewhat hard to define near at a clear.",
                    "label": 0
                },
                {
                    "sent": "An part is also clear.",
                    "label": 0
                },
                {
                    "sent": "However, these are their relations which are annotated.",
                    "label": 0
                },
                {
                    "sent": "We have conducted some experiments.",
                    "label": 0
                },
                {
                    "sent": "We have run a five times repeated 5 fold cross validation to compare the different approaches.",
                    "label": 0
                },
                {
                    "sent": "We also use the pre specified split.",
                    "label": 0
                },
                {
                    "sent": "You know the status head comes within test training data pre specified.",
                    "label": 0
                },
                {
                    "sent": "Split, so let's have a look.",
                    "label": 0
                },
                {
                    "sent": "We have some relations with.",
                    "label": 0
                },
                {
                    "sent": "A lot of training instances like role.",
                    "label": 0
                },
                {
                    "sent": "And part.",
                    "label": 0
                },
                {
                    "sent": "And at and.",
                    "label": 0
                },
                {
                    "sent": "However, there are some was only very few training examples.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look into real light in more detail.",
                    "label": 0
                },
                {
                    "sent": "What we see here is we have reimplemented the shortest path Colonel DDK kernel, of course, but cause we are needed it so we have run them all on the same data on the same set and compare the results.",
                    "label": 0
                },
                {
                    "sent": "What we see here?",
                    "label": 0
                },
                {
                    "sent": "Is that the shortest path which quite a high F micro value for this simple approach of 58.6%, However, we can reach F micro by using the pass TTC of up to nearly 70%.",
                    "label": 0
                },
                {
                    "sent": "This is a rather big improvement even over the.",
                    "label": 0
                },
                {
                    "sent": "TTK, but we also see here is the F macro value, you know, recall macros, overall category, and micros overall instances so.",
                    "label": 0
                },
                {
                    "sent": "Off 5070.7 this is even better than our past TTC, but however we look into the results in more detail to see why this happens.",
                    "label": 0
                },
                {
                    "sent": "Basically, one can say capacity carries a good kernel for relation extraction, so we also run this on the pre specified training test.",
                    "label": 0
                },
                {
                    "sent": "Split what we see here is also a big improvement over the previous approaches of our proposed approach.",
                    "label": 0
                },
                {
                    "sent": "However, what's nice to see is that the precision of the alpines TTK is quite higher.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One can say that for some relations will reach quite high results which make maybe this approach a pliable for real world problems like semantic search for information retrieval, something like that or or knowledge based population versus what we see here is.",
                    "label": 0
                },
                {
                    "sent": "That the authors depends if we can perform best.",
                    "label": 0
                },
                {
                    "sent": "On the small relations where there are not many training data available, you remember there were only like 40 or 50 training data available there.",
                    "label": 0
                },
                {
                    "sent": "For the other relations there, like 405 hundred training instances available, what we see here is that on all pick relations where there is a lot of training data available, the party TK outperforms or other kernels.",
                    "label": 0
                },
                {
                    "sent": "Of course we have run a significance test and there's very significant difference and.",
                    "label": 0
                },
                {
                    "sent": "It's only outperformed on the relations.",
                    "label": 0
                },
                {
                    "sent": "It's not that much training data by the opacity K, so it seems as the positive case a good color for relation extraction when there is a lot of.",
                    "label": 0
                },
                {
                    "sent": "Training data available will, whereas a lot is not that much and you think about it's only what 506 hundred sentence one can annotate this in about one day today so.",
                    "label": 0
                },
                {
                    "sent": "We're running you tests on this right now, so yes, this was already told.",
                    "label": 0
                },
                {
                    "sent": "Party care better for relations.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's more later this is a conclusion, so now I will conclude my talk that performance what we've seen here is the performance depends on the type of relations into training size.",
                    "label": 1
                },
                {
                    "sent": "Because some relations are reservoir, so the social relation is Reza unclear.",
                    "label": 0
                },
                {
                    "sent": "What exactly means?",
                    "label": 0
                },
                {
                    "sent": "So there I think there would be one human annotators.",
                    "label": 0
                },
                {
                    "sent": "There would be a low Inter annotator agreement for this, so for some.",
                    "label": 0
                },
                {
                    "sent": "Relations, we have sufficient performance for real applications like knowledge based population or semantic search or something like that.",
                    "label": 1
                },
                {
                    "sent": "So however we only considered.",
                    "label": 0
                },
                {
                    "sent": "Dependency past trees in this work.",
                    "label": 0
                },
                {
                    "sent": "However, I combination with other kernels with kernels for other types of past researchers also reasonable at sea.",
                    "label": 0
                },
                {
                    "sent": "2009 we've shown does the combination with the state of the art fast, past three kernel outperforms all other approaches.",
                    "label": 0
                },
                {
                    "sent": "So what are what is future work we can do with this is for once make our.",
                    "label": 0
                },
                {
                    "sent": "SV M3 Kernel toolkit freely available.",
                    "label": 0
                },
                {
                    "sent": "It's rather nice.",
                    "label": 1
                },
                {
                    "sent": "We're working on this.",
                    "label": 0
                },
                {
                    "sent": "Another part would be considered more sophisticated, not similarity functions, you know.",
                    "label": 0
                },
                {
                    "sent": "And we can exploit this.",
                    "label": 0
                },
                {
                    "sent": "Be active mappings even more because words can have subtypes or something like that.",
                    "label": 0
                },
                {
                    "sent": "You can determine the sense of the word is all.",
                    "label": 0
                },
                {
                    "sent": "This should improve this note similarity functions.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Work to be done there.",
                    "label": 1
                },
                {
                    "sent": "We're looking into this so as nice would be a combination of different machine learning approaches.",
                    "label": 0
                },
                {
                    "sent": "For example pattern recognition or pattern mining approaches and.",
                    "label": 0
                },
                {
                    "sent": "Last but not least, the application to other language would be nice.",
                    "label": 0
                },
                {
                    "sent": "We are working on this right now.",
                    "label": 0
                },
                {
                    "sent": "The problem example, an example for drama set down is no publicly available data set, so we need to work on annotated data to make a data set so that we can compare the different approaches.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Thank you all.",
                    "label": 0
                },
                {
                    "sent": "Earlier slides that.",
                    "label": 0
                },
                {
                    "sent": "First ones that are structured in input.",
                    "label": 0
                },
                {
                    "sent": "You only have the option of the implicit inner product.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Say stop.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "It's one of the first OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It was the shortest path corner.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so I guess I'm asking that is.",
                    "label": 0
                },
                {
                    "sent": "Concerned with with performance and I was just wondering if you had looked at at explicit propositional isation as an option for speeding things up.",
                    "label": 0
                },
                {
                    "sent": "If that works for your data set, that's not possible.",
                    "label": 0
                },
                {
                    "sent": "So so in other words, explicitly representing the feature space.",
                    "label": 0
                },
                {
                    "sent": "Yes, it is sometimes possible.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, it's it's for this kernel would be possible to have the mapping in the feature space, but it's rather hard to define them.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "King of the collateral to feature space.",
                    "label": 0
                },
                {
                    "sent": "This girl is so park we don't know the expected mapping, so.",
                    "label": 0
                },
                {
                    "sent": "Because this with all this.",
                    "label": 0
                },
                {
                    "sent": "Copper is made there.",
                    "label": 0
                },
                {
                    "sent": "We don't know this mapping, but wherever nice for kernels is that we don't need to know this mapping and still computers efficiently.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes we have thrown off course that these are kernels.",
                    "label": 0
                },
                {
                    "sent": "This but is rather clear.",
                    "label": 0
                },
                {
                    "sent": "Becausw could have shown that this is a kernel and we make only sums and product of valid kernels so.",
                    "label": 0
                },
                {
                    "sent": "Our.",
                    "label": 0
                },
                {
                    "sent": "Also Colonel so.",
                    "label": 0
                },
                {
                    "sent": "You say?",
                    "label": 0
                },
                {
                    "sent": "For relationships.",
                    "label": 0
                },
                {
                    "sent": "Or Russia.",
                    "label": 0
                },
                {
                    "sent": "First, then the more restrictive approach is more real.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They won't make more, oh.",
                    "label": 0
                },
                {
                    "sent": "Oh no, you you got me wrong.",
                    "label": 0
                },
                {
                    "sent": "Actually this is our second approach and actually this is a more general approach.",
                    "label": 0
                },
                {
                    "sent": "This is to all pairs, kernel, the all pairs kind of computes the similarity between every pair, so of course it's more general than only comparing notes on the path.",
                    "label": 0
                },
                {
                    "sent": "And the substructure below them.",
                    "label": 0
                },
                {
                    "sent": "Better when you accept yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, when there is more data available, the more restricted one of their power.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Optimizing the.",
                    "label": 0
                },
                {
                    "sent": "Which came back with caching, yes.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About it and send it to me, but how does this response work better with the find the bus space?",
                    "label": 0
                },
                {
                    "sent": "Oh oh, the past the passing times of course very high.",
                    "label": 0
                },
                {
                    "sent": "It's about one sentence, the second.",
                    "label": 0
                },
                {
                    "sent": "However, this can be done in parallel like we have like 10 machines and can run this and it can be stored.",
                    "label": 0
                },
                {
                    "sent": "Of course we have run this one time it have take sometimes have take about 9000 seconds.",
                    "label": 0
                },
                {
                    "sent": "You can break that down.",
                    "label": 0
                },
                {
                    "sent": "Then we have saved the trees and use it for more.",
                    "label": 0
                },
                {
                    "sent": "However, this however, all these current computation are quite take quite some time, so this efficient.",
                    "label": 0
                },
                {
                    "sent": "Computation of the TTK is for the DDK alone.",
                    "label": 1
                },
                {
                    "sent": "It was not important to optimize it that much, however, because we used the TTK so often the optimization was needed to reduce the runtime to realistic levels so that we can apply it on real datasets.",
                    "label": 0
                },
                {
                    "sent": "Which we may face in our work.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}