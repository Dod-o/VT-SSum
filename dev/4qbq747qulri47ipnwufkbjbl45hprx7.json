{
    "id": "4qbq747qulri47ipnwufkbjbl45hprx7",
    "title": "Active and passive learning of linear separators under log-concave distributions",
    "info": {
        "author": [
            "Maria-Florina Balcan, College of Computing, Georgia Institute of Technology"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_balcan_learning/",
    "segmentation": [
        [
            "Alright, thank you so as Sunday mentioned, this is joint work with Phil Long.",
            "It is about active."
        ],
        [
            "Possible learning of linear separators.",
            "OK, let me actually start to the two minute version of this talk.",
            "So in this work we provide new results concerning labor efficient polynomial time algorithms for active and passive learning of linear separators, and more specifically we first show, but active learning provides an exponential improvement about passive supervised means.",
            "Park learning of linear separators, homogeneous linear separators under lock and key.",
            "Exhibitions.",
            "An building on this we then provide polynomial time park algorithms for learning.",
            "I will provide the polynomial time part of optimal sample complexity for such problems, and this resolves an open question concerning label complexity of polynomial time algorithms for the case on the underlying distribution is uniform up in the unit ball.",
            "And more generally, provides as far as we know the first bound, but is tight for a polynomial time pack.",
            "Algorithms for learning and interesting infinite class or hypothesis functions under a natural and broad class of data distributions, and this is related to a longstanding open question concerning the label complexity of efficient computational efficient algorithms and with an open question standing from Stanley Papers impact learning.",
            "OK."
        ],
        [
            "However, we also provide new improved bounds for active, both active and passive learning.",
            "In the case of the data might not be linearly separable both in the agnostic case and we do so by analyzing the disagreement coefficient and under specialized conditions like the popular to back of loud noise condition, and Furthermore we provide extensions to nearly low concave distributions, and here we present our results providing structure results for this distribution as well.",
            "And for simplicity in this talk I'll focus only on the noisy setting on the realizable case, and low concave distributions.",
            "Hype."
        ],
        [
            "So.",
            "Well, as you probably all know, in the passive supervised park setting, so the learning algorithm is given a set S of labeled examples drawn IID from some fixed but unknown distribution over distance space labeled according to some fixed but unknown target function, and the goal of the algorithm is to do some optimization over the sample in order to find hypothesis of small classification error with respect to the underlying distribution.",
            "Now the case where the target function belongs to the concept class is called the realisable case, right?",
            "The case or the function does not belong to our search space is called the agnostic case and again in the park setting.",
            "We also explicitly requiring that our algorithms are polynomial time."
        ],
        [
            "Cool, and so is everyone here.",
            "Now this has been long studied and for instance, it's well known that if the number of labeled examples we see in the training process is the over epsilon times, Logan over epsilon plus one over epsilon Delta, then if we find the classifier that is consistent with this, many examples will guarantee that with probability at least someone is Delta the classifier.",
            "But we output hazard classification are at most epsilon.",
            "So for example for linear separator this immediately implies.",
            "A computational efficient algorithm with sample complexity.",
            "Because we know how to find.",
            "Consistent linear separators in polynomial time.",
            "OK, now this is about upper bounds.",
            "Now, at the other extreme, the best.",
            "On the other hand, the best known lower bound that we know is the over epsilon plus one over epsilon Logan over Delta.",
            "And in fact this lower bound also holds for learning linear separators when the underlying distribution is uniform that you need ball.",
            "Now clearly there is a lot of epsilon gap between these basic upper bound and this lower bound.",
            "Now there has been a lot of work, huge amount of work on getting a tighter upper bound.",
            "So for instance.",
            "Seminole work of house early to Stone and Ward moved or more recent work in statistical learning theory using the Alexandra capacity.",
            "However.",
            "There is still about that.",
            "There are still pesky polylogarithmic gaps between the best known bounds and the best known upper bounds, and this lower bound and this pesky gaps are very even for learning your separators under the uniform distribution.",
            "Now in this work, we provide computational efficient polymer time algorithm with tight sample complexity for learning linear separators, homogeneous operators under lock and key distributions, and Interestingly, we are able to provide this bond by drawing a connection by connection from active learning.",
            "So different learning paradigm.",
            "OK, so."
        ],
        [
            "You have seniority with active learning.",
            "Let me just briefly remind you so.",
            "In this setting, the algorithm first gets a large set of unlabeled examples, and then it can interactively ask for labels available to examples of its own choice.",
            "So the goal here is the goal of the learner here is to somehow use this interaction in order to drastically reduce the number of labels needed to learn lower hypothesis.",
            "So here the goal of the learner is to somehow be smarter to work harder in order to learn.",
            "If you are labeled examples.",
            "So the learner would want to.",
            "What examples without somehow informative in a certain sense?",
            "Ben"
        ],
        [
            "This has been a.",
            "This is that has been a well studied learning paradigms in recent years has been motivated by practical applications where have huge amounts of unlabeled data on the theoretical."
        ],
        [
            "I'd have been lots of exciting results in recent years as well, and now roughly classify these types of results so very rough classification is the following.",
            "On one hand we have very very we have very specific results.",
            "So for instance several result, several works have shown that if we are trying to learn.",
            "Medina senior separators and the underlying distribution is uniform in the unit ball.",
            "Then we can get an exponential improvement in the sample complexity.",
            "So clearly such results are very specific.",
            "On the other hand, at the other extreme we have very general algorithms that work for very generic concept classes.",
            "They are given in the presence of arbitrary forms of noise, so that means they work in the agnostic case.",
            "However, the vast majority of these results are suboptimal in query complexity.",
            "So for instance, a popular class class of algorithms studied here.",
            "This agreement based active learning algorithms, and these algorithms are incredibly mellow query for the label of an example.",
            "Unlabeled example, if there is just tiny bit of disagreement about the label of example.",
            "OK, so I think it's really important to develop algorithms that are more aggressive active learning algorithms that have better query complexity and."
        ],
        [
            "We're doing this work so specifically in this work, when allies in aggressive margin based active learning algorithm for learning homogeneous linear separators when the underlying distribution is low concave.",
            "And with outlook concave distributions we have seen yesterday language talk.",
            "This is a wide class of distributions, but includes the Gaussian distribution, exponential distribution and the uniform distribution over any convex set, and they have been widely used, studied actually in various areas including sampling, optimization, integration and also learning.",
            "OK, and what we show?",
            "In our work is that if when learning homogeneous linear separators and the underlying distribution is low concave, then in the realizable case we get an exponential improvement in active learning over passive learning.",
            "That is a true exponential improvement.",
            "So we get the local reps we deal over epsilon examples to find the classifier almost epsilon visiting the realizable case here.",
            "The algorithm will also be efficient, computational efficient and under various noise conditions.",
            "We are social label complexity improvements.",
            "The algorithms are not efficient anymore.",
            "OK, and as I promised and actually."
        ],
        [
            "To present the let me also mention right?",
            "So actually this broadened significantly lower than the class of problems for which we have concrete nearly optimal bounds for active learning.",
            "In particular for this class of problems, we're going to show improvement in the one epsilon factor, but without increasing the D factor as many of the other active learning results have been doing recently.",
            "OK, and in the rest of my talk I'm."
        ],
        [
            "To try to present the algorithm for the realizable case only.",
            "So the algorithm is very simple, is a margin based active learning algorithm.",
            "We first draw a number of unlabeled examples.",
            "We then label them with them to the working set within preceding rounds in each round.",
            "So for instance around K, we first find the hypothesis that is consistent with all the labels seen so far.",
            "All deliberate examples I've seen so far.",
            "Then we draw a number of unlabeled examples subject to being within a certain margin of the current guests of the target function.",
            "We label those and then we proceed.",
            "We go to the next round.",
            "OK so."
        ],
        [
            "Victoria Lee, let me just quickly give a registration, so we first draw a small number of unlabeled examples.",
            "We label them and then to the working set.",
            "Then at the beginning of the second round, we first find the hypothesis that is consistent.",
            "We've labeled examples.",
            "Then we draw a number of examples and two examples subject to being within margin, one at the current separator, cover current W under the target function.",
            "So they label this examples and add them to the working set and proceed with the next round so.",
            "At the beginning of the next round, we first find a classifier that is consistent with the examples we've seen so far.",
            "The labeled examples so far.",
            "We then draw a number of unlabeled examples subject to being within a certain margin of the current guess.",
            "We label those.",
            "Add them to the working set and proceed with the next round.",
            "OK, so it's a margin based active learning.",
            "And."
        ],
        [
            "We can show about this algorithm is that if we pick if the margin gamma K between picking around K is roughly one over to the K, then if the number of labels we are asking for in down case roughly D or these are these dimensions dimension of our concept space at the dimensional space, then after Logan or Epsilon iterations with high probability will output the classifier abroad most epsilon.",
            "And so we're going to get so the number of labeled examples we need not only be deal over epsilon.",
            "If we sum the examples over all rounds.",
            "OK, and I actually want to briefly sketch the analysis of this.",
            "Of this algorithm, but before doing so, I want to."
        ],
        [
            "I will first describe some useful properties that we proved about low concave distributions.",
            "OK, so first of all here is the effect we can show that if the underlying distribution is low concave if we have two linear separators specified by normal vector CMB, then the probability that the two linear separators disagree is exactly proportional to the angle between the two normal vectors.",
            "And this can be proven by projecting the entire region of disagreement of these two separators onto the space spanned by the two dimensional space by the normal vectors U&V and then by using properties of low concave distributions in two dimensions.",
            "So for instance, we can use affect that into dimensions and density of concave distribution is lower bounded if we're closing up to the origin.",
            "Text.",
            "It is proportional is yeah.",
            "Here is it, like Wolf constant types distinguished notice that we've been a constant is not equal.",
            "It's been a constant is upper bounded by constant times the angle and lower body back on, sometimes the angle.",
            "OK, yeah, maybe senses proportional now.",
            "The 2nd that we have.",
            "Actually the second fact is a well known fact is the following.",
            "But if we have a lock and key distribution, the probability mass within margin gum over given separator is exactly is again upper bound by costs and times gamma OK. And this is 1 result and then finally a."
        ],
        [
            "Our result that we prove that this is actually affecting itself and requires a careful proof is the following.",
            "We can show that if we have again two linear separators, U&V specified yes.",
            "Yeah, it's a topic.",
            "Yeah, we can always put it in isotropic position first.",
            "Yeah, so it's isotropic.",
            "Yes, oh absolutely.",
            "So that I don't know in transformations to put in isotropic position.",
            "Right, so effect #3 is the following.",
            "So if you have two linear separators, specify binormal vectors U&V, but have an angle beta.",
            "So.",
            "So whether the angle between these two normal vectors is better, then we can show that we can pick a margin gamma, but is as small as a constant times beta and then still insured by the probability of disagreement of is to separators outside margin gamma of one event.",
            "So pictorial is the shaded region over here.",
            "So we can still be sure.",
            "But if gamma is as small as a constant times beta.",
            "The probability that we disagree outside marginal gamma.",
            "One of them is upper bounded by Betta over 2 orbital before say OK and this is a fact requires careful proof clearly if Betta, if, is very large then we expect will obviously be true.",
            "The key is to show that we can pick gamma to be a constant times better OK, and in order to do."
        ],
        [
            "To do so, I'm not going to go through the analysis, but what we have to do is to again project onto the 2 dimensional space spanned by normal vectors U&V, and then to do a careful shelling argument in that space.",
            "Fake."
        ],
        [
            "Focus after.",
            "Justin is not only helping.",
            "Yeah, you'll see the proof, so I'm not going to describe the excellent question.",
            "So now let me even respect.",
            "Let me describe the proof.",
            "OK, so we're going to show, so going back to the proof that we're going to show is that we're going to show by induction, but all the the separators that are consistent with the data in the working set up to one K have are almost over to the K. OK, so let's assume that this is Tilman K -- 1, and let's prove it for one K. So let's consider a separator that is consistent with the data in the working set up to one K -- 1.",
            "Say W. Now we're going to decompose the aerated W as separate outside margin, K minus one of the current separator.",
            "So pictorially, this is a blue region plus the error rate inside margin, K of the month or the current separator Victoria Lee.",
            "This is the red region.",
            "OK, now we pick W so that is consistent with the date and working set up to one K -- 1 and so it is double K -- 1.",
            "Our current guess.",
            "So that means that we separate also have error most unable to K -- 1 and so that means that their angle to the target function will be roughly.",
            "Sometimes in order to the game and so on, and then by using the fact free.",
            "But I had on the previous slide, we can then upper bound this term over here the error rate outside margin gamma came under some of the current separator by one over to the K plus one OK. Now So what it means now is that in or."
        ],
        [
            "We our goal is to show that the error of W will be at most unable to K, so that means that we only need to ensure that the second term over here they're right inside margin, came and some of the current separator is at most unable to the K plus one right.",
            "But now just notice."
        ],
        [
            "This term we can now rewrite it as the probability that the classifier makes an error subject to appoint following.",
            "Imagine, came and some of the current separator times the probability without example falls within margin,, none of the current separator.",
            "But now the key point is that this term over here the probability mass within gamma came under the current separator is small is the smallest gamma K -- 1 by fact 2.",
            "But I had on my slides and so gamma K -- 1 is roughly 1 / 2 K minus one.",
            "Make sure that this whole term over here second term is almost over to the K, so it means that you only need to ensure that the probability of our classifier makes an error subject falling within margin, came and some of the current separator is upper bounded by constant.",
            "OK, but in order to do so by standard sample complexity bounds or passive supervised learning, we only need order of the labeled examples.",
            "OK I have this log log on over epsilon term here because there is always a small chance of failure ING a failure and we need to spread the confidence parameters over the log one over epsilon around.",
            "She should be log log one over epsilon over Delta.",
            "OK, so that's the analysis for active learning Now what is now?"
        ],
        [
            "Facts about this analysis is that it can be extended to give an argument for ERM, for passive learning, an argument to give us optimal sample complexity.",
            "So specifically, what we can show is that any passive learning algorithm that outputs a classifier that is consistent with the over epsilon plus one over epsilon Logan over Delta label random labeled examples with probability at least another Delta output the classifier over almost epsilon.",
            "And I'm not going to go through.",
            "Detail conceptually will just going to the similarities and differ active learning.",
            "We're going to run.",
            "Imagine running the algorithm online on progressively larger chunks, and we're going to be able to show is that the algorithm will perform well.",
            "If it periodically builds against the target function and also ignore some of the examples and it only updates based on the examples that are borderline examples, borderline cases for the previously built hypothesis.",
            "OK, so let's say high level idea.",
            "Now we have to be careful to in order to get the optimal bound, which is the point of this result, we have to be careful to distribute the data parameter carefully.",
            "In particular, we need to allow higher probability of Taylor in the later rounds because we have to account for the fact that once.",
            "Hypothesis pretty good, will take longer to be able to get an example that is informative, OK, but going for the details carefully we get this result.",
            "The promise result, and yes, even just analysis of the margin that you are likely to get to will not suffice for local distributions.",
            "Yeah, it will suffice, but you still have to display Delta unevenly.",
            "Drive random sample and then just look what kind of marginal you you expect to get, and then because you know that the margin will immediately translate for any distributional, easily translated for hours, so yeah."
        ],
        [
            "That's where you will.",
            "You have to pay the other log one over epsilon factor.",
            "So if you analyze it directly or do we buy that actually?",
            "So this is the formula, so I just have to say round Ki need that I know.",
            "So I need roughly so many examples now how many examples I visit?",
            "The margin BK?",
            "Like how many examples when it wait is simple.",
            "So we can say uniform, analyze what's the expected margin on the samples that you will see, and then that that expected marginal determined the error of your hypothesis, right?",
            "I'm not sure I understand the question.",
            "Asking the question about this algorithm or about this argument.",
            "Like look that expected margin that you see on this article under uniform distribution and then do what?",
            "Ever be true level based on the path?",
            "Quicken says I'm confused.",
            "So what do you propose?",
            "You proposed an argument for PRM or do proposed right at the proposal you propose?",
            "How to analyze the label complexity of the passive learning algorithm?",
            "So just trying to see if this similar argument will of what simple argument for what?",
            "For for learning, for learning lesson, learning.",
            "So what is, erm?",
            "I mean, yeah, yeah, basically well, so I can.",
            "I mean people have tried to analyze your time in many, many ways they had tried to do localization.",
            "The point is that all those bonds are not strong enough.",
            "So basically what we do here.",
            "Conceptual today we do a more aggressive localization, so we basically show that you go conceptually run online your algorithm and you can ignore some of the examples.",
            "And if you carefully basically only pick examples at the better informative, you can still argue progress.",
            "So it's a.",
            "It's a very aggressive localization somehow.",
            "Localization in space.",
            "So all the known allowed, although now this is a volume for this problem, suffer additional log factors.",
            "Logan over Epsilon Lock of the Alexander Capacity which is large, is actually sort of before this case, so I don't know any simple analysis that gives optimal bound here except for this one.",
            "OK, actually basically one or two to think about this result for passive learning is that our algorithm for active learning can be implemented with optimal unlabeled sample complexity as well.",
            "This is one way to think about it, so we can prove it.",
            "Basically, the number of unlabeled examples for algorithm that would be roughly that suffices would be over epsilon plus over epsilon Delta.",
            "This is an equivalent way to think about this is all for passive learning.",
            "OK, I have 0 minutes, so I'll just quickly finish and then I'll take more questions."
        ],
        [
            "So.",
            "Alright, so basically to summarize, in this work, analyze active and passive learning of linear separators on the local K distributions.",
            "For active learning we broaden the class of problems for which we can show with active learning.",
            "Probably provides an exponential improvement over passive learning and exponential improvement in true exponential improvement.",
            "We improve exponentially in one over epsilon factor, but without increasing the D factor is done by many other analysis.",
            "And then for passive learning things that we provide as far as I know, we provide the first bound for a polynomial time pack for a polynomial time pack algorithm that is tight for an interesting infinite class of hypothesis functions under the general and broad class and natural class of distributions model.",
            "And I didn't talk about this in my talk, but we also have extensions to nearly low concave distributions and we also show matching lower bound.",
            "Obviously only showed the upper bound, but everything that I showed his matching lower bound.",
            "And in terms of open questions, obviously the very natural interesting open questions.",
            "The first one is our analysis is currently still specific to linear separators and lock and key distributions to very interesting to provide aggressive query efficient active learning algorithms for general settings for general distributions for linear separators and for general concept spaces as well, and for passive learning.",
            "Obviously it will be interesting to close the gaps between upper.",
            "Between our upper and lower bounds for general concept spaces.",
            "Alright, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thank you so as Sunday mentioned, this is joint work with Phil Long.",
                    "label": 0
                },
                {
                    "sent": "It is about active.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possible learning of linear separators.",
                    "label": 0
                },
                {
                    "sent": "OK, let me actually start to the two minute version of this talk.",
                    "label": 0
                },
                {
                    "sent": "So in this work we provide new results concerning labor efficient polynomial time algorithms for active and passive learning of linear separators, and more specifically we first show, but active learning provides an exponential improvement about passive supervised means.",
                    "label": 1
                },
                {
                    "sent": "Park learning of linear separators, homogeneous linear separators under lock and key.",
                    "label": 0
                },
                {
                    "sent": "Exhibitions.",
                    "label": 0
                },
                {
                    "sent": "An building on this we then provide polynomial time park algorithms for learning.",
                    "label": 1
                },
                {
                    "sent": "I will provide the polynomial time part of optimal sample complexity for such problems, and this resolves an open question concerning label complexity of polynomial time algorithms for the case on the underlying distribution is uniform up in the unit ball.",
                    "label": 0
                },
                {
                    "sent": "And more generally, provides as far as we know the first bound, but is tight for a polynomial time pack.",
                    "label": 0
                },
                {
                    "sent": "Algorithms for learning and interesting infinite class or hypothesis functions under a natural and broad class of data distributions, and this is related to a longstanding open question concerning the label complexity of efficient computational efficient algorithms and with an open question standing from Stanley Papers impact learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, we also provide new improved bounds for active, both active and passive learning.",
                    "label": 1
                },
                {
                    "sent": "In the case of the data might not be linearly separable both in the agnostic case and we do so by analyzing the disagreement coefficient and under specialized conditions like the popular to back of loud noise condition, and Furthermore we provide extensions to nearly low concave distributions, and here we present our results providing structure results for this distribution as well.",
                    "label": 1
                },
                {
                    "sent": "And for simplicity in this talk I'll focus only on the noisy setting on the realizable case, and low concave distributions.",
                    "label": 0
                },
                {
                    "sent": "Hype.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, as you probably all know, in the passive supervised park setting, so the learning algorithm is given a set S of labeled examples drawn IID from some fixed but unknown distribution over distance space labeled according to some fixed but unknown target function, and the goal of the algorithm is to do some optimization over the sample in order to find hypothesis of small classification error with respect to the underlying distribution.",
                    "label": 1
                },
                {
                    "sent": "Now the case where the target function belongs to the concept class is called the realisable case, right?",
                    "label": 0
                },
                {
                    "sent": "The case or the function does not belong to our search space is called the agnostic case and again in the park setting.",
                    "label": 0
                },
                {
                    "sent": "We also explicitly requiring that our algorithms are polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool, and so is everyone here.",
                    "label": 0
                },
                {
                    "sent": "Now this has been long studied and for instance, it's well known that if the number of labeled examples we see in the training process is the over epsilon times, Logan over epsilon plus one over epsilon Delta, then if we find the classifier that is consistent with this, many examples will guarantee that with probability at least someone is Delta the classifier.",
                    "label": 0
                },
                {
                    "sent": "But we output hazard classification are at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "So for example for linear separator this immediately implies.",
                    "label": 0
                },
                {
                    "sent": "A computational efficient algorithm with sample complexity.",
                    "label": 1
                },
                {
                    "sent": "Because we know how to find.",
                    "label": 0
                },
                {
                    "sent": "Consistent linear separators in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is about upper bounds.",
                    "label": 0
                },
                {
                    "sent": "Now, at the other extreme, the best.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the best known lower bound that we know is the over epsilon plus one over epsilon Logan over Delta.",
                    "label": 0
                },
                {
                    "sent": "And in fact this lower bound also holds for learning linear separators when the underlying distribution is uniform that you need ball.",
                    "label": 0
                },
                {
                    "sent": "Now clearly there is a lot of epsilon gap between these basic upper bound and this lower bound.",
                    "label": 0
                },
                {
                    "sent": "Now there has been a lot of work, huge amount of work on getting a tighter upper bound.",
                    "label": 1
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "Seminole work of house early to Stone and Ward moved or more recent work in statistical learning theory using the Alexandra capacity.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "There is still about that.",
                    "label": 0
                },
                {
                    "sent": "There are still pesky polylogarithmic gaps between the best known bounds and the best known upper bounds, and this lower bound and this pesky gaps are very even for learning your separators under the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "Now in this work, we provide computational efficient polymer time algorithm with tight sample complexity for learning linear separators, homogeneous operators under lock and key distributions, and Interestingly, we are able to provide this bond by drawing a connection by connection from active learning.",
                    "label": 0
                },
                {
                    "sent": "So different learning paradigm.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have seniority with active learning.",
                    "label": 1
                },
                {
                    "sent": "Let me just briefly remind you so.",
                    "label": 0
                },
                {
                    "sent": "In this setting, the algorithm first gets a large set of unlabeled examples, and then it can interactively ask for labels available to examples of its own choice.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is the goal of the learner here is to somehow use this interaction in order to drastically reduce the number of labels needed to learn lower hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So here the goal of the learner is to somehow be smarter to work harder in order to learn.",
                    "label": 1
                },
                {
                    "sent": "If you are labeled examples.",
                    "label": 1
                },
                {
                    "sent": "So the learner would want to.",
                    "label": 0
                },
                {
                    "sent": "What examples without somehow informative in a certain sense?",
                    "label": 0
                },
                {
                    "sent": "Ben",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This has been a.",
                    "label": 0
                },
                {
                    "sent": "This is that has been a well studied learning paradigms in recent years has been motivated by practical applications where have huge amounts of unlabeled data on the theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd have been lots of exciting results in recent years as well, and now roughly classify these types of results so very rough classification is the following.",
                    "label": 1
                },
                {
                    "sent": "On one hand we have very very we have very specific results.",
                    "label": 0
                },
                {
                    "sent": "So for instance several result, several works have shown that if we are trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Medina senior separators and the underlying distribution is uniform in the unit ball.",
                    "label": 0
                },
                {
                    "sent": "Then we can get an exponential improvement in the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So clearly such results are very specific.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, at the other extreme we have very general algorithms that work for very generic concept classes.",
                    "label": 1
                },
                {
                    "sent": "They are given in the presence of arbitrary forms of noise, so that means they work in the agnostic case.",
                    "label": 0
                },
                {
                    "sent": "However, the vast majority of these results are suboptimal in query complexity.",
                    "label": 0
                },
                {
                    "sent": "So for instance, a popular class class of algorithms studied here.",
                    "label": 0
                },
                {
                    "sent": "This agreement based active learning algorithms, and these algorithms are incredibly mellow query for the label of an example.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled example, if there is just tiny bit of disagreement about the label of example.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think it's really important to develop algorithms that are more aggressive active learning algorithms that have better query complexity and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're doing this work so specifically in this work, when allies in aggressive margin based active learning algorithm for learning homogeneous linear separators when the underlying distribution is low concave.",
                    "label": 1
                },
                {
                    "sent": "And with outlook concave distributions we have seen yesterday language talk.",
                    "label": 0
                },
                {
                    "sent": "This is a wide class of distributions, but includes the Gaussian distribution, exponential distribution and the uniform distribution over any convex set, and they have been widely used, studied actually in various areas including sampling, optimization, integration and also learning.",
                    "label": 1
                },
                {
                    "sent": "OK, and what we show?",
                    "label": 1
                },
                {
                    "sent": "In our work is that if when learning homogeneous linear separators and the underlying distribution is low concave, then in the realizable case we get an exponential improvement in active learning over passive learning.",
                    "label": 0
                },
                {
                    "sent": "That is a true exponential improvement.",
                    "label": 0
                },
                {
                    "sent": "So we get the local reps we deal over epsilon examples to find the classifier almost epsilon visiting the realizable case here.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will also be efficient, computational efficient and under various noise conditions.",
                    "label": 0
                },
                {
                    "sent": "We are social label complexity improvements.",
                    "label": 0
                },
                {
                    "sent": "The algorithms are not efficient anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, and as I promised and actually.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To present the let me also mention right?",
                    "label": 0
                },
                {
                    "sent": "So actually this broadened significantly lower than the class of problems for which we have concrete nearly optimal bounds for active learning.",
                    "label": 1
                },
                {
                    "sent": "In particular for this class of problems, we're going to show improvement in the one epsilon factor, but without increasing the D factor as many of the other active learning results have been doing recently.",
                    "label": 0
                },
                {
                    "sent": "OK, and in the rest of my talk I'm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To try to present the algorithm for the realizable case only.",
                    "label": 1
                },
                {
                    "sent": "So the algorithm is very simple, is a margin based active learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "We first draw a number of unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "We then label them with them to the working set within preceding rounds in each round.",
                    "label": 1
                },
                {
                    "sent": "So for instance around K, we first find the hypothesis that is consistent with all the labels seen so far.",
                    "label": 0
                },
                {
                    "sent": "All deliberate examples I've seen so far.",
                    "label": 0
                },
                {
                    "sent": "Then we draw a number of unlabeled examples subject to being within a certain margin of the current guests of the target function.",
                    "label": 0
                },
                {
                    "sent": "We label those and then we proceed.",
                    "label": 0
                },
                {
                    "sent": "We go to the next round.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Victoria Lee, let me just quickly give a registration, so we first draw a small number of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "We label them and then to the working set.",
                    "label": 1
                },
                {
                    "sent": "Then at the beginning of the second round, we first find the hypothesis that is consistent.",
                    "label": 0
                },
                {
                    "sent": "We've labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Then we draw a number of examples and two examples subject to being within margin, one at the current separator, cover current W under the target function.",
                    "label": 0
                },
                {
                    "sent": "So they label this examples and add them to the working set and proceed with the next round so.",
                    "label": 1
                },
                {
                    "sent": "At the beginning of the next round, we first find a classifier that is consistent with the examples we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "The labeled examples so far.",
                    "label": 0
                },
                {
                    "sent": "We then draw a number of unlabeled examples subject to being within a certain margin of the current guess.",
                    "label": 1
                },
                {
                    "sent": "We label those.",
                    "label": 0
                },
                {
                    "sent": "Add them to the working set and proceed with the next round.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a margin based active learning.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show about this algorithm is that if we pick if the margin gamma K between picking around K is roughly one over to the K, then if the number of labels we are asking for in down case roughly D or these are these dimensions dimension of our concept space at the dimensional space, then after Logan or Epsilon iterations with high probability will output the classifier abroad most epsilon.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to get so the number of labeled examples we need not only be deal over epsilon.",
                    "label": 0
                },
                {
                    "sent": "If we sum the examples over all rounds.",
                    "label": 0
                },
                {
                    "sent": "OK, and I actually want to briefly sketch the analysis of this.",
                    "label": 0
                },
                {
                    "sent": "Of this algorithm, but before doing so, I want to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will first describe some useful properties that we proved about low concave distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all here is the effect we can show that if the underlying distribution is low concave if we have two linear separators specified by normal vector CMB, then the probability that the two linear separators disagree is exactly proportional to the angle between the two normal vectors.",
                    "label": 0
                },
                {
                    "sent": "And this can be proven by projecting the entire region of disagreement of these two separators onto the space spanned by the two dimensional space by the normal vectors U&V and then by using properties of low concave distributions in two dimensions.",
                    "label": 1
                },
                {
                    "sent": "So for instance, we can use affect that into dimensions and density of concave distribution is lower bounded if we're closing up to the origin.",
                    "label": 0
                },
                {
                    "sent": "Text.",
                    "label": 0
                },
                {
                    "sent": "It is proportional is yeah.",
                    "label": 0
                },
                {
                    "sent": "Here is it, like Wolf constant types distinguished notice that we've been a constant is not equal.",
                    "label": 0
                },
                {
                    "sent": "It's been a constant is upper bounded by constant times the angle and lower body back on, sometimes the angle.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, maybe senses proportional now.",
                    "label": 0
                },
                {
                    "sent": "The 2nd that we have.",
                    "label": 0
                },
                {
                    "sent": "Actually the second fact is a well known fact is the following.",
                    "label": 0
                },
                {
                    "sent": "But if we have a lock and key distribution, the probability mass within margin gum over given separator is exactly is again upper bound by costs and times gamma OK. And this is 1 result and then finally a.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our result that we prove that this is actually affecting itself and requires a careful proof is the following.",
                    "label": 0
                },
                {
                    "sent": "We can show that if we have again two linear separators, U&V specified yes.",
                    "label": 1
                },
                {
                    "sent": "Yeah, it's a topic.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can always put it in isotropic position first.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's isotropic.",
                    "label": 0
                },
                {
                    "sent": "Yes, oh absolutely.",
                    "label": 0
                },
                {
                    "sent": "So that I don't know in transformations to put in isotropic position.",
                    "label": 0
                },
                {
                    "sent": "Right, so effect #3 is the following.",
                    "label": 0
                },
                {
                    "sent": "So if you have two linear separators, specify binormal vectors U&V, but have an angle beta.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So whether the angle between these two normal vectors is better, then we can show that we can pick a margin gamma, but is as small as a constant times beta and then still insured by the probability of disagreement of is to separators outside margin gamma of one event.",
                    "label": 0
                },
                {
                    "sent": "So pictorial is the shaded region over here.",
                    "label": 0
                },
                {
                    "sent": "So we can still be sure.",
                    "label": 0
                },
                {
                    "sent": "But if gamma is as small as a constant times beta.",
                    "label": 0
                },
                {
                    "sent": "The probability that we disagree outside marginal gamma.",
                    "label": 0
                },
                {
                    "sent": "One of them is upper bounded by Betta over 2 orbital before say OK and this is a fact requires careful proof clearly if Betta, if, is very large then we expect will obviously be true.",
                    "label": 0
                },
                {
                    "sent": "The key is to show that we can pick gamma to be a constant times better OK, and in order to do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do so, I'm not going to go through the analysis, but what we have to do is to again project onto the 2 dimensional space spanned by normal vectors U&V, and then to do a careful shelling argument in that space.",
                    "label": 0
                },
                {
                    "sent": "Fake.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focus after.",
                    "label": 0
                },
                {
                    "sent": "Justin is not only helping.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you'll see the proof, so I'm not going to describe the excellent question.",
                    "label": 0
                },
                {
                    "sent": "So now let me even respect.",
                    "label": 0
                },
                {
                    "sent": "Let me describe the proof.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to show, so going back to the proof that we're going to show is that we're going to show by induction, but all the the separators that are consistent with the data in the working set up to one K have are almost over to the K. OK, so let's assume that this is Tilman K -- 1, and let's prove it for one K. So let's consider a separator that is consistent with the data in the working set up to one K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Say W. Now we're going to decompose the aerated W as separate outside margin, K minus one of the current separator.",
                    "label": 0
                },
                {
                    "sent": "So pictorially, this is a blue region plus the error rate inside margin, K of the month or the current separator Victoria Lee.",
                    "label": 0
                },
                {
                    "sent": "This is the red region.",
                    "label": 0
                },
                {
                    "sent": "OK, now we pick W so that is consistent with the date and working set up to one K -- 1 and so it is double K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Our current guess.",
                    "label": 0
                },
                {
                    "sent": "So that means that we separate also have error most unable to K -- 1 and so that means that their angle to the target function will be roughly.",
                    "label": 0
                },
                {
                    "sent": "Sometimes in order to the game and so on, and then by using the fact free.",
                    "label": 0
                },
                {
                    "sent": "But I had on the previous slide, we can then upper bound this term over here the error rate outside margin gamma came under some of the current separator by one over to the K plus one OK. Now So what it means now is that in or.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We our goal is to show that the error of W will be at most unable to K, so that means that we only need to ensure that the second term over here they're right inside margin, came and some of the current separator is at most unable to the K plus one right.",
                    "label": 0
                },
                {
                    "sent": "But now just notice.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This term we can now rewrite it as the probability that the classifier makes an error subject to appoint following.",
                    "label": 0
                },
                {
                    "sent": "Imagine, came and some of the current separator times the probability without example falls within margin,, none of the current separator.",
                    "label": 0
                },
                {
                    "sent": "But now the key point is that this term over here the probability mass within gamma came under the current separator is small is the smallest gamma K -- 1 by fact 2.",
                    "label": 0
                },
                {
                    "sent": "But I had on my slides and so gamma K -- 1 is roughly 1 / 2 K minus one.",
                    "label": 0
                },
                {
                    "sent": "Make sure that this whole term over here second term is almost over to the K, so it means that you only need to ensure that the probability of our classifier makes an error subject falling within margin, came and some of the current separator is upper bounded by constant.",
                    "label": 0
                },
                {
                    "sent": "OK, but in order to do so by standard sample complexity bounds or passive supervised learning, we only need order of the labeled examples.",
                    "label": 0
                },
                {
                    "sent": "OK I have this log log on over epsilon term here because there is always a small chance of failure ING a failure and we need to spread the confidence parameters over the log one over epsilon around.",
                    "label": 0
                },
                {
                    "sent": "She should be log log one over epsilon over Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the analysis for active learning Now what is now?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Facts about this analysis is that it can be extended to give an argument for ERM, for passive learning, an argument to give us optimal sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So specifically, what we can show is that any passive learning algorithm that outputs a classifier that is consistent with the over epsilon plus one over epsilon Logan over Delta label random labeled examples with probability at least another Delta output the classifier over almost epsilon.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to go through.",
                    "label": 0
                },
                {
                    "sent": "Detail conceptually will just going to the similarities and differ active learning.",
                    "label": 0
                },
                {
                    "sent": "We're going to run.",
                    "label": 0
                },
                {
                    "sent": "Imagine running the algorithm online on progressively larger chunks, and we're going to be able to show is that the algorithm will perform well.",
                    "label": 0
                },
                {
                    "sent": "If it periodically builds against the target function and also ignore some of the examples and it only updates based on the examples that are borderline examples, borderline cases for the previously built hypothesis.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's say high level idea.",
                    "label": 0
                },
                {
                    "sent": "Now we have to be careful to in order to get the optimal bound, which is the point of this result, we have to be careful to distribute the data parameter carefully.",
                    "label": 1
                },
                {
                    "sent": "In particular, we need to allow higher probability of Taylor in the later rounds because we have to account for the fact that once.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis pretty good, will take longer to be able to get an example that is informative, OK, but going for the details carefully we get this result.",
                    "label": 0
                },
                {
                    "sent": "The promise result, and yes, even just analysis of the margin that you are likely to get to will not suffice for local distributions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it will suffice, but you still have to display Delta unevenly.",
                    "label": 0
                },
                {
                    "sent": "Drive random sample and then just look what kind of marginal you you expect to get, and then because you know that the margin will immediately translate for any distributional, easily translated for hours, so yeah.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's where you will.",
                    "label": 0
                },
                {
                    "sent": "You have to pay the other log one over epsilon factor.",
                    "label": 0
                },
                {
                    "sent": "So if you analyze it directly or do we buy that actually?",
                    "label": 0
                },
                {
                    "sent": "So this is the formula, so I just have to say round Ki need that I know.",
                    "label": 0
                },
                {
                    "sent": "So I need roughly so many examples now how many examples I visit?",
                    "label": 0
                },
                {
                    "sent": "The margin BK?",
                    "label": 0
                },
                {
                    "sent": "Like how many examples when it wait is simple.",
                    "label": 0
                },
                {
                    "sent": "So we can say uniform, analyze what's the expected margin on the samples that you will see, and then that that expected marginal determined the error of your hypothesis, right?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I understand the question.",
                    "label": 0
                },
                {
                    "sent": "Asking the question about this algorithm or about this argument.",
                    "label": 0
                },
                {
                    "sent": "Like look that expected margin that you see on this article under uniform distribution and then do what?",
                    "label": 0
                },
                {
                    "sent": "Ever be true level based on the path?",
                    "label": 0
                },
                {
                    "sent": "Quicken says I'm confused.",
                    "label": 0
                },
                {
                    "sent": "So what do you propose?",
                    "label": 0
                },
                {
                    "sent": "You proposed an argument for PRM or do proposed right at the proposal you propose?",
                    "label": 0
                },
                {
                    "sent": "How to analyze the label complexity of the passive learning algorithm?",
                    "label": 1
                },
                {
                    "sent": "So just trying to see if this similar argument will of what simple argument for what?",
                    "label": 0
                },
                {
                    "sent": "For for learning, for learning lesson, learning.",
                    "label": 0
                },
                {
                    "sent": "So what is, erm?",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, yeah, basically well, so I can.",
                    "label": 0
                },
                {
                    "sent": "I mean people have tried to analyze your time in many, many ways they had tried to do localization.",
                    "label": 0
                },
                {
                    "sent": "The point is that all those bonds are not strong enough.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do here.",
                    "label": 0
                },
                {
                    "sent": "Conceptual today we do a more aggressive localization, so we basically show that you go conceptually run online your algorithm and you can ignore some of the examples.",
                    "label": 0
                },
                {
                    "sent": "And if you carefully basically only pick examples at the better informative, you can still argue progress.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a very aggressive localization somehow.",
                    "label": 0
                },
                {
                    "sent": "Localization in space.",
                    "label": 0
                },
                {
                    "sent": "So all the known allowed, although now this is a volume for this problem, suffer additional log factors.",
                    "label": 0
                },
                {
                    "sent": "Logan over Epsilon Lock of the Alexander Capacity which is large, is actually sort of before this case, so I don't know any simple analysis that gives optimal bound here except for this one.",
                    "label": 0
                },
                {
                    "sent": "OK, actually basically one or two to think about this result for passive learning is that our algorithm for active learning can be implemented with optimal unlabeled sample complexity as well.",
                    "label": 0
                },
                {
                    "sent": "This is one way to think about it, so we can prove it.",
                    "label": 0
                },
                {
                    "sent": "Basically, the number of unlabeled examples for algorithm that would be roughly that suffices would be over epsilon plus over epsilon Delta.",
                    "label": 1
                },
                {
                    "sent": "This is an equivalent way to think about this is all for passive learning.",
                    "label": 0
                },
                {
                    "sent": "OK, I have 0 minutes, so I'll just quickly finish and then I'll take more questions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Alright, so basically to summarize, in this work, analyze active and passive learning of linear separators on the local K distributions.",
                    "label": 0
                },
                {
                    "sent": "For active learning we broaden the class of problems for which we can show with active learning.",
                    "label": 1
                },
                {
                    "sent": "Probably provides an exponential improvement over passive learning and exponential improvement in true exponential improvement.",
                    "label": 1
                },
                {
                    "sent": "We improve exponentially in one over epsilon factor, but without increasing the D factor is done by many other analysis.",
                    "label": 0
                },
                {
                    "sent": "And then for passive learning things that we provide as far as I know, we provide the first bound for a polynomial time pack for a polynomial time pack algorithm that is tight for an interesting infinite class of hypothesis functions under the general and broad class and natural class of distributions model.",
                    "label": 1
                },
                {
                    "sent": "And I didn't talk about this in my talk, but we also have extensions to nearly low concave distributions and we also show matching lower bound.",
                    "label": 0
                },
                {
                    "sent": "Obviously only showed the upper bound, but everything that I showed his matching lower bound.",
                    "label": 1
                },
                {
                    "sent": "And in terms of open questions, obviously the very natural interesting open questions.",
                    "label": 0
                },
                {
                    "sent": "The first one is our analysis is currently still specific to linear separators and lock and key distributions to very interesting to provide aggressive query efficient active learning algorithms for general settings for general distributions for linear separators and for general concept spaces as well, and for passive learning.",
                    "label": 0
                },
                {
                    "sent": "Obviously it will be interesting to close the gaps between upper.",
                    "label": 1
                },
                {
                    "sent": "Between our upper and lower bounds for general concept spaces.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                }
            ]
        }
    }
}