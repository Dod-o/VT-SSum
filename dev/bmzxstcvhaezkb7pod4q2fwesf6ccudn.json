{
    "id": "bmzxstcvhaezkb7pod4q2fwesf6ccudn",
    "title": "Optimal Dimensionality of Metric Space for Classification",
    "info": {
        "author": [
            "Xiangyang Xue, Department of Computer Science and Engineering, Fudan University"
        ],
        "published": "July 27, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/icml07_xiangyang_odm/",
    "segmentation": [
        [
            "Hello.",
            "Hello everybody, my name is Shannon she the first also because he did not get visas.",
            "OK not to be here to present his work so I give this talk this morning so I work is working on the Metro learning and dimensionality reduction as well as the feature extraction."
        ],
        [
            "Yeah, this is the outline of our paper, so I will first give presentation and then present the algorithms.",
            "Analyze our experiment results on toilet and real world data and last will give our conclude."
        ],
        [
            "So for our motivation as we know, there are a lot of measures to learning.",
            "Artisan magic OK. Reduce the dimension so this methods value but.",
            "In all these methods that have some drawbacks, because we this must cannot select the dimensionality.",
            "We must determine dimensionality manually empirically.",
            "So can we solve this problem in our work?",
            "We want to find some method to find the."
        ],
        [
            "Optimal dimensionality.",
            "Yeah, this is the best idea.",
            "For example, this is datacine origonal space, so you can see if we use KNN KNN classification and at this point will be misclassified in original space.",
            "So if we can find another space.",
            "And we project all these points.",
            "On to this.",
            "Space is so we can see in this in this one dimensional space all points can be correctly classified, so this is the main idea of our work.",
            "So question is, can we learn a low dimensional invention for that the key endpoints in the same class has small distance to each other, then to the points in a different classes.",
            "If this question is yes, the answer is yes.",
            "So can we estimate the optimal dimensionality of the new metric space in the mean time?"
        ],
        [
            "Yeah."
        ],
        [
            "So, given N labeled multiple multiple class points.",
            "And then we first define the set for the Point XI would define the nearest labels in the same classes, and we can also define another set for Pointer XI, the K nearest labels in other classes.",
            "So we first define processes and then we can define the agent metrics if that means if IJ equals positive one.",
            "If this careless points in the same.",
            "Class is otherwise in a different classes we give minus one and four other.",
            "Otherwise we give 0.",
            "So we first define the metrics."
        ],
        [
            "And then we can define objective function that five P5P has two parts, one is developing another is Sigma T. So we define the objective function 5 equals therapy minus Sigma P and the Delta P means.",
            "Let competitive we measure compactness in a new space for the K nearest neighbors.",
            "An inner class supply supply ability, so Sigma P. So if we can find the minimum of five P, so this P is just what we what we wanted we want.",
            "Yeah, this is just the best idea, so we."
        ],
        [
            "You can.",
            "So this problem after some algebra operations, we can get this problem.",
            "So we also give some constraints constraints on P. So for this Max we can see it is symmetric, But this match is not positive definite.",
            "So it might have negative, zero or positive eigenvalues.",
            "And for the optimal transformation transformation P. Can obtain by by collecting all the investors of these metrics.",
            "Can responding to all it's it's all negative eigenvalues.",
            "Why with?",
            "Select the converters corresponding to the negative values."
        ],
        [
            "Because we can see the.",
            "Physical meaning of wagon value.",
            "If we project all.",
            "Points to the ice invite Pi.",
            "Which is clearly pointing to eyes.",
            "Evaluate Lambda I so we can get the lemma I means.",
            "That physical means it means in this vector along this vector that project all points projected on it.",
            "We can get a total difference a total distance.",
            "In the same class, single Pi means the total K labels pairwise distance in the different classes.",
            "So this this one minus this one it means.",
            "If I is less than 0.",
            "We we can imagine most most points might be correctly classified becausw the.",
            "Therapy is less violent, far less than the Sigma P. Otherwise.",
            "We can see most points might be misclassified."
        ],
        [
            "So that's why we select the leg 2.",
            "Values.",
            "Yeah.",
            "If we select all negative values.",
            "I maybe we we can finally smaller demand T if we if we satisfy this condition, that means we need not select all active negative values.",
            "We only keep the most energy of the eigenvalues.",
            "So this is our in our experimental.",
            "We can define the theater trader factor yeah and then select the final dimension key."
        ],
        [
            "And we can also say we in fact get the Mahala lobbies distance.",
            "Because you can see in the original space, the distance between any pairs of points can be calculated using this formula.",
            "Yeah, so it is in fact the mabius distance."
        ],
        [
            "Now we go to the experiment experiment.",
            "Yeah, this is a.",
            "Quiet it.",
            "We can see we have three classes of data and slipped under there.",
            "Well classified.",
            "Yeah, so we can calculate the egg and vectors and also the eigenvalues.",
            "In this case the angle values level and #2.",
            "There are both negative and compatible.",
            "And in fact, in this case we can easy find find that we need not perform that dimensionality reduction.",
            "Yeah, 'cause we have both negative and they're compatible and have both importance."
        ],
        [
            "Anna.",
            "This is another toy data.",
            "We have two classes, but this class have to model.",
            "Yeah, so you can see lemme know well and that we can now when absolute value of Lambda wind is far greater than that.",
            "Lambda 2.",
            "So in this cases we can only keep.",
            "I'm not with.",
            "You can see all data projected on to this vector.",
            "We can perform very well."
        ],
        [
            "Hello 3 classes data.",
            "So in these cases we can get to it values, one is negative and one is positive.",
            "So we can easy if we put it all data along this vector.",
            "Yeah, I think in a new space the classification will be better, but if we project all this, all this is to undo this vectors.",
            "Yeah you can.",
            "You can imagine the performance is not good, so in this case is we can only keep this vector.",
            "Yeah and this value and discard this."
        ],
        [
            "Now finally.",
            "We can say in this case is yeah, both values are positive and it means that we could not.",
            "Perform locating classification both in the original space or in the new spaces.",
            "Even if you do some transformation.",
            "And at this for Toyotas and follow this toy data, we can find some interesting."
        ],
        [
            "Phenomenal.",
            "NFL to practical data set a real place.",
            "Yeah, we use the UCI solar data set and in this case is here that in original Space D is 60 and K is is set to 1.",
            "So you can see when all the eigenvalues is less than zero here, here less than zero.",
            "If we get more dimensionality, the occurrence will increase.",
            "Yeah, multi dimensional mode perform better performance.",
            "And then.",
            "This Q is accumulative value Q and this is the value Q yeah each value.",
            "So you can see we can select the final dimension dimensionality.",
            "Along this line, yeah, so this may be the optimum optimum point here.",
            "And also useful.",
            "Useful life values is greater than zero.",
            "Yeah, in this.",
            "So if you choose more eigenvalues or dimensionality, the performance will become worse.",
            "So we can see the trends of the performance and as we entered into the communitive agrammatic use, yeah."
        ],
        [
            "And then we also.",
            "Conducted many experience on the solar or WDC flow UCI data set and also different methods.",
            "For example PCA, ADL, PLD and NCA.",
            "And in this case is the key.",
            "Is this one?",
            "And in this case is the key is 4 different care and?",
            "Different data set so you can see our performances.",
            "A little better than other methods."
        ],
        [
            "And then we also conduct experiments on the face database.",
            "Yeah, for face, it's dimensionality is very high.",
            "So we can also.",
            "Say la trends.",
            "Yeah, for example, in this case we can.",
            "Optimal dimensionality is about 10 to 20.",
            "Yeah, this is a change.",
            "And a lot of the Lambda is the luminous are near 0 and when you get much more dimensionalities, yeah, the performance will become versa."
        ],
        [
            "And this result for different training samples.",
            "For example, we pick four faces, six faces, 86 faces and attend classes and in different training samples we can get performance.",
            "Yeah, when change becomes more and more performance, we're increase and this our performance and these other.",
            "Performance yeah for face database and K is.",
            "Seattle 1."
        ],
        [
            "Yeah.",
            "Conclusions so in our paper we present a low level low dimensional inventing allowed Dominican Candyland for the beta equities in KN classification.",
            "Given fluid labeled training samples and also we can get the optimal dimensionality.",
            "For future work, for we think that for large scale data set, how to reduce the computational complexity becausw KNN, where is performed in here.",
            "So how to reduce the computational complexity for logistical data set?",
            "Yeah."
        ],
        [
            "Yeah, that's all.",
            "Question.",
            "You're claiming that.",
            "Theoretical.",
            "Times.",
            "Sucks.",
            "Eigenvalues yeah.",
            "Yeah.",
            "The number of values increase.",
            "So it means that these cubes and these cubes that theoretical analysis?",
            "Yeah, we you know future work we have to eat.",
            "Yeah, because for KNN classification we know the bound P to two, P 4K and the performance has theoretical analysis is.",
            "But if we transform the original data into the new space, how about the?",
            "Performance, yeah, we will.",
            "Currently we are working on this.",
            "Yeah.",
            "Just await the.",
            "Like you would wait the distance in each dimension for you.",
            "Do that or is that what you do?",
            "So we first transform the original data into the new space and use with.",
            "The distance will be you can you can read it in a new spaces.",
            "Each dimension with the same weight.",
            "But you could wait.",
            "Give more, give more weight to the ones with the larger negative.",
            "I know because we we give contracts on auto level.",
            "Fear.",
            "So not later will be modulated in Lambda.",
            "Yolanda so far for the transformation we we try to find the author of transformation also normal matches.",
            "Solo along each person will know wait here.",
            "Only transformed the original space into a new space.",
            "Davis."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Hello everybody, my name is Shannon she the first also because he did not get visas.",
                    "label": 0
                },
                {
                    "sent": "OK not to be here to present his work so I give this talk this morning so I work is working on the Metro learning and dimensionality reduction as well as the feature extraction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this is the outline of our paper, so I will first give presentation and then present the algorithms.",
                    "label": 0
                },
                {
                    "sent": "Analyze our experiment results on toilet and real world data and last will give our conclude.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for our motivation as we know, there are a lot of measures to learning.",
                    "label": 0
                },
                {
                    "sent": "Artisan magic OK. Reduce the dimension so this methods value but.",
                    "label": 0
                },
                {
                    "sent": "In all these methods that have some drawbacks, because we this must cannot select the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "We must determine dimensionality manually empirically.",
                    "label": 0
                },
                {
                    "sent": "So can we solve this problem in our work?",
                    "label": 0
                },
                {
                    "sent": "We want to find some method to find the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimal dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the best idea.",
                    "label": 0
                },
                {
                    "sent": "For example, this is datacine origonal space, so you can see if we use KNN KNN classification and at this point will be misclassified in original space.",
                    "label": 0
                },
                {
                    "sent": "So if we can find another space.",
                    "label": 0
                },
                {
                    "sent": "And we project all these points.",
                    "label": 0
                },
                {
                    "sent": "On to this.",
                    "label": 0
                },
                {
                    "sent": "Space is so we can see in this in this one dimensional space all points can be correctly classified, so this is the main idea of our work.",
                    "label": 0
                },
                {
                    "sent": "So question is, can we learn a low dimensional invention for that the key endpoints in the same class has small distance to each other, then to the points in a different classes.",
                    "label": 1
                },
                {
                    "sent": "If this question is yes, the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So can we estimate the optimal dimensionality of the new metric space in the mean time?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given N labeled multiple multiple class points.",
                    "label": 1
                },
                {
                    "sent": "And then we first define the set for the Point XI would define the nearest labels in the same classes, and we can also define another set for Pointer XI, the K nearest labels in other classes.",
                    "label": 1
                },
                {
                    "sent": "So we first define processes and then we can define the agent metrics if that means if IJ equals positive one.",
                    "label": 1
                },
                {
                    "sent": "If this careless points in the same.",
                    "label": 0
                },
                {
                    "sent": "Class is otherwise in a different classes we give minus one and four other.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we give 0.",
                    "label": 0
                },
                {
                    "sent": "So we first define the metrics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can define objective function that five P5P has two parts, one is developing another is Sigma T. So we define the objective function 5 equals therapy minus Sigma P and the Delta P means.",
                    "label": 0
                },
                {
                    "sent": "Let competitive we measure compactness in a new space for the K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "An inner class supply supply ability, so Sigma P. So if we can find the minimum of five P, so this P is just what we what we wanted we want.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is just the best idea, so we.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "So this problem after some algebra operations, we can get this problem.",
                    "label": 0
                },
                {
                    "sent": "So we also give some constraints constraints on P. So for this Max we can see it is symmetric, But this match is not positive definite.",
                    "label": 0
                },
                {
                    "sent": "So it might have negative, zero or positive eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And for the optimal transformation transformation P. Can obtain by by collecting all the investors of these metrics.",
                    "label": 0
                },
                {
                    "sent": "Can responding to all it's it's all negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Why with?",
                    "label": 0
                },
                {
                    "sent": "Select the converters corresponding to the negative values.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we can see the.",
                    "label": 0
                },
                {
                    "sent": "Physical meaning of wagon value.",
                    "label": 0
                },
                {
                    "sent": "If we project all.",
                    "label": 0
                },
                {
                    "sent": "Points to the ice invite Pi.",
                    "label": 1
                },
                {
                    "sent": "Which is clearly pointing to eyes.",
                    "label": 0
                },
                {
                    "sent": "Evaluate Lambda I so we can get the lemma I means.",
                    "label": 0
                },
                {
                    "sent": "That physical means it means in this vector along this vector that project all points projected on it.",
                    "label": 0
                },
                {
                    "sent": "We can get a total difference a total distance.",
                    "label": 0
                },
                {
                    "sent": "In the same class, single Pi means the total K labels pairwise distance in the different classes.",
                    "label": 1
                },
                {
                    "sent": "So this this one minus this one it means.",
                    "label": 0
                },
                {
                    "sent": "If I is less than 0.",
                    "label": 0
                },
                {
                    "sent": "We we can imagine most most points might be correctly classified becausw the.",
                    "label": 0
                },
                {
                    "sent": "Therapy is less violent, far less than the Sigma P. Otherwise.",
                    "label": 0
                },
                {
                    "sent": "We can see most points might be misclassified.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's why we select the leg 2.",
                    "label": 0
                },
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "If we select all negative values.",
                    "label": 0
                },
                {
                    "sent": "I maybe we we can finally smaller demand T if we if we satisfy this condition, that means we need not select all active negative values.",
                    "label": 0
                },
                {
                    "sent": "We only keep the most energy of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So this is our in our experimental.",
                    "label": 0
                },
                {
                    "sent": "We can define the theater trader factor yeah and then select the final dimension key.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can also say we in fact get the Mahala lobbies distance.",
                    "label": 0
                },
                {
                    "sent": "Because you can see in the original space, the distance between any pairs of points can be calculated using this formula.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so it is in fact the mabius distance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we go to the experiment experiment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a.",
                    "label": 0
                },
                {
                    "sent": "Quiet it.",
                    "label": 0
                },
                {
                    "sent": "We can see we have three classes of data and slipped under there.",
                    "label": 1
                },
                {
                    "sent": "Well classified.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we can calculate the egg and vectors and also the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "In this case the angle values level and #2.",
                    "label": 1
                },
                {
                    "sent": "There are both negative and compatible.",
                    "label": 1
                },
                {
                    "sent": "And in fact, in this case we can easy find find that we need not perform that dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause we have both negative and they're compatible and have both importance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anna.",
                    "label": 0
                },
                {
                    "sent": "This is another toy data.",
                    "label": 0
                },
                {
                    "sent": "We have two classes, but this class have to model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can see lemme know well and that we can now when absolute value of Lambda wind is far greater than that.",
                    "label": 0
                },
                {
                    "sent": "Lambda 2.",
                    "label": 0
                },
                {
                    "sent": "So in this cases we can only keep.",
                    "label": 0
                },
                {
                    "sent": "I'm not with.",
                    "label": 0
                },
                {
                    "sent": "You can see all data projected on to this vector.",
                    "label": 0
                },
                {
                    "sent": "We can perform very well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello 3 classes data.",
                    "label": 0
                },
                {
                    "sent": "So in these cases we can get to it values, one is negative and one is positive.",
                    "label": 0
                },
                {
                    "sent": "So we can easy if we put it all data along this vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think in a new space the classification will be better, but if we project all this, all this is to undo this vectors.",
                    "label": 0
                },
                {
                    "sent": "Yeah you can.",
                    "label": 0
                },
                {
                    "sent": "You can imagine the performance is not good, so in this case is we can only keep this vector.",
                    "label": 0
                },
                {
                    "sent": "Yeah and this value and discard this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now finally.",
                    "label": 0
                },
                {
                    "sent": "We can say in this case is yeah, both values are positive and it means that we could not.",
                    "label": 1
                },
                {
                    "sent": "Perform locating classification both in the original space or in the new spaces.",
                    "label": 0
                },
                {
                    "sent": "Even if you do some transformation.",
                    "label": 0
                },
                {
                    "sent": "And at this for Toyotas and follow this toy data, we can find some interesting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Phenomenal.",
                    "label": 0
                },
                {
                    "sent": "NFL to practical data set a real place.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we use the UCI solar data set and in this case is here that in original Space D is 60 and K is is set to 1.",
                    "label": 0
                },
                {
                    "sent": "So you can see when all the eigenvalues is less than zero here, here less than zero.",
                    "label": 0
                },
                {
                    "sent": "If we get more dimensionality, the occurrence will increase.",
                    "label": 1
                },
                {
                    "sent": "Yeah, multi dimensional mode perform better performance.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This Q is accumulative value Q and this is the value Q yeah each value.",
                    "label": 0
                },
                {
                    "sent": "So you can see we can select the final dimension dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Along this line, yeah, so this may be the optimum optimum point here.",
                    "label": 0
                },
                {
                    "sent": "And also useful.",
                    "label": 0
                },
                {
                    "sent": "Useful life values is greater than zero.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this.",
                    "label": 1
                },
                {
                    "sent": "So if you choose more eigenvalues or dimensionality, the performance will become worse.",
                    "label": 0
                },
                {
                    "sent": "So we can see the trends of the performance and as we entered into the communitive agrammatic use, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we also.",
                    "label": 0
                },
                {
                    "sent": "Conducted many experience on the solar or WDC flow UCI data set and also different methods.",
                    "label": 0
                },
                {
                    "sent": "For example PCA, ADL, PLD and NCA.",
                    "label": 0
                },
                {
                    "sent": "And in this case is the key.",
                    "label": 0
                },
                {
                    "sent": "Is this one?",
                    "label": 0
                },
                {
                    "sent": "And in this case is the key is 4 different care and?",
                    "label": 0
                },
                {
                    "sent": "Different data set so you can see our performances.",
                    "label": 0
                },
                {
                    "sent": "A little better than other methods.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we also conduct experiments on the face database.",
                    "label": 1
                },
                {
                    "sent": "Yeah, for face, it's dimensionality is very high.",
                    "label": 0
                },
                {
                    "sent": "So we can also.",
                    "label": 0
                },
                {
                    "sent": "Say la trends.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for example, in this case we can.",
                    "label": 0
                },
                {
                    "sent": "Optimal dimensionality is about 10 to 20.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a change.",
                    "label": 0
                },
                {
                    "sent": "And a lot of the Lambda is the luminous are near 0 and when you get much more dimensionalities, yeah, the performance will become versa.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this result for different training samples.",
                    "label": 0
                },
                {
                    "sent": "For example, we pick four faces, six faces, 86 faces and attend classes and in different training samples we can get performance.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when change becomes more and more performance, we're increase and this our performance and these other.",
                    "label": 0
                },
                {
                    "sent": "Performance yeah for face database and K is.",
                    "label": 1
                },
                {
                    "sent": "Seattle 1.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Conclusions so in our paper we present a low level low dimensional inventing allowed Dominican Candyland for the beta equities in KN classification.",
                    "label": 0
                },
                {
                    "sent": "Given fluid labeled training samples and also we can get the optimal dimensionality.",
                    "label": 0
                },
                {
                    "sent": "For future work, for we think that for large scale data set, how to reduce the computational complexity becausw KNN, where is performed in here.",
                    "label": 0
                },
                {
                    "sent": "So how to reduce the computational complexity for logistical data set?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, that's all.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "You're claiming that.",
                    "label": 0
                },
                {
                    "sent": "Theoretical.",
                    "label": 0
                },
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "Sucks.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The number of values increase.",
                    "label": 0
                },
                {
                    "sent": "So it means that these cubes and these cubes that theoretical analysis?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we you know future work we have to eat.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because for KNN classification we know the bound P to two, P 4K and the performance has theoretical analysis is.",
                    "label": 0
                },
                {
                    "sent": "But if we transform the original data into the new space, how about the?",
                    "label": 0
                },
                {
                    "sent": "Performance, yeah, we will.",
                    "label": 0
                },
                {
                    "sent": "Currently we are working on this.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Just await the.",
                    "label": 0
                },
                {
                    "sent": "Like you would wait the distance in each dimension for you.",
                    "label": 0
                },
                {
                    "sent": "Do that or is that what you do?",
                    "label": 0
                },
                {
                    "sent": "So we first transform the original data into the new space and use with.",
                    "label": 0
                },
                {
                    "sent": "The distance will be you can you can read it in a new spaces.",
                    "label": 0
                },
                {
                    "sent": "Each dimension with the same weight.",
                    "label": 0
                },
                {
                    "sent": "But you could wait.",
                    "label": 0
                },
                {
                    "sent": "Give more, give more weight to the ones with the larger negative.",
                    "label": 0
                },
                {
                    "sent": "I know because we we give contracts on auto level.",
                    "label": 0
                },
                {
                    "sent": "Fear.",
                    "label": 0
                },
                {
                    "sent": "So not later will be modulated in Lambda.",
                    "label": 0
                },
                {
                    "sent": "Yolanda so far for the transformation we we try to find the author of transformation also normal matches.",
                    "label": 0
                },
                {
                    "sent": "Solo along each person will know wait here.",
                    "label": 0
                },
                {
                    "sent": "Only transformed the original space into a new space.",
                    "label": 0
                },
                {
                    "sent": "Davis.",
                    "label": 0
                }
            ]
        }
    }
}