{
    "id": "nwj4iutmjcjmytk5ansjkdsevqgqfhnz",
    "title": "Active Learning for Biomedical Citation Screening",
    "info": {
        "author": [
            "Byron C. Wallace, Brown Laboratory for Linguistic Information Processing, Brown University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_wallace_albc/",
    "segmentation": [
        [
            "Who wishes alright?",
            "So my name is Byron Wallace.",
            "I actually have a joint RA at Tufts Medical Center and the Department of Computer Science at Tufts University, so I'm going to be talking about a new active learning approach that we developed while working collaboratively with the evidence based practice center at Tufts Medical Center, and this was in order to reduce the workload that's involved in conducting these things called systematic reviews, so this is joint work with Kevin Small.",
            "And also my advisor, Carla Brodley, as well as Thomas Tricoli knows who is the Co director of the Center for Evidence based Medicine at the medical."
        ],
        [
            "Enter.",
            "So first of all, I'm going to introduce this idea of a systematic review, so this is sort of the end goal of what we're trying to help them conduct with machine learning techniques.",
            "So basically a systematic review is an exhaustive assessment of all of the medical evidence regarding a precise clinical question, so I have a toy example here of maybe you're interested in figuring out OK?",
            "Is aspirin better than leeches and inducing more than 50% relief in patients with tension headaches?",
            "So the idea here is that you want to go out and you want to find all of the published medical evidence.",
            "That is relevant in answering this question, so an important caveat here is that it's absolutely imperative that all relevant medical evidence that exists and has been published is discovered an included in your systematic review.",
            "So it's very important that you don't miss any of the relevant studies, and the way that this is accomplished actually is through this process that they."
        ],
        [
            "Call citation screening.",
            "So here's the typical workflow an A citation screening task that would be done for a particular systematic review.",
            "So first of all pub Med which is a repository of biomedical literature, currently indexes around 19 million citations.",
            "So the researchers, the reviewer's conducting these reviews, who are generally doctors or clinicians.",
            "They create a search, and they retrieve, usually for a typical review, around 10,000 articles from Pub Med that are potentially relevant to the systematic review that they are conducting out of these, they screen the screen this pile down too.",
            "And by screen I mean they they sit down and they actually read one by one and decide whether it's relevant or not.",
            "They do this for all 10,000 of these and they usually end up keeping around 5%, or you know, typically around 500 of these are potentially eligible.",
            "Actually, there's a further winnowing process that happens here where this pile gets further winnowed down, but that actually is not something that concerns this work, so this screening component is the bit that we're in.",
            "We're interested in mitigating the workload involved in doing this.",
            "And so yeah, we're going to apply active learning here to try and speed up this step."
        ],
        [
            "OK, so so this picture really just sort of illustrates the amount of work that really is involved in doing one of these, so the doctors, as you can imagine, it's not their favorite thing to do, and right now they like me because they'd really rather be doing something else rather than sitting down and screening through these one by one.",
            "It's very tedious process.",
            "So when I first started working with the group, I thought great.",
            "Well this is a natural fit for active learning.",
            "You know where you can interactively train the model and the reason I thought this was, well, one you have to build a new model for each systematic review that's conducted and they do lots of these.",
            "And also because the reviewer is the doctors are willing to sit down with the system and sort of work with it interactively.",
            "So I was hoping we could reduce the workload by using act."
        ],
        [
            "Learning, so I thought great, you know, will throw off the shelf uncertainty sampling with a bag of words as PM and will be done with it.",
            "Unfortunately what we found was that using active learning well using uncertainty sampling, the sort of standard uncertainty sampling with support vector machines, wallet wallet definitely increased accuracy quickly as shown on the left, the left handed the left hand of this plot is accuracy is on the Y and we have uncertainty.",
            "Sampling is the dotted line.",
            "And passive or randomly sampling, learning is the thick black line and you see that accuracy does increase overall accuracy.",
            "But given that our imbalances such that we have about 5% of the minority class, that doesn't mean too much, because you can get a 95% accurate classifier by just calling everything irrelevant, right?",
            "And what's interesting here is that on the right on, if you look at recall or sensitivity to the minority examples to the relevant abstracts, you see that actually random sampling or passive learning dominates active learning, and so it actually.",
            "Performs better for our purposes, and so this was a little discouraging to me because I wanted to use active learning in a real setting, and this this is."
        ],
        [
            "Happened.",
            "So a natural question is, well, you know why might this be happening?",
            "What's going on?",
            "Why is sensitivity degraded when you apply uncertainty sampling to this problem, and the theory that we've come up with is something we call hasty generalization, and so in this case, what's going on is that uncertainty sampling is missing clusters or certain clusters of our sort of areas of the minority class.",
            "So this toy example illustrates the phenomenon, and we have two classes here.",
            "We have got the red squares and the blue circles.",
            "And the red squares comprise the minority class, and there are two areas or clusters of the red squares, one in the lower left, one in the upper right, and you can see that when we do random sampling, what's shown here is so in both of these plots the darkened examples are those examples for which a label request was requested by the querying strategy, and you see that when we do random sampling, what happens is you get of course a uniform sampling of the space, and you end up sampling both clusters of the minority class of the red squares.",
            "And that's great, but when you do uncertainty sampling, what you see is that it finds that first region of it finds the first decision boundary in the lower left hand corner, there between the blue circles in the red squares.",
            "But notice that it completely ignores the upper right hand cluster.",
            "It does actually query for some examples that are up in the upper right hand corner.",
            "Those are all blue circles, and that's a remnant of our using RBF kernel, but it misses all of the squares in the upper right hand corner.",
            "And just as a note.",
            "We tried pre clustering here.",
            "It doesn't help.",
            "We think probably there are two reasons for this.",
            "One is that pre clustering or I mean clustering in general is unreliable in high dimensions and also because the clusters that we're interested in are very small.",
            "So the pre clustering really doesn't help."
        ],
        [
            "So we thought, well, maybe prior knowledge is a way around this problem.",
            "Maybe we can use active learning.",
            "Or rather, maybe we can use prior knowledge in some way to guide the active learning an therefore discovering these clusters of interest quickly using prior knowledge and.",
            "So there's this question of, well, how do you?",
            "How do you do that?",
            "How can you guide active learning using domain knowledge?",
            "An when I talk about prior knowledge here, I'm just talking about labeled terms really, and in this case so labeled terms would be just words or in grams that are indicative of class membership.",
            "So for example, in our toy example, where we're investigating the relative efficacies of aspirin and leeches in treating tension headaches, some positive terms that may indicate that a document is relevant.",
            "A include tension headache, leeches, aspirin and some terms that might indicate that the document is irrelevant.",
            "Include migraine headache because we're probably only interested.",
            "Well, we are only interested in tension headaches and also mice because we're only interested in human subjects, so these are simple.",
            "These are sort of silly examples, but you get the idea.",
            "Nice about this case is the doctors conducting these reviews?",
            "Have these prior have these terms prior to beginning their search?"
        ],
        [
            "Their literature search.",
            "So how can we use these?",
            "So what we did was we plug these into the Co testing framework so Co testing is this framework where you have two models and you ask the expert to label those points about which these two models disagree.",
            "This is a variant of query by committee.",
            "The idea is that about the idea is that at least one of your models is wrong about that point, so basically."
        ],
        [
            "We combine labeled terms with Co testing by constructing first standard bag of words.",
            "Linear kernel, SVM.",
            "That's sort of our black box classification model that we ultimately use as our classifier, but then alongside in tandem we have this second model if you will, which is really just the log odds of the positive terms over the negative terms found in a given document right?",
            "And so basically the strategy is you want to find all of the documents about which these two models disagree.",
            "And then so these are sometimes called contention points, and then you want to select for labeling the document whose terms indicate most strongly one class.",
            "So maybe the terms indicate very strongly that it's relevant, but the support vector machine thinks it's irrelevant and those documents are interesting because the black box classifier the SVM is disagreeing with what the expert has told us, and so those are the examples that we want."
        ],
        [
            "To label.",
            "And so now I'm just going to go through some results that we have very quickly.",
            "I don't actually have time to get into the metric that we use on the Y access, but it's really just a weighted accuracy where we wait sensitivity more highly than specificity for the reasons they discussed or alluded to, and what you see here is the Co feature, which is what we're calling our method because it's Co.",
            "Testing plus labeled features we have as the thick black line again on the X axis, we have the number of labels.",
            "That have been provided by the expert, we have random sampling is the thin dotted lines simple, which is uncertainty sampling with support vector machines on the center line and finally we have something called features simple which is the as far as we know.",
            "The only other active learning strategy that's been proposed that explicitly that exploits labeled features an.",
            "Basically what you see is that we can achieve very good performance very quickly using these labeled features and using this code feature.",
            "Approach."
        ],
        [
            "As another example, we have the systematic review that was conducted to investigate the effects of micronutrients on health."
        ],
        [
            "So I'm sorry I didn't mention that this systematic review was looking at genetic associations with chronic obstructive pulmonary pulmonary."
        ],
        [
            "Disorder.",
            "And again in this case, we again see that using the code features it really dominates, and it's interesting to see on this data set as well that uncertainty sampling OK.",
            "So basically we went, and that's actually that's really truly love got so.",
            "Questions."
        ],
        [
            "If you could come up and get ready and use your microphone.",
            "Question about the experiment.",
            "Yeah, the input data for.",
            "So when you increase the labels and labels the label you got from the prior knowledge or is the label of example, so the label is over the example.",
            "And So what about the input feature?",
            "Do the method on uncertainties sampling, like the active learning using the same input feature as you?",
            "What you have, it's the same feature encoding that's right in both.",
            "So in both cases.",
            "So when we do active learning with uncertainty sampling, it's only using the bag of words feature space.",
            "But the Co testing model has access to that and the label terms.",
            "So essentially you have better features.",
            "On your message, when using your message, but active learning actually using the worst feature, just a bag of words, right?",
            "Well, so we actually had two variants of uncertainty sampling.",
            "This other one was using the same exact labeled features that we're using, and one is using the standard bag of words features.",
            "What we're arguing is that a these labeled features should be used, which I guess would be what you're saying, but, but also that you know doing this sort of uncertainty sampling just over the labeled feature space actually performed very poorly on our data set as was.",
            "Shown in the plot.",
            "So you're both message using the same features.",
            "We had two variants of active of uncertainty sampling.",
            "I would tell you, but we had two variants of active learning with uncertainty sampling, one used for the version that used the same features to improve future based on prior knowledge that your message better.",
            "Yes, that method actually does poorly in this case.",
            "Yes, OK. We can talk offline if you want, and just curious when you say 1000 thousand reviews by single physician or attention, yeah, we so we often have multiple physicians where about four per review really 4 for 10,000.",
            "They did it, that's right, but I would.",
            "I would.",
            "I would say again that they read the abstracts and titles.",
            "These are not full papers.",
            "10,000 yes.",
            "They don't like it.",
            "Yeah, it's just regarding the application of this, so obviously they want to find every single paper.",
            "That's right.",
            "That's right this review.",
            "So I guess my question is, how do they know when to stop reading through the papers?",
            "Because even if you rank them very well and you can say that these are probably interesting, that might get you to, you know, are they still going to have to read to all 10,000 anyway to just to satisfy their own?",
            "You know, I, you know their names on the line in this, and if they miss a key paper then you know.",
            "So I mean, how useful was it in the practical application I guess.",
            "Yes, so we're still in the early ISH stages of developing the algorithm and the tools.",
            "It's a good question and this is actually exactly why it's not a ranking problem and it really is a binary classification problem because we need to tell them whether or not they need to read this abstract.",
            "So right now actually we can achieve 100% recall with respect to the to the papers that were finally included in the review, which is a further winnowing process I mentioned earlier.",
            "We can actually get 100% recall to those while illuminating about half of the abstracts, so we can kind of have their workload without sacrificing any recall.",
            "Right now, that's sort of where we're at, and that's over three datasets that we have so.",
            "Well, thanks Byron, as well as people every account."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who wishes alright?",
                    "label": 0
                },
                {
                    "sent": "So my name is Byron Wallace.",
                    "label": 0
                },
                {
                    "sent": "I actually have a joint RA at Tufts Medical Center and the Department of Computer Science at Tufts University, so I'm going to be talking about a new active learning approach that we developed while working collaboratively with the evidence based practice center at Tufts Medical Center, and this was in order to reduce the workload that's involved in conducting these things called systematic reviews, so this is joint work with Kevin Small.",
                    "label": 1
                },
                {
                    "sent": "And also my advisor, Carla Brodley, as well as Thomas Tricoli knows who is the Co director of the Center for Evidence based Medicine at the medical.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I'm going to introduce this idea of a systematic review, so this is sort of the end goal of what we're trying to help them conduct with machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "So basically a systematic review is an exhaustive assessment of all of the medical evidence regarding a precise clinical question, so I have a toy example here of maybe you're interested in figuring out OK?",
                    "label": 1
                },
                {
                    "sent": "Is aspirin better than leeches and inducing more than 50% relief in patients with tension headaches?",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that you want to go out and you want to find all of the published medical evidence.",
                    "label": 0
                },
                {
                    "sent": "That is relevant in answering this question, so an important caveat here is that it's absolutely imperative that all relevant medical evidence that exists and has been published is discovered an included in your systematic review.",
                    "label": 0
                },
                {
                    "sent": "So it's very important that you don't miss any of the relevant studies, and the way that this is accomplished actually is through this process that they.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call citation screening.",
                    "label": 0
                },
                {
                    "sent": "So here's the typical workflow an A citation screening task that would be done for a particular systematic review.",
                    "label": 1
                },
                {
                    "sent": "So first of all pub Med which is a repository of biomedical literature, currently indexes around 19 million citations.",
                    "label": 0
                },
                {
                    "sent": "So the researchers, the reviewer's conducting these reviews, who are generally doctors or clinicians.",
                    "label": 0
                },
                {
                    "sent": "They create a search, and they retrieve, usually for a typical review, around 10,000 articles from Pub Med that are potentially relevant to the systematic review that they are conducting out of these, they screen the screen this pile down too.",
                    "label": 0
                },
                {
                    "sent": "And by screen I mean they they sit down and they actually read one by one and decide whether it's relevant or not.",
                    "label": 0
                },
                {
                    "sent": "They do this for all 10,000 of these and they usually end up keeping around 5%, or you know, typically around 500 of these are potentially eligible.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's a further winnowing process that happens here where this pile gets further winnowed down, but that actually is not something that concerns this work, so this screening component is the bit that we're in.",
                    "label": 0
                },
                {
                    "sent": "We're interested in mitigating the workload involved in doing this.",
                    "label": 0
                },
                {
                    "sent": "And so yeah, we're going to apply active learning here to try and speed up this step.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so this picture really just sort of illustrates the amount of work that really is involved in doing one of these, so the doctors, as you can imagine, it's not their favorite thing to do, and right now they like me because they'd really rather be doing something else rather than sitting down and screening through these one by one.",
                    "label": 0
                },
                {
                    "sent": "It's very tedious process.",
                    "label": 0
                },
                {
                    "sent": "So when I first started working with the group, I thought great.",
                    "label": 0
                },
                {
                    "sent": "Well this is a natural fit for active learning.",
                    "label": 1
                },
                {
                    "sent": "You know where you can interactively train the model and the reason I thought this was, well, one you have to build a new model for each systematic review that's conducted and they do lots of these.",
                    "label": 0
                },
                {
                    "sent": "And also because the reviewer is the doctors are willing to sit down with the system and sort of work with it interactively.",
                    "label": 0
                },
                {
                    "sent": "So I was hoping we could reduce the workload by using act.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning, so I thought great, you know, will throw off the shelf uncertainty sampling with a bag of words as PM and will be done with it.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately what we found was that using active learning well using uncertainty sampling, the sort of standard uncertainty sampling with support vector machines, wallet wallet definitely increased accuracy quickly as shown on the left, the left handed the left hand of this plot is accuracy is on the Y and we have uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Sampling is the dotted line.",
                    "label": 0
                },
                {
                    "sent": "And passive or randomly sampling, learning is the thick black line and you see that accuracy does increase overall accuracy.",
                    "label": 0
                },
                {
                    "sent": "But given that our imbalances such that we have about 5% of the minority class, that doesn't mean too much, because you can get a 95% accurate classifier by just calling everything irrelevant, right?",
                    "label": 0
                },
                {
                    "sent": "And what's interesting here is that on the right on, if you look at recall or sensitivity to the minority examples to the relevant abstracts, you see that actually random sampling or passive learning dominates active learning, and so it actually.",
                    "label": 0
                },
                {
                    "sent": "Performs better for our purposes, and so this was a little discouraging to me because I wanted to use active learning in a real setting, and this this is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happened.",
                    "label": 0
                },
                {
                    "sent": "So a natural question is, well, you know why might this be happening?",
                    "label": 1
                },
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "Why is sensitivity degraded when you apply uncertainty sampling to this problem, and the theory that we've come up with is something we call hasty generalization, and so in this case, what's going on is that uncertainty sampling is missing clusters or certain clusters of our sort of areas of the minority class.",
                    "label": 1
                },
                {
                    "sent": "So this toy example illustrates the phenomenon, and we have two classes here.",
                    "label": 0
                },
                {
                    "sent": "We have got the red squares and the blue circles.",
                    "label": 0
                },
                {
                    "sent": "And the red squares comprise the minority class, and there are two areas or clusters of the red squares, one in the lower left, one in the upper right, and you can see that when we do random sampling, what's shown here is so in both of these plots the darkened examples are those examples for which a label request was requested by the querying strategy, and you see that when we do random sampling, what happens is you get of course a uniform sampling of the space, and you end up sampling both clusters of the minority class of the red squares.",
                    "label": 0
                },
                {
                    "sent": "And that's great, but when you do uncertainty sampling, what you see is that it finds that first region of it finds the first decision boundary in the lower left hand corner, there between the blue circles in the red squares.",
                    "label": 0
                },
                {
                    "sent": "But notice that it completely ignores the upper right hand cluster.",
                    "label": 0
                },
                {
                    "sent": "It does actually query for some examples that are up in the upper right hand corner.",
                    "label": 0
                },
                {
                    "sent": "Those are all blue circles, and that's a remnant of our using RBF kernel, but it misses all of the squares in the upper right hand corner.",
                    "label": 0
                },
                {
                    "sent": "And just as a note.",
                    "label": 0
                },
                {
                    "sent": "We tried pre clustering here.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help.",
                    "label": 0
                },
                {
                    "sent": "We think probably there are two reasons for this.",
                    "label": 0
                },
                {
                    "sent": "One is that pre clustering or I mean clustering in general is unreliable in high dimensions and also because the clusters that we're interested in are very small.",
                    "label": 1
                },
                {
                    "sent": "So the pre clustering really doesn't help.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we thought, well, maybe prior knowledge is a way around this problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can use active learning.",
                    "label": 0
                },
                {
                    "sent": "Or rather, maybe we can use prior knowledge in some way to guide the active learning an therefore discovering these clusters of interest quickly using prior knowledge and.",
                    "label": 0
                },
                {
                    "sent": "So there's this question of, well, how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "How can you guide active learning using domain knowledge?",
                    "label": 1
                },
                {
                    "sent": "An when I talk about prior knowledge here, I'm just talking about labeled terms really, and in this case so labeled terms would be just words or in grams that are indicative of class membership.",
                    "label": 1
                },
                {
                    "sent": "So for example, in our toy example, where we're investigating the relative efficacies of aspirin and leeches in treating tension headaches, some positive terms that may indicate that a document is relevant.",
                    "label": 1
                },
                {
                    "sent": "A include tension headache, leeches, aspirin and some terms that might indicate that the document is irrelevant.",
                    "label": 0
                },
                {
                    "sent": "Include migraine headache because we're probably only interested.",
                    "label": 0
                },
                {
                    "sent": "Well, we are only interested in tension headaches and also mice because we're only interested in human subjects, so these are simple.",
                    "label": 0
                },
                {
                    "sent": "These are sort of silly examples, but you get the idea.",
                    "label": 0
                },
                {
                    "sent": "Nice about this case is the doctors conducting these reviews?",
                    "label": 0
                },
                {
                    "sent": "Have these prior have these terms prior to beginning their search?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their literature search.",
                    "label": 0
                },
                {
                    "sent": "So how can we use these?",
                    "label": 0
                },
                {
                    "sent": "So what we did was we plug these into the Co testing framework so Co testing is this framework where you have two models and you ask the expert to label those points about which these two models disagree.",
                    "label": 0
                },
                {
                    "sent": "This is a variant of query by committee.",
                    "label": 1
                },
                {
                    "sent": "The idea is that about the idea is that at least one of your models is wrong about that point, so basically.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We combine labeled terms with Co testing by constructing first standard bag of words.",
                    "label": 0
                },
                {
                    "sent": "Linear kernel, SVM.",
                    "label": 0
                },
                {
                    "sent": "That's sort of our black box classification model that we ultimately use as our classifier, but then alongside in tandem we have this second model if you will, which is really just the log odds of the positive terms over the negative terms found in a given document right?",
                    "label": 0
                },
                {
                    "sent": "And so basically the strategy is you want to find all of the documents about which these two models disagree.",
                    "label": 1
                },
                {
                    "sent": "And then so these are sometimes called contention points, and then you want to select for labeling the document whose terms indicate most strongly one class.",
                    "label": 0
                },
                {
                    "sent": "So maybe the terms indicate very strongly that it's relevant, but the support vector machine thinks it's irrelevant and those documents are interesting because the black box classifier the SVM is disagreeing with what the expert has told us, and so those are the examples that we want.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To label.",
                    "label": 0
                },
                {
                    "sent": "And so now I'm just going to go through some results that we have very quickly.",
                    "label": 0
                },
                {
                    "sent": "I don't actually have time to get into the metric that we use on the Y access, but it's really just a weighted accuracy where we wait sensitivity more highly than specificity for the reasons they discussed or alluded to, and what you see here is the Co feature, which is what we're calling our method because it's Co.",
                    "label": 0
                },
                {
                    "sent": "Testing plus labeled features we have as the thick black line again on the X axis, we have the number of labels.",
                    "label": 0
                },
                {
                    "sent": "That have been provided by the expert, we have random sampling is the thin dotted lines simple, which is uncertainty sampling with support vector machines on the center line and finally we have something called features simple which is the as far as we know.",
                    "label": 0
                },
                {
                    "sent": "The only other active learning strategy that's been proposed that explicitly that exploits labeled features an.",
                    "label": 0
                },
                {
                    "sent": "Basically what you see is that we can achieve very good performance very quickly using these labeled features and using this code feature.",
                    "label": 0
                },
                {
                    "sent": "Approach.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As another example, we have the systematic review that was conducted to investigate the effects of micronutrients on health.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm sorry I didn't mention that this systematic review was looking at genetic associations with chronic obstructive pulmonary pulmonary.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Disorder.",
                    "label": 0
                },
                {
                    "sent": "And again in this case, we again see that using the code features it really dominates, and it's interesting to see on this data set as well that uncertainty sampling OK.",
                    "label": 0
                },
                {
                    "sent": "So basically we went, and that's actually that's really truly love got so.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you could come up and get ready and use your microphone.",
                    "label": 0
                },
                {
                    "sent": "Question about the experiment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the input data for.",
                    "label": 0
                },
                {
                    "sent": "So when you increase the labels and labels the label you got from the prior knowledge or is the label of example, so the label is over the example.",
                    "label": 0
                },
                {
                    "sent": "And So what about the input feature?",
                    "label": 0
                },
                {
                    "sent": "Do the method on uncertainties sampling, like the active learning using the same input feature as you?",
                    "label": 0
                },
                {
                    "sent": "What you have, it's the same feature encoding that's right in both.",
                    "label": 0
                },
                {
                    "sent": "So in both cases.",
                    "label": 0
                },
                {
                    "sent": "So when we do active learning with uncertainty sampling, it's only using the bag of words feature space.",
                    "label": 0
                },
                {
                    "sent": "But the Co testing model has access to that and the label terms.",
                    "label": 0
                },
                {
                    "sent": "So essentially you have better features.",
                    "label": 0
                },
                {
                    "sent": "On your message, when using your message, but active learning actually using the worst feature, just a bag of words, right?",
                    "label": 0
                },
                {
                    "sent": "Well, so we actually had two variants of uncertainty sampling.",
                    "label": 0
                },
                {
                    "sent": "This other one was using the same exact labeled features that we're using, and one is using the standard bag of words features.",
                    "label": 0
                },
                {
                    "sent": "What we're arguing is that a these labeled features should be used, which I guess would be what you're saying, but, but also that you know doing this sort of uncertainty sampling just over the labeled feature space actually performed very poorly on our data set as was.",
                    "label": 0
                },
                {
                    "sent": "Shown in the plot.",
                    "label": 0
                },
                {
                    "sent": "So you're both message using the same features.",
                    "label": 0
                },
                {
                    "sent": "We had two variants of active of uncertainty sampling.",
                    "label": 0
                },
                {
                    "sent": "I would tell you, but we had two variants of active learning with uncertainty sampling, one used for the version that used the same features to improve future based on prior knowledge that your message better.",
                    "label": 1
                },
                {
                    "sent": "Yes, that method actually does poorly in this case.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK. We can talk offline if you want, and just curious when you say 1000 thousand reviews by single physician or attention, yeah, we so we often have multiple physicians where about four per review really 4 for 10,000.",
                    "label": 0
                },
                {
                    "sent": "They did it, that's right, but I would.",
                    "label": 0
                },
                {
                    "sent": "I would.",
                    "label": 0
                },
                {
                    "sent": "I would say again that they read the abstracts and titles.",
                    "label": 0
                },
                {
                    "sent": "These are not full papers.",
                    "label": 0
                },
                {
                    "sent": "10,000 yes.",
                    "label": 0
                },
                {
                    "sent": "They don't like it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's just regarding the application of this, so obviously they want to find every single paper.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "That's right this review.",
                    "label": 0
                },
                {
                    "sent": "So I guess my question is, how do they know when to stop reading through the papers?",
                    "label": 0
                },
                {
                    "sent": "Because even if you rank them very well and you can say that these are probably interesting, that might get you to, you know, are they still going to have to read to all 10,000 anyway to just to satisfy their own?",
                    "label": 0
                },
                {
                    "sent": "You know, I, you know their names on the line in this, and if they miss a key paper then you know.",
                    "label": 1
                },
                {
                    "sent": "So I mean, how useful was it in the practical application I guess.",
                    "label": 0
                },
                {
                    "sent": "Yes, so we're still in the early ISH stages of developing the algorithm and the tools.",
                    "label": 0
                },
                {
                    "sent": "It's a good question and this is actually exactly why it's not a ranking problem and it really is a binary classification problem because we need to tell them whether or not they need to read this abstract.",
                    "label": 0
                },
                {
                    "sent": "So right now actually we can achieve 100% recall with respect to the to the papers that were finally included in the review, which is a further winnowing process I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "We can actually get 100% recall to those while illuminating about half of the abstracts, so we can kind of have their workload without sacrificing any recall.",
                    "label": 0
                },
                {
                    "sent": "Right now, that's sort of where we're at, and that's over three datasets that we have so.",
                    "label": 0
                },
                {
                    "sent": "Well, thanks Byron, as well as people every account.",
                    "label": 0
                }
            ]
        }
    }
}