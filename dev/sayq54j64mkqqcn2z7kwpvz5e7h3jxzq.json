{
    "id": "sayq54j64mkqqcn2z7kwpvz5e7h3jxzq",
    "title": "Statistical Learning Theory",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2004",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/mlss04_taylor_slt/",
    "segmentation": [
        [
            "Talk about basic statistical learning theory.",
            "I was trying to think what I could hope to do for you in this period of time allotted and what you might be able to take away from this short set of lectures.",
            "Um?",
            "And something that came to mind was a series of books that are actually sold in in the UK, known as the Bluffers Guide to.",
            "And you can get a bluffer's guide for all sorts of topics, from psychoanalysis to whiskey, to skiing to philosophy or whatever.",
            "And the idea of a bluffer's guide.",
            "Bluffer is somebody who sort of makes his way by not really knowing what's going on, but has the right terminology and can sort of get by at the cocktail party.",
            "So the Bluffers guide book gives you all the key terms and tells you you know how to use them in the right context.",
            "So for instance, you know if you start talking in statistical learning theory about the ghost sample, you know you should know that that isn't the data set you used in your last NIPS submission.",
            "And the double sample trick is not actually something to do with cards and so on.",
            "Concentration is not about level of alcohol in your blood and so on.",
            "So my intention there.",
            "I mean, I mean, that's joking aside, I think these books actually also are quite instructive.",
            "I mean, they're surprisingly informative.",
            "They give you the sort of level strategic level view of that subject.",
            "That often is very hard to get.",
            "From a textbook which will go into a lot more detail and a lot more of the you get lost in the detail and you don't get the sort of overview.",
            "So in a sense that's the first level that I would like to try and reach that people come away from this with a sense of what are what statistical learning theory is attempting to achieve.",
            "What is it's sort of contribution?",
            "If you like, what does it bring to the table of machine learning?",
            "To what extent is successful in doing that?",
            "And to what extent it fails or needs to be further refined?",
            "And.",
            "I suppose in a sense putting it in the context of of the research that you might be doing and how it relates to that research.",
            "So if you like, that will be the level.",
            "I would hope that everybody might take away from this.",
            "I think also I would also hope that many of you might take a little bit more than that in that I will hope to cover.",
            "Some of the key techniques, theorems, and.",
            "Ideas behind the theory in a way that I hope will infuse you that there's actually a lot of interesting things in that theory, and a lot of nice ideas that are fun to work with.",
            "So in the sense, I would like to hope that some of you will actually feel that's really interesting.",
            "That's something I didn't know and now have learned about.",
            "And finally, you know.",
            "Also, I might hope that many of you will take.",
            "An inspiration from this to try and apply some of the ideas to the research that you're doing.",
            "So in a sense, that would be the.",
            "The major goal would be to see that yes, OK, that actually applies to what I'm doing.",
            "It can help me understand or further illuminate the actual algorithms I'm working with.",
            "The, you know, the techniques that I'm using in my research, and actually those techniques could be applied in that area.",
            "So if you like those, those are the sort of three levels I'm striving for.",
            "But certainly I don't want to get lost in a lot of detail and I want to try and, but I want to try and give you some of the enthusiasm for some of the key ideas.",
            "So on that note, I'll move forward."
        ],
        [
            "And give you an overview of the basic structure that I'm hoping the things I'm hoping to cover.",
            "So first of all, setting the context clearly, statistical learning theory is something I'll need to put in context with other attempts to understand learning systems.",
            "So I'll spend most of this lecture looking at that.",
            "Then move into the basic.",
            "Pack ideas, probably approximately correct, is the what the acronym stands for and their proofs.",
            "And again here I will be looking at some of the detailed proofs.",
            "I don't want to just give you a series of results, I want to try and show you how those results are arrived at, where the proofs are instructive in themselves.",
            "So it is my intention to try and look at proofs and see how they work.",
            "At least the key ideas of how they work, so I'll be looking at those in a little bit more detail that will lead us to the sort of point at which.",
            "If you like learning, theory was at when suddenly support vector machines broke on the scene and we're performing extremely well, and the theory that was at that point developed was completely unable to explain their performance, and I think that was a critical moment where if you like theory and practice were completely at odds with each other, and I think that that was a very challenged to the theory community to try.",
            "And if you like upgrade their theory to match practice.",
            "And that's what I'll talk about next, which is this idea of real valued function classes and the margin.",
            "That will hopefully be the point I reach at the end of today, so that's a V4 lectures and then starting tomorrow will look at more modern.",
            "Approaches to statistical learning theory, concentration and stability, and then Rademacher complexity and its main theorem an onto some applications.",
            "So again, I would hope to cover or go to the next slide."
        ],
        [
            "And give a sort of aim of what I would hope, since this is just giving a recapitulation to some extent of what I said at the beginning, but some thoughts on why theory, what it can do for you.",
            "The basic techniques, with some deference to history, but not a strict historical sequence.",
            "Insights into the proof techniques and statistical learning approaches.",
            "And tomorrow a complete proof of the SVM bound using rather radical approach.",
            "I think somehow taking you through the whole thing.",
            "You know it's like climbing Everest or something.",
            "You know it's not that hard, but it can give you an idea of what the different components that are needed in a proof."
        ],
        [
            "OK, so that is what I'm hoping to include.",
            "What I won't include is the most general results by any stretch.",
            "I certainly don't know them.",
            "I certainly won't include a complete history.",
            "I won't be looking at an analysis of Bayesian inference.",
            "Typically that is.",
            "Viewed as outside statistical learning theory until recently when development of the PAC Bayes.",
            "Approach which sort of links the two, but I gain won't be covering that, and Olivier Busquet, who's giving the advanced section of this course will be looking at the PAC Bayes results and also at this idea of local Rademacher complexity.",
            "So developing on from the Rademacher stuff I'll be talking about tomorrow.",
            "OK, so that's hopefully set the scene.",
            "I would welcome you to ask questions and.",
            "Again, give feedback about speed and appropriateness of what I'm saying.",
            "Please just drop your hand and you know I'll try and spot you and.",
            "OK."
        ],
        [
            "So I'm going to start with an attempt to.",
            "By the way, is there a pointer that I can use for the just that I see?",
            "Right, OK, so I'm I'm going to start with.",
            "If you like what?",
            "How does the statistical learning Community approach what they're trying to do in understanding learning systems?",
            "So the basic viewpoint is that is a statistical one.",
            "Clearly, as the title suggests.",
            "But as with any theory, the aim is to try and model some phenomenon, and in this case it could be it's part real part.",
            "Artificial real data may be real, but the actual algorithms are artificial, but it is a phenomenon and you're trying to understand what's going on.",
            "It's the nature of science is exactly this.",
            "You know you have some phenomenon in the world and you try and write down a theory which explains that.",
            "Behavior of that system and the aim.",
            "Of course of doing that is in one sense just the intellectual enjoyment of having an understanding of the world, but the more practical approach is in order that you can predict how that will perform and hopefully get it to perform better, that is, exploit your knowledge in terms of actually improving the algorithm, improving whatever you're trying to do.",
            "So a theory is always there to try and support practice.",
            "And there's always this interleaving between theory and practice, and the more active that interleaving, the better.",
            "I think for both for the theory and for the practice.",
            "I should emphasize right away.",
            "Statistical learning theory is just one approach to understanding these learning systems.",
            "And other approaches include Bayesian inference.",
            "As I said, I'm not going to speak about that inductive inference.",
            "Statistical physics, traditional statistical analysis.",
            "All of these have perhaps slightly different emphasis and different premises in the way they approach and different strengths.",
            "So I'll mention that on the next slide."
        ],
        [
            "Each theory makes assumptions about the phenomenon of learning, so if you're trying to model something, your immediate task is to abstract away from the details of the particular into a level that you can actually make some conclusions and derive some results about, and in that abstraction you're going to lose something for sure, and the critical thing is to choose your modeling at a level that captures the essential and doesn't lose critical elements.",
            "Of your of the thing you study, if you abstract too far, you lose.",
            "You may get a beautiful theory, but it will no longer be applicable.",
            "And each theory abstracts in its own way, and so you know to say one theory is better than another is always a very risky thing to say.",
            "I think it's a case of looking at one theory may be more applicable in these type of situations because the abstraction it makes is appropriate and captures what's going on in that situation.",
            "This theory may be more appropriate in this situation where the actual conditions are slightly different.",
            "So I think you know to sort of get into turf wars about Bayesian and frequentist.",
            "Statistics seems to be very short sighted, and really what we should be trying to do is understand when one works and when the other works.",
            "So on that note, I think that you know, hopefully as part of this course one will begin to see what the modeling that statistical learning theory does contributes and where it can be of use.",
            "And I certainly don't want to make a claim that it's the be all and end all of understanding.",
            "Learning by any stretch of the imagination, but hopefully it has something.",
            "To bring to the table of of this very interesting subject."
        ],
        [
            "OK.",
            "So now getting into a little bit more down into the particular approach of statistical approach to learning.",
            "I'm going to simplify at this point, but I think it's a setting a framework that will be very generally applicable in this in this approach.",
            "The assumption that you typically begin with is that the data is generated by an underlying distribution, so it's a sort of probabilistic assumption about the way the data arises.",
            "And the learner himself actually has very little access to information about that distribution.",
            "You may have some prior knowledge, some, but not enough to do what a Bayesian would do, which is build a model of that process.",
            "So you're making the assumption that you have very little knowledge about the way the data is generated.",
            "For instance, if we are trying to classify tissue samples to decide whether they are cancer cells or not, cancer cells there, there is a distribution of the way those cells look from a particular population, so a typical healthy cell looks in this sort of way.",
            "An if you generate a typical healthy cell, you'll get an example of that probability distribution typical cancerous cells.",
            "Look this way an if you generate one from the probability distribution of cancer cells, you get an example of that distribution.",
            "So this is the assumption behind.",
            "Now of course this is already making maybe some.",
            "Two strong steps and of course this is, you know, as soon as you make these assumptions, you may lose something and and that's the price you pay for making a theory.",
            "Um?",
            "But that's the, that's the."
        ],
        [
            "Approach this taken.",
            "In a sense, this assumption of a distribution subsumes everything that we might hope to know about the processes in the natural artificial world that we are studying.",
            "Now that's not going to be true in the end, because we're going to make in the way we design our learning algorithms and so on.",
            "We're going to incorporate our knowledge about the world.",
            "We are the problem we're studying, but in a sense, the distribution contains all the real information.",
            "Now the problem in learning is precisely the fact that that distribution contains the information, but we have no way of accessing it directly.",
            "We have no way of getting to.",
            "If you like an explicit formula for the probability density function of that distribution, rather than that we're given rather poor training sample or training set, which is just a sample of points generated according to that distribution, the classical.",
            "Situation in the training sample.",
            "Now I've given an example here with the input output pairing, I'm probably going to focus mainly in this course on classification, so one would think of the wise as being a label indicating membership of a class or not.",
            "But of course in general they could be real valued outputs, or they could be vector valued outputs, or they could be.",
            "It could be just no output and just a.",
            "A clustering problem, for example, where you only have affectively an input to your system.",
            "But as an example, here we have the pairs input, say, classification output X1Y one up to XMYM and again the assumption we're going to make and again this is already making another assumption is that this sample is generated independently and identically according to this distribution P. So in a sense we're getting our information about P comes through this sample.",
            "It comes through independent generations of examples by that distribution.",
            "OK."
        ],
        [
            "Now this leads very.",
            "Immediately to what we would then try and do.",
            "Typically a learning algorithm and I'm using a curly a here would choose a function from a function space F. Up here and I'm denoting the function it chooses by the algorithm sub F applied to the training set S. So notice that the actual choice of function will depend on the training set.",
            "Even if you're learning the same problem, if you generate a different sample and apply your algorithm to it, it's going to give a different result.",
            "Hopefully not that different, but it will be different, so that's the sort of result of your learning algorithm.",
            "That's the actual function that it gives an from a statistical point of view, the quantity that we're really interested in is this quantity here, which is the expected value of the loss.",
            "When we apply this function to a randomly generated knew test point.",
            "So XY is a new test point, and this is the function we learned from the sample which we were given an when we apply that function to the new example and according to some measure measure.",
            "How well it matches the output, so the loss here is some measure of the fit.",
            "If you like of the output of your function that you've learned and the actual output that should be given an, we want really to estimate the expectation of that loss.",
            "And I'm going to call that epsilon SA&F, so this S is the sample.",
            "It depends on the particular sample.",
            "Now this is the loss for a particular training set, an algorithm, and a function class.",
            "So this is if you like the measure of performance of the actual algorithm on the training on the particular training set."
        ],
        [
            "So for example, in the case of classification, we can take the loss L to be one if the two disagree.",
            "In other words, if the output of your function applied to the test input is different from the output that should be given, so it gives the wrong classification and otherwise to have zero.",
            "If it's the right classification.",
            "Multiple regression, it could be the square of the difference between the output of the function.",
            "So this will now be a real value and this will be.",
            "The true output.",
            "Of course this would be difficult to say.",
            "It could be correct, so you typically you know exactly will be only accurate to a certain degree.",
            "So you take the difference between these and square them and the losses is not a discrete quantity in that case.",
            "So we're going to refer to this random variable epsilon.",
            "SAF is the generalization of the learner.",
            "OK, so it's a very very core concept.",
            "So rather than you know, I think it might be worth doing a very little simple exercise at this point just to sort of before we lose ourselves in the symbols.",
            "So what I'd like you to do is."
        ],
        [
            "Find out another way of expressing this quantity.",
            "In the case of classification.",
            "So as I've written it, it's the expectation of a loss.",
            "OK, but in the case of classification you can simplify that down to something perhaps more understandable.",
            "So if you just want to take a couple of minutes to try and write down what that would be in the case where the loss function is this discrete loss, one of your misclassified 0, if you get the right label.",
            "OK. No one OK yeah OK, do you want to?",
            "I'm not ready.",
            "I just wrote this sum from one to end of an indicator function.",
            "Whether the sign of.",
            "The output of your algorithm for your function is equal to the target.",
            "While you're looking for.",
            "Yeah, that's it.",
            "So that's actually the test.",
            "So that's the training error.",
            "I guess you're thinking of that this what this is is the test error, so it's more it's.",
            "I mean that certainly would be the right estimation on the training set.",
            "But what I was looking for was a an expression for this in terms of the probabilities of these events.",
            "Anyone else have a yeah?",
            "Integral of the Delta function.",
            "Um?",
            "It could be expressed in that way.",
            "That's for sure.",
            "Yeah, I actually was thinking of.",
            "Well, let me let me show you what I was thinking and then hopefully it will be clear.",
            "So if we take that.",
            "I have to do this and that.",
            "We take that expectation.",
            "Over XY.",
            "Of the loss of function, I'll just write a of S leaving out, yeah.",
            "X.",
            "Why OK so?",
            "Essentially, we can write that as the.",
            "Thank you, OK. We can write that as the probability that a S. Sorry, the probability that a as of X is equal to y * 0 because the loss is zero in that case.",
            "Plus, the probability that a S of X is not equal to y * 1 because the loss in that case.",
            "This is just working out the expectation in terms of the different events that can occur.",
            "Remember, the discrete loss is.",
            "Perfectly focused, is it?",
            "OK, can you see at the back is that OK?",
            "Yeah, OK, so that means that of course this.",
            "Is doesn't occur and it actually just equals the probability that the function applied to the GNU test point is not equal to the correct value.",
            "So in other words, this in this case this generalization for the discrete loss is just the probability of misclassification.",
            "That's all I was trying to drive out.",
            "Maybe I didn't express it very clearly, but it's just in that case it very becomes a very simple expression.",
            "Is everyone happy with that idea?",
            "Any questions?",
            "OK, OK.",
            "So I just don't want you to get phased by all these.",
            "I mean one of the big problems with theory is that because you have to write things down precisely, you get all of this sort of garbage of symbols and and it's just, I mean, once you get used to, it is just a set of symbols.",
            "I mean, the ideas are not actually that difficult or or complicated really, it's just you get phased by the symbols.",
            "So part of it is to just work with him a little bit and get familiar with the symbols on the way that they.",
            "So you know, this looks horrible, horrid, but in this case it just reduces to something very simple.",
            "OK.",
            "So this is really a very I think, important or I need to put this back."
        ],
        [
            "So this quantity epsilon safs.",
            "This is the expected value of the loss on a randomly generated test point in the case of classification is just the probability that a randomly generated point is misclassified.",
            "If you think of running your algorithm with a training and test set, your test set error, which perhaps is also what you were referring to, maybe is an estimate of this.",
            "It's a sample estimate based on actually looking at the test examples, which you assume again are generated according to the same distribution, so it's a good estimate of that quantity, but from a theoretical point of view, we prefer to think of this as the quantity we're really interested in.",
            "It's the pure probability of misclassification of a randomly generated point."
        ],
        [
            "So I want to try and give an example of what this random variable, how it looks in a particular concrete case, 'cause I think if you can build up a picture in your mind of what this random variable distribution looks like, a lot of learning theory starts to make sense.",
            "You can see what learning theory is attempting to achieve, so I'm not going to give you an example which is real world data, breast cancer data set from the UCI repository.",
            "I'm going to do a very simple parsing window classifier as described by Bernard Scholkopf at the beginning of his lecture.",
            "So the weight vector is actually just it's a linear function classifier, thresholded linear function.",
            "The weight vector is just the average of the positive training examples minus the average of the negative.",
            "So if you can imagine the as a cloud of positives and you put a point in the center of that, there's a cloud of negative point in the center of that.",
            "Join the two and bisect the line between them.",
            "That is the hyperplane that we're going to use to classify.",
            "OK so very very simple."
        ],
        [
            "And what I'm going to look at is, I'm going to repeatedly draw random training sets S. And I'm going to get an estimate of this distribution.",
            "I'm actually going to create a histogram of this distribution of the errors.",
            "So each time I draw a random training set, I'm going to compute this guy.",
            "Of course, I'm going to compute it in the way that we always do, estimating it on on a test set.",
            "OK, I can't help that, but because it's a, you know, if you had some abstract problem where you could define the true error.",
            "But in our case we've got a concrete data set, so I'm just going to use the test error as a proxy for this, so I'm going to repeatedly generate random training sets, use the remainder of the data.",
            "Is a test set an estimate this quantity after applying this very simple algorithm, the parsing window estimator and I'm going to plot the histogram and the average of the distribution of this quantity for various sizes of training set.",
            "Now if I take the whole data set then that will give a single value, because essentially there's just one function is the average of the positive and negatives, and I'm going to apply it to the same set set and justice just like a sort of if you like thing we might aim for the best you could possibly do with this type of classifier in this situation.",
            "So I'm going to use that if you like as a starting point, the whole data set.",
            "And then I'm going to look at smaller and smaller values of training data, and we're going to look at what happens to the distribution.",
            "In that situation is unclear what I'm trying to do, so I'm going to say fixed my size 342.",
            "That's half actually 50% of the training set, so I will then.",
            "You know train deposit window on that half tested on the other half and I get one value of that error and then I'm going to do that 1000 times and get a histogram of the errors that I get out OK, and then I'll compute the average OK."
        ],
        [
            "The reason this is quite a interesting example is that actually the expected classifier that we get in all cases, whatever sum whatever sample size we take, is the same because the expected value of this result of this learning algorithm is the expected value under the particular training set of the positive average of the positives, which I'm denoting W plus.",
            "Sub S and the average of the negatives I'm denoting W minus of S because the expectation is can go through addition.",
            "We can actually write this as the expectation of the positives minus the expectation, the negatives, and then we've got.",
            "This is just actually the expectation of the points that have positive label because the average of a set of points of positive label.",
            "If you.",
            "Sample size, whatever.",
            "You're always going to.",
            "The expected value is always going to be the true mean of all of the positives.",
            "So in fact this this is going to be the true mean of all the negatives, so the expected value.",
            "Of course, as you get small training sets, you're going to get lots of variation, but the expected value of this classifier is always the same.",
            "That doesn't actually mean that the expected loss.",
            "The average loss is going to be always the same because the losses are non linear function, so you can't do this trick for the loss, but it does sort of give you a feeling that yeah, you know the loss shouldn't change the expected loss shouldn't change that much and will see that is to some extent true OK?"
        ],
        [
            "So here's the sort of perfect.",
            "Case where you take the whole training set, so essentially there's just one value.",
            "The distribution is a single point at about .1 four.",
            "That's the error of the parsing window estimator.",
            "If you like on the whole data set.",
            "OK, so now we're going to."
        ],
        [
            "Move to say, size 342, so of course notice that the expected value is almost identical.",
            "So as we predicted, the expected value is sort of pretty pretty tight there, but now we're getting quite a bit of variation in the actual random values of this generalization, so on some training sets we're getting errors as high as point to .2, and on some we're getting errors as low as .1, and it's just the luck of the game.",
            "You know, if you happen to be lucky and get a good training set.",
            "Well, maybe you might say good test set it depends which we want to look at.",
            "Then you actually get a better generalization.",
            "So now as we reduce the size the the distribution begins to slightly change, so that's down to .4."
        ],
        [
            "40% of the training data that's three."
        ],
        [
            "80% of the training data, sorry, 30% of the data for training 20% and now you can start to."
        ],
        [
            "See that the distribution is beginning to get a little disheveled with starting because the training set size is small.",
            "But notice that the mean is still very nicely centered.",
            "Where we we started OK. Anne."
        ],
        [
            "If we keep going now, we're already getting, you know, quite a little bit of tail.",
            "We're getting errors as high as .3 on.",
            "This means started to move a little bit, but not very much.",
            "OK, and if we."
        ],
        [
            "Going that's down to I can't remember what the percentage was 34."
        ],
        [
            "2720 and the mean is still below .2, so it's only increased by about .5 the mean.",
            "But you know we're already getting errors as high as .6 up here.",
            "And some training sets.",
            "OK, I did make sure the training set had the right proportion of positives and negatives, so I wasn't cheating, and I mean it's not bad because of that, it's just bad luck in terms of statistics."
        ],
        [
            "So that's 14 and now we're getting errors right up there.",
            "And finally, that's."
        ],
        [
            "You know, training set size 7.",
            "OK."
        ],
        [
            "So clearly you know, hopefully from those you can start to see the way in which this distribution behaves and thing.",
            "Perhaps I wanted to emphasize, particularly is the difference between the mean of the distribution, which actually kept to be quite nicely focused around the value you had at the optimum.",
            "In the sense the start and the actual spread of the distribution which started to get to look quite bad, OK?",
            "Now traditional statistics is actually concentrated on.",
            "Looking at that mean.",
            "And typical sort of consideration is the consistency of an algorithm and consistency of a classification algorithm A is defined to be the property that the limit as N tends to Infinity of this.",
            "So the sample size is tending to Infinity.",
            "The expected value of this distribution of errors tends to what's known as the Bayes optimal function.",
            "The Bayes optimal function is essentially the best you could ever do in a classification algorithm, because it's simply.",
            "Classifies according to the label that is more likely so it covers the case where you might have noise in your data set so that a particular example might in some cases be classified both positively and negatively with different probabilities, and in that case obviously you have to pick the label that's most likely, and that's what the base thing does, yes.",
            "What computer do you have on one selling limits?",
            "Examples purchase an expected expectation of generalization here.",
            "This should be the generalization of the Bayes.",
            "Sorry that you're right.",
            "Yeah, sorry, that should be the loss of the base expected loss of the base function on this side.",
            "Thank you, I'll spot it.",
            "Everyone else that you might want to add that so this should say the expected value of the loss of F bays on XY.",
            "So what I meant was that the you know the function.",
            "Essentially it converges to.",
            "Or you could just say you know the limit as N tends to Infinity of AS of F is F days.",
            "So that's the sort of the study, but I think you know from the previous slide you can see that the problem with N tends to Infinity is that it's as long as a piece of string.",
            "You know.",
            "I mean, does it happen at a million?",
            "That's it happen as a billion.",
            "You know?",
            "I mean, there's this.",
            "What we want to know is what happens on a finite sample, really."
        ],
        [
            "For finite sample generalization.",
            "Has a distribution depending on the algorithm, function, class and sample size.",
            "So, as I've indicated above, the traditional statistics has looked at the mean of this distribution, but I think we hopefully have seen that it can be misleading and crossflow fold cross validation is another example where you know you can prove properties about the expected value of cross validation that are very nice, but obviously in practice you can see that if you take a low fold cross validation, you can get quite serious.",
            "Errors in what you do, and that's due to this variance in the actual value."
        ],
        [
            "So what statistical learning theory has preferred to analyze?",
            "Is the tail of the distribution.",
            "So rather than look for that mean of that distribution, it's focused in to try and estimate if you like worst case.",
            "But how far out you might go, provided you're willing to throw away a certain amount of probability.",
            "So let me."
        ],
        [
            "Back to one of those pictures and I think it should be clear.",
            "So what you might say in statistical learning theory is you want to bound.",
            "That says OK, I'm willing to accept that one in 1000 or one in 100 times.",
            "I'm going to completely mess up.",
            "I'll just be unlucky.",
            "My training set will just be so weird that I won't be able to learn, but I want to be 99% confident that I'm going to have an error better than something.",
            "So if we take that example here, what we would say?",
            "I mean, this is actually 1000 simulations, so he cut off the top 10.",
            "The best we could say would be something like about .65.",
            "So you imagine you know we're allowing ourselves the luxury of throwing away the tail of 10.",
            "Sort of worst case runs and then what is the best?",
            "What is the worst error we can get .65 so this is the kind of estimate that we might expect from a statistical learning analysis.",
            "And clearly there's a very big difference between that and the expected value, which is still down way in the below .3.",
            "Similarly, if we."
        ],
        [
            "Go back one more, maybe the top 10.",
            "Here might bring us to about .5, but the expected value is Stillwell, well close to the true value in oh.",
            "Point 2.1 four so you can see that there's a very great difference in what you actually doing in some concrete cases."
        ],
        [
            "So in a way we can see it a bit like a statistical test.",
            "If we're significant at the 1% confidence, that means in a statistical test, that means that the chances that that data arose if the hypothesis is true.",
            "I've got the right way round is less than 1% and so you throw out the hypothesis.",
            "You know you say that you know, OK, I may be wrong 1% of the time I will be, but the chances of that data arriving if this hypothesis was true is so low that I'm going to see my hypothesis is false here.",
            "We've sensually got the same thing we've got.",
            "OK, we affirm that the generalization error is less than something.",
            "Of course, there's a chance that we might be wrong that the data set was so misleading that we actually made that mistake.",
            "And this actually is the source of the acronym PACK, which stands for probably approximately correct, and the confidence parameter, Delta, is this probably bit, which is this confidence.",
            "So it's the.",
            "There's a chance that we were misled by the training set in our calculations, so you know, we say probably in the sense that with probability 1 minus DD is the probability that OK. We were just unlucky and we got a really bad training set.",
            "Probability 1 minus Delta is that probably part and approximately correct is the generalization error part.",
            "The actual value of the generalization?",
            "Um, OK so?"
        ],
        [
            "How we doing for?",
            "So I think I would like to try and just do this slide if we can.",
            "I'm sort of got one minute.",
            "Well, I'll start doing this slide and we so one of the things I wanted to try as I said at the beginning to do is to sort of give you the flavor of the key approaches that statistical learning theory uses, and one of the sort of key ideas when you're applying this is to aim to bound that value Delta so you try when you're trying to work out a sort of bound on the probabilities of these things you aim at bounding the probability that will then be set equal to Delta.",
            "So for instance in and that is this probability essentially being misled.",
            "So let's take a very, very simple case of a single bad function F. So this is a function that has a bad generalization error, has error less than greater than epsilon.",
            "Where epsilon is some number which we won't specify at the moment, but let's say .1 or something, but actually looks good.",
            "It has zero training error, so I'm denoting errors of S as the training error, an error, the sort of generalization error that we're looking at.",
            "The actual error on true probability error.",
            "So if we try and what we try and do is bound the probability that a sample will actually.",
            "Make F look this good when actually it's not.",
            "OK, so this is the idea.",
            "You're sort of approaching it to bound over a sample that it can mislead you.",
            "OK, so how would we?",
            "How would this look?",
            "Is the probability that the error is the training error is 0 the true error is epsilon.",
            "Now we use the fact that the sample is generated independently at random according to the distribution.",
            "So as we generate the first example, the chance that it looked good was of course 1 minus the error.",
            "Of F, right?",
            "The error of F is the chance of a randomly chosen example being misclassified by F. So in order that for it to look good, it has to be correctly classified.",
            "So it has probability of 1 minus the error of F. Because of the independence, the probabilities multiply up, so the probability that M examples mislead us independently chosen is that probability 1 minus error of F to the power M. An becausw error of F is greater than epsilon.",
            "This is less than or equal to 1 minus epsilon to the M, so we've just slightly reduced this value here, which is increased this value.",
            "And now there's a step here from this expression to this expression, which is very sort of often used 1 minus epsilon, we could imagine again making it a bit bigger by expanding it by plus epsilon squared over 2 factorial minus epsilon cubed over 3 factorial plus epsilon to the 4th over 4 factorial, and so on.",
            "And that would make it slightly bigger than that actually.",
            "Is the expansion of E to the minus epsilon.",
            "So we get each of the minus epsilon to the power M, which is the E to the minus epsilon M. So it's just a very standard little approximation that we make.",
            "So what we have is that the probability of this guy misleading us with the IT.",
            "It looks very good on the training set, but actually has some error is getting to be exponentially small as the training set size increases.",
            "OK.",
            "So at that point I think I'll take a break, but if you in the breaker looking to you know have some fun, you might want to try and arrive this quantity by setting this equal to T. So this is saying, let's say we want this value to be equal to T. How what value of epsilon should we choose to make this equal to T?",
            "And this is the value, so you might want to look at that.",
            "OK, so I think we how long do we have quarter and arrow?",
            "Cortana OK, so we meet back at.",
            "Well let's make it 10 if we can.",
            "OK little maybe 2 minutes after thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about basic statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "I was trying to think what I could hope to do for you in this period of time allotted and what you might be able to take away from this short set of lectures.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And something that came to mind was a series of books that are actually sold in in the UK, known as the Bluffers Guide to.",
                    "label": 0
                },
                {
                    "sent": "And you can get a bluffer's guide for all sorts of topics, from psychoanalysis to whiskey, to skiing to philosophy or whatever.",
                    "label": 0
                },
                {
                    "sent": "And the idea of a bluffer's guide.",
                    "label": 0
                },
                {
                    "sent": "Bluffer is somebody who sort of makes his way by not really knowing what's going on, but has the right terminology and can sort of get by at the cocktail party.",
                    "label": 0
                },
                {
                    "sent": "So the Bluffers guide book gives you all the key terms and tells you you know how to use them in the right context.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you know if you start talking in statistical learning theory about the ghost sample, you know you should know that that isn't the data set you used in your last NIPS submission.",
                    "label": 0
                },
                {
                    "sent": "And the double sample trick is not actually something to do with cards and so on.",
                    "label": 0
                },
                {
                    "sent": "Concentration is not about level of alcohol in your blood and so on.",
                    "label": 0
                },
                {
                    "sent": "So my intention there.",
                    "label": 0
                },
                {
                    "sent": "I mean, I mean, that's joking aside, I think these books actually also are quite instructive.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're surprisingly informative.",
                    "label": 0
                },
                {
                    "sent": "They give you the sort of level strategic level view of that subject.",
                    "label": 0
                },
                {
                    "sent": "That often is very hard to get.",
                    "label": 0
                },
                {
                    "sent": "From a textbook which will go into a lot more detail and a lot more of the you get lost in the detail and you don't get the sort of overview.",
                    "label": 0
                },
                {
                    "sent": "So in a sense that's the first level that I would like to try and reach that people come away from this with a sense of what are what statistical learning theory is attempting to achieve.",
                    "label": 0
                },
                {
                    "sent": "What is it's sort of contribution?",
                    "label": 0
                },
                {
                    "sent": "If you like, what does it bring to the table of machine learning?",
                    "label": 0
                },
                {
                    "sent": "To what extent is successful in doing that?",
                    "label": 0
                },
                {
                    "sent": "And to what extent it fails or needs to be further refined?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I suppose in a sense putting it in the context of of the research that you might be doing and how it relates to that research.",
                    "label": 0
                },
                {
                    "sent": "So if you like, that will be the level.",
                    "label": 0
                },
                {
                    "sent": "I would hope that everybody might take away from this.",
                    "label": 0
                },
                {
                    "sent": "I think also I would also hope that many of you might take a little bit more than that in that I will hope to cover.",
                    "label": 0
                },
                {
                    "sent": "Some of the key techniques, theorems, and.",
                    "label": 0
                },
                {
                    "sent": "Ideas behind the theory in a way that I hope will infuse you that there's actually a lot of interesting things in that theory, and a lot of nice ideas that are fun to work with.",
                    "label": 0
                },
                {
                    "sent": "So in the sense, I would like to hope that some of you will actually feel that's really interesting.",
                    "label": 0
                },
                {
                    "sent": "That's something I didn't know and now have learned about.",
                    "label": 0
                },
                {
                    "sent": "And finally, you know.",
                    "label": 0
                },
                {
                    "sent": "Also, I might hope that many of you will take.",
                    "label": 0
                },
                {
                    "sent": "An inspiration from this to try and apply some of the ideas to the research that you're doing.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, that would be the.",
                    "label": 0
                },
                {
                    "sent": "The major goal would be to see that yes, OK, that actually applies to what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "It can help me understand or further illuminate the actual algorithms I'm working with.",
                    "label": 0
                },
                {
                    "sent": "The, you know, the techniques that I'm using in my research, and actually those techniques could be applied in that area.",
                    "label": 0
                },
                {
                    "sent": "So if you like those, those are the sort of three levels I'm striving for.",
                    "label": 0
                },
                {
                    "sent": "But certainly I don't want to get lost in a lot of detail and I want to try and, but I want to try and give you some of the enthusiasm for some of the key ideas.",
                    "label": 0
                },
                {
                    "sent": "So on that note, I'll move forward.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And give you an overview of the basic structure that I'm hoping the things I'm hoping to cover.",
                    "label": 0
                },
                {
                    "sent": "So first of all, setting the context clearly, statistical learning theory is something I'll need to put in context with other attempts to understand learning systems.",
                    "label": 0
                },
                {
                    "sent": "So I'll spend most of this lecture looking at that.",
                    "label": 0
                },
                {
                    "sent": "Then move into the basic.",
                    "label": 0
                },
                {
                    "sent": "Pack ideas, probably approximately correct, is the what the acronym stands for and their proofs.",
                    "label": 0
                },
                {
                    "sent": "And again here I will be looking at some of the detailed proofs.",
                    "label": 0
                },
                {
                    "sent": "I don't want to just give you a series of results, I want to try and show you how those results are arrived at, where the proofs are instructive in themselves.",
                    "label": 0
                },
                {
                    "sent": "So it is my intention to try and look at proofs and see how they work.",
                    "label": 0
                },
                {
                    "sent": "At least the key ideas of how they work, so I'll be looking at those in a little bit more detail that will lead us to the sort of point at which.",
                    "label": 0
                },
                {
                    "sent": "If you like learning, theory was at when suddenly support vector machines broke on the scene and we're performing extremely well, and the theory that was at that point developed was completely unable to explain their performance, and I think that was a critical moment where if you like theory and practice were completely at odds with each other, and I think that that was a very challenged to the theory community to try.",
                    "label": 0
                },
                {
                    "sent": "And if you like upgrade their theory to match practice.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'll talk about next, which is this idea of real valued function classes and the margin.",
                    "label": 1
                },
                {
                    "sent": "That will hopefully be the point I reach at the end of today, so that's a V4 lectures and then starting tomorrow will look at more modern.",
                    "label": 1
                },
                {
                    "sent": "Approaches to statistical learning theory, concentration and stability, and then Rademacher complexity and its main theorem an onto some applications.",
                    "label": 0
                },
                {
                    "sent": "So again, I would hope to cover or go to the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And give a sort of aim of what I would hope, since this is just giving a recapitulation to some extent of what I said at the beginning, but some thoughts on why theory, what it can do for you.",
                    "label": 0
                },
                {
                    "sent": "The basic techniques, with some deference to history, but not a strict historical sequence.",
                    "label": 1
                },
                {
                    "sent": "Insights into the proof techniques and statistical learning approaches.",
                    "label": 1
                },
                {
                    "sent": "And tomorrow a complete proof of the SVM bound using rather radical approach.",
                    "label": 0
                },
                {
                    "sent": "I think somehow taking you through the whole thing.",
                    "label": 0
                },
                {
                    "sent": "You know it's like climbing Everest or something.",
                    "label": 0
                },
                {
                    "sent": "You know it's not that hard, but it can give you an idea of what the different components that are needed in a proof.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that is what I'm hoping to include.",
                    "label": 0
                },
                {
                    "sent": "What I won't include is the most general results by any stretch.",
                    "label": 0
                },
                {
                    "sent": "I certainly don't know them.",
                    "label": 0
                },
                {
                    "sent": "I certainly won't include a complete history.",
                    "label": 0
                },
                {
                    "sent": "I won't be looking at an analysis of Bayesian inference.",
                    "label": 1
                },
                {
                    "sent": "Typically that is.",
                    "label": 0
                },
                {
                    "sent": "Viewed as outside statistical learning theory until recently when development of the PAC Bayes.",
                    "label": 0
                },
                {
                    "sent": "Approach which sort of links the two, but I gain won't be covering that, and Olivier Busquet, who's giving the advanced section of this course will be looking at the PAC Bayes results and also at this idea of local Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "So developing on from the Rademacher stuff I'll be talking about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's hopefully set the scene.",
                    "label": 0
                },
                {
                    "sent": "I would welcome you to ask questions and.",
                    "label": 0
                },
                {
                    "sent": "Again, give feedback about speed and appropriateness of what I'm saying.",
                    "label": 0
                },
                {
                    "sent": "Please just drop your hand and you know I'll try and spot you and.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to start with an attempt to.",
                    "label": 0
                },
                {
                    "sent": "By the way, is there a pointer that I can use for the just that I see?",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so I'm I'm going to start with.",
                    "label": 0
                },
                {
                    "sent": "If you like what?",
                    "label": 0
                },
                {
                    "sent": "How does the statistical learning Community approach what they're trying to do in understanding learning systems?",
                    "label": 0
                },
                {
                    "sent": "So the basic viewpoint is that is a statistical one.",
                    "label": 0
                },
                {
                    "sent": "Clearly, as the title suggests.",
                    "label": 0
                },
                {
                    "sent": "But as with any theory, the aim is to try and model some phenomenon, and in this case it could be it's part real part.",
                    "label": 0
                },
                {
                    "sent": "Artificial real data may be real, but the actual algorithms are artificial, but it is a phenomenon and you're trying to understand what's going on.",
                    "label": 0
                },
                {
                    "sent": "It's the nature of science is exactly this.",
                    "label": 0
                },
                {
                    "sent": "You know you have some phenomenon in the world and you try and write down a theory which explains that.",
                    "label": 0
                },
                {
                    "sent": "Behavior of that system and the aim.",
                    "label": 0
                },
                {
                    "sent": "Of course of doing that is in one sense just the intellectual enjoyment of having an understanding of the world, but the more practical approach is in order that you can predict how that will perform and hopefully get it to perform better, that is, exploit your knowledge in terms of actually improving the algorithm, improving whatever you're trying to do.",
                    "label": 0
                },
                {
                    "sent": "So a theory is always there to try and support practice.",
                    "label": 0
                },
                {
                    "sent": "And there's always this interleaving between theory and practice, and the more active that interleaving, the better.",
                    "label": 0
                },
                {
                    "sent": "I think for both for the theory and for the practice.",
                    "label": 0
                },
                {
                    "sent": "I should emphasize right away.",
                    "label": 0
                },
                {
                    "sent": "Statistical learning theory is just one approach to understanding these learning systems.",
                    "label": 1
                },
                {
                    "sent": "And other approaches include Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "As I said, I'm not going to speak about that inductive inference.",
                    "label": 1
                },
                {
                    "sent": "Statistical physics, traditional statistical analysis.",
                    "label": 0
                },
                {
                    "sent": "All of these have perhaps slightly different emphasis and different premises in the way they approach and different strengths.",
                    "label": 0
                },
                {
                    "sent": "So I'll mention that on the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each theory makes assumptions about the phenomenon of learning, so if you're trying to model something, your immediate task is to abstract away from the details of the particular into a level that you can actually make some conclusions and derive some results about, and in that abstraction you're going to lose something for sure, and the critical thing is to choose your modeling at a level that captures the essential and doesn't lose critical elements.",
                    "label": 1
                },
                {
                    "sent": "Of your of the thing you study, if you abstract too far, you lose.",
                    "label": 0
                },
                {
                    "sent": "You may get a beautiful theory, but it will no longer be applicable.",
                    "label": 0
                },
                {
                    "sent": "And each theory abstracts in its own way, and so you know to say one theory is better than another is always a very risky thing to say.",
                    "label": 0
                },
                {
                    "sent": "I think it's a case of looking at one theory may be more applicable in these type of situations because the abstraction it makes is appropriate and captures what's going on in that situation.",
                    "label": 0
                },
                {
                    "sent": "This theory may be more appropriate in this situation where the actual conditions are slightly different.",
                    "label": 0
                },
                {
                    "sent": "So I think you know to sort of get into turf wars about Bayesian and frequentist.",
                    "label": 0
                },
                {
                    "sent": "Statistics seems to be very short sighted, and really what we should be trying to do is understand when one works and when the other works.",
                    "label": 0
                },
                {
                    "sent": "So on that note, I think that you know, hopefully as part of this course one will begin to see what the modeling that statistical learning theory does contributes and where it can be of use.",
                    "label": 0
                },
                {
                    "sent": "And I certainly don't want to make a claim that it's the be all and end all of understanding.",
                    "label": 0
                },
                {
                    "sent": "Learning by any stretch of the imagination, but hopefully it has something.",
                    "label": 0
                },
                {
                    "sent": "To bring to the table of of this very interesting subject.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now getting into a little bit more down into the particular approach of statistical approach to learning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to simplify at this point, but I think it's a setting a framework that will be very generally applicable in this in this approach.",
                    "label": 0
                },
                {
                    "sent": "The assumption that you typically begin with is that the data is generated by an underlying distribution, so it's a sort of probabilistic assumption about the way the data arises.",
                    "label": 1
                },
                {
                    "sent": "And the learner himself actually has very little access to information about that distribution.",
                    "label": 0
                },
                {
                    "sent": "You may have some prior knowledge, some, but not enough to do what a Bayesian would do, which is build a model of that process.",
                    "label": 0
                },
                {
                    "sent": "So you're making the assumption that you have very little knowledge about the way the data is generated.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we are trying to classify tissue samples to decide whether they are cancer cells or not, cancer cells there, there is a distribution of the way those cells look from a particular population, so a typical healthy cell looks in this sort of way.",
                    "label": 0
                },
                {
                    "sent": "An if you generate a typical healthy cell, you'll get an example of that probability distribution typical cancerous cells.",
                    "label": 0
                },
                {
                    "sent": "Look this way an if you generate one from the probability distribution of cancer cells, you get an example of that distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is the assumption behind.",
                    "label": 0
                },
                {
                    "sent": "Now of course this is already making maybe some.",
                    "label": 0
                },
                {
                    "sent": "Two strong steps and of course this is, you know, as soon as you make these assumptions, you may lose something and and that's the price you pay for making a theory.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But that's the, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach this taken.",
                    "label": 0
                },
                {
                    "sent": "In a sense, this assumption of a distribution subsumes everything that we might hope to know about the processes in the natural artificial world that we are studying.",
                    "label": 1
                },
                {
                    "sent": "Now that's not going to be true in the end, because we're going to make in the way we design our learning algorithms and so on.",
                    "label": 0
                },
                {
                    "sent": "We're going to incorporate our knowledge about the world.",
                    "label": 1
                },
                {
                    "sent": "We are the problem we're studying, but in a sense, the distribution contains all the real information.",
                    "label": 0
                },
                {
                    "sent": "Now the problem in learning is precisely the fact that that distribution contains the information, but we have no way of accessing it directly.",
                    "label": 0
                },
                {
                    "sent": "We have no way of getting to.",
                    "label": 1
                },
                {
                    "sent": "If you like an explicit formula for the probability density function of that distribution, rather than that we're given rather poor training sample or training set, which is just a sample of points generated according to that distribution, the classical.",
                    "label": 0
                },
                {
                    "sent": "Situation in the training sample.",
                    "label": 0
                },
                {
                    "sent": "Now I've given an example here with the input output pairing, I'm probably going to focus mainly in this course on classification, so one would think of the wise as being a label indicating membership of a class or not.",
                    "label": 0
                },
                {
                    "sent": "But of course in general they could be real valued outputs, or they could be vector valued outputs, or they could be.",
                    "label": 0
                },
                {
                    "sent": "It could be just no output and just a.",
                    "label": 0
                },
                {
                    "sent": "A clustering problem, for example, where you only have affectively an input to your system.",
                    "label": 0
                },
                {
                    "sent": "But as an example, here we have the pairs input, say, classification output X1Y one up to XMYM and again the assumption we're going to make and again this is already making another assumption is that this sample is generated independently and identically according to this distribution P. So in a sense we're getting our information about P comes through this sample.",
                    "label": 0
                },
                {
                    "sent": "It comes through independent generations of examples by that distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this leads very.",
                    "label": 0
                },
                {
                    "sent": "Immediately to what we would then try and do.",
                    "label": 0
                },
                {
                    "sent": "Typically a learning algorithm and I'm using a curly a here would choose a function from a function space F. Up here and I'm denoting the function it chooses by the algorithm sub F applied to the training set S. So notice that the actual choice of function will depend on the training set.",
                    "label": 0
                },
                {
                    "sent": "Even if you're learning the same problem, if you generate a different sample and apply your algorithm to it, it's going to give a different result.",
                    "label": 0
                },
                {
                    "sent": "Hopefully not that different, but it will be different, so that's the sort of result of your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's the actual function that it gives an from a statistical point of view, the quantity that we're really interested in is this quantity here, which is the expected value of the loss.",
                    "label": 0
                },
                {
                    "sent": "When we apply this function to a randomly generated knew test point.",
                    "label": 0
                },
                {
                    "sent": "So XY is a new test point, and this is the function we learned from the sample which we were given an when we apply that function to the new example and according to some measure measure.",
                    "label": 0
                },
                {
                    "sent": "How well it matches the output, so the loss here is some measure of the fit.",
                    "label": 0
                },
                {
                    "sent": "If you like of the output of your function that you've learned and the actual output that should be given an, we want really to estimate the expectation of that loss.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to call that epsilon SA&F, so this S is the sample.",
                    "label": 0
                },
                {
                    "sent": "It depends on the particular sample.",
                    "label": 0
                },
                {
                    "sent": "Now this is the loss for a particular training set, an algorithm, and a function class.",
                    "label": 0
                },
                {
                    "sent": "So this is if you like the measure of performance of the actual algorithm on the training on the particular training set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, in the case of classification, we can take the loss L to be one if the two disagree.",
                    "label": 1
                },
                {
                    "sent": "In other words, if the output of your function applied to the test input is different from the output that should be given, so it gives the wrong classification and otherwise to have zero.",
                    "label": 0
                },
                {
                    "sent": "If it's the right classification.",
                    "label": 0
                },
                {
                    "sent": "Multiple regression, it could be the square of the difference between the output of the function.",
                    "label": 1
                },
                {
                    "sent": "So this will now be a real value and this will be.",
                    "label": 0
                },
                {
                    "sent": "The true output.",
                    "label": 0
                },
                {
                    "sent": "Of course this would be difficult to say.",
                    "label": 0
                },
                {
                    "sent": "It could be correct, so you typically you know exactly will be only accurate to a certain degree.",
                    "label": 0
                },
                {
                    "sent": "So you take the difference between these and square them and the losses is not a discrete quantity in that case.",
                    "label": 0
                },
                {
                    "sent": "So we're going to refer to this random variable epsilon.",
                    "label": 0
                },
                {
                    "sent": "SAF is the generalization of the learner.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very very core concept.",
                    "label": 0
                },
                {
                    "sent": "So rather than you know, I think it might be worth doing a very little simple exercise at this point just to sort of before we lose ourselves in the symbols.",
                    "label": 0
                },
                {
                    "sent": "So what I'd like you to do is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find out another way of expressing this quantity.",
                    "label": 0
                },
                {
                    "sent": "In the case of classification.",
                    "label": 1
                },
                {
                    "sent": "So as I've written it, it's the expectation of a loss.",
                    "label": 0
                },
                {
                    "sent": "OK, but in the case of classification you can simplify that down to something perhaps more understandable.",
                    "label": 0
                },
                {
                    "sent": "So if you just want to take a couple of minutes to try and write down what that would be in the case where the loss function is this discrete loss, one of your misclassified 0, if you get the right label.",
                    "label": 0
                },
                {
                    "sent": "OK. No one OK yeah OK, do you want to?",
                    "label": 0
                },
                {
                    "sent": "I'm not ready.",
                    "label": 0
                },
                {
                    "sent": "I just wrote this sum from one to end of an indicator function.",
                    "label": 0
                },
                {
                    "sent": "Whether the sign of.",
                    "label": 0
                },
                {
                    "sent": "The output of your algorithm for your function is equal to the target.",
                    "label": 0
                },
                {
                    "sent": "While you're looking for.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's it.",
                    "label": 0
                },
                {
                    "sent": "So that's actually the test.",
                    "label": 0
                },
                {
                    "sent": "So that's the training error.",
                    "label": 0
                },
                {
                    "sent": "I guess you're thinking of that this what this is is the test error, so it's more it's.",
                    "label": 0
                },
                {
                    "sent": "I mean that certainly would be the right estimation on the training set.",
                    "label": 0
                },
                {
                    "sent": "But what I was looking for was a an expression for this in terms of the probabilities of these events.",
                    "label": 0
                },
                {
                    "sent": "Anyone else have a yeah?",
                    "label": 0
                },
                {
                    "sent": "Integral of the Delta function.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "It could be expressed in that way.",
                    "label": 0
                },
                {
                    "sent": "That's for sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I actually was thinking of.",
                    "label": 0
                },
                {
                    "sent": "Well, let me let me show you what I was thinking and then hopefully it will be clear.",
                    "label": 0
                },
                {
                    "sent": "So if we take that.",
                    "label": 0
                },
                {
                    "sent": "I have to do this and that.",
                    "label": 0
                },
                {
                    "sent": "We take that expectation.",
                    "label": 0
                },
                {
                    "sent": "Over XY.",
                    "label": 0
                },
                {
                    "sent": "Of the loss of function, I'll just write a of S leaving out, yeah.",
                    "label": 0
                },
                {
                    "sent": "X.",
                    "label": 0
                },
                {
                    "sent": "Why OK so?",
                    "label": 0
                },
                {
                    "sent": "Essentially, we can write that as the.",
                    "label": 0
                },
                {
                    "sent": "Thank you, OK. We can write that as the probability that a S. Sorry, the probability that a as of X is equal to y * 0 because the loss is zero in that case.",
                    "label": 0
                },
                {
                    "sent": "Plus, the probability that a S of X is not equal to y * 1 because the loss in that case.",
                    "label": 0
                },
                {
                    "sent": "This is just working out the expectation in terms of the different events that can occur.",
                    "label": 0
                },
                {
                    "sent": "Remember, the discrete loss is.",
                    "label": 0
                },
                {
                    "sent": "Perfectly focused, is it?",
                    "label": 0
                },
                {
                    "sent": "OK, can you see at the back is that OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that means that of course this.",
                    "label": 0
                },
                {
                    "sent": "Is doesn't occur and it actually just equals the probability that the function applied to the GNU test point is not equal to the correct value.",
                    "label": 0
                },
                {
                    "sent": "So in other words, this in this case this generalization for the discrete loss is just the probability of misclassification.",
                    "label": 0
                },
                {
                    "sent": "That's all I was trying to drive out.",
                    "label": 0
                },
                {
                    "sent": "Maybe I didn't express it very clearly, but it's just in that case it very becomes a very simple expression.",
                    "label": 0
                },
                {
                    "sent": "Is everyone happy with that idea?",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "So I just don't want you to get phased by all these.",
                    "label": 0
                },
                {
                    "sent": "I mean one of the big problems with theory is that because you have to write things down precisely, you get all of this sort of garbage of symbols and and it's just, I mean, once you get used to, it is just a set of symbols.",
                    "label": 0
                },
                {
                    "sent": "I mean, the ideas are not actually that difficult or or complicated really, it's just you get phased by the symbols.",
                    "label": 0
                },
                {
                    "sent": "So part of it is to just work with him a little bit and get familiar with the symbols on the way that they.",
                    "label": 0
                },
                {
                    "sent": "So you know, this looks horrible, horrid, but in this case it just reduces to something very simple.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is really a very I think, important or I need to put this back.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this quantity epsilon safs.",
                    "label": 0
                },
                {
                    "sent": "This is the expected value of the loss on a randomly generated test point in the case of classification is just the probability that a randomly generated point is misclassified.",
                    "label": 1
                },
                {
                    "sent": "If you think of running your algorithm with a training and test set, your test set error, which perhaps is also what you were referring to, maybe is an estimate of this.",
                    "label": 0
                },
                {
                    "sent": "It's a sample estimate based on actually looking at the test examples, which you assume again are generated according to the same distribution, so it's a good estimate of that quantity, but from a theoretical point of view, we prefer to think of this as the quantity we're really interested in.",
                    "label": 1
                },
                {
                    "sent": "It's the pure probability of misclassification of a randomly generated point.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to try and give an example of what this random variable, how it looks in a particular concrete case, 'cause I think if you can build up a picture in your mind of what this random variable distribution looks like, a lot of learning theory starts to make sense.",
                    "label": 0
                },
                {
                    "sent": "You can see what learning theory is attempting to achieve, so I'm not going to give you an example which is real world data, breast cancer data set from the UCI repository.",
                    "label": 1
                },
                {
                    "sent": "I'm going to do a very simple parsing window classifier as described by Bernard Scholkopf at the beginning of his lecture.",
                    "label": 0
                },
                {
                    "sent": "So the weight vector is actually just it's a linear function classifier, thresholded linear function.",
                    "label": 0
                },
                {
                    "sent": "The weight vector is just the average of the positive training examples minus the average of the negative.",
                    "label": 1
                },
                {
                    "sent": "So if you can imagine the as a cloud of positives and you put a point in the center of that, there's a cloud of negative point in the center of that.",
                    "label": 0
                },
                {
                    "sent": "Join the two and bisect the line between them.",
                    "label": 0
                },
                {
                    "sent": "That is the hyperplane that we're going to use to classify.",
                    "label": 0
                },
                {
                    "sent": "OK so very very simple.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what I'm going to look at is, I'm going to repeatedly draw random training sets S. And I'm going to get an estimate of this distribution.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to create a histogram of this distribution of the errors.",
                    "label": 0
                },
                {
                    "sent": "So each time I draw a random training set, I'm going to compute this guy.",
                    "label": 0
                },
                {
                    "sent": "Of course, I'm going to compute it in the way that we always do, estimating it on on a test set.",
                    "label": 0
                },
                {
                    "sent": "OK, I can't help that, but because it's a, you know, if you had some abstract problem where you could define the true error.",
                    "label": 0
                },
                {
                    "sent": "But in our case we've got a concrete data set, so I'm just going to use the test error as a proxy for this, so I'm going to repeatedly generate random training sets, use the remainder of the data.",
                    "label": 1
                },
                {
                    "sent": "Is a test set an estimate this quantity after applying this very simple algorithm, the parsing window estimator and I'm going to plot the histogram and the average of the distribution of this quantity for various sizes of training set.",
                    "label": 1
                },
                {
                    "sent": "Now if I take the whole data set then that will give a single value, because essentially there's just one function is the average of the positive and negatives, and I'm going to apply it to the same set set and justice just like a sort of if you like thing we might aim for the best you could possibly do with this type of classifier in this situation.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use that if you like as a starting point, the whole data set.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to look at smaller and smaller values of training data, and we're going to look at what happens to the distribution.",
                    "label": 1
                },
                {
                    "sent": "In that situation is unclear what I'm trying to do, so I'm going to say fixed my size 342.",
                    "label": 0
                },
                {
                    "sent": "That's half actually 50% of the training set, so I will then.",
                    "label": 0
                },
                {
                    "sent": "You know train deposit window on that half tested on the other half and I get one value of that error and then I'm going to do that 1000 times and get a histogram of the errors that I get out OK, and then I'll compute the average OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason this is quite a interesting example is that actually the expected classifier that we get in all cases, whatever sum whatever sample size we take, is the same because the expected value of this result of this learning algorithm is the expected value under the particular training set of the positive average of the positives, which I'm denoting W plus.",
                    "label": 1
                },
                {
                    "sent": "Sub S and the average of the negatives I'm denoting W minus of S because the expectation is can go through addition.",
                    "label": 0
                },
                {
                    "sent": "We can actually write this as the expectation of the positives minus the expectation, the negatives, and then we've got.",
                    "label": 0
                },
                {
                    "sent": "This is just actually the expectation of the points that have positive label because the average of a set of points of positive label.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "Sample size, whatever.",
                    "label": 0
                },
                {
                    "sent": "You're always going to.",
                    "label": 0
                },
                {
                    "sent": "The expected value is always going to be the true mean of all of the positives.",
                    "label": 0
                },
                {
                    "sent": "So in fact this this is going to be the true mean of all the negatives, so the expected value.",
                    "label": 1
                },
                {
                    "sent": "Of course, as you get small training sets, you're going to get lots of variation, but the expected value of this classifier is always the same.",
                    "label": 0
                },
                {
                    "sent": "That doesn't actually mean that the expected loss.",
                    "label": 0
                },
                {
                    "sent": "The average loss is going to be always the same because the losses are non linear function, so you can't do this trick for the loss, but it does sort of give you a feeling that yeah, you know the loss shouldn't change the expected loss shouldn't change that much and will see that is to some extent true OK?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the sort of perfect.",
                    "label": 0
                },
                {
                    "sent": "Case where you take the whole training set, so essentially there's just one value.",
                    "label": 0
                },
                {
                    "sent": "The distribution is a single point at about .1 four.",
                    "label": 0
                },
                {
                    "sent": "That's the error of the parsing window estimator.",
                    "label": 0
                },
                {
                    "sent": "If you like on the whole data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to say, size 342, so of course notice that the expected value is almost identical.",
                    "label": 0
                },
                {
                    "sent": "So as we predicted, the expected value is sort of pretty pretty tight there, but now we're getting quite a bit of variation in the actual random values of this generalization, so on some training sets we're getting errors as high as point to .2, and on some we're getting errors as low as .1, and it's just the luck of the game.",
                    "label": 0
                },
                {
                    "sent": "You know, if you happen to be lucky and get a good training set.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe you might say good test set it depends which we want to look at.",
                    "label": 0
                },
                {
                    "sent": "Then you actually get a better generalization.",
                    "label": 0
                },
                {
                    "sent": "So now as we reduce the size the the distribution begins to slightly change, so that's down to .4.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "40% of the training data that's three.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "80% of the training data, sorry, 30% of the data for training 20% and now you can start to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See that the distribution is beginning to get a little disheveled with starting because the training set size is small.",
                    "label": 0
                },
                {
                    "sent": "But notice that the mean is still very nicely centered.",
                    "label": 0
                },
                {
                    "sent": "Where we we started OK. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we keep going now, we're already getting, you know, quite a little bit of tail.",
                    "label": 0
                },
                {
                    "sent": "We're getting errors as high as .3 on.",
                    "label": 0
                },
                {
                    "sent": "This means started to move a little bit, but not very much.",
                    "label": 0
                },
                {
                    "sent": "OK, and if we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going that's down to I can't remember what the percentage was 34.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2720 and the mean is still below .2, so it's only increased by about .5 the mean.",
                    "label": 0
                },
                {
                    "sent": "But you know we're already getting errors as high as .6 up here.",
                    "label": 0
                },
                {
                    "sent": "And some training sets.",
                    "label": 0
                },
                {
                    "sent": "OK, I did make sure the training set had the right proportion of positives and negatives, so I wasn't cheating, and I mean it's not bad because of that, it's just bad luck in terms of statistics.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's 14 and now we're getting errors right up there.",
                    "label": 0
                },
                {
                    "sent": "And finally, that's.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, training set size 7.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So clearly you know, hopefully from those you can start to see the way in which this distribution behaves and thing.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I wanted to emphasize, particularly is the difference between the mean of the distribution, which actually kept to be quite nicely focused around the value you had at the optimum.",
                    "label": 0
                },
                {
                    "sent": "In the sense the start and the actual spread of the distribution which started to get to look quite bad, OK?",
                    "label": 0
                },
                {
                    "sent": "Now traditional statistics is actually concentrated on.",
                    "label": 1
                },
                {
                    "sent": "Looking at that mean.",
                    "label": 0
                },
                {
                    "sent": "And typical sort of consideration is the consistency of an algorithm and consistency of a classification algorithm A is defined to be the property that the limit as N tends to Infinity of this.",
                    "label": 1
                },
                {
                    "sent": "So the sample size is tending to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The expected value of this distribution of errors tends to what's known as the Bayes optimal function.",
                    "label": 0
                },
                {
                    "sent": "The Bayes optimal function is essentially the best you could ever do in a classification algorithm, because it's simply.",
                    "label": 0
                },
                {
                    "sent": "Classifies according to the label that is more likely so it covers the case where you might have noise in your data set so that a particular example might in some cases be classified both positively and negatively with different probabilities, and in that case obviously you have to pick the label that's most likely, and that's what the base thing does, yes.",
                    "label": 0
                },
                {
                    "sent": "What computer do you have on one selling limits?",
                    "label": 0
                },
                {
                    "sent": "Examples purchase an expected expectation of generalization here.",
                    "label": 0
                },
                {
                    "sent": "This should be the generalization of the Bayes.",
                    "label": 0
                },
                {
                    "sent": "Sorry that you're right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, that should be the loss of the base expected loss of the base function on this side.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I'll spot it.",
                    "label": 0
                },
                {
                    "sent": "Everyone else that you might want to add that so this should say the expected value of the loss of F bays on XY.",
                    "label": 0
                },
                {
                    "sent": "So what I meant was that the you know the function.",
                    "label": 0
                },
                {
                    "sent": "Essentially it converges to.",
                    "label": 0
                },
                {
                    "sent": "Or you could just say you know the limit as N tends to Infinity of AS of F is F days.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of the study, but I think you know from the previous slide you can see that the problem with N tends to Infinity is that it's as long as a piece of string.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I mean, does it happen at a million?",
                    "label": 0
                },
                {
                    "sent": "That's it happen as a billion.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's this.",
                    "label": 0
                },
                {
                    "sent": "What we want to know is what happens on a finite sample, really.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For finite sample generalization.",
                    "label": 0
                },
                {
                    "sent": "Has a distribution depending on the algorithm, function, class and sample size.",
                    "label": 1
                },
                {
                    "sent": "So, as I've indicated above, the traditional statistics has looked at the mean of this distribution, but I think we hopefully have seen that it can be misleading and crossflow fold cross validation is another example where you know you can prove properties about the expected value of cross validation that are very nice, but obviously in practice you can see that if you take a low fold cross validation, you can get quite serious.",
                    "label": 0
                },
                {
                    "sent": "Errors in what you do, and that's due to this variance in the actual value.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what statistical learning theory has preferred to analyze?",
                    "label": 1
                },
                {
                    "sent": "Is the tail of the distribution.",
                    "label": 1
                },
                {
                    "sent": "So rather than look for that mean of that distribution, it's focused in to try and estimate if you like worst case.",
                    "label": 0
                },
                {
                    "sent": "But how far out you might go, provided you're willing to throw away a certain amount of probability.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to one of those pictures and I think it should be clear.",
                    "label": 0
                },
                {
                    "sent": "So what you might say in statistical learning theory is you want to bound.",
                    "label": 0
                },
                {
                    "sent": "That says OK, I'm willing to accept that one in 1000 or one in 100 times.",
                    "label": 0
                },
                {
                    "sent": "I'm going to completely mess up.",
                    "label": 0
                },
                {
                    "sent": "I'll just be unlucky.",
                    "label": 0
                },
                {
                    "sent": "My training set will just be so weird that I won't be able to learn, but I want to be 99% confident that I'm going to have an error better than something.",
                    "label": 0
                },
                {
                    "sent": "So if we take that example here, what we would say?",
                    "label": 0
                },
                {
                    "sent": "I mean, this is actually 1000 simulations, so he cut off the top 10.",
                    "label": 0
                },
                {
                    "sent": "The best we could say would be something like about .65.",
                    "label": 0
                },
                {
                    "sent": "So you imagine you know we're allowing ourselves the luxury of throwing away the tail of 10.",
                    "label": 0
                },
                {
                    "sent": "Sort of worst case runs and then what is the best?",
                    "label": 0
                },
                {
                    "sent": "What is the worst error we can get .65 so this is the kind of estimate that we might expect from a statistical learning analysis.",
                    "label": 0
                },
                {
                    "sent": "And clearly there's a very big difference between that and the expected value, which is still down way in the below .3.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if we.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back one more, maybe the top 10.",
                    "label": 0
                },
                {
                    "sent": "Here might bring us to about .5, but the expected value is Stillwell, well close to the true value in oh.",
                    "label": 0
                },
                {
                    "sent": "Point 2.1 four so you can see that there's a very great difference in what you actually doing in some concrete cases.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in a way we can see it a bit like a statistical test.",
                    "label": 0
                },
                {
                    "sent": "If we're significant at the 1% confidence, that means in a statistical test, that means that the chances that that data arose if the hypothesis is true.",
                    "label": 1
                },
                {
                    "sent": "I've got the right way round is less than 1% and so you throw out the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You know you say that you know, OK, I may be wrong 1% of the time I will be, but the chances of that data arriving if this hypothesis was true is so low that I'm going to see my hypothesis is false here.",
                    "label": 0
                },
                {
                    "sent": "We've sensually got the same thing we've got.",
                    "label": 0
                },
                {
                    "sent": "OK, we affirm that the generalization error is less than something.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's a chance that we might be wrong that the data set was so misleading that we actually made that mistake.",
                    "label": 0
                },
                {
                    "sent": "And this actually is the source of the acronym PACK, which stands for probably approximately correct, and the confidence parameter, Delta, is this probably bit, which is this confidence.",
                    "label": 1
                },
                {
                    "sent": "So it's the.",
                    "label": 1
                },
                {
                    "sent": "There's a chance that we were misled by the training set in our calculations, so you know, we say probably in the sense that with probability 1 minus DD is the probability that OK. We were just unlucky and we got a really bad training set.",
                    "label": 0
                },
                {
                    "sent": "Probability 1 minus Delta is that probably part and approximately correct is the generalization error part.",
                    "label": 0
                },
                {
                    "sent": "The actual value of the generalization?",
                    "label": 0
                },
                {
                    "sent": "Um, OK so?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How we doing for?",
                    "label": 0
                },
                {
                    "sent": "So I think I would like to try and just do this slide if we can.",
                    "label": 0
                },
                {
                    "sent": "I'm sort of got one minute.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll start doing this slide and we so one of the things I wanted to try as I said at the beginning to do is to sort of give you the flavor of the key approaches that statistical learning theory uses, and one of the sort of key ideas when you're applying this is to aim to bound that value Delta so you try when you're trying to work out a sort of bound on the probabilities of these things you aim at bounding the probability that will then be set equal to Delta.",
                    "label": 0
                },
                {
                    "sent": "So for instance in and that is this probability essentially being misled.",
                    "label": 0
                },
                {
                    "sent": "So let's take a very, very simple case of a single bad function F. So this is a function that has a bad generalization error, has error less than greater than epsilon.",
                    "label": 1
                },
                {
                    "sent": "Where epsilon is some number which we won't specify at the moment, but let's say .1 or something, but actually looks good.",
                    "label": 0
                },
                {
                    "sent": "It has zero training error, so I'm denoting errors of S as the training error, an error, the sort of generalization error that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "The actual error on true probability error.",
                    "label": 0
                },
                {
                    "sent": "So if we try and what we try and do is bound the probability that a sample will actually.",
                    "label": 0
                },
                {
                    "sent": "Make F look this good when actually it's not.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the idea.",
                    "label": 0
                },
                {
                    "sent": "You're sort of approaching it to bound over a sample that it can mislead you.",
                    "label": 0
                },
                {
                    "sent": "OK, so how would we?",
                    "label": 0
                },
                {
                    "sent": "How would this look?",
                    "label": 0
                },
                {
                    "sent": "Is the probability that the error is the training error is 0 the true error is epsilon.",
                    "label": 1
                },
                {
                    "sent": "Now we use the fact that the sample is generated independently at random according to the distribution.",
                    "label": 0
                },
                {
                    "sent": "So as we generate the first example, the chance that it looked good was of course 1 minus the error.",
                    "label": 0
                },
                {
                    "sent": "Of F, right?",
                    "label": 0
                },
                {
                    "sent": "The error of F is the chance of a randomly chosen example being misclassified by F. So in order that for it to look good, it has to be correctly classified.",
                    "label": 0
                },
                {
                    "sent": "So it has probability of 1 minus the error of F. Because of the independence, the probabilities multiply up, so the probability that M examples mislead us independently chosen is that probability 1 minus error of F to the power M. An becausw error of F is greater than epsilon.",
                    "label": 0
                },
                {
                    "sent": "This is less than or equal to 1 minus epsilon to the M, so we've just slightly reduced this value here, which is increased this value.",
                    "label": 0
                },
                {
                    "sent": "And now there's a step here from this expression to this expression, which is very sort of often used 1 minus epsilon, we could imagine again making it a bit bigger by expanding it by plus epsilon squared over 2 factorial minus epsilon cubed over 3 factorial plus epsilon to the 4th over 4 factorial, and so on.",
                    "label": 0
                },
                {
                    "sent": "And that would make it slightly bigger than that actually.",
                    "label": 0
                },
                {
                    "sent": "Is the expansion of E to the minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we get each of the minus epsilon to the power M, which is the E to the minus epsilon M. So it's just a very standard little approximation that we make.",
                    "label": 0
                },
                {
                    "sent": "So what we have is that the probability of this guy misleading us with the IT.",
                    "label": 0
                },
                {
                    "sent": "It looks very good on the training set, but actually has some error is getting to be exponentially small as the training set size increases.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So at that point I think I'll take a break, but if you in the breaker looking to you know have some fun, you might want to try and arrive this quantity by setting this equal to T. So this is saying, let's say we want this value to be equal to T. How what value of epsilon should we choose to make this equal to T?",
                    "label": 0
                },
                {
                    "sent": "And this is the value, so you might want to look at that.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think we how long do we have quarter and arrow?",
                    "label": 0
                },
                {
                    "sent": "Cortana OK, so we meet back at.",
                    "label": 0
                },
                {
                    "sent": "Well let's make it 10 if we can.",
                    "label": 0
                },
                {
                    "sent": "OK little maybe 2 minutes after thanks.",
                    "label": 0
                }
            ]
        }
    }
}