{
    "id": "i26xkbizp2ayk4ufbckrxoq5ywvc6rqb",
    "title": "Monte-Carlo Planning: Basic Principles and Recent Progress",
    "info": {
        "author": [
            "Alan Fern, School of Electrical Engineering and Computer Science, Oregon State University"
        ],
        "published": "Nov. 15, 2010",
        "recorded": "October 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Monte Carlo Methods"
        ]
    },
    "url": "http://videolectures.net/icaps2010_fern_mcpbprp/",
    "segmentation": [
        [
            "Alright, thanks for coming everybody.",
            "So really, this is going to be about the very basics of Monte Carlo planning as I see them and we will talk about some of the recent progress is for example the UCT algorithm that will come at the end so."
        ],
        [
            "You're really waiting for that.",
            "You can take a break and wake up when I mention UCT again.",
            "So we've got off just giving a few preliminaries about Markov decision processes, mainly for notation, and then we're going to try to motivate Monte Carlo planning.",
            "Why is it useful?",
            "What is it?",
            "And then basically I'm going to talk about two classes of Monte Carlo planning techniques as I see it.",
            "One I'll call uniform Monte Carlo.",
            "It's not going to be very intelligent about how it samples the environment, and then we'll talk about adaptive Monte Carlo techniques at the end, culminating in the UCT algorithm which you might be familiar with because it's been so successful and go and more recently other problems."
        ],
        [
            "So yeah, I'm DPS.",
            "We're going to model the world is a Markov decision process.",
            "Will talk about that in a minute, but the basic model is we have this robot or and he can send actions to the world, possibly their stochastic actions and the world gives US states and rewards back.",
            "Or if you like observations and rewards back and we're interested in what's going on in this guys head, he wants to maximize his reward.",
            "So we're going to stick."
        ],
        [
            "Monte Carlo planning techniques.",
            "But in MDP is basically composed of four things.",
            "The first thing is going to be a set of states.",
            "These are the states of the world that sort of completely describe how the world is going to function.",
            "If we take an action, we have a finite set of actions and the thing that makes a Markov decision process have the name Markov in it is because the transition distribution PFT is a first order Markov process.",
            "What this means is this is a conditional probability distribution that tells you if you take a in state S, What's the probability of ending up in S prime.",
            "So that's what we mean by a transition distribution, and it's first order because it only depends on SNA right now.",
            "The reward distribution is also going to be first order Markov, and for any state it tells us if we take action A, what is the distribution over real valued rewards?",
            "And for the sake of this talk, we're going to be assuming that this has finite.",
            "The rewards are bounded within some finite range.",
            "Alright, so those are MD."
        ],
        [
            "He's pretty basic stuff that's just for notation, so if we have an MVP, what is a solution?",
            "Well, it's not good enough generally to output a straight line plan action.",
            "One that action 2, then Action 3 because once you take action one might transition to some state that maybe action two wasn't so well suited for.",
            "So you really need to form a plan that can handle whatever state you end up in.",
            "At any moment, and we call such a plan a policy and right now we're not going to compare.",
            "We're not going to care about how we get that policy.",
            "It could be computed online as you're planning or offline before hand, but a policy is simply a mapping could be stochastic from we're going to use Peiffer policy from states to actions, and basically it tells us what to do.",
            "Or in state S, and this defines a continuous reactive controller for this agent.",
            "At every moment he's in a state.",
            "Takes an action, ends up in a new state, and looks at his policy and figures out what to do next, so that's that's what the policy is, and the main question is, how do we determine whether a policy is good or bad?",
            "And intuitively a policy is good if on app achieves a high reward when we execute it in the environment."
        ],
        [
            "So specifically in this talk we're going to talk about finite horizon, discounted reward, and it's a mouthful, but we're going to note this function by V. \u03a0 SHH is our horizon.",
            "That's how long we're going to think ahead.",
            "That's how far into the future we're going to think ahead, and what that function tells us is what is the expected value we're going to get following this policy starting in S if we follow it for H steps.",
            "Plus we add in discounting.",
            "And so let's think about what this means.",
            "So if you run the policy for each step starting in S, you're going to get a sequence of rewards, right sequence of eight rewards, and this is really a random sequence.",
            "These are random variables because the dynamics are random.",
            "The reward function is random, so this is a random sequence, and what we're going to be interested in is the discounted of these sequences, so the expected value of the discounted sum and.",
            "We discount later rewards more than earlier rewards, so beta T is a discount factor if you set it to one then all of these rewards are equal to our sort of treated equally, and so it allows us to trade off how much we care about the future.",
            "Usually it's just there for mathematical convenience.",
            "Sometimes you can motivate it by economic principles or the probability of dying or whatnot.",
            "But in the end, what we're interested in is an optimal policy, so an optimal policy is one that dominates all other policy's across all states in terms of its value."
        ],
        [
            "So I'm not going to cover this slide in detail.",
            "I'll just say that.",
            "If you really like Infinite Horizon problems, so so in many cases you might have a continuous controller and you plan for the infinite horizon.",
            "They have to introduce a discount factor to make things well defined.",
            "Really finite horizons results are going to be good enough and the main reason is that we can approximate the infinite horizon value function with a finite horizon an if you choose your horizon long enough the approximation error goes to zero quickly.",
            "So basically everything that I say here will apply to infinite Horizon problems.",
            "You can refer to this slide.",
            "Again, if you."
        ],
        [
            "Your character care to see that?",
            "So the basic results in MDP theory is a pretty old results.",
            "Optimal policies are guaranteed to exist, so there are optimal policy zizza is not a partial order on policies and we have algorithms for computing them, so the algorithms are we can do it in polynomial time using linear programming and you can also use a value iteration or policy iteration which tend to be more efficient.",
            "But these algorithms are polynomial time in the number of States and actions, and that's not really so good and less you're in a domain where you can really engineer the problem to have a small number of states.",
            "So it's a nice result, but.",
            "For this talk, we're going to be interested in the case where the states space is exponentially large in terms of the problem encoding size.",
            "So the real world has very, very many States and we're going to try to get algorithms that work in the real world.",
            "Alright, so you can't apply these directly."
        ],
        [
            "So what can you do if you have a large world?",
            "Well, one approach would be what I'll call the model based approach.",
            "There's a lot of work on this, and what you'll do is you'll define a language that can compact scribe very large M DPS.",
            "Now, there are several ways you can do this, but one way would be to use a dynamic Bayesian network.",
            "You don't have to know the details of this or probabilistic strips to describe the transition and reward functions right, so these languages can be used to describe MDP's that are exponentially larger than their encoding size in terms of the language.",
            "Then you have to design A planning algorithm that can deal with that language and a lot of you here know how hard that can be.",
            "For example, with DBN's or probabilistic strips.",
            "So this is what I'm going to call the model based approach, and there are some problems with this.",
            "I don't want to kill research in this area because it's very interesting.",
            "And useful, but one problem is if you take a random application that you care about and you try to encode it in one of these languages, there's always going to be some feature of it.",
            "Usually an important feature that you can't quite fit in the language, and that there could be a couple of reasons for that.",
            "It could be because the problem size and the encoding will blow up for some reason.",
            "I'll give you an example of that in a moment, or it could be there's some fundamental representational shortcomings of these models, for example.",
            "You want exogenous events like 911 calls, but these languages can't handle them nicely.",
            "Alright, so this is a problem when you are faced with an application and there are some planners out here that deal with these languages, but you can't use them because your problem isn't described nicely in those languages."
        ],
        [
            "So that's where Monte Carlo.",
            "the Monte Carlo approach comes in, as opposed to the model based approach.",
            "So in these cases, when when you can't discuss problem compactly in a language that exists.",
            "You can often write a simulator for that domain, so you can use.",
            "Your language could be the C programming language and you could write a simulator that takes an action and a state and spits out a next state in a reward, right?",
            "So you can you can do this for pretty much anything you can imagine if you spend enough time.",
            "So a couple of examples if you try to encode the simple well seemingly simple problem of Klondike Solitaire in.",
            "Probabilistic PDL you'll find at least I can't find a compact way to do it so that you can't use the existing planners and you could model as a palm DP, partially observable MDP, but those planners don't really scale, so when we were interested in trying to solve this problem we were sort of stuck, but it's really easy to write a simulator for Klondike Solitaire, right?",
            "You have a particular situation.",
            "You take an action, you just steal a card.",
            "Same thing with another application I was involved with.",
            "Fire and emergency response here.",
            "We're interested in sort of routing multiple vehicles around town, placing them in places to be ready for emergencies in response to 911 calls that emerge.",
            "And this is a very exogeneous domain.",
            "There's exogenous events in terms of traffic patterns and the 911 calls that come in hard to model.",
            "This is in a standard language that exists today, but.",
            "It's easy to write a simulator.",
            "This is a picture of our simulator.",
            "If you have traffic models and whatnot so."
        ],
        [
            "There's just a couple of motivating examples, So what is Monte Carlo planning?",
            "And I should say, if you do have questions, you can raise your hand and interrupt me.",
            "I'll cut you off if it's if I think it should be dealt with later, but feel free to ask questions.",
            "So what is Monte Carlo planning?",
            "Well, roughly define it as we want to compute a good policy for an MDP by interacting with an MDP simulator as opposed to taking in a compact representation of a model.",
            "An planning with that so it's about computing good policy's by interacting with the simulator will formally define what we mean by a simulator in a moment, but now we have our agent here.",
            "He's gotta World Simulator in his head, and every step he's going to use that simulator play around, figure out what to do.",
            "Then get some feedback from the world.",
            "Then he's going to use the simulator to figure out what to do and take another action.",
            "Yeah, so good question.",
            "So let me define the simulator and then I'll mention that."
        ],
        [
            "It's.",
            "So you can look at this on your own.",
            "Basically there's a whole list of domains where there are simulators and I should mention these slides will be online and in fact if you wait a couple of days are going to be updated with references and whatnot.",
            "These are very fresh."
        ],
        [
            "From 15 minutes ago, in fact, so let's define what we want from our simulation based representation of an MDP.",
            "There's still some States and actions, but now we're instead of being given probability distributions and some language we're going to be given.",
            "RNTR is simply a stochastic function to see.",
            "Program will say that returns a reward when you pass in a state in an action.",
            "So just think of it as a function.",
            "Right and see that returns a random reward according to the distribution of our underlying MDP.",
            "And the same thing with T. This is a transition function.",
            "You pass it a state and action and it's going to spit out the next state, right?",
            "So these are simply stochastic functions that we don't care about their implementation.",
            "You could use C, Java, whatever language you like.",
            "You could encode it in a planning language and simulate that if you cared too.",
            "So that's what we mean when we say a simulation based representation.",
            "Now coming back to reinforcement learning.",
            "So again and reinforcement learning, we don't have a compact description of the environment.",
            "In fact, we start off with no knowledge of the environment whatsoever.",
            "We're just allowed to take actions in the environment.",
            "Here we start off with the simulator of the environment in our head, and so the key distinction is.",
            "That sometimes used is we're using a strong simulator here, and a strong simulator simulator is.",
            "Basically, you can teleport to any state at any moment and take any action that you want and reinforcement learning.",
            "We sort of in a week simulator model where you are where you are.",
            "You cannot teleport to any state that you want.",
            "If you want to get to a state, you have to figure out how to get there.",
            "Possibly in some cases you can have a reset that resets you to an initial state, so that's sort of the key distinction.",
            "A weak simulator is reinforcement learning, where in the strong simulator case, and there are some theoretical relations between these two so.",
            "So let's move on, so that's our."
        ],
        [
            "Relation based MDP representation.",
            "Now we're going to talk about the basic Monte Carlo algorithms.",
            "Now I said our motivation is to scale to very large state spaces, but I'm going to start out by talking about how do we use Monte Carlo techniques for a single state MDP, so that's not exciting by itself, but we're going to try to build on that and.",
            "Create things that can be applied to large MDP's.",
            "So we're going to start with a single state case.",
            "This turns out to be identical to what's called a multi armed bandit problem."
        ],
        [
            "Some of you might have heard of that so.",
            "Suppose we have a single state MDP, so S is our only state.",
            "We have these actions.",
            "We have K actions.",
            "OK, now what happens when we take one of these actions?",
            "Well, you get a random reward by calling this reward function.",
            "And you can view this is pulling the arm of a slot machine and getting some payout according to some distribution.",
            "And the distribution is given by our reward function, which we don't really know the implementation of.",
            "But we can sample from this distribution by just calling the function right and so pulling the arms correspond to us using our simulator to sample the reward function for the particular arm.",
            "Now what is?",
            "You know?",
            "If you knew these are functions.",
            "If you knew what they were, what would be the optimal thing to do for this MDP?",
            "What would be the optimal policy?",
            "Any guesses?",
            "We want to figure out which ARM has the highest expectation right?",
            "And if you were in Vegas and you knew that you would just keep pulling that arm right?",
            "The law of large numbers?",
            "Well, yeah.",
            "So so if the reward was reward payoff was positive, you just keep pulling the arm because law of large numbers says if you pull long enough you're going to come out on top in Vegas you just lose.",
            "So so so let's.",
            "Let's see how we can formulate this problem of picking the arm with the highest expectation."
        ],
        [
            "And there are different formulations.",
            "We're going to start out in this first part, talking about the pack bandit objective.",
            "So we have this single state MDP and I'm defining what our objective is.",
            "We're going to give a really simple algorithm for solving it.",
            "So probably approximately correct orpak.",
            "This is a term from learning theory.",
            "Basically says that we want to select an arm that probably.",
            "Has approximately the best expected reward, right?",
            "We can never really guarantee that we're going to pull the find the arm with the best expected reward in a finite amount of time.",
            "'cause you can always be unlucky and get bad samples, but we want to have an algorithm that probably such that with high probability every time we run it will approximately return the best reward.",
            "So an arm that's approximately optimal.",
            "And the key is we'd like to know how many simulator calls or poles we need to do that, right?",
            "So it's not really very interesting if you don't care about how many simulator calls you need, because you would just mean you can easily get asymptotic results with the law of large numbers.",
            "So this is a pack variant of the Multi armed bandit problem.",
            "Will talk about another."
        ],
        [
            "Later today.",
            "A very simple algorithm can be used to get a pack.",
            "And it is the most obvious algorithm that you might come up with on your own.",
            "We're going to say OK for each machine we're going to pull the arm W times each time we pull the arm, we get a reward.",
            "A sample of the reward function, right?",
            "So after W poles of this machine, we get W samples of that reward function.",
            "Same for this machine.",
            "Same for this machine.",
            "Naively, what you would do is average those results and return the arm that has the highest average, right?",
            "That's sort of.",
            "There's nothing terribly interesting about that algorithm, except we'd like to figure out how large does W need to be?",
            "Does it need to be exponentially large, and the problem size, or in the number of arms?",
            "Or can it be smaller?",
            "So that's sort of the question here that we're interested in.",
            "Reward.",
            "Oh yes, so.",
            "So word function also the results that we define are going to be for reward functions that are bounded.",
            "We don't care about the distribution, so any distribution, as long as it has a bounded extent.",
            "There are results that work for Gaussian reward functions and whatnot, but these are going to be different.",
            "Every poll if the reward function is stochastic and the variance in that distribution dictates how different they are going to be right.",
            "But on average, if you if it gets very very large, it's going to approach the mean and the question is how large this W need to be so that we can pick a good arm with a pet."
        ],
        [
            "Guarantee so this is so to figure out what W is.",
            "It's useful to know this is really the only mathematical thing I'm going to talk about here.",
            "If you're reading this stuff, you need to know the turnoff bound.",
            "At the very least.",
            "Most basic principle and random algorithms and theory.",
            "So.",
            "What this says is that if we have a random variable R and you can think of this as one of the arms are sort of dictates the rewards that we get.",
            "To sample that random variable IID identically, we're going to sample it W times and get a sequence of rewards, are I?",
            "Right the turn off bound.",
            "Is able to bound the probability that the average of these samples is far from the expectation of the samples and in particular.",
            "Is going to tell us that.",
            "We know that the bound is exponentially tight as W increases, so let's just look at the result so the turnoff bound basically says so.",
            "It might be a brain fall to see this all, but this is basically the difference between the expected reward, which, if we knew this would be done and the sample average right?",
            "So we sampled W rewards and we average them and this is the absolute difference between those two things.",
            "If this is zero, then we.",
            "Are done right, the law of large numbers says this goes to zero as W goes to Infinity.",
            "But the turnoff bound is much more powerful.",
            "It says that the probability that this quantity is greater than epsilon so is going to be less so greater than epsilon means that we don't have a good bound.",
            "It's going to be less than this quantity here.",
            "And the key thing is that this decreases exponentially fast with W, right?",
            "So it says that we don't have to pull the arms very much to bound the probability of the difference between our sample average and the expectation.",
            "Now, if you use a more basic bounds like the mark of inequality, you'll get a rate here instead of an exponential rate.",
            "This is just a slightly more.",
            "Sophisticated analysis.",
            "This is a turn off bound and it's used very commonly in computer science because of this exponential rate of decrease in the error that you can get right.",
            "So basically you can write this in a different way.",
            "And what it says is that with probability, so if you just rearrange terms with probability, at least one minus Delta.",
            "We can get the following bound.",
            "The difference between the expected value and the sampled average is going to be bounded by this amount, right?",
            "And so this is again, you can see that.",
            "Well, as Delta decreases, we want our probability guarantee to get smaller and smaller.",
            "This is going to go up, but only sort of log rhythmic rate.",
            "And W increases this T gets tighter and tighter, so there's just two different views of the same thing, right?",
            "So that's a turnoff bound.",
            "I recommend that you all know the proof of it is not really that hard.",
            "Look on Wikipedia so."
        ],
        [
            "You're going to need to know that if you ever read this literature.",
            "So now back to our problem.",
            "We have the Uniform bandit algorithm.",
            "It's called the Naive Bandit in the original paper.",
            "This is a paper on Pak Pak Learning and the multi Arm bandit setting.",
            "We have these samples and we wanted to know how large W needs to be.",
            "It's really easy to use our turn off bound now to figure out how large W needs to be."
        ],
        [
            "So I'll just give you the result.",
            "If you use a bit of algebra and the turnoff bound, you can derive that if W is larger than this quantity.",
            "So notice that K appears in the logarithm here for all arms simultaneously will have this bound, right?",
            "So if you pick W to be that quantity, at least we're going to be within epsilon for all arms with probability at least one minus Delta.",
            "So the estimates of all the actions are going to be epsilon accurate.",
            "With high probability and that means that we can select the highest the arm that has the highest average and be pretty sure we've got approximately an optimal arm, right?",
            "So technically it will be be within two epsilon of optimal."
        ],
        [
            "So now what is the sample complexity of this though?",
            "So the total number of simulator calls, we're going to need is just K, so the number each the number of arms times W which is K. This quantity right here.",
            "So it's roughly scales.",
            "Is K log K. Now more sophisticated algorithms?",
            "This show you here.",
            "You can use an algorithm called median elimination to get rid of this.",
            "Part right here if you want, and that's actually there's a lower bound for that.",
            "You can't do any better than this, right here?",
            "There's some other garbage as well, but fundamentally it's sort of linear in K. Alright, so it's kind of a fundamental result on PAC learning for multi arm bandit problems."
        ],
        [
            "Alright, so that was a single state case, not very exciting since our goal was to go to exponentially large state spaces.",
            "So how can we apply that basic idea to the exponentially large state spaces?",
            "Well, first I'm going to before we talk about, sort of.",
            "Trying to find an optimal policy, we're going to take an intermediate step which is very useful in practice and I'm going to call this policy rollout spend going to France is in the final version of this that will be online at icaps and on my web page, so you'll have some nice references to look at.",
            "It was developed by birthday, Coussan Tessaro, sort of simultaneously.",
            "So policy, rollo."
        ],
        [
            "Idea is very simple.",
            "So consider a multi state MDP.",
            "Now we want to act in it and suppose we have a simulator.",
            "That's what we've been assuming and now also suppose that we have some non optimal policy.",
            "If you're in a computer networking domain you can code up some non optimal allocation policy and see if you're interested in Klondike Solitaire you can code up the best policy you can and see it won't be very good most likely, but you can code it up.",
            "And it might get nontrivial performance.",
            "You will get performance greater than 0.",
            "So given these two things were going to give our agent these things, and what we'd like the agent to do is be able to improve on that policy right in the more computation time we give it, we would like the agent to improve more and more.",
            "Alright, so we're giving it a base policy, and it's sort of like a hint, and we want it to improve on that based policy.",
            "And it turns out that often this algorithm will give very big improvements with a little bit of work on the space policy, you can get big performance gains.",
            "Much bigger than if you spend three more months trying to improve this space policy often so."
        ],
        [
            "Policy rollout, that idea is called policy rollout, so to talk about policy rollout, we need to introduce the policy improvement theorem from MDP literature.",
            "This is one of the most basic results.",
            "We need to define something called the Q function for this, so Q \u03c0.",
            "So every policy Q function that SCOOP I the Q function is a function of state and action and the horizon and all it is is simply the expected reward we get when we start in state S and we take action A.",
            "So we start in State S, take action A and then follow the policy for H -- 1 steps right?",
            "So it's a very simple.",
            "Function to define.",
            "Garden State a certain status.",
            "Take action A then follow the policy for each minus one steps that gives you a sequence of rewards.",
            "The expected some of those rewards is the Q function, right?",
            "So this is a very common function to see an MDP theory.",
            "Now it turns out that if we define \u03c0 prime of S. To be the action that maximizes this Q function.",
            "So we have S fixed or policy is a function of S. We want to maximize over these actions.",
            "It turns out that policy is going to be a strict improvement over the original policy Pi.",
            "Alright, so some state it's going to improve, and it's not going to be any worse than any other state.",
            "If Pi happens to be optimal, then Pi prime will also be optimal.",
            "So this is the policy improvement theorem and it's the basis of.",
            "The policy iteration algorithm.",
            "So we're going to try to use this fact to somehow use develop a Monte Carlo based approach to policy improvement.",
            "So.",
            "Computing Pi prime amounts to maximizing this Q function.",
            "How can we use the bandit ideas to solve this to compute pipeline?"
        ],
        [
            "That's the question.",
            "So set up our bandit where in some State S we have our K actions.",
            "What we're going to do is define a stochastic function which we can implement.",
            "We're going to call it SIM Q.",
            "We're going to define this stochastic function and the expected value if we run this function over and over and over is going to be equal to the Q function.",
            "Right so intuitively, as I'll show you on the next slide, this is simply going to simulate taking action A and following the policy, and you sum up the rewards.",
            "Very simple to implement.",
            "And if we could do this if we can figure out what this simulate this function had had to implement this function since the expected value is Q Pi, we can use our uniform bandit algorithm to get a pack result for selecting an approximately optimal improved action.",
            "And approximately improved action alright?",
            "So how do we employ?"
        ],
        [
            "That's imcu, it's actually pretty simple.",
            "So let's say we pull this arm.",
            "And this is a function that we're going to implement and somehow using our simulator.",
            "So what do we do?",
            "We're just going to put the simulator and state S. Alright, that's the first thing we'll do.",
            "Then we're going to use a simulator, will take action, A will end up in some state according to our transition function, and then we're going to simulate.",
            "Then at each state we can look at \u03c0.",
            "Figure out what action it suggests, take that action, and we end up in a new state.",
            "We do that for H -- 1 steps.",
            "We sum up the rewards that we saw and this is what we output for SMQ.",
            "This is a random function because these trajectories are random trajectories 'cause our simulator is random potentially.",
            "So that's how we do some Q4A1 do the same thing for the other actions and.",
            "So thank you is easy to implement if you have the simulator.",
            "I think we could agree on that, and it's also easy to verify that the expected value MQ is the actual Q function, sort of by definition.",
            "It sort of doesn't even need a proof, it's."
        ],
        [
            "So.",
            "We have an algorithm for policy improvement.",
            "Now using our simulator.",
            "So for each arm we're going to run some QW times, right?",
            "These are these are.",
            "Basically, different runs of the same queue function, and these values are just the total rewards that we observed when we ran some queue.",
            "And then we're going to return the arm that has the highest highest average of these values.",
            "And according to our Uniform bandit result, it tells us well if we really wanted to pick W so that we get a pack guarantee we get a Pat guarantee on choosing an approximately improved action.",
            "Alright.",
            "Don't change your policy.",
            "Actually do the policy right right so the situation you want to imagine is you're a robot.",
            "You're in a state of the world that sass, and now you're trying to pick an action, and so in your head you're going to do all of this stuff with pie, and then you're going to pick the one that maximizes that, right?",
            "So you don't change pie, but that leads into the next thing.",
            "I think it's fairly.",
            "Well, you could nest rollouts right so we could stick in here instead of Pi.",
            "We could stick, stick in the rollout policy of \u03c0. I think that's a."
        ],
        [
            "Next slide.",
            "No, so so that will be coming in a moment.",
            "So this let's just finish up the rollout.",
            "So.",
            "So basically what we need?",
            "How many samples do we need to generate?",
            "Well, for each machine we need to generate H times W samples.",
            "There are K machines, so the number of calls to the simulator, which is sort of.",
            "The dominating complexity here is K HW.",
            "Everything else is just a constant.",
            "We're doing additions over K things and whatnot.",
            "Alright, so KHW you can get a pack guarantee."
        ],
        [
            "Multi stage rollout so.",
            "If you want to get fancy, once you have rollout implemented, you can easily call it sort of recursively on itself.",
            "So let's suppose that this whole process I just described, we're going to say is this policy here roll out pie, right?",
            "So roll out pie does all of the stuff I just talked about.",
            "We could pass this in two SIM Q write an SMQ would simulate freeze of the roll out policy of pie.",
            "So it's quite a mouthful here, but this is sort of accurate.",
            "Two stages is going to compute the roll out policy of the roll out policy of \u03c0.",
            "And what this corresponds to is if you're familiar with policy iteration.",
            "If you started at \u03c0.",
            "The action you end up picking is going to be what you would get if you had done two iterations of policy iteration, right?",
            "And of course there's a cost, because now at each of these nodes, in order to compute the action here, we have to do a rollout.",
            "That's going to cause cost.",
            "US KHW simulator calls, and so our complexity now is going to be quadratic in KHW.",
            "You can get by with this for for a few levels of nesting, of course, but it's going to miss exponential in the number of stages that you want, so if you want to simulate.",
            "Case will say M steps of policy iteration.",
            "You're going to have to have KQ to the M simulator calls alright?"
        ],
        [
            "So.",
            "I won't go over the details here, but it's just some example domains.",
            "I'll have references where you can write simple, mediocre policies.",
            "Networking is a big one.",
            "The game of Hearts, backgammon solitaire.",
            "There are non game ones as well.",
            "Emergency response and here are some example of domains where policy rollout has been actually shown to work well."
        ],
        [
            "You get some substantial improvements.",
            "I'll have references in the final version of this, but just to give you a sense, this is fairly old paper and NIPS 04 this is the domain of thoughtful solitaire.",
            "Well, it's not.",
            "Klondike will forget the details of what thoughtful solitaire is, but the human expert, this mathematician who's obsessed with the game.",
            "He records his winning percentage and it's about 36%.",
            "20 minutes per game.",
            "Write a very naive based policy.",
            "I mean, this is something that is really naive.",
            "You can get a success rate of 13%.",
            "Now you could spend months trying to improve that based policy or apply one level of rollout.",
            "You increase your time from nothing to slightly less than nothing.",
            "You can get up to 31%.",
            "How about 2 levels of rollout?",
            "You jump up to 47, beating the human expert and these are statistically significant.",
            "These probably aren't.",
            "You increase your time and so on and so forth.",
            "You see a pretty steady increase as you increase is increases stages of rollout.",
            "Of course, the time is increasing exponentially as well.",
            "At this point, we're into close to two hours to play a game, so it's actually hard to get statistically meaningful results in that case, but you can see there's a pretty Big Bang for your Buck here question.",
            "Sort of diminishing returns here.",
            "The more rollouts you do, well there probably there probably would be just, you know.",
            "It's getting so complex, the time complexity is getting large enough that we can't actually sort of evaluate these.",
            "You know, probably you're looking at.",
            "Five hours here.",
            "Who knows what it is.",
            "Maybe it's going to be a day in another stage or two to actually implement this this policy, so that's why we're not seeing that.",
            "But yeah, obviously eventually you would sort.",
            "I mean, the improvement is pretty striking.",
            "I think that's what you're pointing out for this domain, so that was a fairly surprising, but it's expensive.",
            "I've got some work where you can try to get rid of this time by using machine learning.",
            "We're not going to talk about that here.",
            "So.",
            "As to why, but then it's a, it's an hour per.",
            "You have to relearn the policy for for each time, so there's not really learning the where the hours coming from is."
        ],
        [
            "We're doing multiple stages of rollout, and so if we do two stages, the complexity the number of simulator calls goes to KHW squared.",
            "If we do five stages, this two turns into a five.",
            "That's sort of the problem here, so that's why you can't do too many stages, yes?",
            "They actually use WS specified by their own.",
            "Oh no, you never actually use the theoretical constants."
        ],
        [
            "Never, ever."
        ],
        [
            "So.",
            "Just pick it, pick it empirically so.",
            "Yeah, I don't know if they talked about that paper is pretty my memory.",
            "I usually pick 10 I don't.",
            "I don't know why it's not too much but.",
            "So, but you're yes.",
            "How much tweaking in playing with parameters?",
            "So this result, really there's only the one parameter W and then basically your time complexity is going to scale quite dramatically with W if you have a lot of stages, so there's not a lot of tweaking, you just pick W. Start with 10 an.",
            "Increase it if you don't like that.",
            "Horizon so.",
            "And these games are horizon is sort of naturally defined, but usually if a game could go on forever.",
            "You could just have a horizon cut off and.",
            "Well, so that's going to require domain knowledge, so you sort of look at the domain and you say, well, roughly.",
            "I mean, you can run your policy for example, and see roughly how long does it typically take to terminate and then choose a value like that.",
            "So in the game of go, it's going to be larger than in some other domain.",
            "Alright, yes.",
            "In some sense, it seems like the things you're comparing in the in the results."
        ],
        [
            "The baseline and human.",
            "Seem to be.",
            "General.",
            "Take much more time because they came out this particular game.",
            "Yeah, they're doing search.",
            "You can view this as a search process to some degree is so weird type of trying to visualize the Spanish state space for these multi stage rollouts is pretty hard I find but but I usually just think of it is it's doing multiple iterations of policy iteration.",
            "That's how I try to understand it.",
            "But yeah, you're adding search just like if you have a basic heuristic and it gets a baseline performance.",
            "You could do Mini Mac search or just a heuristic search process.",
            "When we talk about reinforcement learning before in the connection.",
            "And.",
            "Change it.",
            "So that.",
            "The updates.",
            "Work on.",
            "Would generalize to other games Solitaire.",
            "Here your training costs right across games.",
            "Is there a way to is there working right so?",
            "Yeah, so sort of alluding to you can use machine learning to sort of get rid of this cost and try to learn sort of a general version of the rollout four policy or rollout 100 policy as a technique called.",
            "Well there are different variants but approximate policy iteration.",
            "I've got a paper on that basically that tries to do this, so if you search for if you go to my web page and look for a paper with approximate policy iteration in the title, you'll see that.",
            "Some also trying to get a connection policy iteration and it is clear that policy iteration goes through all states.",
            "This is not going through all states, so that's good.",
            "But policy iteration or validation remember the value and so if you reach that state again over any other case, you don't need to recompute it.",
            "But we're not doing that kind of learning in policies alright?",
            "Add that learning element in policies allowed so that you don't rule out things if you know the answer already.",
            "Yeah, so so that the API approximate policy iteration, sort of.",
            "Will first learn this and then it will use version that generalizes to generate this in a cheap way.",
            "I mean, you could.",
            "I suppose you can remember whatever you want, so you could define a new algorithm that tries to remember states you've been too, but the basic basic idea here is instead of, there's no generalization in the basic rollout, it's just instead of doing policy iteration across all states, you're just doing it for one state and it takes you time to do that.",
            "Instead of memory.",
            "Alright, so."
        ],
        [
            "So basically you're trading off time memory for time, alright, so sparse sampling.",
            "Is an approach that."
        ],
        [
            "It's really here for historical reasons.",
            "I'm covering it slightly differently in terms of bandits, but rollout doesn't guarantee near optimal near optimality guarantees.",
            "Approximate policy improvement, right?",
            "And if you don't have a policy, maybe you'd like to just have a Monte Carlo algorithm that can output in approximately optimal action, right?",
            "So so we just have the simulator.",
            "How can we compute an approximately optimal action?",
            "So so the key thing though is we would like a Monte Carlo algorithm that we could probably say gives us an approximately optimal action, but we want the guarantee to be we want the number of simulator calls to be independent of the number of states.",
            "So this is sort of a theoretical question.",
            "Can you get a Monte Carlo algorithm that's independent of the number of states but computes in approximately optimal action?",
            "And yes, 1999 there was an affirmative answer that yes you can.",
            "It's the sparse sampling algorithm.",
            "By Kerns and forget a few other authors.",
            "So.",
            "So now let's just look at this algorithm that has this."
        ],
        [
            "Theoretical guarantee is going to be the most practical algorithm.",
            "So we need a few MDP basics again, so there is an optimal policy and let's let V star represent its optimal value function right and Q star is going to be the optimal Q function for that policy.",
            "Now.",
            "This is a way that you can define Q star.",
            "It's basically the immediate reward that you get for taking a an S plus the reward you expect to get in the next state.",
            "So let's TSA after taking the action right, so it's the immediate reward.",
            "There should be a discount factor here.",
            "The immediate reward plus what you expect to get in the future in a horizon of H -- 1 and the expectation of this is the Q value, so basic relationship.",
            "Now the optimal policy turns out to just be.",
            "The policy maximizes this Q value.",
            "Alright, so let's first consider what we would do if we happen to know V star.",
            "We don't know vstar.",
            "That's sort of the problem, but if we did know it, how could we apply the bandit algorithms?",
            "Alright, so to approximately optimize this thing?",
            "So.",
            "If we know vstar, we're going to define a new simulation function, right?",
            "This is something that we write in C. And what's MQ is going to do is simply sample a next state right?",
            "So it samples the next state, S prime is going to sample a reward and it's going to return R + V star, which we assume we know of that next state, right?",
            "So this is sort of a sample of.",
            "What was inside this expectation?",
            "We're just implementing this stochastic function because we are simulated for our in a simulator for T and we know V star."
        ],
        [
            "So let's all SIM Q star is going to do if we know the star and the expected value of SIM Q star is actually Q star.",
            "Just by definition.",
            "I just showed you on the previous slide so we could use our uniform bandit to select an approximately optimal action right to get a pack result, but we don't know the star.",
            "So how do we deal with that?"
        ],
        [
            "Alright, so we don't know vstar.",
            "And we notice that to compute SIM Q star.",
            "For Horizon H we needed the value of V star for all states at Horizon H -- 1.",
            "So this sort of suggests that maybe you could use a recursive definition to solve our problem.",
            "And the basic idea that we need is known as the Bellman equation.",
            "So vstar SH minus one.",
            "Is equal to the maximum Q value of the H -- 1 Q function, right?",
            "So to compute Q star we need V star at horizon H -- 1.",
            "We can get that if we know the Q star function at H -- 1 and you can sort of.",
            "Imagine now that we can apply these bandits recursively.",
            "So we could implement SIM Q star by recursively calling H -- 1 horizon bandits.",
            "In the base case, is going to be V star at Horizon Zero is equal to 0, so it's just."
        ],
        [
            "See what this looks like.",
            "So.",
            "Our uniform bandit algorithm is going to try to generate these samples, right?",
            "And we don't know vstar, so we can't do the simple SIM Q star that I just showed you.",
            "There should be some Q star.",
            "So simple Q star is going to have to recursively call a whole tree of bandits.",
            "So let's say we want to we call this arm once and we want to compute this.",
            "What's it going to do?",
            "Is trying to sample some Q star is first going to sample a state?",
            "Remember that that's what we said.",
            "SIM Q star.",
            "Does it samples a state samples or reward and then it has to have that vstar part samples of state samples or reward?",
            "And now we've got a bandit problem here.",
            "But it's at horizon H -- 1 and the maximum of these guys are going to return their Q value and that will be an estimate of V. Alright, so.",
            "So that's sort of the key idea behind sparse sampling.",
            "So you see that basically there's a whole tree of these bandits, right?",
            "That's yes.",
            "Similar.",
            "Sort of sort of a different animal.",
            "Our GDP is more like you generate trajectories through the environment.",
            "This is really more like a standard mini Mac search or expecting Max search, so it's generated.",
            "This is going to be generating a search tree because after we're done here, we're going to generate this this and it's not until we know all of these values.",
            "All of the values of these subtrees that we can compute the value for this action, so it's more like a search tree type approach.",
            "So.",
            "So then we'll do the same thing for Q12."
        ],
        [
            "Alright, so the real and I've got pseudocode here.",
            "If you actually want to see this."
        ],
        [
            "Code of this process.",
            "So.",
            "We can view this as a tree with root S and each state S will generate kW new states, right?",
            "So we have K of these arms and they're each going to each want to generate W samples.",
            "So kW each sample is going to generate a new state, so you have kW new states down here and.",
            "There's W states for K bandits, and so for horizon of H we're going to have K times WH states in this tree, and that's sort of the number of simulator calls that you're going to need so.",
            "It might be disappointing that we have an H in the exponential here.",
            "It turns it, but notice well, we're not quite there yet.",
            "It turns out that you can't get rid of this.",
            "There's actually a lower bound that matches it, so now now we've almost got an algorithm for approximately.",
            "Optimally solving MDP at a state.",
            "If we can specify W needs to be to guarantee if W can be independent of the number of states we've got a result.",
            "Now we can't pick W the way that we just did earlier.",
            "For the uniform Bandit, because notice that you know these values here are only approximate values of the star, because there's error that propagates up the tree.",
            "And so the real sort of contribution theoretically of that paper is to show that doing a careful analysis of this recursion and showing that these errors don't propagate too badly, and in fact you can pick to pick a larger W than what we did before, but you can pick W in a way that does not depend on the number of states, so."
        ],
        [
            "That is the basic theoretical result in the current paper.",
            "This is the Journal version.",
            "It really occurred in 1999, which is surprising.",
            "Basic nature of that problem.",
            "So the good news is that we can achieve near optimality for a value of W that's independent of the state space size.",
            "And this was the first sort of near optimal MDP algorithm that was independent of the state space size, so that was sort of a landmark result.",
            "The bad news is the theoretical values are typically quite large.",
            "That's always the case, and we're exponential in age.",
            "They show a lower bound in that paper that shows you can't.",
            "It's a tight lower bound.",
            "You can't get rid of H from the exponent.",
            "So we sort of tradeoff dependence on the number of states to dependence to the horizon.",
            "More or less that, so that's what you end up with.",
            "In practice.",
            "If you have a heuristic function, use a small value of H and you'll get some improvements.",
            "The structure of the state space, does it say MDP?",
            "No, it just assumes it's an MDP, so the Markov property is all it really assumes it can be cyclic doesn't doesn't matter.",
            "Alright, so I think they also assume bounded rewards.",
            "You could probably get rid of that if you want."
        ],
        [
            "Alright, so that's sort of a theoretical result that that is sort of there for historical interest.",
            "It's not the most practical algorithm.",
            "Obviously you have to build a big tree.",
            "And it doesn't actually seem so good from an intuitive standpoint.",
            "If we think about what it's doing is generating a huge amount of work for this action here, and what if we have two actions that sort of one is a lot better than the other one?",
            "It feels like a waste to spend the same effort generating this tree.",
            "Is this tree.",
            "It would be nice somehow.",
            "We could adaptively control the amount of work we put into figuring out the values for these actions, so that actions that are clearly suboptimal early on.",
            "We don't worry about.",
            "We put our resources into disambiguating closer to optimal actions, right?",
            "So for that case, for that reason I'm referring to sparse sampling is sort of a uniform Monte Carlo approach.",
            "It's not very smart about where it spends its effort.",
            "It's just let's just do this thing and return the action.",
            "Alright."
        ],
        [
            "So.",
            "Now we're going to get into adaptive Monte Carlo, and we're going to start out by going to the single state case again.",
            "So we end up with a."
        ],
        [
            "Di Carlo with a multi armed bandit problem and I'm going to change the objective and this is actually the original objective of the Multi armed bandit problem from the 1960s.",
            "This problem in experimental design that's sort of where it came out originate iddc.",
            "So we're going to call this the regret minimization objective for bandits.",
            "Or loss minimisation.",
            "It's been called that as well, So what we want to do is find the arm pulling strategy such that the expected total reward at time N is close to the best possible.",
            "So if we went to Vegas and we had these slot machines, and some of them were biased positively and somewhere biased negatively.",
            "If we knew that best arm, we go there and pull it and that's the best we could do on average in the limit.",
            "Now we don't know the biases of these arms and so the question is we want to pull arms 'cause we want to make money.",
            "But after North Poles we'd like to somehow ensure that were close to the optimal.",
            "If we had known the best arm was.",
            "So if you don't know what the best arm is, how close can we do after a particular number of arm pulls too?",
            "If we had known the optimal arm, and so this is sort of a problem of exploration and exploitation.",
            "'cause you need to explore arms at some rate in order to find ones that are promising.",
            "But you also need to exploit arms that already look promising in order to make sure your profits don't go too low.",
            "And one thing that's pretty clear is that the Uniform Bandit is a really bad choice for if you're in Vegas and you have a slot bunch of slot machines in front of U, uniform bandit will start at the first one.",
            "Pull W Times second one W times, and so on and so forth.",
            "But really, you would probably.",
            "Sort of, walk around and pull different arms and keep statistics of these arms and pull the one that looks most promising.",
            "The question here is other algorithms that can get some theoretical bounds on how close to optimal you're going to be without knowing."
        ],
        [
            "It's optimal.",
            "So this is a paper from 2002, sort of a pretty well cited paper.",
            "Now on that analyze this regret minimization problem for the multi Arm Bandit.",
            "And I'm just going to tell you the algorithm is extremely simple, so at every moment in time we're going to have some statistics we've collected.",
            "So we've pulled a bunch of arms.",
            "QA is going to be the average payoff that we've observed for our May alright, so we pulled our may some number of times.",
            "QA is the payoff that we've observed on average.",
            "NA is a number of times that we've pulled our May right so we can store these things easily.",
            "And now the question is, what arm do I pull next?",
            "You see before upper confidence bound is going to pull this arm a star that maximizes this quantity.",
            "Alright, so talk about this quantity in a moment that this assumes that the pay offs are in 01.",
            "If they are in 0 to B, have to be squared there, but this is the quantity that it's going to maximize and it's going to pull that arm.",
            "Now the basic theorem is that the expected regret, so the difference between the sum of rewards that we would get if we pulled the over the optimal arm and the sum of rewards we get according to the strategy.",
            "That's the expected regret.",
            "The difference in the sums on expectation.",
            "Is going to be bounded by log N, the number of arm poles, and this is a pretty pretty good bound, and in fact there's actually a lower bound that matches this, so it says that the sum of arm poles that I get with my strategy compared to some of our poles.",
            "If I had always pulled the optimal arm, is going to be bounded by this at a particular formally overtime, it's not an asymptotically result, and so if you think about what is the average.",
            "Regret, right?",
            "You're going to divide this by N. And so on.",
            "Average your regret.",
            "Your payoff is going to be log divided by N away from optimal, so that's pretty good.",
            "That goes to zero quite quickly.",
            "So that's a it's a pretty famous result.",
            "This problem was defined in the 60s and it's just 2002 that this result."
        ],
        [
            "Came around.",
            "There is scoring rules that were defined for at least the classical version, so those results.",
            "So the original scoring rules required quite a bit of computation the the indices this requires practically no computation.",
            "It has simple statistics.",
            "They had some later work so they could prove bounds of logged in for those, but they were asymptotically.",
            "As Angus goes to Infinity.",
            "Then they had some.",
            "Easier to compute versions and also the bounds were asymptotic.",
            "This was the first bound that was not this.",
            "Actually every single end it satisfies the property.",
            "They are also optimal.",
            "I don't know about that.",
            "I think they yeah I would have to think about that so forget about this or some animation problems.",
            "So let's look at this this.",
            "Rule this is our algorithm right?",
            "Our decision rule.",
            "So what what it is is a value term QA and so arms that have high value or we're going to be more inclined to pull because the sum will be larger arms that have looked really bad in the past will be less inclined.",
            "But remember, we always have to make sure we explore at some rate because maybe you get unlucky with the optimal arm in the beginning and you don't want to avoid it forever.",
            "So we have this exploration term.",
            "The and what this is going to do is so notice that log N is in the numerator of this term.",
            "So remember N is the total number of poles, so as the total number of poles increases.",
            "And let's say we don't pull this arm, so Na stays the same.",
            "This term is going to keep getting larger and larger and larger, and eventually you're going to pull that arm again, right?",
            "Now when you pull it again, this is going to increase by one and it doesn't take too many armholes for this to sort of dominate the log in.",
            "And if you pull an arm a lot, this basically goes to 0, so so arms that are pulled a lot or basically dominated by this term arms that are pulled a little or dominated by this term, and he only pulled them a little bit.",
            "Now.",
            "Sort of.",
            "The fundamental result of that paper following.",
            "So the expected number of times that we're going to pull a suboptimal arm so there.",
            "Now I'm going to define OK, so the expected number of times that we pull in a suboptimal arm after end total poles is going to be bounded by this.",
            "So notice 1st that it's login.",
            "So if an arm is suboptimal asymptotically and actually it every moment is dominated by login, so we're not going to pull it very much because login is a lot smaller than.",
            "Anne.",
            "What is this?",
            "This is the difference between the expected value of the optimal arm and arm.",
            "A right.",
            "And notice if R May is really bad compared to the optimal arm.",
            "This term is going to be very small, right?",
            "Whereas if our May is very close to optimal, this constant will be large, but in some sense that's OK because it's close to optimal.",
            "We're happy to pull it more than in ARM.",
            "That is really bad, and so this is the fundamental result.",
            "And basically if you multiply this quantity by this difference, you get the expected regret at time and so.",
            "So you just get one of these in the denominator.",
            "Alright, so.",
            "So that's so.",
            "It doesn't waste time on bad arms.",
            "Sort of a fundamental result from the theory of multi arm."
        ],
        [
            "Bandits.",
            "So now how can we apply this result?",
            "We have about 20 minutes I'll.",
            "So one way would be to show all the algorithms I described before were described in terms of the Uniform bandit, right?",
            "So you can plug in?",
            "And what does the uniform bandit do?",
            "It allocates samples per action, right?",
            "So for rollout we allocated WC fraction instead.",
            "We can allocate samples at states.",
            "So W samples per state and the multi arm bandit algorithm I just showed you will figure out how to allocate the samples to the actions that sort of the key difference here and so we could develop a UCB based role policy rollout version.",
            "I'll give you a citation for somebody who's tried that.",
            "Annual typically do as well as the uniform, but with fewer total samples.",
            "Similarly, a sparse sampling you could do that.",
            "You could use a. UCB Bandit algorithm instead of the uniform bandit.",
            "And."
        ],
        [
            "Yeah, I do have a picture of that.",
            "I think I'm going to skip it for the interest of time.",
            "I'll just show you what it does.",
            "So the.",
            "So this was developed in five, and it's an improvement on basic sparse sampling, but but it's not really a very practical improvement.",
            "So if we think about what sparse sampling will do with this, root is going to allocate W. Subtrees to each of these guys right and just blindly go down and generate all those subtrees.",
            "What the UCB based version is going to do is, well, it's going to 1st generate this subtree.",
            "One of these subtrees an of course it will then generate it's going to generate one subtree for every arm, because with zero polls, the UCB algorithm goes to Infinity and you'll pull that arm.",
            "But then based on the results that we observed previously, it's going to maybe this one looked really good.",
            "Then it's going to generate over here instead of uniformly.",
            "If this looks really bad, first thing is going to avoid pulling this for a little while, so it's going to focus on generating these subtrees.",
            "On places that look most promising, with a little bit of exploration.",
            "Still, this is not a very practical algorithm, so if you imagine what's going on.",
            "Just to generate this Q, you're generating a whole subtree, sure, inside the subtree the samples are being allocated a little more efficiently, but still there's a huge amount of computation here, and you don't.",
            "You can't really do anything until that's done, so it has horrible anytime performance, better anytime.",
            "Performance in sparse sampling.",
            "But until you've generated a whole bunch of subtrees, you can't really make any decision at the root, so I would say this is an improvement on sparse sampling, but.",
            "Not really in a practical sense."
        ],
        [
            "So this is where I don't know if this is where the idea of UCT originated.",
            "But it came after that 2005 paper.",
            "UCT is going to be.",
            "It's intended to be a practical way of using UCB to generate these trees, and it's going to have much better anytime performance, so we want to be able to stop at anytime and have something nontrivial to return.",
            "That's sort of the idea, whereas with sparse sampling, both variants.",
            "You couldn't stop at anytime because you have to wait until some number of these subtrees have been generated to get any information.",
            "So anytime performances."
        ],
        [
            "Really, what you see T is aimed at compared to sparse sampling.",
            "So sparse sampling is an instance of what's being called Monte Carlo Game Tree Search.",
            "Short research spend used a lot in go in the past with not a whole lot of success until you see T came around, but you see, by the way stands for upper Confidence Tree as opposed to UCB, which is upper confidence bound.",
            "So sort of ECT is basic.",
            "Basic publicity.",
            "The most publicized fact is that it's a huge advance in computer go when you apply it to go, which is kind of surprising, but but it sort of revolutionized that area and I'll talk about that a little bit later.",
            "Has some nice theoretical properties as well.",
            "Basically all the theoretical properties that sparse sampling has.",
            "Um?",
            "But it better anytime performance.",
            "So what is Monte Carlo tree search?",
            "We first have to describe that and then describing UCT is really simple.",
            "Alright, so Monte Carlo tree search, what we're going to do is.",
            "We're going to do repeated Monte Carlo simulations of a rollout policy an the choice of this rollout policy is the critical thing.",
            "Each rollout is going to add one or more nodes to our tree, so building a search tree and each rollout is going to start at the root and add one or more nodes to the tree, and I'm just going to show."
        ],
        [
            "Are you a picture of this?",
            "So remember, we're in a setting.",
            "Where are robots in the state of the world and the robot is going to think for awhile about this state, and this is the state that we're in an now.",
            "We're going to talk about the thinking process when we do Monte Carlo Tree search.",
            "So initially the tree that we're building is an initial leaf node.",
            "Our current state and when we don't have any information when we're at a leaf node in our tree, all we do is we're going to follow something called the rollout policy.",
            "This could be random.",
            "It could be something that's smarter and after we follow this, let's say we end up with a reward of 1.",
            "Let's just assume there's just terminal rewards.",
            "We are going to update sort of the rewards that have been observed at all of these nodes.",
            "And now we're going to."
        ],
        [
            "Do another simulation so we built a partial tree and the critical thing about Monte Carlo Tree search is that the simulation policy is going to be sensitive to the tree that you've built so far.",
            "Now, so our next simulation is going to start at the root node.",
            "Remember, this is all done in the robots head.",
            "And there's always you have to try every action once, so we haven't tried one of the actions here.",
            "Say there's two actions, so we're just going to again simulate.",
            "The other action, and do our rollout policy is whenever we're at a leaf node, we don't have any information, we just do our default rollout policy."
        ],
        [
            "And we maybe we get a zero reward there.",
            "So now and notice that the root we sort of have a value of 1/2 because half of the times we got."
        ],
        [
            "Got a reward of 1.",
            "Now we actually have a non trivial tree.",
            "I should say that most implementations won't bother adding all these nodes, they'll just add the first node.",
            "That sort of was off the tree that you previously had, 'cause you generate a lot of garbage space.",
            "If you do this, but it is sort of a detail.",
            "Theoretically it doesn't matter.",
            "So now we have a tree and when we're at a node that is not a leaf node, we're going to follow the tree policy.",
            "And again, I haven't defined what this is, but it's gonna be based on UCB.",
            "I'll let the cat out of the bag.",
            "So.",
            "So what Monte Carlo Tree search will do is it will look at the statistics it's collected and have some rule for picking whether to go here or to go here.",
            "That's what the tree policy does.",
            "The tree policy has to sort of decide which place should I explore.",
            "It needs to exploit.",
            "Places that look good but also explore places that it hasn't looked at much.",
            "Is the value in the notice the average?",
            "Let's just say it's the average.",
            "There are different things.",
            "Right, so so it works out theoretically, but let's just say for now that.",
            "You just store the average of what you've seen.",
            "You can do different types of backups if you want, but UCT does this.",
            "So.",
            "Basically, the tree policy might say, well, I guess I'll go down here because it's looked more promising and it selects this node.",
            "And now we're at a state where we."
        ],
        [
            "Selected all the actions so it's going to just generate this and run the default policy."
        ],
        [
            "Right?",
            "Now our trees bigger.",
            "We update our values and perhaps now we're at the root and maybe it would select this again and now maybe based on what it's seen.",
            "Maybe it would explore this path so there's always two distinct phases.",
            "The tree policy that will sort of find the first leaf and then the default policy that once you find the run to the end and the values we store just the averages and so the critical question is what is the appropriate rollout policy and what is an appropriate tree policy?",
            "And UCT is a particular choice of those two."
        ],
        [
            "Things so the basic UCT algorithm is basically just going to use a random rollout policy.",
            "That's theoretically what it uses.",
            "You typically use something that's informed.",
            "The tree policy is based on UCB, so remember every state we encounter we can keep counts of how many times we've been there.",
            "That is what we're going to store here, so it's status.",
            "We've been there NS times.",
            "We can also keep counts on how many times we've tried each action.",
            "That's NSA and we can keep counts on the average reward we've seen when we've gone through that state that skew essay.",
            "Alright.",
            "And the UCT tree policy is simply the B form, except theoretically you have to have a constant here.",
            "Because theoretically these these these pay offs you get when you pull the arms are not independent from one another, and they're also not stationary.",
            "So theoretically the main contribution is to show that the same form works as long as you have a constant.",
            "The theoretical constant is the horizon.",
            "Typically this is something you tune for your particular application domain.",
            "Just try a bunch of values as a best value for go.",
            "For example, alright, so let's."
        ],
        [
            "UCT, so just to show you one more view of this.",
            "So suppose that this is our current tree.",
            "We've already generated it.",
            "ECT now wants to do another policy.",
            "It starts at the root.",
            "It has all the statistics.",
            "You have a bandit problem here, right?",
            "To choose this armor this arm and.",
            "Basically use this rule and it will choose."
        ],
        [
            "Of the arms.",
            "And similarly you have a bandit problem there, and it will choose one of the arms and then it will just do a random roll out from there, yes?",
            "Theoretical guarantees applied for any constant you choose or for the correct one.",
            "The theoretical result is for the constant is the horizon time, so yeah, but in practice you don't use that 'cause it ends up exploring too much basically.",
            "So the I don't actually have a theorem of the UCT result, but basically it's in terms of yes.",
            "Instead of the value.",
            "Yeah so.",
            "So you might think that maybe you should do sort of a store.",
            "The maximum of the Q values of these.",
            "You might be able to improve doing that.",
            "People play with this stuff.",
            "Theoretically what happens is remember what UCB does is if an arm is suboptimal, it's only going to be pulled sort of log N times, and so the average sort of is dominated by the best arm that's actually the key insight behind these algorithms.",
            "You don't have to do a Max, you can do an average as long as you don't pull the bad arms too often, right?",
            "So that's the that's actually the key insight.",
            "Behind the 2005 paper in fact, and then this sort of built on that.",
            "Alright, so theoretically the results are in terms of number of trajectories and basically as you there's a polynomial relation between the number of trajectories and how close you're going to be to the optimal value here, and it's independent of the state space size.",
            "You know the balance to sparse sampling.",
            "There's sort of equivalent.",
            "You can also bound the probability of selecting the optimal action, so in terms of the number of trajectories, and that as a polynomial bound in 1 / T, where T is the number of trajectories.",
            "So as you pull the arm more and more, the probability you select a suboptimal arm goes down fairly quickly with the number of trajectories you generate.",
            "But but notice the key thing here is you can stop pretty much at anytime.",
            "It doesn't take much time to generate a trajectory, so you generate 10 thousand 100,000 trajectories.",
            "You can stop and you'll have something nontrivial to work with at the root.",
            "And it just so happens that it's an entree."
        ],
        [
            "Well enough so you get state of the art performance and go so this is sort of a recap."
        ],
        [
            "I'm not going to go through it here.",
            "Computer go, here's just some quotes that indicate how hard people think it is.",
            "It is a hard problem.",
            "We're not even close to master level performance yet, even with UCT, but it's a I think it has.",
            "Chinese origins is played a lot in Asia.",
            "So 19 by 19 board or 9 by 9 board and you're basically trying to surround your opponents and with these black and white stones.",
            "So many maxed research is very bad.",
            "Nobody's ever got that to work very well, primarily, I guess because we don't have good heuristics and also the branching factor is a lot larger than chess.",
            "For example, you know the branching factor is basically all the places you could put a stone, and that's enormous, and the heuristics.",
            "I mean, it's a very visual game.",
            "We don't really know how to compute good heuristics for go at least."
        ],
        [
            "So I'll just put this up.",
            "Animations screwed up, but 2005.",
            "I mean basically.",
            "Goes impossible.",
            "There is a go conference, but I mean for years and years just minor bits of progress about little tiny aspects of go.",
            "2006 UCT was invented in it was applied to go.",
            "And it actually works surprisingly well in nine by nine, though I think they were beating some of the better programs at that point.",
            "So now you know they start playing with this basic UCT algorithm.",
            "You add bells and whistles to it.",
            "You don't do a random rollout policy, you do something that's based on a small amount of go knowledge and 2007.",
            "You're basically at master level performance and 9 by 9.",
            "An 2008 human grandmaster level some.",
            "And if you look at sort of 2005 to 2008, this is an ELO rating is sort of the power of a computer program on this ghost server.",
            "It has a huge jump.",
            "This is after years and years of work on go programs.",
            "Yoga new go is been around for a long long time and it spend one of the better programs, but I think I think a lot of those guys were a bit annoyed that this stupid algorithm.",
            "It's not really thinking.",
            "Very much can beat the pants off them, but you know the it's not clear how far it will go, whether it will."
        ],
        [
            "Actually, go to the full 19 by 19 board.",
            "So some other successes.",
            "Yeah, last year we were interested in Klondike Solitaire.",
            "There were no sort of theoretical bounds on.",
            "How many what the deck could be?",
            "One?",
            "What fraction of the decks could be one in Klondike Solitaire?",
            "And it didn't seem very.",
            "We tried to think about applying planning algorithms model based approaches, but it wasn't very promising.",
            "So we tried a bunch of Monte Carlo techniques and UCT worked surprisingly well.",
            "40% of the games we did have a little bit of knowledge in the roll out policy, but ECT improved on that dramatically.",
            "It's been used in this general game playing competition.",
            "I think it's one of the better approaches now.",
            "Maybe the best.",
            "I haven't followed that too closely.",
            "I've applied it to real time strategy games, combinatorial optimization.",
            "I think maltez a paper at triple AI on this.",
            "And so the list is growing.",
            "It's a pretty interesting algorithm.",
            "But you know these.",
            "These results usually come with some basic extension to UCT.",
            "And so it's useful to know."
        ],
        [
            "What you want to do?",
            "I'm not going to enumerate them here.",
            "But after you insert some sort of domain knowledge and this could be the base policy, that's the obvious place.",
            "There's actually.",
            "An interesting some interesting results with these base policies that you choose.",
            "So you would think that if you have two based policies and one of those based policy's is better than the other, if you just play them head to head, you would think that maybe if you use the better one in UCT, it would do better than the worst one when using UCT.",
            "And it turns out to not be the case.",
            "So people that there's been some interesting results and go where you learn two policies, ones a lot better than the other.",
            "You plug them into UCT and the performance is just reversed and there's not a lot of understanding of this sort of.",
            "I think it's a pretty interesting open issue, especially when it comes to learning.",
            "How do you do the proper learning problem for improving the base policy so that people do this?",
            "They handcraft policy.",
            "That's a big part of the ghost success.",
            "You can also incorporate learned heuristic functions into UCT in various ways.",
            "I'll have references."
        ],
        [
            "On the final slides.",
            "So I'll just summarize, we're actually right on time, amazingly, not by design.",
            "So when you have a tough planning problem in a simulator.",
            "You might as well try Monte Carlo planning.",
            "A lot of these things are not very hard to implement.",
            "In fact, I should.",
            "I put I'm developing right now.",
            "I've got to developing a nice library for Java that has pretty standard interfaces for simulators and worlds, and these Monte Carlo planners will have some implementations of a bunch of domains.",
            "We've got backgammon.",
            "Yeah, yeah yeah, but a bunch of interesting domains connect four and will have implementations of UCT so pretty soon when this guy defends his thesis.",
            "I'll make him put all of this online.",
            "So so send me an email if you don't see it anytime soon.",
            "So.",
            "The basic principles of these algorithms can really be derived from the multi arm bandit.",
            "And I would say that policy rollout gives you some really Big Bang for your Buck.",
            "If you have a basic existing policy.",
            "In most of the cases that I've tried it out, you get you get a significant gain if your base policy isn't close to optimal.",
            "So if good heuristics exist, you can use shallow sparse sampling or the UCB variance and ECT is something that definitely people are just beginning to explore and understand the potential.",
            "Done the tutorial.",
            "If there are any questions, we have a few minutes.",
            "Yes, one common questions.",
            "So looks at our typical can be seen as a Monte Carlo planning, so it's just the entrance and so on, but it's definitely not policy rollout.",
            "There's more invariant of value iteration that of policy iteration, so it's not doing, yeah.",
            "My question is about the discount factor.",
            "What is the meaning or what is?",
            "The justification for the use of discount factors in problems where you really trying to reach a goal.",
            "So I actually went through a number of iterations or whether I should put the discount factor in or not.",
            "Typically don't, so there's a required factors well.",
            "Not really so.",
            "So if you want to be in a finite horizon setting, so if you fix age there are a whole bunch of ways to deal with this.",
            "If you fix H. You're suddenly in a non stationary policy setting.",
            "All of these things sort of hold there.",
            "So UCT was actually designed for the.",
            "Fixed 8 Horizon Total horizon reward and they mentioned well if you have a discount factor you can make each large enough.",
            "You can also make it if you don't want to fix H, you can make an assumption that for all policies they'll eventually terminate, then picking the value for H. That gives you a guarantee.",
            "I don't think it's straightforward, heuristically you just have to pick it, so those are sort of the basic ways that you would deal with that.",
            "Your suggestion that a good initial policy may not result in the good best final policy as a very strong local optimal kind of feel that maybe it's a local policy anyway.",
            "If you increase improve it further, which is a local optimal.",
            "But in another policy measure, better optimal has had people thought of connection of you city with local search at all.",
            "Or maybe taking some of those ideas of random restarts and stuff.",
            "So one thing I wish I had been able to put in here is I've got some work right now.",
            "On ensembles of these UCT trees, so the one problem you can run into is.",
            "If the beginning trajectories are unlucky and you see tree ECT, it's hard to recover quickly.",
            "So you can use random restarts, so if you have a fixed budget of trajectories.",
            "Allocate them to different trees and average the results.",
            "Sort of like ensemble methods in machine learning and in my case I've always found that this does as well for a reasonable number of total trajectories as a single large tree.",
            "Sometimes better, but with much less memory, so you only generate these small trees.",
            "You throw them away, so the only thing I know of, I don't think this issue is really well understood.",
            "It's just been observed.",
            "Anything else?",
            "Alright well thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thanks for coming everybody.",
                    "label": 0
                },
                {
                    "sent": "So really, this is going to be about the very basics of Monte Carlo planning as I see them and we will talk about some of the recent progress is for example the UCT algorithm that will come at the end so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're really waiting for that.",
                    "label": 0
                },
                {
                    "sent": "You can take a break and wake up when I mention UCT again.",
                    "label": 0
                },
                {
                    "sent": "So we've got off just giving a few preliminaries about Markov decision processes, mainly for notation, and then we're going to try to motivate Monte Carlo planning.",
                    "label": 1
                },
                {
                    "sent": "Why is it useful?",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "And then basically I'm going to talk about two classes of Monte Carlo planning techniques as I see it.",
                    "label": 0
                },
                {
                    "sent": "One I'll call uniform Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be very intelligent about how it samples the environment, and then we'll talk about adaptive Monte Carlo techniques at the end, culminating in the UCT algorithm which you might be familiar with because it's been so successful and go and more recently other problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, I'm DPS.",
                    "label": 0
                },
                {
                    "sent": "We're going to model the world is a Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "Will talk about that in a minute, but the basic model is we have this robot or and he can send actions to the world, possibly their stochastic actions and the world gives US states and rewards back.",
                    "label": 0
                },
                {
                    "sent": "Or if you like observations and rewards back and we're interested in what's going on in this guys head, he wants to maximize his reward.",
                    "label": 0
                },
                {
                    "sent": "So we're going to stick.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Monte Carlo planning techniques.",
                    "label": 0
                },
                {
                    "sent": "But in MDP is basically composed of four things.",
                    "label": 0
                },
                {
                    "sent": "The first thing is going to be a set of states.",
                    "label": 0
                },
                {
                    "sent": "These are the states of the world that sort of completely describe how the world is going to function.",
                    "label": 0
                },
                {
                    "sent": "If we take an action, we have a finite set of actions and the thing that makes a Markov decision process have the name Markov in it is because the transition distribution PFT is a first order Markov process.",
                    "label": 0
                },
                {
                    "sent": "What this means is this is a conditional probability distribution that tells you if you take a in state S, What's the probability of ending up in S prime.",
                    "label": 1
                },
                {
                    "sent": "So that's what we mean by a transition distribution, and it's first order because it only depends on SNA right now.",
                    "label": 1
                },
                {
                    "sent": "The reward distribution is also going to be first order Markov, and for any state it tells us if we take action A, what is the distribution over real valued rewards?",
                    "label": 0
                },
                {
                    "sent": "And for the sake of this talk, we're going to be assuming that this has finite.",
                    "label": 0
                },
                {
                    "sent": "The rewards are bounded within some finite range.",
                    "label": 0
                },
                {
                    "sent": "Alright, so those are MD.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He's pretty basic stuff that's just for notation, so if we have an MVP, what is a solution?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not good enough generally to output a straight line plan action.",
                    "label": 0
                },
                {
                    "sent": "One that action 2, then Action 3 because once you take action one might transition to some state that maybe action two wasn't so well suited for.",
                    "label": 0
                },
                {
                    "sent": "So you really need to form a plan that can handle whatever state you end up in.",
                    "label": 0
                },
                {
                    "sent": "At any moment, and we call such a plan a policy and right now we're not going to compare.",
                    "label": 0
                },
                {
                    "sent": "We're not going to care about how we get that policy.",
                    "label": 0
                },
                {
                    "sent": "It could be computed online as you're planning or offline before hand, but a policy is simply a mapping could be stochastic from we're going to use Peiffer policy from states to actions, and basically it tells us what to do.",
                    "label": 1
                },
                {
                    "sent": "Or in state S, and this defines a continuous reactive controller for this agent.",
                    "label": 0
                },
                {
                    "sent": "At every moment he's in a state.",
                    "label": 0
                },
                {
                    "sent": "Takes an action, ends up in a new state, and looks at his policy and figures out what to do next, so that's that's what the policy is, and the main question is, how do we determine whether a policy is good or bad?",
                    "label": 0
                },
                {
                    "sent": "And intuitively a policy is good if on app achieves a high reward when we execute it in the environment.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So specifically in this talk we're going to talk about finite horizon, discounted reward, and it's a mouthful, but we're going to note this function by V. \u03a0 SHH is our horizon.",
                    "label": 0
                },
                {
                    "sent": "That's how long we're going to think ahead.",
                    "label": 0
                },
                {
                    "sent": "That's how far into the future we're going to think ahead, and what that function tells us is what is the expected value we're going to get following this policy starting in S if we follow it for H steps.",
                    "label": 1
                },
                {
                    "sent": "Plus we add in discounting.",
                    "label": 0
                },
                {
                    "sent": "And so let's think about what this means.",
                    "label": 0
                },
                {
                    "sent": "So if you run the policy for each step starting in S, you're going to get a sequence of rewards, right sequence of eight rewards, and this is really a random sequence.",
                    "label": 0
                },
                {
                    "sent": "These are random variables because the dynamics are random.",
                    "label": 0
                },
                {
                    "sent": "The reward function is random, so this is a random sequence, and what we're going to be interested in is the discounted of these sequences, so the expected value of the discounted sum and.",
                    "label": 1
                },
                {
                    "sent": "We discount later rewards more than earlier rewards, so beta T is a discount factor if you set it to one then all of these rewards are equal to our sort of treated equally, and so it allows us to trade off how much we care about the future.",
                    "label": 0
                },
                {
                    "sent": "Usually it's just there for mathematical convenience.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you can motivate it by economic principles or the probability of dying or whatnot.",
                    "label": 1
                },
                {
                    "sent": "But in the end, what we're interested in is an optimal policy, so an optimal policy is one that dominates all other policy's across all states in terms of its value.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to cover this slide in detail.",
                    "label": 0
                },
                {
                    "sent": "I'll just say that.",
                    "label": 0
                },
                {
                    "sent": "If you really like Infinite Horizon problems, so so in many cases you might have a continuous controller and you plan for the infinite horizon.",
                    "label": 1
                },
                {
                    "sent": "They have to introduce a discount factor to make things well defined.",
                    "label": 1
                },
                {
                    "sent": "Really finite horizons results are going to be good enough and the main reason is that we can approximate the infinite horizon value function with a finite horizon an if you choose your horizon long enough the approximation error goes to zero quickly.",
                    "label": 0
                },
                {
                    "sent": "So basically everything that I say here will apply to infinite Horizon problems.",
                    "label": 1
                },
                {
                    "sent": "You can refer to this slide.",
                    "label": 0
                },
                {
                    "sent": "Again, if you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your character care to see that?",
                    "label": 0
                },
                {
                    "sent": "So the basic results in MDP theory is a pretty old results.",
                    "label": 0
                },
                {
                    "sent": "Optimal policies are guaranteed to exist, so there are optimal policy zizza is not a partial order on policies and we have algorithms for computing them, so the algorithms are we can do it in polynomial time using linear programming and you can also use a value iteration or policy iteration which tend to be more efficient.",
                    "label": 1
                },
                {
                    "sent": "But these algorithms are polynomial time in the number of States and actions, and that's not really so good and less you're in a domain where you can really engineer the problem to have a small number of states.",
                    "label": 0
                },
                {
                    "sent": "So it's a nice result, but.",
                    "label": 1
                },
                {
                    "sent": "For this talk, we're going to be interested in the case where the states space is exponentially large in terms of the problem encoding size.",
                    "label": 0
                },
                {
                    "sent": "So the real world has very, very many States and we're going to try to get algorithms that work in the real world.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you can't apply these directly.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what can you do if you have a large world?",
                    "label": 0
                },
                {
                    "sent": "Well, one approach would be what I'll call the model based approach.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work on this, and what you'll do is you'll define a language that can compact scribe very large M DPS.",
                    "label": 0
                },
                {
                    "sent": "Now, there are several ways you can do this, but one way would be to use a dynamic Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "You don't have to know the details of this or probabilistic strips to describe the transition and reward functions right, so these languages can be used to describe MDP's that are exponentially larger than their encoding size in terms of the language.",
                    "label": 0
                },
                {
                    "sent": "Then you have to design A planning algorithm that can deal with that language and a lot of you here know how hard that can be.",
                    "label": 1
                },
                {
                    "sent": "For example, with DBN's or probabilistic strips.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm going to call the model based approach, and there are some problems with this.",
                    "label": 0
                },
                {
                    "sent": "I don't want to kill research in this area because it's very interesting.",
                    "label": 0
                },
                {
                    "sent": "And useful, but one problem is if you take a random application that you care about and you try to encode it in one of these languages, there's always going to be some feature of it.",
                    "label": 0
                },
                {
                    "sent": "Usually an important feature that you can't quite fit in the language, and that there could be a couple of reasons for that.",
                    "label": 0
                },
                {
                    "sent": "It could be because the problem size and the encoding will blow up for some reason.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of that in a moment, or it could be there's some fundamental representational shortcomings of these models, for example.",
                    "label": 0
                },
                {
                    "sent": "You want exogenous events like 911 calls, but these languages can't handle them nicely.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is a problem when you are faced with an application and there are some planners out here that deal with these languages, but you can't use them because your problem isn't described nicely in those languages.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's where Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "the Monte Carlo approach comes in, as opposed to the model based approach.",
                    "label": 0
                },
                {
                    "sent": "So in these cases, when when you can't discuss problem compactly in a language that exists.",
                    "label": 0
                },
                {
                    "sent": "You can often write a simulator for that domain, so you can use.",
                    "label": 0
                },
                {
                    "sent": "Your language could be the C programming language and you could write a simulator that takes an action and a state and spits out a next state in a reward, right?",
                    "label": 0
                },
                {
                    "sent": "So you can you can do this for pretty much anything you can imagine if you spend enough time.",
                    "label": 0
                },
                {
                    "sent": "So a couple of examples if you try to encode the simple well seemingly simple problem of Klondike Solitaire in.",
                    "label": 1
                },
                {
                    "sent": "Probabilistic PDL you'll find at least I can't find a compact way to do it so that you can't use the existing planners and you could model as a palm DP, partially observable MDP, but those planners don't really scale, so when we were interested in trying to solve this problem we were sort of stuck, but it's really easy to write a simulator for Klondike Solitaire, right?",
                    "label": 0
                },
                {
                    "sent": "You have a particular situation.",
                    "label": 0
                },
                {
                    "sent": "You take an action, you just steal a card.",
                    "label": 0
                },
                {
                    "sent": "Same thing with another application I was involved with.",
                    "label": 0
                },
                {
                    "sent": "Fire and emergency response here.",
                    "label": 1
                },
                {
                    "sent": "We're interested in sort of routing multiple vehicles around town, placing them in places to be ready for emergencies in response to 911 calls that emerge.",
                    "label": 0
                },
                {
                    "sent": "And this is a very exogeneous domain.",
                    "label": 0
                },
                {
                    "sent": "There's exogenous events in terms of traffic patterns and the 911 calls that come in hard to model.",
                    "label": 0
                },
                {
                    "sent": "This is in a standard language that exists today, but.",
                    "label": 1
                },
                {
                    "sent": "It's easy to write a simulator.",
                    "label": 0
                },
                {
                    "sent": "This is a picture of our simulator.",
                    "label": 0
                },
                {
                    "sent": "If you have traffic models and whatnot so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's just a couple of motivating examples, So what is Monte Carlo planning?",
                    "label": 0
                },
                {
                    "sent": "And I should say, if you do have questions, you can raise your hand and interrupt me.",
                    "label": 0
                },
                {
                    "sent": "I'll cut you off if it's if I think it should be dealt with later, but feel free to ask questions.",
                    "label": 0
                },
                {
                    "sent": "So what is Monte Carlo planning?",
                    "label": 0
                },
                {
                    "sent": "Well, roughly define it as we want to compute a good policy for an MDP by interacting with an MDP simulator as opposed to taking in a compact representation of a model.",
                    "label": 1
                },
                {
                    "sent": "An planning with that so it's about computing good policy's by interacting with the simulator will formally define what we mean by a simulator in a moment, but now we have our agent here.",
                    "label": 0
                },
                {
                    "sent": "He's gotta World Simulator in his head, and every step he's going to use that simulator play around, figure out what to do.",
                    "label": 0
                },
                {
                    "sent": "Then get some feedback from the world.",
                    "label": 0
                },
                {
                    "sent": "Then he's going to use the simulator to figure out what to do and take another action.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so good question.",
                    "label": 0
                },
                {
                    "sent": "So let me define the simulator and then I'll mention that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So you can look at this on your own.",
                    "label": 0
                },
                {
                    "sent": "Basically there's a whole list of domains where there are simulators and I should mention these slides will be online and in fact if you wait a couple of days are going to be updated with references and whatnot.",
                    "label": 0
                },
                {
                    "sent": "These are very fresh.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From 15 minutes ago, in fact, so let's define what we want from our simulation based representation of an MDP.",
                    "label": 0
                },
                {
                    "sent": "There's still some States and actions, but now we're instead of being given probability distributions and some language we're going to be given.",
                    "label": 0
                },
                {
                    "sent": "RNTR is simply a stochastic function to see.",
                    "label": 1
                },
                {
                    "sent": "Program will say that returns a reward when you pass in a state in an action.",
                    "label": 1
                },
                {
                    "sent": "So just think of it as a function.",
                    "label": 1
                },
                {
                    "sent": "Right and see that returns a random reward according to the distribution of our underlying MDP.",
                    "label": 0
                },
                {
                    "sent": "And the same thing with T. This is a transition function.",
                    "label": 0
                },
                {
                    "sent": "You pass it a state and action and it's going to spit out the next state, right?",
                    "label": 0
                },
                {
                    "sent": "So these are simply stochastic functions that we don't care about their implementation.",
                    "label": 0
                },
                {
                    "sent": "You could use C, Java, whatever language you like.",
                    "label": 0
                },
                {
                    "sent": "You could encode it in a planning language and simulate that if you cared too.",
                    "label": 0
                },
                {
                    "sent": "So that's what we mean when we say a simulation based representation.",
                    "label": 0
                },
                {
                    "sent": "Now coming back to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So again and reinforcement learning, we don't have a compact description of the environment.",
                    "label": 1
                },
                {
                    "sent": "In fact, we start off with no knowledge of the environment whatsoever.",
                    "label": 0
                },
                {
                    "sent": "We're just allowed to take actions in the environment.",
                    "label": 0
                },
                {
                    "sent": "Here we start off with the simulator of the environment in our head, and so the key distinction is.",
                    "label": 0
                },
                {
                    "sent": "That sometimes used is we're using a strong simulator here, and a strong simulator simulator is.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can teleport to any state at any moment and take any action that you want and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We sort of in a week simulator model where you are where you are.",
                    "label": 0
                },
                {
                    "sent": "You cannot teleport to any state that you want.",
                    "label": 0
                },
                {
                    "sent": "If you want to get to a state, you have to figure out how to get there.",
                    "label": 0
                },
                {
                    "sent": "Possibly in some cases you can have a reset that resets you to an initial state, so that's sort of the key distinction.",
                    "label": 0
                },
                {
                    "sent": "A weak simulator is reinforcement learning, where in the strong simulator case, and there are some theoretical relations between these two so.",
                    "label": 0
                },
                {
                    "sent": "So let's move on, so that's our.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relation based MDP representation.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to talk about the basic Monte Carlo algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now I said our motivation is to scale to very large state spaces, but I'm going to start out by talking about how do we use Monte Carlo techniques for a single state MDP, so that's not exciting by itself, but we're going to try to build on that and.",
                    "label": 0
                },
                {
                    "sent": "Create things that can be applied to large MDP's.",
                    "label": 0
                },
                {
                    "sent": "So we're going to start with a single state case.",
                    "label": 1
                },
                {
                    "sent": "This turns out to be identical to what's called a multi armed bandit problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of you might have heard of that so.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a single state MDP, so S is our only state.",
                    "label": 1
                },
                {
                    "sent": "We have these actions.",
                    "label": 0
                },
                {
                    "sent": "We have K actions.",
                    "label": 0
                },
                {
                    "sent": "OK, now what happens when we take one of these actions?",
                    "label": 0
                },
                {
                    "sent": "Well, you get a random reward by calling this reward function.",
                    "label": 1
                },
                {
                    "sent": "And you can view this is pulling the arm of a slot machine and getting some payout according to some distribution.",
                    "label": 0
                },
                {
                    "sent": "And the distribution is given by our reward function, which we don't really know the implementation of.",
                    "label": 0
                },
                {
                    "sent": "But we can sample from this distribution by just calling the function right and so pulling the arms correspond to us using our simulator to sample the reward function for the particular arm.",
                    "label": 0
                },
                {
                    "sent": "Now what is?",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "If you knew these are functions.",
                    "label": 0
                },
                {
                    "sent": "If you knew what they were, what would be the optimal thing to do for this MDP?",
                    "label": 0
                },
                {
                    "sent": "What would be the optimal policy?",
                    "label": 0
                },
                {
                    "sent": "Any guesses?",
                    "label": 0
                },
                {
                    "sent": "We want to figure out which ARM has the highest expectation right?",
                    "label": 1
                },
                {
                    "sent": "And if you were in Vegas and you knew that you would just keep pulling that arm right?",
                    "label": 0
                },
                {
                    "sent": "The law of large numbers?",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so if the reward was reward payoff was positive, you just keep pulling the arm because law of large numbers says if you pull long enough you're going to come out on top in Vegas you just lose.",
                    "label": 0
                },
                {
                    "sent": "So so so let's.",
                    "label": 0
                },
                {
                    "sent": "Let's see how we can formulate this problem of picking the arm with the highest expectation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are different formulations.",
                    "label": 0
                },
                {
                    "sent": "We're going to start out in this first part, talking about the pack bandit objective.",
                    "label": 0
                },
                {
                    "sent": "So we have this single state MDP and I'm defining what our objective is.",
                    "label": 0
                },
                {
                    "sent": "We're going to give a really simple algorithm for solving it.",
                    "label": 0
                },
                {
                    "sent": "So probably approximately correct orpak.",
                    "label": 1
                },
                {
                    "sent": "This is a term from learning theory.",
                    "label": 0
                },
                {
                    "sent": "Basically says that we want to select an arm that probably.",
                    "label": 1
                },
                {
                    "sent": "Has approximately the best expected reward, right?",
                    "label": 1
                },
                {
                    "sent": "We can never really guarantee that we're going to pull the find the arm with the best expected reward in a finite amount of time.",
                    "label": 0
                },
                {
                    "sent": "'cause you can always be unlucky and get bad samples, but we want to have an algorithm that probably such that with high probability every time we run it will approximately return the best reward.",
                    "label": 0
                },
                {
                    "sent": "So an arm that's approximately optimal.",
                    "label": 0
                },
                {
                    "sent": "And the key is we'd like to know how many simulator calls or poles we need to do that, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not really very interesting if you don't care about how many simulator calls you need, because you would just mean you can easily get asymptotic results with the law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "So this is a pack variant of the Multi armed bandit problem.",
                    "label": 0
                },
                {
                    "sent": "Will talk about another.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later today.",
                    "label": 0
                },
                {
                    "sent": "A very simple algorithm can be used to get a pack.",
                    "label": 0
                },
                {
                    "sent": "And it is the most obvious algorithm that you might come up with on your own.",
                    "label": 0
                },
                {
                    "sent": "We're going to say OK for each machine we're going to pull the arm W times each time we pull the arm, we get a reward.",
                    "label": 1
                },
                {
                    "sent": "A sample of the reward function, right?",
                    "label": 0
                },
                {
                    "sent": "So after W poles of this machine, we get W samples of that reward function.",
                    "label": 0
                },
                {
                    "sent": "Same for this machine.",
                    "label": 0
                },
                {
                    "sent": "Same for this machine.",
                    "label": 0
                },
                {
                    "sent": "Naively, what you would do is average those results and return the arm that has the highest average, right?",
                    "label": 0
                },
                {
                    "sent": "That's sort of.",
                    "label": 0
                },
                {
                    "sent": "There's nothing terribly interesting about that algorithm, except we'd like to figure out how large does W need to be?",
                    "label": 0
                },
                {
                    "sent": "Does it need to be exponentially large, and the problem size, or in the number of arms?",
                    "label": 0
                },
                {
                    "sent": "Or can it be smaller?",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the question here that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Reward.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, so.",
                    "label": 0
                },
                {
                    "sent": "So word function also the results that we define are going to be for reward functions that are bounded.",
                    "label": 0
                },
                {
                    "sent": "We don't care about the distribution, so any distribution, as long as it has a bounded extent.",
                    "label": 0
                },
                {
                    "sent": "There are results that work for Gaussian reward functions and whatnot, but these are going to be different.",
                    "label": 0
                },
                {
                    "sent": "Every poll if the reward function is stochastic and the variance in that distribution dictates how different they are going to be right.",
                    "label": 0
                },
                {
                    "sent": "But on average, if you if it gets very very large, it's going to approach the mean and the question is how large this W need to be so that we can pick a good arm with a pet.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guarantee so this is so to figure out what W is.",
                    "label": 0
                },
                {
                    "sent": "It's useful to know this is really the only mathematical thing I'm going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "If you're reading this stuff, you need to know the turnoff bound.",
                    "label": 0
                },
                {
                    "sent": "At the very least.",
                    "label": 0
                },
                {
                    "sent": "Most basic principle and random algorithms and theory.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What this says is that if we have a random variable R and you can think of this as one of the arms are sort of dictates the rewards that we get.",
                    "label": 1
                },
                {
                    "sent": "To sample that random variable IID identically, we're going to sample it W times and get a sequence of rewards, are I?",
                    "label": 0
                },
                {
                    "sent": "Right the turn off bound.",
                    "label": 0
                },
                {
                    "sent": "Is able to bound the probability that the average of these samples is far from the expectation of the samples and in particular.",
                    "label": 1
                },
                {
                    "sent": "Is going to tell us that.",
                    "label": 0
                },
                {
                    "sent": "We know that the bound is exponentially tight as W increases, so let's just look at the result so the turnoff bound basically says so.",
                    "label": 0
                },
                {
                    "sent": "It might be a brain fall to see this all, but this is basically the difference between the expected reward, which, if we knew this would be done and the sample average right?",
                    "label": 0
                },
                {
                    "sent": "So we sampled W rewards and we average them and this is the absolute difference between those two things.",
                    "label": 0
                },
                {
                    "sent": "If this is zero, then we.",
                    "label": 0
                },
                {
                    "sent": "Are done right, the law of large numbers says this goes to zero as W goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "But the turnoff bound is much more powerful.",
                    "label": 0
                },
                {
                    "sent": "It says that the probability that this quantity is greater than epsilon so is going to be less so greater than epsilon means that we don't have a good bound.",
                    "label": 0
                },
                {
                    "sent": "It's going to be less than this quantity here.",
                    "label": 0
                },
                {
                    "sent": "And the key thing is that this decreases exponentially fast with W, right?",
                    "label": 0
                },
                {
                    "sent": "So it says that we don't have to pull the arms very much to bound the probability of the difference between our sample average and the expectation.",
                    "label": 0
                },
                {
                    "sent": "Now, if you use a more basic bounds like the mark of inequality, you'll get a rate here instead of an exponential rate.",
                    "label": 0
                },
                {
                    "sent": "This is just a slightly more.",
                    "label": 0
                },
                {
                    "sent": "Sophisticated analysis.",
                    "label": 0
                },
                {
                    "sent": "This is a turn off bound and it's used very commonly in computer science because of this exponential rate of decrease in the error that you can get right.",
                    "label": 0
                },
                {
                    "sent": "So basically you can write this in a different way.",
                    "label": 1
                },
                {
                    "sent": "And what it says is that with probability, so if you just rearrange terms with probability, at least one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "We can get the following bound.",
                    "label": 0
                },
                {
                    "sent": "The difference between the expected value and the sampled average is going to be bounded by this amount, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is again, you can see that.",
                    "label": 0
                },
                {
                    "sent": "Well, as Delta decreases, we want our probability guarantee to get smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "This is going to go up, but only sort of log rhythmic rate.",
                    "label": 0
                },
                {
                    "sent": "And W increases this T gets tighter and tighter, so there's just two different views of the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a turnoff bound.",
                    "label": 0
                },
                {
                    "sent": "I recommend that you all know the proof of it is not really that hard.",
                    "label": 0
                },
                {
                    "sent": "Look on Wikipedia so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're going to need to know that if you ever read this literature.",
                    "label": 0
                },
                {
                    "sent": "So now back to our problem.",
                    "label": 0
                },
                {
                    "sent": "We have the Uniform bandit algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's called the Naive Bandit in the original paper.",
                    "label": 0
                },
                {
                    "sent": "This is a paper on Pak Pak Learning and the multi Arm bandit setting.",
                    "label": 0
                },
                {
                    "sent": "We have these samples and we wanted to know how large W needs to be.",
                    "label": 0
                },
                {
                    "sent": "It's really easy to use our turn off bound now to figure out how large W needs to be.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just give you the result.",
                    "label": 0
                },
                {
                    "sent": "If you use a bit of algebra and the turnoff bound, you can derive that if W is larger than this quantity.",
                    "label": 1
                },
                {
                    "sent": "So notice that K appears in the logarithm here for all arms simultaneously will have this bound, right?",
                    "label": 1
                },
                {
                    "sent": "So if you pick W to be that quantity, at least we're going to be within epsilon for all arms with probability at least one minus Delta.",
                    "label": 1
                },
                {
                    "sent": "So the estimates of all the actions are going to be epsilon accurate.",
                    "label": 0
                },
                {
                    "sent": "With high probability and that means that we can select the highest the arm that has the highest average and be pretty sure we've got approximately an optimal arm, right?",
                    "label": 0
                },
                {
                    "sent": "So technically it will be be within two epsilon of optimal.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now what is the sample complexity of this though?",
                    "label": 0
                },
                {
                    "sent": "So the total number of simulator calls, we're going to need is just K, so the number each the number of arms times W which is K. This quantity right here.",
                    "label": 0
                },
                {
                    "sent": "So it's roughly scales.",
                    "label": 0
                },
                {
                    "sent": "Is K log K. Now more sophisticated algorithms?",
                    "label": 0
                },
                {
                    "sent": "This show you here.",
                    "label": 0
                },
                {
                    "sent": "You can use an algorithm called median elimination to get rid of this.",
                    "label": 1
                },
                {
                    "sent": "Part right here if you want, and that's actually there's a lower bound for that.",
                    "label": 0
                },
                {
                    "sent": "You can't do any better than this, right here?",
                    "label": 0
                },
                {
                    "sent": "There's some other garbage as well, but fundamentally it's sort of linear in K. Alright, so it's kind of a fundamental result on PAC learning for multi arm bandit problems.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so that was a single state case, not very exciting since our goal was to go to exponentially large state spaces.",
                    "label": 1
                },
                {
                    "sent": "So how can we apply that basic idea to the exponentially large state spaces?",
                    "label": 0
                },
                {
                    "sent": "Well, first I'm going to before we talk about, sort of.",
                    "label": 0
                },
                {
                    "sent": "Trying to find an optimal policy, we're going to take an intermediate step which is very useful in practice and I'm going to call this policy rollout spend going to France is in the final version of this that will be online at icaps and on my web page, so you'll have some nice references to look at.",
                    "label": 0
                },
                {
                    "sent": "It was developed by birthday, Coussan Tessaro, sort of simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So policy, rollo.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "So consider a multi state MDP.",
                    "label": 1
                },
                {
                    "sent": "Now we want to act in it and suppose we have a simulator.",
                    "label": 1
                },
                {
                    "sent": "That's what we've been assuming and now also suppose that we have some non optimal policy.",
                    "label": 0
                },
                {
                    "sent": "If you're in a computer networking domain you can code up some non optimal allocation policy and see if you're interested in Klondike Solitaire you can code up the best policy you can and see it won't be very good most likely, but you can code it up.",
                    "label": 0
                },
                {
                    "sent": "And it might get nontrivial performance.",
                    "label": 0
                },
                {
                    "sent": "You will get performance greater than 0.",
                    "label": 0
                },
                {
                    "sent": "So given these two things were going to give our agent these things, and what we'd like the agent to do is be able to improve on that policy right in the more computation time we give it, we would like the agent to improve more and more.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're giving it a base policy, and it's sort of like a hint, and we want it to improve on that based policy.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that often this algorithm will give very big improvements with a little bit of work on the space policy, you can get big performance gains.",
                    "label": 0
                },
                {
                    "sent": "Much bigger than if you spend three more months trying to improve this space policy often so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Policy rollout, that idea is called policy rollout, so to talk about policy rollout, we need to introduce the policy improvement theorem from MDP literature.",
                    "label": 0
                },
                {
                    "sent": "This is one of the most basic results.",
                    "label": 0
                },
                {
                    "sent": "We need to define something called the Q function for this, so Q \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So every policy Q function that SCOOP I the Q function is a function of state and action and the horizon and all it is is simply the expected reward we get when we start in state S and we take action A.",
                    "label": 0
                },
                {
                    "sent": "So we start in State S, take action A and then follow the policy for H -- 1 steps right?",
                    "label": 1
                },
                {
                    "sent": "So it's a very simple.",
                    "label": 0
                },
                {
                    "sent": "Function to define.",
                    "label": 0
                },
                {
                    "sent": "Garden State a certain status.",
                    "label": 0
                },
                {
                    "sent": "Take action A then follow the policy for each minus one steps that gives you a sequence of rewards.",
                    "label": 0
                },
                {
                    "sent": "The expected some of those rewards is the Q function, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a very common function to see an MDP theory.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that if we define \u03c0 prime of S. To be the action that maximizes this Q function.",
                    "label": 0
                },
                {
                    "sent": "So we have S fixed or policy is a function of S. We want to maximize over these actions.",
                    "label": 0
                },
                {
                    "sent": "It turns out that policy is going to be a strict improvement over the original policy Pi.",
                    "label": 0
                },
                {
                    "sent": "Alright, so some state it's going to improve, and it's not going to be any worse than any other state.",
                    "label": 0
                },
                {
                    "sent": "If Pi happens to be optimal, then Pi prime will also be optimal.",
                    "label": 0
                },
                {
                    "sent": "So this is the policy improvement theorem and it's the basis of.",
                    "label": 0
                },
                {
                    "sent": "The policy iteration algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try to use this fact to somehow use develop a Monte Carlo based approach to policy improvement.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Computing Pi prime amounts to maximizing this Q function.",
                    "label": 0
                },
                {
                    "sent": "How can we use the bandit ideas to solve this to compute pipeline?",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "So set up our bandit where in some State S we have our K actions.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is define a stochastic function which we can implement.",
                    "label": 1
                },
                {
                    "sent": "We're going to call it SIM Q.",
                    "label": 0
                },
                {
                    "sent": "We're going to define this stochastic function and the expected value if we run this function over and over and over is going to be equal to the Q function.",
                    "label": 0
                },
                {
                    "sent": "Right so intuitively, as I'll show you on the next slide, this is simply going to simulate taking action A and following the policy, and you sum up the rewards.",
                    "label": 0
                },
                {
                    "sent": "Very simple to implement.",
                    "label": 0
                },
                {
                    "sent": "And if we could do this if we can figure out what this simulate this function had had to implement this function since the expected value is Q Pi, we can use our uniform bandit algorithm to get a pack result for selecting an approximately optimal improved action.",
                    "label": 1
                },
                {
                    "sent": "And approximately improved action alright?",
                    "label": 0
                },
                {
                    "sent": "So how do we employ?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's imcu, it's actually pretty simple.",
                    "label": 0
                },
                {
                    "sent": "So let's say we pull this arm.",
                    "label": 0
                },
                {
                    "sent": "And this is a function that we're going to implement and somehow using our simulator.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We're just going to put the simulator and state S. Alright, that's the first thing we'll do.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to use a simulator, will take action, A will end up in some state according to our transition function, and then we're going to simulate.",
                    "label": 0
                },
                {
                    "sent": "Then at each state we can look at \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Figure out what action it suggests, take that action, and we end up in a new state.",
                    "label": 0
                },
                {
                    "sent": "We do that for H -- 1 steps.",
                    "label": 0
                },
                {
                    "sent": "We sum up the rewards that we saw and this is what we output for SMQ.",
                    "label": 0
                },
                {
                    "sent": "This is a random function because these trajectories are random trajectories 'cause our simulator is random potentially.",
                    "label": 0
                },
                {
                    "sent": "So that's how we do some Q4A1 do the same thing for the other actions and.",
                    "label": 0
                },
                {
                    "sent": "So thank you is easy to implement if you have the simulator.",
                    "label": 0
                },
                {
                    "sent": "I think we could agree on that, and it's also easy to verify that the expected value MQ is the actual Q function, sort of by definition.",
                    "label": 0
                },
                {
                    "sent": "It sort of doesn't even need a proof, it's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm for policy improvement.",
                    "label": 0
                },
                {
                    "sent": "Now using our simulator.",
                    "label": 0
                },
                {
                    "sent": "So for each arm we're going to run some QW times, right?",
                    "label": 1
                },
                {
                    "sent": "These are these are.",
                    "label": 0
                },
                {
                    "sent": "Basically, different runs of the same queue function, and these values are just the total rewards that we observed when we ran some queue.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to return the arm that has the highest highest average of these values.",
                    "label": 0
                },
                {
                    "sent": "And according to our Uniform bandit result, it tells us well if we really wanted to pick W so that we get a pack guarantee we get a Pat guarantee on choosing an approximately improved action.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Don't change your policy.",
                    "label": 0
                },
                {
                    "sent": "Actually do the policy right right so the situation you want to imagine is you're a robot.",
                    "label": 0
                },
                {
                    "sent": "You're in a state of the world that sass, and now you're trying to pick an action, and so in your head you're going to do all of this stuff with pie, and then you're going to pick the one that maximizes that, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't change pie, but that leads into the next thing.",
                    "label": 0
                },
                {
                    "sent": "I think it's fairly.",
                    "label": 0
                },
                {
                    "sent": "Well, you could nest rollouts right so we could stick in here instead of Pi.",
                    "label": 0
                },
                {
                    "sent": "We could stick, stick in the rollout policy of \u03c0. I think that's a.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next slide.",
                    "label": 0
                },
                {
                    "sent": "No, so so that will be coming in a moment.",
                    "label": 0
                },
                {
                    "sent": "So this let's just finish up the rollout.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically what we need?",
                    "label": 0
                },
                {
                    "sent": "How many samples do we need to generate?",
                    "label": 0
                },
                {
                    "sent": "Well, for each machine we need to generate H times W samples.",
                    "label": 1
                },
                {
                    "sent": "There are K machines, so the number of calls to the simulator, which is sort of.",
                    "label": 1
                },
                {
                    "sent": "The dominating complexity here is K HW.",
                    "label": 0
                },
                {
                    "sent": "Everything else is just a constant.",
                    "label": 0
                },
                {
                    "sent": "We're doing additions over K things and whatnot.",
                    "label": 0
                },
                {
                    "sent": "Alright, so KHW you can get a pack guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multi stage rollout so.",
                    "label": 0
                },
                {
                    "sent": "If you want to get fancy, once you have rollout implemented, you can easily call it sort of recursively on itself.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that this whole process I just described, we're going to say is this policy here roll out pie, right?",
                    "label": 0
                },
                {
                    "sent": "So roll out pie does all of the stuff I just talked about.",
                    "label": 0
                },
                {
                    "sent": "We could pass this in two SIM Q write an SMQ would simulate freeze of the roll out policy of pie.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a mouthful here, but this is sort of accurate.",
                    "label": 0
                },
                {
                    "sent": "Two stages is going to compute the roll out policy of the roll out policy of \u03c0.",
                    "label": 1
                },
                {
                    "sent": "And what this corresponds to is if you're familiar with policy iteration.",
                    "label": 0
                },
                {
                    "sent": "If you started at \u03c0.",
                    "label": 0
                },
                {
                    "sent": "The action you end up picking is going to be what you would get if you had done two iterations of policy iteration, right?",
                    "label": 0
                },
                {
                    "sent": "And of course there's a cost, because now at each of these nodes, in order to compute the action here, we have to do a rollout.",
                    "label": 0
                },
                {
                    "sent": "That's going to cause cost.",
                    "label": 0
                },
                {
                    "sent": "US KHW simulator calls, and so our complexity now is going to be quadratic in KHW.",
                    "label": 0
                },
                {
                    "sent": "You can get by with this for for a few levels of nesting, of course, but it's going to miss exponential in the number of stages that you want, so if you want to simulate.",
                    "label": 1
                },
                {
                    "sent": "Case will say M steps of policy iteration.",
                    "label": 0
                },
                {
                    "sent": "You're going to have to have KQ to the M simulator calls alright?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I won't go over the details here, but it's just some example domains.",
                    "label": 0
                },
                {
                    "sent": "I'll have references where you can write simple, mediocre policies.",
                    "label": 1
                },
                {
                    "sent": "Networking is a big one.",
                    "label": 0
                },
                {
                    "sent": "The game of Hearts, backgammon solitaire.",
                    "label": 1
                },
                {
                    "sent": "There are non game ones as well.",
                    "label": 0
                },
                {
                    "sent": "Emergency response and here are some example of domains where policy rollout has been actually shown to work well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You get some substantial improvements.",
                    "label": 0
                },
                {
                    "sent": "I'll have references in the final version of this, but just to give you a sense, this is fairly old paper and NIPS 04 this is the domain of thoughtful solitaire.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not.",
                    "label": 0
                },
                {
                    "sent": "Klondike will forget the details of what thoughtful solitaire is, but the human expert, this mathematician who's obsessed with the game.",
                    "label": 0
                },
                {
                    "sent": "He records his winning percentage and it's about 36%.",
                    "label": 0
                },
                {
                    "sent": "20 minutes per game.",
                    "label": 0
                },
                {
                    "sent": "Write a very naive based policy.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is something that is really naive.",
                    "label": 0
                },
                {
                    "sent": "You can get a success rate of 13%.",
                    "label": 1
                },
                {
                    "sent": "Now you could spend months trying to improve that based policy or apply one level of rollout.",
                    "label": 0
                },
                {
                    "sent": "You increase your time from nothing to slightly less than nothing.",
                    "label": 0
                },
                {
                    "sent": "You can get up to 31%.",
                    "label": 1
                },
                {
                    "sent": "How about 2 levels of rollout?",
                    "label": 1
                },
                {
                    "sent": "You jump up to 47, beating the human expert and these are statistically significant.",
                    "label": 0
                },
                {
                    "sent": "These probably aren't.",
                    "label": 0
                },
                {
                    "sent": "You increase your time and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "You see a pretty steady increase as you increase is increases stages of rollout.",
                    "label": 0
                },
                {
                    "sent": "Of course, the time is increasing exponentially as well.",
                    "label": 0
                },
                {
                    "sent": "At this point, we're into close to two hours to play a game, so it's actually hard to get statistically meaningful results in that case, but you can see there's a pretty Big Bang for your Buck here question.",
                    "label": 0
                },
                {
                    "sent": "Sort of diminishing returns here.",
                    "label": 0
                },
                {
                    "sent": "The more rollouts you do, well there probably there probably would be just, you know.",
                    "label": 0
                },
                {
                    "sent": "It's getting so complex, the time complexity is getting large enough that we can't actually sort of evaluate these.",
                    "label": 0
                },
                {
                    "sent": "You know, probably you're looking at.",
                    "label": 0
                },
                {
                    "sent": "Five hours here.",
                    "label": 0
                },
                {
                    "sent": "Who knows what it is.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's going to be a day in another stage or two to actually implement this this policy, so that's why we're not seeing that.",
                    "label": 0
                },
                {
                    "sent": "But yeah, obviously eventually you would sort.",
                    "label": 0
                },
                {
                    "sent": "I mean, the improvement is pretty striking.",
                    "label": 0
                },
                {
                    "sent": "I think that's what you're pointing out for this domain, so that was a fairly surprising, but it's expensive.",
                    "label": 0
                },
                {
                    "sent": "I've got some work where you can try to get rid of this time by using machine learning.",
                    "label": 0
                },
                {
                    "sent": "We're not going to talk about that here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As to why, but then it's a, it's an hour per.",
                    "label": 0
                },
                {
                    "sent": "You have to relearn the policy for for each time, so there's not really learning the where the hours coming from is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're doing multiple stages of rollout, and so if we do two stages, the complexity the number of simulator calls goes to KHW squared.",
                    "label": 1
                },
                {
                    "sent": "If we do five stages, this two turns into a five.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the problem here, so that's why you can't do too many stages, yes?",
                    "label": 0
                },
                {
                    "sent": "They actually use WS specified by their own.",
                    "label": 0
                },
                {
                    "sent": "Oh no, you never actually use the theoretical constants.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never, ever.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just pick it, pick it empirically so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know if they talked about that paper is pretty my memory.",
                    "label": 0
                },
                {
                    "sent": "I usually pick 10 I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't know why it's not too much but.",
                    "label": 0
                },
                {
                    "sent": "So, but you're yes.",
                    "label": 0
                },
                {
                    "sent": "How much tweaking in playing with parameters?",
                    "label": 0
                },
                {
                    "sent": "So this result, really there's only the one parameter W and then basically your time complexity is going to scale quite dramatically with W if you have a lot of stages, so there's not a lot of tweaking, you just pick W. Start with 10 an.",
                    "label": 0
                },
                {
                    "sent": "Increase it if you don't like that.",
                    "label": 0
                },
                {
                    "sent": "Horizon so.",
                    "label": 0
                },
                {
                    "sent": "And these games are horizon is sort of naturally defined, but usually if a game could go on forever.",
                    "label": 0
                },
                {
                    "sent": "You could just have a horizon cut off and.",
                    "label": 0
                },
                {
                    "sent": "Well, so that's going to require domain knowledge, so you sort of look at the domain and you say, well, roughly.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can run your policy for example, and see roughly how long does it typically take to terminate and then choose a value like that.",
                    "label": 0
                },
                {
                    "sent": "So in the game of go, it's going to be larger than in some other domain.",
                    "label": 0
                },
                {
                    "sent": "Alright, yes.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it seems like the things you're comparing in the in the results.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The baseline and human.",
                    "label": 0
                },
                {
                    "sent": "Seem to be.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "Take much more time because they came out this particular game.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they're doing search.",
                    "label": 0
                },
                {
                    "sent": "You can view this as a search process to some degree is so weird type of trying to visualize the Spanish state space for these multi stage rollouts is pretty hard I find but but I usually just think of it is it's doing multiple iterations of policy iteration.",
                    "label": 0
                },
                {
                    "sent": "That's how I try to understand it.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're adding search just like if you have a basic heuristic and it gets a baseline performance.",
                    "label": 0
                },
                {
                    "sent": "You could do Mini Mac search or just a heuristic search process.",
                    "label": 0
                },
                {
                    "sent": "When we talk about reinforcement learning before in the connection.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Change it.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "The updates.",
                    "label": 0
                },
                {
                    "sent": "Work on.",
                    "label": 0
                },
                {
                    "sent": "Would generalize to other games Solitaire.",
                    "label": 0
                },
                {
                    "sent": "Here your training costs right across games.",
                    "label": 0
                },
                {
                    "sent": "Is there a way to is there working right so?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so sort of alluding to you can use machine learning to sort of get rid of this cost and try to learn sort of a general version of the rollout four policy or rollout 100 policy as a technique called.",
                    "label": 0
                },
                {
                    "sent": "Well there are different variants but approximate policy iteration.",
                    "label": 0
                },
                {
                    "sent": "I've got a paper on that basically that tries to do this, so if you search for if you go to my web page and look for a paper with approximate policy iteration in the title, you'll see that.",
                    "label": 0
                },
                {
                    "sent": "Some also trying to get a connection policy iteration and it is clear that policy iteration goes through all states.",
                    "label": 0
                },
                {
                    "sent": "This is not going through all states, so that's good.",
                    "label": 0
                },
                {
                    "sent": "But policy iteration or validation remember the value and so if you reach that state again over any other case, you don't need to recompute it.",
                    "label": 0
                },
                {
                    "sent": "But we're not doing that kind of learning in policies alright?",
                    "label": 0
                },
                {
                    "sent": "Add that learning element in policies allowed so that you don't rule out things if you know the answer already.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so that the API approximate policy iteration, sort of.",
                    "label": 0
                },
                {
                    "sent": "Will first learn this and then it will use version that generalizes to generate this in a cheap way.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could.",
                    "label": 0
                },
                {
                    "sent": "I suppose you can remember whatever you want, so you could define a new algorithm that tries to remember states you've been too, but the basic basic idea here is instead of, there's no generalization in the basic rollout, it's just instead of doing policy iteration across all states, you're just doing it for one state and it takes you time to do that.",
                    "label": 0
                },
                {
                    "sent": "Instead of memory.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically you're trading off time memory for time, alright, so sparse sampling.",
                    "label": 0
                },
                {
                    "sent": "Is an approach that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's really here for historical reasons.",
                    "label": 0
                },
                {
                    "sent": "I'm covering it slightly differently in terms of bandits, but rollout doesn't guarantee near optimal near optimality guarantees.",
                    "label": 1
                },
                {
                    "sent": "Approximate policy improvement, right?",
                    "label": 0
                },
                {
                    "sent": "And if you don't have a policy, maybe you'd like to just have a Monte Carlo algorithm that can output in approximately optimal action, right?",
                    "label": 0
                },
                {
                    "sent": "So so we just have the simulator.",
                    "label": 1
                },
                {
                    "sent": "How can we compute an approximately optimal action?",
                    "label": 0
                },
                {
                    "sent": "So so the key thing though is we would like a Monte Carlo algorithm that we could probably say gives us an approximately optimal action, but we want the guarantee to be we want the number of simulator calls to be independent of the number of states.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a theoretical question.",
                    "label": 1
                },
                {
                    "sent": "Can you get a Monte Carlo algorithm that's independent of the number of states but computes in approximately optimal action?",
                    "label": 1
                },
                {
                    "sent": "And yes, 1999 there was an affirmative answer that yes you can.",
                    "label": 0
                },
                {
                    "sent": "It's the sparse sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "By Kerns and forget a few other authors.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So now let's just look at this algorithm that has this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theoretical guarantee is going to be the most practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we need a few MDP basics again, so there is an optimal policy and let's let V star represent its optimal value function right and Q star is going to be the optimal Q function for that policy.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is a way that you can define Q star.",
                    "label": 0
                },
                {
                    "sent": "It's basically the immediate reward that you get for taking a an S plus the reward you expect to get in the next state.",
                    "label": 0
                },
                {
                    "sent": "So let's TSA after taking the action right, so it's the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "There should be a discount factor here.",
                    "label": 0
                },
                {
                    "sent": "The immediate reward plus what you expect to get in the future in a horizon of H -- 1 and the expectation of this is the Q value, so basic relationship.",
                    "label": 0
                },
                {
                    "sent": "Now the optimal policy turns out to just be.",
                    "label": 0
                },
                {
                    "sent": "The policy maximizes this Q value.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's first consider what we would do if we happen to know V star.",
                    "label": 0
                },
                {
                    "sent": "We don't know vstar.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the problem, but if we did know it, how could we apply the bandit algorithms?",
                    "label": 0
                },
                {
                    "sent": "Alright, so to approximately optimize this thing?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we know vstar, we're going to define a new simulation function, right?",
                    "label": 0
                },
                {
                    "sent": "This is something that we write in C. And what's MQ is going to do is simply sample a next state right?",
                    "label": 0
                },
                {
                    "sent": "So it samples the next state, S prime is going to sample a reward and it's going to return R + V star, which we assume we know of that next state, right?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a sample of.",
                    "label": 0
                },
                {
                    "sent": "What was inside this expectation?",
                    "label": 0
                },
                {
                    "sent": "We're just implementing this stochastic function because we are simulated for our in a simulator for T and we know V star.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's all SIM Q star is going to do if we know the star and the expected value of SIM Q star is actually Q star.",
                    "label": 1
                },
                {
                    "sent": "Just by definition.",
                    "label": 0
                },
                {
                    "sent": "I just showed you on the previous slide so we could use our uniform bandit to select an approximately optimal action right to get a pack result, but we don't know the star.",
                    "label": 1
                },
                {
                    "sent": "So how do we deal with that?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so we don't know vstar.",
                    "label": 1
                },
                {
                    "sent": "And we notice that to compute SIM Q star.",
                    "label": 0
                },
                {
                    "sent": "For Horizon H we needed the value of V star for all states at Horizon H -- 1.",
                    "label": 0
                },
                {
                    "sent": "So this sort of suggests that maybe you could use a recursive definition to solve our problem.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea that we need is known as the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "So vstar SH minus one.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the maximum Q value of the H -- 1 Q function, right?",
                    "label": 0
                },
                {
                    "sent": "So to compute Q star we need V star at horizon H -- 1.",
                    "label": 0
                },
                {
                    "sent": "We can get that if we know the Q star function at H -- 1 and you can sort of.",
                    "label": 0
                },
                {
                    "sent": "Imagine now that we can apply these bandits recursively.",
                    "label": 0
                },
                {
                    "sent": "So we could implement SIM Q star by recursively calling H -- 1 horizon bandits.",
                    "label": 0
                },
                {
                    "sent": "In the base case, is going to be V star at Horizon Zero is equal to 0, so it's just.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what this looks like.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our uniform bandit algorithm is going to try to generate these samples, right?",
                    "label": 0
                },
                {
                    "sent": "And we don't know vstar, so we can't do the simple SIM Q star that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "There should be some Q star.",
                    "label": 0
                },
                {
                    "sent": "So simple Q star is going to have to recursively call a whole tree of bandits.",
                    "label": 0
                },
                {
                    "sent": "So let's say we want to we call this arm once and we want to compute this.",
                    "label": 0
                },
                {
                    "sent": "What's it going to do?",
                    "label": 0
                },
                {
                    "sent": "Is trying to sample some Q star is first going to sample a state?",
                    "label": 0
                },
                {
                    "sent": "Remember that that's what we said.",
                    "label": 0
                },
                {
                    "sent": "SIM Q star.",
                    "label": 0
                },
                {
                    "sent": "Does it samples a state samples or reward and then it has to have that vstar part samples of state samples or reward?",
                    "label": 0
                },
                {
                    "sent": "And now we've got a bandit problem here.",
                    "label": 0
                },
                {
                    "sent": "But it's at horizon H -- 1 and the maximum of these guys are going to return their Q value and that will be an estimate of V. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the key idea behind sparse sampling.",
                    "label": 0
                },
                {
                    "sent": "So you see that basically there's a whole tree of these bandits, right?",
                    "label": 0
                },
                {
                    "sent": "That's yes.",
                    "label": 0
                },
                {
                    "sent": "Similar.",
                    "label": 0
                },
                {
                    "sent": "Sort of sort of a different animal.",
                    "label": 0
                },
                {
                    "sent": "Our GDP is more like you generate trajectories through the environment.",
                    "label": 0
                },
                {
                    "sent": "This is really more like a standard mini Mac search or expecting Max search, so it's generated.",
                    "label": 0
                },
                {
                    "sent": "This is going to be generating a search tree because after we're done here, we're going to generate this this and it's not until we know all of these values.",
                    "label": 0
                },
                {
                    "sent": "All of the values of these subtrees that we can compute the value for this action, so it's more like a search tree type approach.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So then we'll do the same thing for Q12.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the real and I've got pseudocode here.",
                    "label": 0
                },
                {
                    "sent": "If you actually want to see this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Code of this process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can view this as a tree with root S and each state S will generate kW new states, right?",
                    "label": 1
                },
                {
                    "sent": "So we have K of these arms and they're each going to each want to generate W samples.",
                    "label": 0
                },
                {
                    "sent": "So kW each sample is going to generate a new state, so you have kW new states down here and.",
                    "label": 0
                },
                {
                    "sent": "There's W states for K bandits, and so for horizon of H we're going to have K times WH states in this tree, and that's sort of the number of simulator calls that you're going to need so.",
                    "label": 0
                },
                {
                    "sent": "It might be disappointing that we have an H in the exponential here.",
                    "label": 0
                },
                {
                    "sent": "It turns it, but notice well, we're not quite there yet.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can't get rid of this.",
                    "label": 0
                },
                {
                    "sent": "There's actually a lower bound that matches it, so now now we've almost got an algorithm for approximately.",
                    "label": 0
                },
                {
                    "sent": "Optimally solving MDP at a state.",
                    "label": 0
                },
                {
                    "sent": "If we can specify W needs to be to guarantee if W can be independent of the number of states we've got a result.",
                    "label": 0
                },
                {
                    "sent": "Now we can't pick W the way that we just did earlier.",
                    "label": 0
                },
                {
                    "sent": "For the uniform Bandit, because notice that you know these values here are only approximate values of the star, because there's error that propagates up the tree.",
                    "label": 0
                },
                {
                    "sent": "And so the real sort of contribution theoretically of that paper is to show that doing a careful analysis of this recursion and showing that these errors don't propagate too badly, and in fact you can pick to pick a larger W than what we did before, but you can pick W in a way that does not depend on the number of states, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is the basic theoretical result in the current paper.",
                    "label": 0
                },
                {
                    "sent": "This is the Journal version.",
                    "label": 0
                },
                {
                    "sent": "It really occurred in 1999, which is surprising.",
                    "label": 0
                },
                {
                    "sent": "Basic nature of that problem.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that we can achieve near optimality for a value of W that's independent of the state space size.",
                    "label": 1
                },
                {
                    "sent": "And this was the first sort of near optimal MDP algorithm that was independent of the state space size, so that was sort of a landmark result.",
                    "label": 1
                },
                {
                    "sent": "The bad news is the theoretical values are typically quite large.",
                    "label": 0
                },
                {
                    "sent": "That's always the case, and we're exponential in age.",
                    "label": 0
                },
                {
                    "sent": "They show a lower bound in that paper that shows you can't.",
                    "label": 0
                },
                {
                    "sent": "It's a tight lower bound.",
                    "label": 0
                },
                {
                    "sent": "You can't get rid of H from the exponent.",
                    "label": 0
                },
                {
                    "sent": "So we sort of tradeoff dependence on the number of states to dependence to the horizon.",
                    "label": 0
                },
                {
                    "sent": "More or less that, so that's what you end up with.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "If you have a heuristic function, use a small value of H and you'll get some improvements.",
                    "label": 0
                },
                {
                    "sent": "The structure of the state space, does it say MDP?",
                    "label": 0
                },
                {
                    "sent": "No, it just assumes it's an MDP, so the Markov property is all it really assumes it can be cyclic doesn't doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I think they also assume bounded rewards.",
                    "label": 0
                },
                {
                    "sent": "You could probably get rid of that if you want.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's sort of a theoretical result that that is sort of there for historical interest.",
                    "label": 0
                },
                {
                    "sent": "It's not the most practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "Obviously you have to build a big tree.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't actually seem so good from an intuitive standpoint.",
                    "label": 0
                },
                {
                    "sent": "If we think about what it's doing is generating a huge amount of work for this action here, and what if we have two actions that sort of one is a lot better than the other one?",
                    "label": 0
                },
                {
                    "sent": "It feels like a waste to spend the same effort generating this tree.",
                    "label": 0
                },
                {
                    "sent": "Is this tree.",
                    "label": 0
                },
                {
                    "sent": "It would be nice somehow.",
                    "label": 0
                },
                {
                    "sent": "We could adaptively control the amount of work we put into figuring out the values for these actions, so that actions that are clearly suboptimal early on.",
                    "label": 0
                },
                {
                    "sent": "We don't worry about.",
                    "label": 0
                },
                {
                    "sent": "We put our resources into disambiguating closer to optimal actions, right?",
                    "label": 0
                },
                {
                    "sent": "So for that case, for that reason I'm referring to sparse sampling is sort of a uniform Monte Carlo approach.",
                    "label": 0
                },
                {
                    "sent": "It's not very smart about where it spends its effort.",
                    "label": 0
                },
                {
                    "sent": "It's just let's just do this thing and return the action.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to get into adaptive Monte Carlo, and we're going to start out by going to the single state case again.",
                    "label": 1
                },
                {
                    "sent": "So we end up with a.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Di Carlo with a multi armed bandit problem and I'm going to change the objective and this is actually the original objective of the Multi armed bandit problem from the 1960s.",
                    "label": 0
                },
                {
                    "sent": "This problem in experimental design that's sort of where it came out originate iddc.",
                    "label": 0
                },
                {
                    "sent": "So we're going to call this the regret minimization objective for bandits.",
                    "label": 0
                },
                {
                    "sent": "Or loss minimisation.",
                    "label": 0
                },
                {
                    "sent": "It's been called that as well, So what we want to do is find the arm pulling strategy such that the expected total reward at time N is close to the best possible.",
                    "label": 1
                },
                {
                    "sent": "So if we went to Vegas and we had these slot machines, and some of them were biased positively and somewhere biased negatively.",
                    "label": 0
                },
                {
                    "sent": "If we knew that best arm, we go there and pull it and that's the best we could do on average in the limit.",
                    "label": 0
                },
                {
                    "sent": "Now we don't know the biases of these arms and so the question is we want to pull arms 'cause we want to make money.",
                    "label": 0
                },
                {
                    "sent": "But after North Poles we'd like to somehow ensure that were close to the optimal.",
                    "label": 0
                },
                {
                    "sent": "If we had known the best arm was.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know what the best arm is, how close can we do after a particular number of arm pulls too?",
                    "label": 0
                },
                {
                    "sent": "If we had known the optimal arm, and so this is sort of a problem of exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "'cause you need to explore arms at some rate in order to find ones that are promising.",
                    "label": 0
                },
                {
                    "sent": "But you also need to exploit arms that already look promising in order to make sure your profits don't go too low.",
                    "label": 0
                },
                {
                    "sent": "And one thing that's pretty clear is that the Uniform Bandit is a really bad choice for if you're in Vegas and you have a slot bunch of slot machines in front of U, uniform bandit will start at the first one.",
                    "label": 0
                },
                {
                    "sent": "Pull W Times second one W times, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But really, you would probably.",
                    "label": 0
                },
                {
                    "sent": "Sort of, walk around and pull different arms and keep statistics of these arms and pull the one that looks most promising.",
                    "label": 0
                },
                {
                    "sent": "The question here is other algorithms that can get some theoretical bounds on how close to optimal you're going to be without knowing.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's optimal.",
                    "label": 0
                },
                {
                    "sent": "So this is a paper from 2002, sort of a pretty well cited paper.",
                    "label": 0
                },
                {
                    "sent": "Now on that analyze this regret minimization problem for the multi Arm Bandit.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to tell you the algorithm is extremely simple, so at every moment in time we're going to have some statistics we've collected.",
                    "label": 0
                },
                {
                    "sent": "So we've pulled a bunch of arms.",
                    "label": 0
                },
                {
                    "sent": "QA is going to be the average payoff that we've observed for our May alright, so we pulled our may some number of times.",
                    "label": 1
                },
                {
                    "sent": "QA is the payoff that we've observed on average.",
                    "label": 0
                },
                {
                    "sent": "NA is a number of times that we've pulled our May right so we can store these things easily.",
                    "label": 0
                },
                {
                    "sent": "And now the question is, what arm do I pull next?",
                    "label": 0
                },
                {
                    "sent": "You see before upper confidence bound is going to pull this arm a star that maximizes this quantity.",
                    "label": 1
                },
                {
                    "sent": "Alright, so talk about this quantity in a moment that this assumes that the pay offs are in 01.",
                    "label": 0
                },
                {
                    "sent": "If they are in 0 to B, have to be squared there, but this is the quantity that it's going to maximize and it's going to pull that arm.",
                    "label": 0
                },
                {
                    "sent": "Now the basic theorem is that the expected regret, so the difference between the sum of rewards that we would get if we pulled the over the optimal arm and the sum of rewards we get according to the strategy.",
                    "label": 1
                },
                {
                    "sent": "That's the expected regret.",
                    "label": 0
                },
                {
                    "sent": "The difference in the sums on expectation.",
                    "label": 0
                },
                {
                    "sent": "Is going to be bounded by log N, the number of arm poles, and this is a pretty pretty good bound, and in fact there's actually a lower bound that matches this, so it says that the sum of arm poles that I get with my strategy compared to some of our poles.",
                    "label": 1
                },
                {
                    "sent": "If I had always pulled the optimal arm, is going to be bounded by this at a particular formally overtime, it's not an asymptotically result, and so if you think about what is the average.",
                    "label": 0
                },
                {
                    "sent": "Regret, right?",
                    "label": 0
                },
                {
                    "sent": "You're going to divide this by N. And so on.",
                    "label": 0
                },
                {
                    "sent": "Average your regret.",
                    "label": 0
                },
                {
                    "sent": "Your payoff is going to be log divided by N away from optimal, so that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "That goes to zero quite quickly.",
                    "label": 0
                },
                {
                    "sent": "So that's a it's a pretty famous result.",
                    "label": 0
                },
                {
                    "sent": "This problem was defined in the 60s and it's just 2002 that this result.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Came around.",
                    "label": 0
                },
                {
                    "sent": "There is scoring rules that were defined for at least the classical version, so those results.",
                    "label": 0
                },
                {
                    "sent": "So the original scoring rules required quite a bit of computation the the indices this requires practically no computation.",
                    "label": 0
                },
                {
                    "sent": "It has simple statistics.",
                    "label": 0
                },
                {
                    "sent": "They had some later work so they could prove bounds of logged in for those, but they were asymptotically.",
                    "label": 0
                },
                {
                    "sent": "As Angus goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then they had some.",
                    "label": 0
                },
                {
                    "sent": "Easier to compute versions and also the bounds were asymptotic.",
                    "label": 0
                },
                {
                    "sent": "This was the first bound that was not this.",
                    "label": 0
                },
                {
                    "sent": "Actually every single end it satisfies the property.",
                    "label": 0
                },
                {
                    "sent": "They are also optimal.",
                    "label": 0
                },
                {
                    "sent": "I don't know about that.",
                    "label": 0
                },
                {
                    "sent": "I think they yeah I would have to think about that so forget about this or some animation problems.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this this.",
                    "label": 0
                },
                {
                    "sent": "Rule this is our algorithm right?",
                    "label": 0
                },
                {
                    "sent": "Our decision rule.",
                    "label": 0
                },
                {
                    "sent": "So what what it is is a value term QA and so arms that have high value or we're going to be more inclined to pull because the sum will be larger arms that have looked really bad in the past will be less inclined.",
                    "label": 0
                },
                {
                    "sent": "But remember, we always have to make sure we explore at some rate because maybe you get unlucky with the optimal arm in the beginning and you don't want to avoid it forever.",
                    "label": 0
                },
                {
                    "sent": "So we have this exploration term.",
                    "label": 1
                },
                {
                    "sent": "The and what this is going to do is so notice that log N is in the numerator of this term.",
                    "label": 0
                },
                {
                    "sent": "So remember N is the total number of poles, so as the total number of poles increases.",
                    "label": 0
                },
                {
                    "sent": "And let's say we don't pull this arm, so Na stays the same.",
                    "label": 0
                },
                {
                    "sent": "This term is going to keep getting larger and larger and larger, and eventually you're going to pull that arm again, right?",
                    "label": 0
                },
                {
                    "sent": "Now when you pull it again, this is going to increase by one and it doesn't take too many armholes for this to sort of dominate the log in.",
                    "label": 0
                },
                {
                    "sent": "And if you pull an arm a lot, this basically goes to 0, so so arms that are pulled a lot or basically dominated by this term arms that are pulled a little or dominated by this term, and he only pulled them a little bit.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "The fundamental result of that paper following.",
                    "label": 0
                },
                {
                    "sent": "So the expected number of times that we're going to pull a suboptimal arm so there.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to define OK, so the expected number of times that we pull in a suboptimal arm after end total poles is going to be bounded by this.",
                    "label": 1
                },
                {
                    "sent": "So notice 1st that it's login.",
                    "label": 0
                },
                {
                    "sent": "So if an arm is suboptimal asymptotically and actually it every moment is dominated by login, so we're not going to pull it very much because login is a lot smaller than.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "This is the difference between the expected value of the optimal arm and arm.",
                    "label": 0
                },
                {
                    "sent": "A right.",
                    "label": 0
                },
                {
                    "sent": "And notice if R May is really bad compared to the optimal arm.",
                    "label": 0
                },
                {
                    "sent": "This term is going to be very small, right?",
                    "label": 0
                },
                {
                    "sent": "Whereas if our May is very close to optimal, this constant will be large, but in some sense that's OK because it's close to optimal.",
                    "label": 0
                },
                {
                    "sent": "We're happy to pull it more than in ARM.",
                    "label": 0
                },
                {
                    "sent": "That is really bad, and so this is the fundamental result.",
                    "label": 0
                },
                {
                    "sent": "And basically if you multiply this quantity by this difference, you get the expected regret at time and so.",
                    "label": 0
                },
                {
                    "sent": "So you just get one of these in the denominator.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So that's so.",
                    "label": 1
                },
                {
                    "sent": "It doesn't waste time on bad arms.",
                    "label": 0
                },
                {
                    "sent": "Sort of a fundamental result from the theory of multi arm.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bandits.",
                    "label": 0
                },
                {
                    "sent": "So now how can we apply this result?",
                    "label": 0
                },
                {
                    "sent": "We have about 20 minutes I'll.",
                    "label": 0
                },
                {
                    "sent": "So one way would be to show all the algorithms I described before were described in terms of the Uniform bandit, right?",
                    "label": 0
                },
                {
                    "sent": "So you can plug in?",
                    "label": 0
                },
                {
                    "sent": "And what does the uniform bandit do?",
                    "label": 0
                },
                {
                    "sent": "It allocates samples per action, right?",
                    "label": 0
                },
                {
                    "sent": "So for rollout we allocated WC fraction instead.",
                    "label": 0
                },
                {
                    "sent": "We can allocate samples at states.",
                    "label": 0
                },
                {
                    "sent": "So W samples per state and the multi arm bandit algorithm I just showed you will figure out how to allocate the samples to the actions that sort of the key difference here and so we could develop a UCB based role policy rollout version.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a citation for somebody who's tried that.",
                    "label": 0
                },
                {
                    "sent": "Annual typically do as well as the uniform, but with fewer total samples.",
                    "label": 0
                },
                {
                    "sent": "Similarly, a sparse sampling you could do that.",
                    "label": 1
                },
                {
                    "sent": "You could use a. UCB Bandit algorithm instead of the uniform bandit.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I do have a picture of that.",
                    "label": 0
                },
                {
                    "sent": "I think I'm going to skip it for the interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you what it does.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "So this was developed in five, and it's an improvement on basic sparse sampling, but but it's not really a very practical improvement.",
                    "label": 0
                },
                {
                    "sent": "So if we think about what sparse sampling will do with this, root is going to allocate W. Subtrees to each of these guys right and just blindly go down and generate all those subtrees.",
                    "label": 0
                },
                {
                    "sent": "What the UCB based version is going to do is, well, it's going to 1st generate this subtree.",
                    "label": 0
                },
                {
                    "sent": "One of these subtrees an of course it will then generate it's going to generate one subtree for every arm, because with zero polls, the UCB algorithm goes to Infinity and you'll pull that arm.",
                    "label": 0
                },
                {
                    "sent": "But then based on the results that we observed previously, it's going to maybe this one looked really good.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to generate over here instead of uniformly.",
                    "label": 0
                },
                {
                    "sent": "If this looks really bad, first thing is going to avoid pulling this for a little while, so it's going to focus on generating these subtrees.",
                    "label": 0
                },
                {
                    "sent": "On places that look most promising, with a little bit of exploration.",
                    "label": 0
                },
                {
                    "sent": "Still, this is not a very practical algorithm, so if you imagine what's going on.",
                    "label": 0
                },
                {
                    "sent": "Just to generate this Q, you're generating a whole subtree, sure, inside the subtree the samples are being allocated a little more efficiently, but still there's a huge amount of computation here, and you don't.",
                    "label": 0
                },
                {
                    "sent": "You can't really do anything until that's done, so it has horrible anytime performance, better anytime.",
                    "label": 0
                },
                {
                    "sent": "Performance in sparse sampling.",
                    "label": 0
                },
                {
                    "sent": "But until you've generated a whole bunch of subtrees, you can't really make any decision at the root, so I would say this is an improvement on sparse sampling, but.",
                    "label": 0
                },
                {
                    "sent": "Not really in a practical sense.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is where I don't know if this is where the idea of UCT originated.",
                    "label": 0
                },
                {
                    "sent": "But it came after that 2005 paper.",
                    "label": 0
                },
                {
                    "sent": "UCT is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's intended to be a practical way of using UCB to generate these trees, and it's going to have much better anytime performance, so we want to be able to stop at anytime and have something nontrivial to return.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the idea, whereas with sparse sampling, both variants.",
                    "label": 0
                },
                {
                    "sent": "You couldn't stop at anytime because you have to wait until some number of these subtrees have been generated to get any information.",
                    "label": 0
                },
                {
                    "sent": "So anytime performances.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, what you see T is aimed at compared to sparse sampling.",
                    "label": 0
                },
                {
                    "sent": "So sparse sampling is an instance of what's being called Monte Carlo Game Tree Search.",
                    "label": 1
                },
                {
                    "sent": "Short research spend used a lot in go in the past with not a whole lot of success until you see T came around, but you see, by the way stands for upper Confidence Tree as opposed to UCB, which is upper confidence bound.",
                    "label": 0
                },
                {
                    "sent": "So sort of ECT is basic.",
                    "label": 0
                },
                {
                    "sent": "Basic publicity.",
                    "label": 0
                },
                {
                    "sent": "The most publicized fact is that it's a huge advance in computer go when you apply it to go, which is kind of surprising, but but it sort of revolutionized that area and I'll talk about that a little bit later.",
                    "label": 1
                },
                {
                    "sent": "Has some nice theoretical properties as well.",
                    "label": 0
                },
                {
                    "sent": "Basically all the theoretical properties that sparse sampling has.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "But it better anytime performance.",
                    "label": 0
                },
                {
                    "sent": "So what is Monte Carlo tree search?",
                    "label": 0
                },
                {
                    "sent": "We first have to describe that and then describing UCT is really simple.",
                    "label": 1
                },
                {
                    "sent": "Alright, so Monte Carlo tree search, what we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "We're going to do repeated Monte Carlo simulations of a rollout policy an the choice of this rollout policy is the critical thing.",
                    "label": 1
                },
                {
                    "sent": "Each rollout is going to add one or more nodes to our tree, so building a search tree and each rollout is going to start at the root and add one or more nodes to the tree, and I'm just going to show.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are you a picture of this?",
                    "label": 0
                },
                {
                    "sent": "So remember, we're in a setting.",
                    "label": 0
                },
                {
                    "sent": "Where are robots in the state of the world and the robot is going to think for awhile about this state, and this is the state that we're in an now.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about the thinking process when we do Monte Carlo Tree search.",
                    "label": 0
                },
                {
                    "sent": "So initially the tree that we're building is an initial leaf node.",
                    "label": 0
                },
                {
                    "sent": "Our current state and when we don't have any information when we're at a leaf node in our tree, all we do is we're going to follow something called the rollout policy.",
                    "label": 1
                },
                {
                    "sent": "This could be random.",
                    "label": 0
                },
                {
                    "sent": "It could be something that's smarter and after we follow this, let's say we end up with a reward of 1.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume there's just terminal rewards.",
                    "label": 0
                },
                {
                    "sent": "We are going to update sort of the rewards that have been observed at all of these nodes.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do another simulation so we built a partial tree and the critical thing about Monte Carlo Tree search is that the simulation policy is going to be sensitive to the tree that you've built so far.",
                    "label": 0
                },
                {
                    "sent": "Now, so our next simulation is going to start at the root node.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is all done in the robots head.",
                    "label": 0
                },
                {
                    "sent": "And there's always you have to try every action once, so we haven't tried one of the actions here.",
                    "label": 0
                },
                {
                    "sent": "Say there's two actions, so we're just going to again simulate.",
                    "label": 0
                },
                {
                    "sent": "The other action, and do our rollout policy is whenever we're at a leaf node, we don't have any information, we just do our default rollout policy.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we maybe we get a zero reward there.",
                    "label": 0
                },
                {
                    "sent": "So now and notice that the root we sort of have a value of 1/2 because half of the times we got.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Got a reward of 1.",
                    "label": 0
                },
                {
                    "sent": "Now we actually have a non trivial tree.",
                    "label": 0
                },
                {
                    "sent": "I should say that most implementations won't bother adding all these nodes, they'll just add the first node.",
                    "label": 0
                },
                {
                    "sent": "That sort of was off the tree that you previously had, 'cause you generate a lot of garbage space.",
                    "label": 0
                },
                {
                    "sent": "If you do this, but it is sort of a detail.",
                    "label": 0
                },
                {
                    "sent": "Theoretically it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So now we have a tree and when we're at a node that is not a leaf node, we're going to follow the tree policy.",
                    "label": 0
                },
                {
                    "sent": "And again, I haven't defined what this is, but it's gonna be based on UCB.",
                    "label": 0
                },
                {
                    "sent": "I'll let the cat out of the bag.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what Monte Carlo Tree search will do is it will look at the statistics it's collected and have some rule for picking whether to go here or to go here.",
                    "label": 0
                },
                {
                    "sent": "That's what the tree policy does.",
                    "label": 1
                },
                {
                    "sent": "The tree policy has to sort of decide which place should I explore.",
                    "label": 0
                },
                {
                    "sent": "It needs to exploit.",
                    "label": 0
                },
                {
                    "sent": "Places that look good but also explore places that it hasn't looked at much.",
                    "label": 0
                },
                {
                    "sent": "Is the value in the notice the average?",
                    "label": 0
                },
                {
                    "sent": "Let's just say it's the average.",
                    "label": 0
                },
                {
                    "sent": "There are different things.",
                    "label": 0
                },
                {
                    "sent": "Right, so so it works out theoretically, but let's just say for now that.",
                    "label": 0
                },
                {
                    "sent": "You just store the average of what you've seen.",
                    "label": 0
                },
                {
                    "sent": "You can do different types of backups if you want, but UCT does this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically, the tree policy might say, well, I guess I'll go down here because it's looked more promising and it selects this node.",
                    "label": 0
                },
                {
                    "sent": "And now we're at a state where we.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Selected all the actions so it's going to just generate this and run the default policy.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now our trees bigger.",
                    "label": 0
                },
                {
                    "sent": "We update our values and perhaps now we're at the root and maybe it would select this again and now maybe based on what it's seen.",
                    "label": 0
                },
                {
                    "sent": "Maybe it would explore this path so there's always two distinct phases.",
                    "label": 0
                },
                {
                    "sent": "The tree policy that will sort of find the first leaf and then the default policy that once you find the run to the end and the values we store just the averages and so the critical question is what is the appropriate rollout policy and what is an appropriate tree policy?",
                    "label": 1
                },
                {
                    "sent": "And UCT is a particular choice of those two.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things so the basic UCT algorithm is basically just going to use a random rollout policy.",
                    "label": 1
                },
                {
                    "sent": "That's theoretically what it uses.",
                    "label": 0
                },
                {
                    "sent": "You typically use something that's informed.",
                    "label": 0
                },
                {
                    "sent": "The tree policy is based on UCB, so remember every state we encounter we can keep counts of how many times we've been there.",
                    "label": 1
                },
                {
                    "sent": "That is what we're going to store here, so it's status.",
                    "label": 0
                },
                {
                    "sent": "We've been there NS times.",
                    "label": 0
                },
                {
                    "sent": "We can also keep counts on how many times we've tried each action.",
                    "label": 0
                },
                {
                    "sent": "That's NSA and we can keep counts on the average reward we've seen when we've gone through that state that skew essay.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "And the UCT tree policy is simply the B form, except theoretically you have to have a constant here.",
                    "label": 0
                },
                {
                    "sent": "Because theoretically these these these pay offs you get when you pull the arms are not independent from one another, and they're also not stationary.",
                    "label": 0
                },
                {
                    "sent": "So theoretically the main contribution is to show that the same form works as long as you have a constant.",
                    "label": 0
                },
                {
                    "sent": "The theoretical constant is the horizon.",
                    "label": 0
                },
                {
                    "sent": "Typically this is something you tune for your particular application domain.",
                    "label": 0
                },
                {
                    "sent": "Just try a bunch of values as a best value for go.",
                    "label": 0
                },
                {
                    "sent": "For example, alright, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "UCT, so just to show you one more view of this.",
                    "label": 0
                },
                {
                    "sent": "So suppose that this is our current tree.",
                    "label": 0
                },
                {
                    "sent": "We've already generated it.",
                    "label": 0
                },
                {
                    "sent": "ECT now wants to do another policy.",
                    "label": 0
                },
                {
                    "sent": "It starts at the root.",
                    "label": 0
                },
                {
                    "sent": "It has all the statistics.",
                    "label": 0
                },
                {
                    "sent": "You have a bandit problem here, right?",
                    "label": 0
                },
                {
                    "sent": "To choose this armor this arm and.",
                    "label": 0
                },
                {
                    "sent": "Basically use this rule and it will choose.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the arms.",
                    "label": 0
                },
                {
                    "sent": "And similarly you have a bandit problem there, and it will choose one of the arms and then it will just do a random roll out from there, yes?",
                    "label": 0
                },
                {
                    "sent": "Theoretical guarantees applied for any constant you choose or for the correct one.",
                    "label": 0
                },
                {
                    "sent": "The theoretical result is for the constant is the horizon time, so yeah, but in practice you don't use that 'cause it ends up exploring too much basically.",
                    "label": 0
                },
                {
                    "sent": "So the I don't actually have a theorem of the UCT result, but basically it's in terms of yes.",
                    "label": 0
                },
                {
                    "sent": "Instead of the value.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "So you might think that maybe you should do sort of a store.",
                    "label": 0
                },
                {
                    "sent": "The maximum of the Q values of these.",
                    "label": 0
                },
                {
                    "sent": "You might be able to improve doing that.",
                    "label": 0
                },
                {
                    "sent": "People play with this stuff.",
                    "label": 0
                },
                {
                    "sent": "Theoretically what happens is remember what UCB does is if an arm is suboptimal, it's only going to be pulled sort of log N times, and so the average sort of is dominated by the best arm that's actually the key insight behind these algorithms.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do a Max, you can do an average as long as you don't pull the bad arms too often, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the that's actually the key insight.",
                    "label": 0
                },
                {
                    "sent": "Behind the 2005 paper in fact, and then this sort of built on that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so theoretically the results are in terms of number of trajectories and basically as you there's a polynomial relation between the number of trajectories and how close you're going to be to the optimal value here, and it's independent of the state space size.",
                    "label": 0
                },
                {
                    "sent": "You know the balance to sparse sampling.",
                    "label": 0
                },
                {
                    "sent": "There's sort of equivalent.",
                    "label": 0
                },
                {
                    "sent": "You can also bound the probability of selecting the optimal action, so in terms of the number of trajectories, and that as a polynomial bound in 1 / T, where T is the number of trajectories.",
                    "label": 0
                },
                {
                    "sent": "So as you pull the arm more and more, the probability you select a suboptimal arm goes down fairly quickly with the number of trajectories you generate.",
                    "label": 0
                },
                {
                    "sent": "But but notice the key thing here is you can stop pretty much at anytime.",
                    "label": 0
                },
                {
                    "sent": "It doesn't take much time to generate a trajectory, so you generate 10 thousand 100,000 trajectories.",
                    "label": 0
                },
                {
                    "sent": "You can stop and you'll have something nontrivial to work with at the root.",
                    "label": 0
                },
                {
                    "sent": "And it just so happens that it's an entree.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well enough so you get state of the art performance and go so this is sort of a recap.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not going to go through it here.",
                    "label": 0
                },
                {
                    "sent": "Computer go, here's just some quotes that indicate how hard people think it is.",
                    "label": 1
                },
                {
                    "sent": "It is a hard problem.",
                    "label": 0
                },
                {
                    "sent": "We're not even close to master level performance yet, even with UCT, but it's a I think it has.",
                    "label": 0
                },
                {
                    "sent": "Chinese origins is played a lot in Asia.",
                    "label": 0
                },
                {
                    "sent": "So 19 by 19 board or 9 by 9 board and you're basically trying to surround your opponents and with these black and white stones.",
                    "label": 0
                },
                {
                    "sent": "So many maxed research is very bad.",
                    "label": 0
                },
                {
                    "sent": "Nobody's ever got that to work very well, primarily, I guess because we don't have good heuristics and also the branching factor is a lot larger than chess.",
                    "label": 0
                },
                {
                    "sent": "For example, you know the branching factor is basically all the places you could put a stone, and that's enormous, and the heuristics.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a very visual game.",
                    "label": 0
                },
                {
                    "sent": "We don't really know how to compute good heuristics for go at least.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll just put this up.",
                    "label": 0
                },
                {
                    "sent": "Animations screwed up, but 2005.",
                    "label": 0
                },
                {
                    "sent": "I mean basically.",
                    "label": 0
                },
                {
                    "sent": "Goes impossible.",
                    "label": 0
                },
                {
                    "sent": "There is a go conference, but I mean for years and years just minor bits of progress about little tiny aspects of go.",
                    "label": 0
                },
                {
                    "sent": "2006 UCT was invented in it was applied to go.",
                    "label": 1
                },
                {
                    "sent": "And it actually works surprisingly well in nine by nine, though I think they were beating some of the better programs at that point.",
                    "label": 0
                },
                {
                    "sent": "So now you know they start playing with this basic UCT algorithm.",
                    "label": 0
                },
                {
                    "sent": "You add bells and whistles to it.",
                    "label": 0
                },
                {
                    "sent": "You don't do a random rollout policy, you do something that's based on a small amount of go knowledge and 2007.",
                    "label": 0
                },
                {
                    "sent": "You're basically at master level performance and 9 by 9.",
                    "label": 0
                },
                {
                    "sent": "An 2008 human grandmaster level some.",
                    "label": 1
                },
                {
                    "sent": "And if you look at sort of 2005 to 2008, this is an ELO rating is sort of the power of a computer program on this ghost server.",
                    "label": 0
                },
                {
                    "sent": "It has a huge jump.",
                    "label": 0
                },
                {
                    "sent": "This is after years and years of work on go programs.",
                    "label": 0
                },
                {
                    "sent": "Yoga new go is been around for a long long time and it spend one of the better programs, but I think I think a lot of those guys were a bit annoyed that this stupid algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not really thinking.",
                    "label": 0
                },
                {
                    "sent": "Very much can beat the pants off them, but you know the it's not clear how far it will go, whether it will.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, go to the full 19 by 19 board.",
                    "label": 0
                },
                {
                    "sent": "So some other successes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, last year we were interested in Klondike Solitaire.",
                    "label": 0
                },
                {
                    "sent": "There were no sort of theoretical bounds on.",
                    "label": 0
                },
                {
                    "sent": "How many what the deck could be?",
                    "label": 0
                },
                {
                    "sent": "One?",
                    "label": 0
                },
                {
                    "sent": "What fraction of the decks could be one in Klondike Solitaire?",
                    "label": 0
                },
                {
                    "sent": "And it didn't seem very.",
                    "label": 0
                },
                {
                    "sent": "We tried to think about applying planning algorithms model based approaches, but it wasn't very promising.",
                    "label": 0
                },
                {
                    "sent": "So we tried a bunch of Monte Carlo techniques and UCT worked surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "40% of the games we did have a little bit of knowledge in the roll out policy, but ECT improved on that dramatically.",
                    "label": 0
                },
                {
                    "sent": "It's been used in this general game playing competition.",
                    "label": 1
                },
                {
                    "sent": "I think it's one of the better approaches now.",
                    "label": 0
                },
                {
                    "sent": "Maybe the best.",
                    "label": 0
                },
                {
                    "sent": "I haven't followed that too closely.",
                    "label": 1
                },
                {
                    "sent": "I've applied it to real time strategy games, combinatorial optimization.",
                    "label": 1
                },
                {
                    "sent": "I think maltez a paper at triple AI on this.",
                    "label": 0
                },
                {
                    "sent": "And so the list is growing.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty interesting algorithm.",
                    "label": 0
                },
                {
                    "sent": "But you know these.",
                    "label": 0
                },
                {
                    "sent": "These results usually come with some basic extension to UCT.",
                    "label": 0
                },
                {
                    "sent": "And so it's useful to know.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you want to do?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to enumerate them here.",
                    "label": 0
                },
                {
                    "sent": "But after you insert some sort of domain knowledge and this could be the base policy, that's the obvious place.",
                    "label": 0
                },
                {
                    "sent": "There's actually.",
                    "label": 0
                },
                {
                    "sent": "An interesting some interesting results with these base policies that you choose.",
                    "label": 0
                },
                {
                    "sent": "So you would think that if you have two based policies and one of those based policy's is better than the other, if you just play them head to head, you would think that maybe if you use the better one in UCT, it would do better than the worst one when using UCT.",
                    "label": 0
                },
                {
                    "sent": "And it turns out to not be the case.",
                    "label": 0
                },
                {
                    "sent": "So people that there's been some interesting results and go where you learn two policies, ones a lot better than the other.",
                    "label": 0
                },
                {
                    "sent": "You plug them into UCT and the performance is just reversed and there's not a lot of understanding of this sort of.",
                    "label": 0
                },
                {
                    "sent": "I think it's a pretty interesting open issue, especially when it comes to learning.",
                    "label": 0
                },
                {
                    "sent": "How do you do the proper learning problem for improving the base policy so that people do this?",
                    "label": 0
                },
                {
                    "sent": "They handcraft policy.",
                    "label": 0
                },
                {
                    "sent": "That's a big part of the ghost success.",
                    "label": 0
                },
                {
                    "sent": "You can also incorporate learned heuristic functions into UCT in various ways.",
                    "label": 0
                },
                {
                    "sent": "I'll have references.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the final slides.",
                    "label": 0
                },
                {
                    "sent": "So I'll just summarize, we're actually right on time, amazingly, not by design.",
                    "label": 0
                },
                {
                    "sent": "So when you have a tough planning problem in a simulator.",
                    "label": 1
                },
                {
                    "sent": "You might as well try Monte Carlo planning.",
                    "label": 0
                },
                {
                    "sent": "A lot of these things are not very hard to implement.",
                    "label": 0
                },
                {
                    "sent": "In fact, I should.",
                    "label": 0
                },
                {
                    "sent": "I put I'm developing right now.",
                    "label": 0
                },
                {
                    "sent": "I've got to developing a nice library for Java that has pretty standard interfaces for simulators and worlds, and these Monte Carlo planners will have some implementations of a bunch of domains.",
                    "label": 0
                },
                {
                    "sent": "We've got backgammon.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah, but a bunch of interesting domains connect four and will have implementations of UCT so pretty soon when this guy defends his thesis.",
                    "label": 0
                },
                {
                    "sent": "I'll make him put all of this online.",
                    "label": 0
                },
                {
                    "sent": "So so send me an email if you don't see it anytime soon.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "The basic principles of these algorithms can really be derived from the multi arm bandit.",
                    "label": 0
                },
                {
                    "sent": "And I would say that policy rollout gives you some really Big Bang for your Buck.",
                    "label": 0
                },
                {
                    "sent": "If you have a basic existing policy.",
                    "label": 0
                },
                {
                    "sent": "In most of the cases that I've tried it out, you get you get a significant gain if your base policy isn't close to optimal.",
                    "label": 0
                },
                {
                    "sent": "So if good heuristics exist, you can use shallow sparse sampling or the UCB variance and ECT is something that definitely people are just beginning to explore and understand the potential.",
                    "label": 0
                },
                {
                    "sent": "Done the tutorial.",
                    "label": 0
                },
                {
                    "sent": "If there are any questions, we have a few minutes.",
                    "label": 0
                },
                {
                    "sent": "Yes, one common questions.",
                    "label": 0
                },
                {
                    "sent": "So looks at our typical can be seen as a Monte Carlo planning, so it's just the entrance and so on, but it's definitely not policy rollout.",
                    "label": 0
                },
                {
                    "sent": "There's more invariant of value iteration that of policy iteration, so it's not doing, yeah.",
                    "label": 0
                },
                {
                    "sent": "My question is about the discount factor.",
                    "label": 0
                },
                {
                    "sent": "What is the meaning or what is?",
                    "label": 0
                },
                {
                    "sent": "The justification for the use of discount factors in problems where you really trying to reach a goal.",
                    "label": 0
                },
                {
                    "sent": "So I actually went through a number of iterations or whether I should put the discount factor in or not.",
                    "label": 0
                },
                {
                    "sent": "Typically don't, so there's a required factors well.",
                    "label": 0
                },
                {
                    "sent": "Not really so.",
                    "label": 0
                },
                {
                    "sent": "So if you want to be in a finite horizon setting, so if you fix age there are a whole bunch of ways to deal with this.",
                    "label": 0
                },
                {
                    "sent": "If you fix H. You're suddenly in a non stationary policy setting.",
                    "label": 0
                },
                {
                    "sent": "All of these things sort of hold there.",
                    "label": 0
                },
                {
                    "sent": "So UCT was actually designed for the.",
                    "label": 0
                },
                {
                    "sent": "Fixed 8 Horizon Total horizon reward and they mentioned well if you have a discount factor you can make each large enough.",
                    "label": 0
                },
                {
                    "sent": "You can also make it if you don't want to fix H, you can make an assumption that for all policies they'll eventually terminate, then picking the value for H. That gives you a guarantee.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's straightforward, heuristically you just have to pick it, so those are sort of the basic ways that you would deal with that.",
                    "label": 0
                },
                {
                    "sent": "Your suggestion that a good initial policy may not result in the good best final policy as a very strong local optimal kind of feel that maybe it's a local policy anyway.",
                    "label": 0
                },
                {
                    "sent": "If you increase improve it further, which is a local optimal.",
                    "label": 0
                },
                {
                    "sent": "But in another policy measure, better optimal has had people thought of connection of you city with local search at all.",
                    "label": 0
                },
                {
                    "sent": "Or maybe taking some of those ideas of random restarts and stuff.",
                    "label": 0
                },
                {
                    "sent": "So one thing I wish I had been able to put in here is I've got some work right now.",
                    "label": 0
                },
                {
                    "sent": "On ensembles of these UCT trees, so the one problem you can run into is.",
                    "label": 0
                },
                {
                    "sent": "If the beginning trajectories are unlucky and you see tree ECT, it's hard to recover quickly.",
                    "label": 0
                },
                {
                    "sent": "So you can use random restarts, so if you have a fixed budget of trajectories.",
                    "label": 0
                },
                {
                    "sent": "Allocate them to different trees and average the results.",
                    "label": 0
                },
                {
                    "sent": "Sort of like ensemble methods in machine learning and in my case I've always found that this does as well for a reasonable number of total trajectories as a single large tree.",
                    "label": 0
                },
                {
                    "sent": "Sometimes better, but with much less memory, so you only generate these small trees.",
                    "label": 0
                },
                {
                    "sent": "You throw them away, so the only thing I know of, I don't think this issue is really well understood.",
                    "label": 0
                },
                {
                    "sent": "It's just been observed.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "Alright well thanks.",
                    "label": 0
                }
            ]
        }
    }
}