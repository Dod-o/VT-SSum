{
    "id": "jfayddypkx4xbrwe3pwvjoc4urjjb4gx",
    "title": "Discussion of Igor Pruenster\u00b4s talk",
    "info": {
        "author": [
            "Peter Orbanz, Institute of Computational Science, ETH Zurich"
        ],
        "published": "Jan. 24, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_orbanz_discussant/",
    "segmentation": [
        [
            "Comment on a few points that of the many, many points that you made."
        ],
        [
            "And try to put them into a bit of context.",
            "Machine learning, so one thing that also came up this morning and zubin's talk in a question is is the point of exchange ability and this morning basically Zuben said, yeah, maybe Exchangeability is a strong assumption and that's a real restriction for based on parametric models, and I just would like to point out that exchange ability is not just a technical assumption like like an IID assumption.",
            "It's not just a simplification that allows us to decouple variables and so fit a model more nicely, but.",
            "There's a.",
            "There's another meaning to it, and that is the basic question whether we can do any kind of prediction.",
            "So if we want to do prediction or generalization, let's just say prediction where we have the past.",
            "If we want to predict the future, then we need some kind of assumption that basically what happened in the past tells us something about the future and this relates.",
            "Sorry, and philosophies as relates to humans induction problem.",
            "There is a million and one proofs that induction in general is impossible.",
            "Ising copper producers when every second weekend or something.",
            "And you know, I'll leave this to the people right next door.",
            "But basically Humes induction problem says, can we if we have if we have some examples of.",
            "What happens of reality doesn't tell us anything about what's going to happen in the future, and in general, no.",
            "If you don't have any kind of underlying structure that is preserved from the past into the future, then it doesn't, and exchangeability formalizes exactly that, and in the context of DEFINITY theorem, this basically means that prediction is is, so we could reformulate exchangeability indefinite theorem by saying prediction is possible if the underlying pattern.",
            "So this this infinite dimensional random quantity, indefinitely theorem that decouples the data.",
            "If that is preserved overtime in some sense, so for non exchangeable data, what can we hope to do?",
            "Well if you have non exchangeable data then you have to think very carefully.",
            "How you can justify prediction or out of sample generalization and what it will usually mean.",
            "What you will usually end up with is that you substitute the simple form of sequential exchangeability where you have a sequence of points and these are exchangeable with some other form of permutation invariants or symmetry.",
            "So you will find that, for example, if you say OK, my my points are not exchangeable because I have time series kind of data, But basically maybe maybe chunks of my sequence are exchangeable.",
            "So I have some coherence over blocks, and then these blocks are exchangeable.",
            "Then again, you get a different type theorem, which is, which is the Diaconis frequency Roman.",
            "You have a mixture of Markov chains instead of factorial distributions, But basically you will end up with some form of exchangeability once again."
        ],
        [
            "So the next point.",
            "Relates to this this point that you were made about about consistency that in order to.",
            "If we if we have, if we look at what our models are actually meant for, then they will in general be consistent.",
            "So if we look again at the phonetic theorem.",
            "We have this quantity here.",
            "This is random probability measure which which decouples our data and the original idea of Bayesian nonparametric was like a very, very general idea.",
            "Kind of a bit philosophical approach, maybe where people said let's model this distribution directly.",
            "Yeah, so the original idea was directly process was can we.",
            "Can we put?",
            "Can we model this probability distribution Q here and somehow sample this probability P sample our likelihood directly?",
            "From some probability distribution, and then we plug that into this product here and we can if we can do that.",
            "If we can produce a prior here which can produce any probability measure, then we can model any exchangeable distribution basically.",
            "So the directly process in this kind of approach would be this distribution Q here.",
            "OK, and then in the 1990s.",
            "So this this idea?",
            "Got out of hand a little bit, I would say because there was this idea that you could somehow build universal inference approach so there is a very interesting discussion by David Draper on that in this.",
            "In the in the discussion of his discussion of this summary paper survey paper by Walker, Damien, Lot and Smiths in 1999.",
            "So the idea is if you can do this, this very general approach if we have.",
            "If you have a prior that can essentially produce any probability measure and then we have the Gibbs sampler.",
            "These ideas developed in the wake of the Gibbs sampler, then we kind of have universal inference engine.",
            "We can throw any kind of data edit and we will get out some form of result and the.",
            "I would just like to point out that the approach we're taking today in modeling is innocence.",
            "Very, very different.",
            "And that is that we are not actually using a space of probability measures here, but instead we're using a parameter space and we re parameterize this.",
            "This integral here essentially so that this year is a set of patterns that we're interested in.",
            "If you have, you may have seen our tutorial and we call these parameters patterns so the parameter is this years ago, and process for example.",
            "Then the parameter that we draw from that.",
            "This random quantity is actually a function, so these measures get re parameterized in terms of of a parameter, which is a function, and so we build our models in a way that that what the model generates, what the prior generates is a kind of quantity that solves our problem like a functional regression problem, for example and.",
            "And then if we if we if we get things from this perspective then basically in a lot of cases it's possible to show consistency.",
            "OK. And then, in the context of all this consistency and convergence results that are now kind of shown in mathematical statistics for basic nonparametric models, I think.",
            "And in machine learning it's may be a good idea to ask ourselves what do we know about our models?",
            "Because our models are slightly different from the models that are usually studied in mathematical statistics or mathematical statistics.",
            "Models that have gotten a lot of attention is directed process and the directive process mixture true and also the Gaussian process.",
            "But in machine learning we have models that usually rely on a lot of latent variables, right?",
            "If you look at the average kind of nonparametric Bayesian model published here NIPS and it's some model that has, you know several layers of hierarchy where we have like.",
            "And ICP on top of a DP on top of coalescent or whatever and there is a lot of layers of maybe several layers of latent variables.",
            "And even if we just have a simple directive process mixture, then in machine learning we are not really.",
            "We're not only interested in how well this model.",
            "Converges as we try to estimate the density.",
            "But really, what the latent variables to the latent variables are, the cluster parameters and we are interested in the clustering, right and?",
            "So what we would need is kind of different consistency results or convergence results which look at how well the model does DES converge with respect to the latent variables.",
            "Do the other latent variables recovered?",
            "And for example for?",
            "For directly process mixture problem, if we say we have for example we have a, we have a density.",
            "We apply directly process mixtures of normals on the line or something like that and then we have a consistency result.",
            "But this consistency result says is that the density that we get by summing up the mixture.",
            "That that is a good estimator essentially does not say that the underlying mixing distribution is recovered correctly, and at least I'm not aware of any results which say something like this.",
            "I talked about it with the grass this morning and basically, unless unless you make very restrictive assumptions, I don't think there are any results on that, and this is something that I think we have to think about in machine learning.",
            "If we're serious about Bayesian non parametrics because I don't think that mathematical statisticians are going to solve this problem for us.",
            "They are not so much interested in these models and.",
            "I don't think it's something that we can't do becauses in if you think back to two classification problems, for example, the model a lot of important contributions along these lines have been made in learning theory.",
            "Actually.",
            "So machine learning people have produced these results dress a nonparametric base somehow.",
            "Learning theorists have not really gotten interested in it for some for some reason or other."
        ],
        [
            "OK, and the final point I would like to mention is.",
            "In the same context, what do we know about our models?",
            "This is something that I'm very concerned about, and I know from discussions at dinner last night was the organizers in Eric's orders, for example, that they're also kind of concerned about this is.",
            "When we study our models, we rely a lot on Gibbs sampling for these latent variable models and.",
            "One thing is that there is not really that much systematic work about how you should do that, right?",
            "I mean, kind of.",
            "If you imagine yourself an outsider and you see OK, there's a lot of work on unformatted based machine learning.",
            "What kind of papers would you expect?",
            "I guess I would expect like a lot of papers on how do you do Gibbs sampling properly?",
            "How do you assess convergence and so on?",
            "And there's not really much about that, right?",
            "I think Murray is working on that and Eric's orders, and that's that's kind of about it and.",
            "I wonder, I keep wondering for how many of the models that we that we have seen published we have actually ever seen a sample that comes from the actual model and it's not just burning and.",
            "That is especially true for hierarchical models, because when we start building a hierarchy then we dressed, it could blow up the state space at our assembly has to explore right and.",
            "I had a discussion about this with.",
            "With colleagues in Cambridge recently and and the answer that I got was, well, that is not really that much of a problem.",
            "We shouldn't stress about that because as long as our predicted performance is good, what does it matter whether our chain is actually mixed or not?",
            "And I kind of appreciate the point from a purely engineering point of view.",
            "I think from a scientific point of view it's very unsatisfying because we want to understand what the models are doing right?",
            "We don't just want to design them one off and then throw them away an.",
            "I think that actually also from an engineering point of view.",
            "This kind of this kind of ideas is dangerous because we can drastically misinterpret our models and in order to make that point, I borrowed a slide from Eric Sudderth, which is kind of my favorite slide ever.",
            "I think it's.",
            "So.",
            "This relates to the Markov random field model.",
            "Set Potts model that Eric talked about earlier as well, and basically in."
        ],
        [
            "Be in the Demon Demon 1984 paper where this was introduced.",
            "And which I think 30 years on still accounts for most of the impact factor of Palmi we have.",
            "There is this idea that you can use a Markov random field for image segmentation and they draw samples from Markov random field simply from the prior Markov random field.",
            "Prior they drop samples of patches like this and what comes out are like these decompositions of this image kind of image Patch into coherent regions and then the argument is that.",
            "That means that a Markov random field is in some sense a natural prior on segmentations of images, because this looks kind of like a segmentation of an image right?",
            "So and what Eric did here?",
            "Is he simply reproduced there?",
            "The experimental setup?",
            "He ran 5 Markov chains for 200 iterations as agreements did in 1984, with like more limited computing power, and this is what you see and then you continue d'etre running these change and change for 10,000 iterations.",
            "And this is what you see.",
            "It's just black, right?",
            "So in a sense, this really makes a lot of sense because Markov random field is a smoothness prior and if you sample from it and what you see should better be smooth, right?",
            "And this year is not smooth, but I think the important lesson here is that.",
            "If we just say OK.",
            "This has mixed for fine as long as the predicted performance is right, then we still completely misinterpret what these models mean.",
            "And.",
            "I guess I I mean, I guess if you want to, if you write software and ship it for your model, you don't want to ship.",
            "It was a warning that you know don't run this for more than 200 iterations because it's going to look weird.",
            "But I think this is not just a scientific issue, but this is also an engineering issue.",
            "Yeah, thank you, that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comment on a few points that of the many, many points that you made.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And try to put them into a bit of context.",
                    "label": 0
                },
                {
                    "sent": "Machine learning, so one thing that also came up this morning and zubin's talk in a question is is the point of exchange ability and this morning basically Zuben said, yeah, maybe Exchangeability is a strong assumption and that's a real restriction for based on parametric models, and I just would like to point out that exchange ability is not just a technical assumption like like an IID assumption.",
                    "label": 0
                },
                {
                    "sent": "It's not just a simplification that allows us to decouple variables and so fit a model more nicely, but.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's another meaning to it, and that is the basic question whether we can do any kind of prediction.",
                    "label": 0
                },
                {
                    "sent": "So if we want to do prediction or generalization, let's just say prediction where we have the past.",
                    "label": 0
                },
                {
                    "sent": "If we want to predict the future, then we need some kind of assumption that basically what happened in the past tells us something about the future and this relates.",
                    "label": 0
                },
                {
                    "sent": "Sorry, and philosophies as relates to humans induction problem.",
                    "label": 0
                },
                {
                    "sent": "There is a million and one proofs that induction in general is impossible.",
                    "label": 0
                },
                {
                    "sent": "Ising copper producers when every second weekend or something.",
                    "label": 0
                },
                {
                    "sent": "And you know, I'll leave this to the people right next door.",
                    "label": 0
                },
                {
                    "sent": "But basically Humes induction problem says, can we if we have if we have some examples of.",
                    "label": 1
                },
                {
                    "sent": "What happens of reality doesn't tell us anything about what's going to happen in the future, and in general, no.",
                    "label": 0
                },
                {
                    "sent": "If you don't have any kind of underlying structure that is preserved from the past into the future, then it doesn't, and exchangeability formalizes exactly that, and in the context of DEFINITY theorem, this basically means that prediction is is, so we could reformulate exchangeability indefinite theorem by saying prediction is possible if the underlying pattern.",
                    "label": 1
                },
                {
                    "sent": "So this this infinite dimensional random quantity, indefinitely theorem that decouples the data.",
                    "label": 0
                },
                {
                    "sent": "If that is preserved overtime in some sense, so for non exchangeable data, what can we hope to do?",
                    "label": 0
                },
                {
                    "sent": "Well if you have non exchangeable data then you have to think very carefully.",
                    "label": 0
                },
                {
                    "sent": "How you can justify prediction or out of sample generalization and what it will usually mean.",
                    "label": 1
                },
                {
                    "sent": "What you will usually end up with is that you substitute the simple form of sequential exchangeability where you have a sequence of points and these are exchangeable with some other form of permutation invariants or symmetry.",
                    "label": 0
                },
                {
                    "sent": "So you will find that, for example, if you say OK, my my points are not exchangeable because I have time series kind of data, But basically maybe maybe chunks of my sequence are exchangeable.",
                    "label": 0
                },
                {
                    "sent": "So I have some coherence over blocks, and then these blocks are exchangeable.",
                    "label": 0
                },
                {
                    "sent": "Then again, you get a different type theorem, which is, which is the Diaconis frequency Roman.",
                    "label": 0
                },
                {
                    "sent": "You have a mixture of Markov chains instead of factorial distributions, But basically you will end up with some form of exchangeability once again.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next point.",
                    "label": 0
                },
                {
                    "sent": "Relates to this this point that you were made about about consistency that in order to.",
                    "label": 0
                },
                {
                    "sent": "If we if we have, if we look at what our models are actually meant for, then they will in general be consistent.",
                    "label": 0
                },
                {
                    "sent": "So if we look again at the phonetic theorem.",
                    "label": 0
                },
                {
                    "sent": "We have this quantity here.",
                    "label": 0
                },
                {
                    "sent": "This is random probability measure which which decouples our data and the original idea of Bayesian nonparametric was like a very, very general idea.",
                    "label": 0
                },
                {
                    "sent": "Kind of a bit philosophical approach, maybe where people said let's model this distribution directly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the original idea was directly process was can we.",
                    "label": 0
                },
                {
                    "sent": "Can we put?",
                    "label": 0
                },
                {
                    "sent": "Can we model this probability distribution Q here and somehow sample this probability P sample our likelihood directly?",
                    "label": 0
                },
                {
                    "sent": "From some probability distribution, and then we plug that into this product here and we can if we can do that.",
                    "label": 0
                },
                {
                    "sent": "If we can produce a prior here which can produce any probability measure, then we can model any exchangeable distribution basically.",
                    "label": 0
                },
                {
                    "sent": "So the directly process in this kind of approach would be this distribution Q here.",
                    "label": 1
                },
                {
                    "sent": "OK, and then in the 1990s.",
                    "label": 0
                },
                {
                    "sent": "So this this idea?",
                    "label": 0
                },
                {
                    "sent": "Got out of hand a little bit, I would say because there was this idea that you could somehow build universal inference approach so there is a very interesting discussion by David Draper on that in this.",
                    "label": 0
                },
                {
                    "sent": "In the in the discussion of his discussion of this summary paper survey paper by Walker, Damien, Lot and Smiths in 1999.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if you can do this, this very general approach if we have.",
                    "label": 1
                },
                {
                    "sent": "If you have a prior that can essentially produce any probability measure and then we have the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "These ideas developed in the wake of the Gibbs sampler, then we kind of have universal inference engine.",
                    "label": 1
                },
                {
                    "sent": "We can throw any kind of data edit and we will get out some form of result and the.",
                    "label": 0
                },
                {
                    "sent": "I would just like to point out that the approach we're taking today in modeling is innocence.",
                    "label": 0
                },
                {
                    "sent": "Very, very different.",
                    "label": 0
                },
                {
                    "sent": "And that is that we are not actually using a space of probability measures here, but instead we're using a parameter space and we re parameterize this.",
                    "label": 0
                },
                {
                    "sent": "This integral here essentially so that this year is a set of patterns that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "If you have, you may have seen our tutorial and we call these parameters patterns so the parameter is this years ago, and process for example.",
                    "label": 0
                },
                {
                    "sent": "Then the parameter that we draw from that.",
                    "label": 0
                },
                {
                    "sent": "This random quantity is actually a function, so these measures get re parameterized in terms of of a parameter, which is a function, and so we build our models in a way that that what the model generates, what the prior generates is a kind of quantity that solves our problem like a functional regression problem, for example and.",
                    "label": 0
                },
                {
                    "sent": "And then if we if we if we get things from this perspective then basically in a lot of cases it's possible to show consistency.",
                    "label": 0
                },
                {
                    "sent": "OK. And then, in the context of all this consistency and convergence results that are now kind of shown in mathematical statistics for basic nonparametric models, I think.",
                    "label": 0
                },
                {
                    "sent": "And in machine learning it's may be a good idea to ask ourselves what do we know about our models?",
                    "label": 0
                },
                {
                    "sent": "Because our models are slightly different from the models that are usually studied in mathematical statistics or mathematical statistics.",
                    "label": 0
                },
                {
                    "sent": "Models that have gotten a lot of attention is directed process and the directive process mixture true and also the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "But in machine learning we have models that usually rely on a lot of latent variables, right?",
                    "label": 0
                },
                {
                    "sent": "If you look at the average kind of nonparametric Bayesian model published here NIPS and it's some model that has, you know several layers of hierarchy where we have like.",
                    "label": 0
                },
                {
                    "sent": "And ICP on top of a DP on top of coalescent or whatever and there is a lot of layers of maybe several layers of latent variables.",
                    "label": 0
                },
                {
                    "sent": "And even if we just have a simple directive process mixture, then in machine learning we are not really.",
                    "label": 0
                },
                {
                    "sent": "We're not only interested in how well this model.",
                    "label": 0
                },
                {
                    "sent": "Converges as we try to estimate the density.",
                    "label": 0
                },
                {
                    "sent": "But really, what the latent variables to the latent variables are, the cluster parameters and we are interested in the clustering, right and?",
                    "label": 0
                },
                {
                    "sent": "So what we would need is kind of different consistency results or convergence results which look at how well the model does DES converge with respect to the latent variables.",
                    "label": 0
                },
                {
                    "sent": "Do the other latent variables recovered?",
                    "label": 0
                },
                {
                    "sent": "And for example for?",
                    "label": 0
                },
                {
                    "sent": "For directly process mixture problem, if we say we have for example we have a, we have a density.",
                    "label": 0
                },
                {
                    "sent": "We apply directly process mixtures of normals on the line or something like that and then we have a consistency result.",
                    "label": 0
                },
                {
                    "sent": "But this consistency result says is that the density that we get by summing up the mixture.",
                    "label": 0
                },
                {
                    "sent": "That that is a good estimator essentially does not say that the underlying mixing distribution is recovered correctly, and at least I'm not aware of any results which say something like this.",
                    "label": 0
                },
                {
                    "sent": "I talked about it with the grass this morning and basically, unless unless you make very restrictive assumptions, I don't think there are any results on that, and this is something that I think we have to think about in machine learning.",
                    "label": 0
                },
                {
                    "sent": "If we're serious about Bayesian non parametrics because I don't think that mathematical statisticians are going to solve this problem for us.",
                    "label": 0
                },
                {
                    "sent": "They are not so much interested in these models and.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's something that we can't do becauses in if you think back to two classification problems, for example, the model a lot of important contributions along these lines have been made in learning theory.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So machine learning people have produced these results dress a nonparametric base somehow.",
                    "label": 0
                },
                {
                    "sent": "Learning theorists have not really gotten interested in it for some for some reason or other.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and the final point I would like to mention is.",
                    "label": 0
                },
                {
                    "sent": "In the same context, what do we know about our models?",
                    "label": 1
                },
                {
                    "sent": "This is something that I'm very concerned about, and I know from discussions at dinner last night was the organizers in Eric's orders, for example, that they're also kind of concerned about this is.",
                    "label": 0
                },
                {
                    "sent": "When we study our models, we rely a lot on Gibbs sampling for these latent variable models and.",
                    "label": 0
                },
                {
                    "sent": "One thing is that there is not really that much systematic work about how you should do that, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, kind of.",
                    "label": 0
                },
                {
                    "sent": "If you imagine yourself an outsider and you see OK, there's a lot of work on unformatted based machine learning.",
                    "label": 0
                },
                {
                    "sent": "What kind of papers would you expect?",
                    "label": 0
                },
                {
                    "sent": "I guess I would expect like a lot of papers on how do you do Gibbs sampling properly?",
                    "label": 0
                },
                {
                    "sent": "How do you assess convergence and so on?",
                    "label": 0
                },
                {
                    "sent": "And there's not really much about that, right?",
                    "label": 0
                },
                {
                    "sent": "I think Murray is working on that and Eric's orders, and that's that's kind of about it and.",
                    "label": 0
                },
                {
                    "sent": "I wonder, I keep wondering for how many of the models that we that we have seen published we have actually ever seen a sample that comes from the actual model and it's not just burning and.",
                    "label": 0
                },
                {
                    "sent": "That is especially true for hierarchical models, because when we start building a hierarchy then we dressed, it could blow up the state space at our assembly has to explore right and.",
                    "label": 0
                },
                {
                    "sent": "I had a discussion about this with.",
                    "label": 0
                },
                {
                    "sent": "With colleagues in Cambridge recently and and the answer that I got was, well, that is not really that much of a problem.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't stress about that because as long as our predicted performance is good, what does it matter whether our chain is actually mixed or not?",
                    "label": 0
                },
                {
                    "sent": "And I kind of appreciate the point from a purely engineering point of view.",
                    "label": 0
                },
                {
                    "sent": "I think from a scientific point of view it's very unsatisfying because we want to understand what the models are doing right?",
                    "label": 0
                },
                {
                    "sent": "We don't just want to design them one off and then throw them away an.",
                    "label": 0
                },
                {
                    "sent": "I think that actually also from an engineering point of view.",
                    "label": 0
                },
                {
                    "sent": "This kind of this kind of ideas is dangerous because we can drastically misinterpret our models and in order to make that point, I borrowed a slide from Eric Sudderth, which is kind of my favorite slide ever.",
                    "label": 0
                },
                {
                    "sent": "I think it's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This relates to the Markov random field model.",
                    "label": 0
                },
                {
                    "sent": "Set Potts model that Eric talked about earlier as well, and basically in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be in the Demon Demon 1984 paper where this was introduced.",
                    "label": 0
                },
                {
                    "sent": "And which I think 30 years on still accounts for most of the impact factor of Palmi we have.",
                    "label": 0
                },
                {
                    "sent": "There is this idea that you can use a Markov random field for image segmentation and they draw samples from Markov random field simply from the prior Markov random field.",
                    "label": 0
                },
                {
                    "sent": "Prior they drop samples of patches like this and what comes out are like these decompositions of this image kind of image Patch into coherent regions and then the argument is that.",
                    "label": 0
                },
                {
                    "sent": "That means that a Markov random field is in some sense a natural prior on segmentations of images, because this looks kind of like a segmentation of an image right?",
                    "label": 0
                },
                {
                    "sent": "So and what Eric did here?",
                    "label": 0
                },
                {
                    "sent": "Is he simply reproduced there?",
                    "label": 0
                },
                {
                    "sent": "The experimental setup?",
                    "label": 0
                },
                {
                    "sent": "He ran 5 Markov chains for 200 iterations as agreements did in 1984, with like more limited computing power, and this is what you see and then you continue d'etre running these change and change for 10,000 iterations.",
                    "label": 0
                },
                {
                    "sent": "And this is what you see.",
                    "label": 0
                },
                {
                    "sent": "It's just black, right?",
                    "label": 0
                },
                {
                    "sent": "So in a sense, this really makes a lot of sense because Markov random field is a smoothness prior and if you sample from it and what you see should better be smooth, right?",
                    "label": 0
                },
                {
                    "sent": "And this year is not smooth, but I think the important lesson here is that.",
                    "label": 0
                },
                {
                    "sent": "If we just say OK.",
                    "label": 0
                },
                {
                    "sent": "This has mixed for fine as long as the predicted performance is right, then we still completely misinterpret what these models mean.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I guess I I mean, I guess if you want to, if you write software and ship it for your model, you don't want to ship.",
                    "label": 0
                },
                {
                    "sent": "It was a warning that you know don't run this for more than 200 iterations because it's going to look weird.",
                    "label": 0
                },
                {
                    "sent": "But I think this is not just a scientific issue, but this is also an engineering issue.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you, that's it.",
                    "label": 0
                }
            ]
        }
    }
}