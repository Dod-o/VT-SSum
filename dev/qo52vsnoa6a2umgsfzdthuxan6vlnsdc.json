{
    "id": "qo52vsnoa6a2umgsfzdthuxan6vlnsdc",
    "title": "Combining Data Sources Nonlinearly for Cell Nucleus Classification of Renal Cell Carcinoma",
    "info": {
        "author": [
            "Mehmet G\u00f6nen, Department of Information and Computer Science, Aalto University"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Multiple Kernel Learning"
        ]
    },
    "url": "http://videolectures.net/simbad2011_gonen_carcinoma/",
    "segmentation": [
        [
            "Hi, I won't repeat the long title again.",
            "Basically I will describe the nonlinear MKL methods, multiple kernel learning methods today."
        ],
        [
            "This is the outline of my talk.",
            "I will introduce are not.",
            "I will introduce our methodology first, then I will describe the experimental result on the renal cell carcinoma data set."
        ],
        [
            "OK, basically in machine learning kernel based algorithms uses kernel function instead of similar as similarity measures and the kernel function used is the main factor.",
            "Of the empirical performance for that classifier, and generally we can do cross validation strategy to pick the best kernel.",
            "Than we use this kernel in our predictions, but instead of picking the best kernel, we can combine different kernels using multiple kernel learning."
        ],
        [
            "And our contribution is formulating a nonlinear amcar variant for the large margin classification case, and we tested it on cell nucleus classification of RCC datasets and basically we combine different feature representations obtained from TMA images, and we compared our MCL variants with single kernel lesbians and also it.",
            "Benchmark linear MCL algorithms."
        ],
        [
            "OK, as I mentioned earlier.",
            "We have the option of picking the best single kernel using cross validation on some subset of the training data.",
            "But instead of this we can combine different kernels, and by doing this we can combine different similarity measures on the same data representation.",
            "So we can basically be combined.",
            "We can combine different kernel functions they can be.",
            "The same kernel function with different parameters.",
            "For example, we can combine different codes in kernels with different radius parameters.",
            "Also, we can combine different feature representations using the same kernel.",
            "For example, if you have different data sources or modalities, speak and combine them using the same kernel, or we can.",
            "Have a hybrid approach of these two possibilities.",
            "Also we can combine different similarity measures and different feature repres."
        ],
        [
            "Tations also.",
            "In multiple kernel learning we basically have P different kernel functions and this can be.",
            "Different kernel functions or different data sources, and these P can be a very large number, or possibly P can be infinite for the Gaussian kernel.",
            "For example, we can have.",
            "Infinite possibilities for the Gaussian kernel radius parameter.",
            "Basically we are trying to learn some combination function which is called F ETA and this function has some kind of.",
            "Parametric form and the parameters are denoted by as denoted by ETA.",
            "At the end we learn these model parameters and basically these model parameters are used to combine these kernels into a better one, which is called K ETA in our notation."
        ],
        [
            "First of all, we have different.",
            "Possibilities to construct kernels from the existing kernels.",
            "But these are the three simple procedures.",
            "First, we can scale a kernel with a positive number to obtain another kernel, or we can sum up two kernels talked in another, or we can multiply 2 different kernels to obtain a new one."
        ],
        [
            "And in the literature there are many MKL algorithms proposed, but the majority of these algorithms use some kind of linear combination in linear combination we have combination parameter for each of these kernels and we will basically take a weighted sum of input kernels.",
            "There are."
        ],
        [
            "Three different approach.",
            "There are three different priors on these kernel weights.",
            "For example, we can combine them linearly, which means we can put arbitrary kernel weights, but there is a problem with this approach.",
            "We are not guaranteed to have a positive semidefinite kernel at the end.",
            "If these kernel parameters are not restricted to be positive.",
            "At least none negative and the second combination option is conic combination in this.",
            "Approach we have positive kernel weights and we are guaranteed to have a positive semidefinite kernel at the end.",
            "The last one is putting some unknown constraint on the kernel weights.",
            "In that case we have citric Lee, positive kernel rates and they sum up to one."
        ],
        [
            "Instead of using these linear approach, we can combine our kernels within and near approach in this approach.",
            "Basically we will take entry, wised up entry wise products of kernels, but we consider all possible pairs of kernels and as you can see from the figure we have P * P different kernels that can be obtained using.",
            "Multiplication between pairs, but Please note that we don't have any individual parameter for each of these.",
            "Like linear amcar methods, we have a single parameter for each kernel, but then we are multiplying two different kernels.",
            "We also multiply their kernel parameters.",
            "So in order to reduce the possibility of overfitting, instead of using P square.",
            "Kernel weights we basically use just P different parameters.",
            "For example, if 8 two is equal to 0, this second kernel will be eliminated from all possibilities right?",
            "The second column and the 2nd row will be eliminated in the combination."
        ],
        [
            "This one.",
            "Basically have peace, require different input kernels, but we have.",
            "Multiplication of kernel weights as their weight, and we basically sum.",
            "These piece required different kernels in the combination funk."
        ],
        [
            "OK, we modified the linear ramchal optimization problem and in this.",
            "Form we have min Max optimization problem.",
            "Basically we are minimizing over kernel weights and maximizing over support vector coefficients.",
            "As you can see this is not a convex optimization problem cause the objective function is not jointly convex in ETA and also Alpha.",
            "So we use some kind of alternating optimization procedure.",
            "In this procedure, we are fixing these kernel weights and so where Canonical SVM optimization problem and at each generation we are updating these kernel weights using some projection based gradient descent algorithm.",
            "As you can see, the gradients with respect to.",
            "Colonel rates can be calculated using the input kernels, which are the multiplication of pairs of different kernels and also support vector coefficients at the current iteration an at.",
            "At each iteration we are projecting them too.",
            "Simplex again, to preserve the feasibility for kernel weights."
        ],
        [
            "OK, I will briefly mention the data set we used because I don't already mentioned the data set in the previous session.",
            "We have ATM images from 8 patients and.",
            "Nuclear extraction is performed by two pathologists.",
            "We have more than 1500 pages in total and pathologists agreed on more than 1200 patches and we are using these data points in our classifiers."
        ],
        [
            "And these are the eight different feature descripcion.",
            "Extracted from these datasets and all of them are histogram based feature representations."
        ],
        [
            "And in our experiments we did comfort stratified cross validation for this eight patients and we have 8 feature representation in total and we use three basic kernel functions, linear kernel, second degree, polynomial kernel and Gaussian kernel."
        ],
        [
            "And in our experiments, we compared these five approach.",
            "The first one is training.",
            "An individual SVM on each of the feature representations separately, so we have eight different SVM classifiers.",
            "We have rule based and Carol approach which basically means that we are training a single kernel SVM using the average of the input kernels.",
            "Simple and Carol is the benchmark linear amcar logarithm used in the literature very much and group last Sam Carroll is a is another variant for linear amcar approach, which basically use group plus plus.",
            "So regularization on the kernel weights.",
            "And the last one is out, nonlinear and calvaria."
        ],
        [
            "It's.",
            "If you look at the results with single kernel SVM, we see that.",
            "Pyramid histogram based classification is very much better than the other possibilities, and we can obtain more than 75% accuracy by using linear or Gaussian kernel only representation.",
            "But if you look at the classification results, we see that all feature representations carry some information.",
            "None of them is very bad in terms of classification."
        ],
        [
            "And this is.",
            "This is a bad data set for linear amcar algorithms.",
            "Be cause if all feature representation or of similarity measures carry some kind of information about the classification task.",
            "We cannot beat rule based uncared algorithm using linear MCL.",
            "OK, rule based and Carol is basically SVM beach trains.",
            "These dual optimization problem by using the simple mean of the input kernels.",
            "Then if you look at the results, this simple MK land group loss and Carol is very much the same with the mean of these kernels.",
            "OK, so we can.",
            "See the increase if we use nonlinear version instead of linear amcar version.",
            "If you look at the results with Gaussian kernels.",
            "Basically we are combining eight different codes in kernels.",
            "And we can improve the classification by around 6% compared to single kernel SVM and more than 1% for Linear AM Carol versions.",
            "OK, one another thing if it rain.",
            "These uncared algorithms, using all possible 24 kernels.",
            "We see that again our nonlinear version is better than the linear kernels.",
            "And in all possible.",
            "Journal selections the nonlinear version is better than the other approach."
        ],
        [
            "OK, if you look at the training times.",
            "We see that an increase in the training time with these nonlinear Ram Kelly approach, but this is mainly because of these gradient calculation step.",
            "At each iteration we have to calculate all pairwise kernels again and again.",
            "If you have a large memory, we can catch them and by calculating these.",
            "Gradients on the fly.",
            "We can reduce these training time.",
            "To the level for the linear MCL methods."
        ],
        [
            "In order to conclude that the we can see that our nonlinear version is better than single kernel SVM and linear ankle methods for this specific data set an for this specific data set, we can achieve better results using more complex combination approaches or adding new modalities.",
            "Other than these eight feature represented."
        ],
        [
            "Ocean.",
            "And we compared only with three different linear amcar algorithms.",
            "There are many uncared algorithms in the literature, and you can see a recent survey by going in and out Hayden, and also my MCR Matlab toolbox is available at on my website and these toolbar tool box contains more than 10 ancala algorithms you can use if you need some kind of.",
            "Carol implementation, thank you very much.",
            "Just sleeping.",
            "Many would put Carolyn columns.",
            "You use kernelization.",
            "I provided the current by the more educational.",
            "Use kernelization in your arms.",
            "Sorry, I forgot to mention in our experiments all kernels are normalized to unit trace unit diagonal before training.",
            "So to my understanding, one of the success stories and how methods was the usually these optimization problems are convex.",
            "My concern as we answer related algorithms.",
            "So now you you add an extension which finally Sunol complex problems.",
            "So somehow I mean, is it really worth going into this direction?",
            "I guess exactly it is worth to go to these directions, because in our survey paper we compared 17 MK logarithms and nonlinear algorithms.",
            "Significantly outperforms linear curve linear Ram, Kelly approaches because if you don't have any noisy Colonel in your classification task, you can achieve the best results by using the mean of the kernels, and there is no need to learn these kernel combination weights.",
            "In order to achieve good accuracy you need to go deeper into this combination function and formulate some new combination approaches instead of using different regularization terms on the same combination function which is linear, you cannot do much.",
            "Yes, but then you have to put it the other way around.",
            "So if you have if you have no loss, convexity anyway.",
            "So so, why are you still using kernels we can not use anything.",
            "They don't have to be positive definite program.",
            "But at each iteration we are solving an SVM problem.",
            "We need these positive semidefinite's for.",
            "The efficiency.",
            "For each step you are solving a conical SPM problem.",
            "So we have any information about the wireless work because you don't have more freedom, they are in your methods.",
            "Just have a vector of weights.",
            "So what is the?",
            "OK. Actually, it would correspond to linear amcar with.",
            "OK, it's been correspond to linear RAM care training.",
            "A linear ramp care if you use unique parameter for each possible kernel.",
            "Basically, we can construct these all possible pairs of kernels and we can assign them at separate wait.",
            "Basically we can train a linear and care with this type of setup, but any parameters to learn and.",
            "It is not equivalent because we are we don't have more degrees of freedom.",
            "Where does the extra performance OK Extra performs comes from the similarity measure.",
            "Because we are multiplying two different kernels, we are projecting time to a completely different feature space.",
            "Is there possibility that this is equivalent?",
            "To be repaired with another service.",
            "It may correspond to you.",
            "But not exactly becausw.",
            "OK yeah, if it rain linear amcar with all possible weights we can use some of the kernels.",
            "But for example if eight or two is zero we can eliminate all second row and 2nd column kernels.",
            "We cannot do the same thing with a linear approach.",
            "We are fine.",
            "Interpretation of your optimization problem in the sense that we put coloring in the linear case very combined linearly as should any interpretation.",
            "So you have different data sources.",
            "You can, for example, further, sample sources are more important than others.",
            "What would you say in your case, which there is the product and is more complex combination?",
            "OK, can you repeat the question?",
            "Yeah, the thing yeah yeah.",
            "Nice feature on the combining the related current occurrence in with what kind of learning is interpretation that you can get get at the end you optimize and the weights of atomization can be interpreted in some way.",
            "This case very seems less easy.",
            "Actually.",
            "We are not losing that information because if some of datas are bigger than others, we can see that these kernel this specific kernel carries more information than the others.",
            "Even if we use the pairwise kernels, we can.",
            "Basically we are giving more importance to detect specific kernel.",
            "Actually we have these interpretation capability with the Zetas also.",
            "Questions.",
            "OK, thank you sexy."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I won't repeat the long title again.",
                    "label": 0
                },
                {
                    "sent": "Basically I will describe the nonlinear MKL methods, multiple kernel learning methods today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will introduce are not.",
                    "label": 0
                },
                {
                    "sent": "I will introduce our methodology first, then I will describe the experimental result on the renal cell carcinoma data set.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, basically in machine learning kernel based algorithms uses kernel function instead of similar as similarity measures and the kernel function used is the main factor.",
                    "label": 0
                },
                {
                    "sent": "Of the empirical performance for that classifier, and generally we can do cross validation strategy to pick the best kernel.",
                    "label": 1
                },
                {
                    "sent": "Than we use this kernel in our predictions, but instead of picking the best kernel, we can combine different kernels using multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our contribution is formulating a nonlinear amcar variant for the large margin classification case, and we tested it on cell nucleus classification of RCC datasets and basically we combine different feature representations obtained from TMA images, and we compared our MCL variants with single kernel lesbians and also it.",
                    "label": 0
                },
                {
                    "sent": "Benchmark linear MCL algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, as I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "We have the option of picking the best single kernel using cross validation on some subset of the training data.",
                    "label": 1
                },
                {
                    "sent": "But instead of this we can combine different kernels, and by doing this we can combine different similarity measures on the same data representation.",
                    "label": 0
                },
                {
                    "sent": "So we can basically be combined.",
                    "label": 0
                },
                {
                    "sent": "We can combine different kernel functions they can be.",
                    "label": 1
                },
                {
                    "sent": "The same kernel function with different parameters.",
                    "label": 0
                },
                {
                    "sent": "For example, we can combine different codes in kernels with different radius parameters.",
                    "label": 0
                },
                {
                    "sent": "Also, we can combine different feature representations using the same kernel.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have different data sources or modalities, speak and combine them using the same kernel, or we can.",
                    "label": 1
                },
                {
                    "sent": "Have a hybrid approach of these two possibilities.",
                    "label": 0
                },
                {
                    "sent": "Also we can combine different similarity measures and different feature repres.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tations also.",
                    "label": 0
                },
                {
                    "sent": "In multiple kernel learning we basically have P different kernel functions and this can be.",
                    "label": 0
                },
                {
                    "sent": "Different kernel functions or different data sources, and these P can be a very large number, or possibly P can be infinite for the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "For example, we can have.",
                    "label": 0
                },
                {
                    "sent": "Infinite possibilities for the Gaussian kernel radius parameter.",
                    "label": 0
                },
                {
                    "sent": "Basically we are trying to learn some combination function which is called F ETA and this function has some kind of.",
                    "label": 0
                },
                {
                    "sent": "Parametric form and the parameters are denoted by as denoted by ETA.",
                    "label": 0
                },
                {
                    "sent": "At the end we learn these model parameters and basically these model parameters are used to combine these kernels into a better one, which is called K ETA in our notation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, we have different.",
                    "label": 0
                },
                {
                    "sent": "Possibilities to construct kernels from the existing kernels.",
                    "label": 0
                },
                {
                    "sent": "But these are the three simple procedures.",
                    "label": 0
                },
                {
                    "sent": "First, we can scale a kernel with a positive number to obtain another kernel, or we can sum up two kernels talked in another, or we can multiply 2 different kernels to obtain a new one.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the literature there are many MKL algorithms proposed, but the majority of these algorithms use some kind of linear combination in linear combination we have combination parameter for each of these kernels and we will basically take a weighted sum of input kernels.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three different approach.",
                    "label": 0
                },
                {
                    "sent": "There are three different priors on these kernel weights.",
                    "label": 1
                },
                {
                    "sent": "For example, we can combine them linearly, which means we can put arbitrary kernel weights, but there is a problem with this approach.",
                    "label": 1
                },
                {
                    "sent": "We are not guaranteed to have a positive semidefinite kernel at the end.",
                    "label": 0
                },
                {
                    "sent": "If these kernel parameters are not restricted to be positive.",
                    "label": 0
                },
                {
                    "sent": "At least none negative and the second combination option is conic combination in this.",
                    "label": 0
                },
                {
                    "sent": "Approach we have positive kernel weights and we are guaranteed to have a positive semidefinite kernel at the end.",
                    "label": 1
                },
                {
                    "sent": "The last one is putting some unknown constraint on the kernel weights.",
                    "label": 0
                },
                {
                    "sent": "In that case we have citric Lee, positive kernel rates and they sum up to one.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of using these linear approach, we can combine our kernels within and near approach in this approach.",
                    "label": 0
                },
                {
                    "sent": "Basically we will take entry, wised up entry wise products of kernels, but we consider all possible pairs of kernels and as you can see from the figure we have P * P different kernels that can be obtained using.",
                    "label": 0
                },
                {
                    "sent": "Multiplication between pairs, but Please note that we don't have any individual parameter for each of these.",
                    "label": 0
                },
                {
                    "sent": "Like linear amcar methods, we have a single parameter for each kernel, but then we are multiplying two different kernels.",
                    "label": 0
                },
                {
                    "sent": "We also multiply their kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "So in order to reduce the possibility of overfitting, instead of using P square.",
                    "label": 0
                },
                {
                    "sent": "Kernel weights we basically use just P different parameters.",
                    "label": 0
                },
                {
                    "sent": "For example, if 8 two is equal to 0, this second kernel will be eliminated from all possibilities right?",
                    "label": 0
                },
                {
                    "sent": "The second column and the 2nd row will be eliminated in the combination.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Basically have peace, require different input kernels, but we have.",
                    "label": 0
                },
                {
                    "sent": "Multiplication of kernel weights as their weight, and we basically sum.",
                    "label": 0
                },
                {
                    "sent": "These piece required different kernels in the combination funk.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we modified the linear ramchal optimization problem and in this.",
                    "label": 0
                },
                {
                    "sent": "Form we have min Max optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Basically we are minimizing over kernel weights and maximizing over support vector coefficients.",
                    "label": 0
                },
                {
                    "sent": "As you can see this is not a convex optimization problem cause the objective function is not jointly convex in ETA and also Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we use some kind of alternating optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "In this procedure, we are fixing these kernel weights and so where Canonical SVM optimization problem and at each generation we are updating these kernel weights using some projection based gradient descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the gradients with respect to.",
                    "label": 0
                },
                {
                    "sent": "Colonel rates can be calculated using the input kernels, which are the multiplication of pairs of different kernels and also support vector coefficients at the current iteration an at.",
                    "label": 0
                },
                {
                    "sent": "At each iteration we are projecting them too.",
                    "label": 0
                },
                {
                    "sent": "Simplex again, to preserve the feasibility for kernel weights.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I will briefly mention the data set we used because I don't already mentioned the data set in the previous session.",
                    "label": 0
                },
                {
                    "sent": "We have ATM images from 8 patients and.",
                    "label": 1
                },
                {
                    "sent": "Nuclear extraction is performed by two pathologists.",
                    "label": 0
                },
                {
                    "sent": "We have more than 1500 pages in total and pathologists agreed on more than 1200 patches and we are using these data points in our classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the eight different feature descripcion.",
                    "label": 0
                },
                {
                    "sent": "Extracted from these datasets and all of them are histogram based feature representations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in our experiments we did comfort stratified cross validation for this eight patients and we have 8 feature representation in total and we use three basic kernel functions, linear kernel, second degree, polynomial kernel and Gaussian kernel.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in our experiments, we compared these five approach.",
                    "label": 0
                },
                {
                    "sent": "The first one is training.",
                    "label": 0
                },
                {
                    "sent": "An individual SVM on each of the feature representations separately, so we have eight different SVM classifiers.",
                    "label": 0
                },
                {
                    "sent": "We have rule based and Carol approach which basically means that we are training a single kernel SVM using the average of the input kernels.",
                    "label": 1
                },
                {
                    "sent": "Simple and Carol is the benchmark linear amcar logarithm used in the literature very much and group last Sam Carroll is a is another variant for linear amcar approach, which basically use group plus plus.",
                    "label": 0
                },
                {
                    "sent": "So regularization on the kernel weights.",
                    "label": 0
                },
                {
                    "sent": "And the last one is out, nonlinear and calvaria.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "If you look at the results with single kernel SVM, we see that.",
                    "label": 0
                },
                {
                    "sent": "Pyramid histogram based classification is very much better than the other possibilities, and we can obtain more than 75% accuracy by using linear or Gaussian kernel only representation.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the classification results, we see that all feature representations carry some information.",
                    "label": 0
                },
                {
                    "sent": "None of them is very bad in terms of classification.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "This is a bad data set for linear amcar algorithms.",
                    "label": 0
                },
                {
                    "sent": "Be cause if all feature representation or of similarity measures carry some kind of information about the classification task.",
                    "label": 0
                },
                {
                    "sent": "We cannot beat rule based uncared algorithm using linear MCL.",
                    "label": 0
                },
                {
                    "sent": "OK, rule based and Carol is basically SVM beach trains.",
                    "label": 0
                },
                {
                    "sent": "These dual optimization problem by using the simple mean of the input kernels.",
                    "label": 0
                },
                {
                    "sent": "Then if you look at the results, this simple MK land group loss and Carol is very much the same with the mean of these kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can.",
                    "label": 0
                },
                {
                    "sent": "See the increase if we use nonlinear version instead of linear amcar version.",
                    "label": 0
                },
                {
                    "sent": "If you look at the results with Gaussian kernels.",
                    "label": 0
                },
                {
                    "sent": "Basically we are combining eight different codes in kernels.",
                    "label": 0
                },
                {
                    "sent": "And we can improve the classification by around 6% compared to single kernel SVM and more than 1% for Linear AM Carol versions.",
                    "label": 0
                },
                {
                    "sent": "OK, one another thing if it rain.",
                    "label": 0
                },
                {
                    "sent": "These uncared algorithms, using all possible 24 kernels.",
                    "label": 0
                },
                {
                    "sent": "We see that again our nonlinear version is better than the linear kernels.",
                    "label": 0
                },
                {
                    "sent": "And in all possible.",
                    "label": 0
                },
                {
                    "sent": "Journal selections the nonlinear version is better than the other approach.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, if you look at the training times.",
                    "label": 0
                },
                {
                    "sent": "We see that an increase in the training time with these nonlinear Ram Kelly approach, but this is mainly because of these gradient calculation step.",
                    "label": 0
                },
                {
                    "sent": "At each iteration we have to calculate all pairwise kernels again and again.",
                    "label": 0
                },
                {
                    "sent": "If you have a large memory, we can catch them and by calculating these.",
                    "label": 0
                },
                {
                    "sent": "Gradients on the fly.",
                    "label": 0
                },
                {
                    "sent": "We can reduce these training time.",
                    "label": 0
                },
                {
                    "sent": "To the level for the linear MCL methods.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to conclude that the we can see that our nonlinear version is better than single kernel SVM and linear ankle methods for this specific data set an for this specific data set, we can achieve better results using more complex combination approaches or adding new modalities.",
                    "label": 0
                },
                {
                    "sent": "Other than these eight feature represented.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "And we compared only with three different linear amcar algorithms.",
                    "label": 0
                },
                {
                    "sent": "There are many uncared algorithms in the literature, and you can see a recent survey by going in and out Hayden, and also my MCR Matlab toolbox is available at on my website and these toolbar tool box contains more than 10 ancala algorithms you can use if you need some kind of.",
                    "label": 1
                },
                {
                    "sent": "Carol implementation, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Just sleeping.",
                    "label": 0
                },
                {
                    "sent": "Many would put Carolyn columns.",
                    "label": 0
                },
                {
                    "sent": "You use kernelization.",
                    "label": 0
                },
                {
                    "sent": "I provided the current by the more educational.",
                    "label": 0
                },
                {
                    "sent": "Use kernelization in your arms.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I forgot to mention in our experiments all kernels are normalized to unit trace unit diagonal before training.",
                    "label": 0
                },
                {
                    "sent": "So to my understanding, one of the success stories and how methods was the usually these optimization problems are convex.",
                    "label": 0
                },
                {
                    "sent": "My concern as we answer related algorithms.",
                    "label": 0
                },
                {
                    "sent": "So now you you add an extension which finally Sunol complex problems.",
                    "label": 0
                },
                {
                    "sent": "So somehow I mean, is it really worth going into this direction?",
                    "label": 0
                },
                {
                    "sent": "I guess exactly it is worth to go to these directions, because in our survey paper we compared 17 MK logarithms and nonlinear algorithms.",
                    "label": 0
                },
                {
                    "sent": "Significantly outperforms linear curve linear Ram, Kelly approaches because if you don't have any noisy Colonel in your classification task, you can achieve the best results by using the mean of the kernels, and there is no need to learn these kernel combination weights.",
                    "label": 0
                },
                {
                    "sent": "In order to achieve good accuracy you need to go deeper into this combination function and formulate some new combination approaches instead of using different regularization terms on the same combination function which is linear, you cannot do much.",
                    "label": 0
                },
                {
                    "sent": "Yes, but then you have to put it the other way around.",
                    "label": 0
                },
                {
                    "sent": "So if you have if you have no loss, convexity anyway.",
                    "label": 0
                },
                {
                    "sent": "So so, why are you still using kernels we can not use anything.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be positive definite program.",
                    "label": 0
                },
                {
                    "sent": "But at each iteration we are solving an SVM problem.",
                    "label": 0
                },
                {
                    "sent": "We need these positive semidefinite's for.",
                    "label": 0
                },
                {
                    "sent": "The efficiency.",
                    "label": 0
                },
                {
                    "sent": "For each step you are solving a conical SPM problem.",
                    "label": 0
                },
                {
                    "sent": "So we have any information about the wireless work because you don't have more freedom, they are in your methods.",
                    "label": 0
                },
                {
                    "sent": "Just have a vector of weights.",
                    "label": 0
                },
                {
                    "sent": "So what is the?",
                    "label": 0
                },
                {
                    "sent": "OK. Actually, it would correspond to linear amcar with.",
                    "label": 0
                },
                {
                    "sent": "OK, it's been correspond to linear RAM care training.",
                    "label": 0
                },
                {
                    "sent": "A linear ramp care if you use unique parameter for each possible kernel.",
                    "label": 0
                },
                {
                    "sent": "Basically, we can construct these all possible pairs of kernels and we can assign them at separate wait.",
                    "label": 0
                },
                {
                    "sent": "Basically we can train a linear and care with this type of setup, but any parameters to learn and.",
                    "label": 0
                },
                {
                    "sent": "It is not equivalent because we are we don't have more degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Where does the extra performance OK Extra performs comes from the similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Because we are multiplying two different kernels, we are projecting time to a completely different feature space.",
                    "label": 0
                },
                {
                    "sent": "Is there possibility that this is equivalent?",
                    "label": 0
                },
                {
                    "sent": "To be repaired with another service.",
                    "label": 0
                },
                {
                    "sent": "It may correspond to you.",
                    "label": 0
                },
                {
                    "sent": "But not exactly becausw.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, if it rain linear amcar with all possible weights we can use some of the kernels.",
                    "label": 0
                },
                {
                    "sent": "But for example if eight or two is zero we can eliminate all second row and 2nd column kernels.",
                    "label": 0
                },
                {
                    "sent": "We cannot do the same thing with a linear approach.",
                    "label": 0
                },
                {
                    "sent": "We are fine.",
                    "label": 0
                },
                {
                    "sent": "Interpretation of your optimization problem in the sense that we put coloring in the linear case very combined linearly as should any interpretation.",
                    "label": 0
                },
                {
                    "sent": "So you have different data sources.",
                    "label": 0
                },
                {
                    "sent": "You can, for example, further, sample sources are more important than others.",
                    "label": 0
                },
                {
                    "sent": "What would you say in your case, which there is the product and is more complex combination?",
                    "label": 0
                },
                {
                    "sent": "OK, can you repeat the question?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the thing yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Nice feature on the combining the related current occurrence in with what kind of learning is interpretation that you can get get at the end you optimize and the weights of atomization can be interpreted in some way.",
                    "label": 0
                },
                {
                    "sent": "This case very seems less easy.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "We are not losing that information because if some of datas are bigger than others, we can see that these kernel this specific kernel carries more information than the others.",
                    "label": 0
                },
                {
                    "sent": "Even if we use the pairwise kernels, we can.",
                    "label": 0
                },
                {
                    "sent": "Basically we are giving more importance to detect specific kernel.",
                    "label": 0
                },
                {
                    "sent": "Actually we have these interpretation capability with the Zetas also.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you sexy.",
                    "label": 0
                }
            ]
        }
    }
}