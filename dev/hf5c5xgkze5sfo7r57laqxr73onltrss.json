{
    "id": "hf5c5xgkze5sfo7r57laqxr73onltrss",
    "title": "Introduction by the Organizer",
    "info": {
        "author": [
            "Philipp Hennig, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_hennig_introduction/",
    "segmentation": [
        [
            "For those of you who don't know me yet, my name is Philip Anick.",
            "This is John Cunningham.",
            "Unfortunately, Mike Osborne cannot be here today because the UK Border Agency basically lost his passport.",
            "So it's my it's my job to spend the next 15 minutes while everyone's filing in telling you something that's not so interesting.",
            "So this is the workshop on probabilistic numerics.",
            "Over the course of the past week, several people have asked me what do we mean by this?",
            "Words by this combination of words, probabilistic numerics.",
            "Well, let me answer that by first thinking a little bit about the numerical algorithm is so here are."
        ],
        [
            "Examples of a numerical algorithm and numerical method is something an algorithm that takes in a handle to a function.",
            "It's called F. And then evaluate that function at various points to return an estimate of some property of the function that you cannot directly observe.",
            "So one example would be you could say I would like to evaluate the function at various points to estimate the integral over the function over some domain that's called quadrature automatical integration.",
            "Or you could say I'm assuming that the function can be normalized that it's a probability distribution if you divide it by its indefinite integral, and then I would like to produce samples from this distribution by evaluating that function at various points.",
            "That's called sampling or Monte Carlo methods.",
            "And of course there are lots and lots of variations of that.",
            "Or you could say I'm I'm interested in the minimum of their function, and I would like to evaluate the function to get repeated and hopefully better and better estimates for the minimum of the function.",
            "That's called optimization.",
            "And finally, you could say I'm going to interpret this function as describing a dynamical system, and I would like to have solutions of trajectories from that system.",
            "Maybe I want particular trajectories and that's called planning and control or solving ordinary differential equations.",
            "By the way, will I think we might be hearing examples from all four of these cases over the course of the day?",
            "The thing that unites all of these algorithms that they're making a statement about something that you cannot directly state or observe or measure.",
            "Minimum integrals, samples, trajectories.",
            "From something that you can directly measure, which is the function.",
            "And that's inference or learning.",
            "So this is really a kind of problem that we can address with our methods from our community.",
            "Now the first time.",
            "People here this kind of idea.",
            "We have now tried this with several people.",
            "The usual reaction is this sounds really weird because the object you're using, the function that you evaluating is not a random object.",
            "It's a deterministic object.",
            "It's something that the user provides to you, and that's just one of it, and it's not going to change.",
            "So that's true, but there are two important observations.",
            "The first one is that again, we're not asking for the function, which is the terministic we are asking for something that we do not know.",
            "The optimum quadratures at the bottom of the integral, the samples trajectories.",
            "So in fact we are uncertain at the probabilistic sense about this object.",
            "But even if you don't like this interpretation of uncertainty versus randomness.",
            "There's another argument argument, which I think is actually stronger, which is that.",
            "Yes, for the user of the numerical algorithm, the function is the terministic object, but for maybe ask design into the medical algorithm, the function really is an unknown object that will be generated at runtime.",
            "So we is drawn from a distribution over problems that users have.",
            "So in that sense, it's actually also run random, not just uncertain.",
            "So this kind of idea is not new, and we here in this workshop are not by far not the first people to think about this.",
            "So one very old quote that I found actually cited in the next paper is a paper form to Swedish authors from 1960, and I can't actually Paris the title and read the paper because I don't speak Swedish, but I can read it well enough to understand that this is about statistics in America integration.",
            "So roughly at least this should fit.",
            "More relevant.",
            "Maybe is a paper, a great paper from 1988 by Percy Diaconis will be very happy to have in the audience today, and he will speak later today on beige and numerical analysis and the paper starts with essentially exactly the argument that I just made, just that it's more focused on quadrature, but in the end it's essentially this idea.",
            "So this has been around for at least 25 years.",
            "Nevertheless, there is not really a community addressing these issues.",
            "There are very few papers written in this in this area.",
            "One, maybe big example is actually there is a book on an introduction to Bayesian scientific computing, which I have here by Daniela Calvetti and Arkansas Masado who are covering a few of these ideas.",
            "But again, this book is very little impact.",
            "I think few people have actually heard about this.",
            "So and of course you can have a look during the breaks if you like so.",
            "Maybe?"
        ],
        [
            "It helps to actually have a concrete example, and this is actually example.",
            "We will come back to today.",
            "David David know who is unfortunately not here so you won't even notice that I've talked about.",
            "This will give a talk about this later today.",
            "Here's a very simple example of a numerical algorithm derived from probabilistic perspective.",
            "It's a quadrature algorithm, so let's say we are interested in the integral over someone dimensional function and we want to derive a quadrature rule that can be reused for all sorts of problems.",
            "Now in probabilistic framework will start with the prior over the over the problems we have to solve.",
            "So prior over functions.",
            "And of course I use everyones favorite prior Gaussian process.",
            "In this particular case it's an orange diamondback process and we're interested maybe in the integral over the domain from zero to 1 because Gaussians are closed under linear operations and integration is a linear operation.",
            "It's essentially just the sum.",
            "Gaussian process prior over this domain induces a Gaussian belief analytically over the integral over that function.",
            "So I've done this here.",
            "So symbolically, this is a univariate Gaussian with a mean and two standard deviations."
        ],
        [
            "And now I could ask if I evaluate at some point, maybe over here, then my uncertainty over the value of the integral will drop.",
            "And of course my estimate will also adapt, and how much the uncertainty?"
        ],
        [
            "This depends on where I evaluate, so if evaluate more to the center of the domain.",
            "If you can see that the error bars get smaller, so I'll become more certain about the value is and evaluate further outside.",
            "I'll be less certain about the value, so now I could ask.",
            "If I'm going to use this over and over again for many, many problems, where should I put these evaluation points given a particular number of evaluations, I'm going to make such that this posterior uncertainty drops as quickly as possible, so maybe less sort of Bayesian formulation of that is, given this distribution over problems, how should I put the evaluation points such that my expected quadratic error on the true integral drops as quickly?"
        ],
        [
            "Possible, and here are those points.",
            "So if you evaluate just once then you have to put the point to .5.",
            "Not surprisingly, if you evaluate twice, we should put them somewhere over here and over here.",
            "These are pretty non obvious numbers, they just show up if you do 3 or evaluations that you put in here, here and here and so on up to 10 evaluations."
        ],
        [
            "So this whole framework is now a quadrature rule.",
            "We can write this down in sort of closed form.",
            "We can implement it very efficiently.",
            "And by the way, this is obviously not a new idea.",
            "This is called Bayesian quadrature.",
            "As I said, maybe purses paper from 1988 is one of the first mentions of this is also a great paper by atomic are from 2000 and many papers, well, not actually, not so many, but several papers in this community, including one at this year's lips.",
            "So some of those things you immediately see from this kind of formulation is.",
            "Obviously, the location of these points are, well, maybe not so obviously, but I can tell you that the location of these points depend on the prior.",
            "So by saying I expect various kinds of functions, I'll get different locations and also get different weights for the function evaluations.",
            "So if somebody comes along and tells me something a bit more about the problem I'm trying to solve.",
            "Then I can immediately adapt my quadrature rule and get better performance out of it.",
            "For example, if I had some nontrivial function that has uncertainty here and here, but we're pretty certain about the middle and will get a very nontrivial distribution of these points, so that's a great one great advantage of these kind of methods by deriving.",
            "An American algorithm, from explicit assumptions about the problems we're going to face, we can adapt to these problems, improve their performance.",
            "We can also sometimes generalize, so you can imagine situations where we know."
        ],
        [
            "Think about noise or dynamics or changing problems, and then we might get even more general algorithms.",
            "So these are two advantages of this kind of formulation.",
            "Then I think are pretty obvious.",
            "M. A probabilistic formulation for numerical algorithm may help us understand implicit assumptions in those algorithms and to improve, generalize, invent new algorithms.",
            "However.",
            "And I think this is going to be a feature of today as well.",
            "We also have to have to point out that of course numerical maths is not the same as machine learning, and it would be a big error.",
            "Big mistake to assume that we can just do everything we do in machine learning and throw it at numerics.",
            "And then it's just going to work because America is a field that has very different requirements on its products.",
            "If you like so machine learning algorithm can be should be fancy and produce interesting colorful plots that can have two smart like that.",
            "Show that it can do smart.",
            "Imagine things while America algorithm is something that runs at the core of everyone else's algorithm.",
            "In the inner loop.",
            "So it has to basically always work.",
            "Ideally, always work.",
            "Of course, it won't actually always work, and it also has to have very low computational costs or court costs as a completely different role in numerics that has a machine learning on our papers.",
            "We sort of point out that things work, and in America we have to basically restrict ourselves to linear and constant time algorithms so.",
            "That's going to be some of the things we have to discuss today.",
            "We'll have to get a feeling for what these two fields can actually give each other, and.",
            "Well, how we can make sure that that what they can give to each other actually gets delivered?",
            "I think this is actually maybe the biggest prob."
        ],
        [
            "To that end, we've invited a great panel of speakers that I'm very much looking forward to all the talks today will start now with and I guess the times are already kind of works out with Matthias Seeger, who's already sitting in here.",
            "I'll just do some in a second.",
            "As a machine learning researcher, then will have a talk by adjusting console who is a mathematician.",
            "American mathematician.",
            "I hope you are happy with that.",
            "Qualification and enable coffee break originally had planned to have a talk by the organizers by Mike, but as I already said, Mike unfortunately cannot be here on account of the UK Border Agency.",
            "So in his place David Divinol will give a talk on Bayesian quadrature on the thing that I just sort of try to very briefly outline.",
            "Then we'll have the poster spotlights and then a poster session, and then the break.",
            "And after the lunch break at 4:00 o'clock, we are very happy to have.",
            "Persi Diaconis will give a give a talk the keynote talk of this workshop, followed by Ben colder head from UCL, who is a statistician.",
            "So I think we have the three communities in here, followed by a coffee break and then we had planned to have which package speak for Microsoft Research.",
            "Unfortunately, he also cannot be here for exactly the same reason as Mike because the UK Border Agency again managed to keep him from leaving the country.",
            "So in this place I will give a talk.",
            "Then we'll have a panel discussion at the end, and the workshop lends itself."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For those of you who don't know me yet, my name is Philip Anick.",
                    "label": 0
                },
                {
                    "sent": "This is John Cunningham.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, Mike Osborne cannot be here today because the UK Border Agency basically lost his passport.",
                    "label": 0
                },
                {
                    "sent": "So it's my it's my job to spend the next 15 minutes while everyone's filing in telling you something that's not so interesting.",
                    "label": 0
                },
                {
                    "sent": "So this is the workshop on probabilistic numerics.",
                    "label": 1
                },
                {
                    "sent": "Over the course of the past week, several people have asked me what do we mean by this?",
                    "label": 0
                },
                {
                    "sent": "Words by this combination of words, probabilistic numerics.",
                    "label": 0
                },
                {
                    "sent": "Well, let me answer that by first thinking a little bit about the numerical algorithm is so here are.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples of a numerical algorithm and numerical method is something an algorithm that takes in a handle to a function.",
                    "label": 0
                },
                {
                    "sent": "It's called F. And then evaluate that function at various points to return an estimate of some property of the function that you cannot directly observe.",
                    "label": 0
                },
                {
                    "sent": "So one example would be you could say I would like to evaluate the function at various points to estimate the integral over the function over some domain that's called quadrature automatical integration.",
                    "label": 0
                },
                {
                    "sent": "Or you could say I'm assuming that the function can be normalized that it's a probability distribution if you divide it by its indefinite integral, and then I would like to produce samples from this distribution by evaluating that function at various points.",
                    "label": 0
                },
                {
                    "sent": "That's called sampling or Monte Carlo methods.",
                    "label": 1
                },
                {
                    "sent": "And of course there are lots and lots of variations of that.",
                    "label": 0
                },
                {
                    "sent": "Or you could say I'm I'm interested in the minimum of their function, and I would like to evaluate the function to get repeated and hopefully better and better estimates for the minimum of the function.",
                    "label": 0
                },
                {
                    "sent": "That's called optimization.",
                    "label": 0
                },
                {
                    "sent": "And finally, you could say I'm going to interpret this function as describing a dynamical system, and I would like to have solutions of trajectories from that system.",
                    "label": 0
                },
                {
                    "sent": "Maybe I want particular trajectories and that's called planning and control or solving ordinary differential equations.",
                    "label": 0
                },
                {
                    "sent": "By the way, will I think we might be hearing examples from all four of these cases over the course of the day?",
                    "label": 0
                },
                {
                    "sent": "The thing that unites all of these algorithms that they're making a statement about something that you cannot directly state or observe or measure.",
                    "label": 0
                },
                {
                    "sent": "Minimum integrals, samples, trajectories.",
                    "label": 0
                },
                {
                    "sent": "From something that you can directly measure, which is the function.",
                    "label": 0
                },
                {
                    "sent": "And that's inference or learning.",
                    "label": 0
                },
                {
                    "sent": "So this is really a kind of problem that we can address with our methods from our community.",
                    "label": 0
                },
                {
                    "sent": "Now the first time.",
                    "label": 0
                },
                {
                    "sent": "People here this kind of idea.",
                    "label": 0
                },
                {
                    "sent": "We have now tried this with several people.",
                    "label": 0
                },
                {
                    "sent": "The usual reaction is this sounds really weird because the object you're using, the function that you evaluating is not a random object.",
                    "label": 0
                },
                {
                    "sent": "It's a deterministic object.",
                    "label": 0
                },
                {
                    "sent": "It's something that the user provides to you, and that's just one of it, and it's not going to change.",
                    "label": 0
                },
                {
                    "sent": "So that's true, but there are two important observations.",
                    "label": 0
                },
                {
                    "sent": "The first one is that again, we're not asking for the function, which is the terministic we are asking for something that we do not know.",
                    "label": 0
                },
                {
                    "sent": "The optimum quadratures at the bottom of the integral, the samples trajectories.",
                    "label": 0
                },
                {
                    "sent": "So in fact we are uncertain at the probabilistic sense about this object.",
                    "label": 0
                },
                {
                    "sent": "But even if you don't like this interpretation of uncertainty versus randomness.",
                    "label": 0
                },
                {
                    "sent": "There's another argument argument, which I think is actually stronger, which is that.",
                    "label": 0
                },
                {
                    "sent": "Yes, for the user of the numerical algorithm, the function is the terministic object, but for maybe ask design into the medical algorithm, the function really is an unknown object that will be generated at runtime.",
                    "label": 1
                },
                {
                    "sent": "So we is drawn from a distribution over problems that users have.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, it's actually also run random, not just uncertain.",
                    "label": 0
                },
                {
                    "sent": "So this kind of idea is not new, and we here in this workshop are not by far not the first people to think about this.",
                    "label": 0
                },
                {
                    "sent": "So one very old quote that I found actually cited in the next paper is a paper form to Swedish authors from 1960, and I can't actually Paris the title and read the paper because I don't speak Swedish, but I can read it well enough to understand that this is about statistics in America integration.",
                    "label": 0
                },
                {
                    "sent": "So roughly at least this should fit.",
                    "label": 0
                },
                {
                    "sent": "More relevant.",
                    "label": 0
                },
                {
                    "sent": "Maybe is a paper, a great paper from 1988 by Percy Diaconis will be very happy to have in the audience today, and he will speak later today on beige and numerical analysis and the paper starts with essentially exactly the argument that I just made, just that it's more focused on quadrature, but in the end it's essentially this idea.",
                    "label": 0
                },
                {
                    "sent": "So this has been around for at least 25 years.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, there is not really a community addressing these issues.",
                    "label": 0
                },
                {
                    "sent": "There are very few papers written in this in this area.",
                    "label": 0
                },
                {
                    "sent": "One, maybe big example is actually there is a book on an introduction to Bayesian scientific computing, which I have here by Daniela Calvetti and Arkansas Masado who are covering a few of these ideas.",
                    "label": 1
                },
                {
                    "sent": "But again, this book is very little impact.",
                    "label": 0
                },
                {
                    "sent": "I think few people have actually heard about this.",
                    "label": 0
                },
                {
                    "sent": "So and of course you can have a look during the breaks if you like so.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It helps to actually have a concrete example, and this is actually example.",
                    "label": 0
                },
                {
                    "sent": "We will come back to today.",
                    "label": 0
                },
                {
                    "sent": "David David know who is unfortunately not here so you won't even notice that I've talked about.",
                    "label": 0
                },
                {
                    "sent": "This will give a talk about this later today.",
                    "label": 0
                },
                {
                    "sent": "Here's a very simple example of a numerical algorithm derived from probabilistic perspective.",
                    "label": 0
                },
                {
                    "sent": "It's a quadrature algorithm, so let's say we are interested in the integral over someone dimensional function and we want to derive a quadrature rule that can be reused for all sorts of problems.",
                    "label": 0
                },
                {
                    "sent": "Now in probabilistic framework will start with the prior over the over the problems we have to solve.",
                    "label": 0
                },
                {
                    "sent": "So prior over functions.",
                    "label": 0
                },
                {
                    "sent": "And of course I use everyones favorite prior Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "In this particular case it's an orange diamondback process and we're interested maybe in the integral over the domain from zero to 1 because Gaussians are closed under linear operations and integration is a linear operation.",
                    "label": 0
                },
                {
                    "sent": "It's essentially just the sum.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process prior over this domain induces a Gaussian belief analytically over the integral over that function.",
                    "label": 1
                },
                {
                    "sent": "So I've done this here.",
                    "label": 0
                },
                {
                    "sent": "So symbolically, this is a univariate Gaussian with a mean and two standard deviations.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I could ask if I evaluate at some point, maybe over here, then my uncertainty over the value of the integral will drop.",
                    "label": 0
                },
                {
                    "sent": "And of course my estimate will also adapt, and how much the uncertainty?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This depends on where I evaluate, so if evaluate more to the center of the domain.",
                    "label": 0
                },
                {
                    "sent": "If you can see that the error bars get smaller, so I'll become more certain about the value is and evaluate further outside.",
                    "label": 0
                },
                {
                    "sent": "I'll be less certain about the value, so now I could ask.",
                    "label": 0
                },
                {
                    "sent": "If I'm going to use this over and over again for many, many problems, where should I put these evaluation points given a particular number of evaluations, I'm going to make such that this posterior uncertainty drops as quickly as possible, so maybe less sort of Bayesian formulation of that is, given this distribution over problems, how should I put the evaluation points such that my expected quadratic error on the true integral drops as quickly?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible, and here are those points.",
                    "label": 0
                },
                {
                    "sent": "So if you evaluate just once then you have to put the point to .5.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, if you evaluate twice, we should put them somewhere over here and over here.",
                    "label": 0
                },
                {
                    "sent": "These are pretty non obvious numbers, they just show up if you do 3 or evaluations that you put in here, here and here and so on up to 10 evaluations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this whole framework is now a quadrature rule.",
                    "label": 0
                },
                {
                    "sent": "We can write this down in sort of closed form.",
                    "label": 0
                },
                {
                    "sent": "We can implement it very efficiently.",
                    "label": 0
                },
                {
                    "sent": "And by the way, this is obviously not a new idea.",
                    "label": 0
                },
                {
                    "sent": "This is called Bayesian quadrature.",
                    "label": 0
                },
                {
                    "sent": "As I said, maybe purses paper from 1988 is one of the first mentions of this is also a great paper by atomic are from 2000 and many papers, well, not actually, not so many, but several papers in this community, including one at this year's lips.",
                    "label": 0
                },
                {
                    "sent": "So some of those things you immediately see from this kind of formulation is.",
                    "label": 0
                },
                {
                    "sent": "Obviously, the location of these points are, well, maybe not so obviously, but I can tell you that the location of these points depend on the prior.",
                    "label": 0
                },
                {
                    "sent": "So by saying I expect various kinds of functions, I'll get different locations and also get different weights for the function evaluations.",
                    "label": 0
                },
                {
                    "sent": "So if somebody comes along and tells me something a bit more about the problem I'm trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Then I can immediately adapt my quadrature rule and get better performance out of it.",
                    "label": 0
                },
                {
                    "sent": "For example, if I had some nontrivial function that has uncertainty here and here, but we're pretty certain about the middle and will get a very nontrivial distribution of these points, so that's a great one great advantage of these kind of methods by deriving.",
                    "label": 0
                },
                {
                    "sent": "An American algorithm, from explicit assumptions about the problems we're going to face, we can adapt to these problems, improve their performance.",
                    "label": 0
                },
                {
                    "sent": "We can also sometimes generalize, so you can imagine situations where we know.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about noise or dynamics or changing problems, and then we might get even more general algorithms.",
                    "label": 0
                },
                {
                    "sent": "So these are two advantages of this kind of formulation.",
                    "label": 0
                },
                {
                    "sent": "Then I think are pretty obvious.",
                    "label": 0
                },
                {
                    "sent": "M. A probabilistic formulation for numerical algorithm may help us understand implicit assumptions in those algorithms and to improve, generalize, invent new algorithms.",
                    "label": 1
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "And I think this is going to be a feature of today as well.",
                    "label": 1
                },
                {
                    "sent": "We also have to have to point out that of course numerical maths is not the same as machine learning, and it would be a big error.",
                    "label": 0
                },
                {
                    "sent": "Big mistake to assume that we can just do everything we do in machine learning and throw it at numerics.",
                    "label": 0
                },
                {
                    "sent": "And then it's just going to work because America is a field that has very different requirements on its products.",
                    "label": 0
                },
                {
                    "sent": "If you like so machine learning algorithm can be should be fancy and produce interesting colorful plots that can have two smart like that.",
                    "label": 0
                },
                {
                    "sent": "Show that it can do smart.",
                    "label": 0
                },
                {
                    "sent": "Imagine things while America algorithm is something that runs at the core of everyone else's algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the inner loop.",
                    "label": 0
                },
                {
                    "sent": "So it has to basically always work.",
                    "label": 0
                },
                {
                    "sent": "Ideally, always work.",
                    "label": 0
                },
                {
                    "sent": "Of course, it won't actually always work, and it also has to have very low computational costs or court costs as a completely different role in numerics that has a machine learning on our papers.",
                    "label": 0
                },
                {
                    "sent": "We sort of point out that things work, and in America we have to basically restrict ourselves to linear and constant time algorithms so.",
                    "label": 0
                },
                {
                    "sent": "That's going to be some of the things we have to discuss today.",
                    "label": 0
                },
                {
                    "sent": "We'll have to get a feeling for what these two fields can actually give each other, and.",
                    "label": 0
                },
                {
                    "sent": "Well, how we can make sure that that what they can give to each other actually gets delivered?",
                    "label": 0
                },
                {
                    "sent": "I think this is actually maybe the biggest prob.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To that end, we've invited a great panel of speakers that I'm very much looking forward to all the talks today will start now with and I guess the times are already kind of works out with Matthias Seeger, who's already sitting in here.",
                    "label": 0
                },
                {
                    "sent": "I'll just do some in a second.",
                    "label": 0
                },
                {
                    "sent": "As a machine learning researcher, then will have a talk by adjusting console who is a mathematician.",
                    "label": 0
                },
                {
                    "sent": "American mathematician.",
                    "label": 0
                },
                {
                    "sent": "I hope you are happy with that.",
                    "label": 0
                },
                {
                    "sent": "Qualification and enable coffee break originally had planned to have a talk by the organizers by Mike, but as I already said, Mike unfortunately cannot be here on account of the UK Border Agency.",
                    "label": 0
                },
                {
                    "sent": "So in his place David Divinol will give a talk on Bayesian quadrature on the thing that I just sort of try to very briefly outline.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have the poster spotlights and then a poster session, and then the break.",
                    "label": 0
                },
                {
                    "sent": "And after the lunch break at 4:00 o'clock, we are very happy to have.",
                    "label": 0
                },
                {
                    "sent": "Persi Diaconis will give a give a talk the keynote talk of this workshop, followed by Ben colder head from UCL, who is a statistician.",
                    "label": 0
                },
                {
                    "sent": "So I think we have the three communities in here, followed by a coffee break and then we had planned to have which package speak for Microsoft Research.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, he also cannot be here for exactly the same reason as Mike because the UK Border Agency again managed to keep him from leaving the country.",
                    "label": 0
                },
                {
                    "sent": "So in this place I will give a talk.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have a panel discussion at the end, and the workshop lends itself.",
                    "label": 0
                }
            ]
        }
    }
}