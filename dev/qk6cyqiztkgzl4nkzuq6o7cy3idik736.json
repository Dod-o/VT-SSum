{
    "id": "qk6cyqiztkgzl4nkzuq6o7cy3idik736",
    "title": "Leveraging and Balancing Heterogeneous Sources of Evidence in Ontology Learning",
    "info": {
        "author": [
            "Gerhard Wohlgenannt, Institute for Information Business, Vienna University of Economics and Business"
        ],
        "published": "Oct. 21, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_wohlgenannt_ontology_learning/",
    "segmentation": [
        [
            "Good afternoon and welcome to my talk.",
            "Oh yeah, I'm got working on from University of Business University in Vienna.",
            "Institute of Information business and."
        ],
        [
            "This talk is the general aim is about it.",
            "Cinerama from college learning and what we're doing here is, we're having multiple and heterogeneous input sources.",
            "Evidence sources which we use in the learning process, and we assess how this combination of different and very hit Regina sources can affect and improve the accuracy of the system.",
            "I would start with some basic concepts, just introducing briefly, but ontology learning is give a brief description of our systems so the rest becomes more easily understandable and also show the evidence sources which we use in the first part and then the second part will be an overview of the experiments with it to assess how these different sources affect results.",
            "How many sources you should use to get benefits.",
            "How should you balance the input from the sources and so on?"
        ],
        [
            "Yeah, I start with the basic concepts.",
            "I think I don't have to explain what ontologies are on this conference, so I just go to the next point, which is ontology construction.",
            "As everybody knows, it's very cumbersome and complicated and can be very expensive and that's why we have methods such as ontology learning or in more recent times, or using crowdsourcing to improve and bootstrap this process.",
            "Crowdsourcing rather to verify.",
            "Things which will learn Indian Tower from the ontology.",
            "Learning ontology.",
            "Learning is using supervised and unsupervised methods through semi automatically generate ontologies from data and traditionally this is a bit simplified but traditionally it has been done from one input source, usually at the main corpus which is used for the ontology learning to learn the ontology from the data.",
            "And now we have a look at as a set before using many sources.",
            "How to integrate them.",
            "And especially how to balance the input from those sources?"
        ],
        [
            "OK, we have a brief look at our ontology learning system which we are using for this experiment.",
            "The process is very simple.",
            "We start from a small state ontology which is usually just a couple of concepts from a domain and then we look in evidence sources.",
            "I will talk about it later for evidence for new concepts we gather all that evidence and then we have an algorithm to select the most important concepts found in the evidence.",
            "And reposition them in the ontology in the existing ontology.",
            "And this gives us the extended ontology, and we can repeat this process.",
            "We can use this extended ontology again as seed ontology and further extended in a couple of iterations to make it bigger and bigger until we are satisfied with what we have."
        ],
        [
            "The ontology is.",
            "This is a graphical snipplet of part of an ontology are very lightweight, so we usually have concepts.",
            "We have taxonomic relations and we have unlabeled non taxonomic relations in these ontologies and the different colors on the on the slides of the concept just reflecting what iteration step the concepts have been learned.",
            "So this is about climate change or a seed concept was just climate change itself and then we learned this.",
            "Blue at the screen.",
            "Concepts in the next step and then the light green concept and so on.",
            "In this extension process."
        ],
        [
            "A more detailed look at the system itself, but still keeping this short because it's just a help to understand the rest how the system works.",
            "We have to cede ontology.",
            "We collect evidence from evidence sources, we integrate all that evidence, which is then usually network of many thousands of terms and their connections into a semantic network.",
            "And from this semantic network with the help of spreading activation, which is an algorithm to find important nodes in the network.",
            "We select the most important concepts.",
            "These concepts are then presented to domain experts which evaluate the content concepts if they are relevant to the domain or not.",
            "This is also the only stop step in the system where we have some manual inputs and manual verification of results.",
            "And then this verified concepts will be purchased.",
            "Positions in existing ontology and this gives us the extended ontology which can then be used again as seed ontology and further extended.",
            "So that's the basic gist of how this system is working."
        ],
        [
            "Yeah, now we'll have a look at the evidence sources, because that's an important factor in the system which learns from various evidence.",
            "Sources will have a look at them.",
            "What sources are there and what are their characteristics?",
            "So for even for each given seat concept, the evidence sources provide terms and relations to the system.",
            "In the current configuration, where the evidence sources are mostly based on the main text, we use keyword extraction and lexical syntactical patterns.",
            "First.",
            "Patterns for extracting evidence there.",
            "But we also use social media sources and structured sources like DB pedia or word net which provide evidence to the system.",
            "In sum, we have 32 evidence sources which are used which are very heterogeneous, which I have a different quality of terms.",
            "They provide different number of terms they provide and so on.",
            "So we have to see how we can integrate it in a meaningful way."
        ],
        [
            "This is a brief overview of the data sources which we have as I already said, text is very prominent, so it's text mirrored from the web.",
            "So what we basically do, we learn ontologies every month from scratch from this small seed ontology.",
            "And we do this with data collected in that month.",
            "So every month we have snapshot which represents the domain as it occurs in the sources.",
            "Yeah, so each.",
            "Each month we learn a new ontology representing the data from the month.",
            "And in those text sources we have news media sources mirrored from the web from different countries.",
            "USUK, and so on.",
            "We have social media sources, Facebook, Twitter and the usual candidates for social media.",
            "And we have some other samples which we mirror Fortune 1000 companies, their websites and stuff like that and we have the other kinds of sources like API's, Twitter and Flickr have an API which provides for a given input term.",
            "You can give it an input Romanet gives you related terms, so terms related to the term.",
            "In Flickr, I guess it's just if you have tax on a photo, which other tax usually Co occur with this tech.",
            "So this is another kind of source which provides you terms and we have.",
            "Structured sources were not in DB pedia."
        ],
        [
            "I'm Anne from this data sources, which are not the same as the evidence sources we now get to the evidence versus for each data source we can have multiple evidence sources.",
            "So for example, US news media text, we can apply different algorithms cooccurrence algorithms to generate keywords on a page level on a sentence level.",
            "We can use first patterns, so these are different sources and then some for all the text based sources, we have 26 evidence sources.",
            "Um?"
        ],
        [
            "And for the other ones, there are 6 remaining from word net hyponyms hypernyms synonyms for a given term, and the API queries and sparkle used on DB pedia to get with a couple of predicates which we're using to get terms related to an input term from DB pedia."
        ],
        [
            "Just to give you an idea how such evidence looks like, this is a list of terms which we got at some point.",
            "I think from UK news media sources for the input concept, input label CO2 and these are keywords generated from that corpus which we generate which we have for that given point in time.",
            "And as you can see, some of the terms are really relevant to the domain.",
            "Some are too specific, like maybe I don't know this PM 10 which is I think about.",
            "Little particles which are in the air and which doesn't don't really have something to do with climate change or not.",
            "A lot and terms, which are two general like air or maybe two general depends on how you will model the domain.",
            "So it's different quality.",
            "What you get from the sources, so the individual sources pry provide ladder rather low quality results, but by aggregating them we want to get better concept candidate."
        ],
        [
            "Upstairs was one too much.",
            "So what are we doing with all this evidence we're getting?",
            "I think you already said that we put it in a semantic network, which is then tends to be very big couple of thousands or even 10s of thousands of terms and their relations in this big network.",
            "And then we select the configurations.",
            "Now with like 25 counts of candidates, that's just a setting which we're using per extension step where we extend the ontology.",
            "So we have 25 candidates generated by the spreading activation algorithm.",
            "So this is how we get to the Council candidates from the evidence."
        ],
        [
            "Which brings us back to the research question or what this is all about.",
            "Um, my tip is sources have to ban at the benefit that they, I mean a benefit.",
            "That's a question they provide redundancy, and they provide complementary information and the general assumption of this research, which is also backed by literature, is that redundancy can help you to find.",
            "You can see it as a measure if something occurs in multiple sources, you can see there's a measure of trust because it occurs in multiple sources, or maybe is more relevant than something which occurred in just one source and the measure of relevance.",
            "So this thing we want to use or leverage to improve the quality of our ontologies.",
            "Um?",
            "Yeah, and therefore heterogeneous sources provide the opportunity to have higher levels of accuracy, so the research questions are now how many sources do we need to make use of this facts of this hopeful facts?",
            "The redundancy helps us and how much evidence should be used for source and so on, which we will look at in the evaluation."
        ],
        [
            "OK, So what did we do?",
            "We did a lot of experiments over some over couple of more than a year learning ontologies and different domains with different settings.",
            "Seeing what are the effects of all those parameters which we're using.",
            "And the goal was to find an answer which can be at least to some point generalized also to other systems, ontology learning systems or just systems which make use of different evidence sources which combine things.",
            "So that was the point, the goal did not just have something for our system, but at least at some point the IT has have to be made experiments to see if it works in other systems, but that we think because we have a very intuitive evidence.",
            "Creation logic there, spreading activation that we can expect at least similar results with other integration logics, which combine evidence from different sources."
        ],
        [
            "OK, what was the setup of the evaluation?",
            "So we had two domains, climate change and tennis.",
            "Um and yeah, we try it.",
            "Those different settings, number of sources, evidences per source generated ontologies over time frame from July 2013 to November 2014 and had the results.",
            "The overall.",
            "The resulting ontologies where were which were like 10 or 15 or even more per month assessed by domain experts.",
            "Always checking if the concept can.",
            "How many concept candidates produced are really relevant to the domain.",
            "According to the judgment of the domain expert, so your currency ratio is really simple.",
            "It's something like it's more or less precision.",
            "Relevant concepts generated divided by all concepts generated by the system.",
            "Which we will use in the evaluations."
        ],
        [
            "Um?",
            "But why is this balancing needing and needed anyway between the sources?",
            "So couldn't we just use all evidence produced by all sources and would be fine?"
        ],
        [
            "The.",
            "At least in our case, and I guess that will be also the same thing will happen in many other systems.",
            "The sources have very different characteristics.",
            "First of all, the number of evidence produced per source can vary a lot.",
            "So in our system, the keyword sources, the cooccurrence keyword sources which produce keywords from a text corpus, mirrors from, uh, from some news media source, for example.",
            "They usually provide us with a lot of evidence.",
            "Which has a significant which is ordered and we have between an average 200 and 400 keywords depending if it's sentence level or pH level keywords.",
            "For other sources we have much fewer.",
            "A lot fewer evidences for DB pedia.",
            "Example on average per input concept.",
            "Proceed concept which put into DB pedia without sparkle queries.",
            "We have 13 evidences only so there's a big gap and if we would just use all the sources then the keyword sources with just overwhelmed.",
            "All the other things which go into the system.",
            "And there's also a big difference between quality of the sort of the sources we manually had a look at all the terms or not all the terms, but a subset of the terms from every source, and to verify how many terms are relevant to the domain.",
            "And there is a big difference.",
            "So for example, the Twitter API, for some reason it gives us a quite a lot of an information which doesn't make sense.",
            "Or does it is not useful.",
            "It's a lot of.",
            "Abbreviations and stuff like that or just stuff which is not relevant to the domain.",
            "So only 10% of the terms from the Twitter API were relevant.",
            "Various for other sources like DB Pedia.",
            "It's almost 30% and for the keywords it depends.",
            "So if the keywords are ordered by significance and the first one is the first went top 25 have acquired quite good and the lower your gets the lower the significance.",
            "Is there?",
            "Uh yeah and the keywords get worse.",
            "So it depends how many key was it.",
            "Off the keywords are using the system, so there is obviously a need to think about how can we balance all this evidence which goes into the system."
        ],
        [
            "And that is."
        ],
        [
            "What we made the experiments for and I would just jump to the first experiment roommates.",
            "Here, we're looking at the number of evidences we use that we have to 30, two sources and.",
            "Percocet concepts they give us evidence and we limit this number of evidences to a number which is given in the left hand column so we can say we use only five terms.",
            "Suggested person concept or we use more like whatever up to 500 or even don't limit it at all and if we limit it to a very low number then well this is also quite intuitive then like the learning algorithm will just not have enough information to make a really.",
            "To really benefit from all this aggregation and redundancy so the accuracy is rather low, but also as we can see if we use a lot of information and we use all the keywords even with lower significance and the overwhelm, the other evidence sources, then the accuracy is going down again.",
            "So it's somewhere in the middle between in our system 20 to 50 terms per evidence source.",
            "An concept gave us the best results, the best accuracy to the system.",
            "If you don't have a system where you have, at least for some sources in order of by quality of your evidence is, then it will be better to use more evidence.",
            "Becausw we tested this with them just picking random keywords and not the best keywords and then when we use not so much evidence then results will not be so good because maybe the best keywords will be at the at the end of the list.",
            "So it depends on how you saw your evidence is structured.",
            "If you have some ranking in there or not."
        ],
        [
            "And what I think is more interesting even is then how many sources do you really need to benefit from your app from the aggregation from redundancy and we try a different settings just using one source using five sources in the ontology learning system using 15 sources, or using all the 30 two sources which we have.",
            "And if you use only one source, especially if it's one source which provides you low quality terms like Twitter, then.",
            "Obviously, or is it?",
            "The learning algorithm has no chance to produce a good ontology or ontology with a lot of relevant concepts from from that as you can see, only 20% accuracy for Twitter.",
            "If you have a high quality source.",
            "On the other hand, you can easily get to 50% with.",
            "In this case keywords from UK media, five sources then start to give you the benefits of aggregation and of redundancy and using this in the spreading activation algorithm.",
            "Raising the accuracy of the 60% or something like that and 15 sources we saw for, at least for our experiments, were enough to get all the benefits from redundancy.",
            "So going from 15 sources to 30 two sources, the dishes blowed up there.",
            "The whole system made the computations are much, much lower, made the semantic network huge, but didn't give any significant improvements to the system anymore.",
            "So that's maybe an important point if you have many sources.",
            "You would really have to check how many do you really need to do to get those benefits from from redundancy and complementary information."
        ],
        [
            "And then Lastly, we had a look at.",
            "The number of sort of seat concepts which we have and how that affects the results.",
            "Stages are charged how the points where we iterate your weather iterations comes into play come into place.",
            "So this stage one is when we just have the two seat concepts and nothing else and use this to learn new concepts and in stage two we have the two seat concepts.",
            "Plus they learned concepts from the first stage which are then about 20 concepts and so on and.",
            "If you have only a limited number of seat concepts, then obviously which also makes sense intuitively.",
            "You needs at least a lot of evidence for those two seat concepts to get to some meaningful results.",
            "So in the first stage, using only 5 limited 5 gave us really poor results.",
            "On the other hand, because the seat concepts are really relevant to domain to the domain, they are the most top concepts in a way in the hierarchy when you use a lot of evidence then you also get a lot of good.",
            "A relevant concept.",
            "And when you have, then when you have more seat concepts than even for limit of five evidences per concept, you still get decent results because you have so many concepts and you have trusted the the sum of all this.",
            "This terms which gives you the bonus of redundancy again."
        ],
        [
            "I was."
        ],
        [
            "This one, yeah, I'm.",
            "I I didn't check.",
            "Then I'm already coming to the results.",
            "Yeah, take away point.",
            "So in a system like that, if it's in a similar configuration or whatever, a few thousand terms will be enough to leverage all these effects.",
            "Which redundancy and complementary information.",
            "If you're bloated up and make it bigger, use more evidence.",
            "Maybe it won't give you much much benefit, so this can help too.",
            "If you set up a new system to really check, do you need all this information or is it just?",
            "Do you just slow down your computations by having too much input and not in terms of how many sources in needs?",
            "We came to the conclusion like 10 to 15 heterogeneous evidence sources are enough to gain from the oldest benefits from redundancy, which I was talking about all the time.",
            "And yeah, balancing input is generally more important than just the wrong number of evidence, so it's generally better to have different sources which give you input than one source which gives you a lot of inputs.",
            "And yeah, nothing else."
        ],
        [
            "And finally, if you to work, yeah, we obviously want to test this system in different domains.",
            "We would like to have a look at other systems and test them with the same assumptions and see if our claim that this is at least partly generalizable really holds.",
            "And finally we it's in the current version.",
            "Every evidence source had the same input impact on the learning algorithms, so we also are now optimizing the system to give evidence sources which provide better keywords or better term.",
            "A higher import or higher weights in the learning algorithm and thereby can optimize the system even more."
        ],
        [
            "OK, that's it for me.",
            "Thank you.",
            "If there any questions.",
            "So these are question about the ontology that you learn through the system.",
            "So when you say it's a non tallied, do you mean that it's a set of triples?",
            "Are is it only simple subclass relationships or does it have something beyond that likes a union of concepts are cardinality constraints and now we have we're doing this which separated light weights and I think most ontology learning systems they some try to turn him to learn axioms.",
            "But most ontology learning systems focus on rather lightweight ontology, that is concepts up.",
            "Yeah, subclass relations, non tech, just relations with without a non taxonomic relations, but unlabeled so you don't really know how things are connected to just that they are connected.",
            "So that's mostly what we're learning with this system.",
            "You showed that 15 sources was this sweet spot where we could get the most out of it, and I assume that the pool of sources available where these 32 or maybe more.",
            "The question is to get to this 15 number.",
            "Did you try multiple combinations like multiple groups of 15 and then you got to this conclusion?",
            "Or how did you proceed together?",
            "Yes we but tried a couple of different groups so we had to two domains.",
            "10 years in climate change and there we try that.",
            "Not too many.",
            "A couple of different combinations, but it gave roughly the same result.",
            "Becaused those sources they have similar characteristics, especially if the keyword sources does not much different.",
            "If you take keywords from US media or UK media.",
            "So if you change those it it has a very low impact on on your result.",
            "So yeah, but I guess I mean it was always a mix of very heterogeneous sources, so we didn't ever try to just have keyword sources or something like that, but different kinds of sources with different characteristics and then mix them together to get to those 15.",
            "OK, thank you.",
            "Other questions.",
            "Alright, thanks for the talk.",
            "I am interested in how did the concepts evolve overtime in your sources.",
            "So can you say something about in which month it was especially?",
            "Or if they changed overtime so much, that's something which is one of the future work will be already started doing it, and we have also a small platform where we can check which concepts are added in one month and get removed.",
            "And there's always a shift.",
            "And obviously, for example in climate change domain you have.",
            "You also have cyclic patterns where for example in summer time you have concepts like melting glaciers and stuff like that which happens in summer time and this is reported then, but not in winter time.",
            "And things like that.",
            "So there are definitely things which which have yet trends and patterns rising, falling, cyclic.",
            "But we didn't do a really study on it, was just looking at the data we saw.",
            "It's there when we want to do that in the future to to compare the ontologies and see how how they go off.",
            "OK, let's thank God again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon and welcome to my talk.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I'm got working on from University of Business University in Vienna.",
                    "label": 0
                },
                {
                    "sent": "Institute of Information business and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk is the general aim is about it.",
                    "label": 1
                },
                {
                    "sent": "Cinerama from college learning and what we're doing here is, we're having multiple and heterogeneous input sources.",
                    "label": 1
                },
                {
                    "sent": "Evidence sources which we use in the learning process, and we assess how this combination of different and very hit Regina sources can affect and improve the accuracy of the system.",
                    "label": 0
                },
                {
                    "sent": "I would start with some basic concepts, just introducing briefly, but ontology learning is give a brief description of our systems so the rest becomes more easily understandable and also show the evidence sources which we use in the first part and then the second part will be an overview of the experiments with it to assess how these different sources affect results.",
                    "label": 1
                },
                {
                    "sent": "How many sources you should use to get benefits.",
                    "label": 0
                },
                {
                    "sent": "How should you balance the input from the sources and so on?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I start with the basic concepts.",
                    "label": 0
                },
                {
                    "sent": "I think I don't have to explain what ontologies are on this conference, so I just go to the next point, which is ontology construction.",
                    "label": 0
                },
                {
                    "sent": "As everybody knows, it's very cumbersome and complicated and can be very expensive and that's why we have methods such as ontology learning or in more recent times, or using crowdsourcing to improve and bootstrap this process.",
                    "label": 1
                },
                {
                    "sent": "Crowdsourcing rather to verify.",
                    "label": 0
                },
                {
                    "sent": "Things which will learn Indian Tower from the ontology.",
                    "label": 0
                },
                {
                    "sent": "Learning ontology.",
                    "label": 0
                },
                {
                    "sent": "Learning is using supervised and unsupervised methods through semi automatically generate ontologies from data and traditionally this is a bit simplified but traditionally it has been done from one input source, usually at the main corpus which is used for the ontology learning to learn the ontology from the data.",
                    "label": 1
                },
                {
                    "sent": "And now we have a look at as a set before using many sources.",
                    "label": 0
                },
                {
                    "sent": "How to integrate them.",
                    "label": 0
                },
                {
                    "sent": "And especially how to balance the input from those sources?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we have a brief look at our ontology learning system which we are using for this experiment.",
                    "label": 1
                },
                {
                    "sent": "The process is very simple.",
                    "label": 0
                },
                {
                    "sent": "We start from a small state ontology which is usually just a couple of concepts from a domain and then we look in evidence sources.",
                    "label": 1
                },
                {
                    "sent": "I will talk about it later for evidence for new concepts we gather all that evidence and then we have an algorithm to select the most important concepts found in the evidence.",
                    "label": 1
                },
                {
                    "sent": "And reposition them in the ontology in the existing ontology.",
                    "label": 0
                },
                {
                    "sent": "And this gives us the extended ontology, and we can repeat this process.",
                    "label": 0
                },
                {
                    "sent": "We can use this extended ontology again as seed ontology and further extended in a couple of iterations to make it bigger and bigger until we are satisfied with what we have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ontology is.",
                    "label": 0
                },
                {
                    "sent": "This is a graphical snipplet of part of an ontology are very lightweight, so we usually have concepts.",
                    "label": 0
                },
                {
                    "sent": "We have taxonomic relations and we have unlabeled non taxonomic relations in these ontologies and the different colors on the on the slides of the concept just reflecting what iteration step the concepts have been learned.",
                    "label": 0
                },
                {
                    "sent": "So this is about climate change or a seed concept was just climate change itself and then we learned this.",
                    "label": 0
                },
                {
                    "sent": "Blue at the screen.",
                    "label": 0
                },
                {
                    "sent": "Concepts in the next step and then the light green concept and so on.",
                    "label": 0
                },
                {
                    "sent": "In this extension process.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A more detailed look at the system itself, but still keeping this short because it's just a help to understand the rest how the system works.",
                    "label": 0
                },
                {
                    "sent": "We have to cede ontology.",
                    "label": 0
                },
                {
                    "sent": "We collect evidence from evidence sources, we integrate all that evidence, which is then usually network of many thousands of terms and their connections into a semantic network.",
                    "label": 0
                },
                {
                    "sent": "And from this semantic network with the help of spreading activation, which is an algorithm to find important nodes in the network.",
                    "label": 0
                },
                {
                    "sent": "We select the most important concepts.",
                    "label": 0
                },
                {
                    "sent": "These concepts are then presented to domain experts which evaluate the content concepts if they are relevant to the domain or not.",
                    "label": 0
                },
                {
                    "sent": "This is also the only stop step in the system where we have some manual inputs and manual verification of results.",
                    "label": 0
                },
                {
                    "sent": "And then this verified concepts will be purchased.",
                    "label": 0
                },
                {
                    "sent": "Positions in existing ontology and this gives us the extended ontology which can then be used again as seed ontology and further extended.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic gist of how this system is working.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, now we'll have a look at the evidence sources, because that's an important factor in the system which learns from various evidence.",
                    "label": 0
                },
                {
                    "sent": "Sources will have a look at them.",
                    "label": 0
                },
                {
                    "sent": "What sources are there and what are their characteristics?",
                    "label": 0
                },
                {
                    "sent": "So for even for each given seat concept, the evidence sources provide terms and relations to the system.",
                    "label": 1
                },
                {
                    "sent": "In the current configuration, where the evidence sources are mostly based on the main text, we use keyword extraction and lexical syntactical patterns.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "Patterns for extracting evidence there.",
                    "label": 0
                },
                {
                    "sent": "But we also use social media sources and structured sources like DB pedia or word net which provide evidence to the system.",
                    "label": 0
                },
                {
                    "sent": "In sum, we have 32 evidence sources which are used which are very heterogeneous, which I have a different quality of terms.",
                    "label": 0
                },
                {
                    "sent": "They provide different number of terms they provide and so on.",
                    "label": 0
                },
                {
                    "sent": "So we have to see how we can integrate it in a meaningful way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a brief overview of the data sources which we have as I already said, text is very prominent, so it's text mirrored from the web.",
                    "label": 0
                },
                {
                    "sent": "So what we basically do, we learn ontologies every month from scratch from this small seed ontology.",
                    "label": 1
                },
                {
                    "sent": "And we do this with data collected in that month.",
                    "label": 0
                },
                {
                    "sent": "So every month we have snapshot which represents the domain as it occurs in the sources.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so each.",
                    "label": 0
                },
                {
                    "sent": "Each month we learn a new ontology representing the data from the month.",
                    "label": 0
                },
                {
                    "sent": "And in those text sources we have news media sources mirrored from the web from different countries.",
                    "label": 0
                },
                {
                    "sent": "USUK, and so on.",
                    "label": 0
                },
                {
                    "sent": "We have social media sources, Facebook, Twitter and the usual candidates for social media.",
                    "label": 0
                },
                {
                    "sent": "And we have some other samples which we mirror Fortune 1000 companies, their websites and stuff like that and we have the other kinds of sources like API's, Twitter and Flickr have an API which provides for a given input term.",
                    "label": 0
                },
                {
                    "sent": "You can give it an input Romanet gives you related terms, so terms related to the term.",
                    "label": 0
                },
                {
                    "sent": "In Flickr, I guess it's just if you have tax on a photo, which other tax usually Co occur with this tech.",
                    "label": 0
                },
                {
                    "sent": "So this is another kind of source which provides you terms and we have.",
                    "label": 0
                },
                {
                    "sent": "Structured sources were not in DB pedia.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm Anne from this data sources, which are not the same as the evidence sources we now get to the evidence versus for each data source we can have multiple evidence sources.",
                    "label": 1
                },
                {
                    "sent": "So for example, US news media text, we can apply different algorithms cooccurrence algorithms to generate keywords on a page level on a sentence level.",
                    "label": 0
                },
                {
                    "sent": "We can use first patterns, so these are different sources and then some for all the text based sources, we have 26 evidence sources.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the other ones, there are 6 remaining from word net hyponyms hypernyms synonyms for a given term, and the API queries and sparkle used on DB pedia to get with a couple of predicates which we're using to get terms related to an input term from DB pedia.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you an idea how such evidence looks like, this is a list of terms which we got at some point.",
                    "label": 0
                },
                {
                    "sent": "I think from UK news media sources for the input concept, input label CO2 and these are keywords generated from that corpus which we generate which we have for that given point in time.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, some of the terms are really relevant to the domain.",
                    "label": 0
                },
                {
                    "sent": "Some are too specific, like maybe I don't know this PM 10 which is I think about.",
                    "label": 0
                },
                {
                    "sent": "Little particles which are in the air and which doesn't don't really have something to do with climate change or not.",
                    "label": 0
                },
                {
                    "sent": "A lot and terms, which are two general like air or maybe two general depends on how you will model the domain.",
                    "label": 0
                },
                {
                    "sent": "So it's different quality.",
                    "label": 0
                },
                {
                    "sent": "What you get from the sources, so the individual sources pry provide ladder rather low quality results, but by aggregating them we want to get better concept candidate.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Upstairs was one too much.",
                    "label": 0
                },
                {
                    "sent": "So what are we doing with all this evidence we're getting?",
                    "label": 0
                },
                {
                    "sent": "I think you already said that we put it in a semantic network, which is then tends to be very big couple of thousands or even 10s of thousands of terms and their relations in this big network.",
                    "label": 1
                },
                {
                    "sent": "And then we select the configurations.",
                    "label": 0
                },
                {
                    "sent": "Now with like 25 counts of candidates, that's just a setting which we're using per extension step where we extend the ontology.",
                    "label": 1
                },
                {
                    "sent": "So we have 25 candidates generated by the spreading activation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is how we get to the Council candidates from the evidence.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which brings us back to the research question or what this is all about.",
                    "label": 0
                },
                {
                    "sent": "Um, my tip is sources have to ban at the benefit that they, I mean a benefit.",
                    "label": 0
                },
                {
                    "sent": "That's a question they provide redundancy, and they provide complementary information and the general assumption of this research, which is also backed by literature, is that redundancy can help you to find.",
                    "label": 1
                },
                {
                    "sent": "You can see it as a measure if something occurs in multiple sources, you can see there's a measure of trust because it occurs in multiple sources, or maybe is more relevant than something which occurred in just one source and the measure of relevance.",
                    "label": 0
                },
                {
                    "sent": "So this thing we want to use or leverage to improve the quality of our ontologies.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Yeah, and therefore heterogeneous sources provide the opportunity to have higher levels of accuracy, so the research questions are now how many sources do we need to make use of this facts of this hopeful facts?",
                    "label": 1
                },
                {
                    "sent": "The redundancy helps us and how much evidence should be used for source and so on, which we will look at in the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what did we do?",
                    "label": 0
                },
                {
                    "sent": "We did a lot of experiments over some over couple of more than a year learning ontologies and different domains with different settings.",
                    "label": 1
                },
                {
                    "sent": "Seeing what are the effects of all those parameters which we're using.",
                    "label": 0
                },
                {
                    "sent": "And the goal was to find an answer which can be at least to some point generalized also to other systems, ontology learning systems or just systems which make use of different evidence sources which combine things.",
                    "label": 1
                },
                {
                    "sent": "So that was the point, the goal did not just have something for our system, but at least at some point the IT has have to be made experiments to see if it works in other systems, but that we think because we have a very intuitive evidence.",
                    "label": 0
                },
                {
                    "sent": "Creation logic there, spreading activation that we can expect at least similar results with other integration logics, which combine evidence from different sources.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what was the setup of the evaluation?",
                    "label": 0
                },
                {
                    "sent": "So we had two domains, climate change and tennis.",
                    "label": 1
                },
                {
                    "sent": "Um and yeah, we try it.",
                    "label": 0
                },
                {
                    "sent": "Those different settings, number of sources, evidences per source generated ontologies over time frame from July 2013 to November 2014 and had the results.",
                    "label": 1
                },
                {
                    "sent": "The overall.",
                    "label": 0
                },
                {
                    "sent": "The resulting ontologies where were which were like 10 or 15 or even more per month assessed by domain experts.",
                    "label": 0
                },
                {
                    "sent": "Always checking if the concept can.",
                    "label": 0
                },
                {
                    "sent": "How many concept candidates produced are really relevant to the domain.",
                    "label": 0
                },
                {
                    "sent": "According to the judgment of the domain expert, so your currency ratio is really simple.",
                    "label": 0
                },
                {
                    "sent": "It's something like it's more or less precision.",
                    "label": 0
                },
                {
                    "sent": "Relevant concepts generated divided by all concepts generated by the system.",
                    "label": 0
                },
                {
                    "sent": "Which we will use in the evaluations.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But why is this balancing needing and needed anyway between the sources?",
                    "label": 1
                },
                {
                    "sent": "So couldn't we just use all evidence produced by all sources and would be fine?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "At least in our case, and I guess that will be also the same thing will happen in many other systems.",
                    "label": 0
                },
                {
                    "sent": "The sources have very different characteristics.",
                    "label": 0
                },
                {
                    "sent": "First of all, the number of evidence produced per source can vary a lot.",
                    "label": 1
                },
                {
                    "sent": "So in our system, the keyword sources, the cooccurrence keyword sources which produce keywords from a text corpus, mirrors from, uh, from some news media source, for example.",
                    "label": 1
                },
                {
                    "sent": "They usually provide us with a lot of evidence.",
                    "label": 0
                },
                {
                    "sent": "Which has a significant which is ordered and we have between an average 200 and 400 keywords depending if it's sentence level or pH level keywords.",
                    "label": 0
                },
                {
                    "sent": "For other sources we have much fewer.",
                    "label": 0
                },
                {
                    "sent": "A lot fewer evidences for DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Example on average per input concept.",
                    "label": 0
                },
                {
                    "sent": "Proceed concept which put into DB pedia without sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "We have 13 evidences only so there's a big gap and if we would just use all the sources then the keyword sources with just overwhelmed.",
                    "label": 1
                },
                {
                    "sent": "All the other things which go into the system.",
                    "label": 0
                },
                {
                    "sent": "And there's also a big difference between quality of the sort of the sources we manually had a look at all the terms or not all the terms, but a subset of the terms from every source, and to verify how many terms are relevant to the domain.",
                    "label": 0
                },
                {
                    "sent": "And there is a big difference.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Twitter API, for some reason it gives us a quite a lot of an information which doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "Or does it is not useful.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Abbreviations and stuff like that or just stuff which is not relevant to the domain.",
                    "label": 0
                },
                {
                    "sent": "So only 10% of the terms from the Twitter API were relevant.",
                    "label": 0
                },
                {
                    "sent": "Various for other sources like DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "It's almost 30% and for the keywords it depends.",
                    "label": 0
                },
                {
                    "sent": "So if the keywords are ordered by significance and the first one is the first went top 25 have acquired quite good and the lower your gets the lower the significance.",
                    "label": 0
                },
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "Uh yeah and the keywords get worse.",
                    "label": 0
                },
                {
                    "sent": "So it depends how many key was it.",
                    "label": 0
                },
                {
                    "sent": "Off the keywords are using the system, so there is obviously a need to think about how can we balance all this evidence which goes into the system.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we made the experiments for and I would just jump to the first experiment roommates.",
                    "label": 0
                },
                {
                    "sent": "Here, we're looking at the number of evidences we use that we have to 30, two sources and.",
                    "label": 0
                },
                {
                    "sent": "Percocet concepts they give us evidence and we limit this number of evidences to a number which is given in the left hand column so we can say we use only five terms.",
                    "label": 0
                },
                {
                    "sent": "Suggested person concept or we use more like whatever up to 500 or even don't limit it at all and if we limit it to a very low number then well this is also quite intuitive then like the learning algorithm will just not have enough information to make a really.",
                    "label": 0
                },
                {
                    "sent": "To really benefit from all this aggregation and redundancy so the accuracy is rather low, but also as we can see if we use a lot of information and we use all the keywords even with lower significance and the overwhelm, the other evidence sources, then the accuracy is going down again.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhere in the middle between in our system 20 to 50 terms per evidence source.",
                    "label": 0
                },
                {
                    "sent": "An concept gave us the best results, the best accuracy to the system.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a system where you have, at least for some sources in order of by quality of your evidence is, then it will be better to use more evidence.",
                    "label": 0
                },
                {
                    "sent": "Becausw we tested this with them just picking random keywords and not the best keywords and then when we use not so much evidence then results will not be so good because maybe the best keywords will be at the at the end of the list.",
                    "label": 0
                },
                {
                    "sent": "So it depends on how you saw your evidence is structured.",
                    "label": 0
                },
                {
                    "sent": "If you have some ranking in there or not.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I think is more interesting even is then how many sources do you really need to benefit from your app from the aggregation from redundancy and we try a different settings just using one source using five sources in the ontology learning system using 15 sources, or using all the 30 two sources which we have.",
                    "label": 0
                },
                {
                    "sent": "And if you use only one source, especially if it's one source which provides you low quality terms like Twitter, then.",
                    "label": 0
                },
                {
                    "sent": "Obviously, or is it?",
                    "label": 0
                },
                {
                    "sent": "The learning algorithm has no chance to produce a good ontology or ontology with a lot of relevant concepts from from that as you can see, only 20% accuracy for Twitter.",
                    "label": 0
                },
                {
                    "sent": "If you have a high quality source.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you can easily get to 50% with.",
                    "label": 0
                },
                {
                    "sent": "In this case keywords from UK media, five sources then start to give you the benefits of aggregation and of redundancy and using this in the spreading activation algorithm.",
                    "label": 0
                },
                {
                    "sent": "Raising the accuracy of the 60% or something like that and 15 sources we saw for, at least for our experiments, were enough to get all the benefits from redundancy.",
                    "label": 0
                },
                {
                    "sent": "So going from 15 sources to 30 two sources, the dishes blowed up there.",
                    "label": 0
                },
                {
                    "sent": "The whole system made the computations are much, much lower, made the semantic network huge, but didn't give any significant improvements to the system anymore.",
                    "label": 0
                },
                {
                    "sent": "So that's maybe an important point if you have many sources.",
                    "label": 0
                },
                {
                    "sent": "You would really have to check how many do you really need to do to get those benefits from from redundancy and complementary information.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then Lastly, we had a look at.",
                    "label": 0
                },
                {
                    "sent": "The number of sort of seat concepts which we have and how that affects the results.",
                    "label": 0
                },
                {
                    "sent": "Stages are charged how the points where we iterate your weather iterations comes into play come into place.",
                    "label": 0
                },
                {
                    "sent": "So this stage one is when we just have the two seat concepts and nothing else and use this to learn new concepts and in stage two we have the two seat concepts.",
                    "label": 0
                },
                {
                    "sent": "Plus they learned concepts from the first stage which are then about 20 concepts and so on and.",
                    "label": 0
                },
                {
                    "sent": "If you have only a limited number of seat concepts, then obviously which also makes sense intuitively.",
                    "label": 0
                },
                {
                    "sent": "You needs at least a lot of evidence for those two seat concepts to get to some meaningful results.",
                    "label": 0
                },
                {
                    "sent": "So in the first stage, using only 5 limited 5 gave us really poor results.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, because the seat concepts are really relevant to domain to the domain, they are the most top concepts in a way in the hierarchy when you use a lot of evidence then you also get a lot of good.",
                    "label": 0
                },
                {
                    "sent": "A relevant concept.",
                    "label": 0
                },
                {
                    "sent": "And when you have, then when you have more seat concepts than even for limit of five evidences per concept, you still get decent results because you have so many concepts and you have trusted the the sum of all this.",
                    "label": 0
                },
                {
                    "sent": "This terms which gives you the bonus of redundancy again.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one, yeah, I'm.",
                    "label": 0
                },
                {
                    "sent": "I I didn't check.",
                    "label": 0
                },
                {
                    "sent": "Then I'm already coming to the results.",
                    "label": 0
                },
                {
                    "sent": "Yeah, take away point.",
                    "label": 0
                },
                {
                    "sent": "So in a system like that, if it's in a similar configuration or whatever, a few thousand terms will be enough to leverage all these effects.",
                    "label": 1
                },
                {
                    "sent": "Which redundancy and complementary information.",
                    "label": 0
                },
                {
                    "sent": "If you're bloated up and make it bigger, use more evidence.",
                    "label": 1
                },
                {
                    "sent": "Maybe it won't give you much much benefit, so this can help too.",
                    "label": 1
                },
                {
                    "sent": "If you set up a new system to really check, do you need all this information or is it just?",
                    "label": 0
                },
                {
                    "sent": "Do you just slow down your computations by having too much input and not in terms of how many sources in needs?",
                    "label": 1
                },
                {
                    "sent": "We came to the conclusion like 10 to 15 heterogeneous evidence sources are enough to gain from the oldest benefits from redundancy, which I was talking about all the time.",
                    "label": 0
                },
                {
                    "sent": "And yeah, balancing input is generally more important than just the wrong number of evidence, so it's generally better to have different sources which give you input than one source which gives you a lot of inputs.",
                    "label": 0
                },
                {
                    "sent": "And yeah, nothing else.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, if you to work, yeah, we obviously want to test this system in different domains.",
                    "label": 0
                },
                {
                    "sent": "We would like to have a look at other systems and test them with the same assumptions and see if our claim that this is at least partly generalizable really holds.",
                    "label": 1
                },
                {
                    "sent": "And finally we it's in the current version.",
                    "label": 0
                },
                {
                    "sent": "Every evidence source had the same input impact on the learning algorithms, so we also are now optimizing the system to give evidence sources which provide better keywords or better term.",
                    "label": 0
                },
                {
                    "sent": "A higher import or higher weights in the learning algorithm and thereby can optimize the system even more.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's it for me.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "If there any questions.",
                    "label": 0
                },
                {
                    "sent": "So these are question about the ontology that you learn through the system.",
                    "label": 0
                },
                {
                    "sent": "So when you say it's a non tallied, do you mean that it's a set of triples?",
                    "label": 0
                },
                {
                    "sent": "Are is it only simple subclass relationships or does it have something beyond that likes a union of concepts are cardinality constraints and now we have we're doing this which separated light weights and I think most ontology learning systems they some try to turn him to learn axioms.",
                    "label": 0
                },
                {
                    "sent": "But most ontology learning systems focus on rather lightweight ontology, that is concepts up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, subclass relations, non tech, just relations with without a non taxonomic relations, but unlabeled so you don't really know how things are connected to just that they are connected.",
                    "label": 0
                },
                {
                    "sent": "So that's mostly what we're learning with this system.",
                    "label": 0
                },
                {
                    "sent": "You showed that 15 sources was this sweet spot where we could get the most out of it, and I assume that the pool of sources available where these 32 or maybe more.",
                    "label": 0
                },
                {
                    "sent": "The question is to get to this 15 number.",
                    "label": 0
                },
                {
                    "sent": "Did you try multiple combinations like multiple groups of 15 and then you got to this conclusion?",
                    "label": 0
                },
                {
                    "sent": "Or how did you proceed together?",
                    "label": 0
                },
                {
                    "sent": "Yes we but tried a couple of different groups so we had to two domains.",
                    "label": 0
                },
                {
                    "sent": "10 years in climate change and there we try that.",
                    "label": 0
                },
                {
                    "sent": "Not too many.",
                    "label": 0
                },
                {
                    "sent": "A couple of different combinations, but it gave roughly the same result.",
                    "label": 0
                },
                {
                    "sent": "Becaused those sources they have similar characteristics, especially if the keyword sources does not much different.",
                    "label": 0
                },
                {
                    "sent": "If you take keywords from US media or UK media.",
                    "label": 0
                },
                {
                    "sent": "So if you change those it it has a very low impact on on your result.",
                    "label": 0
                },
                {
                    "sent": "So yeah, but I guess I mean it was always a mix of very heterogeneous sources, so we didn't ever try to just have keyword sources or something like that, but different kinds of sources with different characteristics and then mix them together to get to those 15.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Alright, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I am interested in how did the concepts evolve overtime in your sources.",
                    "label": 1
                },
                {
                    "sent": "So can you say something about in which month it was especially?",
                    "label": 0
                },
                {
                    "sent": "Or if they changed overtime so much, that's something which is one of the future work will be already started doing it, and we have also a small platform where we can check which concepts are added in one month and get removed.",
                    "label": 0
                },
                {
                    "sent": "And there's always a shift.",
                    "label": 0
                },
                {
                    "sent": "And obviously, for example in climate change domain you have.",
                    "label": 0
                },
                {
                    "sent": "You also have cyclic patterns where for example in summer time you have concepts like melting glaciers and stuff like that which happens in summer time and this is reported then, but not in winter time.",
                    "label": 0
                },
                {
                    "sent": "And things like that.",
                    "label": 0
                },
                {
                    "sent": "So there are definitely things which which have yet trends and patterns rising, falling, cyclic.",
                    "label": 0
                },
                {
                    "sent": "But we didn't do a really study on it, was just looking at the data we saw.",
                    "label": 0
                },
                {
                    "sent": "It's there when we want to do that in the future to to compare the ontologies and see how how they go off.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank God again.",
                    "label": 0
                }
            ]
        }
    }
}