{
    "id": "2q3chqvtpsktojlto5fk65rg6ti6ud3t",
    "title": "Cascading Map-Side Joins over HBase for Scalable Join Processing",
    "info": {
        "author": [
            "Alexander Sch\u00e4tzle, University of Freiburg"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "November 2012",
        "category": [
            "Top->Computer Science->Semantic Web->Applications"
        ]
    },
    "url": "http://videolectures.net/iswc2012_schaetzle_join_processing/",
    "segmentation": [
        [
            "OK, so welcome everybody tomorrow.",
            "Talk about cascading Max.",
            "I've joins.",
            "This is joint work with the people listed below here which is Martin, Christopher Thomas and Professor Lawson.",
            "Fur."
        ],
        [
            "I've also very, very short motivation because I think the title of this workshop is the motivation itself, because if we are singing about scalable or high performance, semantic web systems always talking about big data sets and constantly growing datasets of single off the linking of data cloud and all the other things we don't know.",
            "And as we also argue about it is that we query in these kind of datasets at web scale is delicate and Carson becomes even more challenging if we sing of future, hopefully future bigger datasets that we will get in this area.",
            "For storing and creates empty OK, our approach is somehow different to disgrace and T, so we always from the beginning.",
            "We were looking to make to implement scalable at F engine based on commonly used public frameworks for cloud system.",
            "Like most of you may use Hadoop MapReduce or May know H base and all this.",
            "The project around these MapReduce world.",
            "This was a basic principle in the biggest."
        ],
        [
            "From the beginning.",
            "You know what?",
            "Just talk about previous work we have done using Spark occurring on map reduce in order to say what we wanted to change.",
            "'cause we're not satisfied with the results in some points.",
            "The work before would name is pink Sparkle.",
            "This is because essentially what we've done is we've made a translation from the Sparkle Query language to the pig Latin query language.",
            "With this data analysis language, high level data analysis language on top of Hadoop developed by Yahoo Research.",
            "I don't want to talk a lot about this pig Latin word.",
            "Essentially, what does stasis?",
            "We're taking a sparkle query, we translate it into this pig Latin program, which is executed by the Apache Freak framework on and Hadoop.",
            "Cluster we have to load the data set into the distributed file system of Hadoop as basically was the idea of the beginning.",
            "The better the advantages of these things is that we have all the operators from the Sparkle Query language implemented that we have some benefits because often these Apache pick framers and really ongoing implementations or we benefit from optimizations directly from Apache from the open source world.",
            "It runs out of the box, so we do not have to Jane make any changes to do.",
            "How do framework at all?",
            "You can directly and you can also run it on Amazon Cloud and its principles portable to other platforms because the PIC platform is in principle portable to other.",
            "Platforms as well, not only Hadoop MapReduce an, of course we under map reducing the batch world.",
            "This is where reasonably good scalable we were satisfied with this underperformance for some complex analytical queries says let's say more careers where the output is relatively high, but it's not really satisfying in these kind of framework is that if we have more selective queries like most of the lumen queries, whether the output is rather small compared to the input.",
            "This is something where this kind of system is not really made for.",
            "The reason for that is that the normal traditional join execution member useful is often done by so called Reducer join.",
            "Most of you may know the basic principle where we have to shuffle the datasets from.",
            "We checked in the map phase we take the datasets we have to shovel the datasets, network and do the joint processing and reduce phase, which means for selective queries we have to shuffle a lot of unnecessary data which is discarded in the reduce phase, which means that there's a lot of IO going on.",
            "And we have no built-in index structures in the Hadoop distributed file system out of the box.",
            "So what we basically tried in our new approach was to improve performance on a."
        ],
        [
            "MapReduce based system.",
            "In case of more selective queries that where the output is rather small compared to the input size.",
            "Now idea was to use H base as an additional storage layer on top of HTFS, mostly because we wanted to have these kind of random access to the data which is not possible in HDFC.",
            "In Planet Rivers I will come to that point later and we wanted to get rid of the shuffle and reduce phase.",
            "In MapReduce we just wanted to.",
            "Have to shuffle the data that is really needed for the join in order to reduce the oil cost.",
            "So we just wanted to do it in a map face.",
            "So what we've done is the change in this system parameters we use not only HDS and storage layer, but also each base on top of it and for the all pro tight implementation we do not use the pig framework anymore but made a direct implementation on map reduce so they expected drawbacks.",
            "Also.",
            "Drawbacks from these concept is we increase the communication overhead as we have to talk with these.",
            "No SQL Data store for every request access.",
            "We will have strictly higher RAM consumption, which is especially important for our cluster, which is rather limited in kind of RAM configuration and this is not perhaps not an idea of the solution for high output queries, which is we are not addressing it these things we wanted to improve for more modern, more selective ones were not satisfied about this.",
            "OK, some things about how we."
        ],
        [
            "Or data in the H base no escape."
        ],
        [
            "Data Store first of all, I think most of you may know what is H order hasn't been super idea.",
            "What age bases age base is a clone of Google's big Taylor.",
            "It's a column oriented, semi structured, no SQL Data store.",
            "Mostly it's used in Hadoop cluster.",
            "And it's layer on top of HTFS means the data in H base, essentially stored in HDF.",
            "But using a custom appropriate file format such that the normal drawbacks of HTFS that you only have access to the data in a batch side.",
            "If you want to access random data, you have to scan over the complete over the data set and look for the entries you want to.",
            "This is something we can see from our perspective.",
            "We can see a space as in.",
            "Storage layer on top of HTFS with ads, these random data access facilities to HTFS in what the people from HSS claiming nearly close to real time, and it's a strictly consistent, no escalator, so mean said every data access is guaranteed to have the latest word.",
            "What it is not is also important.",
            "This is not a relational query engine long, so it doesn't have a normalized schema.",
            "It does not have any further processing processing operations, so if you want to make a join, you have to decide whether this is left for the application of topliffe it you cannot do and join in H pairs of its own, and it does not have an expressive query language, SQL or something like that.",
            "So the data model is.",
            "Sparse distributed."
        ],
        [
            "Sorted multidimensional map.",
            "It's perhaps easier understand if we look at the access pattern in the most basic things we have tables in H base and the rows of these tables are sorted and indexed by the roci, which is the key and the columns of these tables are grouped into families and we have can have different versions of every value identified by timestamp.",
            "So.",
            "And of course, these tables are then split into regions, and every regions are stored on one node in the machine in order to distribute the access.",
            "This is not the important thing for what I would want to talk about.",
            "Look at a simple."
        ],
        [
            "Example, which illustrates how we store the data.",
            "Look at this simple RDF graph about some, let's say articulation outdoors and authors of articles and artikkel side other articles.",
            "What we do is we use A2 table schema in this case where we have under one table.",
            "This is T SPO table.",
            "This notation indicates that we use the subject here in this case of the triple AO Ki means that we have an index on subject position for example.",
            "So here.",
            "The article one in the subject for the subject of these RDF graph in the roci and what we do now is we use the predicates as the column names in HH Basin used installed actually value the objects of the triples as in the value precision and if we have a multi value predicates like the author one we stored in the same role in the same column different using different timestamps to store the values.",
            "And the important thing to notice is that.",
            "Rose in an age based table doesn't have to have all the same columns.",
            "For example, this one has an additional column with the predicate side which is not existing in the mean.",
            "We do not have any null values in this case, so.",
            "We only have these two things.",
            "We have SPO and Opsi will talk about what we came to.",
            "This kind of decisions about.",
            "The other thing is the opsa table which is send it the same design as the SPO table but for objects.",
            "The key.",
            "The other question was, let's say we could use 6 tables in order to support all the different combinations of patterns, But this would ensure essentially mean that we would store the data 18 times in HDFC because of the replication and all these things and we are using a special functionality of.",
            "H base with the field API to overcome these drawbacks of these missing proper index tables, make an example like here for example, look at this kind of pattern where we have given subject given predicate and."
        ],
        [
            "Looking for the object so it would be nice to have a table with an index on subject and predicate combination.",
            "In our design we have only did at the table with an index on the subject position, but what we can do in edge cases we can add filter to requests.",
            "In this case we would add some column filter becausw.",
            "The predicate is our column names, so the column is known in advance.",
            "What we do is we restrict.",
            "In this case the request for the corresponding.",
            "Predicate we want to access and as the number of predicates and also here in the most of the scenarios is rather smaller.",
            "We compare this one to the six if using Zix tables.",
            "The performance we got in our system was essentially not much worse than than storing six different tables with everything having index on all of it.",
            "Because this filters is the interesting thing, is all these filters are directly applied?",
            "From running and see one of the most common way Parton says, when you have subject and object unbound right, and in this case you will discount.",
            "If we have, The thing is if we have a table or index on predicate, this would not have a lot of us before us becausw they are most of the datasets, the product number of predicates is rather small ones, so we would have a table with only a small with a small number of rows where each row stores nearly a big portion of the data set would also explain for scalability because every row is stored on one machine in the cluster.",
            "So if we if the data set increase, these roles becomes bigger and bigger.",
            "And then some in some point in time the limit the resources of a single machine in the cluster becomes a problem for scalability.",
            "Which we didn't want to have.",
            "So the interesting thing from the services that they applied on the filter system they applied on the server side means that not the data is transferred, and afterwards the filters applied on these columns.",
            "But either every single region server H base applies directly.",
            "These filters on server side means that only the value is matching the given predicate really transferred to the client requesting the data."
        ],
        [
            "OK. That's."
        ],
        [
            "Look at what we're doing or what we how do we use these HP storage in order to process this?",
            "These relatively simple sparkle query.",
            "The idea is relatively simple.",
            "It's from the relational databases units known as normal index, nested loop join.",
            "Like for example, we begin for the first triple pattern.",
            "We do some some scan on an H base table which is executed in parallel.",
            "In Hadoop clusters means we retrieve all the mappings for these first triple pattern is always getting locally from old age based region servers and the results from these mapping are fed into the map phase as an input.",
            "Let's for example we've seen this.",
            "This is a specific instance of the map running in.",
            "On one node.",
            "You have to think of it.",
            "This is what this is running on every node in the cluster in parallel, and what we assume here is we get an input mapping with, let's say artikkel one and and title for this article.",
            "What we now have to do in these map phases we have to find the corresponding mappings for the second triple pattern.",
            "Also for the article one and what we didn't do is optionally we make a substitution that ripple pattern.",
            "We use the mapping we already have.",
            "Substitute develops we already know and then access these people pattern.",
            "Using these mapping I've illustrated here and this case we would access the TSB oh table using the artikkel azero key defining a filter for the author's column and retrieve the object.",
            "We want to have.",
            "It is the same for optical tool and then we can output the output of the metaphase is the new mapping that is the solution to the joint between the combination of the first 2 triple patterns.",
            "And obviously, did the what we have mappings in the input of the map phase and we have mappings in the output of my face.",
            "So these two things are directly compatibles or this makes us easy to just iterate over these kind of joints.",
            "So essentially from one triple to the next we can iterate these met phases.",
            "We do not need any shuffle or reduce phase in order to process the data for the next step which is in most of the other map side.",
            "Drawing approaches in MapReduce you mostly have problems if you want to compute these joins over and over again.",
            "OK, so the idea."
        ],
        [
            "It's rather simple one.",
            "What we now look at."
        ],
        [
            "For this example I showed before."
        ],
        [
            "Article One, Article 2 If we look at the overall execution we have.",
            "As I said, we would need two iterations and.",
            "These are these common pet."
        ],
        [
            "We start like query patterns where we have essentially here the join key is always on.",
            "Our ticket was always the article and if we look at what's happening in the 1st and the 2nd iteration then what we see is that we are in both iterations.",
            "We essentially doing the same.",
            "We are using the article here.",
            "The input mapping article one for the rookie and the same for this one.",
            "But they are the only thing which differences is the is the filter we are using to restricting the special other bindings.",
            "We want to get so.",
            "What's essentially is that the result for this Sir triple pattern is not really.",
            "It doesn't really depend on the result from the second one, 'cause the author binding is not really the important thing for next steps or what we also could do a diss iteration.",
            "We can get rid of these two iterations we."
        ],
        [
            "Also apply this in a single iteration.",
            "Let's say we start with the Article one asking for the authors of the article, and if there are results to this request, then we afterwards ask for the year of this article and can directly do is in one phase.",
            "And if there are no results for the first request, we can directly skip the second request and drop the mapping from the result list.",
            "The same for the second pattern AN.",
            "Now again, we're nearly finished what we have done is reduced the number of iterations from 2 iterations to one.",
            "And Furthermore, if we consider the data design, what we do here is we do two requests on the same table using the same row key.",
            "The only difference is again is the column, so this is also."
        ],
        [
            "It really necessary if you look at the H Base Request API Becausw in HPS we cannot only apply filter 2 and request, instead we can also apply a list of filters in an arbitrary combination of filters.",
            "So what we can do is we can use this request and somebody."
        ],
        [
            "To request you can directly use one request where we not only use a filter for the hour thought columns on again use a filter for the column with the corresponds to year so that we knew is only one request to answer the order to get the mapping which matches the 2nd and so triple pattern.",
            "So and we end up with.",
            "Have you only one iteration instead of two variations?",
            "And we reduce the number of requests from 4 to only two requests, and if you think of this is a small small except of the overall computations of this runs in parallel.",
            "Every machine for many mapping, so this is an important thing, especially to reduce the number of requests because the number of requests against each node square H data source.",
            "The main key driver in the query execution time we use.",
            "This using our system H base is something we know there's a space was able to handle something about 10,000 requests a second, so directly the query execution time relates to the number of requests you have to set against age based in order to."
        ],
        [
            "Answer the query OK Again, for some want to talk about some results for the loop and benchmark."
        ],
        [
            "First of all, our cluster setup wasn't pretty small cluster with 10 nodes.",
            "For us more power Power Edge servers are 200 great, 8 gig of RAM which is rather small configuration for systems running these nodes called data stores and frameworks we use was the Cloudera distribution for Hadoop.",
            "Most of you may know we use the H base in the 1990 version and we generated some datasets for the loop M University.",
            "Number from 10,000 two 1000 universities.",
            "What we did is we applied the reasoning before executing queries in order to have not rewrite the queries and so the biggest data set brass, something around 630 million triples.",
            "One of these nodes in the cluster is always used as a as a master node is not really in compute node, it's it's running all the master deems used for the database and for the name node and all these other things.",
            "OK, and what's up?"
        ],
        [
            "Really surprising is if you look at the Curie one of Le Pen which is in this case are really the base case is also only has a single joint involved and we look at the comparison of the weather.",
            "What we do here is a comparison of the system we use before these pics bugs are mentioned where we use the reducer.",
            "I draw an approach in a map reduce scenario.",
            "Now compare it to the this map side join approach where we want to express we improve the selective queries and this one is a really selective query.",
            "And as you see with this is 2 times the same graphics but with different scales.",
            "We've seen linear scale on the right side and we have a log scale on the left side for the linear scale.",
            "It's hard to read the numbers here, but what you can see is that both approaches called MapReduce framework, which is yes question.",
            "This way is the web PY inference functions for group Two.",
            "Yes.",
            "OK, So what you see here is the linear scaling behavior of both approaches, which is not really surprising because of the map reduce framework, but the most more interesting thing is that for the new approach is something around one order of magnitude faster compared to the we have to ship all the data which is nearly unnecessary and discarded afterwards in the reduce phase.",
            "This was resulting in sometimes from 8 to 30 times faster.",
            "Very good execution.",
            "The more interesting thing is something like for example query 4, which is more as more general case for approach, which is a sequence of joints and also interesting here is that we can have these multi way join optimization I mentioned or because this is a star pattern query and also what's illustrated here is the comparison between.",
            "Again the reuse side version execution in the cluster and the map side version execution.",
            "And what we see again is a difference in one order of executing a query execution time and again what we have done is also in the reduce side version approach.",
            "We use these multi joint optimization is also applicable in reduced side version that we only use one single map reduce phase to compute the overall result for the query and we do the same for the website version and overall again we get even certain times faster.",
            "Every situation using these optimizations where we use only one iteration instead of in the normal execution without module we would use 1234."
        ],
        [
            "Map reduce iterations to compute the process."
        ],
        [
            "No result.",
            "So for conclusion and future work would say the things to take away is what we have essentially done is that we implemented mechanism that produces the order that calculates the joins in completely.",
            "In that phase.",
            "We do not use any shuffle reduce phase in MapReduce.",
            "What's especially it was important for us is that we are able to iterate these these joins without any auxiliary shuffle and reduce phases that you normally have to use.",
            "For example, if you do some merge map side merge join which is interesting.",
            "Version of the map side.",
            "Join your wizard.",
            "The problem that you have some preconditions you have to fulfill for the input data set as they have to be sorted in some way, partitioned in some way.",
            "This is something we do not really use in this case and we again for the star pattern theories we can reduce the number of iterations which is the more important thing is created to reduce the number of H base requests and.",
            "What we hope to achieve is that we can outperform.",
            "To reduce that version for these more selective queries, which was clearly achieved.",
            "One thing to mention is OK.",
            "This new approach has also disadvantages because the performance of these technique degrades with the increasing variable output.",
            "If you can think of the factory output, largely output gets the more request against H space.",
            "We have to do the worst performance getting comparison and could be some some point in time where again the batch processing version of the user.",
            "Join where your ship all the data and output during the reduce phase could be even more efficient than the website version because of if you have too many requests against the database in H base, you can get into problems and for future work it's really obvious.",
            "One thing is we're looking at improvements for leftover scheme because we have some identified some problems concerning the scalability mechanism, like for example if we would have a table for predicates that we store essentially big portions of the data in a single.",
            "Rowan H basin.",
            "As Rose always thought on a single machine, then again we have the problem that only one machine can get a bottleneck in, which is essentially that the idea of MapReduce you want to prevent if you want to.",
            "If you want to process bigger datasets, we just use a bigger cluster.",
            "But this would help.",
            "In this case you have to increase the resources of a single machine, which is not really the idea behind MapReduce.",
            "And the second thing is, of course, if we handle if you have some kind of system which is good for for queries with high output, and we have other kind of system which is good for queries with a low output, then of course we want to have is incorporation of these two things depending on query only query selectivity or in the joint activity that we could use them or the effective version for the map side version for the joins if there's no incentive selective or use the more batch oriented version.",
            "If the output is.",
            "Bigger.",
            "OK."
        ],
        [
            "That's what I want to say.",
            "Thank you for attention.",
            "Are there any further questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so welcome everybody tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Talk about cascading Max.",
                    "label": 0
                },
                {
                    "sent": "I've joins.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with the people listed below here which is Martin, Christopher Thomas and Professor Lawson.",
                    "label": 0
                },
                {
                    "sent": "Fur.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've also very, very short motivation because I think the title of this workshop is the motivation itself, because if we are singing about scalable or high performance, semantic web systems always talking about big data sets and constantly growing datasets of single off the linking of data cloud and all the other things we don't know.",
                    "label": 0
                },
                {
                    "sent": "And as we also argue about it is that we query in these kind of datasets at web scale is delicate and Carson becomes even more challenging if we sing of future, hopefully future bigger datasets that we will get in this area.",
                    "label": 0
                },
                {
                    "sent": "For storing and creates empty OK, our approach is somehow different to disgrace and T, so we always from the beginning.",
                    "label": 0
                },
                {
                    "sent": "We were looking to make to implement scalable at F engine based on commonly used public frameworks for cloud system.",
                    "label": 0
                },
                {
                    "sent": "Like most of you may use Hadoop MapReduce or May know H base and all this.",
                    "label": 0
                },
                {
                    "sent": "The project around these MapReduce world.",
                    "label": 0
                },
                {
                    "sent": "This was a basic principle in the biggest.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the beginning.",
                    "label": 0
                },
                {
                    "sent": "You know what?",
                    "label": 0
                },
                {
                    "sent": "Just talk about previous work we have done using Spark occurring on map reduce in order to say what we wanted to change.",
                    "label": 0
                },
                {
                    "sent": "'cause we're not satisfied with the results in some points.",
                    "label": 0
                },
                {
                    "sent": "The work before would name is pink Sparkle.",
                    "label": 0
                },
                {
                    "sent": "This is because essentially what we've done is we've made a translation from the Sparkle Query language to the pig Latin query language.",
                    "label": 0
                },
                {
                    "sent": "With this data analysis language, high level data analysis language on top of Hadoop developed by Yahoo Research.",
                    "label": 1
                },
                {
                    "sent": "I don't want to talk a lot about this pig Latin word.",
                    "label": 0
                },
                {
                    "sent": "Essentially, what does stasis?",
                    "label": 0
                },
                {
                    "sent": "We're taking a sparkle query, we translate it into this pig Latin program, which is executed by the Apache Freak framework on and Hadoop.",
                    "label": 0
                },
                {
                    "sent": "Cluster we have to load the data set into the distributed file system of Hadoop as basically was the idea of the beginning.",
                    "label": 0
                },
                {
                    "sent": "The better the advantages of these things is that we have all the operators from the Sparkle Query language implemented that we have some benefits because often these Apache pick framers and really ongoing implementations or we benefit from optimizations directly from Apache from the open source world.",
                    "label": 0
                },
                {
                    "sent": "It runs out of the box, so we do not have to Jane make any changes to do.",
                    "label": 0
                },
                {
                    "sent": "How do framework at all?",
                    "label": 0
                },
                {
                    "sent": "You can directly and you can also run it on Amazon Cloud and its principles portable to other platforms because the PIC platform is in principle portable to other.",
                    "label": 0
                },
                {
                    "sent": "Platforms as well, not only Hadoop MapReduce an, of course we under map reducing the batch world.",
                    "label": 0
                },
                {
                    "sent": "This is where reasonably good scalable we were satisfied with this underperformance for some complex analytical queries says let's say more careers where the output is relatively high, but it's not really satisfying in these kind of framework is that if we have more selective queries like most of the lumen queries, whether the output is rather small compared to the input.",
                    "label": 1
                },
                {
                    "sent": "This is something where this kind of system is not really made for.",
                    "label": 0
                },
                {
                    "sent": "The reason for that is that the normal traditional join execution member useful is often done by so called Reducer join.",
                    "label": 0
                },
                {
                    "sent": "Most of you may know the basic principle where we have to shuffle the datasets from.",
                    "label": 0
                },
                {
                    "sent": "We checked in the map phase we take the datasets we have to shovel the datasets, network and do the joint processing and reduce phase, which means for selective queries we have to shuffle a lot of unnecessary data which is discarded in the reduce phase, which means that there's a lot of IO going on.",
                    "label": 0
                },
                {
                    "sent": "And we have no built-in index structures in the Hadoop distributed file system out of the box.",
                    "label": 1
                },
                {
                    "sent": "So what we basically tried in our new approach was to improve performance on a.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "MapReduce based system.",
                    "label": 0
                },
                {
                    "sent": "In case of more selective queries that where the output is rather small compared to the input size.",
                    "label": 0
                },
                {
                    "sent": "Now idea was to use H base as an additional storage layer on top of HTFS, mostly because we wanted to have these kind of random access to the data which is not possible in HDFC.",
                    "label": 0
                },
                {
                    "sent": "In Planet Rivers I will come to that point later and we wanted to get rid of the shuffle and reduce phase.",
                    "label": 0
                },
                {
                    "sent": "In MapReduce we just wanted to.",
                    "label": 0
                },
                {
                    "sent": "Have to shuffle the data that is really needed for the join in order to reduce the oil cost.",
                    "label": 1
                },
                {
                    "sent": "So we just wanted to do it in a map face.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is the change in this system parameters we use not only HDS and storage layer, but also each base on top of it and for the all pro tight implementation we do not use the pig framework anymore but made a direct implementation on map reduce so they expected drawbacks.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 1
                },
                {
                    "sent": "Drawbacks from these concept is we increase the communication overhead as we have to talk with these.",
                    "label": 1
                },
                {
                    "sent": "No SQL Data store for every request access.",
                    "label": 0
                },
                {
                    "sent": "We will have strictly higher RAM consumption, which is especially important for our cluster, which is rather limited in kind of RAM configuration and this is not perhaps not an idea of the solution for high output queries, which is we are not addressing it these things we wanted to improve for more modern, more selective ones were not satisfied about this.",
                    "label": 0
                },
                {
                    "sent": "OK, some things about how we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or data in the H base no escape.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data Store first of all, I think most of you may know what is H order hasn't been super idea.",
                    "label": 0
                },
                {
                    "sent": "What age bases age base is a clone of Google's big Taylor.",
                    "label": 1
                },
                {
                    "sent": "It's a column oriented, semi structured, no SQL Data store.",
                    "label": 0
                },
                {
                    "sent": "Mostly it's used in Hadoop cluster.",
                    "label": 0
                },
                {
                    "sent": "And it's layer on top of HTFS means the data in H base, essentially stored in HDF.",
                    "label": 0
                },
                {
                    "sent": "But using a custom appropriate file format such that the normal drawbacks of HTFS that you only have access to the data in a batch side.",
                    "label": 0
                },
                {
                    "sent": "If you want to access random data, you have to scan over the complete over the data set and look for the entries you want to.",
                    "label": 0
                },
                {
                    "sent": "This is something we can see from our perspective.",
                    "label": 0
                },
                {
                    "sent": "We can see a space as in.",
                    "label": 0
                },
                {
                    "sent": "Storage layer on top of HTFS with ads, these random data access facilities to HTFS in what the people from HSS claiming nearly close to real time, and it's a strictly consistent, no escalator, so mean said every data access is guaranteed to have the latest word.",
                    "label": 1
                },
                {
                    "sent": "What it is not is also important.",
                    "label": 1
                },
                {
                    "sent": "This is not a relational query engine long, so it doesn't have a normalized schema.",
                    "label": 0
                },
                {
                    "sent": "It does not have any further processing processing operations, so if you want to make a join, you have to decide whether this is left for the application of topliffe it you cannot do and join in H pairs of its own, and it does not have an expressive query language, SQL or something like that.",
                    "label": 0
                },
                {
                    "sent": "So the data model is.",
                    "label": 0
                },
                {
                    "sent": "Sparse distributed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorted multidimensional map.",
                    "label": 0
                },
                {
                    "sent": "It's perhaps easier understand if we look at the access pattern in the most basic things we have tables in H base and the rows of these tables are sorted and indexed by the roci, which is the key and the columns of these tables are grouped into families and we have can have different versions of every value identified by timestamp.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "And of course, these tables are then split into regions, and every regions are stored on one node in the machine in order to distribute the access.",
                    "label": 0
                },
                {
                    "sent": "This is not the important thing for what I would want to talk about.",
                    "label": 0
                },
                {
                    "sent": "Look at a simple.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, which illustrates how we store the data.",
                    "label": 0
                },
                {
                    "sent": "Look at this simple RDF graph about some, let's say articulation outdoors and authors of articles and artikkel side other articles.",
                    "label": 0
                },
                {
                    "sent": "What we do is we use A2 table schema in this case where we have under one table.",
                    "label": 0
                },
                {
                    "sent": "This is T SPO table.",
                    "label": 0
                },
                {
                    "sent": "This notation indicates that we use the subject here in this case of the triple AO Ki means that we have an index on subject position for example.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "The article one in the subject for the subject of these RDF graph in the roci and what we do now is we use the predicates as the column names in HH Basin used installed actually value the objects of the triples as in the value precision and if we have a multi value predicates like the author one we stored in the same role in the same column different using different timestamps to store the values.",
                    "label": 0
                },
                {
                    "sent": "And the important thing to notice is that.",
                    "label": 0
                },
                {
                    "sent": "Rose in an age based table doesn't have to have all the same columns.",
                    "label": 0
                },
                {
                    "sent": "For example, this one has an additional column with the predicate side which is not existing in the mean.",
                    "label": 0
                },
                {
                    "sent": "We do not have any null values in this case, so.",
                    "label": 0
                },
                {
                    "sent": "We only have these two things.",
                    "label": 0
                },
                {
                    "sent": "We have SPO and Opsi will talk about what we came to.",
                    "label": 0
                },
                {
                    "sent": "This kind of decisions about.",
                    "label": 0
                },
                {
                    "sent": "The other thing is the opsa table which is send it the same design as the SPO table but for objects.",
                    "label": 0
                },
                {
                    "sent": "The key.",
                    "label": 0
                },
                {
                    "sent": "The other question was, let's say we could use 6 tables in order to support all the different combinations of patterns, But this would ensure essentially mean that we would store the data 18 times in HDFC because of the replication and all these things and we are using a special functionality of.",
                    "label": 0
                },
                {
                    "sent": "H base with the field API to overcome these drawbacks of these missing proper index tables, make an example like here for example, look at this kind of pattern where we have given subject given predicate and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking for the object so it would be nice to have a table with an index on subject and predicate combination.",
                    "label": 0
                },
                {
                    "sent": "In our design we have only did at the table with an index on the subject position, but what we can do in edge cases we can add filter to requests.",
                    "label": 0
                },
                {
                    "sent": "In this case we would add some column filter becausw.",
                    "label": 0
                },
                {
                    "sent": "The predicate is our column names, so the column is known in advance.",
                    "label": 0
                },
                {
                    "sent": "What we do is we restrict.",
                    "label": 0
                },
                {
                    "sent": "In this case the request for the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Predicate we want to access and as the number of predicates and also here in the most of the scenarios is rather smaller.",
                    "label": 0
                },
                {
                    "sent": "We compare this one to the six if using Zix tables.",
                    "label": 0
                },
                {
                    "sent": "The performance we got in our system was essentially not much worse than than storing six different tables with everything having index on all of it.",
                    "label": 0
                },
                {
                    "sent": "Because this filters is the interesting thing, is all these filters are directly applied?",
                    "label": 0
                },
                {
                    "sent": "From running and see one of the most common way Parton says, when you have subject and object unbound right, and in this case you will discount.",
                    "label": 0
                },
                {
                    "sent": "If we have, The thing is if we have a table or index on predicate, this would not have a lot of us before us becausw they are most of the datasets, the product number of predicates is rather small ones, so we would have a table with only a small with a small number of rows where each row stores nearly a big portion of the data set would also explain for scalability because every row is stored on one machine in the cluster.",
                    "label": 0
                },
                {
                    "sent": "So if we if the data set increase, these roles becomes bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "And then some in some point in time the limit the resources of a single machine in the cluster becomes a problem for scalability.",
                    "label": 0
                },
                {
                    "sent": "Which we didn't want to have.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing from the services that they applied on the filter system they applied on the server side means that not the data is transferred, and afterwards the filters applied on these columns.",
                    "label": 0
                },
                {
                    "sent": "But either every single region server H base applies directly.",
                    "label": 0
                },
                {
                    "sent": "These filters on server side means that only the value is matching the given predicate really transferred to the client requesting the data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. That's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at what we're doing or what we how do we use these HP storage in order to process this?",
                    "label": 0
                },
                {
                    "sent": "These relatively simple sparkle query.",
                    "label": 0
                },
                {
                    "sent": "The idea is relatively simple.",
                    "label": 0
                },
                {
                    "sent": "It's from the relational databases units known as normal index, nested loop join.",
                    "label": 0
                },
                {
                    "sent": "Like for example, we begin for the first triple pattern.",
                    "label": 0
                },
                {
                    "sent": "We do some some scan on an H base table which is executed in parallel.",
                    "label": 0
                },
                {
                    "sent": "In Hadoop clusters means we retrieve all the mappings for these first triple pattern is always getting locally from old age based region servers and the results from these mapping are fed into the map phase as an input.",
                    "label": 0
                },
                {
                    "sent": "Let's for example we've seen this.",
                    "label": 0
                },
                {
                    "sent": "This is a specific instance of the map running in.",
                    "label": 0
                },
                {
                    "sent": "On one node.",
                    "label": 0
                },
                {
                    "sent": "You have to think of it.",
                    "label": 0
                },
                {
                    "sent": "This is what this is running on every node in the cluster in parallel, and what we assume here is we get an input mapping with, let's say artikkel one and and title for this article.",
                    "label": 0
                },
                {
                    "sent": "What we now have to do in these map phases we have to find the corresponding mappings for the second triple pattern.",
                    "label": 0
                },
                {
                    "sent": "Also for the article one and what we didn't do is optionally we make a substitution that ripple pattern.",
                    "label": 0
                },
                {
                    "sent": "We use the mapping we already have.",
                    "label": 0
                },
                {
                    "sent": "Substitute develops we already know and then access these people pattern.",
                    "label": 0
                },
                {
                    "sent": "Using these mapping I've illustrated here and this case we would access the TSB oh table using the artikkel azero key defining a filter for the author's column and retrieve the object.",
                    "label": 0
                },
                {
                    "sent": "We want to have.",
                    "label": 0
                },
                {
                    "sent": "It is the same for optical tool and then we can output the output of the metaphase is the new mapping that is the solution to the joint between the combination of the first 2 triple patterns.",
                    "label": 0
                },
                {
                    "sent": "And obviously, did the what we have mappings in the input of the map phase and we have mappings in the output of my face.",
                    "label": 0
                },
                {
                    "sent": "So these two things are directly compatibles or this makes us easy to just iterate over these kind of joints.",
                    "label": 0
                },
                {
                    "sent": "So essentially from one triple to the next we can iterate these met phases.",
                    "label": 0
                },
                {
                    "sent": "We do not need any shuffle or reduce phase in order to process the data for the next step which is in most of the other map side.",
                    "label": 0
                },
                {
                    "sent": "Drawing approaches in MapReduce you mostly have problems if you want to compute these joins over and over again.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's rather simple one.",
                    "label": 0
                },
                {
                    "sent": "What we now look at.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this example I showed before.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Article One, Article 2 If we look at the overall execution we have.",
                    "label": 0
                },
                {
                    "sent": "As I said, we would need two iterations and.",
                    "label": 0
                },
                {
                    "sent": "These are these common pet.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We start like query patterns where we have essentially here the join key is always on.",
                    "label": 0
                },
                {
                    "sent": "Our ticket was always the article and if we look at what's happening in the 1st and the 2nd iteration then what we see is that we are in both iterations.",
                    "label": 0
                },
                {
                    "sent": "We essentially doing the same.",
                    "label": 0
                },
                {
                    "sent": "We are using the article here.",
                    "label": 0
                },
                {
                    "sent": "The input mapping article one for the rookie and the same for this one.",
                    "label": 0
                },
                {
                    "sent": "But they are the only thing which differences is the is the filter we are using to restricting the special other bindings.",
                    "label": 0
                },
                {
                    "sent": "We want to get so.",
                    "label": 0
                },
                {
                    "sent": "What's essentially is that the result for this Sir triple pattern is not really.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really depend on the result from the second one, 'cause the author binding is not really the important thing for next steps or what we also could do a diss iteration.",
                    "label": 0
                },
                {
                    "sent": "We can get rid of these two iterations we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also apply this in a single iteration.",
                    "label": 0
                },
                {
                    "sent": "Let's say we start with the Article one asking for the authors of the article, and if there are results to this request, then we afterwards ask for the year of this article and can directly do is in one phase.",
                    "label": 0
                },
                {
                    "sent": "And if there are no results for the first request, we can directly skip the second request and drop the mapping from the result list.",
                    "label": 0
                },
                {
                    "sent": "The same for the second pattern AN.",
                    "label": 0
                },
                {
                    "sent": "Now again, we're nearly finished what we have done is reduced the number of iterations from 2 iterations to one.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if we consider the data design, what we do here is we do two requests on the same table using the same row key.",
                    "label": 0
                },
                {
                    "sent": "The only difference is again is the column, so this is also.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It really necessary if you look at the H Base Request API Becausw in HPS we cannot only apply filter 2 and request, instead we can also apply a list of filters in an arbitrary combination of filters.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can use this request and somebody.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To request you can directly use one request where we not only use a filter for the hour thought columns on again use a filter for the column with the corresponds to year so that we knew is only one request to answer the order to get the mapping which matches the 2nd and so triple pattern.",
                    "label": 0
                },
                {
                    "sent": "So and we end up with.",
                    "label": 0
                },
                {
                    "sent": "Have you only one iteration instead of two variations?",
                    "label": 0
                },
                {
                    "sent": "And we reduce the number of requests from 4 to only two requests, and if you think of this is a small small except of the overall computations of this runs in parallel.",
                    "label": 0
                },
                {
                    "sent": "Every machine for many mapping, so this is an important thing, especially to reduce the number of requests because the number of requests against each node square H data source.",
                    "label": 0
                },
                {
                    "sent": "The main key driver in the query execution time we use.",
                    "label": 0
                },
                {
                    "sent": "This using our system H base is something we know there's a space was able to handle something about 10,000 requests a second, so directly the query execution time relates to the number of requests you have to set against age based in order to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer the query OK Again, for some want to talk about some results for the loop and benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, our cluster setup wasn't pretty small cluster with 10 nodes.",
                    "label": 0
                },
                {
                    "sent": "For us more power Power Edge servers are 200 great, 8 gig of RAM which is rather small configuration for systems running these nodes called data stores and frameworks we use was the Cloudera distribution for Hadoop.",
                    "label": 0
                },
                {
                    "sent": "Most of you may know we use the H base in the 1990 version and we generated some datasets for the loop M University.",
                    "label": 0
                },
                {
                    "sent": "Number from 10,000 two 1000 universities.",
                    "label": 0
                },
                {
                    "sent": "What we did is we applied the reasoning before executing queries in order to have not rewrite the queries and so the biggest data set brass, something around 630 million triples.",
                    "label": 0
                },
                {
                    "sent": "One of these nodes in the cluster is always used as a as a master node is not really in compute node, it's it's running all the master deems used for the database and for the name node and all these other things.",
                    "label": 0
                },
                {
                    "sent": "OK, and what's up?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really surprising is if you look at the Curie one of Le Pen which is in this case are really the base case is also only has a single joint involved and we look at the comparison of the weather.",
                    "label": 0
                },
                {
                    "sent": "What we do here is a comparison of the system we use before these pics bugs are mentioned where we use the reducer.",
                    "label": 0
                },
                {
                    "sent": "I draw an approach in a map reduce scenario.",
                    "label": 0
                },
                {
                    "sent": "Now compare it to the this map side join approach where we want to express we improve the selective queries and this one is a really selective query.",
                    "label": 0
                },
                {
                    "sent": "And as you see with this is 2 times the same graphics but with different scales.",
                    "label": 0
                },
                {
                    "sent": "We've seen linear scale on the right side and we have a log scale on the left side for the linear scale.",
                    "label": 0
                },
                {
                    "sent": "It's hard to read the numbers here, but what you can see is that both approaches called MapReduce framework, which is yes question.",
                    "label": 0
                },
                {
                    "sent": "This way is the web PY inference functions for group Two.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you see here is the linear scaling behavior of both approaches, which is not really surprising because of the map reduce framework, but the most more interesting thing is that for the new approach is something around one order of magnitude faster compared to the we have to ship all the data which is nearly unnecessary and discarded afterwards in the reduce phase.",
                    "label": 1
                },
                {
                    "sent": "This was resulting in sometimes from 8 to 30 times faster.",
                    "label": 0
                },
                {
                    "sent": "Very good execution.",
                    "label": 0
                },
                {
                    "sent": "The more interesting thing is something like for example query 4, which is more as more general case for approach, which is a sequence of joints and also interesting here is that we can have these multi way join optimization I mentioned or because this is a star pattern query and also what's illustrated here is the comparison between.",
                    "label": 0
                },
                {
                    "sent": "Again the reuse side version execution in the cluster and the map side version execution.",
                    "label": 0
                },
                {
                    "sent": "And what we see again is a difference in one order of executing a query execution time and again what we have done is also in the reduce side version approach.",
                    "label": 0
                },
                {
                    "sent": "We use these multi joint optimization is also applicable in reduced side version that we only use one single map reduce phase to compute the overall result for the query and we do the same for the website version and overall again we get even certain times faster.",
                    "label": 0
                },
                {
                    "sent": "Every situation using these optimizations where we use only one iteration instead of in the normal execution without module we would use 1234.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Map reduce iterations to compute the process.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No result.",
                    "label": 0
                },
                {
                    "sent": "So for conclusion and future work would say the things to take away is what we have essentially done is that we implemented mechanism that produces the order that calculates the joins in completely.",
                    "label": 0
                },
                {
                    "sent": "In that phase.",
                    "label": 0
                },
                {
                    "sent": "We do not use any shuffle reduce phase in MapReduce.",
                    "label": 1
                },
                {
                    "sent": "What's especially it was important for us is that we are able to iterate these these joins without any auxiliary shuffle and reduce phases that you normally have to use.",
                    "label": 1
                },
                {
                    "sent": "For example, if you do some merge map side merge join which is interesting.",
                    "label": 1
                },
                {
                    "sent": "Version of the map side.",
                    "label": 0
                },
                {
                    "sent": "Join your wizard.",
                    "label": 0
                },
                {
                    "sent": "The problem that you have some preconditions you have to fulfill for the input data set as they have to be sorted in some way, partitioned in some way.",
                    "label": 1
                },
                {
                    "sent": "This is something we do not really use in this case and we again for the star pattern theories we can reduce the number of iterations which is the more important thing is created to reduce the number of H base requests and.",
                    "label": 1
                },
                {
                    "sent": "What we hope to achieve is that we can outperform.",
                    "label": 0
                },
                {
                    "sent": "To reduce that version for these more selective queries, which was clearly achieved.",
                    "label": 0
                },
                {
                    "sent": "One thing to mention is OK.",
                    "label": 0
                },
                {
                    "sent": "This new approach has also disadvantages because the performance of these technique degrades with the increasing variable output.",
                    "label": 0
                },
                {
                    "sent": "If you can think of the factory output, largely output gets the more request against H space.",
                    "label": 0
                },
                {
                    "sent": "We have to do the worst performance getting comparison and could be some some point in time where again the batch processing version of the user.",
                    "label": 0
                },
                {
                    "sent": "Join where your ship all the data and output during the reduce phase could be even more efficient than the website version because of if you have too many requests against the database in H base, you can get into problems and for future work it's really obvious.",
                    "label": 0
                },
                {
                    "sent": "One thing is we're looking at improvements for leftover scheme because we have some identified some problems concerning the scalability mechanism, like for example if we would have a table for predicates that we store essentially big portions of the data in a single.",
                    "label": 0
                },
                {
                    "sent": "Rowan H basin.",
                    "label": 0
                },
                {
                    "sent": "As Rose always thought on a single machine, then again we have the problem that only one machine can get a bottleneck in, which is essentially that the idea of MapReduce you want to prevent if you want to.",
                    "label": 0
                },
                {
                    "sent": "If you want to process bigger datasets, we just use a bigger cluster.",
                    "label": 0
                },
                {
                    "sent": "But this would help.",
                    "label": 0
                },
                {
                    "sent": "In this case you have to increase the resources of a single machine, which is not really the idea behind MapReduce.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is, of course, if we handle if you have some kind of system which is good for for queries with high output, and we have other kind of system which is good for queries with a low output, then of course we want to have is incorporation of these two things depending on query only query selectivity or in the joint activity that we could use them or the effective version for the map side version for the joins if there's no incentive selective or use the more batch oriented version.",
                    "label": 0
                },
                {
                    "sent": "If the output is.",
                    "label": 0
                },
                {
                    "sent": "Bigger.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what I want to say.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "Are there any further questions?",
                    "label": 0
                }
            ]
        }
    }
}