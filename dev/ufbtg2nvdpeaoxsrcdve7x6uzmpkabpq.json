{
    "id": "ufbtg2nvdpeaoxsrcdve7x6uzmpkabpq",
    "title": "Performance Analysis and Optimization (Part 1)",
    "info": {
        "author": [
            "Christian Feld, J\u00fclich Supercomputing Centre (JSC)",
            "Philip Blood, Pittsburgh Supercomputing Center"
        ],
        "published": "Sept. 19, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science",
            "Top->Computers->Programming"
        ]
    },
    "url": "http://videolectures.net/ihpcss2016_blood_feld_performance_analysis_part1/",
    "segmentation": [
        [
            "So then we are continuing this afternoon with the first part of the performance analysis and optimization.",
            "Say that will be continued tomorrow, and it's a pleasure for me to introduce again also as last year and the year before.",
            "Phil Plait in Christian Felt, who jointly will cover this session, so please go ahead, Thanks Sermon.",
            "So.",
            "My name is Philip, Blood from the Pittsburgh Supercomputing Center.",
            "I'm going to be starting out here with this performance session and I'll be introducing the topic and then Christian will be taking over with the hands on in a little bit and we'll see how far we get.",
            "So I think this is number 7 that of the summer schools that I've been able to present at since the first one, which which came about because I think the person they'd invited couldn't make it, and so I was the second choice, and I'm glad that they called on me.",
            "It's been a lot of fun to teach these at these summer schools, so we're looking forward to talking about how to engineer your app, your codes to be more performant, and in particular how to go about.",
            "Analyzing the performance using the variety of performance tools that are out there so."
        ],
        [
            "I just upfront I just wanted to thank Christian who's had a lot of input in what you'll see today and also my interactions with the groups that are now part of this virtual Institute.",
            "For high productivity supercomputing, including you lick and University of Oregon and others, have really generated the ideas and and the things that will discuss today and also a colleague of mine, former colleague from PSC, Robbie ready."
        ],
        [
            "Who contributed to this talk way back seven years ago when we started this?",
            "So this is the rough outline for the session.",
            "This is probably an optimistic outline, so today I'll be giving this introduction.",
            "As I mentioned, and then Christian will start to do some hands-on with performance profiling of a test application using a tool called Scorpion, which we'll talk about further today.",
            "We probably won't get quite all the way through that.",
            "Today we will see and.",
            "And then tomorrow I'll show you how to use a tool called Para Profits in Tau for analyzing these profiles that you'll generate on bridges for this application.",
            "And then which you could also use with your own codes.",
            "And I welcome you to try that out at your earliest opportunity as you as you pick up how to do this through the hands-on sessions and maybe before you leave.",
            "We can even help you with your own code, which would be, which would be the most relevant and helpful thing that we could do here so.",
            "Please keep that in mind and will go through some different aspects of performance analysis and in particular trace measurements, which is a time resolved measurement of what's going on in your code will talk more about that later, but I just want to emphasize here as we get started that we want this.",
            "We want to go to pace that will be beneficial to you.",
            "And so if we start to leave too many people far behind and then just let us know, feel free razor and ask questions.",
            "Will try to get.",
            "I'll try to get through my material fairly quickly so that we can get into the hands on part of it.",
            "But if I go through too quickly, feel free to stop me, raise your hand and ask a question.",
            "And same thing with the hands on as we go through it.",
            "Feel free to sort of stop us and say you're lost and we'll try to get together and at least help.",
            "Most people who wanted who want to be involved get caught up so."
        ],
        [
            "OK, so why do we need to worry about optimizing our scientific codes 444?",
            "Performance on the current HPC systems.",
            "Well, one answer to that.",
            "An important aspect of it is that the processors that we're using in these HPC systems are essentially the same processors you have in your laptops, and so they're not designed for your HPC algorithm.",
            "Your scientific algorithms.",
            "They're designed for streaming multimedia and and doing these general things.",
            "This is why we can build such big systems because it's become cost effective because so the whole world is buying these processors, so they're not just making it.",
            "For you and for your science, which is great, but the problem is, is that your algorithm does not fit the architecture and so we need to try to adapt our algorithm as much as possible to to the architecture that we're given, which is which are these commodity processors.",
            "In some rare cases, just to highlight the counterexample or a counterexample?",
            "You can invest a lot of time and money in developing a processor that is specifically just for your scientific application.",
            "This is a very successful effort to develop a molecular dynamics special purpose application specific integrated circuit.",
            "It's called Anton from the Shell research, we actually host one of these at the Pittsburgh Supercomputing Center and so the algorithm formula dynamics the important aspects of the algorithm are actually hard wired into the hardware and so it's very very very fast.",
            "Orders of magnitude faster in general than what you can do with with commodity hardware from like dynamics, so that's not an option for.",
            "Unfortunately, most of us, so we're going to talk about how to fit our algorithms.",
            "Are applications, how to understand how we can do that better with the hardware that we have."
        ],
        [
            "And so, having said that, talk about algorithms.",
            "I just want to drive home at the beginning that the choice of algorithm is going to be the single most important thing.",
            "Decision that you can make in terms of the performance of your scientific application.",
            "This is where you're going to get orders of magnitudes.",
            "Gains in performance is if you can find a better algorithm that does what you need more efficiently with fewer floating point operations or whatever, the best optimization can make is to not do that floating point operation at all.",
            "So that is the most important thing, and that's where you get.",
            "I huge huge gains, so this is most important consideration if you're planning to implement a new code, or if you're trying to refactor your existing code to be much better, choose choose the right algorithm and that will is very going to be dependent on what your specific application is.",
            "And then just along with that, once you have well the other side of that is that you do have to think about these things from the beginning and plan them, and you can certainly go in and optimize your existing code.",
            "But the biggest speedups will be either sort of starting from scratch or as Eric taught us today.",
            "You don't have to start from scratch, but you do have to go in and re engineer your code pretty seriously if you want massive speedups in performance.",
            "So with open MP just adding a few directives here and there, you might get up to 16 cores, 16 processors on a serial code, but you're going to have to do more work to really get things to scale up to hundreds and thousands of processors, so just setting expectations on that.",
            "And as you do this, you may actually may help reveal the need for new algorithms or much better implementation.",
            "So the focus of this discussion and what the hands-on session will be later, will really be how you can use the tools that are available to assess the parallel parallel performance of your codes and identify the areas where they're not performing as you would expect.",
            "And so OK.",
            "So we look at this sort of line here, choosing the algorithm implementing that on the hardware you have, and then analyzing the performance and then finding out what's wrong, doing some optimization and then maybe repeating this process over and over."
        ],
        [
            "So that's what."
        ],
        [
            "Is shown here that as you go through this analysis process, then you have to prepare your code somehow.",
            "Usually, except in certain cases I'll mention you're going to sometimes insert something into your code to do the measurement, and then you actually measure the data you collect data about the performance, aggregate it, and then you have some sort of analysis tool that will allow you to calculate metrics, identify performance problems, present the results so that you can see them, and understand them, and then identify hopefully your area that you can optimize and go ahead and.",
            "And apply those optimizations and then repeat that process until you're happy with the performance of your code."
        ],
        [
            "OK, so to set the stage for some of this discussion I want to define some terms and give a little background when we start talking about performance analysis and especially the performance tools that we'll be talking about."
        ],
        [
            "Certain things come up repeatedly.",
            "One is hardware counters.",
            "Who knows what a hardware counter is.",
            "OK, so maybe 20 or 30% people so the counters are a set of registers that are sit there in the processor on the processor and they count events that are happening on that processor so they can count.",
            "For example floating number of floating point operations that have happened, or a certain number the number of cycles that have passed and so.",
            "So these are special hardware registers that count these events and different processors have different registers that they support.",
            "So for example, an Opteron Istanbul processor processor has six different registers on it, so that means you can count 6 different types of events simultaneously on that that hardware.",
            "So you can't.",
            "It's not unlimited.",
            "What you can count, and different processors allow you to count different things, although many of them allow sort of standard certain standard things, because every processors different group at University of Tennessee, I believe, amid possibly other collaborators develop something called the performance API or PAPI.",
            "And this was an effort to create a standard API for accessing these hardware counters across all the different architectures that are available, and so will.",
            "So they have this API and the nice thing about it is it's portable.",
            "You can use it across all these different architectures without having to get into the architecture specific stuff every time you want to count the performance performance counters.",
            "And so."
        ],
        [
            "They have these predefined standard events that they have created.",
            "The standard way of accessing and when you install PAPI on your system then they have this command papi avail, which will list all of the hardware counters that are available on that on that particular architecture.",
            "That processor that you're running on that that you're using on that system so, so this is sort of high level standard interface.",
            "They also have a low level interface that exposes the native counters on that system.",
            "The one the actually.",
            "The architecture specific counters that are that are defined for that particular processor.",
            "If you want to get more.",
            "If you want to get more detail about or you want to use these native things because they are more specific to what you're doing, this is probably not a level you have to worry about too much, with the caveat that I'll mention in a little bit and then so third party tools for doing performance analysis utilized these extensively, so we'll be talking bout towen.",
            "During this presentation and Chris will talk about Scholastica and score P. And, but there's many, many tools that utilize these things, and as of this version of the kernel there's actually direct support for these hardware counters in in Linux."
        ],
        [
            "OK, so another part of the setup is measurement techniques and what we mean by certain terms.",
            "So so one question is in terms of how you analyze performance of these applications is when do you measure performance?",
            "How is that measurement triggered?",
            "So one way to do it is through sampling and so sampling and sampling.",
            "You don't have to modify your code at all.",
            "You don't have to put any new stuff into your code, so that's really nice and it periodically it will interrupt the execution of the code and see where it is.",
            "In its execution, and so you can start to see where the code is spending time by these periodic interrupts.",
            "So it's a sampling statistical based measurement of what's going on in your code.",
            "The other is through direct instrumentation, so actually putting brackets around your code in certain regions and saying OK from here to here measure how many times this happens, and so that has both of these have certain benefits and drawbacks that will mention.",
            "And then how do we actually?",
            "So that's when the measurements triggered.",
            "How do we actually record the data?",
            "One is just through a summary or a profile where you collect all the data and then you average it out and you get one average number over the timeframe that you were analyzed that you are measuring the performance.",
            "And the other is tracing, which actually gets a time resolved view of the performance.",
            "You can.",
            "You can see how the performance changes over the course of time and different look at events that are happening over a certain course of time within your code and will Christian will talk about more about that later as well so."
        ],
        [
            "And then finally, I think for our for our little vocabulary lesson we have, we talk about doing xclusive measurements and inclusive measurements.",
            "Right now this is kind of important and it's easy to forget which is which.",
            "But for if we want to exclusive measurement, then we're looking at only the the the, the events that happen within a given function, for example so or within a given region within a function so.",
            "In this example here we are looking at this function Foo and it calls another function bar.",
            "But if we're looking at the exclusive duration of this function and the things that are happening within this exclusive measurement, then we're only looking at the explicit events that are happening within this first level of this function, and we're not looking at any of the events that are happening in this child function that it calls OK, and usually that's what we're most interested in, but there's.",
            "But then you can also look at the inclusive duration of Foo, which includes everything that happens within Foo itself, along with all the sub functions that are called within anything that goes below this.",
            "Everything else in the call path there.",
            "OK, so I think that is yes, that's the end of vocabulary lesson.",
            "How are we doing?",
            "Doing alright.",
            "OK, so the so that these are here for your."
        ],
        [
            "Reference and we can go back and talk about those and answer questions more if if needed so.",
            "So the way I want to go through this then is kind of is telling a story.",
            "One of the things that I do as working at HPC Center is people come to us and say our code is bad.",
            "It doesn't perform well.",
            "Please help us to fix it, and so this story I'm going to tell us from one experience that I had helping a group to understand the performance of their code and to fix the problems.",
            "And as you might expect, the story ends happily, so we will see at the end.",
            "So this particular code was one that was developed, and I think it's still being developed at Cornell.",
            "In Harold Chirag's group there, this is a code for doing coarse grained simulation used to protect protein folding pathways and so it's called unres is the name of the codes will be referring to that often this code."
        ],
        [
            "Just had a master worker arrangement and so you have a master process, a master rank that would have a certain call path and then all of the worker ranks would then have a different call path so it's a different path to path.",
            "Is the master of the worker.",
            "It also in terms of measurement, measuring performance and doing performance analysis?",
            "One thing that is significant is you want to get to the core of your application.",
            "The thing that's going to be repeated over and over again.",
            "You're not so worried about the startup time, but you also don't always want to run it for hours and hours to get results, and so there's significant setup time in this code is like 5 minutes just to get past the startup, and so that was one thing you have to work around when analyzing these codes so we're only interested in the really the kernel part of it."
        ],
        [
            "So just just a couple of things that were things to think about as we got started looking at this code.",
            "This is the procedure that will be going through and as we talk about this procedure, I'll be using.",
            "As I mentioned this this unres example to highlight some of the ideas here and also I'll be showing how we tackle this with Tau now.",
            "I don't want to get too hung up on the details of the Tau stuff in this presentation.",
            "Just focus on the sort of the main ideas, the principles, but there will be slides in here that will show you some recipes if you're interested in trying to use Tau for this kind of thing, then there will be some reference slides in there you can go back to and try it, but then in the hands on you'll be will be using score P to illustrate this as well, so you'll get exposure to different ways of doing this through different tools.",
            "OK, so.",
            "So this is the process we go through and we can look at optimizing the serial.",
            "The single core performance of of our application and also looking at optimizing the parallel performance of the application.",
            "So in both both cases the first step is to assess the performance where it's at and then also then to identify where the code is spending most of its time, and then if you really want to start digging in then you can do some instrumentation of those functions.",
            "As we mentioned, introducing these function calls that will.",
            "Where you can start to isolate areas and see how much what's happening within those areas of the of the code, and then we'll actually measure the code performance.",
            "If we're doing this, this processor or single processor performance, then we'll look at hardware counters to help us understand the code performance and identify inefficient regions of the source code and what was causing those inefficiencies in the parallel side.",
            "We start looking at load balance issues looking at regions we didn't think we had to paralyze.",
            "But when we start to scale it up.",
            "They turn out to be bottlenecks and other communication bottlenecks that happen.",
            "If we're using MPI, and tracing can really help with that."
        ],
        [
            "Identifying those OK, so for now we're going to start number one.",
            "Just assessing what is.",
            "What is the performance."
        ],
        [
            "Well, first of all, what is it mean for our code to perform poorly?",
            "How do we know if it's performing well or not?",
            "Pause for effect.",
            "Anyone want?",
            "Offer up a guess or an answer.",
            "How do you know if you're getting performance good, good performance or bad performance?",
            "Sorry what.",
            "Compare with other codes.",
            "Maybe that they would say so.",
            "Maybe you know other codes are doing well and you want to see how it compares, OK?",
            "Right, so parallel efficiency.",
            "Yeah, so you mean you can.",
            "Obviously there's an ideal for parallel efficiency.",
            "There's an ideal right scaling, literally.",
            "Any other thoughts?",
            "Performance modeling.",
            "OK, there's magic word so so you have to understand what's your theoretical best you can do with the algorithm that you're using, right?",
            "So what is the theoretical best?",
            "So it depends on the work that's being done.",
            "Traditionally, we do look at sort of what is the percentage of peak performance that you can get on a particular architecture.",
            "You might know that.",
            "Now most of us are using floating point codes in general in scientific applications, that is generally true, although more and more there's data intensive things that that's not necessarily true, so that's interesting territory to explore.",
            "But we won't talk about that as much today, but we look at peak performance.",
            "What percentage performance should I expect with my algorithm so Christian actually just tipped me off to the existence of this, and it's probably been along time and I.",
            "This is not something that that I've explored in great detail, but there are people are formalizing ways to establish performance bounds for various numerical methods, and so one way of doing this with these roofline models is to look at it in terms of aermet arithmetic intensity of your algorithm.",
            "So, and that is how many what's the ratio of the floating point operations to the total data movement in your code.",
            "So what you're either getting work done or you're moving stuff around to get work done, and so the ratio of that is the arithmetic intensity.",
            "And so they've got a nice on this on this webpage.",
            "Here you go, check it out.",
            "They've got a nice scale here that shows different types of algorithms and where they fall in this arithmetic intensity line.",
            "And so if you get down to these things like the dense linear algebra, these are because they're doing very dense floating point operations.",
            "You can get very close to the maximum theoretical floating point operation performance of a given processor for these linear algebra routines, so you might get up to 80 or 90% of the theoretical Max.",
            "With some of these are."
        ],
        [
            "Means but.",
            "For most, for most scientific applications, for if you take on average in that sort of average them all out.",
            "For floating point codes, getting 20% of peak is not bad.",
            "I mean it's it's OK.",
            "I mean, if you if you're fully point bound, then the best you could do would be 5 times better than that, right?",
            "And so you're kind of doing OK. And then you know.",
            "So there's been sort of a rule of thumb or a quick check on your floating point code that may not."
        ],
        [
            "Maybe terrable you know if you're doing dense linear algebra, you could do five times better."
        ],
        [
            "Or nearly but from an average, you know, sort of rule of thumb, check you can see and then you should."
        ],
        [
            "Notice something like this if you really want to understand what is your up."
        ],
        [
            "Limit on your algorithm.",
            "So floating point operation or Flop that's flew point operations per second peak.",
            "So you can also look at floating point operations per cycle and that and then multiply that by the speed of the processor.",
            "The cycles per second.",
            "So I'm going to giga Hertz and see how close are getting and then you just want to think.",
            "Well if I'm already at 50% floating point fraction of peak performance, do I really need to look at optimizing further my serial performance?",
            "The best I could do would be 2X and so you get you get the 8020 rule here of the.",
            "You'll be able to optimize most of the performance.",
            "Generally 20% of the effort you get to 80% of your potential performance.",
            "You know, just kind of rule of thumb, so you want to think about not investing too much effort for too little gain when you start to look at this from the beginning.",
            "Maybe you don't have to do anything and your code is doing pretty well.",
            "And then looking at parallel performance, we talk about scalability and we talk about strong scalability and weak scalability.",
            "Strong scalability is with a given problem size.",
            "If I scale up my code to two times, the number of cores, does the time go down by a factor of two as well?",
            "That's perfect.",
            "Strong scaling, weak scaling, which is also important for many applications, is if I increase the amount of work by two and the number of processors by two.",
            "Does the execution time remain the same and that would be perfect weak scalability?"
        ],
        [
            "OK, so so in this process of checking out OK, how is my code doing?",
            "I mentioned that sometimes you have to modify your code to use these performance tools, but with sampling as you mentioned we talked about earlier with sampling you actually don't have to.",
            "You don't have to modify your code.",
            "As I mentioned it will just you can execute your code as normal and it will interrupt execution.",
            "It will sample where it's at and this can be done with very low overhead and it can be done very without very much investment on your part so.",
            "I recommend if you're just going to check to see what code is at 2 to do, use one of these sampling tools.",
            "A very good one is this HPC toolkit.",
            "It can give you function level information about how your code is performing, and TAC Texas Advanced Computing Center is developed.",
            "A sort of a modified HPC toolkit to give you some more automated performance analysis about about your code, and that's worth checking out as well.",
            "So there's some links there.",
            "We won't go into that in detail today, but just sort of recommendation for a procedure of with your own code.",
            "You might want to try a sampling tool."
        ],
        [
            "For a quick dirty look at it, and this is unres.",
            "This is what we did there with Unres.",
            "We were looking.",
            "We used one of these sampling tools just to look at how many floating point operations were getting.",
            "What percentage of peak this was on an Itanium processor, so that tells you this was a little while ago and the theoretical Max was for floating point operations per cycle.",
            "OK, so so we weren't doing very well.",
            "We're getting about 15% of peak performance.",
            "It it needed some optimization on Itanium.",
            "It actually did much better on the X86 which had a theoretical peak of two floating point operations per cycle and we were getting 3%, three, 3% of peak on that so it wasn't terrible but there."
        ],
        [
            "IP room for improvement there.",
            "Looking at parallel performance, this was a scaling.",
            "This was our old computer at PSC called Big Ben and as we go through this is the ideal scaling and we can see that scaling starts to drop off pretty quickly here so this is.",
            "Time steps per second so so we want to go up here and it's starting to level off.",
            "And so as we get to like 32 cores, it really starts to flatline.",
            "So we're not doing too well there."
        ],
        [
            "So we've identified for unrest code how it's doing is single core performance and a parallel performance.",
            "Now we."
        ],
        [
            "Want to see where it spends the most time?",
            "Usually a handful of functions account for 90% of the execution time.",
            "You can very quickly to 90% and those are the ones you want to focus on.",
            "Just make sure as you're doing analysis, you're looking at the things that are actually contributing to the main kernel of your code, the production part and not the startup functions or anything like that.",
            "This is especially important.",
            "It's not your code and you're looking at someone elses.",
            "If it's your code, you probably know and don't have to worry about that so.",
            "Anne.",
            "And then you know, just as you're if you're looking at parallel applications, you want to look at these at scale as you scale up.",
            "Because functions that are insignificant when you're running on a single processor may become a bottleneck as you scale."
        ],
        [
            "And this is just an example of one of these sampling tools that kind of output.",
            "You can get.",
            "This shows the function that shows how much percentage of time was spent in that function.",
            "And here's the running sum, the running total and so here you get to these first 3 here, and you've got to 90% of the total time of the code, and so you just look.",
            "And really, it's just this one right that you can start with focusing on 75% of the code is there."
        ],
        [
            "This is what we saw with unres 50% of the time was in this this elec function here and there.",
            "You see the setup time which you know we only ran a short run so it looks like it's taking up a lot of time in reality that will go down to zero as you run the code over several hours.",
            "But with these sampling tools you actually can do that because there's so little overhead you can just apply the sampling tool to your code and then just let it run in a production setting and then you can collect some nice statistics and see OK. How is it doing you have to worry about startup time or anything else 'cause you've been running for a few days."
        ],
        [
            "Alright, but once you look take a look at these codes and you say OK, there's a performance problem that I want to address.",
            "Either the serial, the parallel performance then."
        ],
        [
            "You want to take a look at instrumenting these functions, so you're going to dig deeper.",
            "Insert insert functions into your source code.",
            "That will tell it to measure certain parts of the code.",
            "The advantages it gives you very precise information about where things happen.",
            "If it's not in that bracket of that function that you're instrumenting your code with, and then it's not going to get measured, so you know you're measuring exactly where things are happening there.",
            "The disadvantage is that putting extra functions into your code, especially high performance, part of your code that you want to formerly well.",
            "That's going to affect the performance, and so there's some overhead there and you need to be wary of and so you want to really only instrument.",
            "Not every function in your code, but only those ones that maybe through doing some sampling experiment that you determined are really important to performance."
        ],
        [
            "And so you need to choose a tool.",
            "There's a nice list here that I think Christian will also show you later that the VI HPS.",
            "Maintains that gives you some guidance on how to choose a tool for this.",
            "I'm going to be using towels.",
            "An example in this presentation, but as I mentioned, let's will focus on the general principles here of this process and not the details of Tau, but there will be reference slides that show you some recipes for how to use it, and then Christian will take you through specific examples of this using the Scorpions klaske tools during the hands-on sessions.",
            "After I get done talking."
        ],
        [
            "OK, so Tao it can do a lot of amazing very good stuff.",
            "For performance analysis you can get loop level information routine level.",
            "You can look at communication and performance counters.",
            "As I mentioned it supports all languages and all platforms.",
            "Basically they work hard to make it available everywhere and this is developed at University of Oregon."
        ],
        [
            "And so this is how you would instrument if you were using tabs is how you instrument your code with it.",
            "Again, I'm not going to go into the details, but it has.",
            "It does automatic instrumentation you give it some some information, some options you run a special script that comes with Tao and it will instrument your code for you so you don't have to buy hand put in these functions to measure performance."
        ],
        [
            "And then this is just another again recipe here that you're going to instrument your code and then and then run it as normal after you.",
            "After Tao instruments it and then you'll get some output from Tao and you'll use a tool called Para Prof to analyze the results and we are going to see Para Prof. Later because paragraphs.",
            "Interesting because it doesn't just work with how it works with scorpis that Christian is going to show you and it works with a bunch of other image works with HPC toolkit and a bunch of other performance tools.",
            "Can you can analyze the output using this pair of Prof analysis tool."
        ],
        [
            "OK, so this as we instrument our code, this is one of the classic things that happens with this high overhead.",
            "You have to be especially aware of tiny functions that get called many, many, many times and so this is what before Tau instrumented this particular unrest code.",
            "There was a function that looked like this and then after it looks like this, so in this tiny little function you now got all these calls to the Tau profiler.",
            "OK, so that's going to make your execution time blow up."
        ],
        [
            "And that's what we saw.",
            "We took a step.",
            "We took a reference measurement without any instrumentation, and it took 51.4 seconds and then we measured it.",
            "After we did the instrumentation without and it took 315 seconds.",
            "So that's not good.",
            "So Tao and other tools will allow you to selectively instrument."
        ],
        [
            "Certain functions.",
            "And in fact, what Tao does it after you run a test run, it generates a list of all these bad functions that are small, and that really shouldn't be instrumented, and so then you can create a selective instrumentation file that towel use at 2X clue."
        ],
        [
            "Those from its measurements.",
            "And so that's what this is.",
            "So this is how it's done in Tau.",
            "But Scorpis can also selectively instrument your code, and others do the similar things.",
            "In addition, something that's very nice that you can do with these tools is so it will automatically help you find or, or you can either automatically or by your own observation you can find which routines to include or exclude.",
            "But you can also define regions of the code, so not just functions but whole regions.",
            "That you want to analyze.",
            "So for example, in molecular dynamics, all the really intensive stuff happens in this one main MD loop, right?",
            "So everything goes, you loop through the time steps and so in this case of unrest, that's where we're really interested in, and so I can define that as a region sort of my production and like the dynamics part of the code.",
            "And then I only look at the performance information coming from that region.",
            "So that can be very."
        ],
        [
            "Helpful.",
            "Anne.",
            "These tools can also give you and the output in the region that you define in the area that you you are studying.",
            "All the call path information you can define that sometimes that might increase overhead depending on the tool.",
            "Christian tells me that's not the case with Scorpis.",
            "That can be the case with Tao."
        ],
        [
            "And this just shows you how to get that information in towel.",
            "Once you open it up.",
            "This is the parrot by the way.",
            "I didn't mention this is with a pair of Prof tool that will be talking about later on.",
            "They'll demonstrate probably tomorrow."
        ],
        [
            "To analyze these profiles.",
            "So this is just what I talked about.",
            "You can isolate different regions of code and just look at the measurements of performance in that."
        ],
        [
            "Region of interest.",
            "So with Unres, if we didn't do that and we ran this short experiment, then the setup time totally dominated and this elect function that we're actually interested in.",
            "It looks like it's only a small."
        ],
        [
            "Part of the the total.",
            "But then if we define just the MD loop as our area of interest, then you can see we see what's really important here.",
            "So the elect function is what takes up most of the time.",
            "So now we've been able to identify that in Tau this is a pair of Prof. Screenshot.",
            "And so we found these are the functions that are important in the main kernel.",
            "Main important part of our code."
        ],
        [
            "So we found those important functions instrumented them, so now we can measure the performance of those functions using these hardware counters they talked about."
        ],
        [
            "And so you could install Papi.",
            "You can look at the different counters that are available using the Papi avail command that I mentioned.",
            "Then you can run tower or whatever code you're actually weather tool you're actually running and feed it a list.",
            "So this is how happy these are.",
            "Some of the names of these PAPI events so this, so these are the events that are defined.",
            "So this is floating point operations for total floating point operations.",
            "This is 1 PAPI event that set this is the total number of cycles so that from this you can get.",
            "And this is a measurement of the time that's elapsed.",
            "So from this you could get full input operations per second in different regions of your code, just a just a quick warning that.",
            "When you really get down to looking at the performance closely using these performance counters, then you do want to be a little bit careful that you understand what those counters mean.",
            "Papi has these preset definitions, but on certain architectures they don't always mean what you what you think they might mean.",
            "So you can look in.",
            "I'll show you a little bit tomorrow, maybe how you can sort of look and verify that it's doing what you think it's doing, but some on some platforms the accuracy of accuracy of the counters may vary.",
            "Some may be better.",
            "Summit may overcount.",
            "A little bit so you have to be a little bit wary there and on some platforms they don't exist at all.",
            "So for example, an interesting fact is that the latest the current generation of Intel processors like Haswell processors, you can't measure the floating point operations number floating point operations, the hardware registers there don't support accurate measurement of the floating point operations, so we have to wait till Broadwell and then that's supposed to be fixed."
        ],
        [
            "OK, so this just shows how impaired profile.",
            "So this more tomorrow.",
            "So I'm going to probably skip this, but you can get a list of these counters here.",
            "Impera Prof and you can start to make metrics out of these that you're interested in floating point operations per second.",
            "Cat total number of cache misses or cache misses per cycle or that sort of thing."
        ],
        [
            "OK, so coming back to our unres example.",
            "So this we know that this elec function was the most important function where most of the time was spent, and so we're looking at what is.",
            "What are the floating point operations per cycle in that ILEC function, so it's not in the top of the list, is not doing the most efficient, but it's doing .6 floating point operations per cycle.",
            "The peak per cycle on this particular processor was two, so it's not doing too badly.",
            "It's getting about 30% of the total floating point operations that it could possibly do per.",
            "For cycle and so not too bad 30% of peak performance.",
            "Floating point performance in this particular function that we're."
        ],
        [
            "But we might want to."
        ],
        [
            "Prove that still say it was really bad.",
            "Then you'd want to look deep more deeply at why aren't you getting good floating point performance and that that involves getting the data in the cache is so that you can access so that the processor is kept busy.",
            "So if you want to minimize the amount of data movement in these codes, data movement is very expensive floating point operations or cheap, and so you want to.",
            "You want to look into that.",
            "In this case we just where you want to really want to start, though is just.",
            "Doing compiler optimizations, so in this case, as we mentioned, there was lots of little functions and the compiler was not inlining those functions.",
            "So after we forced the compiler to inline the functions, now we got a boost already from 30% to 40% of the peak performance.",
            "And that's actually a fairly significant.",
            "So that's a 30% boost in our or no, it's not, not quite.",
            "Yeah, about 30% boost in our performance there, so that's really nice.",
            "I."
        ],
        [
            "Um?",
            "OK, so we're not going to go into the detail.",
            "We don't have time during this very brief time.",
            "We have together in all the things you can do to optimize your serial performance on a core, but there's lots of resources out there where you can find more information at once.",
            "One of these tools point you to the problem spots you can look at different options for your particular implementation and algorithm.",
            "How you go about optimizing."
        ],
        [
            "Performance there.",
            "Um?",
            "OK, so now we were happy with or at least we've identified or addressed some of the serial performance and that's that.",
            "Is 1 important.",
            "Point is that before you invest too much effort in optimizing the scalability of your code, you do want to make sure that you have reasonable single core performance, because a great way to artificially have very highly scalable code is to have terrible single core performance, and then it takes so long for you to single core work that it hides all the bad.",
            "You know all the slow communication.",
            "And that sort of thing so it can look a lot better than it is.",
            "So if you at least have a reasonable optimization of your serial, your single core performance first, then that's a good time to really invest in optimizing your parallel performance.",
            "OK, so this is.",
            "So you want so so like I mentioned, one of the things you want to look for our load imbalance.",
            "You want to look for regions of the code that you didn't think you had to parallelize that now have become important and are now bottle next to your here code.",
            "So after doing this measurement with Tao."
        ],
        [
            "This is a recipe for how you can use Para Prof to look at this.",
            "I'm not going to go through this.",
            "As I mentioned that's there for your reference.",
            "If you ever want to explore this and get in."
        ],
        [
            "Nice plot like this.",
            "This was a screenshot from from paragraph and this is.",
            "Looking at the overall scaling of the MD part of the unrest code so there's this Phase MD just this MD loop OK and so every time we were going from 2 to 32 cores, each time doubling the number of cores so it should be going down by 50% every time and overall the code up to 32 cores doesn't do too badly there, although it starts to drop off here as we go up to 32 cores, but now we can look at that on a per function basis, so this is kind of cool because you can see a per function scaling of your parallel scaling.",
            "Of your application, so which functions are scaling well and which ones are starting to become a bottleneck?",
            "This elec is doing pretty pretty well.",
            "Yeah, it's dropping down each time.",
            "Not too bad, but you see that there's this function here, know right here that was only taking 1.4 seconds on on two cores.",
            "Which out of 68 isn't that much on two cores, but once you get down to just six seconds on 32 cores?",
            "Well, 1.4 seconds, that starts to be actually a bottleneck, so that becomes a problem so."
        ],
        [
            "So this serial function will begin to dominate the runtime."
        ],
        [
            "And.",
            "So that's that's finding a serial function.",
            "Look at the scaling.",
            "This is another recipe for looking at load imbalance using Para Prof."
        ],
        [
            "And you can generate something like this.",
            "And so here we can see that there's quite a bit of imbalance, so this is so.",
            "This pair of problem we explain just a little bit.",
            "This is showing these are all the processes, so process zero to 15.",
            "So this is on 16 processes, 16 MPI processes."
        ],
        [
            "And we're only looking at the time in this."
        ],
        [
            "In the loop an receive, there's multiple functions here that they have imbalance.",
            "And also here's this serial function we saw earlier, right?",
            "There's no, it's not only running on process 0, and that's everything else will be waiting on that."
        ],
        [
            "There's the."
        ],
        [
            "That one OK, so just a little anecdote here.",
            "The so the developers here were actually surprised by this that they thought that their algorithm should have very good load balancing in parallel, and that was not the case.",
            "So they went back to the drawing board and what they did was they actually decided that there was a better algorithm.",
            "So like like we set the very beginning, the best way is to know if it.",
            "The best way, if it's if it's a reasonable thing to do, which was in this case, is to maybe adjust your algorithm, and in that case they found an algorithm that not only should give better load balance, but would perform fewer floating point operations total, so they cut the number of floating point operations."
        ],
        [
            "In half and this is the result.",
            "So this is how it looked at the end.",
            "Now we've got very good load balance across these different functions.",
            "They've gotten rid of the we got rid of the serial bottleneck there.",
            "Nope, we didn't know.",
            "Yes, we did.",
            "That's the mean.",
            "This is zero.",
            "OK, so we got rid of the serial."
        ],
        [
            "Bottleneck there, it went four times faster after this because they were getting like 40% of peak performance.",
            "They cut out half of their floating point operations and so that translates to four times the performance.",
            "So the best thing to do is to get rid of those flops floating point operations if you can, and that's what they did.",
            "And also they improved the scaling, although it also became more difficult to scale because the code is more efficient."
        ],
        [
            "But then you can do around 2:00 and you can look to see OK's or other sources of imbalance that looks like it's idling on this mpiana call too.",
            "This is MPI barrier, but beware of investing too much effort."
        ],
        [
            "2 little gain, but if you do, when you do go looking at MPI calls and where these API calls are spending their time, these call paths can be helpful for designating finding which MPI barrier."
        ],
        [
            "Is actually the one we have to worry about.",
            "This game this is the root view."
        ],
        [
            "Paragraph and but then you might want to do some tracing to actual."
        ],
        [
            "Look at the cause and effect of of those.",
            "Of this, MPI calls and what's causing the slowdown.",
            "Communication bottlenecks?",
            "So I'm going to stop there 'cause later on Christian is going to show you more about tracing and the the parallel performance measurements.",
            "I'm going to finish and just summarize here what we've talked about.",
            "A good choice of algorithms, both in terms of your single cores serial implementation and your parallel implementation is and algorithm is the most important choice you make.",
            "Performance measurement can help you understand whether not you made a good choice in your algorithm and also in your implementation, and then do the simple optimizations first, like compiler optimizations, maybe MPI parameter tweaking, and that sort of thing, and then gradually work up to more serious investigation if you're if the potential benefit warrants it and just check your single core performance before doing intensive parallel scaling, use the right tool for the job, like a sampling tool for a quick look at.",
            "And what's going on and exceed, and I think also praised staff are available to help you with these kind of things as we did with this group from Cornell.",
            "And so one of the things be aware of.",
            "So with that I will turn it over to Christian for the rest of this session.",
            "Oh sorry.",
            "I'm too much in a rush premature optimization.",
            "So welcome to the second part.",
            "Let's switch.",
            "Monitors OK.",
            "This way.",
            "OK, so you can find the slides on the wiki.",
            "So here you see the.",
            "Field slides here and we are now working with this.",
            "I HV C As for one so they're all the commands I will type into my laptop will be shown so if I'm too fast or too slow you can use these slides as a reference.",
            "So I will.",
            "Basically these are the slides here so.",
            "I will.",
            "Tell you a different story.",
            "I will not tell you a success story like filled it, but I will tell you a workflow story how to apply the tools to get some information about the performance of your code.",
            "So what we will do first.",
            "Or we will work on a code developed by one of the guys you saw in the group.",
            "Picture that Thomas Sterling presented us on Monday so it's in Espiral benchmarks developed by NASA.",
            "It's a very old code.",
            "That is highly optimized so we will not find so many tuning opportunities there, but this code is very portable and can be configured to nearly every system.",
            "So this is the reason why we choose the code for this workflow example.",
            "So please log in into bridges.",
            "With your own computer.",
            "Or with the virtual machine.",
            "So depending on your preference.",
            "So is this large enough or shall I increase the font size?",
            "Increase.",
            "OK. Can you increase it to one OK?",
            "So OK.",
            "So now we're on bridges.",
            "We first need to set up our environment so to load some modules.",
            "Therefore I created a script that we need to source.",
            "So just type as I do so source and then you go into.",
            "My home directory.",
            "It's RE double SEL.",
            "There is a subdirectory called IHP CSS16.",
            "A subdirectory called tools and there is a.",
            "Source me.",
            "File and.",
            "Just hit return and you will see what modules are loaded.",
            "There are also the tools are now in in your past, so scorpis calasca.",
            "The tools that we need during this session.",
            "So next thing we do is we copy the code that I prepared into your home directory.",
            "So it's a it's a tab.",
            "Also we extract it.",
            "Anits also in my home directory.",
            "Now it's in the tutorial.",
            "Subdirectory and it's been asked for benchmarks.",
            "So if it's too fast to slow, just raise your hand.",
            "The mentors will help you.",
            "OK, we just extract it.",
            "And then we change into this directory.",
            "We can.",
            "Yeah.",
            "So if it's too fast, all the commands are also in the in the slides, so we can switch between.",
            "So, for example, this would be.",
            "This slide here.",
            "So what we see here in this directory.",
            "So we have three different benchmarks here.",
            "The BTM debt and that this is the one we will use and two other ones.",
            "There's also some some readme stuff here and the makefile.",
            "So this BTM is at code.",
            "What does it do?",
            "So it's a it's a solver for it's a CFD solver.",
            "But it doesn't really matter in particular what it does.",
            "So we just try to get some build instructions and therefore we type make and at the bottom.",
            "Um?",
            "We see a suggestion here.",
            "Uh.",
            "To build this code, we have to specify three parameters.",
            "First the benchmark code we want to build.",
            "Here we want to build the BTM set, then a class which basically describes the size of the problem and the number of processes we want to run the problem on.",
            "So why do we want to run it on 8 processes?",
            "Well, we know that bridges has bridge CPUs have 14.",
            "Course one note has 28 cores, and we're going to use 2 notes, so we have 56 cores in total, and this can be split into eight processes using seven threads in total.",
            "If you are new to machine and you don't find the information about the system configuration in.",
            "In the documentation you can also use a tool called liquid topology, so this will give you.",
            "This will give you.",
            "Information about the system.",
            "So for example, it tells us so that here on this is on the login notes, but the login nodes on bridges are the same as the compute nodes.",
            "We're using an entirely on processor and it's a hassle.",
            "Asshole one.",
            "It has 14 cores.",
            "For sockets and one thread per socket.",
            "So hyper threading here is isn't active and we also see that there are two sockets per note when we Scroll down a little bit we can see more information we can see also the cache topology, which might be interesting for those who work on the programming challenge.",
            "So this tool is not installed on bridges, but if you source the file I just mentioned in the beginning, then this tool is available.",
            "So OK, we will build the benchmark.",
            "Using eight processes.",
            "So basically we just use the command.",
            "That was suggested.",
            "And classy.",
            "Will make it run approximately 18 seconds.",
            "So when we just type this command and start building.",
            "The code you see it's a Fortran 77 code.",
            "Here we use the MPI F-77 compiler from the Open MPI implementation.",
            "It's also open MP code, so therefore we have to specify this F open NP.",
            "Option which depends on the.",
            "On the compiler you use and here in.",
            "At the bottom of the screen you see that it created a new executable in the bin subdirectory.",
            "So we can go into this.",
            "Bin subdirectory.",
            "Do an LS.",
            "And there it is.",
            "So what do we have?",
            "Now we have a.",
            "We have an executable.",
            "We didn't use any tools so far, so this is an executable.",
            "We can do a reference running list, so reference run is.",
            "Is important so that you can can either check when you're on a new system that your code runs as expected, so you should have some tests or some verification built in, and you can also later on when you run with tools.",
            "See what kind of overheads the tools create.",
            "So how to run this code so we have a job script?",
            "That you can copy into.",
            "This directory, so just.",
            "Do a CP, go one directory up.",
            "Then there is a job script directory.",
            "Machine so if you want to use this example on on your machine at home, there are job scripts for several other systems available, so you don't need to write your own, so you can use several of these as a template.",
            "But here we are going to use the bridges ones and we take the reference as batch.",
            "And copy it into our directory.",
            "So.",
            "This was in command to copy.",
            "OK, so we can look into this reference batch script.",
            "So here we have the.",
            "Our reservation for the summer school we give thee.",
            "The job name we specify output file, so where all the output is written and also an error file we want to run on two nodes.",
            "We want to use eight MPI processes and we want to use seven threads per process.",
            "We also specify a time here.",
            "It's one one minute, so the code is supposed to run in 18 seconds approximately.",
            "What we do here is we we source our tools.",
            "So what we did in the beginning.",
            "So actually for the reference run this isn't really necessary.",
            "But for example, if you wanted to use this topology liquid topology program here in on the compute nodes, then this would be necessary.",
            "We need to export the number of threads we want to use, so.",
            "This is the usual way you know from the classic session from David.",
            "So here we can make use of the number of threads we specified here.",
            "So this Dash C7 is then stored in the slum.",
            "Slam environment variable that we can use here.",
            "So if you want to change the number of thread, you need to just change this one here.",
            "Everything else?",
            "Can stay as it is, so if.",
            "What is my drinking is done?",
            "Sorry.",
            "This year yeah.",
            "OK, this this is a number of threats you will use per process, so it's a hybrid program.",
            "MPI Plus open MP.",
            "And if you want to restrict or if you want to set the number of open MP threads, you need to export this variable.",
            "So this is standard way to to specify the number of open MP threats you are going to use when you run a program.",
            "When you are a hybrid program.",
            "There are other ways to do this, but this is the most common way.",
            "Just specify this open and P NUM threads environment variable and you're done.",
            "Um, OK.",
            "So then we just.",
            "Define some variables here to to build the build.",
            "The xname here of of this program and in the end we do an MPI run.",
            "We also specify Dash Dash report bindings, so if you're interested you can then after the run look where individual process season individual threads run on which part of the hardware.",
            "So if you use a different MPI, so this is the open MPI we're using here, then there is this.",
            "This option here might be named differently.",
            "So we need to specify the number of processes we are going to use, so this is here in the variable slum and tasks and this comes from here.",
            "So we configured the.",
            "The benchmark to use 88 processes we specified here and then we just add the.",
            "Binary we're going to execute.",
            "So we are using this job script and modify it a little bit later on when we use it with scorpion's calasca, but from now we can just execute our reference run.",
            "Bye.",
            "Using the S batch command to.",
            "Submit it to the queue.",
            "So he it says, submit it so we can.",
            "Um?",
            "With SQ dash U.",
            "And our user name.",
            "We can look if our job is running or if it's queuing.",
            "So here.",
            "It's already running.",
            "So we have to wait a little bit so at least 1818 seconds and we can see we will see if there are some output created.",
            "Oh, there is some output already.",
            "Um?",
            "So there is an error file written and the output file.",
            "So if if you have problems following the instructions right, sore hands.",
            "If it's slow to fast, please let me know.",
            "And remember, you can also always go back to to the PDF slides.",
            "To see, yeah.",
            "Also.",
            "Installing the tools previously replied that would help us with that added.",
            "So OK. Let's look at at the output.",
            "Let's look at the.",
            "Alpha Delta program run.",
            "So it tells you that it's an Esper benchmark.",
            "It uses.",
            "16 by 16 zones it uses a process is here and the total number of threads is 56, so seven threads per process.",
            "This is what we specified to use.",
            "We see here that it's running for 200 timesteps.",
            "And at the end it does a verification.",
            "So this is a test I mentioned in the beginning.",
            "So if you're on a new machine, you first have to check if you run instrumented code.",
            "Um?",
            "Run successfully and the results are as you expected and here the verification is successful.",
            "This is a good time, good sign.",
            "And this benchmark already print some some some data out so it tells us that it took 17.82 seconds to run.",
            "This is something to keep in mind because we want to compare it to an instrumented run that we're going to.",
            "Do I think we have enough time?",
            "We can do it today.",
            "So.",
            "Were you successful to to run the reference?",
            "Using your your access to bridges, yeah.",
            "OK.",
            "So then.",
            "I think I will or what we're going to do next is we use the.",
            "We use the same.",
            "Use the same program and we now instrument it, so this is one of the two techniques Phil mentioned, so they're sampling to get some information out of your program and instrumentation.",
            "But before we going to do this, I will show you quick just a few slides about the tool we we are going to use.",
            "It's a tool called.",
            "Scorpis school."
        ],
        [
            "OK I will.",
            "Just go quickly over a few slides, so there are lots of reference slides in there.",
            "I."
        ],
        [
            "Just tell you the most important stuff.",
            "So some time ago there was a fragmentation in the in the two landscape, so there were several tools available so some of them were already mentioned, so it was calasca towel bumpier.",
            "It's a trace visualizer and they all came with their own measurement system and their native formats.",
            "So this was for the developers and also for the users.",
            "And pretty bad situation becausw.",
            "Usually what the users want to do is."
        ],
        [
            "They want to do a measurement and then use different analysis tools.",
            "Look at the data so how it looked in the old days.",
            "So you ask Alaska, which came within all with a known measurement system.",
            "It created traces in epilogue format.",
            "Then there was a trace analyzer that produced cube output and then you could look at these output with a cube presenter.",
            "So then there was a tool one pier that came with one peer trace as a measurement.",
            "They created a different.",
            "Output format and I looked at it with Vampyr.",
            "Then there was towel towel had just creates profiles, but also came with its own measurement system.",
            "And it could also read the cube profiles and show it in paragraph, something we are showing you tomorrow.",
            "Then there were parallel traces that didn't work together out of the box.",
            "And then tell had the idea not only to do profiles, but traces, so they were taught races.",
            "Then people started building some some converters from different trace formats, but so trace formats or trace files usually are.",
            "Huge in size, so this is a very time consuming process which you normally don't want to do.",
            "They work for one or two 2 versions of the of the tools and then the support was discontinued.",
            "You also for the user a very bad situation and this went on and on, so there are in the end there were lots of possibilities and you were just lucky if some of them really worked out.",
            "Well, even more."
        ],
        [
            "So then we came the tool tool developers set together and thought well we need to do it differently and we were lucky to get some funding from the German government and also from the Department of Energy and we created a new measurement system called Score P that creates one profile output and one trace output.",
            "These outputs are informants at a well specified and several tools can sit on top of it.",
            "And use it so you have just one measurement system, one learning curve how to instrument your application and then you can use different tools to look at your on your data.",
            "So I just skip over this."
        ],
        [
            "Here.",
            "But will spend some time on this slide, so here you have your application and what we do with score P is we insert some hooks like Phil already told you so you can insert some MPI hooks.",
            "By library into position you can do the same for for shmem communication there is the same for or something similar.",
            "For open MP.",
            "There we use source to source instrumentation instrumentation to get some information about the Open, MP directorates and Open MP API functions for pthreads.",
            "We use library into position.",
            "Then we also have some support for accelerators so CUDA provides a special library that can be used to get some information.",
            "And out of the CUDA device we have support for open CL and in the upcoming version also for open ACC.",
            "Um?",
            "Then we can also utilize the compiler to insert hooks at every function entry and exit this is.",
            "This can be our the overhead created by this can be quite high, so you need to use this with caution, but we will come to this later.",
            "During the hands-on.",
            "There's also a source of source instrumental called PDT where you can do the same.",
            "And there are some macros provided so that you as a user can just instrument the portion you're interested in.",
            "So I will show you an example tomorrow where I did this with the Davids Davids traffic example.",
            "So it's basically quite easy to do this and then to to create a trace and see what really is going on.",
            "So what who's which processes communicating with you.",
            "Then we also have the possibility to do sampling this only on X86 platforms, so sampling is.",
            "It's difficult to get right in a portable way.",
            "And.",
            "So either way, so if you have some hooks here or if you have some some sampling interrupts when one of these events are triggered, then.",
            "This copy measurement in infrastructure is called there.",
            "We take a timestamp.",
            "We take some thread local storage.",
            "We may take some hardware counters.",
            "So Poppy are usage or Perth counters.",
            "You can also plug in some yes on external plugins.",
            "And then we store this information.",
            "Either in a profile format so it's it's a cube format, or as event traces.",
            "That preserves the time dimension, and that's basically it.",
            "For measurements, so there's only some some online interface, but we will not cover this during these lecture here.",
            "So what can we do with the call?",
            "Past profiles we can look at the results using the cube browser.",
            "We will use a cube browser tomorrow, but so for our lecture here we create cube files and we look with towels, power Prof. Analyze the profile using this tool.",
            "Tomorrow we will create some OTF traces.",
            "And we will.",
            "Then automatically analyze them with calasca.",
            "Scholastica will then create as and as a result a cube file that we then can use that we then can visualize using a cube browser.",
            "So, but for you as a user for doing the measurement, you just need to think about this box.",
            "Here.",
            "You need to think about how should I instrument my code so there are different options are already mentioned here.",
            "Some of them have high overhead to some of them don't.",
            "And there are also lots of runtime options, so environment variables where you can.",
            "Change the behavior or the amount of data that is collected when you run your instrumented program."
        ],
        [
            "So."
        ],
        [
            "Oh, OK, let's switch back to the hands-on.",
            "What we are now in.",
            "Going to do is we will use.",
            "Score P so I haven't told you how to use score P, but it's actually quite.",
            "Quite easy, so we're still.",
            "So.",
            "In the top level directory of our benchmark.",
            "Um, so we somehow need to.",
            "To rebuild our application and.",
            "During the rebuild, we need to tell the.",
            "Compiler to do something special so to to use the script functionality to insert the hooks in your code.",
            "So we do this by.",
            "By changing one file so.",
            "You can use any editor you want.",
            "So it's in a config substory directory.",
            "There is a call.",
            "There's a file called Make dot death.",
            "We just opened it so this is the top configuration file for this.",
            "So for this application.",
            "So here you see that we specified the F Open MP.",
            "The compiler option because we're using the GCC compiler.",
            "If you use an Intel compiler, for example, you need to use this Q Open MP and so if you want to play around with different compilers you need to edit.",
            "You need to modify the just this file.",
            "So.",
            "OK, here's the line that specifies the compiler we're using.",
            "It's the MPI F 77.",
            "This is the open MPI Fortran compiler.",
            "This is what we use for the for the reference run.",
            "Um?",
            "Now.",
            "We want to use corpi in addition, so the only thing what we need to do is to just remove the comment here.",
            "So now we are going to use.",
            "Scorpis distress user NPI F 77.",
            "So we just pre pend.",
            "The the score P command an once copy option to the usual compiler.",
            "That's basically all you need to do.",
            "Depending on the build system you use with your applications, it might be easy or not so easy, but there are also some other options that make it easier.",
            "For example, if you use a CMake based build system, or autotools based.",
            "A build system if you want to try it with such a system.",
            "Please let me know.",
            "So we just save this file.",
            "Close it and then.",
            "We need to rebuild it.",
            "So where here's our May command?",
            "First we need to do a make clean.",
            "Otherwise.",
            "You will get no instrumentation.",
            "OK. Let's build it.",
            "OK, what we see here is.",
            "That we know.",
            "Repent the compile command with Scorpid estrus user.",
            "So this test test user keltz copy that it should activate.",
            "User instrumentation user instrumentation is.",
            "Yeah, some kind of macros you insert in your code to highlight some some special regions you're interested in.",
            "So if you want to see all possible.",
            "Options you can pass to score P. You do a scorpid Estes help.",
            "So and you can switch on the bus city level to see what was Scorpius doing under the hood.",
            "So for example, for our MPI Open MP case.",
            "We first.",
            "Use a tool called Aparri to do some source to source instrumentation for the Open MP Pragmas.",
            "And then we do the actual compile command.",
            "If you want to inspect the the intermediate files, you can use this keep files command, so this might be.",
            "Quite interesting.",
            "In case we are using GCC, you can also do some compile time filtering and you can go through this list here.",
            "There are lots of parameters you can you can specify, so some.",
            "The.",
            "Some options are on by default, so this is for example this Dash compiler.",
            "And these this option inserts for every function enter and exit.",
            "Hook into your code.",
            "So this is this is the default if you just want to see where.",
            "Where are my hot spots in the code.",
            "But if you have, if you know your code and it might be and you you're working long time with the code, you might think of adding some user specific instrumentation at some important points so that you can easy easily run it and then keep keep all the instrumentation in your code with some with some defines that might help you also to automatically find some some performance regressions you built into your code.",
            "Um?",
            "OK. Well.",
            "So the compile.",
            "Created a new binary, but not in the bin Directory but in the bin Scorpid directory so.",
            "Can just CD into this directory.",
            "And here we have now the executable.",
            "It's the same name but now with Scorpion instrumentation.",
            "So process is quite simple, you just prepended compile command with this copy command and everything else is done automatically.",
            "OK, to run this code, now we need to.",
            "Copy a different job script.",
            "So this is again in the job script.",
            "Directory jobs groups bridges.",
            "And then we copy the score P dot S Patch into our directory.",
            "And if we look.",
            "And this file it looks similar to the old one, so we still have their reservation.",
            "Here we have the error and output files.",
            "We have two nodes, 8 processes, 7 threads per process.",
            "We source our tools, we set the number of open MP threads and but we also.",
            "Can do some configuration for Skokie, so this is.",
            "Not really necessary, so if you don't specify anything then Scorpia runs in profiling mode.",
            "This is the default, but here we specify the directory so where the output.",
            "Will then.",
            "And.",
            "Will can then be found?",
            "Um?",
            "So if we if you don't specify this directory, then you have the directory name is just Scorpion, then timestamp and then the random number.",
            "So if you do several measurements then this gets a little bit confusing.",
            "Overtime here are some more scorpis.",
            "Variables, but they recommended, so we're not going to use them right now, but here, for example, is a specification of some puppy metrics so that Phil already mentioned.",
            "So it's the only thing you need to do to get a puppy measurement at every function or region.",
            "Enter and Exit is to specify this line here.",
            "And to specify the counters you are interested in.",
            "But be aware that measurement of Papi counters takes quite some time, so this adds a lot of overhead and you might want to do this with serial application.",
            "For example, if you want to just.",
            "Optimize your.",
            "Your computational kernel.",
            "And you're not interested in communication.",
            "We run it as is, so just do an MPI run.",
            "Specify the number of tasks and the name of the executable.",
            "So, um.",
            "OK, I just submit it.",
            "So what we have done so far is we we created a reference run to see that everything works fine on this machine.",
            "So we looked at the output verification was successful, it took approximately 18 seconds.",
            "Then we used tools for P2.",
            "Instrumented code so, and it's quite easy to use, you just prepended to the usual compiler.",
            "You build the tool again.",
            "And.",
            "For the first run, you basically don't need to specify any additional configuration, so this would mean that you will get a profile output.",
            "And.",
            "Let's see if this already happened, so now.",
            "Still running or queuing?",
            "OK, it's ready and here.",
            "Here we are so.",
            "You see the the directory we specified, so we just gave it a name and here we have the number of processes and the number of threads and the job ID.",
            "So if you specify the job ID in the name, you can start several jobs from the same directory and you don't get problems that one measurement want to write into already existing directory.",
            "We have an environment variable.",
            "For this to specify what what will happen then?",
            "But usually it's better to have distinct directories for every measurement, so I already talked about the scorpion environment variables, so but where when you want to use and where to find them.",
            "So therefore we have a different tool, it's called Scrapy info.",
            "And if you pass it a dash dash help.",
            "Um?",
            "You see basically 3 commands here.",
            "This is a conflict virus.",
            "This lists all the measurement configuration variables.",
            "There's also this conflict summary.",
            "This gives you a summary of the installation of Scorpio, so which compilers were used.",
            "This is quite helpful if you run into a back and you want to communicate with us and you can also look at a fire called open issues where we list some problems on some specific architectures that we're at.",
            "So far we're not able to.",
            "Suffix.",
            "OK, let's look at the.",
            "Configuration very variables.",
            "So.",
            "So here you see the.",
            "Are two very important ones, so this is Scorpion able profiling.",
            "This is the default.",
            "You want to do a trace measurement.",
            "Then you need to to enable this variable or you can use a tool like Scholastica.",
            "We do this tomorrow.",
            "Alaska will automatically use this environment variable for the sampling.",
            "One you need to specify this unwinding here.",
            "But this will not be part of this lecture.",
            "Then we have the scope scrapy, total memory.",
            "This is the amount of memory for our internal buffer.",
            "We will need this tomorrow for Trace experiment because Trace experiment can be quite huge and we want to fit everything into a memory buffer because we don't want to intermediate Lee during the measurement.",
            "Flush the buffer to to file.",
            "This is such a huge perturbation.",
            "The measurement is basically.",
            "Use this if you have some intermediate flashes.",
            "OK.",
            "So.",
            "Let's look at the.",
            "The output file.",
            "So it's.",
            "Basically the same.",
            "We still have these two 100 timestamps.",
            "We still have 56 threats, 7 threads per process.",
            "Verification is still fix successful, so this is quite good, but now we see here time and seconds increased quite a lot.",
            "It's now 35 seconds and in the beginning I think it was 17 and something so we need to do something about this measurement overhead.",
            "If we look at the.",
            "At the error file, I think we haven't done it.",
            "For the reference run here.",
            "You see just the commands that were executed in the.",
            "In the job script.",
            "Here basically the the NPR run command.",
            "We ran the application with eight processes and this here all the remaining is the output of this binding report done by open MPI.",
            "So despite.",
            "Law.",
            "OK, so but let's look at the.",
            "Experiment directory.",
            "So what's in there?",
            "So there are two files.",
            "Profile dot cubex.",
            "So this is our profile report.",
            "We are going to use.",
            "Tomorrow with the power Prof and we also have a scar P config file.",
            "This file lists all the active.",
            "Environment variables that were used during this run.",
            "So here we see profiling was true, so we just collect the profile.",
            "Tracing was false.",
            "Unwinding was false.",
            "Total memory was the default.",
            "16 megabytes and yeah, lots of other.",
            "Environment variables that for all purposes here are.",
            "Not interesting and maybe here you see no poppy of us defined.",
            "Now our usage counters were defined.",
            "No plugins were defined, so we just collected.",
            "Information about function enter exits MPI and Open MP and so.",
            "We're not going to.",
            "To use.",
            "Tools on bridge is to look at the data becausw the interactive.",
            "These are interactive interactive graphical tools and this it's just slow if if we all use this remote machine.",
            "To look at them so we need to copy this stuff back to our machine.",
            "So therefore.",
            "So.",
            "Well.",
            "I create another.",
            "Terminal on my.",
            "My laptop here.",
            "And I just copy the stuff.",
            "From bridges back to my laptop.",
            "So if you're using.",
            "If you're using.",
            "Party so there are other ways to copy stuff back from bridges, but if you're using a terminal, just do it like I do, but use your own username.",
            "So and.",
            "The file.",
            "It's located here, so you need to change this.",
            "Should copy back your own experiment.",
            "And the file is named.",
            "Profile that.",
            "Cubex and you just copy it back to your local machine.",
            "OK, there it is.",
            "And so now it's three o'clock.",
            "So what we're going to do tomorrow is we use this specific file and you are going to use power Prof on your machine that you hopefully already installed, and then we look at these files.",
            "Maybe I do.",
            "Very quick preview.",
            "Yeah, I think we should.",
            "I think we need to go back and make sure you've got the tools installed and ready for tomorrow.",
            "First thing you know, copy if you ran the experiment, copy that back to your laptop and then will you spare Prof tomorrow to look at the results and feel free to play around with their Prof and take a look on your own but will go through that.",
            "I'll show you how to look at the data different ways tomorrow and then we'll go into the tracing another hands-on well, yeah.",
            "If you if you didn't, if you weren't successful in collecting your own profile, there are ready made profiles on the wiki page you so you can download them and play with them.",
            "Thank you, thank you just."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we are continuing this afternoon with the first part of the performance analysis and optimization.",
                    "label": 0
                },
                {
                    "sent": "Say that will be continued tomorrow, and it's a pleasure for me to introduce again also as last year and the year before.",
                    "label": 0
                },
                {
                    "sent": "Phil Plait in Christian Felt, who jointly will cover this session, so please go ahead, Thanks Sermon.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "My name is Philip, Blood from the Pittsburgh Supercomputing Center.",
                    "label": 1
                },
                {
                    "sent": "I'm going to be starting out here with this performance session and I'll be introducing the topic and then Christian will be taking over with the hands on in a little bit and we'll see how far we get.",
                    "label": 0
                },
                {
                    "sent": "So I think this is number 7 that of the summer schools that I've been able to present at since the first one, which which came about because I think the person they'd invited couldn't make it, and so I was the second choice, and I'm glad that they called on me.",
                    "label": 0
                },
                {
                    "sent": "It's been a lot of fun to teach these at these summer schools, so we're looking forward to talking about how to engineer your app, your codes to be more performant, and in particular how to go about.",
                    "label": 0
                },
                {
                    "sent": "Analyzing the performance using the variety of performance tools that are out there so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just upfront I just wanted to thank Christian who's had a lot of input in what you'll see today and also my interactions with the groups that are now part of this virtual Institute.",
                    "label": 0
                },
                {
                    "sent": "For high productivity supercomputing, including you lick and University of Oregon and others, have really generated the ideas and and the things that will discuss today and also a colleague of mine, former colleague from PSC, Robbie ready.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who contributed to this talk way back seven years ago when we started this?",
                    "label": 0
                },
                {
                    "sent": "So this is the rough outline for the session.",
                    "label": 1
                },
                {
                    "sent": "This is probably an optimistic outline, so today I'll be giving this introduction.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, and then Christian will start to do some hands-on with performance profiling of a test application using a tool called Scorpion, which we'll talk about further today.",
                    "label": 1
                },
                {
                    "sent": "We probably won't get quite all the way through that.",
                    "label": 0
                },
                {
                    "sent": "Today we will see and.",
                    "label": 0
                },
                {
                    "sent": "And then tomorrow I'll show you how to use a tool called Para Profits in Tau for analyzing these profiles that you'll generate on bridges for this application.",
                    "label": 0
                },
                {
                    "sent": "And then which you could also use with your own codes.",
                    "label": 0
                },
                {
                    "sent": "And I welcome you to try that out at your earliest opportunity as you as you pick up how to do this through the hands-on sessions and maybe before you leave.",
                    "label": 0
                },
                {
                    "sent": "We can even help you with your own code, which would be, which would be the most relevant and helpful thing that we could do here so.",
                    "label": 0
                },
                {
                    "sent": "Please keep that in mind and will go through some different aspects of performance analysis and in particular trace measurements, which is a time resolved measurement of what's going on in your code will talk more about that later, but I just want to emphasize here as we get started that we want this.",
                    "label": 0
                },
                {
                    "sent": "We want to go to pace that will be beneficial to you.",
                    "label": 0
                },
                {
                    "sent": "And so if we start to leave too many people far behind and then just let us know, feel free razor and ask questions.",
                    "label": 0
                },
                {
                    "sent": "Will try to get.",
                    "label": 0
                },
                {
                    "sent": "I'll try to get through my material fairly quickly so that we can get into the hands on part of it.",
                    "label": 0
                },
                {
                    "sent": "But if I go through too quickly, feel free to stop me, raise your hand and ask a question.",
                    "label": 0
                },
                {
                    "sent": "And same thing with the hands on as we go through it.",
                    "label": 0
                },
                {
                    "sent": "Feel free to sort of stop us and say you're lost and we'll try to get together and at least help.",
                    "label": 0
                },
                {
                    "sent": "Most people who wanted who want to be involved get caught up so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so why do we need to worry about optimizing our scientific codes 444?",
                    "label": 0
                },
                {
                    "sent": "Performance on the current HPC systems.",
                    "label": 0
                },
                {
                    "sent": "Well, one answer to that.",
                    "label": 0
                },
                {
                    "sent": "An important aspect of it is that the processors that we're using in these HPC systems are essentially the same processors you have in your laptops, and so they're not designed for your HPC algorithm.",
                    "label": 0
                },
                {
                    "sent": "Your scientific algorithms.",
                    "label": 0
                },
                {
                    "sent": "They're designed for streaming multimedia and and doing these general things.",
                    "label": 0
                },
                {
                    "sent": "This is why we can build such big systems because it's become cost effective because so the whole world is buying these processors, so they're not just making it.",
                    "label": 0
                },
                {
                    "sent": "For you and for your science, which is great, but the problem is, is that your algorithm does not fit the architecture and so we need to try to adapt our algorithm as much as possible to to the architecture that we're given, which is which are these commodity processors.",
                    "label": 0
                },
                {
                    "sent": "In some rare cases, just to highlight the counterexample or a counterexample?",
                    "label": 0
                },
                {
                    "sent": "You can invest a lot of time and money in developing a processor that is specifically just for your scientific application.",
                    "label": 0
                },
                {
                    "sent": "This is a very successful effort to develop a molecular dynamics special purpose application specific integrated circuit.",
                    "label": 1
                },
                {
                    "sent": "It's called Anton from the Shell research, we actually host one of these at the Pittsburgh Supercomputing Center and so the algorithm formula dynamics the important aspects of the algorithm are actually hard wired into the hardware and so it's very very very fast.",
                    "label": 0
                },
                {
                    "sent": "Orders of magnitude faster in general than what you can do with with commodity hardware from like dynamics, so that's not an option for.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, most of us, so we're going to talk about how to fit our algorithms.",
                    "label": 0
                },
                {
                    "sent": "Are applications, how to understand how we can do that better with the hardware that we have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, having said that, talk about algorithms.",
                    "label": 0
                },
                {
                    "sent": "I just want to drive home at the beginning that the choice of algorithm is going to be the single most important thing.",
                    "label": 1
                },
                {
                    "sent": "Decision that you can make in terms of the performance of your scientific application.",
                    "label": 0
                },
                {
                    "sent": "This is where you're going to get orders of magnitudes.",
                    "label": 0
                },
                {
                    "sent": "Gains in performance is if you can find a better algorithm that does what you need more efficiently with fewer floating point operations or whatever, the best optimization can make is to not do that floating point operation at all.",
                    "label": 0
                },
                {
                    "sent": "So that is the most important thing, and that's where you get.",
                    "label": 0
                },
                {
                    "sent": "I huge huge gains, so this is most important consideration if you're planning to implement a new code, or if you're trying to refactor your existing code to be much better, choose choose the right algorithm and that will is very going to be dependent on what your specific application is.",
                    "label": 0
                },
                {
                    "sent": "And then just along with that, once you have well the other side of that is that you do have to think about these things from the beginning and plan them, and you can certainly go in and optimize your existing code.",
                    "label": 0
                },
                {
                    "sent": "But the biggest speedups will be either sort of starting from scratch or as Eric taught us today.",
                    "label": 0
                },
                {
                    "sent": "You don't have to start from scratch, but you do have to go in and re engineer your code pretty seriously if you want massive speedups in performance.",
                    "label": 0
                },
                {
                    "sent": "So with open MP just adding a few directives here and there, you might get up to 16 cores, 16 processors on a serial code, but you're going to have to do more work to really get things to scale up to hundreds and thousands of processors, so just setting expectations on that.",
                    "label": 1
                },
                {
                    "sent": "And as you do this, you may actually may help reveal the need for new algorithms or much better implementation.",
                    "label": 1
                },
                {
                    "sent": "So the focus of this discussion and what the hands-on session will be later, will really be how you can use the tools that are available to assess the parallel parallel performance of your codes and identify the areas where they're not performing as you would expect.",
                    "label": 0
                },
                {
                    "sent": "And so OK.",
                    "label": 0
                },
                {
                    "sent": "So we look at this sort of line here, choosing the algorithm implementing that on the hardware you have, and then analyzing the performance and then finding out what's wrong, doing some optimization and then maybe repeating this process over and over.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is shown here that as you go through this analysis process, then you have to prepare your code somehow.",
                    "label": 0
                },
                {
                    "sent": "Usually, except in certain cases I'll mention you're going to sometimes insert something into your code to do the measurement, and then you actually measure the data you collect data about the performance, aggregate it, and then you have some sort of analysis tool that will allow you to calculate metrics, identify performance problems, present the results so that you can see them, and understand them, and then identify hopefully your area that you can optimize and go ahead and.",
                    "label": 0
                },
                {
                    "sent": "And apply those optimizations and then repeat that process until you're happy with the performance of your code.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to set the stage for some of this discussion I want to define some terms and give a little background when we start talking about performance analysis and especially the performance tools that we'll be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Certain things come up repeatedly.",
                    "label": 0
                },
                {
                    "sent": "One is hardware counters.",
                    "label": 0
                },
                {
                    "sent": "Who knows what a hardware counter is.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe 20 or 30% people so the counters are a set of registers that are sit there in the processor on the processor and they count events that are happening on that processor so they can count.",
                    "label": 0
                },
                {
                    "sent": "For example floating number of floating point operations that have happened, or a certain number the number of cycles that have passed and so.",
                    "label": 0
                },
                {
                    "sent": "So these are special hardware registers that count these events and different processors have different registers that they support.",
                    "label": 0
                },
                {
                    "sent": "So for example, an Opteron Istanbul processor processor has six different registers on it, so that means you can count 6 different types of events simultaneously on that that hardware.",
                    "label": 0
                },
                {
                    "sent": "So you can't.",
                    "label": 0
                },
                {
                    "sent": "It's not unlimited.",
                    "label": 0
                },
                {
                    "sent": "What you can count, and different processors allow you to count different things, although many of them allow sort of standard certain standard things, because every processors different group at University of Tennessee, I believe, amid possibly other collaborators develop something called the performance API or PAPI.",
                    "label": 0
                },
                {
                    "sent": "And this was an effort to create a standard API for accessing these hardware counters across all the different architectures that are available, and so will.",
                    "label": 1
                },
                {
                    "sent": "So they have this API and the nice thing about it is it's portable.",
                    "label": 0
                },
                {
                    "sent": "You can use it across all these different architectures without having to get into the architecture specific stuff every time you want to count the performance performance counters.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have these predefined standard events that they have created.",
                    "label": 1
                },
                {
                    "sent": "The standard way of accessing and when you install PAPI on your system then they have this command papi avail, which will list all of the hardware counters that are available on that on that particular architecture.",
                    "label": 0
                },
                {
                    "sent": "That processor that you're running on that that you're using on that system so, so this is sort of high level standard interface.",
                    "label": 0
                },
                {
                    "sent": "They also have a low level interface that exposes the native counters on that system.",
                    "label": 0
                },
                {
                    "sent": "The one the actually.",
                    "label": 0
                },
                {
                    "sent": "The architecture specific counters that are that are defined for that particular processor.",
                    "label": 0
                },
                {
                    "sent": "If you want to get more.",
                    "label": 0
                },
                {
                    "sent": "If you want to get more detail about or you want to use these native things because they are more specific to what you're doing, this is probably not a level you have to worry about too much, with the caveat that I'll mention in a little bit and then so third party tools for doing performance analysis utilized these extensively, so we'll be talking bout towen.",
                    "label": 1
                },
                {
                    "sent": "During this presentation and Chris will talk about Scholastica and score P. And, but there's many, many tools that utilize these things, and as of this version of the kernel there's actually direct support for these hardware counters in in Linux.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so another part of the setup is measurement techniques and what we mean by certain terms.",
                    "label": 1
                },
                {
                    "sent": "So so one question is in terms of how you analyze performance of these applications is when do you measure performance?",
                    "label": 1
                },
                {
                    "sent": "How is that measurement triggered?",
                    "label": 0
                },
                {
                    "sent": "So one way to do it is through sampling and so sampling and sampling.",
                    "label": 0
                },
                {
                    "sent": "You don't have to modify your code at all.",
                    "label": 0
                },
                {
                    "sent": "You don't have to put any new stuff into your code, so that's really nice and it periodically it will interrupt the execution of the code and see where it is.",
                    "label": 0
                },
                {
                    "sent": "In its execution, and so you can start to see where the code is spending time by these periodic interrupts.",
                    "label": 0
                },
                {
                    "sent": "So it's a sampling statistical based measurement of what's going on in your code.",
                    "label": 0
                },
                {
                    "sent": "The other is through direct instrumentation, so actually putting brackets around your code in certain regions and saying OK from here to here measure how many times this happens, and so that has both of these have certain benefits and drawbacks that will mention.",
                    "label": 0
                },
                {
                    "sent": "And then how do we actually?",
                    "label": 0
                },
                {
                    "sent": "So that's when the measurements triggered.",
                    "label": 0
                },
                {
                    "sent": "How do we actually record the data?",
                    "label": 0
                },
                {
                    "sent": "One is just through a summary or a profile where you collect all the data and then you average it out and you get one average number over the timeframe that you were analyzed that you are measuring the performance.",
                    "label": 0
                },
                {
                    "sent": "And the other is tracing, which actually gets a time resolved view of the performance.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can see how the performance changes over the course of time and different look at events that are happening over a certain course of time within your code and will Christian will talk about more about that later as well so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally, I think for our for our little vocabulary lesson we have, we talk about doing xclusive measurements and inclusive measurements.",
                    "label": 0
                },
                {
                    "sent": "Right now this is kind of important and it's easy to forget which is which.",
                    "label": 0
                },
                {
                    "sent": "But for if we want to exclusive measurement, then we're looking at only the the the, the events that happen within a given function, for example so or within a given region within a function so.",
                    "label": 0
                },
                {
                    "sent": "In this example here we are looking at this function Foo and it calls another function bar.",
                    "label": 0
                },
                {
                    "sent": "But if we're looking at the exclusive duration of this function and the things that are happening within this exclusive measurement, then we're only looking at the explicit events that are happening within this first level of this function, and we're not looking at any of the events that are happening in this child function that it calls OK, and usually that's what we're most interested in, but there's.",
                    "label": 0
                },
                {
                    "sent": "But then you can also look at the inclusive duration of Foo, which includes everything that happens within Foo itself, along with all the sub functions that are called within anything that goes below this.",
                    "label": 0
                },
                {
                    "sent": "Everything else in the call path there.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that is yes, that's the end of vocabulary lesson.",
                    "label": 0
                },
                {
                    "sent": "How are we doing?",
                    "label": 0
                },
                {
                    "sent": "Doing alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so the so that these are here for your.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reference and we can go back and talk about those and answer questions more if if needed so.",
                    "label": 0
                },
                {
                    "sent": "So the way I want to go through this then is kind of is telling a story.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I do as working at HPC Center is people come to us and say our code is bad.",
                    "label": 0
                },
                {
                    "sent": "It doesn't perform well.",
                    "label": 0
                },
                {
                    "sent": "Please help us to fix it, and so this story I'm going to tell us from one experience that I had helping a group to understand the performance of their code and to fix the problems.",
                    "label": 0
                },
                {
                    "sent": "And as you might expect, the story ends happily, so we will see at the end.",
                    "label": 0
                },
                {
                    "sent": "So this particular code was one that was developed, and I think it's still being developed at Cornell.",
                    "label": 0
                },
                {
                    "sent": "In Harold Chirag's group there, this is a code for doing coarse grained simulation used to protect protein folding pathways and so it's called unres is the name of the codes will be referring to that often this code.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just had a master worker arrangement and so you have a master process, a master rank that would have a certain call path and then all of the worker ranks would then have a different call path so it's a different path to path.",
                    "label": 0
                },
                {
                    "sent": "Is the master of the worker.",
                    "label": 0
                },
                {
                    "sent": "It also in terms of measurement, measuring performance and doing performance analysis?",
                    "label": 0
                },
                {
                    "sent": "One thing that is significant is you want to get to the core of your application.",
                    "label": 0
                },
                {
                    "sent": "The thing that's going to be repeated over and over again.",
                    "label": 0
                },
                {
                    "sent": "You're not so worried about the startup time, but you also don't always want to run it for hours and hours to get results, and so there's significant setup time in this code is like 5 minutes just to get past the startup, and so that was one thing you have to work around when analyzing these codes so we're only interested in the really the kernel part of it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just just a couple of things that were things to think about as we got started looking at this code.",
                    "label": 0
                },
                {
                    "sent": "This is the procedure that will be going through and as we talk about this procedure, I'll be using.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned this this unres example to highlight some of the ideas here and also I'll be showing how we tackle this with Tau now.",
                    "label": 0
                },
                {
                    "sent": "I don't want to get too hung up on the details of the Tau stuff in this presentation.",
                    "label": 0
                },
                {
                    "sent": "Just focus on the sort of the main ideas, the principles, but there will be slides in here that will show you some recipes if you're interested in trying to use Tau for this kind of thing, then there will be some reference slides in there you can go back to and try it, but then in the hands on you'll be will be using score P to illustrate this as well, so you'll get exposure to different ways of doing this through different tools.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is the process we go through and we can look at optimizing the serial.",
                    "label": 0
                },
                {
                    "sent": "The single core performance of of our application and also looking at optimizing the parallel performance of the application.",
                    "label": 0
                },
                {
                    "sent": "So in both both cases the first step is to assess the performance where it's at and then also then to identify where the code is spending most of its time, and then if you really want to start digging in then you can do some instrumentation of those functions.",
                    "label": 0
                },
                {
                    "sent": "As we mentioned, introducing these function calls that will.",
                    "label": 0
                },
                {
                    "sent": "Where you can start to isolate areas and see how much what's happening within those areas of the of the code, and then we'll actually measure the code performance.",
                    "label": 0
                },
                {
                    "sent": "If we're doing this, this processor or single processor performance, then we'll look at hardware counters to help us understand the code performance and identify inefficient regions of the source code and what was causing those inefficiencies in the parallel side.",
                    "label": 1
                },
                {
                    "sent": "We start looking at load balance issues looking at regions we didn't think we had to paralyze.",
                    "label": 0
                },
                {
                    "sent": "But when we start to scale it up.",
                    "label": 0
                },
                {
                    "sent": "They turn out to be bottlenecks and other communication bottlenecks that happen.",
                    "label": 0
                },
                {
                    "sent": "If we're using MPI, and tracing can really help with that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identifying those OK, so for now we're going to start number one.",
                    "label": 0
                },
                {
                    "sent": "Just assessing what is.",
                    "label": 0
                },
                {
                    "sent": "What is the performance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, first of all, what is it mean for our code to perform poorly?",
                    "label": 1
                },
                {
                    "sent": "How do we know if it's performing well or not?",
                    "label": 0
                },
                {
                    "sent": "Pause for effect.",
                    "label": 0
                },
                {
                    "sent": "Anyone want?",
                    "label": 0
                },
                {
                    "sent": "Offer up a guess or an answer.",
                    "label": 0
                },
                {
                    "sent": "How do you know if you're getting performance good, good performance or bad performance?",
                    "label": 0
                },
                {
                    "sent": "Sorry what.",
                    "label": 0
                },
                {
                    "sent": "Compare with other codes.",
                    "label": 0
                },
                {
                    "sent": "Maybe that they would say so.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know other codes are doing well and you want to see how it compares, OK?",
                    "label": 0
                },
                {
                    "sent": "Right, so parallel efficiency.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you mean you can.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's an ideal for parallel efficiency.",
                    "label": 0
                },
                {
                    "sent": "There's an ideal right scaling, literally.",
                    "label": 0
                },
                {
                    "sent": "Any other thoughts?",
                    "label": 0
                },
                {
                    "sent": "Performance modeling.",
                    "label": 0
                },
                {
                    "sent": "OK, there's magic word so so you have to understand what's your theoretical best you can do with the algorithm that you're using, right?",
                    "label": 0
                },
                {
                    "sent": "So what is the theoretical best?",
                    "label": 1
                },
                {
                    "sent": "So it depends on the work that's being done.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, we do look at sort of what is the percentage of peak performance that you can get on a particular architecture.",
                    "label": 0
                },
                {
                    "sent": "You might know that.",
                    "label": 0
                },
                {
                    "sent": "Now most of us are using floating point codes in general in scientific applications, that is generally true, although more and more there's data intensive things that that's not necessarily true, so that's interesting territory to explore.",
                    "label": 1
                },
                {
                    "sent": "But we won't talk about that as much today, but we look at peak performance.",
                    "label": 0
                },
                {
                    "sent": "What percentage performance should I expect with my algorithm so Christian actually just tipped me off to the existence of this, and it's probably been along time and I.",
                    "label": 0
                },
                {
                    "sent": "This is not something that that I've explored in great detail, but there are people are formalizing ways to establish performance bounds for various numerical methods, and so one way of doing this with these roofline models is to look at it in terms of aermet arithmetic intensity of your algorithm.",
                    "label": 1
                },
                {
                    "sent": "So, and that is how many what's the ratio of the floating point operations to the total data movement in your code.",
                    "label": 0
                },
                {
                    "sent": "So what you're either getting work done or you're moving stuff around to get work done, and so the ratio of that is the arithmetic intensity.",
                    "label": 0
                },
                {
                    "sent": "And so they've got a nice on this on this webpage.",
                    "label": 0
                },
                {
                    "sent": "Here you go, check it out.",
                    "label": 0
                },
                {
                    "sent": "They've got a nice scale here that shows different types of algorithms and where they fall in this arithmetic intensity line.",
                    "label": 0
                },
                {
                    "sent": "And so if you get down to these things like the dense linear algebra, these are because they're doing very dense floating point operations.",
                    "label": 0
                },
                {
                    "sent": "You can get very close to the maximum theoretical floating point operation performance of a given processor for these linear algebra routines, so you might get up to 80 or 90% of the theoretical Max.",
                    "label": 0
                },
                {
                    "sent": "With some of these are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means but.",
                    "label": 0
                },
                {
                    "sent": "For most, for most scientific applications, for if you take on average in that sort of average them all out.",
                    "label": 0
                },
                {
                    "sent": "For floating point codes, getting 20% of peak is not bad.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you if you're fully point bound, then the best you could do would be 5 times better than that, right?",
                    "label": 0
                },
                {
                    "sent": "And so you're kind of doing OK. And then you know.",
                    "label": 0
                },
                {
                    "sent": "So there's been sort of a rule of thumb or a quick check on your floating point code that may not.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe terrable you know if you're doing dense linear algebra, you could do five times better.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or nearly but from an average, you know, sort of rule of thumb, check you can see and then you should.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice something like this if you really want to understand what is your up.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Limit on your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So floating point operation or Flop that's flew point operations per second peak.",
                    "label": 0
                },
                {
                    "sent": "So you can also look at floating point operations per cycle and that and then multiply that by the speed of the processor.",
                    "label": 0
                },
                {
                    "sent": "The cycles per second.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to giga Hertz and see how close are getting and then you just want to think.",
                    "label": 0
                },
                {
                    "sent": "Well if I'm already at 50% floating point fraction of peak performance, do I really need to look at optimizing further my serial performance?",
                    "label": 0
                },
                {
                    "sent": "The best I could do would be 2X and so you get you get the 8020 rule here of the.",
                    "label": 0
                },
                {
                    "sent": "You'll be able to optimize most of the performance.",
                    "label": 0
                },
                {
                    "sent": "Generally 20% of the effort you get to 80% of your potential performance.",
                    "label": 0
                },
                {
                    "sent": "You know, just kind of rule of thumb, so you want to think about not investing too much effort for too little gain when you start to look at this from the beginning.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't have to do anything and your code is doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "And then looking at parallel performance, we talk about scalability and we talk about strong scalability and weak scalability.",
                    "label": 0
                },
                {
                    "sent": "Strong scalability is with a given problem size.",
                    "label": 0
                },
                {
                    "sent": "If I scale up my code to two times, the number of cores, does the time go down by a factor of two as well?",
                    "label": 0
                },
                {
                    "sent": "That's perfect.",
                    "label": 0
                },
                {
                    "sent": "Strong scaling, weak scaling, which is also important for many applications, is if I increase the amount of work by two and the number of processors by two.",
                    "label": 0
                },
                {
                    "sent": "Does the execution time remain the same and that would be perfect weak scalability?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so in this process of checking out OK, how is my code doing?",
                    "label": 0
                },
                {
                    "sent": "I mentioned that sometimes you have to modify your code to use these performance tools, but with sampling as you mentioned we talked about earlier with sampling you actually don't have to.",
                    "label": 0
                },
                {
                    "sent": "You don't have to modify your code.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned it will just you can execute your code as normal and it will interrupt execution.",
                    "label": 0
                },
                {
                    "sent": "It will sample where it's at and this can be done with very low overhead and it can be done very without very much investment on your part so.",
                    "label": 0
                },
                {
                    "sent": "I recommend if you're just going to check to see what code is at 2 to do, use one of these sampling tools.",
                    "label": 0
                },
                {
                    "sent": "A very good one is this HPC toolkit.",
                    "label": 0
                },
                {
                    "sent": "It can give you function level information about how your code is performing, and TAC Texas Advanced Computing Center is developed.",
                    "label": 0
                },
                {
                    "sent": "A sort of a modified HPC toolkit to give you some more automated performance analysis about about your code, and that's worth checking out as well.",
                    "label": 0
                },
                {
                    "sent": "So there's some links there.",
                    "label": 0
                },
                {
                    "sent": "We won't go into that in detail today, but just sort of recommendation for a procedure of with your own code.",
                    "label": 0
                },
                {
                    "sent": "You might want to try a sampling tool.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a quick dirty look at it, and this is unres.",
                    "label": 0
                },
                {
                    "sent": "This is what we did there with Unres.",
                    "label": 0
                },
                {
                    "sent": "We were looking.",
                    "label": 0
                },
                {
                    "sent": "We used one of these sampling tools just to look at how many floating point operations were getting.",
                    "label": 0
                },
                {
                    "sent": "What percentage of peak this was on an Itanium processor, so that tells you this was a little while ago and the theoretical Max was for floating point operations per cycle.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we weren't doing very well.",
                    "label": 0
                },
                {
                    "sent": "We're getting about 15% of peak performance.",
                    "label": 0
                },
                {
                    "sent": "It it needed some optimization on Itanium.",
                    "label": 0
                },
                {
                    "sent": "It actually did much better on the X86 which had a theoretical peak of two floating point operations per cycle and we were getting 3%, three, 3% of peak on that so it wasn't terrible but there.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "IP room for improvement there.",
                    "label": 0
                },
                {
                    "sent": "Looking at parallel performance, this was a scaling.",
                    "label": 0
                },
                {
                    "sent": "This was our old computer at PSC called Big Ben and as we go through this is the ideal scaling and we can see that scaling starts to drop off pretty quickly here so this is.",
                    "label": 0
                },
                {
                    "sent": "Time steps per second so so we want to go up here and it's starting to level off.",
                    "label": 0
                },
                {
                    "sent": "And so as we get to like 32 cores, it really starts to flatline.",
                    "label": 0
                },
                {
                    "sent": "So we're not doing too well there.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've identified for unrest code how it's doing is single core performance and a parallel performance.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to see where it spends the most time?",
                    "label": 0
                },
                {
                    "sent": "Usually a handful of functions account for 90% of the execution time.",
                    "label": 0
                },
                {
                    "sent": "You can very quickly to 90% and those are the ones you want to focus on.",
                    "label": 0
                },
                {
                    "sent": "Just make sure as you're doing analysis, you're looking at the things that are actually contributing to the main kernel of your code, the production part and not the startup functions or anything like that.",
                    "label": 0
                },
                {
                    "sent": "This is especially important.",
                    "label": 0
                },
                {
                    "sent": "It's not your code and you're looking at someone elses.",
                    "label": 0
                },
                {
                    "sent": "If it's your code, you probably know and don't have to worry about that so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And then you know, just as you're if you're looking at parallel applications, you want to look at these at scale as you scale up.",
                    "label": 0
                },
                {
                    "sent": "Because functions that are insignificant when you're running on a single processor may become a bottleneck as you scale.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is just an example of one of these sampling tools that kind of output.",
                    "label": 0
                },
                {
                    "sent": "You can get.",
                    "label": 0
                },
                {
                    "sent": "This shows the function that shows how much percentage of time was spent in that function.",
                    "label": 0
                },
                {
                    "sent": "And here's the running sum, the running total and so here you get to these first 3 here, and you've got to 90% of the total time of the code, and so you just look.",
                    "label": 1
                },
                {
                    "sent": "And really, it's just this one right that you can start with focusing on 75% of the code is there.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what we saw with unres 50% of the time was in this this elec function here and there.",
                    "label": 0
                },
                {
                    "sent": "You see the setup time which you know we only ran a short run so it looks like it's taking up a lot of time in reality that will go down to zero as you run the code over several hours.",
                    "label": 0
                },
                {
                    "sent": "But with these sampling tools you actually can do that because there's so little overhead you can just apply the sampling tool to your code and then just let it run in a production setting and then you can collect some nice statistics and see OK. How is it doing you have to worry about startup time or anything else 'cause you've been running for a few days.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, but once you look take a look at these codes and you say OK, there's a performance problem that I want to address.",
                    "label": 0
                },
                {
                    "sent": "Either the serial, the parallel performance then.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want to take a look at instrumenting these functions, so you're going to dig deeper.",
                    "label": 0
                },
                {
                    "sent": "Insert insert functions into your source code.",
                    "label": 0
                },
                {
                    "sent": "That will tell it to measure certain parts of the code.",
                    "label": 0
                },
                {
                    "sent": "The advantages it gives you very precise information about where things happen.",
                    "label": 0
                },
                {
                    "sent": "If it's not in that bracket of that function that you're instrumenting your code with, and then it's not going to get measured, so you know you're measuring exactly where things are happening there.",
                    "label": 0
                },
                {
                    "sent": "The disadvantage is that putting extra functions into your code, especially high performance, part of your code that you want to formerly well.",
                    "label": 0
                },
                {
                    "sent": "That's going to affect the performance, and so there's some overhead there and you need to be wary of and so you want to really only instrument.",
                    "label": 0
                },
                {
                    "sent": "Not every function in your code, but only those ones that maybe through doing some sampling experiment that you determined are really important to performance.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you need to choose a tool.",
                    "label": 0
                },
                {
                    "sent": "There's a nice list here that I think Christian will also show you later that the VI HPS.",
                    "label": 0
                },
                {
                    "sent": "Maintains that gives you some guidance on how to choose a tool for this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be using towels.",
                    "label": 0
                },
                {
                    "sent": "An example in this presentation, but as I mentioned, let's will focus on the general principles here of this process and not the details of Tau, but there will be reference slides that show you some recipes for how to use it, and then Christian will take you through specific examples of this using the Scorpions klaske tools during the hands-on sessions.",
                    "label": 0
                },
                {
                    "sent": "After I get done talking.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Tao it can do a lot of amazing very good stuff.",
                    "label": 0
                },
                {
                    "sent": "For performance analysis you can get loop level information routine level.",
                    "label": 0
                },
                {
                    "sent": "You can look at communication and performance counters.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned it supports all languages and all platforms.",
                    "label": 0
                },
                {
                    "sent": "Basically they work hard to make it available everywhere and this is developed at University of Oregon.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is how you would instrument if you were using tabs is how you instrument your code with it.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not going to go into the details, but it has.",
                    "label": 0
                },
                {
                    "sent": "It does automatic instrumentation you give it some some information, some options you run a special script that comes with Tao and it will instrument your code for you so you don't have to buy hand put in these functions to measure performance.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is just another again recipe here that you're going to instrument your code and then and then run it as normal after you.",
                    "label": 0
                },
                {
                    "sent": "After Tao instruments it and then you'll get some output from Tao and you'll use a tool called Para Prof to analyze the results and we are going to see Para Prof. Later because paragraphs.",
                    "label": 0
                },
                {
                    "sent": "Interesting because it doesn't just work with how it works with scorpis that Christian is going to show you and it works with a bunch of other image works with HPC toolkit and a bunch of other performance tools.",
                    "label": 0
                },
                {
                    "sent": "Can you can analyze the output using this pair of Prof analysis tool.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this as we instrument our code, this is one of the classic things that happens with this high overhead.",
                    "label": 0
                },
                {
                    "sent": "You have to be especially aware of tiny functions that get called many, many, many times and so this is what before Tau instrumented this particular unrest code.",
                    "label": 0
                },
                {
                    "sent": "There was a function that looked like this and then after it looks like this, so in this tiny little function you now got all these calls to the Tau profiler.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to make your execution time blow up.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what we saw.",
                    "label": 0
                },
                {
                    "sent": "We took a step.",
                    "label": 0
                },
                {
                    "sent": "We took a reference measurement without any instrumentation, and it took 51.4 seconds and then we measured it.",
                    "label": 0
                },
                {
                    "sent": "After we did the instrumentation without and it took 315 seconds.",
                    "label": 0
                },
                {
                    "sent": "So that's not good.",
                    "label": 0
                },
                {
                    "sent": "So Tao and other tools will allow you to selectively instrument.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Certain functions.",
                    "label": 0
                },
                {
                    "sent": "And in fact, what Tao does it after you run a test run, it generates a list of all these bad functions that are small, and that really shouldn't be instrumented, and so then you can create a selective instrumentation file that towel use at 2X clue.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those from its measurements.",
                    "label": 0
                },
                {
                    "sent": "And so that's what this is.",
                    "label": 0
                },
                {
                    "sent": "So this is how it's done in Tau.",
                    "label": 0
                },
                {
                    "sent": "But Scorpis can also selectively instrument your code, and others do the similar things.",
                    "label": 0
                },
                {
                    "sent": "In addition, something that's very nice that you can do with these tools is so it will automatically help you find or, or you can either automatically or by your own observation you can find which routines to include or exclude.",
                    "label": 0
                },
                {
                    "sent": "But you can also define regions of the code, so not just functions but whole regions.",
                    "label": 0
                },
                {
                    "sent": "That you want to analyze.",
                    "label": 1
                },
                {
                    "sent": "So for example, in molecular dynamics, all the really intensive stuff happens in this one main MD loop, right?",
                    "label": 0
                },
                {
                    "sent": "So everything goes, you loop through the time steps and so in this case of unrest, that's where we're really interested in, and so I can define that as a region sort of my production and like the dynamics part of the code.",
                    "label": 0
                },
                {
                    "sent": "And then I only look at the performance information coming from that region.",
                    "label": 0
                },
                {
                    "sent": "So that can be very.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Helpful.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "These tools can also give you and the output in the region that you define in the area that you you are studying.",
                    "label": 0
                },
                {
                    "sent": "All the call path information you can define that sometimes that might increase overhead depending on the tool.",
                    "label": 0
                },
                {
                    "sent": "Christian tells me that's not the case with Scorpis.",
                    "label": 0
                },
                {
                    "sent": "That can be the case with Tao.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this just shows you how to get that information in towel.",
                    "label": 0
                },
                {
                    "sent": "Once you open it up.",
                    "label": 0
                },
                {
                    "sent": "This is the parrot by the way.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention this is with a pair of Prof tool that will be talking about later on.",
                    "label": 0
                },
                {
                    "sent": "They'll demonstrate probably tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To analyze these profiles.",
                    "label": 0
                },
                {
                    "sent": "So this is just what I talked about.",
                    "label": 0
                },
                {
                    "sent": "You can isolate different regions of code and just look at the measurements of performance in that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Region of interest.",
                    "label": 0
                },
                {
                    "sent": "So with Unres, if we didn't do that and we ran this short experiment, then the setup time totally dominated and this elect function that we're actually interested in.",
                    "label": 0
                },
                {
                    "sent": "It looks like it's only a small.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of the the total.",
                    "label": 0
                },
                {
                    "sent": "But then if we define just the MD loop as our area of interest, then you can see we see what's really important here.",
                    "label": 0
                },
                {
                    "sent": "So the elect function is what takes up most of the time.",
                    "label": 0
                },
                {
                    "sent": "So now we've been able to identify that in Tau this is a pair of Prof. Screenshot.",
                    "label": 0
                },
                {
                    "sent": "And so we found these are the functions that are important in the main kernel.",
                    "label": 0
                },
                {
                    "sent": "Main important part of our code.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we found those important functions instrumented them, so now we can measure the performance of those functions using these hardware counters they talked about.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you could install Papi.",
                    "label": 0
                },
                {
                    "sent": "You can look at the different counters that are available using the Papi avail command that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Then you can run tower or whatever code you're actually weather tool you're actually running and feed it a list.",
                    "label": 0
                },
                {
                    "sent": "So this is how happy these are.",
                    "label": 0
                },
                {
                    "sent": "Some of the names of these PAPI events so this, so these are the events that are defined.",
                    "label": 0
                },
                {
                    "sent": "So this is floating point operations for total floating point operations.",
                    "label": 0
                },
                {
                    "sent": "This is 1 PAPI event that set this is the total number of cycles so that from this you can get.",
                    "label": 0
                },
                {
                    "sent": "And this is a measurement of the time that's elapsed.",
                    "label": 0
                },
                {
                    "sent": "So from this you could get full input operations per second in different regions of your code, just a just a quick warning that.",
                    "label": 0
                },
                {
                    "sent": "When you really get down to looking at the performance closely using these performance counters, then you do want to be a little bit careful that you understand what those counters mean.",
                    "label": 0
                },
                {
                    "sent": "Papi has these preset definitions, but on certain architectures they don't always mean what you what you think they might mean.",
                    "label": 0
                },
                {
                    "sent": "So you can look in.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a little bit tomorrow, maybe how you can sort of look and verify that it's doing what you think it's doing, but some on some platforms the accuracy of accuracy of the counters may vary.",
                    "label": 0
                },
                {
                    "sent": "Some may be better.",
                    "label": 0
                },
                {
                    "sent": "Summit may overcount.",
                    "label": 0
                },
                {
                    "sent": "A little bit so you have to be a little bit wary there and on some platforms they don't exist at all.",
                    "label": 0
                },
                {
                    "sent": "So for example, an interesting fact is that the latest the current generation of Intel processors like Haswell processors, you can't measure the floating point operations number floating point operations, the hardware registers there don't support accurate measurement of the floating point operations, so we have to wait till Broadwell and then that's supposed to be fixed.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this just shows how impaired profile.",
                    "label": 0
                },
                {
                    "sent": "So this more tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to probably skip this, but you can get a list of these counters here.",
                    "label": 0
                },
                {
                    "sent": "Impera Prof and you can start to make metrics out of these that you're interested in floating point operations per second.",
                    "label": 0
                },
                {
                    "sent": "Cat total number of cache misses or cache misses per cycle or that sort of thing.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so coming back to our unres example.",
                    "label": 0
                },
                {
                    "sent": "So this we know that this elec function was the most important function where most of the time was spent, and so we're looking at what is.",
                    "label": 0
                },
                {
                    "sent": "What are the floating point operations per cycle in that ILEC function, so it's not in the top of the list, is not doing the most efficient, but it's doing .6 floating point operations per cycle.",
                    "label": 0
                },
                {
                    "sent": "The peak per cycle on this particular processor was two, so it's not doing too badly.",
                    "label": 0
                },
                {
                    "sent": "It's getting about 30% of the total floating point operations that it could possibly do per.",
                    "label": 0
                },
                {
                    "sent": "For cycle and so not too bad 30% of peak performance.",
                    "label": 0
                },
                {
                    "sent": "Floating point performance in this particular function that we're.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we might want to.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prove that still say it was really bad.",
                    "label": 0
                },
                {
                    "sent": "Then you'd want to look deep more deeply at why aren't you getting good floating point performance and that that involves getting the data in the cache is so that you can access so that the processor is kept busy.",
                    "label": 0
                },
                {
                    "sent": "So if you want to minimize the amount of data movement in these codes, data movement is very expensive floating point operations or cheap, and so you want to.",
                    "label": 0
                },
                {
                    "sent": "You want to look into that.",
                    "label": 0
                },
                {
                    "sent": "In this case we just where you want to really want to start, though is just.",
                    "label": 0
                },
                {
                    "sent": "Doing compiler optimizations, so in this case, as we mentioned, there was lots of little functions and the compiler was not inlining those functions.",
                    "label": 0
                },
                {
                    "sent": "So after we forced the compiler to inline the functions, now we got a boost already from 30% to 40% of the peak performance.",
                    "label": 0
                },
                {
                    "sent": "And that's actually a fairly significant.",
                    "label": 0
                },
                {
                    "sent": "So that's a 30% boost in our or no, it's not, not quite.",
                    "label": 0
                },
                {
                    "sent": "Yeah, about 30% boost in our performance there, so that's really nice.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so we're not going to go into the detail.",
                    "label": 0
                },
                {
                    "sent": "We don't have time during this very brief time.",
                    "label": 0
                },
                {
                    "sent": "We have together in all the things you can do to optimize your serial performance on a core, but there's lots of resources out there where you can find more information at once.",
                    "label": 0
                },
                {
                    "sent": "One of these tools point you to the problem spots you can look at different options for your particular implementation and algorithm.",
                    "label": 0
                },
                {
                    "sent": "How you go about optimizing.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so now we were happy with or at least we've identified or addressed some of the serial performance and that's that.",
                    "label": 0
                },
                {
                    "sent": "Is 1 important.",
                    "label": 0
                },
                {
                    "sent": "Point is that before you invest too much effort in optimizing the scalability of your code, you do want to make sure that you have reasonable single core performance, because a great way to artificially have very highly scalable code is to have terrible single core performance, and then it takes so long for you to single core work that it hides all the bad.",
                    "label": 0
                },
                {
                    "sent": "You know all the slow communication.",
                    "label": 0
                },
                {
                    "sent": "And that sort of thing so it can look a lot better than it is.",
                    "label": 0
                },
                {
                    "sent": "So if you at least have a reasonable optimization of your serial, your single core performance first, then that's a good time to really invest in optimizing your parallel performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "So you want so so like I mentioned, one of the things you want to look for our load imbalance.",
                    "label": 0
                },
                {
                    "sent": "You want to look for regions of the code that you didn't think you had to parallelize that now have become important and are now bottle next to your here code.",
                    "label": 0
                },
                {
                    "sent": "So after doing this measurement with Tao.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a recipe for how you can use Para Prof to look at this.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through this.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned that's there for your reference.",
                    "label": 0
                },
                {
                    "sent": "If you ever want to explore this and get in.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice plot like this.",
                    "label": 0
                },
                {
                    "sent": "This was a screenshot from from paragraph and this is.",
                    "label": 0
                },
                {
                    "sent": "Looking at the overall scaling of the MD part of the unrest code so there's this Phase MD just this MD loop OK and so every time we were going from 2 to 32 cores, each time doubling the number of cores so it should be going down by 50% every time and overall the code up to 32 cores doesn't do too badly there, although it starts to drop off here as we go up to 32 cores, but now we can look at that on a per function basis, so this is kind of cool because you can see a per function scaling of your parallel scaling.",
                    "label": 0
                },
                {
                    "sent": "Of your application, so which functions are scaling well and which ones are starting to become a bottleneck?",
                    "label": 0
                },
                {
                    "sent": "This elec is doing pretty pretty well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's dropping down each time.",
                    "label": 0
                },
                {
                    "sent": "Not too bad, but you see that there's this function here, know right here that was only taking 1.4 seconds on on two cores.",
                    "label": 0
                },
                {
                    "sent": "Which out of 68 isn't that much on two cores, but once you get down to just six seconds on 32 cores?",
                    "label": 0
                },
                {
                    "sent": "Well, 1.4 seconds, that starts to be actually a bottleneck, so that becomes a problem so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this serial function will begin to dominate the runtime.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So that's that's finding a serial function.",
                    "label": 1
                },
                {
                    "sent": "Look at the scaling.",
                    "label": 0
                },
                {
                    "sent": "This is another recipe for looking at load imbalance using Para Prof.",
                    "label": 1
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can generate something like this.",
                    "label": 0
                },
                {
                    "sent": "And so here we can see that there's quite a bit of imbalance, so this is so.",
                    "label": 0
                },
                {
                    "sent": "This pair of problem we explain just a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is showing these are all the processes, so process zero to 15.",
                    "label": 0
                },
                {
                    "sent": "So this is on 16 processes, 16 MPI processes.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're only looking at the time in this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the loop an receive, there's multiple functions here that they have imbalance.",
                    "label": 1
                },
                {
                    "sent": "And also here's this serial function we saw earlier, right?",
                    "label": 0
                },
                {
                    "sent": "There's no, it's not only running on process 0, and that's everything else will be waiting on that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's the.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one OK, so just a little anecdote here.",
                    "label": 0
                },
                {
                    "sent": "The so the developers here were actually surprised by this that they thought that their algorithm should have very good load balancing in parallel, and that was not the case.",
                    "label": 0
                },
                {
                    "sent": "So they went back to the drawing board and what they did was they actually decided that there was a better algorithm.",
                    "label": 0
                },
                {
                    "sent": "So like like we set the very beginning, the best way is to know if it.",
                    "label": 0
                },
                {
                    "sent": "The best way, if it's if it's a reasonable thing to do, which was in this case, is to maybe adjust your algorithm, and in that case they found an algorithm that not only should give better load balance, but would perform fewer floating point operations total, so they cut the number of floating point operations.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In half and this is the result.",
                    "label": 0
                },
                {
                    "sent": "So this is how it looked at the end.",
                    "label": 0
                },
                {
                    "sent": "Now we've got very good load balance across these different functions.",
                    "label": 1
                },
                {
                    "sent": "They've gotten rid of the we got rid of the serial bottleneck there.",
                    "label": 1
                },
                {
                    "sent": "Nope, we didn't know.",
                    "label": 0
                },
                {
                    "sent": "Yes, we did.",
                    "label": 0
                },
                {
                    "sent": "That's the mean.",
                    "label": 0
                },
                {
                    "sent": "This is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so we got rid of the serial.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottleneck there, it went four times faster after this because they were getting like 40% of peak performance.",
                    "label": 0
                },
                {
                    "sent": "They cut out half of their floating point operations and so that translates to four times the performance.",
                    "label": 0
                },
                {
                    "sent": "So the best thing to do is to get rid of those flops floating point operations if you can, and that's what they did.",
                    "label": 0
                },
                {
                    "sent": "And also they improved the scaling, although it also became more difficult to scale because the code is more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then you can do around 2:00 and you can look to see OK's or other sources of imbalance that looks like it's idling on this mpiana call too.",
                    "label": 0
                },
                {
                    "sent": "This is MPI barrier, but beware of investing too much effort.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 little gain, but if you do, when you do go looking at MPI calls and where these API calls are spending their time, these call paths can be helpful for designating finding which MPI barrier.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is actually the one we have to worry about.",
                    "label": 0
                },
                {
                    "sent": "This game this is the root view.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paragraph and but then you might want to do some tracing to actual.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the cause and effect of of those.",
                    "label": 1
                },
                {
                    "sent": "Of this, MPI calls and what's causing the slowdown.",
                    "label": 0
                },
                {
                    "sent": "Communication bottlenecks?",
                    "label": 1
                },
                {
                    "sent": "So I'm going to stop there 'cause later on Christian is going to show you more about tracing and the the parallel performance measurements.",
                    "label": 0
                },
                {
                    "sent": "I'm going to finish and just summarize here what we've talked about.",
                    "label": 0
                },
                {
                    "sent": "A good choice of algorithms, both in terms of your single cores serial implementation and your parallel implementation is and algorithm is the most important choice you make.",
                    "label": 1
                },
                {
                    "sent": "Performance measurement can help you understand whether not you made a good choice in your algorithm and also in your implementation, and then do the simple optimizations first, like compiler optimizations, maybe MPI parameter tweaking, and that sort of thing, and then gradually work up to more serious investigation if you're if the potential benefit warrants it and just check your single core performance before doing intensive parallel scaling, use the right tool for the job, like a sampling tool for a quick look at.",
                    "label": 0
                },
                {
                    "sent": "And what's going on and exceed, and I think also praised staff are available to help you with these kind of things as we did with this group from Cornell.",
                    "label": 0
                },
                {
                    "sent": "And so one of the things be aware of.",
                    "label": 0
                },
                {
                    "sent": "So with that I will turn it over to Christian for the rest of this session.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm too much in a rush premature optimization.",
                    "label": 0
                },
                {
                    "sent": "So welcome to the second part.",
                    "label": 0
                },
                {
                    "sent": "Let's switch.",
                    "label": 0
                },
                {
                    "sent": "Monitors OK.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can find the slides on the wiki.",
                    "label": 0
                },
                {
                    "sent": "So here you see the.",
                    "label": 0
                },
                {
                    "sent": "Field slides here and we are now working with this.",
                    "label": 0
                },
                {
                    "sent": "I HV C As for one so they're all the commands I will type into my laptop will be shown so if I'm too fast or too slow you can use these slides as a reference.",
                    "label": 0
                },
                {
                    "sent": "So I will.",
                    "label": 0
                },
                {
                    "sent": "Basically these are the slides here so.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "Tell you a different story.",
                    "label": 0
                },
                {
                    "sent": "I will not tell you a success story like filled it, but I will tell you a workflow story how to apply the tools to get some information about the performance of your code.",
                    "label": 0
                },
                {
                    "sent": "So what we will do first.",
                    "label": 0
                },
                {
                    "sent": "Or we will work on a code developed by one of the guys you saw in the group.",
                    "label": 0
                },
                {
                    "sent": "Picture that Thomas Sterling presented us on Monday so it's in Espiral benchmarks developed by NASA.",
                    "label": 0
                },
                {
                    "sent": "It's a very old code.",
                    "label": 0
                },
                {
                    "sent": "That is highly optimized so we will not find so many tuning opportunities there, but this code is very portable and can be configured to nearly every system.",
                    "label": 0
                },
                {
                    "sent": "So this is the reason why we choose the code for this workflow example.",
                    "label": 0
                },
                {
                    "sent": "So please log in into bridges.",
                    "label": 0
                },
                {
                    "sent": "With your own computer.",
                    "label": 0
                },
                {
                    "sent": "Or with the virtual machine.",
                    "label": 0
                },
                {
                    "sent": "So depending on your preference.",
                    "label": 0
                },
                {
                    "sent": "So is this large enough or shall I increase the font size?",
                    "label": 0
                },
                {
                    "sent": "Increase.",
                    "label": 0
                },
                {
                    "sent": "OK. Can you increase it to one OK?",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So now we're on bridges.",
                    "label": 0
                },
                {
                    "sent": "We first need to set up our environment so to load some modules.",
                    "label": 0
                },
                {
                    "sent": "Therefore I created a script that we need to source.",
                    "label": 0
                },
                {
                    "sent": "So just type as I do so source and then you go into.",
                    "label": 0
                },
                {
                    "sent": "My home directory.",
                    "label": 0
                },
                {
                    "sent": "It's RE double SEL.",
                    "label": 0
                },
                {
                    "sent": "There is a subdirectory called IHP CSS16.",
                    "label": 0
                },
                {
                    "sent": "A subdirectory called tools and there is a.",
                    "label": 0
                },
                {
                    "sent": "Source me.",
                    "label": 0
                },
                {
                    "sent": "File and.",
                    "label": 0
                },
                {
                    "sent": "Just hit return and you will see what modules are loaded.",
                    "label": 0
                },
                {
                    "sent": "There are also the tools are now in in your past, so scorpis calasca.",
                    "label": 0
                },
                {
                    "sent": "The tools that we need during this session.",
                    "label": 0
                },
                {
                    "sent": "So next thing we do is we copy the code that I prepared into your home directory.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a tab.",
                    "label": 0
                },
                {
                    "sent": "Also we extract it.",
                    "label": 0
                },
                {
                    "sent": "Anits also in my home directory.",
                    "label": 0
                },
                {
                    "sent": "Now it's in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Subdirectory and it's been asked for benchmarks.",
                    "label": 0
                },
                {
                    "sent": "So if it's too fast to slow, just raise your hand.",
                    "label": 0
                },
                {
                    "sent": "The mentors will help you.",
                    "label": 0
                },
                {
                    "sent": "OK, we just extract it.",
                    "label": 0
                },
                {
                    "sent": "And then we change into this directory.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So if it's too fast, all the commands are also in the in the slides, so we can switch between.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this would be.",
                    "label": 0
                },
                {
                    "sent": "This slide here.",
                    "label": 0
                },
                {
                    "sent": "So what we see here in this directory.",
                    "label": 0
                },
                {
                    "sent": "So we have three different benchmarks here.",
                    "label": 0
                },
                {
                    "sent": "The BTM debt and that this is the one we will use and two other ones.",
                    "label": 0
                },
                {
                    "sent": "There's also some some readme stuff here and the makefile.",
                    "label": 0
                },
                {
                    "sent": "So this BTM is at code.",
                    "label": 0
                },
                {
                    "sent": "What does it do?",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a solver for it's a CFD solver.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't really matter in particular what it does.",
                    "label": 0
                },
                {
                    "sent": "So we just try to get some build instructions and therefore we type make and at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We see a suggestion here.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "To build this code, we have to specify three parameters.",
                    "label": 0
                },
                {
                    "sent": "First the benchmark code we want to build.",
                    "label": 0
                },
                {
                    "sent": "Here we want to build the BTM set, then a class which basically describes the size of the problem and the number of processes we want to run the problem on.",
                    "label": 0
                },
                {
                    "sent": "So why do we want to run it on 8 processes?",
                    "label": 0
                },
                {
                    "sent": "Well, we know that bridges has bridge CPUs have 14.",
                    "label": 0
                },
                {
                    "sent": "Course one note has 28 cores, and we're going to use 2 notes, so we have 56 cores in total, and this can be split into eight processes using seven threads in total.",
                    "label": 0
                },
                {
                    "sent": "If you are new to machine and you don't find the information about the system configuration in.",
                    "label": 0
                },
                {
                    "sent": "In the documentation you can also use a tool called liquid topology, so this will give you.",
                    "label": 0
                },
                {
                    "sent": "This will give you.",
                    "label": 0
                },
                {
                    "sent": "Information about the system.",
                    "label": 0
                },
                {
                    "sent": "So for example, it tells us so that here on this is on the login notes, but the login nodes on bridges are the same as the compute nodes.",
                    "label": 0
                },
                {
                    "sent": "We're using an entirely on processor and it's a hassle.",
                    "label": 0
                },
                {
                    "sent": "Asshole one.",
                    "label": 0
                },
                {
                    "sent": "It has 14 cores.",
                    "label": 0
                },
                {
                    "sent": "For sockets and one thread per socket.",
                    "label": 0
                },
                {
                    "sent": "So hyper threading here is isn't active and we also see that there are two sockets per note when we Scroll down a little bit we can see more information we can see also the cache topology, which might be interesting for those who work on the programming challenge.",
                    "label": 0
                },
                {
                    "sent": "So this tool is not installed on bridges, but if you source the file I just mentioned in the beginning, then this tool is available.",
                    "label": 0
                },
                {
                    "sent": "So OK, we will build the benchmark.",
                    "label": 0
                },
                {
                    "sent": "Using eight processes.",
                    "label": 0
                },
                {
                    "sent": "So basically we just use the command.",
                    "label": 0
                },
                {
                    "sent": "That was suggested.",
                    "label": 0
                },
                {
                    "sent": "And classy.",
                    "label": 0
                },
                {
                    "sent": "Will make it run approximately 18 seconds.",
                    "label": 0
                },
                {
                    "sent": "So when we just type this command and start building.",
                    "label": 0
                },
                {
                    "sent": "The code you see it's a Fortran 77 code.",
                    "label": 0
                },
                {
                    "sent": "Here we use the MPI F-77 compiler from the Open MPI implementation.",
                    "label": 0
                },
                {
                    "sent": "It's also open MP code, so therefore we have to specify this F open NP.",
                    "label": 0
                },
                {
                    "sent": "Option which depends on the.",
                    "label": 0
                },
                {
                    "sent": "On the compiler you use and here in.",
                    "label": 0
                },
                {
                    "sent": "At the bottom of the screen you see that it created a new executable in the bin subdirectory.",
                    "label": 0
                },
                {
                    "sent": "So we can go into this.",
                    "label": 0
                },
                {
                    "sent": "Bin subdirectory.",
                    "label": 0
                },
                {
                    "sent": "Do an LS.",
                    "label": 0
                },
                {
                    "sent": "And there it is.",
                    "label": 0
                },
                {
                    "sent": "So what do we have?",
                    "label": 0
                },
                {
                    "sent": "Now we have a.",
                    "label": 0
                },
                {
                    "sent": "We have an executable.",
                    "label": 0
                },
                {
                    "sent": "We didn't use any tools so far, so this is an executable.",
                    "label": 0
                },
                {
                    "sent": "We can do a reference running list, so reference run is.",
                    "label": 0
                },
                {
                    "sent": "Is important so that you can can either check when you're on a new system that your code runs as expected, so you should have some tests or some verification built in, and you can also later on when you run with tools.",
                    "label": 0
                },
                {
                    "sent": "See what kind of overheads the tools create.",
                    "label": 0
                },
                {
                    "sent": "So how to run this code so we have a job script?",
                    "label": 0
                },
                {
                    "sent": "That you can copy into.",
                    "label": 0
                },
                {
                    "sent": "This directory, so just.",
                    "label": 0
                },
                {
                    "sent": "Do a CP, go one directory up.",
                    "label": 0
                },
                {
                    "sent": "Then there is a job script directory.",
                    "label": 0
                },
                {
                    "sent": "Machine so if you want to use this example on on your machine at home, there are job scripts for several other systems available, so you don't need to write your own, so you can use several of these as a template.",
                    "label": 0
                },
                {
                    "sent": "But here we are going to use the bridges ones and we take the reference as batch.",
                    "label": 0
                },
                {
                    "sent": "And copy it into our directory.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This was in command to copy.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can look into this reference batch script.",
                    "label": 0
                },
                {
                    "sent": "So here we have the.",
                    "label": 0
                },
                {
                    "sent": "Our reservation for the summer school we give thee.",
                    "label": 0
                },
                {
                    "sent": "The job name we specify output file, so where all the output is written and also an error file we want to run on two nodes.",
                    "label": 0
                },
                {
                    "sent": "We want to use eight MPI processes and we want to use seven threads per process.",
                    "label": 0
                },
                {
                    "sent": "We also specify a time here.",
                    "label": 0
                },
                {
                    "sent": "It's one one minute, so the code is supposed to run in 18 seconds approximately.",
                    "label": 0
                },
                {
                    "sent": "What we do here is we we source our tools.",
                    "label": 0
                },
                {
                    "sent": "So what we did in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So actually for the reference run this isn't really necessary.",
                    "label": 0
                },
                {
                    "sent": "But for example, if you wanted to use this topology liquid topology program here in on the compute nodes, then this would be necessary.",
                    "label": 0
                },
                {
                    "sent": "We need to export the number of threads we want to use, so.",
                    "label": 0
                },
                {
                    "sent": "This is the usual way you know from the classic session from David.",
                    "label": 0
                },
                {
                    "sent": "So here we can make use of the number of threads we specified here.",
                    "label": 0
                },
                {
                    "sent": "So this Dash C7 is then stored in the slum.",
                    "label": 0
                },
                {
                    "sent": "Slam environment variable that we can use here.",
                    "label": 0
                },
                {
                    "sent": "So if you want to change the number of thread, you need to just change this one here.",
                    "label": 0
                },
                {
                    "sent": "Everything else?",
                    "label": 0
                },
                {
                    "sent": "Can stay as it is, so if.",
                    "label": 0
                },
                {
                    "sent": "What is my drinking is done?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "This year yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, this this is a number of threats you will use per process, so it's a hybrid program.",
                    "label": 0
                },
                {
                    "sent": "MPI Plus open MP.",
                    "label": 0
                },
                {
                    "sent": "And if you want to restrict or if you want to set the number of open MP threads, you need to export this variable.",
                    "label": 0
                },
                {
                    "sent": "So this is standard way to to specify the number of open MP threats you are going to use when you run a program.",
                    "label": 0
                },
                {
                    "sent": "When you are a hybrid program.",
                    "label": 0
                },
                {
                    "sent": "There are other ways to do this, but this is the most common way.",
                    "label": 0
                },
                {
                    "sent": "Just specify this open and P NUM threads environment variable and you're done.",
                    "label": 0
                },
                {
                    "sent": "Um, OK.",
                    "label": 0
                },
                {
                    "sent": "So then we just.",
                    "label": 0
                },
                {
                    "sent": "Define some variables here to to build the build.",
                    "label": 0
                },
                {
                    "sent": "The xname here of of this program and in the end we do an MPI run.",
                    "label": 0
                },
                {
                    "sent": "We also specify Dash Dash report bindings, so if you're interested you can then after the run look where individual process season individual threads run on which part of the hardware.",
                    "label": 0
                },
                {
                    "sent": "So if you use a different MPI, so this is the open MPI we're using here, then there is this.",
                    "label": 0
                },
                {
                    "sent": "This option here might be named differently.",
                    "label": 0
                },
                {
                    "sent": "So we need to specify the number of processes we are going to use, so this is here in the variable slum and tasks and this comes from here.",
                    "label": 0
                },
                {
                    "sent": "So we configured the.",
                    "label": 0
                },
                {
                    "sent": "The benchmark to use 88 processes we specified here and then we just add the.",
                    "label": 0
                },
                {
                    "sent": "Binary we're going to execute.",
                    "label": 0
                },
                {
                    "sent": "So we are using this job script and modify it a little bit later on when we use it with scorpion's calasca, but from now we can just execute our reference run.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "Using the S batch command to.",
                    "label": 0
                },
                {
                    "sent": "Submit it to the queue.",
                    "label": 0
                },
                {
                    "sent": "So he it says, submit it so we can.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "With SQ dash U.",
                    "label": 0
                },
                {
                    "sent": "And our user name.",
                    "label": 0
                },
                {
                    "sent": "We can look if our job is running or if it's queuing.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "It's already running.",
                    "label": 0
                },
                {
                    "sent": "So we have to wait a little bit so at least 1818 seconds and we can see we will see if there are some output created.",
                    "label": 0
                },
                {
                    "sent": "Oh, there is some output already.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there is an error file written and the output file.",
                    "label": 0
                },
                {
                    "sent": "So if if you have problems following the instructions right, sore hands.",
                    "label": 0
                },
                {
                    "sent": "If it's slow to fast, please let me know.",
                    "label": 0
                },
                {
                    "sent": "And remember, you can also always go back to to the PDF slides.",
                    "label": 0
                },
                {
                    "sent": "To see, yeah.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Installing the tools previously replied that would help us with that added.",
                    "label": 0
                },
                {
                    "sent": "So OK. Let's look at at the output.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the.",
                    "label": 0
                },
                {
                    "sent": "Alpha Delta program run.",
                    "label": 0
                },
                {
                    "sent": "So it tells you that it's an Esper benchmark.",
                    "label": 0
                },
                {
                    "sent": "It uses.",
                    "label": 0
                },
                {
                    "sent": "16 by 16 zones it uses a process is here and the total number of threads is 56, so seven threads per process.",
                    "label": 0
                },
                {
                    "sent": "This is what we specified to use.",
                    "label": 0
                },
                {
                    "sent": "We see here that it's running for 200 timesteps.",
                    "label": 0
                },
                {
                    "sent": "And at the end it does a verification.",
                    "label": 0
                },
                {
                    "sent": "So this is a test I mentioned in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So if you're on a new machine, you first have to check if you run instrumented code.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Run successfully and the results are as you expected and here the verification is successful.",
                    "label": 0
                },
                {
                    "sent": "This is a good time, good sign.",
                    "label": 0
                },
                {
                    "sent": "And this benchmark already print some some some data out so it tells us that it took 17.82 seconds to run.",
                    "label": 0
                },
                {
                    "sent": "This is something to keep in mind because we want to compare it to an instrumented run that we're going to.",
                    "label": 0
                },
                {
                    "sent": "Do I think we have enough time?",
                    "label": 0
                },
                {
                    "sent": "We can do it today.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Were you successful to to run the reference?",
                    "label": 0
                },
                {
                    "sent": "Using your your access to bridges, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "I think I will or what we're going to do next is we use the.",
                    "label": 0
                },
                {
                    "sent": "We use the same.",
                    "label": 0
                },
                {
                    "sent": "Use the same program and we now instrument it, so this is one of the two techniques Phil mentioned, so they're sampling to get some information out of your program and instrumentation.",
                    "label": 0
                },
                {
                    "sent": "But before we going to do this, I will show you quick just a few slides about the tool we we are going to use.",
                    "label": 0
                },
                {
                    "sent": "It's a tool called.",
                    "label": 0
                },
                {
                    "sent": "Scorpis school.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK I will.",
                    "label": 0
                },
                {
                    "sent": "Just go quickly over a few slides, so there are lots of reference slides in there.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just tell you the most important stuff.",
                    "label": 0
                },
                {
                    "sent": "So some time ago there was a fragmentation in the in the two landscape, so there were several tools available so some of them were already mentioned, so it was calasca towel bumpier.",
                    "label": 0
                },
                {
                    "sent": "It's a trace visualizer and they all came with their own measurement system and their native formats.",
                    "label": 0
                },
                {
                    "sent": "So this was for the developers and also for the users.",
                    "label": 0
                },
                {
                    "sent": "And pretty bad situation becausw.",
                    "label": 0
                },
                {
                    "sent": "Usually what the users want to do is.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They want to do a measurement and then use different analysis tools.",
                    "label": 0
                },
                {
                    "sent": "Look at the data so how it looked in the old days.",
                    "label": 0
                },
                {
                    "sent": "So you ask Alaska, which came within all with a known measurement system.",
                    "label": 0
                },
                {
                    "sent": "It created traces in epilogue format.",
                    "label": 0
                },
                {
                    "sent": "Then there was a trace analyzer that produced cube output and then you could look at these output with a cube presenter.",
                    "label": 0
                },
                {
                    "sent": "So then there was a tool one pier that came with one peer trace as a measurement.",
                    "label": 0
                },
                {
                    "sent": "They created a different.",
                    "label": 0
                },
                {
                    "sent": "Output format and I looked at it with Vampyr.",
                    "label": 0
                },
                {
                    "sent": "Then there was towel towel had just creates profiles, but also came with its own measurement system.",
                    "label": 0
                },
                {
                    "sent": "And it could also read the cube profiles and show it in paragraph, something we are showing you tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Then there were parallel traces that didn't work together out of the box.",
                    "label": 0
                },
                {
                    "sent": "And then tell had the idea not only to do profiles, but traces, so they were taught races.",
                    "label": 0
                },
                {
                    "sent": "Then people started building some some converters from different trace formats, but so trace formats or trace files usually are.",
                    "label": 0
                },
                {
                    "sent": "Huge in size, so this is a very time consuming process which you normally don't want to do.",
                    "label": 0
                },
                {
                    "sent": "They work for one or two 2 versions of the of the tools and then the support was discontinued.",
                    "label": 0
                },
                {
                    "sent": "You also for the user a very bad situation and this went on and on, so there are in the end there were lots of possibilities and you were just lucky if some of them really worked out.",
                    "label": 0
                },
                {
                    "sent": "Well, even more.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we came the tool tool developers set together and thought well we need to do it differently and we were lucky to get some funding from the German government and also from the Department of Energy and we created a new measurement system called Score P that creates one profile output and one trace output.",
                    "label": 0
                },
                {
                    "sent": "These outputs are informants at a well specified and several tools can sit on top of it.",
                    "label": 0
                },
                {
                    "sent": "And use it so you have just one measurement system, one learning curve how to instrument your application and then you can use different tools to look at your on your data.",
                    "label": 0
                },
                {
                    "sent": "So I just skip over this.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "But will spend some time on this slide, so here you have your application and what we do with score P is we insert some hooks like Phil already told you so you can insert some MPI hooks.",
                    "label": 0
                },
                {
                    "sent": "By library into position you can do the same for for shmem communication there is the same for or something similar.",
                    "label": 0
                },
                {
                    "sent": "For open MP.",
                    "label": 0
                },
                {
                    "sent": "There we use source to source instrumentation instrumentation to get some information about the Open, MP directorates and Open MP API functions for pthreads.",
                    "label": 0
                },
                {
                    "sent": "We use library into position.",
                    "label": 0
                },
                {
                    "sent": "Then we also have some support for accelerators so CUDA provides a special library that can be used to get some information.",
                    "label": 0
                },
                {
                    "sent": "And out of the CUDA device we have support for open CL and in the upcoming version also for open ACC.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then we can also utilize the compiler to insert hooks at every function entry and exit this is.",
                    "label": 0
                },
                {
                    "sent": "This can be our the overhead created by this can be quite high, so you need to use this with caution, but we will come to this later.",
                    "label": 0
                },
                {
                    "sent": "During the hands-on.",
                    "label": 0
                },
                {
                    "sent": "There's also a source of source instrumental called PDT where you can do the same.",
                    "label": 0
                },
                {
                    "sent": "And there are some macros provided so that you as a user can just instrument the portion you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So I will show you an example tomorrow where I did this with the Davids Davids traffic example.",
                    "label": 0
                },
                {
                    "sent": "So it's basically quite easy to do this and then to to create a trace and see what really is going on.",
                    "label": 0
                },
                {
                    "sent": "So what who's which processes communicating with you.",
                    "label": 0
                },
                {
                    "sent": "Then we also have the possibility to do sampling this only on X86 platforms, so sampling is.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to get right in a portable way.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So either way, so if you have some hooks here or if you have some some sampling interrupts when one of these events are triggered, then.",
                    "label": 0
                },
                {
                    "sent": "This copy measurement in infrastructure is called there.",
                    "label": 0
                },
                {
                    "sent": "We take a timestamp.",
                    "label": 0
                },
                {
                    "sent": "We take some thread local storage.",
                    "label": 0
                },
                {
                    "sent": "We may take some hardware counters.",
                    "label": 0
                },
                {
                    "sent": "So Poppy are usage or Perth counters.",
                    "label": 0
                },
                {
                    "sent": "You can also plug in some yes on external plugins.",
                    "label": 0
                },
                {
                    "sent": "And then we store this information.",
                    "label": 0
                },
                {
                    "sent": "Either in a profile format so it's it's a cube format, or as event traces.",
                    "label": 0
                },
                {
                    "sent": "That preserves the time dimension, and that's basically it.",
                    "label": 0
                },
                {
                    "sent": "For measurements, so there's only some some online interface, but we will not cover this during these lecture here.",
                    "label": 0
                },
                {
                    "sent": "So what can we do with the call?",
                    "label": 0
                },
                {
                    "sent": "Past profiles we can look at the results using the cube browser.",
                    "label": 0
                },
                {
                    "sent": "We will use a cube browser tomorrow, but so for our lecture here we create cube files and we look with towels, power Prof. Analyze the profile using this tool.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow we will create some OTF traces.",
                    "label": 0
                },
                {
                    "sent": "And we will.",
                    "label": 0
                },
                {
                    "sent": "Then automatically analyze them with calasca.",
                    "label": 0
                },
                {
                    "sent": "Scholastica will then create as and as a result a cube file that we then can use that we then can visualize using a cube browser.",
                    "label": 0
                },
                {
                    "sent": "So, but for you as a user for doing the measurement, you just need to think about this box.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You need to think about how should I instrument my code so there are different options are already mentioned here.",
                    "label": 0
                },
                {
                    "sent": "Some of them have high overhead to some of them don't.",
                    "label": 0
                },
                {
                    "sent": "And there are also lots of runtime options, so environment variables where you can.",
                    "label": 0
                },
                {
                    "sent": "Change the behavior or the amount of data that is collected when you run your instrumented program.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, OK, let's switch back to the hands-on.",
                    "label": 0
                },
                {
                    "sent": "What we are now in.",
                    "label": 0
                },
                {
                    "sent": "Going to do is we will use.",
                    "label": 0
                },
                {
                    "sent": "Score P so I haven't told you how to use score P, but it's actually quite.",
                    "label": 0
                },
                {
                    "sent": "Quite easy, so we're still.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the top level directory of our benchmark.",
                    "label": 0
                },
                {
                    "sent": "Um, so we somehow need to.",
                    "label": 0
                },
                {
                    "sent": "To rebuild our application and.",
                    "label": 0
                },
                {
                    "sent": "During the rebuild, we need to tell the.",
                    "label": 0
                },
                {
                    "sent": "Compiler to do something special so to to use the script functionality to insert the hooks in your code.",
                    "label": 0
                },
                {
                    "sent": "So we do this by.",
                    "label": 0
                },
                {
                    "sent": "By changing one file so.",
                    "label": 0
                },
                {
                    "sent": "You can use any editor you want.",
                    "label": 0
                },
                {
                    "sent": "So it's in a config substory directory.",
                    "label": 0
                },
                {
                    "sent": "There is a call.",
                    "label": 0
                },
                {
                    "sent": "There's a file called Make dot death.",
                    "label": 0
                },
                {
                    "sent": "We just opened it so this is the top configuration file for this.",
                    "label": 0
                },
                {
                    "sent": "So for this application.",
                    "label": 0
                },
                {
                    "sent": "So here you see that we specified the F Open MP.",
                    "label": 0
                },
                {
                    "sent": "The compiler option because we're using the GCC compiler.",
                    "label": 0
                },
                {
                    "sent": "If you use an Intel compiler, for example, you need to use this Q Open MP and so if you want to play around with different compilers you need to edit.",
                    "label": 0
                },
                {
                    "sent": "You need to modify the just this file.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, here's the line that specifies the compiler we're using.",
                    "label": 0
                },
                {
                    "sent": "It's the MPI F 77.",
                    "label": 0
                },
                {
                    "sent": "This is the open MPI Fortran compiler.",
                    "label": 0
                },
                {
                    "sent": "This is what we use for the for the reference run.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We want to use corpi in addition, so the only thing what we need to do is to just remove the comment here.",
                    "label": 0
                },
                {
                    "sent": "So now we are going to use.",
                    "label": 0
                },
                {
                    "sent": "Scorpis distress user NPI F 77.",
                    "label": 0
                },
                {
                    "sent": "So we just pre pend.",
                    "label": 0
                },
                {
                    "sent": "The the score P command an once copy option to the usual compiler.",
                    "label": 0
                },
                {
                    "sent": "That's basically all you need to do.",
                    "label": 0
                },
                {
                    "sent": "Depending on the build system you use with your applications, it might be easy or not so easy, but there are also some other options that make it easier.",
                    "label": 0
                },
                {
                    "sent": "For example, if you use a CMake based build system, or autotools based.",
                    "label": 0
                },
                {
                    "sent": "A build system if you want to try it with such a system.",
                    "label": 0
                },
                {
                    "sent": "Please let me know.",
                    "label": 0
                },
                {
                    "sent": "So we just save this file.",
                    "label": 0
                },
                {
                    "sent": "Close it and then.",
                    "label": 0
                },
                {
                    "sent": "We need to rebuild it.",
                    "label": 0
                },
                {
                    "sent": "So where here's our May command?",
                    "label": 0
                },
                {
                    "sent": "First we need to do a make clean.",
                    "label": 0
                },
                {
                    "sent": "Otherwise.",
                    "label": 0
                },
                {
                    "sent": "You will get no instrumentation.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's build it.",
                    "label": 0
                },
                {
                    "sent": "OK, what we see here is.",
                    "label": 0
                },
                {
                    "sent": "That we know.",
                    "label": 0
                },
                {
                    "sent": "Repent the compile command with Scorpid estrus user.",
                    "label": 0
                },
                {
                    "sent": "So this test test user keltz copy that it should activate.",
                    "label": 0
                },
                {
                    "sent": "User instrumentation user instrumentation is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, some kind of macros you insert in your code to highlight some some special regions you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So if you want to see all possible.",
                    "label": 0
                },
                {
                    "sent": "Options you can pass to score P. You do a scorpid Estes help.",
                    "label": 0
                },
                {
                    "sent": "So and you can switch on the bus city level to see what was Scorpius doing under the hood.",
                    "label": 0
                },
                {
                    "sent": "So for example, for our MPI Open MP case.",
                    "label": 0
                },
                {
                    "sent": "We first.",
                    "label": 0
                },
                {
                    "sent": "Use a tool called Aparri to do some source to source instrumentation for the Open MP Pragmas.",
                    "label": 0
                },
                {
                    "sent": "And then we do the actual compile command.",
                    "label": 0
                },
                {
                    "sent": "If you want to inspect the the intermediate files, you can use this keep files command, so this might be.",
                    "label": 0
                },
                {
                    "sent": "Quite interesting.",
                    "label": 0
                },
                {
                    "sent": "In case we are using GCC, you can also do some compile time filtering and you can go through this list here.",
                    "label": 0
                },
                {
                    "sent": "There are lots of parameters you can you can specify, so some.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Some options are on by default, so this is for example this Dash compiler.",
                    "label": 0
                },
                {
                    "sent": "And these this option inserts for every function enter and exit.",
                    "label": 0
                },
                {
                    "sent": "Hook into your code.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the default if you just want to see where.",
                    "label": 0
                },
                {
                    "sent": "Where are my hot spots in the code.",
                    "label": 0
                },
                {
                    "sent": "But if you have, if you know your code and it might be and you you're working long time with the code, you might think of adding some user specific instrumentation at some important points so that you can easy easily run it and then keep keep all the instrumentation in your code with some with some defines that might help you also to automatically find some some performance regressions you built into your code.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK. Well.",
                    "label": 0
                },
                {
                    "sent": "So the compile.",
                    "label": 0
                },
                {
                    "sent": "Created a new binary, but not in the bin Directory but in the bin Scorpid directory so.",
                    "label": 0
                },
                {
                    "sent": "Can just CD into this directory.",
                    "label": 0
                },
                {
                    "sent": "And here we have now the executable.",
                    "label": 0
                },
                {
                    "sent": "It's the same name but now with Scorpion instrumentation.",
                    "label": 0
                },
                {
                    "sent": "So process is quite simple, you just prepended compile command with this copy command and everything else is done automatically.",
                    "label": 0
                },
                {
                    "sent": "OK, to run this code, now we need to.",
                    "label": 0
                },
                {
                    "sent": "Copy a different job script.",
                    "label": 0
                },
                {
                    "sent": "So this is again in the job script.",
                    "label": 0
                },
                {
                    "sent": "Directory jobs groups bridges.",
                    "label": 0
                },
                {
                    "sent": "And then we copy the score P dot S Patch into our directory.",
                    "label": 0
                },
                {
                    "sent": "And if we look.",
                    "label": 0
                },
                {
                    "sent": "And this file it looks similar to the old one, so we still have their reservation.",
                    "label": 0
                },
                {
                    "sent": "Here we have the error and output files.",
                    "label": 0
                },
                {
                    "sent": "We have two nodes, 8 processes, 7 threads per process.",
                    "label": 0
                },
                {
                    "sent": "We source our tools, we set the number of open MP threads and but we also.",
                    "label": 0
                },
                {
                    "sent": "Can do some configuration for Skokie, so this is.",
                    "label": 0
                },
                {
                    "sent": "Not really necessary, so if you don't specify anything then Scorpia runs in profiling mode.",
                    "label": 0
                },
                {
                    "sent": "This is the default, but here we specify the directory so where the output.",
                    "label": 0
                },
                {
                    "sent": "Will then.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Will can then be found?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if we if you don't specify this directory, then you have the directory name is just Scorpion, then timestamp and then the random number.",
                    "label": 0
                },
                {
                    "sent": "So if you do several measurements then this gets a little bit confusing.",
                    "label": 0
                },
                {
                    "sent": "Overtime here are some more scorpis.",
                    "label": 0
                },
                {
                    "sent": "Variables, but they recommended, so we're not going to use them right now, but here, for example, is a specification of some puppy metrics so that Phil already mentioned.",
                    "label": 0
                },
                {
                    "sent": "So it's the only thing you need to do to get a puppy measurement at every function or region.",
                    "label": 0
                },
                {
                    "sent": "Enter and Exit is to specify this line here.",
                    "label": 0
                },
                {
                    "sent": "And to specify the counters you are interested in.",
                    "label": 0
                },
                {
                    "sent": "But be aware that measurement of Papi counters takes quite some time, so this adds a lot of overhead and you might want to do this with serial application.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to just.",
                    "label": 0
                },
                {
                    "sent": "Optimize your.",
                    "label": 0
                },
                {
                    "sent": "Your computational kernel.",
                    "label": 0
                },
                {
                    "sent": "And you're not interested in communication.",
                    "label": 0
                },
                {
                    "sent": "We run it as is, so just do an MPI run.",
                    "label": 0
                },
                {
                    "sent": "Specify the number of tasks and the name of the executable.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "OK, I just submit it.",
                    "label": 0
                },
                {
                    "sent": "So what we have done so far is we we created a reference run to see that everything works fine on this machine.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the output verification was successful, it took approximately 18 seconds.",
                    "label": 0
                },
                {
                    "sent": "Then we used tools for P2.",
                    "label": 0
                },
                {
                    "sent": "Instrumented code so, and it's quite easy to use, you just prepended to the usual compiler.",
                    "label": 0
                },
                {
                    "sent": "You build the tool again.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For the first run, you basically don't need to specify any additional configuration, so this would mean that you will get a profile output.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let's see if this already happened, so now.",
                    "label": 0
                },
                {
                    "sent": "Still running or queuing?",
                    "label": 0
                },
                {
                    "sent": "OK, it's ready and here.",
                    "label": 0
                },
                {
                    "sent": "Here we are so.",
                    "label": 0
                },
                {
                    "sent": "You see the the directory we specified, so we just gave it a name and here we have the number of processes and the number of threads and the job ID.",
                    "label": 0
                },
                {
                    "sent": "So if you specify the job ID in the name, you can start several jobs from the same directory and you don't get problems that one measurement want to write into already existing directory.",
                    "label": 0
                },
                {
                    "sent": "We have an environment variable.",
                    "label": 0
                },
                {
                    "sent": "For this to specify what what will happen then?",
                    "label": 0
                },
                {
                    "sent": "But usually it's better to have distinct directories for every measurement, so I already talked about the scorpion environment variables, so but where when you want to use and where to find them.",
                    "label": 0
                },
                {
                    "sent": "So therefore we have a different tool, it's called Scrapy info.",
                    "label": 0
                },
                {
                    "sent": "And if you pass it a dash dash help.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You see basically 3 commands here.",
                    "label": 0
                },
                {
                    "sent": "This is a conflict virus.",
                    "label": 0
                },
                {
                    "sent": "This lists all the measurement configuration variables.",
                    "label": 0
                },
                {
                    "sent": "There's also this conflict summary.",
                    "label": 0
                },
                {
                    "sent": "This gives you a summary of the installation of Scorpio, so which compilers were used.",
                    "label": 0
                },
                {
                    "sent": "This is quite helpful if you run into a back and you want to communicate with us and you can also look at a fire called open issues where we list some problems on some specific architectures that we're at.",
                    "label": 0
                },
                {
                    "sent": "So far we're not able to.",
                    "label": 0
                },
                {
                    "sent": "Suffix.",
                    "label": 0
                },
                {
                    "sent": "OK, let's look at the.",
                    "label": 0
                },
                {
                    "sent": "Configuration very variables.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here you see the.",
                    "label": 0
                },
                {
                    "sent": "Are two very important ones, so this is Scorpion able profiling.",
                    "label": 0
                },
                {
                    "sent": "This is the default.",
                    "label": 0
                },
                {
                    "sent": "You want to do a trace measurement.",
                    "label": 0
                },
                {
                    "sent": "Then you need to to enable this variable or you can use a tool like Scholastica.",
                    "label": 0
                },
                {
                    "sent": "We do this tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Alaska will automatically use this environment variable for the sampling.",
                    "label": 0
                },
                {
                    "sent": "One you need to specify this unwinding here.",
                    "label": 0
                },
                {
                    "sent": "But this will not be part of this lecture.",
                    "label": 0
                },
                {
                    "sent": "Then we have the scope scrapy, total memory.",
                    "label": 0
                },
                {
                    "sent": "This is the amount of memory for our internal buffer.",
                    "label": 0
                },
                {
                    "sent": "We will need this tomorrow for Trace experiment because Trace experiment can be quite huge and we want to fit everything into a memory buffer because we don't want to intermediate Lee during the measurement.",
                    "label": 0
                },
                {
                    "sent": "Flush the buffer to to file.",
                    "label": 0
                },
                {
                    "sent": "This is such a huge perturbation.",
                    "label": 0
                },
                {
                    "sent": "The measurement is basically.",
                    "label": 0
                },
                {
                    "sent": "Use this if you have some intermediate flashes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the.",
                    "label": 0
                },
                {
                    "sent": "The output file.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Basically the same.",
                    "label": 0
                },
                {
                    "sent": "We still have these two 100 timestamps.",
                    "label": 0
                },
                {
                    "sent": "We still have 56 threats, 7 threads per process.",
                    "label": 0
                },
                {
                    "sent": "Verification is still fix successful, so this is quite good, but now we see here time and seconds increased quite a lot.",
                    "label": 0
                },
                {
                    "sent": "It's now 35 seconds and in the beginning I think it was 17 and something so we need to do something about this measurement overhead.",
                    "label": 0
                },
                {
                    "sent": "If we look at the.",
                    "label": 0
                },
                {
                    "sent": "At the error file, I think we haven't done it.",
                    "label": 0
                },
                {
                    "sent": "For the reference run here.",
                    "label": 0
                },
                {
                    "sent": "You see just the commands that were executed in the.",
                    "label": 0
                },
                {
                    "sent": "In the job script.",
                    "label": 0
                },
                {
                    "sent": "Here basically the the NPR run command.",
                    "label": 0
                },
                {
                    "sent": "We ran the application with eight processes and this here all the remaining is the output of this binding report done by open MPI.",
                    "label": 0
                },
                {
                    "sent": "So despite.",
                    "label": 0
                },
                {
                    "sent": "Law.",
                    "label": 0
                },
                {
                    "sent": "OK, so but let's look at the.",
                    "label": 0
                },
                {
                    "sent": "Experiment directory.",
                    "label": 0
                },
                {
                    "sent": "So what's in there?",
                    "label": 0
                },
                {
                    "sent": "So there are two files.",
                    "label": 0
                },
                {
                    "sent": "Profile dot cubex.",
                    "label": 0
                },
                {
                    "sent": "So this is our profile report.",
                    "label": 0
                },
                {
                    "sent": "We are going to use.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow with the power Prof and we also have a scar P config file.",
                    "label": 0
                },
                {
                    "sent": "This file lists all the active.",
                    "label": 0
                },
                {
                    "sent": "Environment variables that were used during this run.",
                    "label": 0
                },
                {
                    "sent": "So here we see profiling was true, so we just collect the profile.",
                    "label": 0
                },
                {
                    "sent": "Tracing was false.",
                    "label": 0
                },
                {
                    "sent": "Unwinding was false.",
                    "label": 0
                },
                {
                    "sent": "Total memory was the default.",
                    "label": 0
                },
                {
                    "sent": "16 megabytes and yeah, lots of other.",
                    "label": 0
                },
                {
                    "sent": "Environment variables that for all purposes here are.",
                    "label": 0
                },
                {
                    "sent": "Not interesting and maybe here you see no poppy of us defined.",
                    "label": 0
                },
                {
                    "sent": "Now our usage counters were defined.",
                    "label": 0
                },
                {
                    "sent": "No plugins were defined, so we just collected.",
                    "label": 0
                },
                {
                    "sent": "Information about function enter exits MPI and Open MP and so.",
                    "label": 0
                },
                {
                    "sent": "We're not going to.",
                    "label": 0
                },
                {
                    "sent": "To use.",
                    "label": 0
                },
                {
                    "sent": "Tools on bridge is to look at the data becausw the interactive.",
                    "label": 0
                },
                {
                    "sent": "These are interactive interactive graphical tools and this it's just slow if if we all use this remote machine.",
                    "label": 0
                },
                {
                    "sent": "To look at them so we need to copy this stuff back to our machine.",
                    "label": 0
                },
                {
                    "sent": "So therefore.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I create another.",
                    "label": 0
                },
                {
                    "sent": "Terminal on my.",
                    "label": 0
                },
                {
                    "sent": "My laptop here.",
                    "label": 0
                },
                {
                    "sent": "And I just copy the stuff.",
                    "label": 0
                },
                {
                    "sent": "From bridges back to my laptop.",
                    "label": 0
                },
                {
                    "sent": "So if you're using.",
                    "label": 0
                },
                {
                    "sent": "If you're using.",
                    "label": 0
                },
                {
                    "sent": "Party so there are other ways to copy stuff back from bridges, but if you're using a terminal, just do it like I do, but use your own username.",
                    "label": 0
                },
                {
                    "sent": "So and.",
                    "label": 0
                },
                {
                    "sent": "The file.",
                    "label": 0
                },
                {
                    "sent": "It's located here, so you need to change this.",
                    "label": 0
                },
                {
                    "sent": "Should copy back your own experiment.",
                    "label": 0
                },
                {
                    "sent": "And the file is named.",
                    "label": 0
                },
                {
                    "sent": "Profile that.",
                    "label": 0
                },
                {
                    "sent": "Cubex and you just copy it back to your local machine.",
                    "label": 0
                },
                {
                    "sent": "OK, there it is.",
                    "label": 0
                },
                {
                    "sent": "And so now it's three o'clock.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do tomorrow is we use this specific file and you are going to use power Prof on your machine that you hopefully already installed, and then we look at these files.",
                    "label": 0
                },
                {
                    "sent": "Maybe I do.",
                    "label": 0
                },
                {
                    "sent": "Very quick preview.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think we should.",
                    "label": 0
                },
                {
                    "sent": "I think we need to go back and make sure you've got the tools installed and ready for tomorrow.",
                    "label": 0
                },
                {
                    "sent": "First thing you know, copy if you ran the experiment, copy that back to your laptop and then will you spare Prof tomorrow to look at the results and feel free to play around with their Prof and take a look on your own but will go through that.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how to look at the data different ways tomorrow and then we'll go into the tracing another hands-on well, yeah.",
                    "label": 0
                },
                {
                    "sent": "If you if you didn't, if you weren't successful in collecting your own profile, there are ready made profiles on the wiki page you so you can download them and play with them.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you just.",
                    "label": 0
                }
            ]
        }
    }
}