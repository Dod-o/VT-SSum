{
    "id": "4f2av3ger6yxuc2l3h2bvjyjbgwonolp",
    "title": "Binary Action Search for Learning Continuous-Action Control Policies",
    "info": {
        "author": [
            "Jason Pazis, Department of Electronic and Computer Engineering, Technical University of Crete"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/icml09_pazis_bas/",
    "segmentation": [
        [
            "Thank you very much for the introduction.",
            "This would be about learning continuous action control policies, and now the motivation for this work."
        ],
        [
            "Is the fact that current reinforcement learning algorithms are very good at handling continuous state spaces?",
            "But mostly handled this."
        ],
        [
            "Free tax in spaces.",
            "On the other hand, many real world problems have continued."
        ],
        [
            "Control variables.",
            "And the problem is that current continuous action approaches are often inefficient.",
            "So the question is, can we control continuous variables using?"
        ],
        [
            "Discrete decisions.",
            "We'll make it brief introduction to the topic and present our approach.",
            "Binary concerts presents some experimental results, and finally draw some conclusions."
        ],
        [
            "Let's start with the intro."
        ],
        [
            "Jackson I hope your most of you are familiar with Markov decision process.",
            "It is a convenient way of modeling an environment where transitions and rewards are independent of history.",
            "Hi Mark, this is in process is defined by S. It state Space AIDS action space B as stochastic transition model are a reward function gamma a discount factor and the the initial state distribution.",
            "Now our."
        ],
        [
            "Clean up time Isation is to optimize the total expected discounted reward.",
            "In order to do that, we need about."
        ],
        [
            "Best Buy, But is a way of making decisions in all situations.",
            "We know that there exists at least one deterministic opt."
        ],
        [
            "Policy \u03c0 star.",
            "Antivirus algorithm exists for finding such a policy."
        ],
        [
            "Now I are going learning.",
            "Is to learn from a process.",
            "We don't have the model nor the reward function.",
            "Instead what you have is a set of samples that are generated by repeated interaction with an unknown process."
        ],
        [
            "The goal of reinforcement learning is first to learn to predict the value of a fixed policy and ultimately to learn a good policy in order to control."
        ],
        [
            "Such a process, once again, virus algorithms exist for learning for reinforcement learning, will be using this course policy iteration and feedback iteration extensively."
        ],
        [
            "Now what do you do?",
            "We need continuous actions in control.",
            "Continuous action so far as smoothness of motion, reduced power consumption, reduced mechanical stresses, and reduced induced power line noise."
        ],
        [
            "Problems with continuous actions are that.",
            "At this time step, we're presented with an infinite number of choices and tabular approaches, discrete maximization or fine discretization are off."
        ],
        [
            "Inefficient.",
            "I.",
            "This is a sample of related work in the area.",
            "There have been approaches using neural networks were were training a neural network in order to have a continuous output multi color subsampling of finite number of actions, a single state tax and approximator.",
            "Where we evaluate our approximator for a number of different actions at each time step or approaches that exploit temporal locality were the action at each time step.",
            "Is dependent on vaccinium the previous time step."
        ],
        [
            "Now let's see our approach by interaction, Sir."
        ],
        [
            "Anne.",
            "In order to we know that choosing a continuous action in a single step is a hard problem.",
            "So how about breaking this hard problem into?"
        ],
        [
            "My knees are once.",
            "The idea is that given a continuous action value at some state, we can decide if it's better to increase it or decrease it.",
            "Ah."
        ],
        [
            "First we need a multistep action series were a binary policy defined over the state and action space is mapped to increase or decrease accents.",
            "And then we perform and then step binary search over the action space, successively approximating the best continuous action value.",
            "As another benefit, binary action searches in anytime algorithm.",
            "The more steps we make in order to approximate their continuous action, the better our approximation will be."
        ],
        [
            "This is the algorithm.",
            "As you can see, it's very simply quite straightforward at the first step we initialize the continuous variable to the middle of the rains.",
            "We naturalize Delta to just little bit over half the reins, and at each step we divide Delta by two, make a binary decision decision whether to increase or decrease the continuous action, and update the continuous.",
            "And based on the decision we made and the current step size Delta."
        ],
        [
            "Now the requirements for learning binary policies are that we need a continuous admitted state space.",
            "Enter discrete binary action space.",
            "The good news is that most reinforcement learning algorithms can be used."
        ],
        [
            "Now, in order to learn, we need to transform a single sample from the environment to a number of transitions in veterans.",
            "For veterans formed in VIA."
        ],
        [
            "A simple example.",
            "Would we make this more clear?",
            "Let's assume that you have a continuous accent range within one and eight, and we choose a 3 bit resolution."
        ],
        [
            "This is actually a sample of the process where we have the transitions from, say, test to status, pride prime using accent too."
        ],
        [
            "And this is, these are the transitions that reinforcement learning algorithm will actually see.",
            "We start at the middle of the reins.",
            "Make binary decisions whether to increase or decrease the current action, and we are led to the continuous action too.",
            "And now it is very important to note that this is the only action that our environment will actually see.",
            "This is also the transition when we receive the reward and the process is discounted.",
            "All these steps are undiscounted and have zero rewards."
        ],
        [
            "Now something that may not be obvious at first glance is that binary X inserts does not get wrapped to local Optima."
        ],
        [
            "Why is it so?",
            "And let's assume that the this transition is optimal and has a value function.",
            "Let's say Q star.",
            "That value will be exactly the same as Susan Jackson increase in this state as will be using the action decrease in this state.",
            "As it will be 8 Susan Laxson degree degrees in this state.",
            "The value of choosing vaccine degrees in this state has nothing to do with choosing the continuous action.",
            "4.5 in this state."
        ],
        [
            "Now I will, as we've already said, binary action sets can be combined with most existing reinforcement learning algorithms.",
            "The only requirement is that we need to be able to support continuous state spaces."
        ],
        [
            "Anne.",
            "Also, binary search is very efficient since first needs only barring search policy and it scales logarithmically with the resolution."
        ],
        [
            "Let's see some experimental."
        ],
        [
            "Result.",
            "First of all, we have the inverted pendulum where our goal is to balance the pendulum at the upright position.",
            "Our state space is a vertical angle and the angular velocity.",
            "And in contrast to user approaches, we will have 256 equally spaced actions in minus 50 to 5050 Newtons.",
            "Also, we will have uniform noise from minus 10 to 10 newtons."
        ],
        [
            "Our learning setup consisted of random training episodes.",
            "And instead of using instead of approaching the task as a regulator task.",
            "As a as an avoidance task, we approach the task as a regulator task where we have penalties both for large values of the angry angular velocity and the force that we use."
        ],
        [
            "We also use the block of radial basis functions, also function approximator."
        ],
        [
            "And these are the results.",
            "Here we can see the reward as a function of training episodes for various approaches, combined with list, correspondence, iteration and fitted to iteration.",
            "Now we can see that there is a clear relationship between the number of actions used and the performance of its algorithm.",
            "Now a. OK, these are discrete approaches for various number of actions.",
            "The combined state tax and Approximator is an approximator over both the state and action space evaluated at the number of points.",
            "And what we saw is that for this highly nonlinear task.",
            "And as the number of actions was increased, the pendulum became more and more unstable.",
            "On the other hand.",
            "Binary concerts, both of feedback.",
            "You iteration, endless queries, projects, iteration.",
            "A achieves much better performance than any of the other approaches.",
            "And has a very steep learning curve."
        ],
        [
            "Here we can see force histograms for 10 years and 20 Newton noise when using the binary action search.",
            "And as you can see, binary action sets.",
            "You can control the pendulum using very low mean force magnitudes.",
            "And this is very important because.",
            "Mini force magnitudes in a real domain would be directly related to power consumption, mechanical stresses, etc.",
            "Also, once they notice and is increased by narcs inserts is still successful.",
            "Almost all of the time, while discrete approaches success."
        ],
        [
            "Less than half of the time.",
            "Another domain that we experimented with is the double integrator domain.",
            "An on this domain we control the acceleration of moving car and our state.",
            "Our state is the position and the velocity.",
            "Once again, this is a regulator task.",
            "And getting penalties both for large values of the position and acceleration.",
            "And we also used a rather large control interval in order to make the problem more challenging."
        ],
        [
            "On this domain we used a simple polynomial approximator."
        ],
        [
            "Here are the results.",
            "For the various approaches.",
            "And now what we can see is that.",
            "When increasing the number of actions from 5 to 11, we get better performance.",
            "On the other hand, when the number of actions is too large.",
            "Meaning is quite difficult.",
            "At some point probably this curve will surpass the other one.",
            "Anyway, the combined state tax and approximator on this simply linear domain performed better than the discrete approaches.",
            "However, it cannot approach the performance reached by binary action search."
        ],
        [
            "Now, in order to conclude."
        ],
        [
            "And the strength of barracks inserts are.",
            "It's implicitly the fact that it requires absolutely no tuning.",
            "It requires only two actions from the from our policy.",
            "And it's it achieves resolutions impossible to reach with discrete accents.",
            "It can also be used with any reinforcement learning algorithm in an online offline owner policy or policy setting, it's we."
        ],
        [
            "This is our the fact that the state space of the process is now more complex and more samples have to be processed by our reinforcement learning algorithm.",
            "An hour."
        ],
        [
            "Going research will where we have already achieved some very good results are.",
            "Using binary search in high dimensional dimensional state space is increasing learning and execution efficiency.",
            "And our future research is about planning with banner action sets and using screen functions over there."
        ],
        [
            "The action Reigns.",
            "That sucked."
        ],
        [
            "Question.",
            "There are other teams action methods like allowing new sections to learn.",
            "Anne.",
            "Actually, after I've most of the approaches that I could see, that could actually work.",
            "And were relatively easy to implement.",
            "Then you know the selling process seem very nice for one domain, but don't really work on another domain or work with a particular algorithm.",
            "The good thing about binary search is that.",
            "I can compare it with pretty much anything.",
            "I can use it with any algorithm.",
            "I don't know.",
            "Spell feeling better with work in the past.",
            "I'm not using Q learning in this particular research, so.",
            "It couldn't possibly work.",
            "So maybe it would be interesting to test.",
            "Then again, binary concerts could work with the.",
            "Q Learning as well.",
            "Yes.",
            "This is a stupid question.",
            "The idea the idea of how you actually use the binary choices in each moment to guarantee you have like an inner loop within each time step.",
            "Yeah, I think.",
            "This figure might help a bit.",
            "Anne.",
            "It will have one transition transition with the environment.",
            "Essentially, yeah, we will have a loop where we will make a number of decisions.",
            "Yes, this is what actually happens.",
            "This is what we see that happens.",
            "So.",
            "How is it this elected OK, we start at the middle of the action range?",
            "And we make it this season whether to increase or decrease the current accident.",
            "And we're led to this point.",
            "Then again, a decision to increase or decrease the accident later this point.",
            "And now finally, this decision will lead us to.",
            "Vaccine two I mean.",
            "Yeah.",
            "Essentially it's like building a tree of possible actions and extending one node at a time.",
            "Anne.",
            "I don't know if it's really answer your question.",
            "Do some number of steps of this or you can do that, yeah?",
            "For a 3 bit resolution, it's three steps for an N bit resolution it's N steps.",
            "So how is it that this would be better then?",
            "Then storing the action there rather than storing a little procedure which takes an why if we store it will have a combined state action approximator.",
            "We may have a lot of local optimum local Optima.",
            "Right, so if we want to have, let's say a night with resolution 256 possible actions.",
            "The way to find which of them is the best is to evaluate all of them.",
            "If you make a binary search on that on the combined state tax state action approximator.",
            "You might get dropped because you know if you're at the middle of the rains.",
            "Maybe it looks like it's better to go left, but there is a very steep optimum.",
            "And at the right so.",
            "Ascentia Lee.",
            "And the good thing about this is that you can achieve what the combined state tax and approximator achieves.",
            "And but you only need a logarithmic number of evaluations.",
            "And I will, as we have seen in the.",
            "Experiments.",
            "Anne.",
            "It appears to work.",
            "A lot better than their combined state tax and approximator.",
            "Now why this happens may not be very clear at first point, but by intuition we could say that.",
            "This policies easier to represent for state tax and approximator.",
            "While this policy is very much harder.",
            "If we fail in one single evaluation for some reason.",
            "The value returned is really large.",
            "And we will be late there directly.",
            "Sorry.",
            "So your average works, finding the very function is convex in the action, and if it's model model then it might get the combined state tax and approximator would have such a problem.",
            "Our our approach has.",
            "Doesn't have any problems like that, so the value function could be anything.",
            "I mean non continuous function.",
            "We are guaranteed to fight the optimum.",
            "Of course.",
            "OK that depends on our faction function approximator.",
            "If our function approximator is not expressive enough to express such a function, we may be led to a sub optimal solution.",
            "Alright, thank you.",
            "My dimensions.",
            "An OK, the obvious naive approach would be instead of doing a binary set search, doing a quad search looking whether to increase or decrease each action at each time step so your lead to four actions in the case of two dimensions, 8 actions in the case of three dimensions, etc.",
            "It turns out that we don't really have to do this.",
            "We can scale linearly to the number of actions as well.",
            "We could talk offline for this.",
            "With the problem earlier.",
            "Predicted.",
            "Something.",
            "At that time, we could take forever to get to mix.",
            "Not really.",
            "Actually, yeah, this is.",
            "It turns out that you can do in in multiple dimensions.",
            "Pretty much what you do for a continuous action, so it's not really a problem.",
            "We've already tried it actually so.",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for the introduction.",
                    "label": 0
                },
                {
                    "sent": "This would be about learning continuous action control policies, and now the motivation for this work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the fact that current reinforcement learning algorithms are very good at handling continuous state spaces?",
                    "label": 0
                },
                {
                    "sent": "But mostly handled this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free tax in spaces.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, many real world problems have continued.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Control variables.",
                    "label": 0
                },
                {
                    "sent": "And the problem is that current continuous action approaches are often inefficient.",
                    "label": 1
                },
                {
                    "sent": "So the question is, can we control continuous variables using?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discrete decisions.",
                    "label": 0
                },
                {
                    "sent": "We'll make it brief introduction to the topic and present our approach.",
                    "label": 0
                },
                {
                    "sent": "Binary concerts presents some experimental results, and finally draw some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start with the intro.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jackson I hope your most of you are familiar with Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "It is a convenient way of modeling an environment where transitions and rewards are independent of history.",
                    "label": 1
                },
                {
                    "sent": "Hi Mark, this is in process is defined by S. It state Space AIDS action space B as stochastic transition model are a reward function gamma a discount factor and the the initial state distribution.",
                    "label": 1
                },
                {
                    "sent": "Now our.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clean up time Isation is to optimize the total expected discounted reward.",
                    "label": 0
                },
                {
                    "sent": "In order to do that, we need about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best Buy, But is a way of making decisions in all situations.",
                    "label": 0
                },
                {
                    "sent": "We know that there exists at least one deterministic opt.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Policy \u03c0 star.",
                    "label": 0
                },
                {
                    "sent": "Antivirus algorithm exists for finding such a policy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I are going learning.",
                    "label": 0
                },
                {
                    "sent": "Is to learn from a process.",
                    "label": 0
                },
                {
                    "sent": "We don't have the model nor the reward function.",
                    "label": 1
                },
                {
                    "sent": "Instead what you have is a set of samples that are generated by repeated interaction with an unknown process.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal of reinforcement learning is first to learn to predict the value of a fixed policy and ultimately to learn a good policy in order to control.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such a process, once again, virus algorithms exist for learning for reinforcement learning, will be using this course policy iteration and feedback iteration extensively.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what do you do?",
                    "label": 0
                },
                {
                    "sent": "We need continuous actions in control.",
                    "label": 1
                },
                {
                    "sent": "Continuous action so far as smoothness of motion, reduced power consumption, reduced mechanical stresses, and reduced induced power line noise.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems with continuous actions are that.",
                    "label": 0
                },
                {
                    "sent": "At this time step, we're presented with an infinite number of choices and tabular approaches, discrete maximization or fine discretization are off.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inefficient.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "This is a sample of related work in the area.",
                    "label": 0
                },
                {
                    "sent": "There have been approaches using neural networks were were training a neural network in order to have a continuous output multi color subsampling of finite number of actions, a single state tax and approximator.",
                    "label": 0
                },
                {
                    "sent": "Where we evaluate our approximator for a number of different actions at each time step or approaches that exploit temporal locality were the action at each time step.",
                    "label": 0
                },
                {
                    "sent": "Is dependent on vaccinium the previous time step.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see our approach by interaction, Sir.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In order to we know that choosing a continuous action in a single step is a hard problem.",
                    "label": 1
                },
                {
                    "sent": "So how about breaking this hard problem into?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My knees are once.",
                    "label": 0
                },
                {
                    "sent": "The idea is that given a continuous action value at some state, we can decide if it's better to increase it or decrease it.",
                    "label": 1
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First we need a multistep action series were a binary policy defined over the state and action space is mapped to increase or decrease accents.",
                    "label": 1
                },
                {
                    "sent": "And then we perform and then step binary search over the action space, successively approximating the best continuous action value.",
                    "label": 1
                },
                {
                    "sent": "As another benefit, binary action searches in anytime algorithm.",
                    "label": 0
                },
                {
                    "sent": "The more steps we make in order to approximate their continuous action, the better our approximation will be.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's very simply quite straightforward at the first step we initialize the continuous variable to the middle of the rains.",
                    "label": 1
                },
                {
                    "sent": "We naturalize Delta to just little bit over half the reins, and at each step we divide Delta by two, make a binary decision decision whether to increase or decrease the continuous action, and update the continuous.",
                    "label": 1
                },
                {
                    "sent": "And based on the decision we made and the current step size Delta.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the requirements for learning binary policies are that we need a continuous admitted state space.",
                    "label": 1
                },
                {
                    "sent": "Enter discrete binary action space.",
                    "label": 1
                },
                {
                    "sent": "The good news is that most reinforcement learning algorithms can be used.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in order to learn, we need to transform a single sample from the environment to a number of transitions in veterans.",
                    "label": 0
                },
                {
                    "sent": "For veterans formed in VIA.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A simple example.",
                    "label": 0
                },
                {
                    "sent": "Would we make this more clear?",
                    "label": 0
                },
                {
                    "sent": "Let's assume that you have a continuous accent range within one and eight, and we choose a 3 bit resolution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is actually a sample of the process where we have the transitions from, say, test to status, pride prime using accent too.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is, these are the transitions that reinforcement learning algorithm will actually see.",
                    "label": 0
                },
                {
                    "sent": "We start at the middle of the reins.",
                    "label": 0
                },
                {
                    "sent": "Make binary decisions whether to increase or decrease the current action, and we are led to the continuous action too.",
                    "label": 0
                },
                {
                    "sent": "And now it is very important to note that this is the only action that our environment will actually see.",
                    "label": 0
                },
                {
                    "sent": "This is also the transition when we receive the reward and the process is discounted.",
                    "label": 0
                },
                {
                    "sent": "All these steps are undiscounted and have zero rewards.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now something that may not be obvious at first glance is that binary X inserts does not get wrapped to local Optima.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why is it so?",
                    "label": 0
                },
                {
                    "sent": "And let's assume that the this transition is optimal and has a value function.",
                    "label": 0
                },
                {
                    "sent": "Let's say Q star.",
                    "label": 0
                },
                {
                    "sent": "That value will be exactly the same as Susan Jackson increase in this state as will be using the action decrease in this state.",
                    "label": 0
                },
                {
                    "sent": "As it will be 8 Susan Laxson degree degrees in this state.",
                    "label": 0
                },
                {
                    "sent": "The value of choosing vaccine degrees in this state has nothing to do with choosing the continuous action.",
                    "label": 0
                },
                {
                    "sent": "4.5 in this state.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I will, as we've already said, binary action sets can be combined with most existing reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "The only requirement is that we need to be able to support continuous state spaces.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Also, binary search is very efficient since first needs only barring search policy and it scales logarithmically with the resolution.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see some experimental.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Result.",
                    "label": 0
                },
                {
                    "sent": "First of all, we have the inverted pendulum where our goal is to balance the pendulum at the upright position.",
                    "label": 1
                },
                {
                    "sent": "Our state space is a vertical angle and the angular velocity.",
                    "label": 1
                },
                {
                    "sent": "And in contrast to user approaches, we will have 256 equally spaced actions in minus 50 to 5050 Newtons.",
                    "label": 0
                },
                {
                    "sent": "Also, we will have uniform noise from minus 10 to 10 newtons.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our learning setup consisted of random training episodes.",
                    "label": 0
                },
                {
                    "sent": "And instead of using instead of approaching the task as a regulator task.",
                    "label": 0
                },
                {
                    "sent": "As a as an avoidance task, we approach the task as a regulator task where we have penalties both for large values of the angry angular velocity and the force that we use.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also use the block of radial basis functions, also function approximator.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are the results.",
                    "label": 0
                },
                {
                    "sent": "Here we can see the reward as a function of training episodes for various approaches, combined with list, correspondence, iteration and fitted to iteration.",
                    "label": 1
                },
                {
                    "sent": "Now we can see that there is a clear relationship between the number of actions used and the performance of its algorithm.",
                    "label": 1
                },
                {
                    "sent": "Now a. OK, these are discrete approaches for various number of actions.",
                    "label": 0
                },
                {
                    "sent": "The combined state tax and Approximator is an approximator over both the state and action space evaluated at the number of points.",
                    "label": 0
                },
                {
                    "sent": "And what we saw is that for this highly nonlinear task.",
                    "label": 0
                },
                {
                    "sent": "And as the number of actions was increased, the pendulum became more and more unstable.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "Binary concerts, both of feedback.",
                    "label": 0
                },
                {
                    "sent": "You iteration, endless queries, projects, iteration.",
                    "label": 0
                },
                {
                    "sent": "A achieves much better performance than any of the other approaches.",
                    "label": 0
                },
                {
                    "sent": "And has a very steep learning curve.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we can see force histograms for 10 years and 20 Newton noise when using the binary action search.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, binary action sets.",
                    "label": 0
                },
                {
                    "sent": "You can control the pendulum using very low mean force magnitudes.",
                    "label": 0
                },
                {
                    "sent": "And this is very important because.",
                    "label": 0
                },
                {
                    "sent": "Mini force magnitudes in a real domain would be directly related to power consumption, mechanical stresses, etc.",
                    "label": 0
                },
                {
                    "sent": "Also, once they notice and is increased by narcs inserts is still successful.",
                    "label": 0
                },
                {
                    "sent": "Almost all of the time, while discrete approaches success.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Less than half of the time.",
                    "label": 0
                },
                {
                    "sent": "Another domain that we experimented with is the double integrator domain.",
                    "label": 1
                },
                {
                    "sent": "An on this domain we control the acceleration of moving car and our state.",
                    "label": 1
                },
                {
                    "sent": "Our state is the position and the velocity.",
                    "label": 0
                },
                {
                    "sent": "Once again, this is a regulator task.",
                    "label": 1
                },
                {
                    "sent": "And getting penalties both for large values of the position and acceleration.",
                    "label": 0
                },
                {
                    "sent": "And we also used a rather large control interval in order to make the problem more challenging.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this domain we used a simple polynomial approximator.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "For the various approaches.",
                    "label": 0
                },
                {
                    "sent": "And now what we can see is that.",
                    "label": 0
                },
                {
                    "sent": "When increasing the number of actions from 5 to 11, we get better performance.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, when the number of actions is too large.",
                    "label": 1
                },
                {
                    "sent": "Meaning is quite difficult.",
                    "label": 0
                },
                {
                    "sent": "At some point probably this curve will surpass the other one.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the combined state tax and approximator on this simply linear domain performed better than the discrete approaches.",
                    "label": 0
                },
                {
                    "sent": "However, it cannot approach the performance reached by binary action search.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in order to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the strength of barracks inserts are.",
                    "label": 0
                },
                {
                    "sent": "It's implicitly the fact that it requires absolutely no tuning.",
                    "label": 0
                },
                {
                    "sent": "It requires only two actions from the from our policy.",
                    "label": 1
                },
                {
                    "sent": "And it's it achieves resolutions impossible to reach with discrete accents.",
                    "label": 1
                },
                {
                    "sent": "It can also be used with any reinforcement learning algorithm in an online offline owner policy or policy setting, it's we.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is our the fact that the state space of the process is now more complex and more samples have to be processed by our reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "An hour.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going research will where we have already achieved some very good results are.",
                    "label": 0
                },
                {
                    "sent": "Using binary search in high dimensional dimensional state space is increasing learning and execution efficiency.",
                    "label": 1
                },
                {
                    "sent": "And our future research is about planning with banner action sets and using screen functions over there.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The action Reigns.",
                    "label": 0
                },
                {
                    "sent": "That sucked.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "There are other teams action methods like allowing new sections to learn.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Actually, after I've most of the approaches that I could see, that could actually work.",
                    "label": 0
                },
                {
                    "sent": "And were relatively easy to implement.",
                    "label": 0
                },
                {
                    "sent": "Then you know the selling process seem very nice for one domain, but don't really work on another domain or work with a particular algorithm.",
                    "label": 0
                },
                {
                    "sent": "The good thing about binary search is that.",
                    "label": 0
                },
                {
                    "sent": "I can compare it with pretty much anything.",
                    "label": 0
                },
                {
                    "sent": "I can use it with any algorithm.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Spell feeling better with work in the past.",
                    "label": 0
                },
                {
                    "sent": "I'm not using Q learning in this particular research, so.",
                    "label": 0
                },
                {
                    "sent": "It couldn't possibly work.",
                    "label": 0
                },
                {
                    "sent": "So maybe it would be interesting to test.",
                    "label": 0
                },
                {
                    "sent": "Then again, binary concerts could work with the.",
                    "label": 0
                },
                {
                    "sent": "Q Learning as well.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This is a stupid question.",
                    "label": 0
                },
                {
                    "sent": "The idea the idea of how you actually use the binary choices in each moment to guarantee you have like an inner loop within each time step.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "This figure might help a bit.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It will have one transition transition with the environment.",
                    "label": 0
                },
                {
                    "sent": "Essentially, yeah, we will have a loop where we will make a number of decisions.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is what actually happens.",
                    "label": 0
                },
                {
                    "sent": "This is what we see that happens.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How is it this elected OK, we start at the middle of the action range?",
                    "label": 0
                },
                {
                    "sent": "And we make it this season whether to increase or decrease the current accident.",
                    "label": 0
                },
                {
                    "sent": "And we're led to this point.",
                    "label": 0
                },
                {
                    "sent": "Then again, a decision to increase or decrease the accident later this point.",
                    "label": 0
                },
                {
                    "sent": "And now finally, this decision will lead us to.",
                    "label": 0
                },
                {
                    "sent": "Vaccine two I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's like building a tree of possible actions and extending one node at a time.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's really answer your question.",
                    "label": 0
                },
                {
                    "sent": "Do some number of steps of this or you can do that, yeah?",
                    "label": 0
                },
                {
                    "sent": "For a 3 bit resolution, it's three steps for an N bit resolution it's N steps.",
                    "label": 0
                },
                {
                    "sent": "So how is it that this would be better then?",
                    "label": 0
                },
                {
                    "sent": "Then storing the action there rather than storing a little procedure which takes an why if we store it will have a combined state action approximator.",
                    "label": 0
                },
                {
                    "sent": "We may have a lot of local optimum local Optima.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we want to have, let's say a night with resolution 256 possible actions.",
                    "label": 0
                },
                {
                    "sent": "The way to find which of them is the best is to evaluate all of them.",
                    "label": 0
                },
                {
                    "sent": "If you make a binary search on that on the combined state tax state action approximator.",
                    "label": 0
                },
                {
                    "sent": "You might get dropped because you know if you're at the middle of the rains.",
                    "label": 0
                },
                {
                    "sent": "Maybe it looks like it's better to go left, but there is a very steep optimum.",
                    "label": 0
                },
                {
                    "sent": "And at the right so.",
                    "label": 0
                },
                {
                    "sent": "Ascentia Lee.",
                    "label": 0
                },
                {
                    "sent": "And the good thing about this is that you can achieve what the combined state tax and approximator achieves.",
                    "label": 0
                },
                {
                    "sent": "And but you only need a logarithmic number of evaluations.",
                    "label": 0
                },
                {
                    "sent": "And I will, as we have seen in the.",
                    "label": 0
                },
                {
                    "sent": "Experiments.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It appears to work.",
                    "label": 0
                },
                {
                    "sent": "A lot better than their combined state tax and approximator.",
                    "label": 0
                },
                {
                    "sent": "Now why this happens may not be very clear at first point, but by intuition we could say that.",
                    "label": 0
                },
                {
                    "sent": "This policies easier to represent for state tax and approximator.",
                    "label": 0
                },
                {
                    "sent": "While this policy is very much harder.",
                    "label": 0
                },
                {
                    "sent": "If we fail in one single evaluation for some reason.",
                    "label": 0
                },
                {
                    "sent": "The value returned is really large.",
                    "label": 0
                },
                {
                    "sent": "And we will be late there directly.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So your average works, finding the very function is convex in the action, and if it's model model then it might get the combined state tax and approximator would have such a problem.",
                    "label": 0
                },
                {
                    "sent": "Our our approach has.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have any problems like that, so the value function could be anything.",
                    "label": 0
                },
                {
                    "sent": "I mean non continuous function.",
                    "label": 0
                },
                {
                    "sent": "We are guaranteed to fight the optimum.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "OK that depends on our faction function approximator.",
                    "label": 0
                },
                {
                    "sent": "If our function approximator is not expressive enough to express such a function, we may be led to a sub optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "My dimensions.",
                    "label": 0
                },
                {
                    "sent": "An OK, the obvious naive approach would be instead of doing a binary set search, doing a quad search looking whether to increase or decrease each action at each time step so your lead to four actions in the case of two dimensions, 8 actions in the case of three dimensions, etc.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we don't really have to do this.",
                    "label": 0
                },
                {
                    "sent": "We can scale linearly to the number of actions as well.",
                    "label": 0
                },
                {
                    "sent": "We could talk offline for this.",
                    "label": 0
                },
                {
                    "sent": "With the problem earlier.",
                    "label": 0
                },
                {
                    "sent": "Predicted.",
                    "label": 0
                },
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "At that time, we could take forever to get to mix.",
                    "label": 0
                },
                {
                    "sent": "Not really.",
                    "label": 0
                },
                {
                    "sent": "Actually, yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can do in in multiple dimensions.",
                    "label": 0
                },
                {
                    "sent": "Pretty much what you do for a continuous action, so it's not really a problem.",
                    "label": 0
                },
                {
                    "sent": "We've already tried it actually so.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}