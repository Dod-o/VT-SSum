{
    "id": "hlcrwiyxb5zydzbuneqbgb6ud67srpk5",
    "title": "Regularization of Kernel Methods by Decreasing the Bandwidth of the Gaussian Kernel",
    "info": {
        "author": [
            "Jean-Philippe Vert, MINES ParisTech"
        ],
        "published": "June 11, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_vert_rkm/",
    "segmentation": [
        [
            "I need to 5:30 and guess what?",
            "I have 35 slides.",
            "But don't worry, I don't tend to talk about all of this.",
            "We got was just to have enough size.",
            "Outline something that I can.",
            "I will put them on the web later, but you can look at them becausw.",
            "I'm going to present something which is very different from what I presented earlier, which is only some theoretical work, so it's basically the field of static learning theory.",
            "The goal is to prove some consistency of some types of algorithms, so I will skip probably all the technical details 'cause he's such result there always technical details and try just to give you some images of some of the results we got.",
            "So this is John Joint work with rigid spheres with invite to my brother was in Paris University until recently and the goal is to study.",
            "It's a practical question that often occurs.",
            "You know there has been many question and if I have ACM, how do I choose C?",
            "How do I choose Sigma here?",
            "Going to play a bit with CNC mine.",
            "In fact we will try to see if we fix C. What was going on and can we have some consistency?",
            "For instance, if we just decrease Sigma so it's a bit more general than that, but this is a."
        ],
        [
            "Be sure to have in minor.",
            "So what we do is giving us spending time on motivations.",
            "Why do we do that?",
            "Why do we care about that?",
            "And giving just the main results and probably is going to be 6:00 PM and so I will skip the proofs."
        ],
        [
            "To the conclusion.",
            "But before the motivations, I need some basic notations, which Fortunately I introduced in the enrichers kernels and ARC HSS.",
            "So now you know what it is, but I would just recap it in the case of the question Colonel.",
            "So I will focus here.",
            "I'm near the Goshen RBF channel, so remember that the typical Goshen kernel K between two vectors, X&X prime, is that if this potential of minors there squared difference than all of the square difference divided by two Sigma squared and compared so here Sigma is usually referred to as the bandwidth of the kernel.",
            "Imagine a question that can be large or small.",
            "This is how Sigma is seem as large as a large portion missing.",
            "This phone is picking ocean and here just for technical reasons I don't think directed essential of this, but I divide this by constant here.",
            "The goal this is a familiar concerns, just to make sure that when you do the integral of the ocean.",
            "You end up with with one.",
            "OK, so it's a probability distribution.",
            "So this is the question.",
            "Color with bond with Sigma and you know that you should allow, you should know now that it is a positive definite camel.",
            "So to each position.",
            "See the allocations of functions and indicates of regression canola.",
            "We have some."
        ],
        [
            "Stacy dissolution of their cages as any OK chase is typically made of functions which are some of can.",
            "Also, the functions are typically sum of Gaussians plus their limits.",
            "The normal you know the square number, function, dark ages.",
            "If the function can be written as a sum of Alpha Kappa psi EXE, it's norm is the sum of AA.",
            "Jake every size J.",
            "So this is general results valid for any kernel."
        ],
        [
            "And in the case of the Gaussian kernel.",
            "We have different ways to quote to describe York Hacc bit more technical.",
            "This involves I just talked about it briefly in the prefectures.",
            "This involves the 40 transform of the function.",
            "So to be more precise here, suppose F is a function from RD2R.",
            "We define dysphoria transform if it's enjoyable.",
            "It has a frequency which is defined as a function of Omega.",
            "Omega will be picking the frequency and therefore it.",
            "Transform is just the familiar equation is the integral of and for these times extension of minus IX on Agar it's over ovaries, so this is a courtesy call for each one of them.",
            "And why do we need to wait for something 'cause we have an equivalent description of the Ark HS of the ocean operations and of the of the norm is accurate within the free transform.",
            "Be cause let's jump to the last line.",
            "We know that for a function to be, well, a function is in the Dark Ages if and only if this thing is finite and this thing will be in fact the normal Doc HS is the integral in the frequency domain, so it's the integral Omega of the 40 function.",
            "My God squared times exponential of Sigma square meters squared divided by two.",
            "So showing that this is equal to this is a bit tricky, but you can do it almost by hand, but this will be this is known.",
            "OK, so this is not new.",
            "Everybody here in the Phil knows that this is a regression or cages.",
            "So why did I do that?",
            "Becausw now let's go to SCM or 20."
        ],
        [
            "Canada, which so I would briefly recap it.",
            "Remember that's running so it cannot be thought for learning.",
            "Suppose you have a 20 set of variable XII and want to run a function F of X to predict Y.",
            "Typically kernel methods you minimize the sum of two terms, which is the average empirical loss on your training data.",
            "Plus from that time is the norm in Dark Ages.",
            "We will focus especially in the case where the loss function here is L of.",
            "So L compares tells you how much it costs to predict something instead of something else.",
            "He will focus on the case where.",
            "Why is just a binary variable so its classification can be 1 + 1 -- 1, and when you can do a real number and the loss function when you is rated instead of Y is given by the function of why you?",
            "So it's a bit abstract if it's frozen, but just think of SVM.",
            "As soon as this properties, you know when we talk about the hinge loss, how much did it cost to predict you when the class is why?",
            "While the hinge loss can be written as the Max between zero and one minus square you, so this covers a wide range of concern, like Cenergistic progression is such a process it around, so we assume that we have a function 5 and that this is what you."
        ],
        [
            "To solve.",
            "So now if you apply this running method with a Gaussian kernel, you can just plug what you know about the norm of the Ocean canal to obtain this equation.",
            "This is what you solve when you run your Asian program with the Gaussian kernel of Bandwidth Sigma an with this number.",
            "So here again usually in practical applications you choose C. There is a one form question between Lambda in C, so I could write it as C times this plus this thing without come down.",
            "OK, so Lambda is exactly equal to.",
            "What season is it?",
            "Sequel to one over another.",
            "So these are the two parameters if you want to tune in at GM, you want to tune C Orlando, which is the same and you want to Sigma.",
            "So here we like to discuss a bit theoretically why?",
            "How we should set Sigma and Lambda and for which reasons we should set them so in learning theory there is a an important problem when you design an algorithm which is to know what happens when you have more and more points.",
            "Because typically here we say that we design an algorithm that will deal with endpoints NTP read at least what we want is that if the number of points increases becomes large we would like at least installation to be increasingly good and a sense of the Kiwi like the what we compute to be to be good.",
            "So there is a notion I don't know how many of you are familiar with it, but there is an option for sense of consistency which so we said that an algorithm is consistent for some loss of species, typically for the classification error if when the number of pollen tends to Infinity then the classification error of what we estimate tends to the best possible.",
            "So of course this notion of limits has to be taken with care because the points are random, so there is a notion of convergence in probability, and to be precise we said that an algorithm is convergent.",
            "If the classification error of this nested function converges to the best possible classification error in probability.",
            "And the question of knowing when an algorithm is consistent or not has to deal with Lambda in Sigma Becausw.",
            "If you think of this Canonical, thus minimizing some loss plus summarization.",
            "In fact, the realization in theory is required to get consistency.",
            "OK, so it's required to have good results in finite dimension in order in order to get consistency of algorithms which minimize some empirical loss plus organization in all the papers I am aware of in all the work I'm aware of, I mean in the traditional way too rigorous program you need to get consistency.",
            "You obtain consistency when.",
            "You decrease the regularization as the number as the number of points increases.",
            "OK, so intuitively, if you had enough points you could just focus on the first term because the first time would be a very good approximation of the true expected loss and you would just minimize this now because in practice you don't have an infinite number of points.",
            "You regularize this by this, but the sensitivity.",
            "So in practice, if you have a SVM with an increasing number of points, you should be ready to decrease Lambda.",
            "OK, because you don't, you want to you want to trigger other days, otherwise you would not be consistent.",
            "OK, because if you hit regularising then the effect of Organism will remain after you have too many points.",
            "So most of the results.",
            "He suppose if we go back to the previous slide, then we forget about this.",
            "Now most of results when you minimize a loss.",
            "Plus in that sense organization obtain when you say that if N tends to Infinity and if Lambda decreases to zero at a good rate, then you can prove that the algorithm is consistent.",
            "There are many papers about that, and for instance there are papers that prove that FGM is consistent, typically saying OK. 10:50 if I decrease under, then the second term.",
            "It is a good range of decrease to choose.",
            "Of course, if it's too fast or too slow then it will not be consistent.",
            "But for some for some rates or decrease it will be consistent.",
            "So you have to kill this term reasonably well.",
            "In the case of the Gaussian kernel, you see that, so decreasing Lambda here means decreasing this term, but you see that if you play now with the with the Sigma of the Gaussian kernel, it also has an effect here, and it has a very precise effect, which is that Sigma is here.",
            "So if you decide to reduce signal, you will also reduce this term.",
            "So in a sense, decreasing Sigma is a is another way to decrease organization.",
            "It doesn't mean that will make it 10 to 0, but at least it will decrease.",
            "So in order to get a kind of idea of.",
            "How you should regularize and how you should kill regularization when the number of points that sweetie, let's look at a very relatively which is suppose they have endpoints.",
            "You know that if you have end points and you train this forgiven on dancing, Now you will find something.",
            "And if these endpoints you reduce them that you recognize this, then you will overfit.",
            "So here I think that when you decrease them that you will overfit and when you decrease the menu will also overfit.",
            "So this is we will have a picture now pictures now showing that.",
            "So it's very clear that there are two things you can do to overfit, and if you generalize this to a sympathy then there are two ways to kill organization.",
            "So let's look at the picture now because I know it's late so it's not as well as the pictures we had yesterday.",
            "But I have movies to show you the effect of organization, so I don't know if I. OK, I have some problem with the.",
            "OK so here you see am working and what happens is that I decrease the Lambda for increase the seat of the SVM and you see that you started from a kind of smooth function decision function and when you decrease organization for the set of points you tend to have less smooth decision function and use them.",
            "For instance here there is 1 red point here inside of blue walls.",
            "If you don't regularize, the edge will overfit.",
            "Will say that this version is red.",
            "Instead of blue ones.",
            "So in fact here the points for generated by three oceans are very simple case and this is just to to be convinced that OK, we knew it if we regularize less.",
            "If we decrease the time that we will overfit, we observe it.",
            "Something which is less of let's come on your service that suppose we keep see this weekend are fixed and we just decrease Sigma then our claim is just that we will also overfit.",
            "So it's a bit different.",
            "But let's look at the Warfield by decreasing Sigma now.",
            "So I do the same.",
            "I just computer SVM.",
            "Except that this time I will very Sigma from a large Sigma to a small Sigma.",
            "And this is what we get here.",
            "Sigma is large and then Sigma decreases.",
            "And at the end, for small silver.",
            "So it's very intuitive.",
            "You know, for small signal you fit very small portions around the points and you get something which is ready overfitting.",
            "So in a sense, when you compare the two pictures, it seems that the second one is a bit better out of 15.",
            "OK, if you really want to say I want to work in my points, is better to put very small questions around the points rather than keeping a large bandwidth address allowing the Alpha to be large.",
            "OK, and in fact so this is that there is a result.",
            "A bit surprising but true, which is that you know when we say that we said that the Goshen Canal can overfit.",
            "Everything is not true.",
            "Because just because of numerical precision, because in practice, suppose I give you a training set and I said to you, use the Goshen Canary with one with one.",
            "I try to overfit.",
            "Read this point.",
            "So try to be positive on the blue one and you got it already.",
            "Walls.",
            "In my case I could do it because I didn't have so many points.",
            "But if you have more points, there are cases where you cannot overfit, just cause you have finite precision in your computer.",
            "So innocently sweaty too.",
            "You should be in theory, when you take a kernel matrix application kernel.",
            "It's invertible, it's really invertible in theory, but if you look at the eigenvalues so none of the eigenvalues is 0, but some are 10 to the minus 200 or 10 to the minus 300, and so if you want to invert it within your computer can do it.",
            "So in a sense, it's not if you keep fixed the bandwidth of the Gaussian kernel, it's not always possible to over fit well.",
            "Points where it seems very seems really trivial to overfit when you allow to decrease Sigma.",
            "So."
        ],
        [
            "So there is therefore first motivation which is, which is a bit of theoretical consideration, which is that in all to manual educator in all consistency results about Vincent kilometres.",
            "There's been a lot of work when you fix OK and also not an integration canal.",
            "But whatever cannot you take, you can show that AGM can be consistent when other tend to zero and for this they are very all techniques.",
            "Techniques of proof that have been used.",
            "So this goes back to the 80s.",
            "That gives you ways to prove consistency when you have.",
            "Lots of shown pressurization.",
            "You kid regularization by diffusing Lambda that tricks is not is not easy, but their strategy to solve that.",
            "Now recent here I am at least of all some work that showed that if you decrease Lambda and you decrease Sigma in the same time, which is a bit natural dinner GMU regulares.",
            "But instead time when you have more points, you take smaller Goshen terminals.",
            "Then you can also prove consistency and you can even get better convergence rates.",
            "So consistent consistency tells you that you will converge to zero D ever will.",
            "Concession the best possible and if you cut the rate to which the condition takes place.",
            "If you don't own your own data contract with the Robot, Sigma two to connect with zero at precise rates, then you have better consistency rates.",
            "So what we wanted to do here is to ask the question.",
            "Well, given the pictures we had, can we get some results if for some reason you decide to keep Lambda fixed and you just play on Sigma these are.",
            "This is the first motivation is a bit sceptical program?",
            "Can we get some consistency just by using signal?",
            "So you might say, OK, well, it's nice to play with such questions.",
            "We can invent many situations, but this question is that."
        ],
        [
            "President for one algorithm, very important from an algorithm which is called the one class support vector machine.",
            "Roughly speaking, I will not tell the details, but it's one cost at the end is a bit like at the end he said that there only one class, so what's the application of this?",
            "The goal is here.",
            "The goal is not to do supervised classification.",
            "You don't assume that you have two classes and you want separated class.",
            "Here you just have a set of points.",
            "Look at this picture.",
            "You have blue see occurs.",
            "This is a cloud of points and the goal is to find a region which is Martin in black with a black line.",
            "Here the goal is just to find the region around the points.",
            "That is intuitively as small as possible, but that contains enough points, so this is very useful.",
            "Typically for outlier detection or for novelty detection.",
            "So given this point, these are actually points we want to find a region of confidence, so that's when you have new points.",
            "If there are inside of division, you would say there are normal, and if they are outside, we said that they are not normal.",
            "There will be some alert ringing cetera.",
            "So this form of attention is very important in industry and in many fields and.",
            "And one algorithm to solve that which seems to work very well, which is using in some industries I'm aware of is the one class SVM, so the one classism is funny because it's very similar to the two classes.",
            "Yeah, he said that you use your program for SVM, but instead of saying that you have, so you have your input points, you give them, you hit them all the same label Y equals, press one and you run USM and you will be able to find this version.",
            "So how does it work?",
            "Well if you look at this formulation, you can have an idea of what it is you see.",
            "That's what you try to minimize.",
            "This is the empirical loss when there is no label, so other labels are plus one and this is the regularization.",
            "So what you're trying to find the function F that is smooth and what's important is that here it's in the Goshen arcade chess.",
            "So here a big difference with the two classes in is that you don't give a function plus a constant in normal.",
            "Again, you're looking for F equals the valuation plus B.",
            "Here is just F equals the value is there is no offset B.",
            "And it's important because all functions integration approaches tend to zero at Infinity.",
            "So if you want to have a small norm, it will be 0 at Infinity, but it cannot be 0 everywhere be cause this function penalizes each of the point is I you have a loss as soon as the value of F on the point is below 1.",
            "So basically imagine at this feature what you're looking for is a function that is above one on most of the points, but that has a small norm, which means that it will tend to zero and Infinity.",
            "And at the end, when you, when you fix a Lambda, you will get such a shape and this blue line is simply the level set of the function equal to 1.",
            "So this this algorithm is not do it exists, but what's important and what is it?",
            "5 very important here is that you see here, Lambda plays a quite different role than SVM because the role of gender is not regularised problem.",
            "The role of Lambda here is to decide what is the proportion of policy want outside.",
            "OK, so there is.",
            "You can show that there is a connection between the choice of Lambda there and the quantity of the percentage of points that will be left outside.",
            "So typically if your problem is can you design me decision functions or region such that 95% of the points will be inside this request form to a Lambda.",
            "And so when the number of points tend to Infinity you will not want to end up with increased to 0 because the 95% with will always be the same.",
            "So will never try to decrease them that you will just observe that as the number of points tend to Infinity, this will converge to something that's rough and so there is for this algorithm.",
            "If you want the form of consistency for these adverse and mystically well as the number of points increases, what is the volume we get?",
            "What is the decision function we get and in particular what we would like is suppose the points are generating according to a distribution.",
            "Is the decision function converging toward what is called?",
            "A quantity that is a region of of minimum lebeck volume for that has a given probability.",
            "So so here we have some background distribution for the points we want.",
            "A sensitive people would want to find the region of the Euclidean space.",
            "I know it's Friday and it's 5:30 and guess what?",
            "I have 35 slides.",
            "But don't worry, I don't tend to talk about all of this.",
            "We got was just have enough size.",
            "Outline something that I can.",
            "I will put them on the web later, but you can look at them becausw.",
            "I'm going to present something which is very different from what I presented earlier, which is only some theoretical work, so it's basically the field of static learning theory.",
            "The goal is to prove some consistency of some types of algorithms, so I will skip probably all the technical details 'cause he's such result there always technical details and try just to give you some images of some of the results we got.",
            "So this is John Joint work with register two is in fact that my brother was in Paris University until recently and the goal is to study its approach."
        ],
        [
            "People question that often occurs.",
            "You know there has been many question and if I have ACM, how do I choose C?",
            "How do I choose Sigma here?",
            "Going to play a bit with CNC mine.",
            "In fact we will try to see if we fix C. What was going on and can we have some consistent difference if we just decrease Sigma so it's a bit more general than that, but this is a picture to have in my."
        ],
        [
            "So finally what I will do is giving us spending time on motivations.",
            "Why do we do that?",
            "Why do we care about that?",
            "And giving just the main results and probably is going to be 6:00 PM and so I will skip the proofs, go to the conclusion.",
            "But before the motivations, I need some basic notations, which Fortunately I introduced in the enrichers kernels and RKH sic.",
            "Now you know what it is, but I would just recap it in the case of the question Colonel.",
            "So I will focus here.",
            "I'm near the Goshen RBF channel, so remember that the typical Goshen kernel K between two vectors, X&X prime, is that if this potential of minors there squared difference than all of the square difference divided by two Sigma squared and compared so here Sigma is usually referred to as the bandwidth of the kernel.",
            "Imagine a question that can be large or small.",
            "This is how similar it seem as large as a large portion is significantly picking ocean, and here just for technical reasons, I don't take directed essential of this, but I divide this by constant here.",
            "The goal this is a familiar concerns, just to make sure that when you do the integral of the ocean.",
            "You end up with with one.",
            "OK, so it's a probability distribution.",
            "So this is the question.",
            "Color with Bandwidth Sigma and you know that you should allow, you should know now that it is a positive definite camel.",
            "So to each position.",
            "See, the allocations are functions and in the case of the Ocean Canal we have some explicit decision of their cages as any OK chase is typically made of functions which are some of can.",
            "Also the functions of the sum of Gaussians plus their limits.",
            "The normal you know the square number function."
        ],
        [
            "As if the function can be written as the sum of us vicariously, EXE is norm is the sum of AA Jake every size J.",
            "So this is general results valid for any kernel.",
            "And in the case of the Gaussian kernel.",
            "We have different ways to quote to describe York Hacc bit more technical.",
            "This involves I just talked about it briefly in the prefectures.",
            "This involves the 40 transform of the function.",
            "So to be more precise here, suppose F is a function from RD2R.",
            "We define dysphoria transformed if it is doable.",
            "It as a free transform which is defined as a function of Omega.",
            "Omega will be speaking the frequency and therefore it transform is just the familiar equation is the integral of F of X times extension of minus I Exxon Agar.",
            "It's over ovaries, so this is a courtesy call for each one of us.",
            "And why do we need to wait for something 'cause we have an equivalent description of the Ark HS of the Goshen operations and of the of the norm is accurate within the free transform.",
            "Because let's jump to the last line, we know that for a function to be, well, a function is in their cages if and only if this thing is finite, and this thing will be in fact the normal Doc HS is the integral in the frequency domain, so it's the integral Omega of the 40 function.",
            "We got squared times exponential of Sigma square meters squared divided by two.",
            "So showing that the."
        ],
        [
            "Is equal to.",
            "This is a bit tricky, but you can do it almost by hand, but this will be this is known.",
            "OK, so this is not new.",
            "Everybody here in the Phil knows that this is a regression or cages.",
            "So why did I do that?",
            "Becausw now let's go to SCM or 20 Canon Masada, which so I will briefly recap it.",
            "Remember that learning so I can only thought for learning.",
            "Suppose you have a 20 set of variable XII and want to run a function F of X to predict Y.",
            "Typically kernel methods to minimize the sum of two terms, which is the average empirical loss on your training data.",
            "Plus from that time is the norm in Dark Ages."
        ],
        [
            "OK, we will focus, especially in the case where the loss function here is.",
            "So L compares tells you how much it costs to predict something instead of something else.",
            "He will focus on the case where Y is just a binary variable, so its classification can be 1 + 1 -- 1.",
            "And when you can do a real number and the loss function when you is relative instead of Y is given by the function of Yu.",
            "So it's a bit of struct if is a for some, but just think of them as these properties.",
            "You know when we talk about the hinge loss, how much does it cost to predict you when the classes Y while the hinge loss can be written as the Max between zero and one minus square you?",
            "So this covers a wide range of concern, like Cenergistic progression is such a process it around, so we assume that we have a function 5 and that this is what you want to solve.",
            "So now if you apply this running method with a Gaussian kernel, you can just plug what you know about the norm of the Goshen kernel to obtain this equation.",
            "This is what you solve when you run your Asian program with the Gaussian kernel of."
        ],
        [
            "One with Sigma an with this number.",
            "So here again, usually in practical applications you choose C. There is a one question between London C, So I could write it as C times this plus this thing without come down.",
            "OK, so Lambda is exactly equal to.",
            "Is it a quote one over another?",
            "So these are the two parameters if you want to tune in at GM, you want to tune C Orlando, which is the same and you want to Sigma.",
            "So here I would like to discuss a bit theoretically why?"
        ],
        [
            "How we should set cinema in London?",
            "For which reasons we should set them so in learning theory there is a an important problem.",
            "When you design an algorithm, which is to know what happens when you have more and more points.",
            "Because typically here we say that we design an algorithm that will deal with endpoints and typically the at least what we want is that if the number of points increases becomes large we would like at least installation to be."
        ],
        [
            "Increasingly, Gouda and a sense of the Kiwi like the what we compute to be to be good.",
            "So there is a notion I don't know how many of you are familiar with it, but there is a national sense of consistency which so we said that an algorithm is consistent for some loss of species, typically for the classification error if when you when the number of pollen tends to Infinity, then the classification error of what we estimate tends to the best possible."
        ],
        [
            "So of course, this notion of limits has to be taken with care because the points are random, so there is a notion of convergence in probability.",
            "And to be precise, we said that an algorithm is convergent if the classification error of estimated function converges to the best possible classification error in probability.",
            "And the question of of knowing when an algorithm is consistent or not has to deal with Lambda in Sigma becausw.",
            "If you think of this kind of thoughts as minimizing some loss plus some organization, in fact the realisation in theory is required to get consistency.",
            "OK, so it's required to have good results in finite dimension an in order in order to get consistency of algorithms which minimize some controversial loss plus organization in all the papers, I am away."
        ],
        [
            "Are often all the work, I'm."
        ],
        [
            "Well, I mean in the traditional way to rigorous program, you need to get consistency.",
            "You obtain consistency when you decrease the regularization as the number as the number of points increases.",
            "OK, so intuitively, if you had enough points, you could just focus on the first term, because the first time would be a very good approximation of the true expected loss, and you would just minimize this now, because in practice you don't have an infinite number of points.",
            "You regularize this by this.",
            "But the sensitivity.",
            "So in practice, if you have a SVM with an increasing number of points, you should be ready to decrease Lambda.",
            "OK, because you don't, you want to you want to trigger other days, otherwise you would not be consistent.",
            "OK, because if you hit regularising then the effect of programs will remain after you have too many points.",
            "So most of the results.",
            "He suppose if we go back to the previous slide, then we forget about this.",
            "Now most of results when you minimize a loss.",
            "Plus in that sense organization obtain when you say that if N tends to Infinity and if Lambda decreases to zero at a good rate, then you can prove that the algorithm is consistent.",
            "There are many papers about that, and for instance there are papers that prove that FGM is consistent, typically saying OK. 10:50 if I decrease on that."
        ],
        [
            "Then the second term.",
            "So there is a good range of decrease to choose.",
            "Of course, if it's too fast or too slow, then it will not be consistent, but for some for some rates or decrease it will be consistent.",
            "So you have to kill this term reasonably well.",
            "In the case of the Gaussian kernel, you see that, so decreasing Lambda here means decreasing this term, but you see that if you play now with the with the signal of the Gaussian kernel, it also has an effect here, and it has a very precise effect, which is that Sigma is here.",
            "So if you decide to reduce signal, you will also reduce this term.",
            "So in a sense, decreasing Sigma is a is another way to decrease organization.",
            "It doesn't mean that will make it 10 to 0, but at least it will decrease.",
            "So in order to get a kind of idea of.",
            "How you should regularize and how you should kill regularization when the number of points that 20?",
            "Let's look at a very relatively which is suppose they have endpoints.",
            "You know that if you have end points and you train this forgiven on dancing, Now you will find something.",
            "And if these endpoints you reduce them that you recognize this, then you will overfit.",
            "So here I think that when you decrease them that you will overfit and when you decrease the menu will also overfit.",
            "So this is we will have a picture now pictures now showing that.",
            "So it's very clear that there are two things you can do to overfit, and if you generalize this to US entities and there are two ways to kill organization, so let's look at the picture now because I know it's late so it's not as well as the pictures we had yesterday.",
            "But I have movies to show you the effect of organization, so I don't know if I.",
            "And.",
            "OK, I have some problem with the.",
            "OK so here you see am working and what happens is that I decrease the Lambda for increase the seat of the SVM and you see that you started from a kind of smooth function decision function and when you decrease organization for the set of points you tend to have less smooth decision function and use them.",
            "For instance here there is 1 red point here inside of blue ones.",
            "If you don't regularize, the engine will overfit.",
            "Will say that this version is red.",
            "Inside of the blue ones.",
            "So in fact here the points for generated by three oceans are very simple case and this is just to to be convinced that OK, we knew it if we regularize less.",
            "If we decrease the time that we will overfit, we observe it.",
            "Something which is less of let's come on your service that suppose we keep see this week another fixed and we just decrease Sigma then our claim is just that we will also overfit.",
            "So it's a bit different.",
            "But let's look at the Warfield by decreasing Sigma now.",
            "So I do the same.",
            "I just computer SVM except this time I will very Sigma from a large team at with Small Sigma.",
            "And this is what we get here.",
            "Sigma is large and then Sigma decreases.",
            "And at the end, for small silver.",
            "So it's very intuitive.",
            "You know, for small signal feed very small Gaussians around the points and you get something which is ready overfitting.",
            "So in a sense, when you compare the two pictures, it seems that the second one is a bit better out of a 15.",
            "OK.",
            "If you really want to say I want to work in my points is better to put very small questions around the points rather than keeping a large bandwidth address allowing the Alpha to be large.",
            "OK, and in fact so this is that there is a result.",
            "A bit surprising but true, which is that you know when we say that we said that the Goshen Canal can overfit.",
            "Everything is not true.",
            "Because just because of numerical precision, because in practice, suppose I give you a training set and I said to you, use the Goshen Canary with bandwidth one.",
            "I try to overfit rate this point.",
            "So try to be positive on the blue.",
            "On a negative already walls.",
            "In my case I could do it because I didn't have so many points.",
            "But if you have more points, there are cases where you cannot overfit, just cause you have finite precision in your computer.",
            "So in a sense it's ready to you should be in theory when you take a kernel matrix application kernel.",
            "It's invertible, it's really invertible in theory, but if you look at the eigenvalues so none of the eigenvalues is 0, but some are 10 to the minus 200 or 10 to the minus 300, and so if you want to invert it with your computer, you can do it.",
            "So in a sense, it's not if you keep fixed the bandwidth of the Gaussian kernel, it's not always possible to over fit well.",
            "Points where it seems very.",
            "Trivial to overfit when you allow to decrease Simba.",
            "So, so there is therefore first motivation, which is which is a bit of theoretical consideration, which is that in all to manual educator in all consistency results about Vincent kilometres.",
            "There's been a lot of work when you fix OK and also not an integration canal.",
            "But whatever cannot you take, you can show that AGM can be consistent.",
            "When not that tend to zero.",
            "And for this there are very all techniques, techniques of proof that have been used.",
            "So this goes back to the 80s.",
            "That gives you three ways to prove consistency when you have lots of shown pressurization.",
            "You kid realization by using Lambda that tricks is not is not easy, but their strategy to solve that.",
            "Now recent here I am at least for some work that showed that if you decrease Lambda and you decrease Sigma in the same time, which is a bit natural dinner GMU regularly."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I need to 5:30 and guess what?",
                    "label": 0
                },
                {
                    "sent": "I have 35 slides.",
                    "label": 0
                },
                {
                    "sent": "But don't worry, I don't tend to talk about all of this.",
                    "label": 0
                },
                {
                    "sent": "We got was just to have enough size.",
                    "label": 0
                },
                {
                    "sent": "Outline something that I can.",
                    "label": 0
                },
                {
                    "sent": "I will put them on the web later, but you can look at them becausw.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present something which is very different from what I presented earlier, which is only some theoretical work, so it's basically the field of static learning theory.",
                    "label": 0
                },
                {
                    "sent": "The goal is to prove some consistency of some types of algorithms, so I will skip probably all the technical details 'cause he's such result there always technical details and try just to give you some images of some of the results we got.",
                    "label": 0
                },
                {
                    "sent": "So this is John Joint work with rigid spheres with invite to my brother was in Paris University until recently and the goal is to study.",
                    "label": 1
                },
                {
                    "sent": "It's a practical question that often occurs.",
                    "label": 0
                },
                {
                    "sent": "You know there has been many question and if I have ACM, how do I choose C?",
                    "label": 0
                },
                {
                    "sent": "How do I choose Sigma here?",
                    "label": 0
                },
                {
                    "sent": "Going to play a bit with CNC mine.",
                    "label": 0
                },
                {
                    "sent": "In fact we will try to see if we fix C. What was going on and can we have some consistency?",
                    "label": 0
                },
                {
                    "sent": "For instance, if we just decrease Sigma so it's a bit more general than that, but this is a.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be sure to have in minor.",
                    "label": 0
                },
                {
                    "sent": "So what we do is giving us spending time on motivations.",
                    "label": 0
                },
                {
                    "sent": "Why do we do that?",
                    "label": 0
                },
                {
                    "sent": "Why do we care about that?",
                    "label": 0
                },
                {
                    "sent": "And giving just the main results and probably is going to be 6:00 PM and so I will skip the proofs.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the conclusion.",
                    "label": 0
                },
                {
                    "sent": "But before the motivations, I need some basic notations, which Fortunately I introduced in the enrichers kernels and ARC HSS.",
                    "label": 0
                },
                {
                    "sent": "So now you know what it is, but I would just recap it in the case of the question Colonel.",
                    "label": 0
                },
                {
                    "sent": "So I will focus here.",
                    "label": 0
                },
                {
                    "sent": "I'm near the Goshen RBF channel, so remember that the typical Goshen kernel K between two vectors, X&X prime, is that if this potential of minors there squared difference than all of the square difference divided by two Sigma squared and compared so here Sigma is usually referred to as the bandwidth of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Imagine a question that can be large or small.",
                    "label": 0
                },
                {
                    "sent": "This is how Sigma is seem as large as a large portion missing.",
                    "label": 0
                },
                {
                    "sent": "This phone is picking ocean and here just for technical reasons I don't think directed essential of this, but I divide this by constant here.",
                    "label": 0
                },
                {
                    "sent": "The goal this is a familiar concerns, just to make sure that when you do the integral of the ocean.",
                    "label": 0
                },
                {
                    "sent": "You end up with with one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is the question.",
                    "label": 0
                },
                {
                    "sent": "Color with bond with Sigma and you know that you should allow, you should know now that it is a positive definite camel.",
                    "label": 0
                },
                {
                    "sent": "So to each position.",
                    "label": 0
                },
                {
                    "sent": "See the allocations of functions and indicates of regression canola.",
                    "label": 0
                },
                {
                    "sent": "We have some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stacy dissolution of their cages as any OK chase is typically made of functions which are some of can.",
                    "label": 0
                },
                {
                    "sent": "Also, the functions are typically sum of Gaussians plus their limits.",
                    "label": 0
                },
                {
                    "sent": "The normal you know the square number, function, dark ages.",
                    "label": 0
                },
                {
                    "sent": "If the function can be written as a sum of Alpha Kappa psi EXE, it's norm is the sum of AA.",
                    "label": 0
                },
                {
                    "sent": "Jake every size J.",
                    "label": 0
                },
                {
                    "sent": "So this is general results valid for any kernel.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the case of the Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "We have different ways to quote to describe York Hacc bit more technical.",
                    "label": 0
                },
                {
                    "sent": "This involves I just talked about it briefly in the prefectures.",
                    "label": 0
                },
                {
                    "sent": "This involves the 40 transform of the function.",
                    "label": 0
                },
                {
                    "sent": "So to be more precise here, suppose F is a function from RD2R.",
                    "label": 0
                },
                {
                    "sent": "We define dysphoria transform if it's enjoyable.",
                    "label": 0
                },
                {
                    "sent": "It has a frequency which is defined as a function of Omega.",
                    "label": 0
                },
                {
                    "sent": "Omega will be picking the frequency and therefore it.",
                    "label": 0
                },
                {
                    "sent": "Transform is just the familiar equation is the integral of and for these times extension of minus IX on Agar it's over ovaries, so this is a courtesy call for each one of them.",
                    "label": 0
                },
                {
                    "sent": "And why do we need to wait for something 'cause we have an equivalent description of the Ark HS of the ocean operations and of the of the norm is accurate within the free transform.",
                    "label": 0
                },
                {
                    "sent": "Be cause let's jump to the last line.",
                    "label": 0
                },
                {
                    "sent": "We know that for a function to be, well, a function is in the Dark Ages if and only if this thing is finite and this thing will be in fact the normal Doc HS is the integral in the frequency domain, so it's the integral Omega of the 40 function.",
                    "label": 0
                },
                {
                    "sent": "My God squared times exponential of Sigma square meters squared divided by two.",
                    "label": 0
                },
                {
                    "sent": "So showing that this is equal to this is a bit tricky, but you can do it almost by hand, but this will be this is known.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not new.",
                    "label": 0
                },
                {
                    "sent": "Everybody here in the Phil knows that this is a regression or cages.",
                    "label": 0
                },
                {
                    "sent": "So why did I do that?",
                    "label": 0
                },
                {
                    "sent": "Becausw now let's go to SCM or 20.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Canada, which so I would briefly recap it.",
                    "label": 0
                },
                {
                    "sent": "Remember that's running so it cannot be thought for learning.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a 20 set of variable XII and want to run a function F of X to predict Y.",
                    "label": 0
                },
                {
                    "sent": "Typically kernel methods you minimize the sum of two terms, which is the average empirical loss on your training data.",
                    "label": 0
                },
                {
                    "sent": "Plus from that time is the norm in Dark Ages.",
                    "label": 0
                },
                {
                    "sent": "We will focus especially in the case where the loss function here is L of.",
                    "label": 0
                },
                {
                    "sent": "So L compares tells you how much it costs to predict something instead of something else.",
                    "label": 0
                },
                {
                    "sent": "He will focus on the case where.",
                    "label": 0
                },
                {
                    "sent": "Why is just a binary variable so its classification can be 1 + 1 -- 1, and when you can do a real number and the loss function when you is rated instead of Y is given by the function of why you?",
                    "label": 0
                },
                {
                    "sent": "So it's a bit abstract if it's frozen, but just think of SVM.",
                    "label": 0
                },
                {
                    "sent": "As soon as this properties, you know when we talk about the hinge loss, how much did it cost to predict you when the class is why?",
                    "label": 0
                },
                {
                    "sent": "While the hinge loss can be written as the Max between zero and one minus square you, so this covers a wide range of concern, like Cenergistic progression is such a process it around, so we assume that we have a function 5 and that this is what you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve.",
                    "label": 0
                },
                {
                    "sent": "So now if you apply this running method with a Gaussian kernel, you can just plug what you know about the norm of the Ocean canal to obtain this equation.",
                    "label": 1
                },
                {
                    "sent": "This is what you solve when you run your Asian program with the Gaussian kernel of Bandwidth Sigma an with this number.",
                    "label": 0
                },
                {
                    "sent": "So here again usually in practical applications you choose C. There is a one form question between Lambda in C, so I could write it as C times this plus this thing without come down.",
                    "label": 0
                },
                {
                    "sent": "OK, so Lambda is exactly equal to.",
                    "label": 0
                },
                {
                    "sent": "What season is it?",
                    "label": 0
                },
                {
                    "sent": "Sequel to one over another.",
                    "label": 0
                },
                {
                    "sent": "So these are the two parameters if you want to tune in at GM, you want to tune C Orlando, which is the same and you want to Sigma.",
                    "label": 0
                },
                {
                    "sent": "So here we like to discuss a bit theoretically why?",
                    "label": 0
                },
                {
                    "sent": "How we should set Sigma and Lambda and for which reasons we should set them so in learning theory there is a an important problem when you design an algorithm which is to know what happens when you have more and more points.",
                    "label": 0
                },
                {
                    "sent": "Because typically here we say that we design an algorithm that will deal with endpoints NTP read at least what we want is that if the number of points increases becomes large we would like at least installation to be increasingly good and a sense of the Kiwi like the what we compute to be to be good.",
                    "label": 0
                },
                {
                    "sent": "So there is a notion I don't know how many of you are familiar with it, but there is an option for sense of consistency which so we said that an algorithm is consistent for some loss of species, typically for the classification error if when the number of pollen tends to Infinity then the classification error of what we estimate tends to the best possible.",
                    "label": 0
                },
                {
                    "sent": "So of course this notion of limits has to be taken with care because the points are random, so there is a notion of convergence in probability, and to be precise we said that an algorithm is convergent.",
                    "label": 0
                },
                {
                    "sent": "If the classification error of this nested function converges to the best possible classification error in probability.",
                    "label": 0
                },
                {
                    "sent": "And the question of knowing when an algorithm is consistent or not has to deal with Lambda in Sigma Becausw.",
                    "label": 0
                },
                {
                    "sent": "If you think of this Canonical, thus minimizing some loss plus summarization.",
                    "label": 0
                },
                {
                    "sent": "In fact, the realization in theory is required to get consistency.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's required to have good results in finite dimension in order in order to get consistency of algorithms which minimize some empirical loss plus organization in all the papers I am aware of in all the work I'm aware of, I mean in the traditional way too rigorous program you need to get consistency.",
                    "label": 0
                },
                {
                    "sent": "You obtain consistency when.",
                    "label": 0
                },
                {
                    "sent": "You decrease the regularization as the number as the number of points increases.",
                    "label": 0
                },
                {
                    "sent": "OK, so intuitively, if you had enough points you could just focus on the first term because the first time would be a very good approximation of the true expected loss and you would just minimize this now because in practice you don't have an infinite number of points.",
                    "label": 0
                },
                {
                    "sent": "You regularize this by this, but the sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So in practice, if you have a SVM with an increasing number of points, you should be ready to decrease Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, because you don't, you want to you want to trigger other days, otherwise you would not be consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, because if you hit regularising then the effect of Organism will remain after you have too many points.",
                    "label": 1
                },
                {
                    "sent": "So most of the results.",
                    "label": 0
                },
                {
                    "sent": "He suppose if we go back to the previous slide, then we forget about this.",
                    "label": 0
                },
                {
                    "sent": "Now most of results when you minimize a loss.",
                    "label": 0
                },
                {
                    "sent": "Plus in that sense organization obtain when you say that if N tends to Infinity and if Lambda decreases to zero at a good rate, then you can prove that the algorithm is consistent.",
                    "label": 0
                },
                {
                    "sent": "There are many papers about that, and for instance there are papers that prove that FGM is consistent, typically saying OK. 10:50 if I decrease under, then the second term.",
                    "label": 0
                },
                {
                    "sent": "It is a good range of decrease to choose.",
                    "label": 0
                },
                {
                    "sent": "Of course, if it's too fast or too slow then it will not be consistent.",
                    "label": 0
                },
                {
                    "sent": "But for some for some rates or decrease it will be consistent.",
                    "label": 0
                },
                {
                    "sent": "So you have to kill this term reasonably well.",
                    "label": 0
                },
                {
                    "sent": "In the case of the Gaussian kernel, you see that, so decreasing Lambda here means decreasing this term, but you see that if you play now with the with the Sigma of the Gaussian kernel, it also has an effect here, and it has a very precise effect, which is that Sigma is here.",
                    "label": 0
                },
                {
                    "sent": "So if you decide to reduce signal, you will also reduce this term.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, decreasing Sigma is a is another way to decrease organization.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that will make it 10 to 0, but at least it will decrease.",
                    "label": 0
                },
                {
                    "sent": "So in order to get a kind of idea of.",
                    "label": 0
                },
                {
                    "sent": "How you should regularize and how you should kill regularization when the number of points that sweetie, let's look at a very relatively which is suppose they have endpoints.",
                    "label": 0
                },
                {
                    "sent": "You know that if you have end points and you train this forgiven on dancing, Now you will find something.",
                    "label": 0
                },
                {
                    "sent": "And if these endpoints you reduce them that you recognize this, then you will overfit.",
                    "label": 0
                },
                {
                    "sent": "So here I think that when you decrease them that you will overfit and when you decrease the menu will also overfit.",
                    "label": 0
                },
                {
                    "sent": "So this is we will have a picture now pictures now showing that.",
                    "label": 0
                },
                {
                    "sent": "So it's very clear that there are two things you can do to overfit, and if you generalize this to a sympathy then there are two ways to kill organization.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the picture now because I know it's late so it's not as well as the pictures we had yesterday.",
                    "label": 0
                },
                {
                    "sent": "But I have movies to show you the effect of organization, so I don't know if I. OK, I have some problem with the.",
                    "label": 0
                },
                {
                    "sent": "OK so here you see am working and what happens is that I decrease the Lambda for increase the seat of the SVM and you see that you started from a kind of smooth function decision function and when you decrease organization for the set of points you tend to have less smooth decision function and use them.",
                    "label": 0
                },
                {
                    "sent": "For instance here there is 1 red point here inside of blue walls.",
                    "label": 0
                },
                {
                    "sent": "If you don't regularize, the edge will overfit.",
                    "label": 0
                },
                {
                    "sent": "Will say that this version is red.",
                    "label": 0
                },
                {
                    "sent": "Instead of blue ones.",
                    "label": 0
                },
                {
                    "sent": "So in fact here the points for generated by three oceans are very simple case and this is just to to be convinced that OK, we knew it if we regularize less.",
                    "label": 0
                },
                {
                    "sent": "If we decrease the time that we will overfit, we observe it.",
                    "label": 0
                },
                {
                    "sent": "Something which is less of let's come on your service that suppose we keep see this weekend are fixed and we just decrease Sigma then our claim is just that we will also overfit.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit different.",
                    "label": 0
                },
                {
                    "sent": "But let's look at the Warfield by decreasing Sigma now.",
                    "label": 0
                },
                {
                    "sent": "So I do the same.",
                    "label": 0
                },
                {
                    "sent": "I just computer SVM.",
                    "label": 0
                },
                {
                    "sent": "Except that this time I will very Sigma from a large Sigma to a small Sigma.",
                    "label": 0
                },
                {
                    "sent": "And this is what we get here.",
                    "label": 0
                },
                {
                    "sent": "Sigma is large and then Sigma decreases.",
                    "label": 0
                },
                {
                    "sent": "And at the end, for small silver.",
                    "label": 0
                },
                {
                    "sent": "So it's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "You know, for small signal you fit very small portions around the points and you get something which is ready overfitting.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, when you compare the two pictures, it seems that the second one is a bit better out of 15.",
                    "label": 0
                },
                {
                    "sent": "OK, if you really want to say I want to work in my points, is better to put very small questions around the points rather than keeping a large bandwidth address allowing the Alpha to be large.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact so this is that there is a result.",
                    "label": 0
                },
                {
                    "sent": "A bit surprising but true, which is that you know when we say that we said that the Goshen Canal can overfit.",
                    "label": 0
                },
                {
                    "sent": "Everything is not true.",
                    "label": 0
                },
                {
                    "sent": "Because just because of numerical precision, because in practice, suppose I give you a training set and I said to you, use the Goshen Canary with one with one.",
                    "label": 0
                },
                {
                    "sent": "I try to overfit.",
                    "label": 0
                },
                {
                    "sent": "Read this point.",
                    "label": 0
                },
                {
                    "sent": "So try to be positive on the blue one and you got it already.",
                    "label": 0
                },
                {
                    "sent": "Walls.",
                    "label": 0
                },
                {
                    "sent": "In my case I could do it because I didn't have so many points.",
                    "label": 0
                },
                {
                    "sent": "But if you have more points, there are cases where you cannot overfit, just cause you have finite precision in your computer.",
                    "label": 0
                },
                {
                    "sent": "So innocently sweaty too.",
                    "label": 0
                },
                {
                    "sent": "You should be in theory, when you take a kernel matrix application kernel.",
                    "label": 0
                },
                {
                    "sent": "It's invertible, it's really invertible in theory, but if you look at the eigenvalues so none of the eigenvalues is 0, but some are 10 to the minus 200 or 10 to the minus 300, and so if you want to invert it within your computer can do it.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, it's not if you keep fixed the bandwidth of the Gaussian kernel, it's not always possible to over fit well.",
                    "label": 0
                },
                {
                    "sent": "Points where it seems very seems really trivial to overfit when you allow to decrease Sigma.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is therefore first motivation which is, which is a bit of theoretical consideration, which is that in all to manual educator in all consistency results about Vincent kilometres.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work when you fix OK and also not an integration canal.",
                    "label": 0
                },
                {
                    "sent": "But whatever cannot you take, you can show that AGM can be consistent when other tend to zero and for this they are very all techniques.",
                    "label": 0
                },
                {
                    "sent": "Techniques of proof that have been used.",
                    "label": 0
                },
                {
                    "sent": "So this goes back to the 80s.",
                    "label": 0
                },
                {
                    "sent": "That gives you ways to prove consistency when you have.",
                    "label": 0
                },
                {
                    "sent": "Lots of shown pressurization.",
                    "label": 0
                },
                {
                    "sent": "You kid regularization by diffusing Lambda that tricks is not is not easy, but their strategy to solve that.",
                    "label": 0
                },
                {
                    "sent": "Now recent here I am at least of all some work that showed that if you decrease Lambda and you decrease Sigma in the same time, which is a bit natural dinner GMU regulares.",
                    "label": 0
                },
                {
                    "sent": "But instead time when you have more points, you take smaller Goshen terminals.",
                    "label": 0
                },
                {
                    "sent": "Then you can also prove consistency and you can even get better convergence rates.",
                    "label": 0
                },
                {
                    "sent": "So consistent consistency tells you that you will converge to zero D ever will.",
                    "label": 0
                },
                {
                    "sent": "Concession the best possible and if you cut the rate to which the condition takes place.",
                    "label": 0
                },
                {
                    "sent": "If you don't own your own data contract with the Robot, Sigma two to connect with zero at precise rates, then you have better consistency rates.",
                    "label": 0
                },
                {
                    "sent": "So what we wanted to do here is to ask the question.",
                    "label": 0
                },
                {
                    "sent": "Well, given the pictures we had, can we get some results if for some reason you decide to keep Lambda fixed and you just play on Sigma these are.",
                    "label": 0
                },
                {
                    "sent": "This is the first motivation is a bit sceptical program?",
                    "label": 0
                },
                {
                    "sent": "Can we get some consistency just by using signal?",
                    "label": 0
                },
                {
                    "sent": "So you might say, OK, well, it's nice to play with such questions.",
                    "label": 0
                },
                {
                    "sent": "We can invent many situations, but this question is that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "President for one algorithm, very important from an algorithm which is called the one class support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, I will not tell the details, but it's one cost at the end is a bit like at the end he said that there only one class, so what's the application of this?",
                    "label": 0
                },
                {
                    "sent": "The goal is here.",
                    "label": 0
                },
                {
                    "sent": "The goal is not to do supervised classification.",
                    "label": 0
                },
                {
                    "sent": "You don't assume that you have two classes and you want separated class.",
                    "label": 0
                },
                {
                    "sent": "Here you just have a set of points.",
                    "label": 0
                },
                {
                    "sent": "Look at this picture.",
                    "label": 0
                },
                {
                    "sent": "You have blue see occurs.",
                    "label": 0
                },
                {
                    "sent": "This is a cloud of points and the goal is to find a region which is Martin in black with a black line.",
                    "label": 0
                },
                {
                    "sent": "Here the goal is just to find the region around the points.",
                    "label": 0
                },
                {
                    "sent": "That is intuitively as small as possible, but that contains enough points, so this is very useful.",
                    "label": 0
                },
                {
                    "sent": "Typically for outlier detection or for novelty detection.",
                    "label": 1
                },
                {
                    "sent": "So given this point, these are actually points we want to find a region of confidence, so that's when you have new points.",
                    "label": 0
                },
                {
                    "sent": "If there are inside of division, you would say there are normal, and if they are outside, we said that they are not normal.",
                    "label": 0
                },
                {
                    "sent": "There will be some alert ringing cetera.",
                    "label": 0
                },
                {
                    "sent": "So this form of attention is very important in industry and in many fields and.",
                    "label": 0
                },
                {
                    "sent": "And one algorithm to solve that which seems to work very well, which is using in some industries I'm aware of is the one class SVM, so the one classism is funny because it's very similar to the two classes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, he said that you use your program for SVM, but instead of saying that you have, so you have your input points, you give them, you hit them all the same label Y equals, press one and you run USM and you will be able to find this version.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "Well if you look at this formulation, you can have an idea of what it is you see.",
                    "label": 0
                },
                {
                    "sent": "That's what you try to minimize.",
                    "label": 0
                },
                {
                    "sent": "This is the empirical loss when there is no label, so other labels are plus one and this is the regularization.",
                    "label": 1
                },
                {
                    "sent": "So what you're trying to find the function F that is smooth and what's important is that here it's in the Goshen arcade chess.",
                    "label": 0
                },
                {
                    "sent": "So here a big difference with the two classes in is that you don't give a function plus a constant in normal.",
                    "label": 0
                },
                {
                    "sent": "Again, you're looking for F equals the valuation plus B.",
                    "label": 0
                },
                {
                    "sent": "Here is just F equals the value is there is no offset B.",
                    "label": 1
                },
                {
                    "sent": "And it's important because all functions integration approaches tend to zero at Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have a small norm, it will be 0 at Infinity, but it cannot be 0 everywhere be cause this function penalizes each of the point is I you have a loss as soon as the value of F on the point is below 1.",
                    "label": 0
                },
                {
                    "sent": "So basically imagine at this feature what you're looking for is a function that is above one on most of the points, but that has a small norm, which means that it will tend to zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "And at the end, when you, when you fix a Lambda, you will get such a shape and this blue line is simply the level set of the function equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this this algorithm is not do it exists, but what's important and what is it?",
                    "label": 0
                },
                {
                    "sent": "5 very important here is that you see here, Lambda plays a quite different role than SVM because the role of gender is not regularised problem.",
                    "label": 0
                },
                {
                    "sent": "The role of Lambda here is to decide what is the proportion of policy want outside.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is.",
                    "label": 0
                },
                {
                    "sent": "You can show that there is a connection between the choice of Lambda there and the quantity of the percentage of points that will be left outside.",
                    "label": 0
                },
                {
                    "sent": "So typically if your problem is can you design me decision functions or region such that 95% of the points will be inside this request form to a Lambda.",
                    "label": 0
                },
                {
                    "sent": "And so when the number of points tend to Infinity you will not want to end up with increased to 0 because the 95% with will always be the same.",
                    "label": 1
                },
                {
                    "sent": "So will never try to decrease them that you will just observe that as the number of points tend to Infinity, this will converge to something that's rough and so there is for this algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you want the form of consistency for these adverse and mystically well as the number of points increases, what is the volume we get?",
                    "label": 0
                },
                {
                    "sent": "What is the decision function we get and in particular what we would like is suppose the points are generating according to a distribution.",
                    "label": 0
                },
                {
                    "sent": "Is the decision function converging toward what is called?",
                    "label": 0
                },
                {
                    "sent": "A quantity that is a region of of minimum lebeck volume for that has a given probability.",
                    "label": 0
                },
                {
                    "sent": "So so here we have some background distribution for the points we want.",
                    "label": 0
                },
                {
                    "sent": "A sensitive people would want to find the region of the Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "I know it's Friday and it's 5:30 and guess what?",
                    "label": 0
                },
                {
                    "sent": "I have 35 slides.",
                    "label": 0
                },
                {
                    "sent": "But don't worry, I don't tend to talk about all of this.",
                    "label": 0
                },
                {
                    "sent": "We got was just have enough size.",
                    "label": 0
                },
                {
                    "sent": "Outline something that I can.",
                    "label": 0
                },
                {
                    "sent": "I will put them on the web later, but you can look at them becausw.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present something which is very different from what I presented earlier, which is only some theoretical work, so it's basically the field of static learning theory.",
                    "label": 0
                },
                {
                    "sent": "The goal is to prove some consistency of some types of algorithms, so I will skip probably all the technical details 'cause he's such result there always technical details and try just to give you some images of some of the results we got.",
                    "label": 0
                },
                {
                    "sent": "So this is John Joint work with register two is in fact that my brother was in Paris University until recently and the goal is to study its approach.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People question that often occurs.",
                    "label": 0
                },
                {
                    "sent": "You know there has been many question and if I have ACM, how do I choose C?",
                    "label": 0
                },
                {
                    "sent": "How do I choose Sigma here?",
                    "label": 0
                },
                {
                    "sent": "Going to play a bit with CNC mine.",
                    "label": 0
                },
                {
                    "sent": "In fact we will try to see if we fix C. What was going on and can we have some consistent difference if we just decrease Sigma so it's a bit more general than that, but this is a picture to have in my.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally what I will do is giving us spending time on motivations.",
                    "label": 0
                },
                {
                    "sent": "Why do we do that?",
                    "label": 0
                },
                {
                    "sent": "Why do we care about that?",
                    "label": 0
                },
                {
                    "sent": "And giving just the main results and probably is going to be 6:00 PM and so I will skip the proofs, go to the conclusion.",
                    "label": 0
                },
                {
                    "sent": "But before the motivations, I need some basic notations, which Fortunately I introduced in the enrichers kernels and RKH sic.",
                    "label": 0
                },
                {
                    "sent": "Now you know what it is, but I would just recap it in the case of the question Colonel.",
                    "label": 0
                },
                {
                    "sent": "So I will focus here.",
                    "label": 0
                },
                {
                    "sent": "I'm near the Goshen RBF channel, so remember that the typical Goshen kernel K between two vectors, X&X prime, is that if this potential of minors there squared difference than all of the square difference divided by two Sigma squared and compared so here Sigma is usually referred to as the bandwidth of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Imagine a question that can be large or small.",
                    "label": 0
                },
                {
                    "sent": "This is how similar it seem as large as a large portion is significantly picking ocean, and here just for technical reasons, I don't take directed essential of this, but I divide this by constant here.",
                    "label": 0
                },
                {
                    "sent": "The goal this is a familiar concerns, just to make sure that when you do the integral of the ocean.",
                    "label": 0
                },
                {
                    "sent": "You end up with with one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is the question.",
                    "label": 0
                },
                {
                    "sent": "Color with Bandwidth Sigma and you know that you should allow, you should know now that it is a positive definite camel.",
                    "label": 0
                },
                {
                    "sent": "So to each position.",
                    "label": 0
                },
                {
                    "sent": "See, the allocations are functions and in the case of the Ocean Canal we have some explicit decision of their cages as any OK chase is typically made of functions which are some of can.",
                    "label": 0
                },
                {
                    "sent": "Also the functions of the sum of Gaussians plus their limits.",
                    "label": 0
                },
                {
                    "sent": "The normal you know the square number function.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As if the function can be written as the sum of us vicariously, EXE is norm is the sum of AA Jake every size J.",
                    "label": 0
                },
                {
                    "sent": "So this is general results valid for any kernel.",
                    "label": 0
                },
                {
                    "sent": "And in the case of the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "We have different ways to quote to describe York Hacc bit more technical.",
                    "label": 0
                },
                {
                    "sent": "This involves I just talked about it briefly in the prefectures.",
                    "label": 0
                },
                {
                    "sent": "This involves the 40 transform of the function.",
                    "label": 0
                },
                {
                    "sent": "So to be more precise here, suppose F is a function from RD2R.",
                    "label": 0
                },
                {
                    "sent": "We define dysphoria transformed if it is doable.",
                    "label": 0
                },
                {
                    "sent": "It as a free transform which is defined as a function of Omega.",
                    "label": 0
                },
                {
                    "sent": "Omega will be speaking the frequency and therefore it transform is just the familiar equation is the integral of F of X times extension of minus I Exxon Agar.",
                    "label": 0
                },
                {
                    "sent": "It's over ovaries, so this is a courtesy call for each one of us.",
                    "label": 0
                },
                {
                    "sent": "And why do we need to wait for something 'cause we have an equivalent description of the Ark HS of the Goshen operations and of the of the norm is accurate within the free transform.",
                    "label": 0
                },
                {
                    "sent": "Because let's jump to the last line, we know that for a function to be, well, a function is in their cages if and only if this thing is finite, and this thing will be in fact the normal Doc HS is the integral in the frequency domain, so it's the integral Omega of the 40 function.",
                    "label": 0
                },
                {
                    "sent": "We got squared times exponential of Sigma square meters squared divided by two.",
                    "label": 0
                },
                {
                    "sent": "So showing that the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is equal to.",
                    "label": 0
                },
                {
                    "sent": "This is a bit tricky, but you can do it almost by hand, but this will be this is known.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not new.",
                    "label": 0
                },
                {
                    "sent": "Everybody here in the Phil knows that this is a regression or cages.",
                    "label": 0
                },
                {
                    "sent": "So why did I do that?",
                    "label": 0
                },
                {
                    "sent": "Becausw now let's go to SCM or 20 Canon Masada, which so I will briefly recap it.",
                    "label": 0
                },
                {
                    "sent": "Remember that learning so I can only thought for learning.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a 20 set of variable XII and want to run a function F of X to predict Y.",
                    "label": 0
                },
                {
                    "sent": "Typically kernel methods to minimize the sum of two terms, which is the average empirical loss on your training data.",
                    "label": 0
                },
                {
                    "sent": "Plus from that time is the norm in Dark Ages.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we will focus, especially in the case where the loss function here is.",
                    "label": 0
                },
                {
                    "sent": "So L compares tells you how much it costs to predict something instead of something else.",
                    "label": 0
                },
                {
                    "sent": "He will focus on the case where Y is just a binary variable, so its classification can be 1 + 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "And when you can do a real number and the loss function when you is relative instead of Y is given by the function of Yu.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit of struct if is a for some, but just think of them as these properties.",
                    "label": 0
                },
                {
                    "sent": "You know when we talk about the hinge loss, how much does it cost to predict you when the classes Y while the hinge loss can be written as the Max between zero and one minus square you?",
                    "label": 0
                },
                {
                    "sent": "So this covers a wide range of concern, like Cenergistic progression is such a process it around, so we assume that we have a function 5 and that this is what you want to solve.",
                    "label": 0
                },
                {
                    "sent": "So now if you apply this running method with a Gaussian kernel, you can just plug what you know about the norm of the Goshen kernel to obtain this equation.",
                    "label": 0
                },
                {
                    "sent": "This is what you solve when you run your Asian program with the Gaussian kernel of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One with Sigma an with this number.",
                    "label": 0
                },
                {
                    "sent": "So here again, usually in practical applications you choose C. There is a one question between London C, So I could write it as C times this plus this thing without come down.",
                    "label": 0
                },
                {
                    "sent": "OK, so Lambda is exactly equal to.",
                    "label": 0
                },
                {
                    "sent": "Is it a quote one over another?",
                    "label": 0
                },
                {
                    "sent": "So these are the two parameters if you want to tune in at GM, you want to tune C Orlando, which is the same and you want to Sigma.",
                    "label": 0
                },
                {
                    "sent": "So here I would like to discuss a bit theoretically why?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we should set cinema in London?",
                    "label": 0
                },
                {
                    "sent": "For which reasons we should set them so in learning theory there is a an important problem.",
                    "label": 0
                },
                {
                    "sent": "When you design an algorithm, which is to know what happens when you have more and more points.",
                    "label": 0
                },
                {
                    "sent": "Because typically here we say that we design an algorithm that will deal with endpoints and typically the at least what we want is that if the number of points increases becomes large we would like at least installation to be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increasingly, Gouda and a sense of the Kiwi like the what we compute to be to be good.",
                    "label": 0
                },
                {
                    "sent": "So there is a notion I don't know how many of you are familiar with it, but there is a national sense of consistency which so we said that an algorithm is consistent for some loss of species, typically for the classification error if when you when the number of pollen tends to Infinity, then the classification error of what we estimate tends to the best possible.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course, this notion of limits has to be taken with care because the points are random, so there is a notion of convergence in probability.",
                    "label": 0
                },
                {
                    "sent": "And to be precise, we said that an algorithm is convergent if the classification error of estimated function converges to the best possible classification error in probability.",
                    "label": 0
                },
                {
                    "sent": "And the question of of knowing when an algorithm is consistent or not has to deal with Lambda in Sigma becausw.",
                    "label": 0
                },
                {
                    "sent": "If you think of this kind of thoughts as minimizing some loss plus some organization, in fact the realisation in theory is required to get consistency.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's required to have good results in finite dimension an in order in order to get consistency of algorithms which minimize some controversial loss plus organization in all the papers, I am away.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are often all the work, I'm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I mean in the traditional way to rigorous program, you need to get consistency.",
                    "label": 0
                },
                {
                    "sent": "You obtain consistency when you decrease the regularization as the number as the number of points increases.",
                    "label": 0
                },
                {
                    "sent": "OK, so intuitively, if you had enough points, you could just focus on the first term, because the first time would be a very good approximation of the true expected loss, and you would just minimize this now, because in practice you don't have an infinite number of points.",
                    "label": 0
                },
                {
                    "sent": "You regularize this by this.",
                    "label": 0
                },
                {
                    "sent": "But the sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So in practice, if you have a SVM with an increasing number of points, you should be ready to decrease Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, because you don't, you want to you want to trigger other days, otherwise you would not be consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, because if you hit regularising then the effect of programs will remain after you have too many points.",
                    "label": 0
                },
                {
                    "sent": "So most of the results.",
                    "label": 0
                },
                {
                    "sent": "He suppose if we go back to the previous slide, then we forget about this.",
                    "label": 0
                },
                {
                    "sent": "Now most of results when you minimize a loss.",
                    "label": 0
                },
                {
                    "sent": "Plus in that sense organization obtain when you say that if N tends to Infinity and if Lambda decreases to zero at a good rate, then you can prove that the algorithm is consistent.",
                    "label": 0
                },
                {
                    "sent": "There are many papers about that, and for instance there are papers that prove that FGM is consistent, typically saying OK. 10:50 if I decrease on that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the second term.",
                    "label": 0
                },
                {
                    "sent": "So there is a good range of decrease to choose.",
                    "label": 0
                },
                {
                    "sent": "Of course, if it's too fast or too slow, then it will not be consistent, but for some for some rates or decrease it will be consistent.",
                    "label": 0
                },
                {
                    "sent": "So you have to kill this term reasonably well.",
                    "label": 0
                },
                {
                    "sent": "In the case of the Gaussian kernel, you see that, so decreasing Lambda here means decreasing this term, but you see that if you play now with the with the signal of the Gaussian kernel, it also has an effect here, and it has a very precise effect, which is that Sigma is here.",
                    "label": 1
                },
                {
                    "sent": "So if you decide to reduce signal, you will also reduce this term.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, decreasing Sigma is a is another way to decrease organization.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that will make it 10 to 0, but at least it will decrease.",
                    "label": 0
                },
                {
                    "sent": "So in order to get a kind of idea of.",
                    "label": 0
                },
                {
                    "sent": "How you should regularize and how you should kill regularization when the number of points that 20?",
                    "label": 0
                },
                {
                    "sent": "Let's look at a very relatively which is suppose they have endpoints.",
                    "label": 0
                },
                {
                    "sent": "You know that if you have end points and you train this forgiven on dancing, Now you will find something.",
                    "label": 0
                },
                {
                    "sent": "And if these endpoints you reduce them that you recognize this, then you will overfit.",
                    "label": 0
                },
                {
                    "sent": "So here I think that when you decrease them that you will overfit and when you decrease the menu will also overfit.",
                    "label": 0
                },
                {
                    "sent": "So this is we will have a picture now pictures now showing that.",
                    "label": 0
                },
                {
                    "sent": "So it's very clear that there are two things you can do to overfit, and if you generalize this to US entities and there are two ways to kill organization, so let's look at the picture now because I know it's late so it's not as well as the pictures we had yesterday.",
                    "label": 0
                },
                {
                    "sent": "But I have movies to show you the effect of organization, so I don't know if I.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, I have some problem with the.",
                    "label": 0
                },
                {
                    "sent": "OK so here you see am working and what happens is that I decrease the Lambda for increase the seat of the SVM and you see that you started from a kind of smooth function decision function and when you decrease organization for the set of points you tend to have less smooth decision function and use them.",
                    "label": 0
                },
                {
                    "sent": "For instance here there is 1 red point here inside of blue ones.",
                    "label": 0
                },
                {
                    "sent": "If you don't regularize, the engine will overfit.",
                    "label": 0
                },
                {
                    "sent": "Will say that this version is red.",
                    "label": 0
                },
                {
                    "sent": "Inside of the blue ones.",
                    "label": 0
                },
                {
                    "sent": "So in fact here the points for generated by three oceans are very simple case and this is just to to be convinced that OK, we knew it if we regularize less.",
                    "label": 0
                },
                {
                    "sent": "If we decrease the time that we will overfit, we observe it.",
                    "label": 0
                },
                {
                    "sent": "Something which is less of let's come on your service that suppose we keep see this week another fixed and we just decrease Sigma then our claim is just that we will also overfit.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit different.",
                    "label": 0
                },
                {
                    "sent": "But let's look at the Warfield by decreasing Sigma now.",
                    "label": 0
                },
                {
                    "sent": "So I do the same.",
                    "label": 0
                },
                {
                    "sent": "I just computer SVM except this time I will very Sigma from a large team at with Small Sigma.",
                    "label": 0
                },
                {
                    "sent": "And this is what we get here.",
                    "label": 0
                },
                {
                    "sent": "Sigma is large and then Sigma decreases.",
                    "label": 0
                },
                {
                    "sent": "And at the end, for small silver.",
                    "label": 0
                },
                {
                    "sent": "So it's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "You know, for small signal feed very small Gaussians around the points and you get something which is ready overfitting.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, when you compare the two pictures, it seems that the second one is a bit better out of a 15.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you really want to say I want to work in my points is better to put very small questions around the points rather than keeping a large bandwidth address allowing the Alpha to be large.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact so this is that there is a result.",
                    "label": 0
                },
                {
                    "sent": "A bit surprising but true, which is that you know when we say that we said that the Goshen Canal can overfit.",
                    "label": 0
                },
                {
                    "sent": "Everything is not true.",
                    "label": 0
                },
                {
                    "sent": "Because just because of numerical precision, because in practice, suppose I give you a training set and I said to you, use the Goshen Canary with bandwidth one.",
                    "label": 0
                },
                {
                    "sent": "I try to overfit rate this point.",
                    "label": 0
                },
                {
                    "sent": "So try to be positive on the blue.",
                    "label": 0
                },
                {
                    "sent": "On a negative already walls.",
                    "label": 0
                },
                {
                    "sent": "In my case I could do it because I didn't have so many points.",
                    "label": 0
                },
                {
                    "sent": "But if you have more points, there are cases where you cannot overfit, just cause you have finite precision in your computer.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it's ready to you should be in theory when you take a kernel matrix application kernel.",
                    "label": 0
                },
                {
                    "sent": "It's invertible, it's really invertible in theory, but if you look at the eigenvalues so none of the eigenvalues is 0, but some are 10 to the minus 200 or 10 to the minus 300, and so if you want to invert it with your computer, you can do it.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, it's not if you keep fixed the bandwidth of the Gaussian kernel, it's not always possible to over fit well.",
                    "label": 1
                },
                {
                    "sent": "Points where it seems very.",
                    "label": 0
                },
                {
                    "sent": "Trivial to overfit when you allow to decrease Simba.",
                    "label": 0
                },
                {
                    "sent": "So, so there is therefore first motivation, which is which is a bit of theoretical consideration, which is that in all to manual educator in all consistency results about Vincent kilometres.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work when you fix OK and also not an integration canal.",
                    "label": 0
                },
                {
                    "sent": "But whatever cannot you take, you can show that AGM can be consistent.",
                    "label": 0
                },
                {
                    "sent": "When not that tend to zero.",
                    "label": 0
                },
                {
                    "sent": "And for this there are very all techniques, techniques of proof that have been used.",
                    "label": 0
                },
                {
                    "sent": "So this goes back to the 80s.",
                    "label": 0
                },
                {
                    "sent": "That gives you three ways to prove consistency when you have lots of shown pressurization.",
                    "label": 0
                },
                {
                    "sent": "You kid realization by using Lambda that tricks is not is not easy, but their strategy to solve that.",
                    "label": 0
                },
                {
                    "sent": "Now recent here I am at least for some work that showed that if you decrease Lambda and you decrease Sigma in the same time, which is a bit natural dinner GMU regularly.",
                    "label": 0
                }
            ]
        }
    }
}