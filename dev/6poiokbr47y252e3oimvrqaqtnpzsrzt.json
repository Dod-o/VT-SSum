{
    "id": "6poiokbr47y252e3oimvrqaqtnpzsrzt",
    "title": "Thompson Sampling: a provably good Bayesian heuristic for bandit problems",
    "info": {
        "author": [
            "Shipra Agrawal, Microsoft Research India"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_agrawal_thompson_sampling/",
    "segmentation": [
        [
            "So I'll be discussing Thompson sampling for multi arm bandits and hopefully I'll try to convince you that it should not be.",
            "It's not just in heuristic anymore and we can consider it an algorithm with a very strong theoretical backing."
        ],
        [
            "So Fortunately, many speakers have given some background on multi armed bandit, so far, so I'm not going over the background very much detail what I want to mention mention here is that I'll be considering stochastic version of the Multi armed bandit problems, which means that every time you pull an arm the reward is coming from a fixed but unknown distribution.",
            "So and often it is assumed that the expected reward is parameterized by some unknown parameter new.",
            "And so your objective is basically to learn this unknown parameter in order to decide which arms to pull so that you maximize your total reward overtime some time horizon, which may not be known to you.",
            "And you so the guarantees in terms are given in terms of minimizing regret.",
            "Regret is nothing, but saying that if what is the loss I'm undergoing because I'm not making optimum choice of arms at every step, and because we are constrained stochastic settings so you minimize regretting expectation or high probability, I'll be considering high probability bounds.",
            "I'll go through more specific details for specific version of this problem that I'll I'll discuss later."
        ],
        [
            "So Thompson sampling is a meta algorithm for this problem.",
            "This is a natural beige and heuristic, and the general it was proposed in by Thomson in his 1933 paper and the general structure of this heuristic looks looks like this.",
            "So it's a beige and is based on Bayesian principles.",
            "So your goal is to learn these unknown parameters.",
            "So what you do is you start with some prior belief on these unknown parameters.",
            "This prior could be very simple, something like a uniform distribution.",
            "Or a Gaussian with mean zero and some identity as covariance or something like that.",
            "So you start with this prior with this prior belief about these parameters an like any version would do you every time you play an arm, and you observe a reward, you update your belief to reflect your new beliefs and you do these updates in a natural beige and manner.",
            "And then it's time step at time step T. Your action is simply to look at your friend posteriors, sample the parameters from them, and then according to the sample para meters, whichever arm appears to be the best time you play that arm.",
            "So effectively you are playing the arm every arm with its probability of being the best arm according to your belief about about the arms."
        ],
        [
            "So this this this, this heuristic has a long history.",
            "It was proposed originally in 1933, but since then it has actually been rediscovered many times in reinforcement learning literature under different names.",
            "And because it's such a natural Bayesian principle that to follow.",
            "Meanwhile, there was UCB based algorithms which are proposed for multi arm bandits and they had huge success in terms of getting strong theoretical theoretical guarantees.",
            "And.",
            "But recently there has been a revival of interest in Thompson sampling.",
            "Several studies have demonstrated very promising empirical performance of this of this method, and it has been used in industrial applications, and as I remonstrate later that it's actually more efficient for many settings than UCB to implement Thompson sampling instead of UCB.",
            "I'll do.",
            "I'll explain this what I mean by that more later."
        ],
        [
            "So, so here's a here's some state of the art on what we know now about Thompson sampling.",
            "So motivated by the recent interest, there has been significant progress in getting theoretical guarantees for for this algorithm, and the guarantees for classic multi Arm Bandit came first, and particularly for the problem with the classic problem, where you choose one out of an arms at every step, and there we have we have.",
            "Absolutely optimal guarantees for Thomson sampling.",
            "Very simple algorithm can get you can match the lower bound.",
            "We also have now extensions to many different versions of multi arm bandits that you can see in the literature.",
            "I'll not go through what these mean, but in case you have seen them before, you might be able to recognize them.",
            "Contact inside observation.",
            "Yeah, I'll I'll, I'll do so.",
            "The difference is yes.",
            "So here what I mean is that every time you play an arm, you also get to observe feedback from some other arms, like the neighborhood arms in the social network, or something like that.",
            "Usually it is used in context of social networks where you can observe.",
            "Observe your friends reaction to your action.",
            "Where is contextual multi armed bandits?",
            "You have a context associated with the arm which changes with time, so I'll so my focus in this talk will be to explain the theoretical results for the contextual multi armed bandit and in particular the linear contextual multi armed bandit.",
            "The we now have guarantees for this problem which match the best available guarantees and not be able to go through go through these things.",
            "Because of time.",
            "So."
        ],
        [
            "So what is a linear contextual bandit problem?",
            "So in this problem in the classic problem, you pull an arm and you get a reward from the distribution, which is unknown but fixed.",
            "Here what happens is when you pull in arm your your reward just doesn't just depend on the arm that you pull, but also on on the context in which you pull it.",
            "So for example, in typical application like clinical trials, when you try your treatment, it doesn't just depend on the treatment, but also on the features of the person.",
            "On which you administer the treatment so so so these kind of context is very important in most practical applications.",
            "So in linear contextual bandits, what you assume is that every time step you get a set of these feature vectors.",
            "So every feature vector can be looked at as an arm.",
            "So you have possibly infinite number of arms that you are getting in every every step, and there is an unknown weight vector.",
            "That you want to learn.",
            "The reward is related in this manner to the unknown weight vector and the feature vector.",
            "So if you pull an arm with feature vector B of T, then then your expected reward is dot product of this weight vector and the feature vector.",
            "So it's a typical linear regression model.",
            "And for simplicity, we assume that the the noise, basically the division of the reward from the expected reward, is bounded.",
            "It also works under the sub Gaussian noise assumption, but it would have been more harder to explain.",
            "That's why I didn't include that here.",
            "One thing to note here is that because the context changes with time, the optimal arm also changes with time.",
            "So in the classical problem.",
            "Because the distribution of reward is fixed, the optimal arm is fixed.",
            "There is one optimal armor, multiple optimums, but your job is to just figure out which arm is optimal.",
            "But here, depending on the context, the arm at optimal arm at time T is changing.",
            "And the so the regret is now just the difference between the expected reward of the optimal arm and the reward of the play expected reward of the play down.",
            "And your objective is to minimize the total regret over over some time horizon T. This one or a lefty.",
            "So you say it.",
            "Yeah, so so.",
            "First of all this, A of T is the set of context which is adversarial which could be adaptive and adversarial.",
            "Be of these your choice.",
            "So that's the algorithm.",
            "So B of T is the arm that you play.",
            "So your algorithm will be choosing disarm, disarm.",
            "Sorry I did not get your question confuses expectation values, time dependence and you know nothing about this time dependence.",
            "No, all you know is that or is this all you know is that if you pick a context, there is an unknown fixed weight vector?",
            "And your expected reward is the dot product.",
            "You don't know anything else about the reward.",
            "Covering the microphone.",
            "OK, sorry.",
            "So yeah, so that's all you know.",
            "This is only thing you know about the reward."
        ],
        [
            "So I'll define the Thompsons, yeah?",
            "Yes, it is finite.",
            "Yeah.",
            "In particular, if you mean bounded or do you mean finite or bounded?",
            "It's bounded.",
            "It may not be finite.",
            "Yeah, it could be a convex set or linear polytope, or.",
            "Or a non convex set for that matter?",
            "So I will define now Thompson sampling for this version of the problem and I will use Gaussian Gaussian belief distributions.",
            "So one point I want to mention is that when I when I say that I'm using Gaussian belief, caution, likelihood, these are all just to define the algorithm.",
            "So caution, believe distribution is the way that the algorithm will maintain its knowledge.",
            "It may have may have nothing to do with the actual reward distribution, so I'm not assuming that the actual reward distribution is Gaussian.",
            "In particular, our regret bounds will hold for any reward distribution.",
            "It will, worst case, regret bound.",
            "So, So what you do is you start with a Gaussian prior.",
            "Simply simply covariance proportional to the identity matrix of dimension D an O mean.",
            "And this coefficient will be making specific choice for this coefficient.",
            "And what you can show by simple vision principle is that if the reward was had a Gaussian likelihood, so if the reward was coming from this distribution.",
            "Then at time T, the vision posterior would be simply simply this mean this here is the least square estimate calculated from T -- 1 step.",
            "Just regularize simple regularly, square estimate and B.",
            "Here Instagram matrix.",
            "So.",
            "So this is simple Gaussian prior Gaussian posterior kind of calculation an and this is the.",
            "This is the Bayesian interpretation that we will use.",
            "So again I want as I said, this is just to give a Bayesian interpretation to the problem.",
            "It's it's not something that we will require for our proof.",
            "For a regret bound.",
            "That we do not require that the actual likelihood of reward is caution.",
            "So using this."
        ],
        [
            "And Brian posterior.",
            "This is the algorithm that you get, so you simply use this Gaussian centered at the least square estimate and covariance as this proportional to this inverse of the gram matrix.",
            "And you sample a beautiful day from this caution D dimensional sample.",
            "So that is your estimate of the weight vector.",
            "Now you would simply play the arm, which maximizes the mean.",
            "If this was the true weight vector.",
            "So you.",
            "If this was the true weight vector, what would have been the optimal arms?",
            "You simply play that arm.",
            "So if if a of T is some nice set, like a finite set or a convex set.",
            "Then you are simply solving a linear.",
            "Optimizing a linear function over a convex set.",
            "So these are the specific choice of the coefficient.",
            "So this choice of the coefficient actually in the experiments that people have done this is not very crucial, but it's useful for proving bounds.",
            "So."
        ],
        [
            "Now I want to quickly compare this to the popular UCB based algorithm just to place them side by side.",
            "And what's the difference so you can in some ways view this algorithm if you so this is the UCB algorithm for linear contextual bandits.",
            "Is also called awful algorithm so.",
            "What you have is there is remarkable similarity.",
            "So we have this Gaussian with center at mu hat and be inverse as the covariance matrix.",
            "In the UCB Amazing garden you take this ellipsoid.",
            "Centered at Manhattan BB inverse covariance matrix.",
            "And so here we are sampling mutillidae from this Gaussian.",
            "Instead you take this ellipsoid and you find for every arm you find the best.",
            "Mutillidae in this ellipsoid.",
            "So for every arm you're choosing, the best direction, best weight vector, and then you are finding the best arm.",
            "So you are.",
            "This is the optimistic principle.",
            "So for every arm you say what is the best weight vector in its confidence ellipsoid?",
            "And then you find the best arm.",
            "Which will maximize its optimistic estimate.",
            "So in so here you're solving a double maximization, so you're maximizing both over Mutillidae and Overby.",
            "Whereas we this is like a randomized version where you sample one single mutillidae.",
            "And then you find the best on so.",
            "So this is so.",
            "This is definitely much more efficient, cause here, even if you're you're set was a nice convex set.",
            "Even then this problem would be a difficult nonconvex problem."
        ],
        [
            "So just to illustrate pictorially why these this algorithm.",
            "Works or what makes it work is so you start with this ellipsoid, which is basically the covariance of the confidence ellipsoid of this Gaussian.",
            "Initially you start with a spherical Goshen, and it's a.",
            "It's a big ellipsoid initially, but as you go as you will progress in your algorithm, you have more and more confidence and the solution, so it will shrink what you can show is that the actual weight vector always lies in the ellipsoid.",
            "So as your absurd shrink.",
            "You are closer and closer to your actual weight vector, and finally your ellipsoid will be very small and you will with high probability you will always your weight.",
            "Your estimate will be very close to your actual weight vector.",
            "So."
        ],
        [
            "Here's the regret bounds that we can provide for this algorithm.",
            "We show that with probability 1 minus Delta, where Delta can be choosen to be anything.",
            "The regret in time horizon T for any T can be bounded by order of the square root T and some lock terms.",
            "So this order notation just is just hiding some constants, not asymptotics bounds.",
            "So finite time time bound and this constant, we can compute them, and we have them in the paper I just for clean display.",
            "They're hiding the constants.",
            "The dimension yeah D is the dimension of the weight vector and the context.",
            "So this doesn't have anywhere the number of arms, so you don't need to finite set of.",
            "Context.",
            "You for UCB based algorithm that I mentioned the best bound is very close is just away by just better by a square root log T factor which you can think of it as a cost that you pay for having a more efficient.",
            "Algorithm I don't know, or it might just be our analysis.",
            "So.",
            "The lower bound is is the square root T as well, so it just doesn't have this lower bound, doesn't have the log factors.",
            "Describe how you could.",
            "Yes, that's the main.",
            "So the remaining talk is just going to be about the proof, so I don't know if that's a good news.",
            "Bad news for most people, but that's what we did.",
            "So algorithm in principle existed.",
            "We just specified the algorithm in terms of Gaussian priors and the main contribution was to get a proof so."
        ],
        [
            "Here's the proof outline.",
            "How much time do I have 20 minutes?",
            "10 OK.",
            "So yeah, hopefully I have this five slides of proof, so let's see so.",
            "So the regret.",
            "So the aim is to bound the regret total.",
            "Regret and recall that regret at time T was defined as the mean of the optimum difference is the mean of the optimum and the play dumb.",
            "B of T is the play dumb and B star of tea is the optimal at time T. So this is the sum of this quantity is what you want to bound.",
            "So now the the key inequality in our proof would be to show that this regret at time T can be bounded in terms of the standard deviation of the play down.",
            "I'm calling this as a standard deviation of an arm, so B is the context of an arm, and then this is what this quantity.",
            "I'm calling the standard deviation and it will become clear in a minute.",
            "That's why I'm calling it a standard deviation, so this is the key inequality that you can bound regret at time T in terms of the standard deviation of the play down in the direction of the play down.",
            "And why is that useful?",
            "It's useful becausw.",
            "Intuitively, this standard deviation in the direction of the play Dom goes down very fast.",
            "And why is that?",
            "Because suppose at some time you didn't know about much about the direction of the play dharman you incur the high regret.",
            "But then in the next step, because you have explored that direction, you know a lot more.",
            "So either you are gaining in exploitation or you are gaining in exploration.",
            "So if you're only considering the standard division in the direction of preedom, it is like a telescopic sum and it will go down very fast so that this sum the sum of these standard deviation overtime is not going to be very large.",
            "So this inequality here was already proven in papers on UCB so you can show that that irrespective of what arms you play, this is not a probabilistic bound.",
            "This is absolute bound.",
            "So irrespective of all times you play this, some of the standard version of in the direction of the played arms is always going to be bounded by this quantity.",
            "Square root DT locked.",
            "So then the so once you have this Ann, you have these supermartingale link.",
            "So this is a supermartingale inequality.",
            "We prove this for any given history of the place.",
            "And by history we mean all the execution until time T -- 1.",
            "This choice of arms the outcomes, reward outcomes and the context set of context time steps T so.",
            "Is that for any planes or that's because you're using Thompson sampling?",
            "This this inequality?",
            "Yeah, this is because you're using Thompson sampling.",
            "Yeah, so this is Thompson sampling specific.",
            "This is for any place.",
            "So once you have this supermartingale inequality and you have this, then you can just use Azuma Harding to prove that the sum of the regret is bounded by.",
            "Buy this quantity here.",
            "So this is outline now so that I guess now the main job is to prove this inequality.",
            "The sun landed our 2000.",
            "It holds for any strategy of choosing choosing B of T. Because it's just like information theoretic bound because there's only so many directions to explore in a diamond in a set of dimension D. So.",
            "So now our job is to prove this key inequality.",
            "And because it's about regret bounding regret in terms of standard deviations.",
            "What will be used?"
        ],
        [
            "Is to know what concentration inequalities we have.",
            "So what can we say about the concentration of our estimates?",
            "So first of all, what can we say about this muhi at the least square estimate?",
            "So what has this is something which is also used in Ucb's because they also use the square estimate only difference is they used an optimistic.",
            "Estimating the confidence ellipsoid and we use a random estimate so they already show that this something about this confidence ellipsoid so they show that mu the actual weight vector always lies in this confidence ellipsoid around new hat.",
            "So using which you can say that so this fact that mu is in the confidence ellipsoid implies that for all.",
            "All the all the possible context B.",
            "The difference in the estimates difference in the means along the direction of B.",
            "Is smaller than this quantity here GD times so GT is simply this confident the coefficient in the confidence ellipsoid.",
            "So it's the time this standard deviation.",
            "So this is something you can show algebraically.",
            "So, so we have a good concentration for the least square estimate and this is borrowed from UCB analysis.",
            "What can we say about our our sample from the confidence from the Gaussian so we can say that the sample is going to be close to the the mean of the Gaussian within.",
            "Within this route, locked factor with high probability.",
            "These all these are with high probability like 1 -- 1 / T squared probability.",
            "So combining these two observations, we can say that with high probability.",
            "The mean at the mean at our sample for any before any any.",
            "In our context, set the mean of our sample and the actual mean is going to be bounded by the standard deviation.",
            "Some factor of the standard deviation in direction of B, so these are all with high probability.",
            "But for simplicity, let's assume that they hold for all history of execution, so.",
            "So these actually don't hold for all 50 -- 1.",
            "They hold with high probability for most FT minus one, but that is a simple thing to handle, so let's assume they hold for all history.",
            "So now."
        ],
        [
            "See if this is enough to prove our our inequality or not.",
            "So suppose I try to apply this this concentration and try to prove our inequality.",
            "So you can express the regret as the difference of these two means.",
            "Now we try to replace them by standard deviations by using our concentrations.",
            "So these are just ideas.",
            "Replace them by concentrations.",
            "And this difference because you know that you chose the arm to play, which maximizes this quantity.",
            "So you know this is difference is going to be smaller than 0.",
            "So you just get these two concentration terms.",
            "The problem now is that this is nice.",
            "We want this.",
            "The play Dom standard deviation, but this is not something that we want.",
            "Why becausw?",
            "This term, this standard deviation, the direction of the optimal arm to argue that this goes down fast enough.",
            "You have to argue that this is played often enough, but then if you can argue that optimal arm is played often enough, then you are actually proving what you want to prove ultimately.",
            "So it's like a circular thing that's happening.",
            "So this is something that we don't want it here.",
            "Just for aside, this concentration would have been enough to prove this inequality for UCB, because you would use this concentration only for the play dumb.",
            "And this quantity would be smaller because you choose.",
            "You optimize over both mutual die and be.",
            "So this concentration is enough to prove this inequality for the UCB, but not for us because we don't want this here.",
            "So what we do is the trick.",
            "The main trick that we have is that instead of getting bound in terms of the optimal Arminda play, damn we will replace this by standard deviation of from a class of arms which is played often enough and which includes optimal arm, but it may include other."
        ],
        [
            "So this class is the we call it unsaturated arms.",
            "There are high variance, low regret arms, so low regret could be because they are very near optimum or high variance could be because they have not been explored enough.",
            "So these classes actually the class you should be playing.",
            "So either because it's regretted smaller because it has not been explored.",
            "So this is actually the class you should be playing.",
            "Often an you can show that you will play this class very often.",
            "That's that's basically follows from the fact that.",
            "Either they were optimal to start with or becausw they have high variance and therefore they have anti concentration of Gaussian.",
            "So Gaussian will choose them often enough.",
            "So so so if you have this class which is played often enough.",
            "You can basically show that this for standard deviation for this class of arms also goes down fast enough because they're played often enough.",
            "So you can bound so if the bar is the arm with smallest variance among the highway.",
            "Among these unsaturated arms.",
            "Then you can.",
            "You can bound the standard deviation of this unsaturated.",
            "In terms of the play down.",
            "Simply from this fact that they are played often enough.",
            "And now what remains to show is that you can bound regret in terms of not standard division, not of optimal and plate.",
            "But of this unsaturated and play down.",
            "And that turns out to be not very difficult because of this definition of the unsaturated arm.",
            "And."
        ],
        [
            "What you simply do is you bound the regret in terms of regret with respect.",
            "To unsaturated arm.",
            "And the difference of these two means.",
            "So anyway, by so I guess I'm short on time, but you can easily bound regret in terms of these played and the unsaturated arm.",
            "And to summarize then you use the fact that this regret this standard deviation goes down as fast as the play down.",
            "So you get your desired inequality, regret it, bounded in terms of the standard deviation of the play down.",
            "Once you have that, you just use.",
            "Use this fact.",
            "The inequality already proven.",
            "For any sequence of player arms.",
            "So that's the proof."
        ],
        [
            "So hopefully the So what I wanted to convey here is that so this inequality is actually something very general, and this inequality bounding regret in terms of the standard deviation of the play Dom.",
            "So once you have this inequality, you can actually apply to many different versions of multi unbanded, so it's not so.",
            "As I mentioned initially, you can get bounds for various versions like sparse contextual bandits, kernelized contextual bandits and so on.",
            "So hopefully I can convince you that we have now strong modular techniques that you can just pick and extend your bounds to different versions.",
            "And we are in the state now that Thompson sampling can match even be attractive theoretically as attractive as UCB."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be discussing Thompson sampling for multi arm bandits and hopefully I'll try to convince you that it should not be.",
                    "label": 0
                },
                {
                    "sent": "It's not just in heuristic anymore and we can consider it an algorithm with a very strong theoretical backing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Fortunately, many speakers have given some background on multi armed bandit, so far, so I'm not going over the background very much detail what I want to mention mention here is that I'll be considering stochastic version of the Multi armed bandit problems, which means that every time you pull an arm the reward is coming from a fixed but unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "So and often it is assumed that the expected reward is parameterized by some unknown parameter new.",
                    "label": 1
                },
                {
                    "sent": "And so your objective is basically to learn this unknown parameter in order to decide which arms to pull so that you maximize your total reward overtime some time horizon, which may not be known to you.",
                    "label": 0
                },
                {
                    "sent": "And you so the guarantees in terms are given in terms of minimizing regret.",
                    "label": 0
                },
                {
                    "sent": "Regret is nothing, but saying that if what is the loss I'm undergoing because I'm not making optimum choice of arms at every step, and because we are constrained stochastic settings so you minimize regretting expectation or high probability, I'll be considering high probability bounds.",
                    "label": 0
                },
                {
                    "sent": "I'll go through more specific details for specific version of this problem that I'll I'll discuss later.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Thompson sampling is a meta algorithm for this problem.",
                    "label": 0
                },
                {
                    "sent": "This is a natural beige and heuristic, and the general it was proposed in by Thomson in his 1933 paper and the general structure of this heuristic looks looks like this.",
                    "label": 0
                },
                {
                    "sent": "So it's a beige and is based on Bayesian principles.",
                    "label": 0
                },
                {
                    "sent": "So your goal is to learn these unknown parameters.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you start with some prior belief on these unknown parameters.",
                    "label": 0
                },
                {
                    "sent": "This prior could be very simple, something like a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Or a Gaussian with mean zero and some identity as covariance or something like that.",
                    "label": 0
                },
                {
                    "sent": "So you start with this prior with this prior belief about these parameters an like any version would do you every time you play an arm, and you observe a reward, you update your belief to reflect your new beliefs and you do these updates in a natural beige and manner.",
                    "label": 0
                },
                {
                    "sent": "And then it's time step at time step T. Your action is simply to look at your friend posteriors, sample the parameters from them, and then according to the sample para meters, whichever arm appears to be the best time you play that arm.",
                    "label": 1
                },
                {
                    "sent": "So effectively you are playing the arm every arm with its probability of being the best arm according to your belief about about the arms.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this this this, this heuristic has a long history.",
                    "label": 0
                },
                {
                    "sent": "It was proposed originally in 1933, but since then it has actually been rediscovered many times in reinforcement learning literature under different names.",
                    "label": 0
                },
                {
                    "sent": "And because it's such a natural Bayesian principle that to follow.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, there was UCB based algorithms which are proposed for multi arm bandits and they had huge success in terms of getting strong theoretical theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But recently there has been a revival of interest in Thompson sampling.",
                    "label": 1
                },
                {
                    "sent": "Several studies have demonstrated very promising empirical performance of this of this method, and it has been used in industrial applications, and as I remonstrate later that it's actually more efficient for many settings than UCB to implement Thompson sampling instead of UCB.",
                    "label": 1
                },
                {
                    "sent": "I'll do.",
                    "label": 0
                },
                {
                    "sent": "I'll explain this what I mean by that more later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so here's a here's some state of the art on what we know now about Thompson sampling.",
                    "label": 1
                },
                {
                    "sent": "So motivated by the recent interest, there has been significant progress in getting theoretical guarantees for for this algorithm, and the guarantees for classic multi Arm Bandit came first, and particularly for the problem with the classic problem, where you choose one out of an arms at every step, and there we have we have.",
                    "label": 0
                },
                {
                    "sent": "Absolutely optimal guarantees for Thomson sampling.",
                    "label": 0
                },
                {
                    "sent": "Very simple algorithm can get you can match the lower bound.",
                    "label": 0
                },
                {
                    "sent": "We also have now extensions to many different versions of multi arm bandits that you can see in the literature.",
                    "label": 0
                },
                {
                    "sent": "I'll not go through what these mean, but in case you have seen them before, you might be able to recognize them.",
                    "label": 0
                },
                {
                    "sent": "Contact inside observation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll I'll, I'll do so.",
                    "label": 0
                },
                {
                    "sent": "The difference is yes.",
                    "label": 0
                },
                {
                    "sent": "So here what I mean is that every time you play an arm, you also get to observe feedback from some other arms, like the neighborhood arms in the social network, or something like that.",
                    "label": 0
                },
                {
                    "sent": "Usually it is used in context of social networks where you can observe.",
                    "label": 0
                },
                {
                    "sent": "Observe your friends reaction to your action.",
                    "label": 0
                },
                {
                    "sent": "Where is contextual multi armed bandits?",
                    "label": 0
                },
                {
                    "sent": "You have a context associated with the arm which changes with time, so I'll so my focus in this talk will be to explain the theoretical results for the contextual multi armed bandit and in particular the linear contextual multi armed bandit.",
                    "label": 0
                },
                {
                    "sent": "The we now have guarantees for this problem which match the best available guarantees and not be able to go through go through these things.",
                    "label": 1
                },
                {
                    "sent": "Because of time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a linear contextual bandit problem?",
                    "label": 1
                },
                {
                    "sent": "So in this problem in the classic problem, you pull an arm and you get a reward from the distribution, which is unknown but fixed.",
                    "label": 0
                },
                {
                    "sent": "Here what happens is when you pull in arm your your reward just doesn't just depend on the arm that you pull, but also on on the context in which you pull it.",
                    "label": 0
                },
                {
                    "sent": "So for example, in typical application like clinical trials, when you try your treatment, it doesn't just depend on the treatment, but also on the features of the person.",
                    "label": 0
                },
                {
                    "sent": "On which you administer the treatment so so so these kind of context is very important in most practical applications.",
                    "label": 0
                },
                {
                    "sent": "So in linear contextual bandits, what you assume is that every time step you get a set of these feature vectors.",
                    "label": 1
                },
                {
                    "sent": "So every feature vector can be looked at as an arm.",
                    "label": 1
                },
                {
                    "sent": "So you have possibly infinite number of arms that you are getting in every every step, and there is an unknown weight vector.",
                    "label": 0
                },
                {
                    "sent": "That you want to learn.",
                    "label": 0
                },
                {
                    "sent": "The reward is related in this manner to the unknown weight vector and the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So if you pull an arm with feature vector B of T, then then your expected reward is dot product of this weight vector and the feature vector.",
                    "label": 1
                },
                {
                    "sent": "So it's a typical linear regression model.",
                    "label": 0
                },
                {
                    "sent": "And for simplicity, we assume that the the noise, basically the division of the reward from the expected reward, is bounded.",
                    "label": 0
                },
                {
                    "sent": "It also works under the sub Gaussian noise assumption, but it would have been more harder to explain.",
                    "label": 0
                },
                {
                    "sent": "That's why I didn't include that here.",
                    "label": 0
                },
                {
                    "sent": "One thing to note here is that because the context changes with time, the optimal arm also changes with time.",
                    "label": 1
                },
                {
                    "sent": "So in the classical problem.",
                    "label": 1
                },
                {
                    "sent": "Because the distribution of reward is fixed, the optimal arm is fixed.",
                    "label": 0
                },
                {
                    "sent": "There is one optimal armor, multiple optimums, but your job is to just figure out which arm is optimal.",
                    "label": 0
                },
                {
                    "sent": "But here, depending on the context, the arm at optimal arm at time T is changing.",
                    "label": 0
                },
                {
                    "sent": "And the so the regret is now just the difference between the expected reward of the optimal arm and the reward of the play expected reward of the play down.",
                    "label": 0
                },
                {
                    "sent": "And your objective is to minimize the total regret over over some time horizon T. This one or a lefty.",
                    "label": 0
                },
                {
                    "sent": "So you say it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "First of all this, A of T is the set of context which is adversarial which could be adaptive and adversarial.",
                    "label": 0
                },
                {
                    "sent": "Be of these your choice.",
                    "label": 0
                },
                {
                    "sent": "So that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So B of T is the arm that you play.",
                    "label": 0
                },
                {
                    "sent": "So your algorithm will be choosing disarm, disarm.",
                    "label": 0
                },
                {
                    "sent": "Sorry I did not get your question confuses expectation values, time dependence and you know nothing about this time dependence.",
                    "label": 0
                },
                {
                    "sent": "No, all you know is that or is this all you know is that if you pick a context, there is an unknown fixed weight vector?",
                    "label": 0
                },
                {
                    "sent": "And your expected reward is the dot product.",
                    "label": 0
                },
                {
                    "sent": "You don't know anything else about the reward.",
                    "label": 0
                },
                {
                    "sent": "Covering the microphone.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that's all you know.",
                    "label": 0
                },
                {
                    "sent": "This is only thing you know about the reward.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll define the Thompsons, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, it is finite.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you mean bounded or do you mean finite or bounded?",
                    "label": 0
                },
                {
                    "sent": "It's bounded.",
                    "label": 0
                },
                {
                    "sent": "It may not be finite.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it could be a convex set or linear polytope, or.",
                    "label": 0
                },
                {
                    "sent": "Or a non convex set for that matter?",
                    "label": 0
                },
                {
                    "sent": "So I will define now Thompson sampling for this version of the problem and I will use Gaussian Gaussian belief distributions.",
                    "label": 1
                },
                {
                    "sent": "So one point I want to mention is that when I when I say that I'm using Gaussian belief, caution, likelihood, these are all just to define the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So caution, believe distribution is the way that the algorithm will maintain its knowledge.",
                    "label": 0
                },
                {
                    "sent": "It may have may have nothing to do with the actual reward distribution, so I'm not assuming that the actual reward distribution is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In particular, our regret bounds will hold for any reward distribution.",
                    "label": 1
                },
                {
                    "sent": "It will, worst case, regret bound.",
                    "label": 0
                },
                {
                    "sent": "So, So what you do is you start with a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "Simply simply covariance proportional to the identity matrix of dimension D an O mean.",
                    "label": 0
                },
                {
                    "sent": "And this coefficient will be making specific choice for this coefficient.",
                    "label": 0
                },
                {
                    "sent": "And what you can show by simple vision principle is that if the reward was had a Gaussian likelihood, so if the reward was coming from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Then at time T, the vision posterior would be simply simply this mean this here is the least square estimate calculated from T -- 1 step.",
                    "label": 0
                },
                {
                    "sent": "Just regularize simple regularly, square estimate and B.",
                    "label": 0
                },
                {
                    "sent": "Here Instagram matrix.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is simple Gaussian prior Gaussian posterior kind of calculation an and this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the Bayesian interpretation that we will use.",
                    "label": 0
                },
                {
                    "sent": "So again I want as I said, this is just to give a Bayesian interpretation to the problem.",
                    "label": 0
                },
                {
                    "sent": "It's it's not something that we will require for our proof.",
                    "label": 0
                },
                {
                    "sent": "For a regret bound.",
                    "label": 0
                },
                {
                    "sent": "That we do not require that the actual likelihood of reward is caution.",
                    "label": 0
                },
                {
                    "sent": "So using this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Brian posterior.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm that you get, so you simply use this Gaussian centered at the least square estimate and covariance as this proportional to this inverse of the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "And you sample a beautiful day from this caution D dimensional sample.",
                    "label": 0
                },
                {
                    "sent": "So that is your estimate of the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Now you would simply play the arm, which maximizes the mean.",
                    "label": 0
                },
                {
                    "sent": "If this was the true weight vector.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                },
                {
                    "sent": "If this was the true weight vector, what would have been the optimal arms?",
                    "label": 0
                },
                {
                    "sent": "You simply play that arm.",
                    "label": 0
                },
                {
                    "sent": "So if if a of T is some nice set, like a finite set or a convex set.",
                    "label": 0
                },
                {
                    "sent": "Then you are simply solving a linear.",
                    "label": 0
                },
                {
                    "sent": "Optimizing a linear function over a convex set.",
                    "label": 0
                },
                {
                    "sent": "So these are the specific choice of the coefficient.",
                    "label": 0
                },
                {
                    "sent": "So this choice of the coefficient actually in the experiments that people have done this is not very crucial, but it's useful for proving bounds.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I want to quickly compare this to the popular UCB based algorithm just to place them side by side.",
                    "label": 0
                },
                {
                    "sent": "And what's the difference so you can in some ways view this algorithm if you so this is the UCB algorithm for linear contextual bandits.",
                    "label": 0
                },
                {
                    "sent": "Is also called awful algorithm so.",
                    "label": 0
                },
                {
                    "sent": "What you have is there is remarkable similarity.",
                    "label": 0
                },
                {
                    "sent": "So we have this Gaussian with center at mu hat and be inverse as the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "In the UCB Amazing garden you take this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "Centered at Manhattan BB inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And so here we are sampling mutillidae from this Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Instead you take this ellipsoid and you find for every arm you find the best.",
                    "label": 0
                },
                {
                    "sent": "Mutillidae in this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So for every arm you're choosing, the best direction, best weight vector, and then you are finding the best arm.",
                    "label": 0
                },
                {
                    "sent": "So you are.",
                    "label": 0
                },
                {
                    "sent": "This is the optimistic principle.",
                    "label": 0
                },
                {
                    "sent": "So for every arm you say what is the best weight vector in its confidence ellipsoid?",
                    "label": 0
                },
                {
                    "sent": "And then you find the best arm.",
                    "label": 0
                },
                {
                    "sent": "Which will maximize its optimistic estimate.",
                    "label": 0
                },
                {
                    "sent": "So in so here you're solving a double maximization, so you're maximizing both over Mutillidae and Overby.",
                    "label": 0
                },
                {
                    "sent": "Whereas we this is like a randomized version where you sample one single mutillidae.",
                    "label": 0
                },
                {
                    "sent": "And then you find the best on so.",
                    "label": 0
                },
                {
                    "sent": "So this is so.",
                    "label": 0
                },
                {
                    "sent": "This is definitely much more efficient, cause here, even if you're you're set was a nice convex set.",
                    "label": 0
                },
                {
                    "sent": "Even then this problem would be a difficult nonconvex problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to illustrate pictorially why these this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Works or what makes it work is so you start with this ellipsoid, which is basically the covariance of the confidence ellipsoid of this Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Initially you start with a spherical Goshen, and it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a big ellipsoid initially, but as you go as you will progress in your algorithm, you have more and more confidence and the solution, so it will shrink what you can show is that the actual weight vector always lies in the ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So as your absurd shrink.",
                    "label": 0
                },
                {
                    "sent": "You are closer and closer to your actual weight vector, and finally your ellipsoid will be very small and you will with high probability you will always your weight.",
                    "label": 0
                },
                {
                    "sent": "Your estimate will be very close to your actual weight vector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the regret bounds that we can provide for this algorithm.",
                    "label": 0
                },
                {
                    "sent": "We show that with probability 1 minus Delta, where Delta can be choosen to be anything.",
                    "label": 0
                },
                {
                    "sent": "The regret in time horizon T for any T can be bounded by order of the square root T and some lock terms.",
                    "label": 0
                },
                {
                    "sent": "So this order notation just is just hiding some constants, not asymptotics bounds.",
                    "label": 0
                },
                {
                    "sent": "So finite time time bound and this constant, we can compute them, and we have them in the paper I just for clean display.",
                    "label": 0
                },
                {
                    "sent": "They're hiding the constants.",
                    "label": 0
                },
                {
                    "sent": "The dimension yeah D is the dimension of the weight vector and the context.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't have anywhere the number of arms, so you don't need to finite set of.",
                    "label": 0
                },
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "You for UCB based algorithm that I mentioned the best bound is very close is just away by just better by a square root log T factor which you can think of it as a cost that you pay for having a more efficient.",
                    "label": 1
                },
                {
                    "sent": "Algorithm I don't know, or it might just be our analysis.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The lower bound is is the square root T as well, so it just doesn't have this lower bound, doesn't have the log factors.",
                    "label": 0
                },
                {
                    "sent": "Describe how you could.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's the main.",
                    "label": 0
                },
                {
                    "sent": "So the remaining talk is just going to be about the proof, so I don't know if that's a good news.",
                    "label": 0
                },
                {
                    "sent": "Bad news for most people, but that's what we did.",
                    "label": 0
                },
                {
                    "sent": "So algorithm in principle existed.",
                    "label": 0
                },
                {
                    "sent": "We just specified the algorithm in terms of Gaussian priors and the main contribution was to get a proof so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the proof outline.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have 20 minutes?",
                    "label": 0
                },
                {
                    "sent": "10 OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah, hopefully I have this five slides of proof, so let's see so.",
                    "label": 0
                },
                {
                    "sent": "So the regret.",
                    "label": 0
                },
                {
                    "sent": "So the aim is to bound the regret total.",
                    "label": 0
                },
                {
                    "sent": "Regret and recall that regret at time T was defined as the mean of the optimum difference is the mean of the optimum and the play dumb.",
                    "label": 0
                },
                {
                    "sent": "B of T is the play dumb and B star of tea is the optimal at time T. So this is the sum of this quantity is what you want to bound.",
                    "label": 0
                },
                {
                    "sent": "So now the the key inequality in our proof would be to show that this regret at time T can be bounded in terms of the standard deviation of the play down.",
                    "label": 0
                },
                {
                    "sent": "I'm calling this as a standard deviation of an arm, so B is the context of an arm, and then this is what this quantity.",
                    "label": 0
                },
                {
                    "sent": "I'm calling the standard deviation and it will become clear in a minute.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm calling it a standard deviation, so this is the key inequality that you can bound regret at time T in terms of the standard deviation of the play down in the direction of the play down.",
                    "label": 1
                },
                {
                    "sent": "And why is that useful?",
                    "label": 0
                },
                {
                    "sent": "It's useful becausw.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this standard deviation in the direction of the play Dom goes down very fast.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "Because suppose at some time you didn't know about much about the direction of the play dharman you incur the high regret.",
                    "label": 0
                },
                {
                    "sent": "But then in the next step, because you have explored that direction, you know a lot more.",
                    "label": 0
                },
                {
                    "sent": "So either you are gaining in exploitation or you are gaining in exploration.",
                    "label": 0
                },
                {
                    "sent": "So if you're only considering the standard division in the direction of preedom, it is like a telescopic sum and it will go down very fast so that this sum the sum of these standard deviation overtime is not going to be very large.",
                    "label": 0
                },
                {
                    "sent": "So this inequality here was already proven in papers on UCB so you can show that that irrespective of what arms you play, this is not a probabilistic bound.",
                    "label": 0
                },
                {
                    "sent": "This is absolute bound.",
                    "label": 0
                },
                {
                    "sent": "So irrespective of all times you play this, some of the standard version of in the direction of the played arms is always going to be bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "Square root DT locked.",
                    "label": 0
                },
                {
                    "sent": "So then the so once you have this Ann, you have these supermartingale link.",
                    "label": 0
                },
                {
                    "sent": "So this is a supermartingale inequality.",
                    "label": 0
                },
                {
                    "sent": "We prove this for any given history of the place.",
                    "label": 0
                },
                {
                    "sent": "And by history we mean all the execution until time T -- 1.",
                    "label": 0
                },
                {
                    "sent": "This choice of arms the outcomes, reward outcomes and the context set of context time steps T so.",
                    "label": 0
                },
                {
                    "sent": "Is that for any planes or that's because you're using Thompson sampling?",
                    "label": 0
                },
                {
                    "sent": "This this inequality?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is because you're using Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is Thompson sampling specific.",
                    "label": 0
                },
                {
                    "sent": "This is for any place.",
                    "label": 0
                },
                {
                    "sent": "So once you have this supermartingale inequality and you have this, then you can just use Azuma Harding to prove that the sum of the regret is bounded by.",
                    "label": 0
                },
                {
                    "sent": "Buy this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So this is outline now so that I guess now the main job is to prove this inequality.",
                    "label": 0
                },
                {
                    "sent": "The sun landed our 2000.",
                    "label": 0
                },
                {
                    "sent": "It holds for any strategy of choosing choosing B of T. Because it's just like information theoretic bound because there's only so many directions to explore in a diamond in a set of dimension D. So.",
                    "label": 0
                },
                {
                    "sent": "So now our job is to prove this key inequality.",
                    "label": 0
                },
                {
                    "sent": "And because it's about regret bounding regret in terms of standard deviations.",
                    "label": 0
                },
                {
                    "sent": "What will be used?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to know what concentration inequalities we have.",
                    "label": 0
                },
                {
                    "sent": "So what can we say about the concentration of our estimates?",
                    "label": 0
                },
                {
                    "sent": "So first of all, what can we say about this muhi at the least square estimate?",
                    "label": 1
                },
                {
                    "sent": "So what has this is something which is also used in Ucb's because they also use the square estimate only difference is they used an optimistic.",
                    "label": 0
                },
                {
                    "sent": "Estimating the confidence ellipsoid and we use a random estimate so they already show that this something about this confidence ellipsoid so they show that mu the actual weight vector always lies in this confidence ellipsoid around new hat.",
                    "label": 0
                },
                {
                    "sent": "So using which you can say that so this fact that mu is in the confidence ellipsoid implies that for all.",
                    "label": 0
                },
                {
                    "sent": "All the all the possible context B.",
                    "label": 0
                },
                {
                    "sent": "The difference in the estimates difference in the means along the direction of B.",
                    "label": 0
                },
                {
                    "sent": "Is smaller than this quantity here GD times so GT is simply this confident the coefficient in the confidence ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So it's the time this standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So this is something you can show algebraically.",
                    "label": 0
                },
                {
                    "sent": "So, so we have a good concentration for the least square estimate and this is borrowed from UCB analysis.",
                    "label": 0
                },
                {
                    "sent": "What can we say about our our sample from the confidence from the Gaussian so we can say that the sample is going to be close to the the mean of the Gaussian within.",
                    "label": 0
                },
                {
                    "sent": "Within this route, locked factor with high probability.",
                    "label": 0
                },
                {
                    "sent": "These all these are with high probability like 1 -- 1 / T squared probability.",
                    "label": 0
                },
                {
                    "sent": "So combining these two observations, we can say that with high probability.",
                    "label": 0
                },
                {
                    "sent": "The mean at the mean at our sample for any before any any.",
                    "label": 0
                },
                {
                    "sent": "In our context, set the mean of our sample and the actual mean is going to be bounded by the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "Some factor of the standard deviation in direction of B, so these are all with high probability.",
                    "label": 0
                },
                {
                    "sent": "But for simplicity, let's assume that they hold for all history of execution, so.",
                    "label": 1
                },
                {
                    "sent": "So these actually don't hold for all 50 -- 1.",
                    "label": 0
                },
                {
                    "sent": "They hold with high probability for most FT minus one, but that is a simple thing to handle, so let's assume they hold for all history.",
                    "label": 1
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See if this is enough to prove our our inequality or not.",
                    "label": 0
                },
                {
                    "sent": "So suppose I try to apply this this concentration and try to prove our inequality.",
                    "label": 0
                },
                {
                    "sent": "So you can express the regret as the difference of these two means.",
                    "label": 0
                },
                {
                    "sent": "Now we try to replace them by standard deviations by using our concentrations.",
                    "label": 0
                },
                {
                    "sent": "So these are just ideas.",
                    "label": 0
                },
                {
                    "sent": "Replace them by concentrations.",
                    "label": 0
                },
                {
                    "sent": "And this difference because you know that you chose the arm to play, which maximizes this quantity.",
                    "label": 0
                },
                {
                    "sent": "So you know this is difference is going to be smaller than 0.",
                    "label": 0
                },
                {
                    "sent": "So you just get these two concentration terms.",
                    "label": 0
                },
                {
                    "sent": "The problem now is that this is nice.",
                    "label": 0
                },
                {
                    "sent": "We want this.",
                    "label": 0
                },
                {
                    "sent": "The play Dom standard deviation, but this is not something that we want.",
                    "label": 0
                },
                {
                    "sent": "Why becausw?",
                    "label": 0
                },
                {
                    "sent": "This term, this standard deviation, the direction of the optimal arm to argue that this goes down fast enough.",
                    "label": 1
                },
                {
                    "sent": "You have to argue that this is played often enough, but then if you can argue that optimal arm is played often enough, then you are actually proving what you want to prove ultimately.",
                    "label": 1
                },
                {
                    "sent": "So it's like a circular thing that's happening.",
                    "label": 1
                },
                {
                    "sent": "So this is something that we don't want it here.",
                    "label": 0
                },
                {
                    "sent": "Just for aside, this concentration would have been enough to prove this inequality for UCB, because you would use this concentration only for the play dumb.",
                    "label": 0
                },
                {
                    "sent": "And this quantity would be smaller because you choose.",
                    "label": 0
                },
                {
                    "sent": "You optimize over both mutual die and be.",
                    "label": 0
                },
                {
                    "sent": "So this concentration is enough to prove this inequality for the UCB, but not for us because we don't want this here.",
                    "label": 0
                },
                {
                    "sent": "So what we do is the trick.",
                    "label": 0
                },
                {
                    "sent": "The main trick that we have is that instead of getting bound in terms of the optimal Arminda play, damn we will replace this by standard deviation of from a class of arms which is played often enough and which includes optimal arm, but it may include other.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this class is the we call it unsaturated arms.",
                    "label": 0
                },
                {
                    "sent": "There are high variance, low regret arms, so low regret could be because they are very near optimum or high variance could be because they have not been explored enough.",
                    "label": 1
                },
                {
                    "sent": "So these classes actually the class you should be playing.",
                    "label": 0
                },
                {
                    "sent": "So either because it's regretted smaller because it has not been explored.",
                    "label": 0
                },
                {
                    "sent": "So this is actually the class you should be playing.",
                    "label": 0
                },
                {
                    "sent": "Often an you can show that you will play this class very often.",
                    "label": 0
                },
                {
                    "sent": "That's that's basically follows from the fact that.",
                    "label": 0
                },
                {
                    "sent": "Either they were optimal to start with or becausw they have high variance and therefore they have anti concentration of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian will choose them often enough.",
                    "label": 0
                },
                {
                    "sent": "So so so if you have this class which is played often enough.",
                    "label": 0
                },
                {
                    "sent": "You can basically show that this for standard deviation for this class of arms also goes down fast enough because they're played often enough.",
                    "label": 0
                },
                {
                    "sent": "So you can bound so if the bar is the arm with smallest variance among the highway.",
                    "label": 1
                },
                {
                    "sent": "Among these unsaturated arms.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                },
                {
                    "sent": "You can bound the standard deviation of this unsaturated.",
                    "label": 0
                },
                {
                    "sent": "In terms of the play down.",
                    "label": 0
                },
                {
                    "sent": "Simply from this fact that they are played often enough.",
                    "label": 0
                },
                {
                    "sent": "And now what remains to show is that you can bound regret in terms of not standard division, not of optimal and plate.",
                    "label": 1
                },
                {
                    "sent": "But of this unsaturated and play down.",
                    "label": 0
                },
                {
                    "sent": "And that turns out to be not very difficult because of this definition of the unsaturated arm.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you simply do is you bound the regret in terms of regret with respect.",
                    "label": 0
                },
                {
                    "sent": "To unsaturated arm.",
                    "label": 0
                },
                {
                    "sent": "And the difference of these two means.",
                    "label": 0
                },
                {
                    "sent": "So anyway, by so I guess I'm short on time, but you can easily bound regret in terms of these played and the unsaturated arm.",
                    "label": 0
                },
                {
                    "sent": "And to summarize then you use the fact that this regret this standard deviation goes down as fast as the play down.",
                    "label": 0
                },
                {
                    "sent": "So you get your desired inequality, regret it, bounded in terms of the standard deviation of the play down.",
                    "label": 0
                },
                {
                    "sent": "Once you have that, you just use.",
                    "label": 0
                },
                {
                    "sent": "Use this fact.",
                    "label": 0
                },
                {
                    "sent": "The inequality already proven.",
                    "label": 0
                },
                {
                    "sent": "For any sequence of player arms.",
                    "label": 0
                },
                {
                    "sent": "So that's the proof.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hopefully the So what I wanted to convey here is that so this inequality is actually something very general, and this inequality bounding regret in terms of the standard deviation of the play Dom.",
                    "label": 0
                },
                {
                    "sent": "So once you have this inequality, you can actually apply to many different versions of multi unbanded, so it's not so.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned initially, you can get bounds for various versions like sparse contextual bandits, kernelized contextual bandits and so on.",
                    "label": 0
                },
                {
                    "sent": "So hopefully I can convince you that we have now strong modular techniques that you can just pick and extend your bounds to different versions.",
                    "label": 1
                },
                {
                    "sent": "And we are in the state now that Thompson sampling can match even be attractive theoretically as attractive as UCB.",
                    "label": 0
                }
            ]
        }
    }
}