{
    "id": "twtulo47f4n3z4wmev5wyetaqgqruazk",
    "title": "NLP and Deep Learning 1: Human Language & Word Vectors",
    "info": {
        "author": [
            "Christopher Manning, Computer Science Department, Stanford University"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_manning_language_vectors/",
    "segmentation": [
        [
            "OK hi everybody, great to be here.",
            "I'll let me start off by asking a couple of questions so you guys have an opportunity to stretch.",
            "So how many of you have taken some language class?",
            "Either a linguistics class or an NLP class?",
            "OK, maybe 33rd 40%.",
            "Quite a quite a bunch.",
            "OK great.",
            "So how many people kind of glad that the content of this summer school is turning in the direction of language.",
            "But not the organizers, apparently.",
            "OK, that's good."
        ],
        [
            "So here's my plan.",
            "So I'm sort of teaching both my sessions today, so for the one this morning, the organizers titled it in or P101.",
            "I'm not quite sure what they hoped I do with that, but I I decided to almost take them seriously, and at least do some linguistics 101 and say some remarks about language, and so hopefully this is interesting to everybody, but in some sense I'm almost especially targeting with that.",
            "The people who haven't done linguistics or NOP V4 of kind of giving you some ideas about how people think about language and what some of the phenomenon in language 'cause.",
            "I think really there are lots of smart people here and maybe if you just know a little bit more about some of the cool things and language then you'll want to start doing natural language processing yourselves and then the main topic I'm going to talk about this morning is word embedding.",
            "So in starting to use in starting to use neural network methods.",
            "For natural language, almost inevitably the first thing that you have to deal with and know about is word embedding.",
            "So I'm going to talk about that.",
            "OK, then in the afternoon I'm going to present an approach to dealing with language that's featuring quite a bit of the work done in my group on deep learning and NLP, which is independent tree, recursive neural networks.",
            "And I'll explain what those are this afternoon and to some extent this material will then be able to be somewhat complementary to some of the material which will be seeing in the coming days.",
            "Well, when full blown some turns up.",
            "OK, and I should say that I'm totally happy to take questions at any point.",
            "So if you have any questions or thoughts, do feel free to stick up your hand and interrupt."
        ],
        [
            "OK, so let me just start off by saying I've sort of alternate some language linguistics content with some deep learning content.",
            "So let's start off with a teeny bit about the nature of human."
        ],
        [
            "Language, so why is language special versus other things such as vision?",
            "Anyone have a good idea on that?",
            "Yeah.",
            "More structure, maybe languages have more structure.",
            "God by that yeah.",
            "Sequential OK, so language has this very strong sequential structure and that leads into an interest in doing sequential models.",
            "And that's not true vision.",
            "That's certainly true, yes, Yasha.",
            "It's generated by human Berthings spoke about the star student in the class.",
            "And since I wanted.",
            "OK. Until it actually, but I want to focus on first of which is, so even more generally about when people talk about data mining.",
            "And sometimes then people move from data mining to text mining.",
            "I think you know their similarity of those terms.",
            "Kind of hides a fairly fundamental distinction, 'cause when your data mining, typically you're just collecting up a pile of data or whatever it is.",
            "You're Safeway shopping receipts.",
            "And you're trying to do statistics and find some order in it, but most of the time when we're doing text mining, that just isn't what we doing.",
            "'cause when we have text, it was created by a human being, and in almost all the circumstances that human being was deliberately trying to communicate information to some recipient or recipients.",
            "And so a lot of the time what we're doing with natural language is that we're seeking to recover the meaning that a live human being tried to encode in the signal.",
            "So if you look at deep learning, most of deep learning up until now has concentrated on the analysis of natural raw signals and things like object recognition and images, board calls detecting cancer where you've got these images and you're off signals and you're trying to find meaning in them.",
            "So in this circumstance it's really up to the beholder to try and find some meaning based on the analysis of the signal."
        ],
        [
            "Human languages aren't like that.",
            "Instead of human languages specifically constructed to convey the speaker or writer's meaning to the other people, I guess when I gave that list of deep learning applications for so far in the last slide, I kind of suddenly didn't mention speech recognition, 'cause speech recognition as the nature of raw signal that is in the same language space of being carefully encoded by human beings to transmit meanings.",
            "And so the really interesting thing is that human language is discrete symbolic categorical signalling systems.",
            "So while the human being I mean, I'm sure you're sure, tried to impress on you in the first day that human brains have these distributed real valued activation levels, and that's the natural state of the world.",
            "So why do human beings use a categorical symbol system to communicate with each other?",
            "Any ideas?",
            "Yeah.",
            "Yeah, so I mean obviously this is just a hypothesis because no one was there when language was invented, but it seems reasonable to believe that this comes out of information theoretic concerns that categorical systems of communication very, very.",
            "This is kind of easy stuff, right?",
            "But they're very very effective for being able to transmit signals over noisy lines and be able to accurately recover what is being transmitted, and so that's presumably.",
            "What human beings some way or another, decided and design their languages that way?",
            "I mean their teeny teeny exceptions to that, so people do do a teeny little bit of expressive signaling.",
            "So I'll say, I love it or whatever else I put down Hoopa and so obviously there's a little bit of meaning there that is being compared conveyed in some other way than the categorical signaling system.",
            "But, you know, that's really a pretty small.",
            "Exception and I think this is actually an important observation.",
            "Not forget.",
            "I mean in the modern deep learning is cool world.",
            "I think people sometimes think that symbols were something they were invented by 1950s and 60s AI researchers and they were just making a huge mistake.",
            "But actually symbols and symbolic systems were invented by human beings thousands of years ago.",
            "And the basis of our language."
        ],
        [
            "I'm here is another interesting thing about language, and it's something that we kind of don't really capture in our work on multitask learning and deep learning.",
            "So far, so humans have this notion of a language and words and meanings conveyed in a language, and to use their language, their encoding, the language in some kind of signal for communication.",
            "But there are at least three prominent substrates that can be used for communication.",
            "People can use sound as I am now.",
            "People can use gesture in sign languages, or people can use the images of writing to convey information."
        ],
        [
            "So these are three really, really different encodings in a signal, but to a human receiver of the message, the human receives essentially exactly the same message, no matter which of those signaling substrates the person is chosen.",
            "OK, so this is kind of interesting, so if we think about what human beings do, we have these distributed representations Now brain.",
            "We then convert them into a symbolic encoding to communicate with other human beings and those, and that symbolic encoding is then transmitted via a continuous signal and then on the other end the decoder kind of does the reverse.",
            "Write the decoder.",
            "Converts the continuous signal back into a symbolic representation of words, which then somehow go into a distributed representation in their brain."
        ],
        [
            "And so we're always going from symbolic and distributed at back, something that just struck me while I was putting together these slides is in practice for what we do in natural language processing with sort of doing the opposite of what brains are doing.",
            "So brains are starting off distributed, converting to a simple like representation for transmission, which gets converted back into distributed representation.",
            "Where almost inevitably, when we're building deep learning NLP systems because we start from the found data which is it's in.",
            "It's simple like encoding.",
            "We're almost always doing the opposite.",
            "We're starting off with something in symbolic form, where then building a distributed representation of language that we use in our deep learning system, and then nearly always we then converting that back to a symbolic representation, which is whatever kind of categorisation or semantics or task.",
            "That we're doing at the end of the day.",
            "OK."
        ],
        [
            "So that's human languages.",
            "OK, so let me go on and start to say a little bit about distributional word representations, and so this is one of the big changes that's come in with neural network approaches to language."
        ],
        [
            "So that's not really new with them, so most of both traditional rule based NLP and statistical NLP of the 1990s and 2000s worked with words as atomic symbols, hotel, conference, walk.",
            "So despite the fact that there are continuous continuous values for the probabilities and statistical NLP, those probabilities were still placed over symbolic representations, and so if we convert, think about that in vector terms.",
            "Then what we have in CS1 hot encodings, where symbol is represented something that is a one in one position and a zero everywhere else, and what's been widely argued is that that's a cause of a lot of the problems with for natural language processing."
        ],
        [
            "So we think of it perhaps as a web search example that if somebody searches for Dell notebook battery size, we'd like to match a document that talks about Dell laptop battery capacity.",
            "Becausw laptop is like notebook and capacity is like size or usage.",
            "Searches for Seattle motels.",
            "Seattle Hotel should also be relevant and the problem is that if we have these one hot encodings, these vectors are orthogonal.",
            "So they have no natural notion of similarity, and so a lot of what we're going to be achieving by having distributed word representations is building a representation of words that does build into it, and natural notion of similarity a distance measure between words, and that's proved to be an incredibly useful and important idea.",
            "OK, so how do we go about building a distributed representation for words?",
            "Embedding OK, but how do we go about embedding?",
            "Said that what they thought source of information we used to learn the representation of the word.",
            "Context context yeah OK?"
        ],
        [
            "So the key idea is that we do distributionally learning, so we learn to represent a word by its context, and so this is referred to as distribution.",
            "All models of meaning or also as you space models and meaning that your idea of the meaning of a word is the context in which it appears.",
            "And this is actually, you know, an idea with longer historical roots.",
            "So if you go to the.",
            "Letter writings, but not the early writings of Ludvik Sunstein, he exposes a view of meaning in language, where meaning should be used, conditions that you've got this context of use, and most famously in linguistics, is normally attributed to Firth was a British phonologists who said you shall know a word by the company it keeps, and you know this is an idea that's much broader than just deep learning approaches to NLP, and it's really being one of the most successful ideas of modern empirical.",
            "An RP.",
            "So if we want to know what the word banking means, we collect up a bunch of uses of the word banking where you look in those those contexts of use, and then we're going to do something based on those.",
            "Something that involves counting to come up with some kind of distributed representation for the word banking."
        ],
        [
            "OK, and so that's the basic idea, so here is very simple toy model just to get the ideas.",
            "OK so we choose some model that aims to predict the word based on other words in its context.",
            "So we might say will use the word to the left and the word to the right.",
            "Let's assume they have a distributed representation, will average them and then will want the dot product of that with the word in the middle to be large.",
            "And so that gives us a loss function, which is how much that isn't true.",
            "And then we can shove around our vector representations to make it more true, so that we have less loss.",
            "And if we keep on looking at words in contexts and keeping on shoving around the word representations and little in the kind of online learning methods that have been talked about in preceding days, then we'll get distributed representations for words that capture."
        ],
        [
            "Their meaning, and so we'll end up with something that the word take is represented with vectors.",
            "Something like this, and so this idea has been just super super successful, so that if you adopt this kind of approach and just start with a large corpus of text and shove around the elements of your word vectors to make words appear similar to their context, it just works absolutely beautifully for clustering words.",
            "So here are forms of the word, haven't hear words of the form.",
            "The and right next to be as remaining become, which are also copula verbs for those people who weren't that kind of stuff, and grandma, and here are other verbs over here and come and go all right next to each other as you think and say and think are right next to each other.",
            "Should want right?",
            "Like you, it's sort of.",
            "I think it's just something that's gotten anybody who's done it.",
            "Super excited that you just do this naive procedure of fiddling around word representations to minimize loss and sort of beholden miracle occurs.",
            "That you feel that you've learned the meaning of all words by representing them in high dimensional vector space."
        ],
        [
            "So when we do this, we have distributor distribution or representations which are two words, both of which start with almost exactly the same beginning.",
            "So let me just get that completely clear.",
            "So the notion of distributed is this idea that you have a concept represented by continuous levels of activation across a number of elements.",
            "So a distributed encoding is to be contrasted with a local encoding like A1, hot vector and so.",
            "That's distinct to this idea of distributional representation.",
            "So distribution ull means that meaning is encoded by contexts of use, and that's in contrast to other models of semantics and word meaning.",
            "The most famous and most used that I'll come back to later is denotational, meaning notions of meaning.",
            "So denotational notion of meaning is that you're picking out the set of the set of objects in the world or in a model that satisfies, so the meaning of laptop isn't its context of use.",
            "In text, the meaning of laptop is the set of that thing.",
            "That thing, that thing, that thing and 100 others in this room that I won't point to.",
            "And so, in particular, you can do build distributional models of meaning that aren't distributed, and in fact there have been some well known ones in NLP.",
            "They're actually also very successful, so an alternative approach that's been used a ton and NLP is just to do hard clustering of words in a couple of well known techniques.",
            "Brown clustering, exchange clustering, where you're using the same context of use and you're building these hard clusters of similar words and that also you know that.",
            "That feels like it shouldn't be as good as having these.",
            "Vector space embeddings, 'cause you know you're either in the same cluster or not, but in practice it's still being good enough that it's been a very useful technique in a lot of NLP.",
            "OK, so we've learned this wonderful model of word similarity with our vector space learning model, and that's really cool.",
            "But something that excited a ton of people and led to the sort of current huge resurgence of interest in distributed representations, well?"
        ],
        [
            "Tama Schmeeckle, often colleagues in 2013, demonstrated this further thing that you could do with word vector spaces, which is that you could actually look in directions in the vector space and find directions in the vector space that encode elements of meaning, and so the way he showed that is by doing.",
            "These word analogy tests so that you sometimes see these on the kind of University admissions tests or something like that law school tests, I think, have, so you have these areas.",
            "To be as C is 2D kind of questions, so it says man is to woman as King is to Queen.",
            "So it turns out that with these vector space models you can start off with the vectors of King, Man and woman.",
            "Here they are in the vector space and then you can take a difference vector for what is the difference between man and woman.",
            "And then you can move that difference vector up so it sits off King.",
            "And then say that the way I want to answer this analogy is by finding the word that's closest to that point up there where I've shifted this vector up to here and lo and behold, with quite reasonable accuracy.",
            "It turns out that the world you find right there in the space is Queen, and so that's.",
            "I'm quite incredible and so formally you're doing this so you just finding the word that is having the highest dot product with the.",
            "Adding this on to there.",
            "OK, and so Mikro phone call."
        ],
        [
            "Lakes showed this in 2003 and in their results that they showed that this method using neural encodings worked considerably better than previous methods that people had used to address some of the same problems.",
            "So they had two kinds of analogies, syntactic and semantic ones, and from the syntactic ones.",
            "They compared against the traditional method.",
            "I'll come back to latent semantic analysis and behold.",
            "The neural network was over as far as getting things correct, and then for the semantic analogies, they compared on a shared task where previous system at that one the shared task got 23.",
            "The Spearman's rho coefficient and they were able to improve on that and get a new state of the art, so this made it appear that neural word embeddings were just much better than any traditional methods of coming up with distributing codings of words.",
            "OK, and.",
            "Wait is this?",
            "Although this this is the dimensionality of the vectors.",
            "Yes, very large.",
            "OK, and in general having these distributional representations of words, it's just been a really useful tool to improve our NLP."
        ],
        [
            "Distance so if you look at symbolic NLP systems, even probabilistic symbolic in RP systems, they're very, very fragile because of their symbolic representations.",
            "So here's an example from the Stanford parser, so I'm not picking on anybody else.",
            "Here is a very simple sentence.",
            "My dog also eats oranges, and it passes it correctly.",
            "That's what you'd hope.",
            "But if you actually change out oranges and put in bananas.",
            "Lo and behold, it pauses the sentence wrong way.",
            "So rather than regarding bananas as a noun phrase like oranges, that comes up with this funny parse where there's an adjective phrase here and that's analyzed as a sentence, and eat is taking this complement of this sentence, we had stuff going on.",
            "Now, you know, if one looks in the training data for the parser, it's perfectly obvious why this happens.",
            "It turns out the word bananas only appears once.",
            "In the training data that's used for these parsers, and that one usage of bananas is in the sense of go bananas.",
            "So here we have eats bananas in the pan is giving it the same kind of analysis.",
            "So yeah, so explainable, but that's obviously not the kind of behavior that we would like.",
            "And if we actually had the model, know that a word like bananas is very similar to oranges and apples.",
            "Then it should be able to share information across usages of those various words and be able to make less mistakes like this one."
        ],
        [
            "And in general, putting in these kind of distributional representations has just given NLP systems, and Norma's performance gains.",
            "So and things like parsing you can get large error reductions by using distributional word representations.",
            "Named entity recognition.",
            "Large error reduction is an often cut out about 1/4 of your errors."
        ],
        [
            "OK so yeah.",
            "But is it really really?",
            "Yeah.",
            "Yeah, so maybe I should come back to that.",
            "'cause I do actually have a couple of slides towards the end that start to say something about word sensors, but I mean it's certainly true and worth thinking about that.",
            "We've got this naive model at the moment where we've got a string and we just find every context it appears in and build some representation based on that and that could be bad for all sorts of reasons of word.",
            "Might have multiple meanings or the usage of a word.",
            "In the news why that you trained it on might be completely different to the usage of the word in medical texts or something used at runtime, yeah?",
            "Yes, yes that that.",
            "Yes, so that is another big well known problem with these word representations is almost always if you build them in the obvious ways that you have words in their antonyms.",
            "Being almost identical, whereas you feel that they should be very different.",
            "She wants to say something, but I think maybe actually I should say I'll let him say his piece, but I think really some of these questions are getting a bit ahead of things 'cause I haven't really gone through the details of building a word embedding before we talk about their problems.",
            "Yeah, it's a couple more.",
            "OK, OK so in almost all deep learning work in NLP, the first stage is mapping words symbols to the distributed vector representation.",
            "In fact, doing this is so useful or so easy you're both, but many people never get any further, so in the last years, NLP conferences.",
            "They were completely full of papers using word embeddings and a lot of them didn't actually use anything more than word embedding, so I do think it is worth actually reminding people that just using word embeddings isn't deep learning.",
            "Yes, you've got distributed representations that there's actually no deep learning right.",
            "Deep learning is a distinct idea where actually claiming, but by having depth of representation, that means that we can do more.",
            "OK, yeah, So what I want to do now is to go a bit more in detail and what people have done with neural embeddings.",
            "Get word vectors and something that turns out to be the case from this is although people initially got very excited about newer word embeddings and their performance that it turns out that some of the communist most used forms of neural embeddings actually turns out they're not really different from the kind of distribution or learning.",
            "Representations that people had been using elsewhere in IR and NLP in the 90s and 2000s, and that they actually relate very closely to them and I want to say a bit about that."
        ],
        [
            "OK, so the most famous example at present of newer word embedding is word two VEC, which was a piece of software released at Google by Tomasz Michaelov."
        ],
        [
            "And so, with word to VEC, you get a big supply of text.",
            "Wikipedia is generally recommended.",
            "You stick it into the word to VEC program.",
            "You wait a couple of hours.",
            "It's actually pretty fast as these things go.",
            "An outcomes matrix where you have one vector as the representation of each word.",
            "To a certain number that's found in your source text."
        ],
        [
            "OK, so the idea of this is that in Word to vector in general, in the new embedding models that you don't directly capture cooccurrence accounts like in some traditional methods like latent semantic analysis.",
            "But instead you set things up as a prediction task.",
            "So you're trying to do prediction between words and their contexts and worked avec isn't one thing.",
            "So word to vec, there are two primary algorithms that included in it.",
            "One of which kind of going opposite directions, so the skip grams algorithm is trying to predict words in the context given the target word in the middle, whereas the continuous bag of words model is doing the opposite is trying to predict the target word given a bag of words context.",
            "And crossed with that is the actual training methods they used in Word, two VEC and there are two different practical training methods implemented in Word.",
            "Two VEC ones, hierarchical softmax, first pioneered by Joshua and the other one is negative sampling."
        ],
        [
            "Oh yeah, OK, so.",
            "So what I'm going to do here is sort of say a bit about the Skip gram model.",
            "So our idea is we've got the target word in the middle, and what we're going to want to do is predict the words in the context of the target word and our loss function is going to be given by having a probability of predicting these.",
            "Each context word and we want to maximize the probability of each context word and each context word is predicted separately, so this probability immediately pulls apart by treating each prediction as independent.",
            "OK, so this is what the model looks for, like for the Skip gram model, and it's really a very simple model, so we start with the word identity for the target word.",
            "So this is just A1 hot vector.",
            "And so we're then going to multiply it by a matrix to get a hidden representation.",
            "But since this is A1 hot vector, that just means we're going to select one column.",
            "Right column row.",
            "Column one column of this vector of this matrix and that will be a representation of the target word.",
            "OK, so then from that hidden representation we're then going to multiply by one other matrix, and this matrix is then going to give a probability distribution over the different words being being context words, and so effectively.",
            "Our word embeddings are contained in these two matrices, and so we actually have two different embeddings.",
            "For each word, we have one embedding for it as a target word, and one embedding of it as a context word.",
            "OK, so the naive algorithm."
        ],
        [
            "Goes like this so our loss function as we want to maximize the probability of this assignments of the words and context.",
            "We immediately assume independence between each word.",
            "And.",
            "OK, so we then go into work for the words in terms of their word embeddings.",
            "So now we turn from the word ID's to their vector representations and the vector representation of the import is just the hidden layer and then we've going to learn a vector representation of the output and we're turning.",
            "We're making our probabilistic model using this kind of softmax model, so our model of probability of an output word given an input.",
            "What output were given input word is the probability of an output vector given input vector and that is done as a softmax model over the DOT product between the two vectors.",
            "OK, yes.",
            "Wait, so far we haven't used either of those.",
            "I do not have any either of those, right?",
            "Can we wait a moment?",
            "I'm not sure what your question is, but I haven't actually done either of those, yeah.",
            "So so yeah, you have this super high dimensional space of all the output of all the words in your vocabulary, and you're giving a probability estimate to each one.",
            "So practically, people start off by choosing some vocabulary, but you normally want the vocabulary to be pretty loud, so your vocabulary might be 100,000 words, 200,000 words.",
            "And for those words, Yep, you've got them in your vocabulary and you're giving them each probability estimates and all remaining words you map to one or more unknown tokens.",
            "Sometimes you just have one unknown, sometimes you have a couple more 'cause you might decide that things like numbers or an actual class that you.",
            "Map to an extra 9 token.",
            "OK so yeah.",
            "No, it's a crummy assumption.",
            "'cause obviously, which for almost any word the chances of word being to build another world being to the left or the right of it will be extremely different.",
            "And the chances of being one or more apart will be different as well.",
            "So I mean, it's actually a quite poor assumption and there are some other work methods of learning your embedding, such as the.",
            "Model proposed by color Band Western that don't have that property.",
            "Their positional and I'll come back to that.",
            "But despite its crummy nurse bag of words assumption, I ignoring order like this is just something that's very commonly used in NLP because it makes things a lot simpler because you make independence assumptions and commonly for a lot of semantic key tasks of meaning.",
            "It doesn't actually behave that badly.",
            "OK, yeah.",
            "Yeah, so as presented, yeah, so there's this slow distribution of very common words like that of two R and then going down into very rare words and actually forgetting models like this to work really well.",
            "A crucial crucial thing is how you do the scaling.",
            "Of the frequency of words which is responding to that this law distribution.",
            "But again, that's not what I've been presented so far.",
            "At the moment I've still got a naive one which is just using probabilities.",
            "OK, so suppose I'm still in this naive world.",
            "Or suppose you're still in this naive world for one more minute, and this is the model and you want to get it running on your laptop to learn vector representations of the input words.",
            "And the output words.",
            "How would you go about doing it?",
            "Fire up theano.",
            "So, So what are the algorithmic steps that you go to if you wanted to train word representations using this kind of model, how would you start?",
            "What would you do?",
            "OK, so you wanted to use gradient descent and so in order to be able to use gradient descent you would do what?",
            "Height.",
            "OK, so we want to find the derivatives of this so it take this softmax form and find the derivative with respect to what?",
            "The parameters so the parameters are simply our word representations, right?",
            "So those are the only parameters of this model, and as I know everyone feels like they could take the derivative of this with respect to.",
            "Different parameters.",
            "Honest question yes no.",
            "Some people think if there if they were forced to do some work, that they could take the derivative of this with respect to the parameters.",
            "The elements of the Vector H Joshua curd.",
            "OK, we can stay.",
            "Yeah, I mean.",
            "I guess I don't know what's useful, I mean.",
            "I could kind of stop and actually quite we could do that concretely, and you could see me screw up all the indices of everything on the blackboard.",
            "But would that be useful to people think that they could work it out or start to figure out how to do this?",
            "Yeah.",
            "OK, I hope so.",
            "OK so yeah, So what you do is you just differentiate with respect to each of these parameters and so it sort of turns into a kind of a pretty example.",
            "I can radiate it, add slides that show it afterwards, right?",
            "So that you can sort of split this apart.",
            "You know you take the product outside the log and turn into a sum.",
            "He then separate the numerator and the denominator and you'll play the chain rule backpropagated all figures out.",
            "In the end."
        ],
        [
            "OK well so.",
            "Apparently.",
            "OK, so if you if you did that, and supposing you got all The Drifters, rident passed gradient checks and all that, and you started running on your laptop, you're likely to be pretty sad.",
            "Why are you likely to be pretty sad, yeah?",
            "Yeah, because of this normalization term.",
            "So here we are.",
            "Normalizing over the entire vocabulary and if we have a vocabulary of 200,000 words, that's a huge space here, right?",
            "So I mean back in the."
        ],
        [
            "Model this part is trivial because it's just selected column out of a matrix that here to workout these output probabilities we're having to calculate and normalize over the entire vocabulary at each step."
        ],
        [
            "So that just doesn't work, and so in the practical version, as in Word to VEC, two methods are then used which people already mentioned.",
            "So one of which is the hierarchical softmax, which I'm not going to."
        ],
        [
            "Talk about and the other one is Skip Gram well as the skip gram with negative sampling and so the idea of this is that we're actually going to change things slightly so effectively using a little logistic regression model and the little logistic regression model is wanting to workout with the word context pair is good one you actually found in your training data or one that is probably bad.",
            "One that wasn't found in your training data, and I say probably bad because you generate the negative examples by just randomly pairing a word in this context so you know sometimes those will be pairs that do occur in the training data, but most of the time they won't be, and so something that's crucial to note.",
            "Then, right?",
            "That if we're getting positives from the training data, how often we see them scales with the problem, the joint probability of a word appearing in the.",
            "With a certain other word in the context, whereas if we are randomly selecting our negative words, how often will get to them is decided by the product of the probability of the target word.",
            "Because we're going through the target words, selecting them times just the independent probability of the context work.",
            "'cause that's how often will select it."
        ],
        [
            "OK, So what we do with the Skip gram negative sampling is what we're wanting to say.",
            "We're wanting to set the parameters now to maximize the probability with.",
            "Switzer model says that something that's from a true context is good.",
            "So D is our true contexts and that's one.",
            "And then we want for things that are in.",
            "Invite Contacts with the tilled.",
            "We want to say that they are not good but D = 0 and so that then model for this probability is then this logistic regression form and so we're working things out with that and so doing this we're now in a state that's way way more computationally feasible."
        ],
        [
            "So in particular, what we have is this sort of binary logistic regression form.",
            "So for each word in context we workout its probability in the binary logistic regression form.",
            "We pick some number of negative examples where we randomly sample words and we then also workout a logistic regression form for them and we want them to have a low probability.",
            "OK, and so.",
            "If we are doing nothing more than that, the interesting thing is that when we have our two word representations, the target words and the context words that we can actually think of this as doing a matrix factorization where we're factorizing matrix, which is the pointwise mutual information between words."
        ],
        [
            "And where that come?"
        ],
        [
            "It's from is right back here.",
            "Good examples are using the joint probability of the words and our fake examples are using these independent probabilities of the words.",
            "So when we then taking the ratio of those in natural space, which is then the difference in."
        ],
        [
            "In log space and this is our difference in log space that that's giving us a pointwise mutual information.",
            "So the pointwise mutual information is like the mutual information, but you're just taking the one term of it, so it's the log of the probability of XY over the probability of X times the probability of Y and pointwise mutual information.",
            "Is this very famous long used quantity in NLP for having a kind of an Association measure?",
            "Between a pair of words and so this result was shown by leaving Goldberg at NIPS 2014 and in some sense that kind of changed the perspective on the specialness of neural word embeddings.",
            "Because this made it made it clear that really what word affect learns is actually very much in the space of what more traditional methods such as latent semantic analysis learn.",
            "That they're doing this starting with account there, starting with account matrix.",
            "Here it's a target word context word Co occurrence, count matrix, and what they're doing is a matrix factorization, and that is how they're getting their representations for words."
        ],
        [
            "OK, of course.",
            "Word two vec isn't the only method of doing your own bedding, so most of all of the current work in this field started out with the author of NGOs work in 2003 of the Neural Probabilistic Language model, which was this basic idea that you could have a better language model by using distributed representations and then those other work on doing your language models.",
            "I'm not going to talk anymore about that now though.",
            "So I was actually going to say more about neural language models later on in the century.",
            "From there there was a couple of steps and the steps we're essentially to make things simpler and faster.",
            "So in Colburn Westerns work, they said well if we just want word representation.",
            "So we have useful similarity measure on words for tasks like named entity recognition, part of speech tagging.",
            "We don't need to worry about this stuff and making sure it's a probabilistic model.",
            "We can just learn word representations and so they did that, but these models were still really, really slow to train.",
            "I think color in Weston spent seven weeks training.",
            "There's or something like that, and so in the more recent work such as me crossword avec.",
            "The suggestion was that maybe we could just use a much simpler model for sort of by linear model that I've just presented, and that would be enough to produce a good word representation that work great for tasks.",
            "And in fact, what was suggested is that you could kind of trade complexity for representation size.",
            "So what people were found with models like word to VEC is you can get extremely extremely good performance by using large vector sizes, whereas conventionally people use sizes like 5100, two, 100 that people now commonly using sizers of 308 hundred 1600 and getting better performance, and they're able to estimate those models because these methods are very.",
            "Simple.",
            "OK so but I mean the the result just pointed out about the sort of matrix factorization holds a word to VEC.",
            "It doesn't hold straightforwardly of these other models.",
            "I said the wifey.",
            "OK, but the word defect code is fast produces good results in Google, so it's become very pop."
        ],
        [
            "Pure and everyone uses it as you can see in this Google Trends graph.",
            "It's just really taking off.",
            "You"
        ],
        [
            "There.",
            "Let's see.",
            "OK, so.",
            "I think I might go quickly on this next bit.",
            "I mean, I think in some sense, the leafeon Goldberg model nailed some of the connections between word to VEC and conventional models, but it was actually an area in which me in a postdoc, Jeffrey Pennington also worked at the same time and we had kind of similar thoughts of saying.",
            "Surely there's a middle ground between neural embedding, learning methods and traditional methods of working by manipulating counts.",
            "And so we came up with our own model that was called Glove, so that this central contrast."
        ],
        [
            "Is that there are these predict models like we've talked about?"
        ],
        [
            "And then there are traditional models that will count models where you start off with account matrix and you do some kind of factorization and the traditional way of doing that was the SVD which then gets called latent semantic analysis, or latent semantic indexing.",
            "And for people initially thought, including as late as in this 2014 paper by Barone was that for some reason the predict based models just worked a lot better than the count based models.",
            "But actually that just turns out not to be true because of really you have this theorem about how word to vector skip Gram can be thought of as a factorization of account based model.",
            "And in fact, somewhat Interestingly, the fact that you get these linear relationships in space had actually been observed well before michaelov so bad."
        ],
        [
            "In 2005, Doug Roadie, who was Psycho Linguistics student and his interest in psycholinguistic modeling.",
            "He built LSA based models of distributed word representations and what the crucial thing he did was the count.",
            "Modified LSA and Doug Grady show that in his Kohl's model, which was discount modified LSA, you got exactly the same linear structure.",
            "Then Make Love showed in his analogies.",
            "So here we've got drive driver swim swimmer, teach, teach.",
            "Mary Pre stripe that.",
            "Anne.",
            "So.",
            "So that you're getting the same kind of linear relationships, and so it turns out that actually a lot of the important part of getting out these nice linear relationships is that count modified bit.",
            "And when people started looking more closely and make Love's source code what people observed was that there's actually some very, very sort of.",
            "I'm not sure clever is the right word, but very careful, just right scaling of counts and making use of.",
            "Contacts with different probabilities rather than naively always sampling at the same and a lot of the success of his methods comes with these fine points of the estimation that go beyond the naive equations that were written up.",
            "And so that was an idea that."
        ],
        [
            "We were also interested in with how we could connect together.",
            "These count based models and direct prediction and the advantages of both."
        ],
        [
            "Earth, and so we developed a model to do that.",
            "Which was the glove model and I'll just say a little bit about that.",
            "I mean, I think the good intuition here is that what you want to have to represent meaning components is ratios of Co occurrence probabilities.",
            "So if you have a word words occurring in the context of ice, you'd expect solid too often occur with ice and gas to rarely occur with ice.",
            "So that looks like you're capturing a meaning component of ice, but simply knowing that those are large and small isn't enough cause the word water often occurs with ice and a random word like random is unlikely to occur with ice, and so those are large and small as well, and you get the same properties reversed the steam with solar and gas.",
            "So if you really want to get out the meaning component that can, that is the dimension where you go from solid liquid gas.",
            "What you want to do is take the ratio of these Co occurrence probabilities because then you get a large to small dimension here where these ones will cancel out about one and well, that's just my made up large and so."
        ],
        [
            "Oh, but it turns out if you count up on Wikipedia, that does actually basically work for you, right?",
            "These ones come out about one, and these come out is about 10 and a tenth, so that works nicely, and so that's the prob."
        ],
        [
            "What do you want to capture to get linear relationships in a vector space?",
            "And so how can you go about doing that?",
            "Well, it seems like the simplest way you could go about doing it is precisely this log bilinear model where if the vector representations of words are such that their dot product corresponds to the log of probability of Co occurrence, then immediately when you can put in differences between vectors.",
            "You're going to get the log of the ratio of chances of Co occurring together."
        ],
        [
            "And So what we did for the glove model is explicitly optimized parameters to get that criterion.",
            "So here it is with a squared loss criterion.",
            "We're saying we want the dot product of two word vectors to be as close as possible to the Co occurrence frequency, which of course is sort of just relative to the Co occurrence probability.",
            "And so that works great again if and only if you're careful to manipulate counts.",
            "And this comes back to the Zipf's Law part, right?",
            "So if you do nothing for ether word to VEC or this glove model but sort of used the raw probabilities, it works terribly, and so you want to use some method that scales the counts to avoid being overwhelmed by the Super common words, and perhaps to boost the rare words.",
            "And both were developed in the glove model.",
            "Do is and the model is used in the glove model.",
            "Is this very simple scaling?",
            "So for word frequencies up to a certain point, you're using this slightly curved three quarter power, which seems in practice to work well, though it's just sort of a hack constant and then it just saturates sofa.",
            "Very common words.",
            "They're not sampled anymore frequently.",
            "Maybe?",
            "Yeah.",
            "Fallen short may occur with.",
            "Yeah.",
            "So so if you sort of just build these models from a lot of text and distributional context, I mean so the answer is you don't.",
            "They end up very similar in meeting, so that's obviously not what you want to do.",
            "So people have explored methods to fix this as one paper from Microsoft afew years ago.",
            "Where they explicitly kind of hardwired the model to may have antonyms be far apart from each other, and that's sort of work, but it seemed very ad hoc and specific to antonyms.",
            "The may.",
            "Yeah, the main way that seems at least better than that is to embed the word vector learning in some tasks.",
            "That makes it necessary to keep antonyms apart, and so one clear example of that is if you do sentiment analysis that as soon as you do sentiment analysis, words that are antonyms, like good, bad, heavy light, get pulled apart.",
            "'cause that's a primary dimension of automation and you're sure suggests another example.",
            "Which was translation.",
            "Yeah."
        ],
        [
            "OK, skipping a bit showing you more glove stuff, but I mean here, here is just an interesting thing to know about if you're doing this for yourself.",
            "So in learning these word vectors.",
            "But it turns out that you know Wikipedia just works way better than anything else that you can get text data for, and the way that makes sense, right?",
            "It's the nature of an encyclopedia that it's just having a lot of meaning of words and discussing their contexts of use and things like that.",
            "So if you want to know about the meanings of words and how they relate to each other as meanings, Wikipedia is distinctively useful.",
            "So here's a 2010 Wikipedia dump with a billion tokens.",
            "And these are results from our glove model, but I think broadly similar things should see with word vector whatever, and so it does brilliantly well on semantic analogy tasks.",
            "It's getting about 80% using these big several 100 dimensional vectors is not very much text to train on, so it's doing relatively modestly on syntactic analogy, task.",
            "And then this is the overall average.",
            "If you wait for years, Wikipedia is grown, and that's had a slight positive effect.",
            "But the contrast here is that if you instead train on giga word, which is a newswire corpus, that's very commonly used in NLP.",
            "It's about three times the size, and because it's about three times the size you do a bit better on syntactic analogy task.",
            "As you learn more about word contexts, but you're doing way way worse on the semantic analogy task, as reading newspapers just doesn't give you the same kind of meaning of words information.",
            "What you get from reading Wikipedia?",
            "People don't seem surprised by that.",
            "OK."
        ],
        [
            "OK.",
            "Done, done.",
            "I'm something is kind of cool, but I'm not going to talk about today is, I think it's cool that some of the CS theory people are starting to get more interested in machine learning and deep learning because it's become such a hot area.",
            "And in particular Sanjeev Arora's been devoting quite a bit of time to trying to understand how you get these kind of linear properties in world spaces and is developing a generative model to do so.",
            "Do you wanna ask a question?"
        ],
        [
            "There's not so many.",
            "English language.",
            "So I didn't sort of really unexplained explain what this syntax is.",
            "I mean syntax and in mcleods analogies are things like being able to go from thin, thinner to easy easier, so it's so not really syntax.",
            "It's sort of more sort of morphology or things like that, and I think you know for facts of that sort, it still turns out that just having seen more text.",
            "That you just have better representations of rare words and knowing more of their plurals and things like that.",
            "OK."
        ],
        [
            "So what else do we need to know?",
            "Most words have multiple meanings.",
            "The first word that always comes to people, it came up before was bank.",
            "But you know it's not only the word bank that has multiple meanings, so Pike.",
            "What are some of the meanings of the word Pike?",
            "People know the word Pike.",
            "Uh, they said there's a polearm right that you can then, yeah, that's for the kind of creative anachronisms people OK.",
            "It's a poem.",
            "It's a kind of a fish.",
            "Any other meanings for Pike?",
            "Are Turnpike yet so this uses a shortened form of Turnpike.",
            "I took the Piketon work.",
            "Yes three other ones.",
            "Really.",
            "OK, I didn't know that.",
            "I'm sure another Vince Summers lexical access is actually difficult for human beings, but I'm sure another meaning that most of you know is it's a kind of dive.",
            "Anyone who's watched the Olympics.",
            "They've seen people doing a Pike, yeah?",
            "OK, so you know words have lots of meanings, so almost all of the current work on your embeddings and is just worked on this simple method where you have one string has one vector representation and that's kind of a crummy linguistic model because words have multiple sensors what So what you actually get is that it's representation is a weighted average of what should be the representations of the different sensors, which when people are being fancy they caught a superposition.",
            "And you could think that this is such a crazy assumption when here we have this word Pike, which could be referring to a Turnpike or medieval weapon, or fish or dive.",
            "How could these systems possibly work?",
            "How could they possibly work?",
            "Anyone have an idea?",
            "Anyone else?",
            "Tight.",
            "But at the end of the day, I've just got one Victor for Pike, right?",
            "So how do I go?",
            "So I've got my one vector for Pike, but really the fish should be over here and the dive should be over there.",
            "So how does that not kill me, yeah?",
            "Yes, exactly so the vector representation really is this superposition of where you're sort of putting together the different vectors for the different sensors by summing them.",
            "So to a fairly large degree when you have a particular context of use.",
            "But that context of use will disambiguate to particular sense, and so you will find that the word does have good similarity with each of with the context corresponding to each different sense, and so that's why it's been a fairly workable system, but you know.",
            "Nevertheless, it still seems a little bit weird that we're not really representing different senses of the meaning of words, and so there's been at least a little bit of work that's tried to come up with vector representations for word sensors.",
            "I was involved with some work a few years ago which did this and sort of about the crude as possible way, but more recently a couple of other people have tried to deal with this task, so there's this paper by trust, which is a recent word embeddings system which does word sense learning inside the training of the the word vectors, which is a nicer approach to doing it, and there is someone else who had a paper at ACL 2015 during word vector learning.",
            "Which I didn't succeed in finding this morning, so it seems like there's a little bit of new interest in doing that.",
            "And this continues all the way out, because of course when you have sentence is I mean, sentences and phrases can also be ambiguous, and so we might be concerned if we're only making one vector as the representation of the whole thing, yeah?",
            "Salmon.",
            "Yeah.",
            "Right, yeah, so direct.",
            "So in some sense you've gotta compromise, right?",
            "'cause it has to be the case, but by piping ambiguous it would be less close to Sam and then if you just had the fish sense of Pike and you'll say how close to that of salmon.",
            "So it is a little bit of a compromise, but.",
            "I think the thing to human beings have exceeded exceedingly bad intuitions about how 100 dimensional spaces work.",
            "And it turns out that 100,000 dimensional spaces you can simultaneously be close to a lot of different things, and so therefore, miraculously, this superimpose vector can be close to all of these different things at the same time, in a way that doesn't work without 2 dimensional.",
            "Intuitions.",
            "OK."
        ],
        [
            "OK, another problem that hasn't been very much dealt with, but it's just an important one to be aware of it.",
            "Yeah, so all of this work is used.",
            "This sort of simplistic assumption of 1 string.",
            "One string, one word vector.",
            "Another way in which this is problematic is all languages have a lot of multi word expressions.",
            "Multiword expressions where you have a multiple word unit that has its own meaning.",
            "So we get things like place names like New York, we get noun compounds like neural network we get.",
            "So in English there are verb, particle constructions, makeup.",
            "There are various other kinds of idioms, like kick the bucket and take a hike.",
            "At all of these non compositional things, some of them have vague sort of metaphorical connections, but you know they're very much things that you have to know the meaning of.",
            "You know that you have to sort of know that a neural network is really this piece of matrix algebra that people program.",
            "It's not referring to something that's in people's heads most of the time.",
            "OK, so basically it seems like human beings also have to store the meanings of these things, and so one way of dealing with this is to actually learn at the same time vectors for these multi word units on the district in the vectors that Google distributes with word to vec, they've actually done this, that they've defined some fixed number of common multi word units, which certainly includes a lot of things like place names, because I think they're probably.",
            "Important to the search engine and so those were sort of pre clumped on some criteria and it's quite likely the criteria might be pointwise mutual information 'cause that's a common criterion for finding associated strongly associated words and then an extra vector was learn for those combinations.",
            "And of course that has some disadvantages are greatly boys out your vocabulary and also you're making this sort of hard decision as to.",
            "Which ones to clump and at least in some cases you can get ambiguities in this sort of same word sequence, and it could be the way so one example there is with these particle verbs.",
            "So.",
            "That that you can have them used sometimes in both the literal prepositional usage as well as in the kind of phrasal verb use where they have a special meaning, yeah.",
            "Well, they said your frequency with which they Co occur, but then you want to scale that by what the actual just bear frequency of the words are and then that immediately gives you pointwise mutual information.",
            "So I think there aren't any such in Word to VEC an actually quite a few quite a bit of the work that does phrase learning only does contiguous phrases, but in principle you can also consider counts at one remove or counter to remove and get counts of how often those occur as well.",
            "And you might some over different close by distances, and so you could start to try and workout those ones as well.",
            "Yeah.",
            "Yes.",
            "Apps.",
            "I've got a few tell it just given in the last slides maybe.",
            "For.",
            "You always do.",
            "So OK, I haven't said much about other languages.",
            "His teeny bit actually coming up, but you know, there's more to say about that.",
            "Yeah, so even in English the particle verbs can be separated.",
            "He made the answer up, but in other languages separated multi word expressions are much more common.",
            "That's true, so you sort of want a method that can deal with it that I agree with that.",
            "So another way in which you can learn word vectors and similar things is by using sequence models like LS, TM's that I haven't really.",
            "I think has anyone talked about, yeah?",
            "The behavior of what you get out of LSD.",
            "M card style word representations often tends to be very different 'cause they're very focused on predicting the next word rather than kind of being a general semantic representation of context.",
            "But maybe I'll leave that for now.",
            "That will come back again a bit later."
        ],
        [
            "OK, so just a few more minutes and so I can stop and have a teeny bit of time for questions.",
            "At the end I thought I'd sort of return at the end now to just having a little bit more linguistics and food for thought on representations and how people do things and might do things in deep learning."
        ],
        [
            "So what is this sort of conventional representations of linguistics?",
            "So at the low end, starts off with phonetics and phonology.",
            "So phonetics is the about the actual sound stream of language, and that's pretty uncontroversial weekly use.",
            "That and deep learning speech systems as well a thing would sort of interesting, is that for the sound systems of language, phenology posits that there's a small set of distinctive categorical units there.",
            "Phonemes or pep sometimes distinctive features of sounds, and there seems to be a fairly kind of universal set of what kind of phonemes and phonemic distinctions there are.",
            "You know, different languages have different subsets of these phonemes, but there's a pretty restricted space of sounds that human languages use that by and large they have bilabial purse person.",
            "They have tongues at the front with third person raising the back of the tongue with dealers likings.",
            "You know the same kind of set of things he used in basically all languages, but their language particular realization can vary quite a bit, and so there's been a lot of study how, for various phenomena, such as voice onset, time or positions of ALS, that they'll vary a lot in their phonetic realization across different languages.",
            "But one of the most studied and famous sets of evidence for categorical perception.",
            "Comes from looking at Phonologie where there's a varying realization of the signal across the continuous space that human beings in listening to these signals will categorical eyes it and say that although the sound is moving continuously that up until some point they regarded basically as categorically as, say, AP sound, and then when the voicing starts a little bit earlier then regard categorically.",
            "As a B sound and you get these very strong categorical perception affects which correspond to languages having these kind of categorical signalling systems.",
            "So these are Alpha."
        ],
        [
            "Names, I mean something that's kind of interesting, I think, is that if you look at what happens in speech, no one works with phonemes or the slightly looser loosely described phones, while people in speech.",
            "And this is regardless of whether they are doing deep learning or previous methods that they work with Triphones, which is a phone in the context of another phone on each side, or sometimes actually Quinn phones of two phones on each side.",
            "And what this is reflecting is that how phoneme sounds like how a sound sounds varies a lot in the context of argar, versus if you've putting it in front of a front vowel, like Iggy or something, right?",
            "That the G sounds realization phonetic Lee varies a lot, and so the conventional wisdom of speech systems, including nearly all deep learning speech systems is.",
            "You get these triphones orquin phones.",
            "And you which means there's a huge huge number of the starting point is you model them separately, but then normally what you do is cluster them because the space is too vast and decide that some of them can be treated together.",
            "So maybe G in between evals is near enough to G in between if else because they're both front valves that you can cluster them together.",
            "So the interesting thing there is that over in deep learning we meant to be interesting, depth of representation and abstracting the higher level units that basically deep learning speech systems don't actually achieve the depth of representation that's present in traditional linguistics and also just seems to be right for a description about human languages work."
        ],
        [
            "OK. Because the highest level of representation is that they've got this clustering of triphones and that they're trying to map on to units.",
            "In this clustering of triphones that they're not getting to a higher level representation, which is roughly equivalent to saying, OK, this is a Bush sound.",
            "But since at the end of the day, for the words that they are written in terms of these high level phonemes that you would hope to get that higher level of representation.",
            "Maybe I'll go run now and we can put out a minute writing systems so most.",
            "We work in deep learning, NLP, work starts with language in a written form as found data that we have lots of.",
            "I mean, so something that I think is at least worth remembering is that human language writing systems aren't one thing.",
            "So depending on what writing system you're using, you actually end up with a very different form.",
            "So some writing systems are true phonemic writing systems where you just got the sounds GI.",
            "Yahoo not a Boo.",
            "You have some of them kind of fairly regular fossilized phonemic systems like English.",
            "People know what this is.",
            "They look to trade one person at least knows the cake.",
            "Canadian language.",
            "They have another language, right?",
            "So so the new tubes are split, some of them using Romanization, but others using Institute syllabics, right?",
            "So each letter is then representing a syllable.",
            "In here graphic rising, essentially each letter is representing roughly a morpheme meaning unit.",
            "So.",
            "So if you just sort of doing the obvious thing in your deep learning system and working with letters that depending on what language you're working with, you're actually working with very different sized linguistic units.",
            "And that's something that hasn't actually been paid very much attention."
        ],
        [
            "So.",
            "Another way in which languages vary a lot is how they represent words or don't.",
            "So there are some languages, Chinese, Japanese that don't put any spaces between words, so you just get a continuous sequence of letters.",
            "Other languages separate into words that they don't, always separating the words the same way.",
            "So some so you tend to get in languages, and little clitics, which are these sort of reduced forms of words.",
            "That sort of final logically hang off another word.",
            "We have some of them in English as well for things like isn't an aisle.",
            "We get these clitic forms that are fun to logically dependent, and so some languages like Arabic join clitics onto the same word, so.",
            "This is the verb say, but you know it's got these information about subjects and objects and other Joyner words as clitics and they all just joined together, whereas other languages like French writing system, you get these clitic pronouns and XR res, which are written as separate words, whereas they might be written connected together.",
            "And things are also very with compound.",
            "So in English you put spaces between words of a compound in German.",
            "He just keep on going.",
            "Almost sorry.",
            "The joint the sorry, I kind of missed a space here.",
            "The world starts there.",
            "Joined as a partner but nevertheless.",
            "OK, but almost inevitably if you're writing systems for German, Dutch, similar languages, you can gain by using a compound splitter as pre processing."
        ],
        [
            "OK, so linguists analyze words as built up of sort of smaller units of meaning, morphology, so unfortunately you've got at least the three parts.",
            "You could also maybe separate, fortunate from fortune and eight most deep learning work hasn't actually dealt with the make up of words.",
            "There was one paper that we did a couple of years ago that did use hierarchical tree model of the kind of look at this afternoon.",
            "I'm to build up word meanings and this is potentially quite useful for dealing with unknown words because a lot of the unknown words are actually different morphologically.",
            "Drive forms of words you already know."
        ],
        [
            "An alternative really doing morphology is to use character engrams, and that's been quite a popular simple alternative.",
            "There was used by Rumelhart and McClelland decades ago and they call them wical phones.",
            "It's been revived by it in a sequence of recent pieces of work at Microsoft there DSM model works over.",
            "Character trigrams and one of the beauties of working with character trigrams is immediately shrinks your vocabulary to a fixed, manageable size, but normally 3 letters is kind of almost enough that you can get the semantics of things in code into your vectors.",
            "I'm."
        ],
        [
            "So then above that we have syntax sentence structure which can often be represented as either this kind of dependency syntax which shows relationships between words or content constituency syntax where you get a tree representation over sentence is and I'll show some more examples of that this afternoon when we look at that."
        ],
        [
            "And so then above that we have semantics and pragmatics.",
            "So semantics is the inherent meaning of linguistic unit and pragmatics's interpretation of meaning and context.",
            "And so from this perspective, it's kind of interesting to note the really our meat, so called meaning representations that we normally end up with in deep learning actually mixing a lot of pragmatics, as if we're building our meaning semantics by starting with a huge amount of found data and collecting in.",
            "Collecting up some kind of counts, we've got a lot of pragmatics mixed in there.",
            "Sort of real world context information of how things are used OK.",
            "So types of semantics, so I mentioned this briefly, but there are different ways of doing semantics, right?",
            "So we've looked a lot at distribution of semantics.",
            "They've been very useful.",
            "It's not the only, or necessarily always a sufficient way of doing semantics.",
            "What other ways of doing semantics are there?",
            "Their levels, like semantic role labeling and frame semantics, have been explored a lot in linguistics and NLP, where essentially sort of having something that's a bit more semantic than a syntactic representation.",
            "Were you wanting to normalize things?"
        ],
        [
            "Into LOL so you got something like this.",
            "Cynthia sold the bike to Bob for 200 and you're working out these different roles of elements of a sentence and so the roles would be the same even when the syntactic expression was changed or something like the bike was sold to Bob, you'd still say the bikers, the goods and Bob is the buyer, so it's kind of a form of more normalized representation.",
            "And then we've got computational formal semantics where you're mapping things into some kind of explicit.",
            "Model, denotation and words are being kind of mapped into their sets.",
            "And that's actually an important issue."
        ],
        [
            "But these kind of denotational semantic models haven't been much explored in deep learning, but I think they're going to have to be explored more.",
            "We already had some discussion of antonyms, but I think another place where denotational models will need to be brought in much more is that for a lot of the tasks we want to do, we want to treat, say, two people as different, right?",
            "Two people have just never the same for names of because they have different denotation.",
            "Is that guy.",
            "And there's that guy, and that's a fundamentally different notation notion.",
            "Two similarity in a distribution all sense.",
            "So if you think of if you build models of similarity in a distributional sense, well, you get results like Ronald Reagan and George Bush very similar to each other.",
            "But if you're doing any kind of task where you want to be looking at sentence, meaning and.",
            "Doing an information retrieval task or doing the kind of textual inference task, you just don't want to get from that.",
            "Ronald Reagan died.",
            "Therefore, you can conclude that George Bush died.",
            "That's that's not a valid step in sentence similarity, and so I think sort of exploring more denotational semantics will be necessary, and it's been neglected in deep learning models.",
            "And not sure now like me to stop."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK hi everybody, great to be here.",
                    "label": 0
                },
                {
                    "sent": "I'll let me start off by asking a couple of questions so you guys have an opportunity to stretch.",
                    "label": 0
                },
                {
                    "sent": "So how many of you have taken some language class?",
                    "label": 0
                },
                {
                    "sent": "Either a linguistics class or an NLP class?",
                    "label": 0
                },
                {
                    "sent": "OK, maybe 33rd 40%.",
                    "label": 0
                },
                {
                    "sent": "Quite a quite a bunch.",
                    "label": 0
                },
                {
                    "sent": "OK great.",
                    "label": 0
                },
                {
                    "sent": "So how many people kind of glad that the content of this summer school is turning in the direction of language.",
                    "label": 1
                },
                {
                    "sent": "But not the organizers, apparently.",
                    "label": 0
                },
                {
                    "sent": "OK, that's good.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's my plan.",
                    "label": 0
                },
                {
                    "sent": "So I'm sort of teaching both my sessions today, so for the one this morning, the organizers titled it in or P101.",
                    "label": 0
                },
                {
                    "sent": "I'm not quite sure what they hoped I do with that, but I I decided to almost take them seriously, and at least do some linguistics 101 and say some remarks about language, and so hopefully this is interesting to everybody, but in some sense I'm almost especially targeting with that.",
                    "label": 0
                },
                {
                    "sent": "The people who haven't done linguistics or NOP V4 of kind of giving you some ideas about how people think about language and what some of the phenomenon in language 'cause.",
                    "label": 0
                },
                {
                    "sent": "I think really there are lots of smart people here and maybe if you just know a little bit more about some of the cool things and language then you'll want to start doing natural language processing yourselves and then the main topic I'm going to talk about this morning is word embedding.",
                    "label": 0
                },
                {
                    "sent": "So in starting to use in starting to use neural network methods.",
                    "label": 0
                },
                {
                    "sent": "For natural language, almost inevitably the first thing that you have to deal with and know about is word embedding.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "OK, then in the afternoon I'm going to present an approach to dealing with language that's featuring quite a bit of the work done in my group on deep learning and NLP, which is independent tree, recursive neural networks.",
                    "label": 1
                },
                {
                    "sent": "And I'll explain what those are this afternoon and to some extent this material will then be able to be somewhat complementary to some of the material which will be seeing in the coming days.",
                    "label": 0
                },
                {
                    "sent": "Well, when full blown some turns up.",
                    "label": 0
                },
                {
                    "sent": "OK, and I should say that I'm totally happy to take questions at any point.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions or thoughts, do feel free to stick up your hand and interrupt.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just start off by saying I've sort of alternate some language linguistics content with some deep learning content.",
                    "label": 0
                },
                {
                    "sent": "So let's start off with a teeny bit about the nature of human.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Language, so why is language special versus other things such as vision?",
                    "label": 0
                },
                {
                    "sent": "Anyone have a good idea on that?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "More structure, maybe languages have more structure.",
                    "label": 0
                },
                {
                    "sent": "God by that yeah.",
                    "label": 0
                },
                {
                    "sent": "Sequential OK, so language has this very strong sequential structure and that leads into an interest in doing sequential models.",
                    "label": 0
                },
                {
                    "sent": "And that's not true vision.",
                    "label": 0
                },
                {
                    "sent": "That's certainly true, yes, Yasha.",
                    "label": 0
                },
                {
                    "sent": "It's generated by human Berthings spoke about the star student in the class.",
                    "label": 0
                },
                {
                    "sent": "And since I wanted.",
                    "label": 0
                },
                {
                    "sent": "OK. Until it actually, but I want to focus on first of which is, so even more generally about when people talk about data mining.",
                    "label": 0
                },
                {
                    "sent": "And sometimes then people move from data mining to text mining.",
                    "label": 0
                },
                {
                    "sent": "I think you know their similarity of those terms.",
                    "label": 0
                },
                {
                    "sent": "Kind of hides a fairly fundamental distinction, 'cause when your data mining, typically you're just collecting up a pile of data or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "You're Safeway shopping receipts.",
                    "label": 0
                },
                {
                    "sent": "And you're trying to do statistics and find some order in it, but most of the time when we're doing text mining, that just isn't what we doing.",
                    "label": 0
                },
                {
                    "sent": "'cause when we have text, it was created by a human being, and in almost all the circumstances that human being was deliberately trying to communicate information to some recipient or recipients.",
                    "label": 0
                },
                {
                    "sent": "And so a lot of the time what we're doing with natural language is that we're seeking to recover the meaning that a live human being tried to encode in the signal.",
                    "label": 0
                },
                {
                    "sent": "So if you look at deep learning, most of deep learning up until now has concentrated on the analysis of natural raw signals and things like object recognition and images, board calls detecting cancer where you've got these images and you're off signals and you're trying to find meaning in them.",
                    "label": 1
                },
                {
                    "sent": "So in this circumstance it's really up to the beholder to try and find some meaning based on the analysis of the signal.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Human languages aren't like that.",
                    "label": 0
                },
                {
                    "sent": "Instead of human languages specifically constructed to convey the speaker or writer's meaning to the other people, I guess when I gave that list of deep learning applications for so far in the last slide, I kind of suddenly didn't mention speech recognition, 'cause speech recognition as the nature of raw signal that is in the same language space of being carefully encoded by human beings to transmit meanings.",
                    "label": 1
                },
                {
                    "sent": "And so the really interesting thing is that human language is discrete symbolic categorical signalling systems.",
                    "label": 0
                },
                {
                    "sent": "So while the human being I mean, I'm sure you're sure, tried to impress on you in the first day that human brains have these distributed real valued activation levels, and that's the natural state of the world.",
                    "label": 0
                },
                {
                    "sent": "So why do human beings use a categorical symbol system to communicate with each other?",
                    "label": 0
                },
                {
                    "sent": "Any ideas?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean obviously this is just a hypothesis because no one was there when language was invented, but it seems reasonable to believe that this comes out of information theoretic concerns that categorical systems of communication very, very.",
                    "label": 0
                },
                {
                    "sent": "This is kind of easy stuff, right?",
                    "label": 0
                },
                {
                    "sent": "But they're very very effective for being able to transmit signals over noisy lines and be able to accurately recover what is being transmitted, and so that's presumably.",
                    "label": 1
                },
                {
                    "sent": "What human beings some way or another, decided and design their languages that way?",
                    "label": 0
                },
                {
                    "sent": "I mean their teeny teeny exceptions to that, so people do do a teeny little bit of expressive signaling.",
                    "label": 0
                },
                {
                    "sent": "So I'll say, I love it or whatever else I put down Hoopa and so obviously there's a little bit of meaning there that is being compared conveyed in some other way than the categorical signaling system.",
                    "label": 0
                },
                {
                    "sent": "But, you know, that's really a pretty small.",
                    "label": 1
                },
                {
                    "sent": "Exception and I think this is actually an important observation.",
                    "label": 0
                },
                {
                    "sent": "Not forget.",
                    "label": 0
                },
                {
                    "sent": "I mean in the modern deep learning is cool world.",
                    "label": 0
                },
                {
                    "sent": "I think people sometimes think that symbols were something they were invented by 1950s and 60s AI researchers and they were just making a huge mistake.",
                    "label": 0
                },
                {
                    "sent": "But actually symbols and symbolic systems were invented by human beings thousands of years ago.",
                    "label": 0
                },
                {
                    "sent": "And the basis of our language.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm here is another interesting thing about language, and it's something that we kind of don't really capture in our work on multitask learning and deep learning.",
                    "label": 0
                },
                {
                    "sent": "So far, so humans have this notion of a language and words and meanings conveyed in a language, and to use their language, their encoding, the language in some kind of signal for communication.",
                    "label": 1
                },
                {
                    "sent": "But there are at least three prominent substrates that can be used for communication.",
                    "label": 0
                },
                {
                    "sent": "People can use sound as I am now.",
                    "label": 0
                },
                {
                    "sent": "People can use gesture in sign languages, or people can use the images of writing to convey information.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are three really, really different encodings in a signal, but to a human receiver of the message, the human receives essentially exactly the same message, no matter which of those signaling substrates the person is chosen.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of interesting, so if we think about what human beings do, we have these distributed representations Now brain.",
                    "label": 0
                },
                {
                    "sent": "We then convert them into a symbolic encoding to communicate with other human beings and those, and that symbolic encoding is then transmitted via a continuous signal and then on the other end the decoder kind of does the reverse.",
                    "label": 0
                },
                {
                    "sent": "Write the decoder.",
                    "label": 0
                },
                {
                    "sent": "Converts the continuous signal back into a symbolic representation of words, which then somehow go into a distributed representation in their brain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're always going from symbolic and distributed at back, something that just struck me while I was putting together these slides is in practice for what we do in natural language processing with sort of doing the opposite of what brains are doing.",
                    "label": 0
                },
                {
                    "sent": "So brains are starting off distributed, converting to a simple like representation for transmission, which gets converted back into distributed representation.",
                    "label": 0
                },
                {
                    "sent": "Where almost inevitably, when we're building deep learning NLP systems because we start from the found data which is it's in.",
                    "label": 0
                },
                {
                    "sent": "It's simple like encoding.",
                    "label": 0
                },
                {
                    "sent": "We're almost always doing the opposite.",
                    "label": 0
                },
                {
                    "sent": "We're starting off with something in symbolic form, where then building a distributed representation of language that we use in our deep learning system, and then nearly always we then converting that back to a symbolic representation, which is whatever kind of categorisation or semantics or task.",
                    "label": 0
                },
                {
                    "sent": "That we're doing at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's human languages.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me go on and start to say a little bit about distributional word representations, and so this is one of the big changes that's come in with neural network approaches to language.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's not really new with them, so most of both traditional rule based NLP and statistical NLP of the 1990s and 2000s worked with words as atomic symbols, hotel, conference, walk.",
                    "label": 1
                },
                {
                    "sent": "So despite the fact that there are continuous continuous values for the probabilities and statistical NLP, those probabilities were still placed over symbolic representations, and so if we convert, think about that in vector terms.",
                    "label": 1
                },
                {
                    "sent": "Then what we have in CS1 hot encodings, where symbol is represented something that is a one in one position and a zero everywhere else, and what's been widely argued is that that's a cause of a lot of the problems with for natural language processing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we think of it perhaps as a web search example that if somebody searches for Dell notebook battery size, we'd like to match a document that talks about Dell laptop battery capacity.",
                    "label": 1
                },
                {
                    "sent": "Becausw laptop is like notebook and capacity is like size or usage.",
                    "label": 0
                },
                {
                    "sent": "Searches for Seattle motels.",
                    "label": 0
                },
                {
                    "sent": "Seattle Hotel should also be relevant and the problem is that if we have these one hot encodings, these vectors are orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So they have no natural notion of similarity, and so a lot of what we're going to be achieving by having distributed word representations is building a representation of words that does build into it, and natural notion of similarity a distance measure between words, and that's proved to be an incredibly useful and important idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we go about building a distributed representation for words?",
                    "label": 0
                },
                {
                    "sent": "Embedding OK, but how do we go about embedding?",
                    "label": 0
                },
                {
                    "sent": "Said that what they thought source of information we used to learn the representation of the word.",
                    "label": 0
                },
                {
                    "sent": "Context context yeah OK?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key idea is that we do distributionally learning, so we learn to represent a word by its context, and so this is referred to as distribution.",
                    "label": 0
                },
                {
                    "sent": "All models of meaning or also as you space models and meaning that your idea of the meaning of a word is the context in which it appears.",
                    "label": 0
                },
                {
                    "sent": "And this is actually, you know, an idea with longer historical roots.",
                    "label": 0
                },
                {
                    "sent": "So if you go to the.",
                    "label": 0
                },
                {
                    "sent": "Letter writings, but not the early writings of Ludvik Sunstein, he exposes a view of meaning in language, where meaning should be used, conditions that you've got this context of use, and most famously in linguistics, is normally attributed to Firth was a British phonologists who said you shall know a word by the company it keeps, and you know this is an idea that's much broader than just deep learning approaches to NLP, and it's really being one of the most successful ideas of modern empirical.",
                    "label": 1
                },
                {
                    "sent": "An RP.",
                    "label": 0
                },
                {
                    "sent": "So if we want to know what the word banking means, we collect up a bunch of uses of the word banking where you look in those those contexts of use, and then we're going to do something based on those.",
                    "label": 0
                },
                {
                    "sent": "Something that involves counting to come up with some kind of distributed representation for the word banking.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so that's the basic idea, so here is very simple toy model just to get the ideas.",
                    "label": 0
                },
                {
                    "sent": "OK so we choose some model that aims to predict the word based on other words in its context.",
                    "label": 1
                },
                {
                    "sent": "So we might say will use the word to the left and the word to the right.",
                    "label": 0
                },
                {
                    "sent": "Let's assume they have a distributed representation, will average them and then will want the dot product of that with the word in the middle to be large.",
                    "label": 0
                },
                {
                    "sent": "And so that gives us a loss function, which is how much that isn't true.",
                    "label": 0
                },
                {
                    "sent": "And then we can shove around our vector representations to make it more true, so that we have less loss.",
                    "label": 0
                },
                {
                    "sent": "And if we keep on looking at words in contexts and keeping on shoving around the word representations and little in the kind of online learning methods that have been talked about in preceding days, then we'll get distributed representations for words that capture.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their meaning, and so we'll end up with something that the word take is represented with vectors.",
                    "label": 0
                },
                {
                    "sent": "Something like this, and so this idea has been just super super successful, so that if you adopt this kind of approach and just start with a large corpus of text and shove around the elements of your word vectors to make words appear similar to their context, it just works absolutely beautifully for clustering words.",
                    "label": 0
                },
                {
                    "sent": "So here are forms of the word, haven't hear words of the form.",
                    "label": 0
                },
                {
                    "sent": "The and right next to be as remaining become, which are also copula verbs for those people who weren't that kind of stuff, and grandma, and here are other verbs over here and come and go all right next to each other as you think and say and think are right next to each other.",
                    "label": 0
                },
                {
                    "sent": "Should want right?",
                    "label": 0
                },
                {
                    "sent": "Like you, it's sort of.",
                    "label": 0
                },
                {
                    "sent": "I think it's just something that's gotten anybody who's done it.",
                    "label": 0
                },
                {
                    "sent": "Super excited that you just do this naive procedure of fiddling around word representations to minimize loss and sort of beholden miracle occurs.",
                    "label": 0
                },
                {
                    "sent": "That you feel that you've learned the meaning of all words by representing them in high dimensional vector space.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we do this, we have distributor distribution or representations which are two words, both of which start with almost exactly the same beginning.",
                    "label": 0
                },
                {
                    "sent": "So let me just get that completely clear.",
                    "label": 0
                },
                {
                    "sent": "So the notion of distributed is this idea that you have a concept represented by continuous levels of activation across a number of elements.",
                    "label": 1
                },
                {
                    "sent": "So a distributed encoding is to be contrasted with a local encoding like A1, hot vector and so.",
                    "label": 0
                },
                {
                    "sent": "That's distinct to this idea of distributional representation.",
                    "label": 1
                },
                {
                    "sent": "So distribution ull means that meaning is encoded by contexts of use, and that's in contrast to other models of semantics and word meaning.",
                    "label": 0
                },
                {
                    "sent": "The most famous and most used that I'll come back to later is denotational, meaning notions of meaning.",
                    "label": 0
                },
                {
                    "sent": "So denotational notion of meaning is that you're picking out the set of the set of objects in the world or in a model that satisfies, so the meaning of laptop isn't its context of use.",
                    "label": 0
                },
                {
                    "sent": "In text, the meaning of laptop is the set of that thing.",
                    "label": 0
                },
                {
                    "sent": "That thing, that thing, that thing and 100 others in this room that I won't point to.",
                    "label": 0
                },
                {
                    "sent": "And so, in particular, you can do build distributional models of meaning that aren't distributed, and in fact there have been some well known ones in NLP.",
                    "label": 0
                },
                {
                    "sent": "They're actually also very successful, so an alternative approach that's been used a ton and NLP is just to do hard clustering of words in a couple of well known techniques.",
                    "label": 0
                },
                {
                    "sent": "Brown clustering, exchange clustering, where you're using the same context of use and you're building these hard clusters of similar words and that also you know that.",
                    "label": 0
                },
                {
                    "sent": "That feels like it shouldn't be as good as having these.",
                    "label": 0
                },
                {
                    "sent": "Vector space embeddings, 'cause you know you're either in the same cluster or not, but in practice it's still being good enough that it's been a very useful technique in a lot of NLP.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've learned this wonderful model of word similarity with our vector space learning model, and that's really cool.",
                    "label": 0
                },
                {
                    "sent": "But something that excited a ton of people and led to the sort of current huge resurgence of interest in distributed representations, well?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tama Schmeeckle, often colleagues in 2013, demonstrated this further thing that you could do with word vector spaces, which is that you could actually look in directions in the vector space and find directions in the vector space that encode elements of meaning, and so the way he showed that is by doing.",
                    "label": 0
                },
                {
                    "sent": "These word analogy tests so that you sometimes see these on the kind of University admissions tests or something like that law school tests, I think, have, so you have these areas.",
                    "label": 0
                },
                {
                    "sent": "To be as C is 2D kind of questions, so it says man is to woman as King is to Queen.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that with these vector space models you can start off with the vectors of King, Man and woman.",
                    "label": 0
                },
                {
                    "sent": "Here they are in the vector space and then you can take a difference vector for what is the difference between man and woman.",
                    "label": 0
                },
                {
                    "sent": "And then you can move that difference vector up so it sits off King.",
                    "label": 0
                },
                {
                    "sent": "And then say that the way I want to answer this analogy is by finding the word that's closest to that point up there where I've shifted this vector up to here and lo and behold, with quite reasonable accuracy.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the world you find right there in the space is Queen, and so that's.",
                    "label": 0
                },
                {
                    "sent": "I'm quite incredible and so formally you're doing this so you just finding the word that is having the highest dot product with the.",
                    "label": 0
                },
                {
                    "sent": "Adding this on to there.",
                    "label": 0
                },
                {
                    "sent": "OK, and so Mikro phone call.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lakes showed this in 2003 and in their results that they showed that this method using neural encodings worked considerably better than previous methods that people had used to address some of the same problems.",
                    "label": 0
                },
                {
                    "sent": "So they had two kinds of analogies, syntactic and semantic ones, and from the syntactic ones.",
                    "label": 0
                },
                {
                    "sent": "They compared against the traditional method.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to latent semantic analysis and behold.",
                    "label": 0
                },
                {
                    "sent": "The neural network was over as far as getting things correct, and then for the semantic analogies, they compared on a shared task where previous system at that one the shared task got 23.",
                    "label": 0
                },
                {
                    "sent": "The Spearman's rho coefficient and they were able to improve on that and get a new state of the art, so this made it appear that neural word embeddings were just much better than any traditional methods of coming up with distributing codings of words.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Wait is this?",
                    "label": 0
                },
                {
                    "sent": "Although this this is the dimensionality of the vectors.",
                    "label": 0
                },
                {
                    "sent": "Yes, very large.",
                    "label": 0
                },
                {
                    "sent": "OK, and in general having these distributional representations of words, it's just been a really useful tool to improve our NLP.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distance so if you look at symbolic NLP systems, even probabilistic symbolic in RP systems, they're very, very fragile because of their symbolic representations.",
                    "label": 1
                },
                {
                    "sent": "So here's an example from the Stanford parser, so I'm not picking on anybody else.",
                    "label": 0
                },
                {
                    "sent": "Here is a very simple sentence.",
                    "label": 0
                },
                {
                    "sent": "My dog also eats oranges, and it passes it correctly.",
                    "label": 0
                },
                {
                    "sent": "That's what you'd hope.",
                    "label": 0
                },
                {
                    "sent": "But if you actually change out oranges and put in bananas.",
                    "label": 0
                },
                {
                    "sent": "Lo and behold, it pauses the sentence wrong way.",
                    "label": 0
                },
                {
                    "sent": "So rather than regarding bananas as a noun phrase like oranges, that comes up with this funny parse where there's an adjective phrase here and that's analyzed as a sentence, and eat is taking this complement of this sentence, we had stuff going on.",
                    "label": 0
                },
                {
                    "sent": "Now, you know, if one looks in the training data for the parser, it's perfectly obvious why this happens.",
                    "label": 0
                },
                {
                    "sent": "It turns out the word bananas only appears once.",
                    "label": 0
                },
                {
                    "sent": "In the training data that's used for these parsers, and that one usage of bananas is in the sense of go bananas.",
                    "label": 0
                },
                {
                    "sent": "So here we have eats bananas in the pan is giving it the same kind of analysis.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so explainable, but that's obviously not the kind of behavior that we would like.",
                    "label": 0
                },
                {
                    "sent": "And if we actually had the model, know that a word like bananas is very similar to oranges and apples.",
                    "label": 0
                },
                {
                    "sent": "Then it should be able to share information across usages of those various words and be able to make less mistakes like this one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in general, putting in these kind of distributional representations has just given NLP systems, and Norma's performance gains.",
                    "label": 1
                },
                {
                    "sent": "So and things like parsing you can get large error reductions by using distributional word representations.",
                    "label": 1
                },
                {
                    "sent": "Named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "Large error reduction is an often cut out about 1/4 of your errors.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so yeah.",
                    "label": 0
                },
                {
                    "sent": "But is it really really?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe I should come back to that.",
                    "label": 0
                },
                {
                    "sent": "'cause I do actually have a couple of slides towards the end that start to say something about word sensors, but I mean it's certainly true and worth thinking about that.",
                    "label": 0
                },
                {
                    "sent": "We've got this naive model at the moment where we've got a string and we just find every context it appears in and build some representation based on that and that could be bad for all sorts of reasons of word.",
                    "label": 0
                },
                {
                    "sent": "Might have multiple meanings or the usage of a word.",
                    "label": 0
                },
                {
                    "sent": "In the news why that you trained it on might be completely different to the usage of the word in medical texts or something used at runtime, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes that that.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that is another big well known problem with these word representations is almost always if you build them in the obvious ways that you have words in their antonyms.",
                    "label": 0
                },
                {
                    "sent": "Being almost identical, whereas you feel that they should be very different.",
                    "label": 0
                },
                {
                    "sent": "She wants to say something, but I think maybe actually I should say I'll let him say his piece, but I think really some of these questions are getting a bit ahead of things 'cause I haven't really gone through the details of building a word embedding before we talk about their problems.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a couple more.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so in almost all deep learning work in NLP, the first stage is mapping words symbols to the distributed vector representation.",
                    "label": 1
                },
                {
                    "sent": "In fact, doing this is so useful or so easy you're both, but many people never get any further, so in the last years, NLP conferences.",
                    "label": 0
                },
                {
                    "sent": "They were completely full of papers using word embeddings and a lot of them didn't actually use anything more than word embedding, so I do think it is worth actually reminding people that just using word embeddings isn't deep learning.",
                    "label": 0
                },
                {
                    "sent": "Yes, you've got distributed representations that there's actually no deep learning right.",
                    "label": 0
                },
                {
                    "sent": "Deep learning is a distinct idea where actually claiming, but by having depth of representation, that means that we can do more.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, So what I want to do now is to go a bit more in detail and what people have done with neural embeddings.",
                    "label": 0
                },
                {
                    "sent": "Get word vectors and something that turns out to be the case from this is although people initially got very excited about newer word embeddings and their performance that it turns out that some of the communist most used forms of neural embeddings actually turns out they're not really different from the kind of distribution or learning.",
                    "label": 0
                },
                {
                    "sent": "Representations that people had been using elsewhere in IR and NLP in the 90s and 2000s, and that they actually relate very closely to them and I want to say a bit about that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the most famous example at present of newer word embedding is word two VEC, which was a piece of software released at Google by Tomasz Michaelov.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, with word to VEC, you get a big supply of text.",
                    "label": 1
                },
                {
                    "sent": "Wikipedia is generally recommended.",
                    "label": 0
                },
                {
                    "sent": "You stick it into the word to VEC program.",
                    "label": 0
                },
                {
                    "sent": "You wait a couple of hours.",
                    "label": 1
                },
                {
                    "sent": "It's actually pretty fast as these things go.",
                    "label": 0
                },
                {
                    "sent": "An outcomes matrix where you have one vector as the representation of each word.",
                    "label": 0
                },
                {
                    "sent": "To a certain number that's found in your source text.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the idea of this is that in Word to vector in general, in the new embedding models that you don't directly capture cooccurrence accounts like in some traditional methods like latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "But instead you set things up as a prediction task.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to do prediction between words and their contexts and worked avec isn't one thing.",
                    "label": 0
                },
                {
                    "sent": "So word to vec, there are two primary algorithms that included in it.",
                    "label": 0
                },
                {
                    "sent": "One of which kind of going opposite directions, so the skip grams algorithm is trying to predict words in the context given the target word in the middle, whereas the continuous bag of words model is doing the opposite is trying to predict the target word given a bag of words context.",
                    "label": 1
                },
                {
                    "sent": "And crossed with that is the actual training methods they used in Word, two VEC and there are two different practical training methods implemented in Word.",
                    "label": 1
                },
                {
                    "sent": "Two VEC ones, hierarchical softmax, first pioneered by Joshua and the other one is negative sampling.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh yeah, OK, so.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do here is sort of say a bit about the Skip gram model.",
                    "label": 0
                },
                {
                    "sent": "So our idea is we've got the target word in the middle, and what we're going to want to do is predict the words in the context of the target word and our loss function is going to be given by having a probability of predicting these.",
                    "label": 0
                },
                {
                    "sent": "Each context word and we want to maximize the probability of each context word and each context word is predicted separately, so this probability immediately pulls apart by treating each prediction as independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what the model looks for, like for the Skip gram model, and it's really a very simple model, so we start with the word identity for the target word.",
                    "label": 0
                },
                {
                    "sent": "So this is just A1 hot vector.",
                    "label": 1
                },
                {
                    "sent": "And so we're then going to multiply it by a matrix to get a hidden representation.",
                    "label": 0
                },
                {
                    "sent": "But since this is A1 hot vector, that just means we're going to select one column.",
                    "label": 0
                },
                {
                    "sent": "Right column row.",
                    "label": 0
                },
                {
                    "sent": "Column one column of this vector of this matrix and that will be a representation of the target word.",
                    "label": 0
                },
                {
                    "sent": "OK, so then from that hidden representation we're then going to multiply by one other matrix, and this matrix is then going to give a probability distribution over the different words being being context words, and so effectively.",
                    "label": 1
                },
                {
                    "sent": "Our word embeddings are contained in these two matrices, and so we actually have two different embeddings.",
                    "label": 0
                },
                {
                    "sent": "For each word, we have one embedding for it as a target word, and one embedding of it as a context word.",
                    "label": 0
                },
                {
                    "sent": "OK, so the naive algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Goes like this so our loss function as we want to maximize the probability of this assignments of the words and context.",
                    "label": 0
                },
                {
                    "sent": "We immediately assume independence between each word.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so we then go into work for the words in terms of their word embeddings.",
                    "label": 0
                },
                {
                    "sent": "So now we turn from the word ID's to their vector representations and the vector representation of the import is just the hidden layer and then we've going to learn a vector representation of the output and we're turning.",
                    "label": 0
                },
                {
                    "sent": "We're making our probabilistic model using this kind of softmax model, so our model of probability of an output word given an input.",
                    "label": 0
                },
                {
                    "sent": "What output were given input word is the probability of an output vector given input vector and that is done as a softmax model over the DOT product between the two vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Wait, so far we haven't used either of those.",
                    "label": 0
                },
                {
                    "sent": "I do not have any either of those, right?",
                    "label": 0
                },
                {
                    "sent": "Can we wait a moment?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what your question is, but I haven't actually done either of those, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so yeah, you have this super high dimensional space of all the output of all the words in your vocabulary, and you're giving a probability estimate to each one.",
                    "label": 0
                },
                {
                    "sent": "So practically, people start off by choosing some vocabulary, but you normally want the vocabulary to be pretty loud, so your vocabulary might be 100,000 words, 200,000 words.",
                    "label": 0
                },
                {
                    "sent": "And for those words, Yep, you've got them in your vocabulary and you're giving them each probability estimates and all remaining words you map to one or more unknown tokens.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you just have one unknown, sometimes you have a couple more 'cause you might decide that things like numbers or an actual class that you.",
                    "label": 0
                },
                {
                    "sent": "Map to an extra 9 token.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah.",
                    "label": 0
                },
                {
                    "sent": "No, it's a crummy assumption.",
                    "label": 0
                },
                {
                    "sent": "'cause obviously, which for almost any word the chances of word being to build another world being to the left or the right of it will be extremely different.",
                    "label": 0
                },
                {
                    "sent": "And the chances of being one or more apart will be different as well.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's actually a quite poor assumption and there are some other work methods of learning your embedding, such as the.",
                    "label": 0
                },
                {
                    "sent": "Model proposed by color Band Western that don't have that property.",
                    "label": 0
                },
                {
                    "sent": "Their positional and I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "But despite its crummy nurse bag of words assumption, I ignoring order like this is just something that's very commonly used in NLP because it makes things a lot simpler because you make independence assumptions and commonly for a lot of semantic key tasks of meaning.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually behave that badly.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so as presented, yeah, so there's this slow distribution of very common words like that of two R and then going down into very rare words and actually forgetting models like this to work really well.",
                    "label": 0
                },
                {
                    "sent": "A crucial crucial thing is how you do the scaling.",
                    "label": 0
                },
                {
                    "sent": "Of the frequency of words which is responding to that this law distribution.",
                    "label": 0
                },
                {
                    "sent": "But again, that's not what I've been presented so far.",
                    "label": 0
                },
                {
                    "sent": "At the moment I've still got a naive one which is just using probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose I'm still in this naive world.",
                    "label": 0
                },
                {
                    "sent": "Or suppose you're still in this naive world for one more minute, and this is the model and you want to get it running on your laptop to learn vector representations of the input words.",
                    "label": 0
                },
                {
                    "sent": "And the output words.",
                    "label": 0
                },
                {
                    "sent": "How would you go about doing it?",
                    "label": 0
                },
                {
                    "sent": "Fire up theano.",
                    "label": 0
                },
                {
                    "sent": "So, So what are the algorithmic steps that you go to if you wanted to train word representations using this kind of model, how would you start?",
                    "label": 0
                },
                {
                    "sent": "What would you do?",
                    "label": 0
                },
                {
                    "sent": "OK, so you wanted to use gradient descent and so in order to be able to use gradient descent you would do what?",
                    "label": 0
                },
                {
                    "sent": "Height.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to find the derivatives of this so it take this softmax form and find the derivative with respect to what?",
                    "label": 0
                },
                {
                    "sent": "The parameters so the parameters are simply our word representations, right?",
                    "label": 0
                },
                {
                    "sent": "So those are the only parameters of this model, and as I know everyone feels like they could take the derivative of this with respect to.",
                    "label": 0
                },
                {
                    "sent": "Different parameters.",
                    "label": 0
                },
                {
                    "sent": "Honest question yes no.",
                    "label": 0
                },
                {
                    "sent": "Some people think if there if they were forced to do some work, that they could take the derivative of this with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "The elements of the Vector H Joshua curd.",
                    "label": 0
                },
                {
                    "sent": "OK, we can stay.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "I guess I don't know what's useful, I mean.",
                    "label": 0
                },
                {
                    "sent": "I could kind of stop and actually quite we could do that concretely, and you could see me screw up all the indices of everything on the blackboard.",
                    "label": 0
                },
                {
                    "sent": "But would that be useful to people think that they could work it out or start to figure out how to do this?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, I hope so.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah, So what you do is you just differentiate with respect to each of these parameters and so it sort of turns into a kind of a pretty example.",
                    "label": 0
                },
                {
                    "sent": "I can radiate it, add slides that show it afterwards, right?",
                    "label": 0
                },
                {
                    "sent": "So that you can sort of split this apart.",
                    "label": 0
                },
                {
                    "sent": "You know you take the product outside the log and turn into a sum.",
                    "label": 0
                },
                {
                    "sent": "He then separate the numerator and the denominator and you'll play the chain rule backpropagated all figures out.",
                    "label": 0
                },
                {
                    "sent": "In the end.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK well so.",
                    "label": 0
                },
                {
                    "sent": "Apparently.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you if you did that, and supposing you got all The Drifters, rident passed gradient checks and all that, and you started running on your laptop, you're likely to be pretty sad.",
                    "label": 0
                },
                {
                    "sent": "Why are you likely to be pretty sad, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, because of this normalization term.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Normalizing over the entire vocabulary and if we have a vocabulary of 200,000 words, that's a huge space here, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean back in the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model this part is trivial because it's just selected column out of a matrix that here to workout these output probabilities we're having to calculate and normalize over the entire vocabulary at each step.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that just doesn't work, and so in the practical version, as in Word to VEC, two methods are then used which people already mentioned.",
                    "label": 0
                },
                {
                    "sent": "So one of which is the hierarchical softmax, which I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about and the other one is Skip Gram well as the skip gram with negative sampling and so the idea of this is that we're actually going to change things slightly so effectively using a little logistic regression model and the little logistic regression model is wanting to workout with the word context pair is good one you actually found in your training data or one that is probably bad.",
                    "label": 1
                },
                {
                    "sent": "One that wasn't found in your training data, and I say probably bad because you generate the negative examples by just randomly pairing a word in this context so you know sometimes those will be pairs that do occur in the training data, but most of the time they won't be, and so something that's crucial to note.",
                    "label": 0
                },
                {
                    "sent": "Then, right?",
                    "label": 0
                },
                {
                    "sent": "That if we're getting positives from the training data, how often we see them scales with the problem, the joint probability of a word appearing in the.",
                    "label": 1
                },
                {
                    "sent": "With a certain other word in the context, whereas if we are randomly selecting our negative words, how often will get to them is decided by the product of the probability of the target word.",
                    "label": 0
                },
                {
                    "sent": "Because we're going through the target words, selecting them times just the independent probability of the context work.",
                    "label": 0
                },
                {
                    "sent": "'cause that's how often will select it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we do with the Skip gram negative sampling is what we're wanting to say.",
                    "label": 1
                },
                {
                    "sent": "We're wanting to set the parameters now to maximize the probability with.",
                    "label": 0
                },
                {
                    "sent": "Switzer model says that something that's from a true context is good.",
                    "label": 0
                },
                {
                    "sent": "So D is our true contexts and that's one.",
                    "label": 0
                },
                {
                    "sent": "And then we want for things that are in.",
                    "label": 0
                },
                {
                    "sent": "Invite Contacts with the tilled.",
                    "label": 0
                },
                {
                    "sent": "We want to say that they are not good but D = 0 and so that then model for this probability is then this logistic regression form and so we're working things out with that and so doing this we're now in a state that's way way more computationally feasible.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular, what we have is this sort of binary logistic regression form.",
                    "label": 0
                },
                {
                    "sent": "So for each word in context we workout its probability in the binary logistic regression form.",
                    "label": 0
                },
                {
                    "sent": "We pick some number of negative examples where we randomly sample words and we then also workout a logistic regression form for them and we want them to have a low probability.",
                    "label": 0
                },
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "If we are doing nothing more than that, the interesting thing is that when we have our two word representations, the target words and the context words that we can actually think of this as doing a matrix factorization where we're factorizing matrix, which is the pointwise mutual information between words.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And where that come?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's from is right back here.",
                    "label": 0
                },
                {
                    "sent": "Good examples are using the joint probability of the words and our fake examples are using these independent probabilities of the words.",
                    "label": 0
                },
                {
                    "sent": "So when we then taking the ratio of those in natural space, which is then the difference in.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In log space and this is our difference in log space that that's giving us a pointwise mutual information.",
                    "label": 0
                },
                {
                    "sent": "So the pointwise mutual information is like the mutual information, but you're just taking the one term of it, so it's the log of the probability of XY over the probability of X times the probability of Y and pointwise mutual information.",
                    "label": 0
                },
                {
                    "sent": "Is this very famous long used quantity in NLP for having a kind of an Association measure?",
                    "label": 0
                },
                {
                    "sent": "Between a pair of words and so this result was shown by leaving Goldberg at NIPS 2014 and in some sense that kind of changed the perspective on the specialness of neural word embeddings.",
                    "label": 1
                },
                {
                    "sent": "Because this made it made it clear that really what word affect learns is actually very much in the space of what more traditional methods such as latent semantic analysis learn.",
                    "label": 0
                },
                {
                    "sent": "That they're doing this starting with account there, starting with account matrix.",
                    "label": 0
                },
                {
                    "sent": "Here it's a target word context word Co occurrence, count matrix, and what they're doing is a matrix factorization, and that is how they're getting their representations for words.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, of course.",
                    "label": 0
                },
                {
                    "sent": "Word two vec isn't the only method of doing your own bedding, so most of all of the current work in this field started out with the author of NGOs work in 2003 of the Neural Probabilistic Language model, which was this basic idea that you could have a better language model by using distributed representations and then those other work on doing your language models.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk anymore about that now though.",
                    "label": 0
                },
                {
                    "sent": "So I was actually going to say more about neural language models later on in the century.",
                    "label": 0
                },
                {
                    "sent": "From there there was a couple of steps and the steps we're essentially to make things simpler and faster.",
                    "label": 0
                },
                {
                    "sent": "So in Colburn Westerns work, they said well if we just want word representation.",
                    "label": 0
                },
                {
                    "sent": "So we have useful similarity measure on words for tasks like named entity recognition, part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "We don't need to worry about this stuff and making sure it's a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "We can just learn word representations and so they did that, but these models were still really, really slow to train.",
                    "label": 0
                },
                {
                    "sent": "I think color in Weston spent seven weeks training.",
                    "label": 0
                },
                {
                    "sent": "There's or something like that, and so in the more recent work such as me crossword avec.",
                    "label": 0
                },
                {
                    "sent": "The suggestion was that maybe we could just use a much simpler model for sort of by linear model that I've just presented, and that would be enough to produce a good word representation that work great for tasks.",
                    "label": 0
                },
                {
                    "sent": "And in fact, what was suggested is that you could kind of trade complexity for representation size.",
                    "label": 0
                },
                {
                    "sent": "So what people were found with models like word to VEC is you can get extremely extremely good performance by using large vector sizes, whereas conventionally people use sizes like 5100, two, 100 that people now commonly using sizers of 308 hundred 1600 and getting better performance, and they're able to estimate those models because these methods are very.",
                    "label": 0
                },
                {
                    "sent": "Simple.",
                    "label": 0
                },
                {
                    "sent": "OK so but I mean the the result just pointed out about the sort of matrix factorization holds a word to VEC.",
                    "label": 0
                },
                {
                    "sent": "It doesn't hold straightforwardly of these other models.",
                    "label": 0
                },
                {
                    "sent": "I said the wifey.",
                    "label": 0
                },
                {
                    "sent": "OK, but the word defect code is fast produces good results in Google, so it's become very pop.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pure and everyone uses it as you can see in this Google Trends graph.",
                    "label": 0
                },
                {
                    "sent": "It's just really taking off.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I think I might go quickly on this next bit.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think in some sense, the leafeon Goldberg model nailed some of the connections between word to VEC and conventional models, but it was actually an area in which me in a postdoc, Jeffrey Pennington also worked at the same time and we had kind of similar thoughts of saying.",
                    "label": 0
                },
                {
                    "sent": "Surely there's a middle ground between neural embedding, learning methods and traditional methods of working by manipulating counts.",
                    "label": 0
                },
                {
                    "sent": "And so we came up with our own model that was called Glove, so that this central contrast.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that there are these predict models like we've talked about?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there are traditional models that will count models where you start off with account matrix and you do some kind of factorization and the traditional way of doing that was the SVD which then gets called latent semantic analysis, or latent semantic indexing.",
                    "label": 0
                },
                {
                    "sent": "And for people initially thought, including as late as in this 2014 paper by Barone was that for some reason the predict based models just worked a lot better than the count based models.",
                    "label": 0
                },
                {
                    "sent": "But actually that just turns out not to be true because of really you have this theorem about how word to vector skip Gram can be thought of as a factorization of account based model.",
                    "label": 0
                },
                {
                    "sent": "And in fact, somewhat Interestingly, the fact that you get these linear relationships in space had actually been observed well before michaelov so bad.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In 2005, Doug Roadie, who was Psycho Linguistics student and his interest in psycholinguistic modeling.",
                    "label": 0
                },
                {
                    "sent": "He built LSA based models of distributed word representations and what the crucial thing he did was the count.",
                    "label": 0
                },
                {
                    "sent": "Modified LSA and Doug Grady show that in his Kohl's model, which was discount modified LSA, you got exactly the same linear structure.",
                    "label": 0
                },
                {
                    "sent": "Then Make Love showed in his analogies.",
                    "label": 0
                },
                {
                    "sent": "So here we've got drive driver swim swimmer, teach, teach.",
                    "label": 0
                },
                {
                    "sent": "Mary Pre stripe that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that you're getting the same kind of linear relationships, and so it turns out that actually a lot of the important part of getting out these nice linear relationships is that count modified bit.",
                    "label": 0
                },
                {
                    "sent": "And when people started looking more closely and make Love's source code what people observed was that there's actually some very, very sort of.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure clever is the right word, but very careful, just right scaling of counts and making use of.",
                    "label": 0
                },
                {
                    "sent": "Contacts with different probabilities rather than naively always sampling at the same and a lot of the success of his methods comes with these fine points of the estimation that go beyond the naive equations that were written up.",
                    "label": 0
                },
                {
                    "sent": "And so that was an idea that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were also interested in with how we could connect together.",
                    "label": 0
                },
                {
                    "sent": "These count based models and direct prediction and the advantages of both.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Earth, and so we developed a model to do that.",
                    "label": 0
                },
                {
                    "sent": "Which was the glove model and I'll just say a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think the good intuition here is that what you want to have to represent meaning components is ratios of Co occurrence probabilities.",
                    "label": 0
                },
                {
                    "sent": "So if you have a word words occurring in the context of ice, you'd expect solid too often occur with ice and gas to rarely occur with ice.",
                    "label": 0
                },
                {
                    "sent": "So that looks like you're capturing a meaning component of ice, but simply knowing that those are large and small isn't enough cause the word water often occurs with ice and a random word like random is unlikely to occur with ice, and so those are large and small as well, and you get the same properties reversed the steam with solar and gas.",
                    "label": 0
                },
                {
                    "sent": "So if you really want to get out the meaning component that can, that is the dimension where you go from solid liquid gas.",
                    "label": 0
                },
                {
                    "sent": "What you want to do is take the ratio of these Co occurrence probabilities because then you get a large to small dimension here where these ones will cancel out about one and well, that's just my made up large and so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, but it turns out if you count up on Wikipedia, that does actually basically work for you, right?",
                    "label": 0
                },
                {
                    "sent": "These ones come out about one, and these come out is about 10 and a tenth, so that works nicely, and so that's the prob.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do you want to capture to get linear relationships in a vector space?",
                    "label": 1
                },
                {
                    "sent": "And so how can you go about doing that?",
                    "label": 0
                },
                {
                    "sent": "Well, it seems like the simplest way you could go about doing it is precisely this log bilinear model where if the vector representations of words are such that their dot product corresponds to the log of probability of Co occurrence, then immediately when you can put in differences between vectors.",
                    "label": 0
                },
                {
                    "sent": "You're going to get the log of the ratio of chances of Co occurring together.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we did for the glove model is explicitly optimized parameters to get that criterion.",
                    "label": 0
                },
                {
                    "sent": "So here it is with a squared loss criterion.",
                    "label": 0
                },
                {
                    "sent": "We're saying we want the dot product of two word vectors to be as close as possible to the Co occurrence frequency, which of course is sort of just relative to the Co occurrence probability.",
                    "label": 0
                },
                {
                    "sent": "And so that works great again if and only if you're careful to manipulate counts.",
                    "label": 0
                },
                {
                    "sent": "And this comes back to the Zipf's Law part, right?",
                    "label": 0
                },
                {
                    "sent": "So if you do nothing for ether word to VEC or this glove model but sort of used the raw probabilities, it works terribly, and so you want to use some method that scales the counts to avoid being overwhelmed by the Super common words, and perhaps to boost the rare words.",
                    "label": 0
                },
                {
                    "sent": "And both were developed in the glove model.",
                    "label": 0
                },
                {
                    "sent": "Do is and the model is used in the glove model.",
                    "label": 0
                },
                {
                    "sent": "Is this very simple scaling?",
                    "label": 0
                },
                {
                    "sent": "So for word frequencies up to a certain point, you're using this slightly curved three quarter power, which seems in practice to work well, though it's just sort of a hack constant and then it just saturates sofa.",
                    "label": 0
                },
                {
                    "sent": "Very common words.",
                    "label": 0
                },
                {
                    "sent": "They're not sampled anymore frequently.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Fallen short may occur with.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So so if you sort of just build these models from a lot of text and distributional context, I mean so the answer is you don't.",
                    "label": 0
                },
                {
                    "sent": "They end up very similar in meeting, so that's obviously not what you want to do.",
                    "label": 0
                },
                {
                    "sent": "So people have explored methods to fix this as one paper from Microsoft afew years ago.",
                    "label": 0
                },
                {
                    "sent": "Where they explicitly kind of hardwired the model to may have antonyms be far apart from each other, and that's sort of work, but it seemed very ad hoc and specific to antonyms.",
                    "label": 0
                },
                {
                    "sent": "The may.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the main way that seems at least better than that is to embed the word vector learning in some tasks.",
                    "label": 0
                },
                {
                    "sent": "That makes it necessary to keep antonyms apart, and so one clear example of that is if you do sentiment analysis that as soon as you do sentiment analysis, words that are antonyms, like good, bad, heavy light, get pulled apart.",
                    "label": 0
                },
                {
                    "sent": "'cause that's a primary dimension of automation and you're sure suggests another example.",
                    "label": 0
                },
                {
                    "sent": "Which was translation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, skipping a bit showing you more glove stuff, but I mean here, here is just an interesting thing to know about if you're doing this for yourself.",
                    "label": 0
                },
                {
                    "sent": "So in learning these word vectors.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you know Wikipedia just works way better than anything else that you can get text data for, and the way that makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "It's the nature of an encyclopedia that it's just having a lot of meaning of words and discussing their contexts of use and things like that.",
                    "label": 0
                },
                {
                    "sent": "So if you want to know about the meanings of words and how they relate to each other as meanings, Wikipedia is distinctively useful.",
                    "label": 0
                },
                {
                    "sent": "So here's a 2010 Wikipedia dump with a billion tokens.",
                    "label": 0
                },
                {
                    "sent": "And these are results from our glove model, but I think broadly similar things should see with word vector whatever, and so it does brilliantly well on semantic analogy tasks.",
                    "label": 0
                },
                {
                    "sent": "It's getting about 80% using these big several 100 dimensional vectors is not very much text to train on, so it's doing relatively modestly on syntactic analogy, task.",
                    "label": 0
                },
                {
                    "sent": "And then this is the overall average.",
                    "label": 0
                },
                {
                    "sent": "If you wait for years, Wikipedia is grown, and that's had a slight positive effect.",
                    "label": 0
                },
                {
                    "sent": "But the contrast here is that if you instead train on giga word, which is a newswire corpus, that's very commonly used in NLP.",
                    "label": 0
                },
                {
                    "sent": "It's about three times the size, and because it's about three times the size you do a bit better on syntactic analogy task.",
                    "label": 0
                },
                {
                    "sent": "As you learn more about word contexts, but you're doing way way worse on the semantic analogy task, as reading newspapers just doesn't give you the same kind of meaning of words information.",
                    "label": 0
                },
                {
                    "sent": "What you get from reading Wikipedia?",
                    "label": 0
                },
                {
                    "sent": "People don't seem surprised by that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Done, done.",
                    "label": 0
                },
                {
                    "sent": "I'm something is kind of cool, but I'm not going to talk about today is, I think it's cool that some of the CS theory people are starting to get more interested in machine learning and deep learning because it's become such a hot area.",
                    "label": 0
                },
                {
                    "sent": "And in particular Sanjeev Arora's been devoting quite a bit of time to trying to understand how you get these kind of linear properties in world spaces and is developing a generative model to do so.",
                    "label": 0
                },
                {
                    "sent": "Do you wanna ask a question?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's not so many.",
                    "label": 0
                },
                {
                    "sent": "English language.",
                    "label": 0
                },
                {
                    "sent": "So I didn't sort of really unexplained explain what this syntax is.",
                    "label": 0
                },
                {
                    "sent": "I mean syntax and in mcleods analogies are things like being able to go from thin, thinner to easy easier, so it's so not really syntax.",
                    "label": 0
                },
                {
                    "sent": "It's sort of more sort of morphology or things like that, and I think you know for facts of that sort, it still turns out that just having seen more text.",
                    "label": 0
                },
                {
                    "sent": "That you just have better representations of rare words and knowing more of their plurals and things like that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what else do we need to know?",
                    "label": 0
                },
                {
                    "sent": "Most words have multiple meanings.",
                    "label": 1
                },
                {
                    "sent": "The first word that always comes to people, it came up before was bank.",
                    "label": 0
                },
                {
                    "sent": "But you know it's not only the word bank that has multiple meanings, so Pike.",
                    "label": 0
                },
                {
                    "sent": "What are some of the meanings of the word Pike?",
                    "label": 0
                },
                {
                    "sent": "People know the word Pike.",
                    "label": 0
                },
                {
                    "sent": "Uh, they said there's a polearm right that you can then, yeah, that's for the kind of creative anachronisms people OK.",
                    "label": 0
                },
                {
                    "sent": "It's a poem.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of a fish.",
                    "label": 0
                },
                {
                    "sent": "Any other meanings for Pike?",
                    "label": 0
                },
                {
                    "sent": "Are Turnpike yet so this uses a shortened form of Turnpike.",
                    "label": 0
                },
                {
                    "sent": "I took the Piketon work.",
                    "label": 0
                },
                {
                    "sent": "Yes three other ones.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "OK, I didn't know that.",
                    "label": 0
                },
                {
                    "sent": "I'm sure another Vince Summers lexical access is actually difficult for human beings, but I'm sure another meaning that most of you know is it's a kind of dive.",
                    "label": 0
                },
                {
                    "sent": "Anyone who's watched the Olympics.",
                    "label": 0
                },
                {
                    "sent": "They've seen people doing a Pike, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so you know words have lots of meanings, so almost all of the current work on your embeddings and is just worked on this simple method where you have one string has one vector representation and that's kind of a crummy linguistic model because words have multiple sensors what So what you actually get is that it's representation is a weighted average of what should be the representations of the different sensors, which when people are being fancy they caught a superposition.",
                    "label": 0
                },
                {
                    "sent": "And you could think that this is such a crazy assumption when here we have this word Pike, which could be referring to a Turnpike or medieval weapon, or fish or dive.",
                    "label": 0
                },
                {
                    "sent": "How could these systems possibly work?",
                    "label": 0
                },
                {
                    "sent": "How could they possibly work?",
                    "label": 0
                },
                {
                    "sent": "Anyone have an idea?",
                    "label": 0
                },
                {
                    "sent": "Anyone else?",
                    "label": 0
                },
                {
                    "sent": "Tight.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day, I've just got one Victor for Pike, right?",
                    "label": 0
                },
                {
                    "sent": "So how do I go?",
                    "label": 0
                },
                {
                    "sent": "So I've got my one vector for Pike, but really the fish should be over here and the dive should be over there.",
                    "label": 0
                },
                {
                    "sent": "So how does that not kill me, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly so the vector representation really is this superposition of where you're sort of putting together the different vectors for the different sensors by summing them.",
                    "label": 0
                },
                {
                    "sent": "So to a fairly large degree when you have a particular context of use.",
                    "label": 0
                },
                {
                    "sent": "But that context of use will disambiguate to particular sense, and so you will find that the word does have good similarity with each of with the context corresponding to each different sense, and so that's why it's been a fairly workable system, but you know.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, it still seems a little bit weird that we're not really representing different senses of the meaning of words, and so there's been at least a little bit of work that's tried to come up with vector representations for word sensors.",
                    "label": 0
                },
                {
                    "sent": "I was involved with some work a few years ago which did this and sort of about the crude as possible way, but more recently a couple of other people have tried to deal with this task, so there's this paper by trust, which is a recent word embeddings system which does word sense learning inside the training of the the word vectors, which is a nicer approach to doing it, and there is someone else who had a paper at ACL 2015 during word vector learning.",
                    "label": 0
                },
                {
                    "sent": "Which I didn't succeed in finding this morning, so it seems like there's a little bit of new interest in doing that.",
                    "label": 0
                },
                {
                    "sent": "And this continues all the way out, because of course when you have sentence is I mean, sentences and phrases can also be ambiguous, and so we might be concerned if we're only making one vector as the representation of the whole thing, yeah?",
                    "label": 1
                },
                {
                    "sent": "Salmon.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, so direct.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you've gotta compromise, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it has to be the case, but by piping ambiguous it would be less close to Sam and then if you just had the fish sense of Pike and you'll say how close to that of salmon.",
                    "label": 0
                },
                {
                    "sent": "So it is a little bit of a compromise, but.",
                    "label": 0
                },
                {
                    "sent": "I think the thing to human beings have exceeded exceedingly bad intuitions about how 100 dimensional spaces work.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that 100,000 dimensional spaces you can simultaneously be close to a lot of different things, and so therefore, miraculously, this superimpose vector can be close to all of these different things at the same time, in a way that doesn't work without 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Intuitions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, another problem that hasn't been very much dealt with, but it's just an important one to be aware of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so all of this work is used.",
                    "label": 0
                },
                {
                    "sent": "This sort of simplistic assumption of 1 string.",
                    "label": 0
                },
                {
                    "sent": "One string, one word vector.",
                    "label": 0
                },
                {
                    "sent": "Another way in which this is problematic is all languages have a lot of multi word expressions.",
                    "label": 0
                },
                {
                    "sent": "Multiword expressions where you have a multiple word unit that has its own meaning.",
                    "label": 0
                },
                {
                    "sent": "So we get things like place names like New York, we get noun compounds like neural network we get.",
                    "label": 1
                },
                {
                    "sent": "So in English there are verb, particle constructions, makeup.",
                    "label": 0
                },
                {
                    "sent": "There are various other kinds of idioms, like kick the bucket and take a hike.",
                    "label": 0
                },
                {
                    "sent": "At all of these non compositional things, some of them have vague sort of metaphorical connections, but you know they're very much things that you have to know the meaning of.",
                    "label": 0
                },
                {
                    "sent": "You know that you have to sort of know that a neural network is really this piece of matrix algebra that people program.",
                    "label": 0
                },
                {
                    "sent": "It's not referring to something that's in people's heads most of the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically it seems like human beings also have to store the meanings of these things, and so one way of dealing with this is to actually learn at the same time vectors for these multi word units on the district in the vectors that Google distributes with word to vec, they've actually done this, that they've defined some fixed number of common multi word units, which certainly includes a lot of things like place names, because I think they're probably.",
                    "label": 0
                },
                {
                    "sent": "Important to the search engine and so those were sort of pre clumped on some criteria and it's quite likely the criteria might be pointwise mutual information 'cause that's a common criterion for finding associated strongly associated words and then an extra vector was learn for those combinations.",
                    "label": 0
                },
                {
                    "sent": "And of course that has some disadvantages are greatly boys out your vocabulary and also you're making this sort of hard decision as to.",
                    "label": 0
                },
                {
                    "sent": "Which ones to clump and at least in some cases you can get ambiguities in this sort of same word sequence, and it could be the way so one example there is with these particle verbs.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That that you can have them used sometimes in both the literal prepositional usage as well as in the kind of phrasal verb use where they have a special meaning, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, they said your frequency with which they Co occur, but then you want to scale that by what the actual just bear frequency of the words are and then that immediately gives you pointwise mutual information.",
                    "label": 0
                },
                {
                    "sent": "So I think there aren't any such in Word to VEC an actually quite a few quite a bit of the work that does phrase learning only does contiguous phrases, but in principle you can also consider counts at one remove or counter to remove and get counts of how often those occur as well.",
                    "label": 0
                },
                {
                    "sent": "And you might some over different close by distances, and so you could start to try and workout those ones as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Apps.",
                    "label": 0
                },
                {
                    "sent": "I've got a few tell it just given in the last slides maybe.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "You always do.",
                    "label": 0
                },
                {
                    "sent": "So OK, I haven't said much about other languages.",
                    "label": 0
                },
                {
                    "sent": "His teeny bit actually coming up, but you know, there's more to say about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so even in English the particle verbs can be separated.",
                    "label": 0
                },
                {
                    "sent": "He made the answer up, but in other languages separated multi word expressions are much more common.",
                    "label": 0
                },
                {
                    "sent": "That's true, so you sort of want a method that can deal with it that I agree with that.",
                    "label": 0
                },
                {
                    "sent": "So another way in which you can learn word vectors and similar things is by using sequence models like LS, TM's that I haven't really.",
                    "label": 0
                },
                {
                    "sent": "I think has anyone talked about, yeah?",
                    "label": 0
                },
                {
                    "sent": "The behavior of what you get out of LSD.",
                    "label": 0
                },
                {
                    "sent": "M card style word representations often tends to be very different 'cause they're very focused on predicting the next word rather than kind of being a general semantic representation of context.",
                    "label": 0
                },
                {
                    "sent": "But maybe I'll leave that for now.",
                    "label": 0
                },
                {
                    "sent": "That will come back again a bit later.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just a few more minutes and so I can stop and have a teeny bit of time for questions.",
                    "label": 0
                },
                {
                    "sent": "At the end I thought I'd sort of return at the end now to just having a little bit more linguistics and food for thought on representations and how people do things and might do things in deep learning.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is this sort of conventional representations of linguistics?",
                    "label": 0
                },
                {
                    "sent": "So at the low end, starts off with phonetics and phonology.",
                    "label": 1
                },
                {
                    "sent": "So phonetics is the about the actual sound stream of language, and that's pretty uncontroversial weekly use.",
                    "label": 0
                },
                {
                    "sent": "That and deep learning speech systems as well a thing would sort of interesting, is that for the sound systems of language, phenology posits that there's a small set of distinctive categorical units there.",
                    "label": 1
                },
                {
                    "sent": "Phonemes or pep sometimes distinctive features of sounds, and there seems to be a fairly kind of universal set of what kind of phonemes and phonemic distinctions there are.",
                    "label": 0
                },
                {
                    "sent": "You know, different languages have different subsets of these phonemes, but there's a pretty restricted space of sounds that human languages use that by and large they have bilabial purse person.",
                    "label": 0
                },
                {
                    "sent": "They have tongues at the front with third person raising the back of the tongue with dealers likings.",
                    "label": 1
                },
                {
                    "sent": "You know the same kind of set of things he used in basically all languages, but their language particular realization can vary quite a bit, and so there's been a lot of study how, for various phenomena, such as voice onset, time or positions of ALS, that they'll vary a lot in their phonetic realization across different languages.",
                    "label": 0
                },
                {
                    "sent": "But one of the most studied and famous sets of evidence for categorical perception.",
                    "label": 0
                },
                {
                    "sent": "Comes from looking at Phonologie where there's a varying realization of the signal across the continuous space that human beings in listening to these signals will categorical eyes it and say that although the sound is moving continuously that up until some point they regarded basically as categorically as, say, AP sound, and then when the voicing starts a little bit earlier then regard categorically.",
                    "label": 0
                },
                {
                    "sent": "As a B sound and you get these very strong categorical perception affects which correspond to languages having these kind of categorical signalling systems.",
                    "label": 0
                },
                {
                    "sent": "So these are Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Names, I mean something that's kind of interesting, I think, is that if you look at what happens in speech, no one works with phonemes or the slightly looser loosely described phones, while people in speech.",
                    "label": 0
                },
                {
                    "sent": "And this is regardless of whether they are doing deep learning or previous methods that they work with Triphones, which is a phone in the context of another phone on each side, or sometimes actually Quinn phones of two phones on each side.",
                    "label": 1
                },
                {
                    "sent": "And what this is reflecting is that how phoneme sounds like how a sound sounds varies a lot in the context of argar, versus if you've putting it in front of a front vowel, like Iggy or something, right?",
                    "label": 0
                },
                {
                    "sent": "That the G sounds realization phonetic Lee varies a lot, and so the conventional wisdom of speech systems, including nearly all deep learning speech systems is.",
                    "label": 1
                },
                {
                    "sent": "You get these triphones orquin phones.",
                    "label": 0
                },
                {
                    "sent": "And you which means there's a huge huge number of the starting point is you model them separately, but then normally what you do is cluster them because the space is too vast and decide that some of them can be treated together.",
                    "label": 0
                },
                {
                    "sent": "So maybe G in between evals is near enough to G in between if else because they're both front valves that you can cluster them together.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing there is that over in deep learning we meant to be interesting, depth of representation and abstracting the higher level units that basically deep learning speech systems don't actually achieve the depth of representation that's present in traditional linguistics and also just seems to be right for a description about human languages work.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Because the highest level of representation is that they've got this clustering of triphones and that they're trying to map on to units.",
                    "label": 0
                },
                {
                    "sent": "In this clustering of triphones that they're not getting to a higher level representation, which is roughly equivalent to saying, OK, this is a Bush sound.",
                    "label": 0
                },
                {
                    "sent": "But since at the end of the day, for the words that they are written in terms of these high level phonemes that you would hope to get that higher level of representation.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll go run now and we can put out a minute writing systems so most.",
                    "label": 0
                },
                {
                    "sent": "We work in deep learning, NLP, work starts with language in a written form as found data that we have lots of.",
                    "label": 1
                },
                {
                    "sent": "I mean, so something that I think is at least worth remembering is that human language writing systems aren't one thing.",
                    "label": 0
                },
                {
                    "sent": "So depending on what writing system you're using, you actually end up with a very different form.",
                    "label": 0
                },
                {
                    "sent": "So some writing systems are true phonemic writing systems where you just got the sounds GI.",
                    "label": 0
                },
                {
                    "sent": "Yahoo not a Boo.",
                    "label": 0
                },
                {
                    "sent": "You have some of them kind of fairly regular fossilized phonemic systems like English.",
                    "label": 0
                },
                {
                    "sent": "People know what this is.",
                    "label": 0
                },
                {
                    "sent": "They look to trade one person at least knows the cake.",
                    "label": 0
                },
                {
                    "sent": "Canadian language.",
                    "label": 0
                },
                {
                    "sent": "They have another language, right?",
                    "label": 0
                },
                {
                    "sent": "So so the new tubes are split, some of them using Romanization, but others using Institute syllabics, right?",
                    "label": 0
                },
                {
                    "sent": "So each letter is then representing a syllable.",
                    "label": 0
                },
                {
                    "sent": "In here graphic rising, essentially each letter is representing roughly a morpheme meaning unit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if you just sort of doing the obvious thing in your deep learning system and working with letters that depending on what language you're working with, you're actually working with very different sized linguistic units.",
                    "label": 0
                },
                {
                    "sent": "And that's something that hasn't actually been paid very much attention.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another way in which languages vary a lot is how they represent words or don't.",
                    "label": 1
                },
                {
                    "sent": "So there are some languages, Chinese, Japanese that don't put any spaces between words, so you just get a continuous sequence of letters.",
                    "label": 0
                },
                {
                    "sent": "Other languages separate into words that they don't, always separating the words the same way.",
                    "label": 0
                },
                {
                    "sent": "So some so you tend to get in languages, and little clitics, which are these sort of reduced forms of words.",
                    "label": 0
                },
                {
                    "sent": "That sort of final logically hang off another word.",
                    "label": 0
                },
                {
                    "sent": "We have some of them in English as well for things like isn't an aisle.",
                    "label": 0
                },
                {
                    "sent": "We get these clitic forms that are fun to logically dependent, and so some languages like Arabic join clitics onto the same word, so.",
                    "label": 0
                },
                {
                    "sent": "This is the verb say, but you know it's got these information about subjects and objects and other Joyner words as clitics and they all just joined together, whereas other languages like French writing system, you get these clitic pronouns and XR res, which are written as separate words, whereas they might be written connected together.",
                    "label": 0
                },
                {
                    "sent": "And things are also very with compound.",
                    "label": 0
                },
                {
                    "sent": "So in English you put spaces between words of a compound in German.",
                    "label": 0
                },
                {
                    "sent": "He just keep on going.",
                    "label": 0
                },
                {
                    "sent": "Almost sorry.",
                    "label": 0
                },
                {
                    "sent": "The joint the sorry, I kind of missed a space here.",
                    "label": 0
                },
                {
                    "sent": "The world starts there.",
                    "label": 0
                },
                {
                    "sent": "Joined as a partner but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "OK, but almost inevitably if you're writing systems for German, Dutch, similar languages, you can gain by using a compound splitter as pre processing.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so linguists analyze words as built up of sort of smaller units of meaning, morphology, so unfortunately you've got at least the three parts.",
                    "label": 0
                },
                {
                    "sent": "You could also maybe separate, fortunate from fortune and eight most deep learning work hasn't actually dealt with the make up of words.",
                    "label": 1
                },
                {
                    "sent": "There was one paper that we did a couple of years ago that did use hierarchical tree model of the kind of look at this afternoon.",
                    "label": 0
                },
                {
                    "sent": "I'm to build up word meanings and this is potentially quite useful for dealing with unknown words because a lot of the unknown words are actually different morphologically.",
                    "label": 1
                },
                {
                    "sent": "Drive forms of words you already know.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An alternative really doing morphology is to use character engrams, and that's been quite a popular simple alternative.",
                    "label": 1
                },
                {
                    "sent": "There was used by Rumelhart and McClelland decades ago and they call them wical phones.",
                    "label": 0
                },
                {
                    "sent": "It's been revived by it in a sequence of recent pieces of work at Microsoft there DSM model works over.",
                    "label": 0
                },
                {
                    "sent": "Character trigrams and one of the beauties of working with character trigrams is immediately shrinks your vocabulary to a fixed, manageable size, but normally 3 letters is kind of almost enough that you can get the semantics of things in code into your vectors.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then above that we have syntax sentence structure which can often be represented as either this kind of dependency syntax which shows relationships between words or content constituency syntax where you get a tree representation over sentence is and I'll show some more examples of that this afternoon when we look at that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so then above that we have semantics and pragmatics.",
                    "label": 1
                },
                {
                    "sent": "So semantics is the inherent meaning of linguistic unit and pragmatics's interpretation of meaning and context.",
                    "label": 1
                },
                {
                    "sent": "And so from this perspective, it's kind of interesting to note the really our meat, so called meaning representations that we normally end up with in deep learning actually mixing a lot of pragmatics, as if we're building our meaning semantics by starting with a huge amount of found data and collecting in.",
                    "label": 0
                },
                {
                    "sent": "Collecting up some kind of counts, we've got a lot of pragmatics mixed in there.",
                    "label": 0
                },
                {
                    "sent": "Sort of real world context information of how things are used OK.",
                    "label": 0
                },
                {
                    "sent": "So types of semantics, so I mentioned this briefly, but there are different ways of doing semantics, right?",
                    "label": 0
                },
                {
                    "sent": "So we've looked a lot at distribution of semantics.",
                    "label": 0
                },
                {
                    "sent": "They've been very useful.",
                    "label": 0
                },
                {
                    "sent": "It's not the only, or necessarily always a sufficient way of doing semantics.",
                    "label": 0
                },
                {
                    "sent": "What other ways of doing semantics are there?",
                    "label": 0
                },
                {
                    "sent": "Their levels, like semantic role labeling and frame semantics, have been explored a lot in linguistics and NLP, where essentially sort of having something that's a bit more semantic than a syntactic representation.",
                    "label": 0
                },
                {
                    "sent": "Were you wanting to normalize things?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into LOL so you got something like this.",
                    "label": 0
                },
                {
                    "sent": "Cynthia sold the bike to Bob for 200 and you're working out these different roles of elements of a sentence and so the roles would be the same even when the syntactic expression was changed or something like the bike was sold to Bob, you'd still say the bikers, the goods and Bob is the buyer, so it's kind of a form of more normalized representation.",
                    "label": 0
                },
                {
                    "sent": "And then we've got computational formal semantics where you're mapping things into some kind of explicit.",
                    "label": 0
                },
                {
                    "sent": "Model, denotation and words are being kind of mapped into their sets.",
                    "label": 0
                },
                {
                    "sent": "And that's actually an important issue.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But these kind of denotational semantic models haven't been much explored in deep learning, but I think they're going to have to be explored more.",
                    "label": 0
                },
                {
                    "sent": "We already had some discussion of antonyms, but I think another place where denotational models will need to be brought in much more is that for a lot of the tasks we want to do, we want to treat, say, two people as different, right?",
                    "label": 0
                },
                {
                    "sent": "Two people have just never the same for names of because they have different denotation.",
                    "label": 0
                },
                {
                    "sent": "Is that guy.",
                    "label": 0
                },
                {
                    "sent": "And there's that guy, and that's a fundamentally different notation notion.",
                    "label": 0
                },
                {
                    "sent": "Two similarity in a distribution all sense.",
                    "label": 0
                },
                {
                    "sent": "So if you think of if you build models of similarity in a distributional sense, well, you get results like Ronald Reagan and George Bush very similar to each other.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing any kind of task where you want to be looking at sentence, meaning and.",
                    "label": 0
                },
                {
                    "sent": "Doing an information retrieval task or doing the kind of textual inference task, you just don't want to get from that.",
                    "label": 0
                },
                {
                    "sent": "Ronald Reagan died.",
                    "label": 0
                },
                {
                    "sent": "Therefore, you can conclude that George Bush died.",
                    "label": 0
                },
                {
                    "sent": "That's that's not a valid step in sentence similarity, and so I think sort of exploring more denotational semantics will be necessary, and it's been neglected in deep learning models.",
                    "label": 0
                },
                {
                    "sent": "And not sure now like me to stop.",
                    "label": 0
                }
            ]
        }
    }
}