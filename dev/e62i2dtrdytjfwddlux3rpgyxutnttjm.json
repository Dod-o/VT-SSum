{
    "id": "e62i2dtrdytjfwddlux3rpgyxutnttjm",
    "title": "Summarizing Data Stream's History",
    "info": {
        "author": [
            "Baptiste Csernel, France Telecom Research"
        ],
        "published": "Nov. 26, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Data Mining->Temporal & Streams Mining",
            "Top->Computer Science->Data Mining->Temporal Data"
        ]
    },
    "url": "http://videolectures.net/mmdss07_csernel_sdsh/",
    "segmentation": [
        [
            "Hello so I'm going to present today's work I've been doing and this provision of water by and fabric level which is a follow up to the talk Georgia by gave you yesterday on data streams and I'm going to present summarizing methods for data stream."
        ],
        [
            "Some first going to give you a brief summary of.",
            "Brief remembering of wet, so that has tremendous definition of what's summary of the data stream.",
            "And while you were trying to build them and then I'm going to prison two methods, one to build a summary of a single data stream and another one to build a summary of several data streams linked by relation."
        ],
        [
            "So first for those who didn't listen yesterday, what should I stream?",
            "So here's the definition by Golub, and those are from 2003.",
            "The test trim is a real time continuous ordered sequence of item and it is impossible to control the order in which the terms arrive, nor is it feasible to locally store streaming St.",
            "I see, so this definition implies of through consequences for algorithm dealing with data streams.",
            "The most obvious one is that this algorithm have to be single path.",
            "Of course those are consequences that they have to use.",
            "Small amounts of memory and time parliaments since most data streams have very high rates and the usually the algorithm used F to use bounding memories."
        ],
        [
            "So then what's the data stream summary?",
            "Well, is the goal of building a data stream somewhere is to have in small space A summary of the streams content that will be usable for.",
            "Many different kinds of data mining tasks or valuable evaluation.",
            "Another goal of the data Stream summary is that this that general summary should be usable to build the summary about any given part of the stream, not necessarily the world stream, but any part the user may want to analyze after the stream's gone."
        ],
        [
            "So why do we wish to build those summaries?",
            "That's a question that was raised by Francoise yesterday.",
            "So.",
            "Well, obviously due to the very nature of data streams.",
            "It's not possible to ask or query stream after it's gone, so once the strings has gone, if you've forgotten to query it for one reason or another, well, you don't have anything you can do.",
            "And in many cases, especially in industrial environments, in the light of new information of new perspective, one may want to pose new queries in the stream that has gone or to one new analysis made for marketing purposes or.",
            "For analysis.",
            "To give answers to these queries, even approximate and says it is necessary to have some kind of summary of the streams contents.",
            "Then there is a second reason why building summaries of data stream may be interesting.",
            "As we vote yesterday, in many instances, data streams aren't actually discarded after the gun, but but are stored in large long-term memory structures.",
            "The problem of the structures is that the data that's stored on them is very difficult to access, and even more difficult to analyze.",
            "So the process of retrieving the data is very long.",
            "Very tedious and of course very expensive.",
            "That's why, even if the data is available, it would be useful to have small, lightweight summaries that can provide approximate answers to any queries that one would need to do on the streams or on the data at little costs in terms of computation."
        ],
        [
            "Or memory.",
            "Yesterday Georgia by vote several methods of summer to summarize data streams.",
            "So those are samples, histograms, sketches, micro clusters.",
            "All these methods of both the interestings of flows.",
            "The sampling methods that were presented yesterday worked only on selected Windows or on the world Stream, which goes against one of the principle.",
            "I announced that we wanted to summarize to be usable for any given Windows in the stream history.",
            "You've got histograms that suffer the same kind of drawbacks, and Moreover most of the times with histograms are quite expensive to build in terms of computation time.",
            "If you want them to give large amount of information or otherwise they give two simple data to simple information that don't allow for precise analysis.",
            "Then you've got sketches.",
            "Sketches are usually very interesting data structures, but they are usually built for one single task and not with a general purpose in mind.",
            "So you can use them to insert query after the time, but you can't use them for general purpose analysis.",
            "And then we've got micro clusters that will vote with the clustering algorithm yesterday.",
            "It's all quite useful.",
            "The answer is our general usage and they can be used to query on any single time window on the stream, but they are quite expensive in terms of computation time.",
            "And in terms of memory usage.",
            "On the computer."
        ],
        [
            "So I'm going to present you a new method that we've developed Coldstream Samp, and that's based on sampling this time and.",
            "So that uses the principle of sampling to build a lightweight summary of one single data stream."
        ],
        [
            "In so here's how the algorithm works.",
            "So as the data streams goes by, it's sampled at a constant rate Alpha to build samples of a given size TT being fixed at the startup of the algorithm.",
            "And once a simple is filled and you simply started until it's filled again, then and you some pearly started again.",
            "So as we can see, if we continue to do that and definitely will get a lot of temples and so will break our bond."
        ],
        [
            "Memory requirements, so we've got to deal with what to do with the samples that are going to voluminous.",
            "Susie idea here is once a given threshold has been raised has been reached.",
            "So for instance, here's especially sweet to work with one swift code, 3 samples.",
            "We say this is enough and we're going to fuse the two oldest samples to one new sample of a higher order.",
            "To do that, we wonder, only select half the elements of each sample to build.",
            "A new sample of a higher order.",
            "At each order, we still follow the same limits of a maximum number of samples, so that if this maximum order of simple is reached for new order will once again fuse 2 samples together to build a new sample of a higher order.",
            "So as time goes by, we'll have.",
            "Higher and higher order of samples, which will represent larger and larger portion of the data stream history so that for recent time will have quite rich, simple and for all the times of the stream the sample will be."
        ],
        [
            "Much smaller.",
            "To constitute a sample on a given portion of the stream, we just have to fuse the sample.",
            "We have to fit the window the time window where we want to analyze and.",
            "Each element will receive a weight that will depend on the order of the sample, so the weight is 2 powers.",
            "The order of the sample, since at each time where dividing sample size by two.",
            "So this gives older simple a much higher weight than newer ones since they have much fewer elements and this lowers to build for any given window in the stream.",
            "Uniform sample with a constant sampling rate and sampling rate and just different weights depending on the element."
        ],
        [
            "Um?",
            "Since what we build finally is a simple, any given data mining task can be applied to this, weighted to this weighting sample.",
            "We've chosen for evaluation purpose to test this sample with the clustering task in order to compare it with plus trim that was mostly tested on clustering.",
            "Travel rates.",
            "How is the scores with used in track list or inertia?",
            "And we've tested the algorithm on."
        ],
        [
            "Several data sets.",
            "So the first one is a small artificial datasets that prove this choose to.",
            "Um, control how this is both algorithms react to brittle changes in the data distribution, so the data set is 100,000 element along with 30 variables following Goshen distribution and the data set is composed of five equal sized cluster.",
            "Um, the small thing is that one cluster is present throughout all the streams.",
            "Was two clusters only present in the first half of the stream, and tours are in the second half, so at the middle of the stream the distribution suddenly changes as to cluster disappear from the distribution and junior I arrive."
        ],
        [
            "If so, let's see how both algorithm reacts to this brutal change.",
            "Since the distribution is quite simple on the first half of the stream, both algorithms perform quite well and have very similar results.",
            "However.",
            "On the second half of the stream.",
            "When the distribution suddenly changes, stream some since it uses a simple Bayes approach, doesn't remember anything of the previous history of the stream and so doesn't have a problem adapting to the new distribution.",
            "Was close trim as we've seen yesterday, I've slowly evolving micro clusters so those micro clusters take some time to adjust to the change in the distribution and this reflects on the stream with a slightly lower score on the second half.",
            "If we use both algorithm on the whole data stream.",
            "This deficiency of close trim on the second part also shows in the total result of the algae."
        ],
        [
            "Prism.",
            "The second set for used was the KDD 98, a shapeable donation, that assets which relates to donation information.",
            "Following the mailing campaign, it's quite a stable that I sets with basically 5 clusters with only selected 20 valuables.",
            "To limit the number of missing elements.",
            "Since we weren't interested in cleaning.",
            "Problems for this test, which resulted in quite small sets of 75,000."
        ],
        [
            "Elements.",
            "Um?",
            "Here again.",
            "The difference is not really sizable on the small examples since swim sums.",
            "As a small advantage, but that's not very visible, and this advantage disappears for more recent data, which proves that since both algorithm have more precision for recent data on recent data is that predicts very similar results.",
            "However, on older data performance a bit better."
        ],
        [
            "Finally, we also tested this algorithm them on the kated in 99 Networking PRISM data set, so this is.",
            "Larger data set of 500,000 elements with 74 variables.",
            "Which which basically come from 5 different clusters, normal TCP connections and four different types of attacks.",
            "This is the record of TCP connections to the MIT Lincoln Labs."
        ],
        [
            "So here, since the data set is much bigger.",
            "Choose me, it's not 500 thousands, 5 million elements.",
            "So we've run different analysis depending on the part of the stream that we want to analyze.",
            "So here's a 1st result for different parts of the stream.",
            "So here you've got the first.",
            "The elements from first 1,000,000 to 1 million 500,000 share from 2 million to 2 million 500,000.",
            "And here from 3 million, 3 million 5.",
            "You've got three algorithm, stream some peas in red here.",
            "Close trimming blue and stream and much older.",
            "Pursuing string clustering algorithm is in yellow.",
            "So here we can see that on the oldest data stream, some is performing much better.",
            "Here we've got quite.",
            "An empty distribution.",
            "Actually nothing much is happening in this part of the data of the data sets, so both algorithms are performing quite well with very similar results.",
            "Um?",
            "Hamstring sample is also performing quite well and on recent data all three algorithms have very comparable results."
        ],
        [
            "No, this curve shows performance of both close streaming red and stream.",
            "Something blue for an increasing number of filaments taken from.",
            "As the very first elements of the stream with an increasing quantity of the elements with the last point representing an analysis on the whole stream.",
            "As we can see.",
            "On the very first elements here, performance are quite similar for both algorithm with over old data and only that.",
            "Um?",
            "Then there's quite a gap forming on this central part of the stream.",
            "And.",
            "If one takes the whole part, this difference slowly gets bigger and bigger."
        ],
        [
            "This is the same thing, but starting with.",
            "The last elements received, so the most recent elements here and then an increasing quantity of stream elements being analyzed at each different points.",
            "So once again, on very recent data performance are quite similar, but if we increase the size of the window.",
            "Stream sons performance becomes."
        ],
        [
            "Better.",
            "And finally, the main point is of course speed, as we could see with Metro cluster approach, one is to compute alot of ultra distances between the micro cluster to add new elements.",
            "This is of course very expensive where the sampling approach on its online part is very simple since we're just sampling the elements and the only complex operation is fusing to sample together, which doesn't take much time.",
            "So here's the difference.",
            "It's quite huge.",
            "In terms of processing speed.",
            "So.",
            "This proves that well.",
            "As was said before, more complex approaches aren't necessarily better approaches.",
            "Sampling is a very, very simple approach that's been used for quite some time, and it products in this instance very good results.",
            "At very fast speed, which is always a desirable quality for data stream processing."
        ],
        [
            "Algorithm.",
            "I'm now going to present.",
            "Cross trim, which is an algorithm for relational stream summaries.",
            "So I'm first going to explain what I mean by what's relational streams and try to explain why we found this to be an interesting and challenging problem.",
            "Then I'll present one tool I'm going to use in my algorithm, which is Bloom filters.",
            "And how is the function?",
            "And then I'll present the algorithm and a few tests with one."
        ],
        [
            "So far.",
            "Oh so first motivation.",
            "The thing is that while most paper and research interest them self in the treatment of one single data stream as it's happened, data seldom stands on its own in real world application, but is often relational by nature.",
            "So most streams and most data in general kind of kind of where don't stand on their own.",
            "For instance.",
            "Can have stream of clients, a stream of services and then you'll have a stream of service usage.",
            "For instance telephone calls and this stream will link a client with the user service.",
            "If one just analyze the service usage stream, one depletes himself of a lot of information that's contained in the link this stream has with those tools or streams or belief is that by being able to build a summary, taking this information into account will be able to give much better results in terms of analysis.",
            "Um?",
            "For this data."
        ],
        [
            "So this is quite a complex problem since we're dealing with relational data.",
            "So for starters we have chosen to address a simplified version of the problem, dealing with only three data streams.",
            "So we've got two streams of entity, so those teams are constituted by.",
            "Well, elements containing a number of valuable a timestamp.",
            "And promote keys that indexes each element so each element has its own unique key.",
            "And then a stream of relation.",
            "Each element of the relation as it's all its own, attributes it some time stamp and as a set of keys that references elements from agent to the streams.",
            "We've made a few further purchases.",
            "On this frames the first one, and that's quite a big part of this is that audio streams are insertion streams only, so one can't remove and elements of the into the streams when it's passed through.",
            "And those reporters is is that the stream of relation goes much faster than both entities stream, which is most of the time through in real world applications.",
            "Another one is that all attributes are numerical, which is more of a convenience things to deal with.",
            "Is a metrics we're going to use.",
            "And the last one is that.",
            "References constraints on never broken or examples that is, you won't find an element in the relation streams that makes reference to elements that have never arrived.",
            "Inns into the streams.",
            "Which of course could happen in real world applications, since elements can get lost or can get delayed or whatever depending on operational circumstances."
        ],
        [
            "So given this problem, what so bold exactly?",
            "It's to build a summary of all the Swiss strings taken individually.",
            "But also to realize a summary of the information contained in the links, as the Swiss streams share with one another.",
            "Um?"
        ],
        [
            "So to build this, ask Verism with inspired of self of two things we've made use of Bloom filters that I'm going to explain an with inspired our self of the clustering algorithms that was presented yesterday and that makes use of micro clusters and cluster feature vectors.",
            "Oh, since I've just proven before that micro clusters might not necessarily be the best approach for data stream summaries.",
            "You might ask yourself why I've decided to use them in that instance and that becausw sampling in this case is not a good approach because of the relational links that.",
            "The strength share together.",
            "If you're only dealing with one stream.",
            "If the stream is massive, missing one element is never a big deal and it won't have large consequences on your distribution.",
            "However, in a relational context.",
            "One elements, even if it doesn't play a big role in the distribution of its own stream.",
            "May play a big role in the relational links that the Swiss streams share together, so it is not feasible to actually skip an element without having tested what his links are with the different streams.",
            "So that's why we've chosen to go for a micro cluster based approach.",
            "Um's classroom was explained yesterday.",
            "I won't go over it again.",
            "I'll just explain what's a Bloom filter."
        ],
        [
            "So Bloom filter is very small memory structure and very fast structure that allows you to basically memorize a set of numbers.",
            "It supports two simple operation, adding a new number to the filter and testing without number has been learned before or not.",
            "Both of the open ocean are very fast, which is."
        ],
        [
            "Why was chosen as a structure for algorithm?",
            "So here's the basic principle.",
            "Abloom filter is just a simple banger, binary word, forgiven lens be.",
            "With associated to it.",
            "The number of K hashing function that map from R to the size of the filter.",
            "They have to map, of course uniformly from art.",
            "Went to build the wise.",
            "The filter will be biased and won't work properly.",
            "At initialization, all the bytes are set to 0.",
            "When the new elements arrived and we want to learn that elements, it is asked by the key function which will return Kane.",
            "This is.",
            "Between one and bills or sector equals two, so we've got touring.",
            "This is that I've been selected and Zeus indices are set to one in the Bloom filter, so this is very simple operation and basically what takes the most time is computing the hash functions."
        ],
        [
            "To test a new element.",
            "Once again, the new element is asked by the different hash functions, which gives several indices.",
            "Then one compares zoos indices with one set at one in the Bloom filter.",
            "If all the indices that have been selected by the hash functions are set at one in the Bloom filter, then with high probability this element has been learned before by the Bloom filter.",
            "If any of the elements any of these indices are set to zero in the Bloom filter, then we are certain that the element hasn't been learned before.",
            "Um?",
            "So both of these operations also very fast and thus very interesting properties to Bloom filter, and the fact that these filters are additives.",
            "So basically, if I have two filters that have learned two different set of numbers.",
            "Um?",
            "Just by doing a logical or between the two filters.",
            "I wrote a new filter that will have learned the unions of the two previously learnset, which with a property that we're going to exploit later on as we."
        ],
        [
            "We'll see.",
            "So I'm now going to present.",
            "My method so far."
        ],
        [
            "First, small overview of the structure of the algorithm.",
            "So.",
            "We're dealing with Swiss dreams.",
            "20 streams here that are going to be summarized in quite a similar fashion as the one used in the clustering algorithm by a set of micro clusters, except that this clusters will also be attached with a bloom filter for each micro cluster.",
            "And summarize other relations trim.",
            "Will use across table that will contain cluster features vector.",
            "In each of its.",
            "Cases."
        ],
        [
            "So what happens when we receive a new entity stream element so?",
            "As the element arrive.",
            "We locate the closest micro cluster to this elements using a distance function appropriate to the data we are dealing with.",
            "We test whether this element lies within an acceptable range of the micro cluster, for instance by using the radius of the micro cluster.",
            "If it is so, we update the cluster feature vector information of the micro cluster.",
            "And.",
            "We learn the key of the element with the Bloom filter attached to the micro cluster.",
            "If the element is not admitted, we create a new micro cluster with this element as its seeds and for that.",
            "Since we're using the fixed amount of micro cluster, we have to make form for this new micro cluster.",
            "In order not to break relational constraints between the three, streams were not allowed to actually deley delete any micro clusters, so our remaining solution is to fuse two of them of the existing micro clusters.",
            "So we locate the two micro cluster which are close to each other and you refuse to fuse them together.",
            "This is possible since both the cluster feature vector.",
            "And the Bloom filter have additive properties.",
            "So with a simple operation we confuse to micro cluster together.",
            "Propane obtain a new."
        ],
        [
            "When an element survives in the relations trim.",
            "The method is a bit different this time.",
            "We'll take first the key, referring to the elements in the first entity stream.",
            "For instance, the entity stream.",
            "And then we'll check whether this key has been learned by any of the bloom filters attached towards micro clusters.",
            "Given the properties of the Bloom filter with wanted that will find at least one of them that will have learned this key before.",
            "Since bioprocesses elements in the relation stream can break relational constraints.",
            "Um?",
            "So we locate one or several clusters.",
            "Will locate at least one cluster.",
            "Having learns this elements.",
            "The problem is that bloom filters being at approximate indexing solution collision may occur within a bloom filters and so it may happen that several bloom filters will actually recognize this element as having been learned.",
            "Use the bloom filter or appropriately sized.",
            "This shouldn't happen too often, so we've decided that if we haven't found a unique temple of indices were going to drop this elements since collision occur at random within the bloom filters.",
            "Normally this shouldn't happen.",
            "Alter the distribution of the world set.",
            "The limit actually.",
            "I've not put in the slide you have actually.",
            "Just says it.",
            "Oh yes, you've got to grow and T on the other way.",
            "Depending on the number of hash function and on the size of the bloom filters that allows you for a given number of land elements gives you another wait so.",
            "Yes, to appropriately sized them.",
            "Um?",
            "We'll see in the testing how the Bloom filter size actually alters the result we get on this problem.",
            "So if the couple is tonight, we put we update the safety information.",
            "Referring to this in this in the Civi cross table.",
            "Which will record information concerning that dilemma."
        ],
        [
            "It's.",
            "Yes.",
            "Action."
        ],
        [
            "I'm warranted because of the.",
            "Of the several properties that.",
            "It has to have been seen before."
        ],
        [
            "So here's a small summary of the structure of the algorithm, so we've got here.",
            "As in Michael clusters summarizing the entity streams and here.",
            "Uh, so stevey crosstable, summarizing the relations stream.",
            "So when a new element in the relation stream arrive, we locate the element is the micro clusters.",
            "That contain Xelement made reference to.",
            "And we update the CV corresponding to the cross of this elements.",
            "So in this way we actually remember information concerning the links this elements share with."
        ],
        [
            "Wanna know them?",
            "So up to now this is AM.",
            "Um, an algorithm is that actually deals only with the world stream.",
            "It should evolve with detection distribution, but you can't actually use it on selected part of the stream.",
            "So just as with plus trim, we've chosen to use.",
            "A snapshot system that will take images of the state of the algorithm at previously selected.",
            "A system Clock times.",
            "Following um.",
            "Oh would you metric distribution in two in powers of two?",
            "At each snapshot, we only keep the CR, V, and idealists of which micro clusters summarizing's into this stream, and we keep all this.",
            "If we cross table.",
            "We don't have to keep the bloom filters since in the summaries are not useful anymore since we're not going to add new elements to old summaries.",
            "Given the properties of the CSV that they can be added and subtracted from one another, we can.",
            "Um?",
            "Subtract an old.",
            "Snapshot from a newer one to recreate what the state of the algorithm would be if it had only been applied to a selected time window.",
            "Um?"
        ],
        [
            "So I'm now going to present our first tests.",
            "So far we've only made test on artificial data, though we've obtained quite a large data set from France Telecom, but we haven't run it through the algorithm yet.",
            "So I'm going to present."
        ],
        [
            "Two series of tests.",
            "Who evaluates?",
            "The principle of the algorithm.",
            "In order to evaluate the performance, use the notion of micro cluster purity.",
            "So basically.",
            "The data will be produced by a distribution containing several clusters, which I'll refer to as micro clusters was not to confuse them with the micro clusters.",
            "Um?",
            "And we'll evaluate the purity of micro cluster by saying.",
            "What's the quantity of elements that come from a given micro cluster?",
            "Our ideal micro cluster will have elements coming only from one given macro cluster, which means it will have emotionless.",
            "Elements in the cluster.",
            "Soter evaluate.",
            "The quality of Entity Stream summary will compute the Purity Falls and micro clusters summarizing when into this stream and for the relation stream will compute security for oils service in the crust."
        ],
        [
            "Able.",
            "So first we've taken a very simple data sets to evaluate the impact of Bloom filter size on the quality of the summaries obtained.",
            "So there's a relational distribution in this data set will be very simple.",
            "The data set is not dinner diner, making that the distribution doesn't change overtime, it stays stable, swallowed the stream."
        ],
        [
            "So here is the data structure, so that's one entity Stream Joes entity streaming they were.",
            "And the relation stream so extreme as a distribution composed of six clusters which are located quite far away from one another.",
            "All these clusters are straight Collins's.",
            "A little overlapping between the clusters, and as you can see, each cluster is only linked to one cluster of the other string, so that's basically the simplest data structure when can have in this kind of situation."
        ],
        [
            "So the data set is stronger than the relations trim counts.",
            "360 K Lemons with 10 variables an into the streams are slightly smaller."
        ],
        [
            "So first let's have a look at the number of lost elements depending on the bloom filter size.",
            "So as you saw, the element closed when I assigning relations trim elements to the cross table.",
            "So as we can see for large Bloom filter size, Zeus numbers can be kept very low.",
            "And increase slightly.",
            "As filter sizes reduce, however, if we go over a certain threshold, suddenly the number of last element explodes.",
            "Since here we are little less than 5000 last elements and for slightly smaller bloom filters, only this number ways is to spend one and 20,000 last elements, which is most of the relations stream so.",
            "Which means that bloom filter size has to be carefully chosen so as not to be too small.",
            "Otherwise most of the relations trim will be lost within the process.",
            "As friend within 60,360 thousand."
        ],
        [
            "So basically here we are losing nearly all the stream.",
            "I'm sure it's very reasonable loss.",
            "Then security or."
        ],
        [
            "Time bye.",
            "And by using micro clusters as we can see in all instances purities very good since the distribution is stable actually losing a lot of filaments.",
            "Not a problem for the quality of the summary obtained.",
            "The problem is that.",
            "In this is only the case in such a simple data set, since the data set is very simple, the algorithm is capable of dealing with it quite accurately."
        ],
        [
            "Then take a look at basically the opposite case.",
            "That is, we're still in.",
            "A static environment, whereas was distribution doesn't change overtime except this time we will be in a very different variable relational distribution.",
            "And the goal is to see how this affects quality."
        ],
        [
            "Of the Summer Rd thing.",
            "So basically in this case, as always, absolutely no relation between the cluster in which one element is located in an entity stream an the cluster in which relations stream element is located."
        ],
        [
            "So as expected, the result of weather bad but not as bad as we could have feared since basically it was a fully random distribution, would be at zero point 18.",
            "Since there are six clusters.",
            "But it's not fully random, so depending on the number of micro cluster used to summarize each entity stream, there's actually.",
            "A bias in the number of filaments for issued from each micro clusters that come in each micro cluster, which means that some micro cluster will give actual information on.",
            "The data obtained the structure of the data within the relation stream.",
            "This is the purity only for the relation stream.",
            "Since result for stream doesn't change from last example.",
            "As we can see, we have slight augmentation with the number of micro clusters.",
            "We assume that we still have better purity score if we still increase this number of micro clusters.",
            "Further unfortunately this implied quite huge augmentation in computational time and in memory usage for the algorithms and the cross table is much bigger and since the algorithm at.",
            "To spend more time to assign each relation stream element as the number of micro cluster increases."
        ],
        [
            "Oh and aside, actually not the size of the Bloom filter, but the number of hash functions that use.",
            "Not, not, yeah, but not exactly actually.",
            "Depending on the size of the filter and the expected number of elements you want to put in them, you have an optimum number of hash function to use that you that will give you the best error rate.",
            "Depends on the hash function you're using, of course, but yes, it's basically linear in the number of hash function.",
            "Typically for this example I had to use 5 hash function for each filter.",
            "Yes.",
            "Since we have to compute the distance to reach out to each squad, yes.",
            "Both Frozen 30 stream and for the relation stream.",
            "Yes.",
            "And in terms of computation time.",
            "So this we actually, I'm actually finishing testing the algorithm on dynamic data sets, so have not computed the full results yet and I'm confident it them to you.",
            "Uh, no, but still with continue with tackled.",
            "I've presented you two things first and you and very fast method to summarize one single data stream.",
            "Um?",
            "So this method is very efficient and very fast and gives good results for one data stream.",
            "And an oven method that interest in self in summarizing several data streams, showing relational information with one another.",
            "This is a problem that hasn't been much stood up to now at least, so miss is quite new kind of algorithm and you kind of problematics, which is why we've had difficulties with testing because we don't have any reference algorithm to compare ourselves to.",
            "Nor is it easy to build data sets to test this method, or in the case of real data, to assess the quality of the summaries will pain.",
            "So we've had them quite a lot of problem in the testing part.",
            "Or perspective include extending question to deal with more complex relational models, and now we're dealing with stress streams and would like to extend it to star models and eagerly to any kind of relational models such as the one with using for instance at France Telecom for our houses.",
            "And we'd like to integrate both methods to and in twin existing database management data stream management system so that they can be used directly as part of the toolkits present in this in this systems.",
            "Got any question?",
            "So.",
            "Transitive.",
            "Can you repeat the properties that's supposed to preserve?"
        ],
        [
            "I can't.",
            "I don't have theoretical arguments that had wanted that.",
            "However, the way the summary is built want is that basically she got information that's worked together in one stream.",
            "Which is related strongly to the elements that are grouped together.",
            "Unless a stream will have only will have a strong micro clusters appearing in the cross table, so you'll be able to identify that these two groups have strong connection to each other.",
            "Basically what happens, at least in the test we've done so far, is that of course this matrices this matrix is actually very sparse, so.",
            "When you Jackie look at it, you were able to locate.",
            "All the big points that actually corresponds to elements strongly linked with one another in the streams.",
            "I'm actually one of the further one work one to do is do.",
            "Oh, clustering on this table so that directly by looking at the result of the cross clustering, you'll be able to infer this type of properties in the summary."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello so I'm going to present today's work I've been doing and this provision of water by and fabric level which is a follow up to the talk Georgia by gave you yesterday on data streams and I'm going to present summarizing methods for data stream.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some first going to give you a brief summary of.",
                    "label": 0
                },
                {
                    "sent": "Brief remembering of wet, so that has tremendous definition of what's summary of the data stream.",
                    "label": 0
                },
                {
                    "sent": "And while you were trying to build them and then I'm going to prison two methods, one to build a summary of a single data stream and another one to build a summary of several data streams linked by relation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first for those who didn't listen yesterday, what should I stream?",
                    "label": 0
                },
                {
                    "sent": "So here's the definition by Golub, and those are from 2003.",
                    "label": 0
                },
                {
                    "sent": "The test trim is a real time continuous ordered sequence of item and it is impossible to control the order in which the terms arrive, nor is it feasible to locally store streaming St.",
                    "label": 1
                },
                {
                    "sent": "I see, so this definition implies of through consequences for algorithm dealing with data streams.",
                    "label": 0
                },
                {
                    "sent": "The most obvious one is that this algorithm have to be single path.",
                    "label": 0
                },
                {
                    "sent": "Of course those are consequences that they have to use.",
                    "label": 0
                },
                {
                    "sent": "Small amounts of memory and time parliaments since most data streams have very high rates and the usually the algorithm used F to use bounding memories.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then what's the data stream summary?",
                    "label": 0
                },
                {
                    "sent": "Well, is the goal of building a data stream somewhere is to have in small space A summary of the streams content that will be usable for.",
                    "label": 1
                },
                {
                    "sent": "Many different kinds of data mining tasks or valuable evaluation.",
                    "label": 0
                },
                {
                    "sent": "Another goal of the data Stream summary is that this that general summary should be usable to build the summary about any given part of the stream, not necessarily the world stream, but any part the user may want to analyze after the stream's gone.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we wish to build those summaries?",
                    "label": 0
                },
                {
                    "sent": "That's a question that was raised by Francoise yesterday.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, obviously due to the very nature of data streams.",
                    "label": 1
                },
                {
                    "sent": "It's not possible to ask or query stream after it's gone, so once the strings has gone, if you've forgotten to query it for one reason or another, well, you don't have anything you can do.",
                    "label": 0
                },
                {
                    "sent": "And in many cases, especially in industrial environments, in the light of new information of new perspective, one may want to pose new queries in the stream that has gone or to one new analysis made for marketing purposes or.",
                    "label": 0
                },
                {
                    "sent": "For analysis.",
                    "label": 0
                },
                {
                    "sent": "To give answers to these queries, even approximate and says it is necessary to have some kind of summary of the streams contents.",
                    "label": 1
                },
                {
                    "sent": "Then there is a second reason why building summaries of data stream may be interesting.",
                    "label": 0
                },
                {
                    "sent": "As we vote yesterday, in many instances, data streams aren't actually discarded after the gun, but but are stored in large long-term memory structures.",
                    "label": 0
                },
                {
                    "sent": "The problem of the structures is that the data that's stored on them is very difficult to access, and even more difficult to analyze.",
                    "label": 1
                },
                {
                    "sent": "So the process of retrieving the data is very long.",
                    "label": 0
                },
                {
                    "sent": "Very tedious and of course very expensive.",
                    "label": 0
                },
                {
                    "sent": "That's why, even if the data is available, it would be useful to have small, lightweight summaries that can provide approximate answers to any queries that one would need to do on the streams or on the data at little costs in terms of computation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or memory.",
                    "label": 0
                },
                {
                    "sent": "Yesterday Georgia by vote several methods of summer to summarize data streams.",
                    "label": 0
                },
                {
                    "sent": "So those are samples, histograms, sketches, micro clusters.",
                    "label": 1
                },
                {
                    "sent": "All these methods of both the interestings of flows.",
                    "label": 0
                },
                {
                    "sent": "The sampling methods that were presented yesterday worked only on selected Windows or on the world Stream, which goes against one of the principle.",
                    "label": 0
                },
                {
                    "sent": "I announced that we wanted to summarize to be usable for any given Windows in the stream history.",
                    "label": 0
                },
                {
                    "sent": "You've got histograms that suffer the same kind of drawbacks, and Moreover most of the times with histograms are quite expensive to build in terms of computation time.",
                    "label": 0
                },
                {
                    "sent": "If you want them to give large amount of information or otherwise they give two simple data to simple information that don't allow for precise analysis.",
                    "label": 0
                },
                {
                    "sent": "Then you've got sketches.",
                    "label": 0
                },
                {
                    "sent": "Sketches are usually very interesting data structures, but they are usually built for one single task and not with a general purpose in mind.",
                    "label": 0
                },
                {
                    "sent": "So you can use them to insert query after the time, but you can't use them for general purpose analysis.",
                    "label": 0
                },
                {
                    "sent": "And then we've got micro clusters that will vote with the clustering algorithm yesterday.",
                    "label": 0
                },
                {
                    "sent": "It's all quite useful.",
                    "label": 0
                },
                {
                    "sent": "The answer is our general usage and they can be used to query on any single time window on the stream, but they are quite expensive in terms of computation time.",
                    "label": 0
                },
                {
                    "sent": "And in terms of memory usage.",
                    "label": 0
                },
                {
                    "sent": "On the computer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to present you a new method that we've developed Coldstream Samp, and that's based on sampling this time and.",
                    "label": 0
                },
                {
                    "sent": "So that uses the principle of sampling to build a lightweight summary of one single data stream.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In so here's how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So as the data streams goes by, it's sampled at a constant rate Alpha to build samples of a given size TT being fixed at the startup of the algorithm.",
                    "label": 1
                },
                {
                    "sent": "And once a simple is filled and you simply started until it's filled again, then and you some pearly started again.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, if we continue to do that and definitely will get a lot of temples and so will break our bond.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory requirements, so we've got to deal with what to do with the samples that are going to voluminous.",
                    "label": 0
                },
                {
                    "sent": "Susie idea here is once a given threshold has been raised has been reached.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here's especially sweet to work with one swift code, 3 samples.",
                    "label": 0
                },
                {
                    "sent": "We say this is enough and we're going to fuse the two oldest samples to one new sample of a higher order.",
                    "label": 0
                },
                {
                    "sent": "To do that, we wonder, only select half the elements of each sample to build.",
                    "label": 0
                },
                {
                    "sent": "A new sample of a higher order.",
                    "label": 0
                },
                {
                    "sent": "At each order, we still follow the same limits of a maximum number of samples, so that if this maximum order of simple is reached for new order will once again fuse 2 samples together to build a new sample of a higher order.",
                    "label": 0
                },
                {
                    "sent": "So as time goes by, we'll have.",
                    "label": 0
                },
                {
                    "sent": "Higher and higher order of samples, which will represent larger and larger portion of the data stream history so that for recent time will have quite rich, simple and for all the times of the stream the sample will be.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much smaller.",
                    "label": 0
                },
                {
                    "sent": "To constitute a sample on a given portion of the stream, we just have to fuse the sample.",
                    "label": 0
                },
                {
                    "sent": "We have to fit the window the time window where we want to analyze and.",
                    "label": 0
                },
                {
                    "sent": "Each element will receive a weight that will depend on the order of the sample, so the weight is 2 powers.",
                    "label": 1
                },
                {
                    "sent": "The order of the sample, since at each time where dividing sample size by two.",
                    "label": 0
                },
                {
                    "sent": "So this gives older simple a much higher weight than newer ones since they have much fewer elements and this lowers to build for any given window in the stream.",
                    "label": 0
                },
                {
                    "sent": "Uniform sample with a constant sampling rate and sampling rate and just different weights depending on the element.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Since what we build finally is a simple, any given data mining task can be applied to this, weighted to this weighting sample.",
                    "label": 0
                },
                {
                    "sent": "We've chosen for evaluation purpose to test this sample with the clustering task in order to compare it with plus trim that was mostly tested on clustering.",
                    "label": 0
                },
                {
                    "sent": "Travel rates.",
                    "label": 0
                },
                {
                    "sent": "How is the scores with used in track list or inertia?",
                    "label": 0
                },
                {
                    "sent": "And we've tested the algorithm on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Several data sets.",
                    "label": 0
                },
                {
                    "sent": "So the first one is a small artificial datasets that prove this choose to.",
                    "label": 0
                },
                {
                    "sent": "Um, control how this is both algorithms react to brittle changes in the data distribution, so the data set is 100,000 element along with 30 variables following Goshen distribution and the data set is composed of five equal sized cluster.",
                    "label": 0
                },
                {
                    "sent": "Um, the small thing is that one cluster is present throughout all the streams.",
                    "label": 0
                },
                {
                    "sent": "Was two clusters only present in the first half of the stream, and tours are in the second half, so at the middle of the stream the distribution suddenly changes as to cluster disappear from the distribution and junior I arrive.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If so, let's see how both algorithm reacts to this brutal change.",
                    "label": 0
                },
                {
                    "sent": "Since the distribution is quite simple on the first half of the stream, both algorithms perform quite well and have very similar results.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "On the second half of the stream.",
                    "label": 0
                },
                {
                    "sent": "When the distribution suddenly changes, stream some since it uses a simple Bayes approach, doesn't remember anything of the previous history of the stream and so doesn't have a problem adapting to the new distribution.",
                    "label": 0
                },
                {
                    "sent": "Was close trim as we've seen yesterday, I've slowly evolving micro clusters so those micro clusters take some time to adjust to the change in the distribution and this reflects on the stream with a slightly lower score on the second half.",
                    "label": 0
                },
                {
                    "sent": "If we use both algorithm on the whole data stream.",
                    "label": 0
                },
                {
                    "sent": "This deficiency of close trim on the second part also shows in the total result of the algae.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prism.",
                    "label": 0
                },
                {
                    "sent": "The second set for used was the KDD 98, a shapeable donation, that assets which relates to donation information.",
                    "label": 0
                },
                {
                    "sent": "Following the mailing campaign, it's quite a stable that I sets with basically 5 clusters with only selected 20 valuables.",
                    "label": 0
                },
                {
                    "sent": "To limit the number of missing elements.",
                    "label": 0
                },
                {
                    "sent": "Since we weren't interested in cleaning.",
                    "label": 0
                },
                {
                    "sent": "Problems for this test, which resulted in quite small sets of 75,000.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elements.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here again.",
                    "label": 0
                },
                {
                    "sent": "The difference is not really sizable on the small examples since swim sums.",
                    "label": 0
                },
                {
                    "sent": "As a small advantage, but that's not very visible, and this advantage disappears for more recent data, which proves that since both algorithm have more precision for recent data on recent data is that predicts very similar results.",
                    "label": 0
                },
                {
                    "sent": "However, on older data performance a bit better.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we also tested this algorithm them on the kated in 99 Networking PRISM data set, so this is.",
                    "label": 0
                },
                {
                    "sent": "Larger data set of 500,000 elements with 74 variables.",
                    "label": 0
                },
                {
                    "sent": "Which which basically come from 5 different clusters, normal TCP connections and four different types of attacks.",
                    "label": 0
                },
                {
                    "sent": "This is the record of TCP connections to the MIT Lincoln Labs.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, since the data set is much bigger.",
                    "label": 0
                },
                {
                    "sent": "Choose me, it's not 500 thousands, 5 million elements.",
                    "label": 0
                },
                {
                    "sent": "So we've run different analysis depending on the part of the stream that we want to analyze.",
                    "label": 0
                },
                {
                    "sent": "So here's a 1st result for different parts of the stream.",
                    "label": 1
                },
                {
                    "sent": "So here you've got the first.",
                    "label": 0
                },
                {
                    "sent": "The elements from first 1,000,000 to 1 million 500,000 share from 2 million to 2 million 500,000.",
                    "label": 0
                },
                {
                    "sent": "And here from 3 million, 3 million 5.",
                    "label": 0
                },
                {
                    "sent": "You've got three algorithm, stream some peas in red here.",
                    "label": 0
                },
                {
                    "sent": "Close trimming blue and stream and much older.",
                    "label": 0
                },
                {
                    "sent": "Pursuing string clustering algorithm is in yellow.",
                    "label": 0
                },
                {
                    "sent": "So here we can see that on the oldest data stream, some is performing much better.",
                    "label": 0
                },
                {
                    "sent": "Here we've got quite.",
                    "label": 0
                },
                {
                    "sent": "An empty distribution.",
                    "label": 0
                },
                {
                    "sent": "Actually nothing much is happening in this part of the data of the data sets, so both algorithms are performing quite well with very similar results.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Hamstring sample is also performing quite well and on recent data all three algorithms have very comparable results.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, this curve shows performance of both close streaming red and stream.",
                    "label": 0
                },
                {
                    "sent": "Something blue for an increasing number of filaments taken from.",
                    "label": 1
                },
                {
                    "sent": "As the very first elements of the stream with an increasing quantity of the elements with the last point representing an analysis on the whole stream.",
                    "label": 1
                },
                {
                    "sent": "As we can see.",
                    "label": 0
                },
                {
                    "sent": "On the very first elements here, performance are quite similar for both algorithm with over old data and only that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then there's quite a gap forming on this central part of the stream.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If one takes the whole part, this difference slowly gets bigger and bigger.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the same thing, but starting with.",
                    "label": 0
                },
                {
                    "sent": "The last elements received, so the most recent elements here and then an increasing quantity of stream elements being analyzed at each different points.",
                    "label": 1
                },
                {
                    "sent": "So once again, on very recent data performance are quite similar, but if we increase the size of the window.",
                    "label": 0
                },
                {
                    "sent": "Stream sons performance becomes.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "And finally, the main point is of course speed, as we could see with Metro cluster approach, one is to compute alot of ultra distances between the micro cluster to add new elements.",
                    "label": 0
                },
                {
                    "sent": "This is of course very expensive where the sampling approach on its online part is very simple since we're just sampling the elements and the only complex operation is fusing to sample together, which doesn't take much time.",
                    "label": 0
                },
                {
                    "sent": "So here's the difference.",
                    "label": 0
                },
                {
                    "sent": "It's quite huge.",
                    "label": 0
                },
                {
                    "sent": "In terms of processing speed.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This proves that well.",
                    "label": 0
                },
                {
                    "sent": "As was said before, more complex approaches aren't necessarily better approaches.",
                    "label": 0
                },
                {
                    "sent": "Sampling is a very, very simple approach that's been used for quite some time, and it products in this instance very good results.",
                    "label": 1
                },
                {
                    "sent": "At very fast speed, which is always a desirable quality for data stream processing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'm now going to present.",
                    "label": 0
                },
                {
                    "sent": "Cross trim, which is an algorithm for relational stream summaries.",
                    "label": 1
                },
                {
                    "sent": "So I'm first going to explain what I mean by what's relational streams and try to explain why we found this to be an interesting and challenging problem.",
                    "label": 1
                },
                {
                    "sent": "Then I'll present one tool I'm going to use in my algorithm, which is Bloom filters.",
                    "label": 0
                },
                {
                    "sent": "And how is the function?",
                    "label": 0
                },
                {
                    "sent": "And then I'll present the algorithm and a few tests with one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "Oh so first motivation.",
                    "label": 0
                },
                {
                    "sent": "The thing is that while most paper and research interest them self in the treatment of one single data stream as it's happened, data seldom stands on its own in real world application, but is often relational by nature.",
                    "label": 0
                },
                {
                    "sent": "So most streams and most data in general kind of kind of where don't stand on their own.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "Can have stream of clients, a stream of services and then you'll have a stream of service usage.",
                    "label": 0
                },
                {
                    "sent": "For instance telephone calls and this stream will link a client with the user service.",
                    "label": 0
                },
                {
                    "sent": "If one just analyze the service usage stream, one depletes himself of a lot of information that's contained in the link this stream has with those tools or streams or belief is that by being able to build a summary, taking this information into account will be able to give much better results in terms of analysis.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For this data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is quite a complex problem since we're dealing with relational data.",
                    "label": 0
                },
                {
                    "sent": "So for starters we have chosen to address a simplified version of the problem, dealing with only three data streams.",
                    "label": 0
                },
                {
                    "sent": "So we've got two streams of entity, so those teams are constituted by.",
                    "label": 0
                },
                {
                    "sent": "Well, elements containing a number of valuable a timestamp.",
                    "label": 0
                },
                {
                    "sent": "And promote keys that indexes each element so each element has its own unique key.",
                    "label": 0
                },
                {
                    "sent": "And then a stream of relation.",
                    "label": 0
                },
                {
                    "sent": "Each element of the relation as it's all its own, attributes it some time stamp and as a set of keys that references elements from agent to the streams.",
                    "label": 0
                },
                {
                    "sent": "We've made a few further purchases.",
                    "label": 0
                },
                {
                    "sent": "On this frames the first one, and that's quite a big part of this is that audio streams are insertion streams only, so one can't remove and elements of the into the streams when it's passed through.",
                    "label": 0
                },
                {
                    "sent": "And those reporters is is that the stream of relation goes much faster than both entities stream, which is most of the time through in real world applications.",
                    "label": 0
                },
                {
                    "sent": "Another one is that all attributes are numerical, which is more of a convenience things to deal with.",
                    "label": 1
                },
                {
                    "sent": "Is a metrics we're going to use.",
                    "label": 0
                },
                {
                    "sent": "And the last one is that.",
                    "label": 1
                },
                {
                    "sent": "References constraints on never broken or examples that is, you won't find an element in the relation streams that makes reference to elements that have never arrived.",
                    "label": 0
                },
                {
                    "sent": "Inns into the streams.",
                    "label": 0
                },
                {
                    "sent": "Which of course could happen in real world applications, since elements can get lost or can get delayed or whatever depending on operational circumstances.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So given this problem, what so bold exactly?",
                    "label": 0
                },
                {
                    "sent": "It's to build a summary of all the Swiss strings taken individually.",
                    "label": 0
                },
                {
                    "sent": "But also to realize a summary of the information contained in the links, as the Swiss streams share with one another.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to build this, ask Verism with inspired of self of two things we've made use of Bloom filters that I'm going to explain an with inspired our self of the clustering algorithms that was presented yesterday and that makes use of micro clusters and cluster feature vectors.",
                    "label": 1
                },
                {
                    "sent": "Oh, since I've just proven before that micro clusters might not necessarily be the best approach for data stream summaries.",
                    "label": 0
                },
                {
                    "sent": "You might ask yourself why I've decided to use them in that instance and that becausw sampling in this case is not a good approach because of the relational links that.",
                    "label": 0
                },
                {
                    "sent": "The strength share together.",
                    "label": 0
                },
                {
                    "sent": "If you're only dealing with one stream.",
                    "label": 0
                },
                {
                    "sent": "If the stream is massive, missing one element is never a big deal and it won't have large consequences on your distribution.",
                    "label": 0
                },
                {
                    "sent": "However, in a relational context.",
                    "label": 0
                },
                {
                    "sent": "One elements, even if it doesn't play a big role in the distribution of its own stream.",
                    "label": 0
                },
                {
                    "sent": "May play a big role in the relational links that the Swiss streams share together, so it is not feasible to actually skip an element without having tested what his links are with the different streams.",
                    "label": 0
                },
                {
                    "sent": "So that's why we've chosen to go for a micro cluster based approach.",
                    "label": 0
                },
                {
                    "sent": "Um's classroom was explained yesterday.",
                    "label": 0
                },
                {
                    "sent": "I won't go over it again.",
                    "label": 0
                },
                {
                    "sent": "I'll just explain what's a Bloom filter.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Bloom filter is very small memory structure and very fast structure that allows you to basically memorize a set of numbers.",
                    "label": 1
                },
                {
                    "sent": "It supports two simple operation, adding a new number to the filter and testing without number has been learned before or not.",
                    "label": 0
                },
                {
                    "sent": "Both of the open ocean are very fast, which is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why was chosen as a structure for algorithm?",
                    "label": 0
                },
                {
                    "sent": "So here's the basic principle.",
                    "label": 0
                },
                {
                    "sent": "Abloom filter is just a simple banger, binary word, forgiven lens be.",
                    "label": 1
                },
                {
                    "sent": "With associated to it.",
                    "label": 1
                },
                {
                    "sent": "The number of K hashing function that map from R to the size of the filter.",
                    "label": 0
                },
                {
                    "sent": "They have to map, of course uniformly from art.",
                    "label": 0
                },
                {
                    "sent": "Went to build the wise.",
                    "label": 0
                },
                {
                    "sent": "The filter will be biased and won't work properly.",
                    "label": 0
                },
                {
                    "sent": "At initialization, all the bytes are set to 0.",
                    "label": 1
                },
                {
                    "sent": "When the new elements arrived and we want to learn that elements, it is asked by the key function which will return Kane.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Between one and bills or sector equals two, so we've got touring.",
                    "label": 0
                },
                {
                    "sent": "This is that I've been selected and Zeus indices are set to one in the Bloom filter, so this is very simple operation and basically what takes the most time is computing the hash functions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To test a new element.",
                    "label": 1
                },
                {
                    "sent": "Once again, the new element is asked by the different hash functions, which gives several indices.",
                    "label": 0
                },
                {
                    "sent": "Then one compares zoos indices with one set at one in the Bloom filter.",
                    "label": 0
                },
                {
                    "sent": "If all the indices that have been selected by the hash functions are set at one in the Bloom filter, then with high probability this element has been learned before by the Bloom filter.",
                    "label": 1
                },
                {
                    "sent": "If any of the elements any of these indices are set to zero in the Bloom filter, then we are certain that the element hasn't been learned before.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So both of these operations also very fast and thus very interesting properties to Bloom filter, and the fact that these filters are additives.",
                    "label": 0
                },
                {
                    "sent": "So basically, if I have two filters that have learned two different set of numbers.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just by doing a logical or between the two filters.",
                    "label": 0
                },
                {
                    "sent": "I wrote a new filter that will have learned the unions of the two previously learnset, which with a property that we're going to exploit later on as we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll see.",
                    "label": 0
                },
                {
                    "sent": "So I'm now going to present.",
                    "label": 0
                },
                {
                    "sent": "My method so far.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, small overview of the structure of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're dealing with Swiss dreams.",
                    "label": 0
                },
                {
                    "sent": "20 streams here that are going to be summarized in quite a similar fashion as the one used in the clustering algorithm by a set of micro clusters, except that this clusters will also be attached with a bloom filter for each micro cluster.",
                    "label": 0
                },
                {
                    "sent": "And summarize other relations trim.",
                    "label": 0
                },
                {
                    "sent": "Will use across table that will contain cluster features vector.",
                    "label": 0
                },
                {
                    "sent": "In each of its.",
                    "label": 0
                },
                {
                    "sent": "Cases.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happens when we receive a new entity stream element so?",
                    "label": 0
                },
                {
                    "sent": "As the element arrive.",
                    "label": 0
                },
                {
                    "sent": "We locate the closest micro cluster to this elements using a distance function appropriate to the data we are dealing with.",
                    "label": 0
                },
                {
                    "sent": "We test whether this element lies within an acceptable range of the micro cluster, for instance by using the radius of the micro cluster.",
                    "label": 0
                },
                {
                    "sent": "If it is so, we update the cluster feature vector information of the micro cluster.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We learn the key of the element with the Bloom filter attached to the micro cluster.",
                    "label": 1
                },
                {
                    "sent": "If the element is not admitted, we create a new micro cluster with this element as its seeds and for that.",
                    "label": 0
                },
                {
                    "sent": "Since we're using the fixed amount of micro cluster, we have to make form for this new micro cluster.",
                    "label": 0
                },
                {
                    "sent": "In order not to break relational constraints between the three, streams were not allowed to actually deley delete any micro clusters, so our remaining solution is to fuse two of them of the existing micro clusters.",
                    "label": 0
                },
                {
                    "sent": "So we locate the two micro cluster which are close to each other and you refuse to fuse them together.",
                    "label": 1
                },
                {
                    "sent": "This is possible since both the cluster feature vector.",
                    "label": 0
                },
                {
                    "sent": "And the Bloom filter have additive properties.",
                    "label": 0
                },
                {
                    "sent": "So with a simple operation we confuse to micro cluster together.",
                    "label": 0
                },
                {
                    "sent": "Propane obtain a new.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When an element survives in the relations trim.",
                    "label": 0
                },
                {
                    "sent": "The method is a bit different this time.",
                    "label": 0
                },
                {
                    "sent": "We'll take first the key, referring to the elements in the first entity stream.",
                    "label": 1
                },
                {
                    "sent": "For instance, the entity stream.",
                    "label": 0
                },
                {
                    "sent": "And then we'll check whether this key has been learned by any of the bloom filters attached towards micro clusters.",
                    "label": 1
                },
                {
                    "sent": "Given the properties of the Bloom filter with wanted that will find at least one of them that will have learned this key before.",
                    "label": 0
                },
                {
                    "sent": "Since bioprocesses elements in the relation stream can break relational constraints.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we locate one or several clusters.",
                    "label": 0
                },
                {
                    "sent": "Will locate at least one cluster.",
                    "label": 0
                },
                {
                    "sent": "Having learns this elements.",
                    "label": 0
                },
                {
                    "sent": "The problem is that bloom filters being at approximate indexing solution collision may occur within a bloom filters and so it may happen that several bloom filters will actually recognize this element as having been learned.",
                    "label": 0
                },
                {
                    "sent": "Use the bloom filter or appropriately sized.",
                    "label": 0
                },
                {
                    "sent": "This shouldn't happen too often, so we've decided that if we haven't found a unique temple of indices were going to drop this elements since collision occur at random within the bloom filters.",
                    "label": 1
                },
                {
                    "sent": "Normally this shouldn't happen.",
                    "label": 0
                },
                {
                    "sent": "Alter the distribution of the world set.",
                    "label": 0
                },
                {
                    "sent": "The limit actually.",
                    "label": 0
                },
                {
                    "sent": "I've not put in the slide you have actually.",
                    "label": 0
                },
                {
                    "sent": "Just says it.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, you've got to grow and T on the other way.",
                    "label": 0
                },
                {
                    "sent": "Depending on the number of hash function and on the size of the bloom filters that allows you for a given number of land elements gives you another wait so.",
                    "label": 0
                },
                {
                    "sent": "Yes, to appropriately sized them.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We'll see in the testing how the Bloom filter size actually alters the result we get on this problem.",
                    "label": 1
                },
                {
                    "sent": "So if the couple is tonight, we put we update the safety information.",
                    "label": 1
                },
                {
                    "sent": "Referring to this in this in the Civi cross table.",
                    "label": 0
                },
                {
                    "sent": "Which will record information concerning that dilemma.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Action.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm warranted because of the.",
                    "label": 0
                },
                {
                    "sent": "Of the several properties that.",
                    "label": 0
                },
                {
                    "sent": "It has to have been seen before.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a small summary of the structure of the algorithm, so we've got here.",
                    "label": 0
                },
                {
                    "sent": "As in Michael clusters summarizing the entity streams and here.",
                    "label": 0
                },
                {
                    "sent": "Uh, so stevey crosstable, summarizing the relations stream.",
                    "label": 0
                },
                {
                    "sent": "So when a new element in the relation stream arrive, we locate the element is the micro clusters.",
                    "label": 0
                },
                {
                    "sent": "That contain Xelement made reference to.",
                    "label": 0
                },
                {
                    "sent": "And we update the CV corresponding to the cross of this elements.",
                    "label": 0
                },
                {
                    "sent": "So in this way we actually remember information concerning the links this elements share with.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanna know them?",
                    "label": 0
                },
                {
                    "sent": "So up to now this is AM.",
                    "label": 0
                },
                {
                    "sent": "Um, an algorithm is that actually deals only with the world stream.",
                    "label": 0
                },
                {
                    "sent": "It should evolve with detection distribution, but you can't actually use it on selected part of the stream.",
                    "label": 0
                },
                {
                    "sent": "So just as with plus trim, we've chosen to use.",
                    "label": 0
                },
                {
                    "sent": "A snapshot system that will take images of the state of the algorithm at previously selected.",
                    "label": 0
                },
                {
                    "sent": "A system Clock times.",
                    "label": 0
                },
                {
                    "sent": "Following um.",
                    "label": 0
                },
                {
                    "sent": "Oh would you metric distribution in two in powers of two?",
                    "label": 0
                },
                {
                    "sent": "At each snapshot, we only keep the CR, V, and idealists of which micro clusters summarizing's into this stream, and we keep all this.",
                    "label": 0
                },
                {
                    "sent": "If we cross table.",
                    "label": 0
                },
                {
                    "sent": "We don't have to keep the bloom filters since in the summaries are not useful anymore since we're not going to add new elements to old summaries.",
                    "label": 0
                },
                {
                    "sent": "Given the properties of the CSV that they can be added and subtracted from one another, we can.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Subtract an old.",
                    "label": 0
                },
                {
                    "sent": "Snapshot from a newer one to recreate what the state of the algorithm would be if it had only been applied to a selected time window.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm now going to present our first tests.",
                    "label": 0
                },
                {
                    "sent": "So far we've only made test on artificial data, though we've obtained quite a large data set from France Telecom, but we haven't run it through the algorithm yet.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to present.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two series of tests.",
                    "label": 0
                },
                {
                    "sent": "Who evaluates?",
                    "label": 0
                },
                {
                    "sent": "The principle of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In order to evaluate the performance, use the notion of micro cluster purity.",
                    "label": 1
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "The data will be produced by a distribution containing several clusters, which I'll refer to as micro clusters was not to confuse them with the micro clusters.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And we'll evaluate the purity of micro cluster by saying.",
                    "label": 1
                },
                {
                    "sent": "What's the quantity of elements that come from a given micro cluster?",
                    "label": 0
                },
                {
                    "sent": "Our ideal micro cluster will have elements coming only from one given macro cluster, which means it will have emotionless.",
                    "label": 0
                },
                {
                    "sent": "Elements in the cluster.",
                    "label": 0
                },
                {
                    "sent": "Soter evaluate.",
                    "label": 0
                },
                {
                    "sent": "The quality of Entity Stream summary will compute the Purity Falls and micro clusters summarizing when into this stream and for the relation stream will compute security for oils service in the crust.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Able.",
                    "label": 0
                },
                {
                    "sent": "So first we've taken a very simple data sets to evaluate the impact of Bloom filter size on the quality of the summaries obtained.",
                    "label": 1
                },
                {
                    "sent": "So there's a relational distribution in this data set will be very simple.",
                    "label": 0
                },
                {
                    "sent": "The data set is not dinner diner, making that the distribution doesn't change overtime, it stays stable, swallowed the stream.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the data structure, so that's one entity Stream Joes entity streaming they were.",
                    "label": 0
                },
                {
                    "sent": "And the relation stream so extreme as a distribution composed of six clusters which are located quite far away from one another.",
                    "label": 0
                },
                {
                    "sent": "All these clusters are straight Collins's.",
                    "label": 0
                },
                {
                    "sent": "A little overlapping between the clusters, and as you can see, each cluster is only linked to one cluster of the other string, so that's basically the simplest data structure when can have in this kind of situation.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the data set is stronger than the relations trim counts.",
                    "label": 0
                },
                {
                    "sent": "360 K Lemons with 10 variables an into the streams are slightly smaller.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let's have a look at the number of lost elements depending on the bloom filter size.",
                    "label": 1
                },
                {
                    "sent": "So as you saw, the element closed when I assigning relations trim elements to the cross table.",
                    "label": 0
                },
                {
                    "sent": "So as we can see for large Bloom filter size, Zeus numbers can be kept very low.",
                    "label": 0
                },
                {
                    "sent": "And increase slightly.",
                    "label": 0
                },
                {
                    "sent": "As filter sizes reduce, however, if we go over a certain threshold, suddenly the number of last element explodes.",
                    "label": 0
                },
                {
                    "sent": "Since here we are little less than 5000 last elements and for slightly smaller bloom filters, only this number ways is to spend one and 20,000 last elements, which is most of the relations stream so.",
                    "label": 0
                },
                {
                    "sent": "Which means that bloom filter size has to be carefully chosen so as not to be too small.",
                    "label": 0
                },
                {
                    "sent": "Otherwise most of the relations trim will be lost within the process.",
                    "label": 0
                },
                {
                    "sent": "As friend within 60,360 thousand.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically here we are losing nearly all the stream.",
                    "label": 0
                },
                {
                    "sent": "I'm sure it's very reasonable loss.",
                    "label": 0
                },
                {
                    "sent": "Then security or.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time bye.",
                    "label": 0
                },
                {
                    "sent": "And by using micro clusters as we can see in all instances purities very good since the distribution is stable actually losing a lot of filaments.",
                    "label": 0
                },
                {
                    "sent": "Not a problem for the quality of the summary obtained.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 0
                },
                {
                    "sent": "In this is only the case in such a simple data set, since the data set is very simple, the algorithm is capable of dealing with it quite accurately.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then take a look at basically the opposite case.",
                    "label": 0
                },
                {
                    "sent": "That is, we're still in.",
                    "label": 0
                },
                {
                    "sent": "A static environment, whereas was distribution doesn't change overtime except this time we will be in a very different variable relational distribution.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to see how this affects quality.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the Summer Rd thing.",
                    "label": 0
                },
                {
                    "sent": "So basically in this case, as always, absolutely no relation between the cluster in which one element is located in an entity stream an the cluster in which relations stream element is located.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as expected, the result of weather bad but not as bad as we could have feared since basically it was a fully random distribution, would be at zero point 18.",
                    "label": 0
                },
                {
                    "sent": "Since there are six clusters.",
                    "label": 0
                },
                {
                    "sent": "But it's not fully random, so depending on the number of micro cluster used to summarize each entity stream, there's actually.",
                    "label": 0
                },
                {
                    "sent": "A bias in the number of filaments for issued from each micro clusters that come in each micro cluster, which means that some micro cluster will give actual information on.",
                    "label": 0
                },
                {
                    "sent": "The data obtained the structure of the data within the relation stream.",
                    "label": 0
                },
                {
                    "sent": "This is the purity only for the relation stream.",
                    "label": 0
                },
                {
                    "sent": "Since result for stream doesn't change from last example.",
                    "label": 0
                },
                {
                    "sent": "As we can see, we have slight augmentation with the number of micro clusters.",
                    "label": 0
                },
                {
                    "sent": "We assume that we still have better purity score if we still increase this number of micro clusters.",
                    "label": 1
                },
                {
                    "sent": "Further unfortunately this implied quite huge augmentation in computational time and in memory usage for the algorithms and the cross table is much bigger and since the algorithm at.",
                    "label": 0
                },
                {
                    "sent": "To spend more time to assign each relation stream element as the number of micro cluster increases.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh and aside, actually not the size of the Bloom filter, but the number of hash functions that use.",
                    "label": 0
                },
                {
                    "sent": "Not, not, yeah, but not exactly actually.",
                    "label": 0
                },
                {
                    "sent": "Depending on the size of the filter and the expected number of elements you want to put in them, you have an optimum number of hash function to use that you that will give you the best error rate.",
                    "label": 0
                },
                {
                    "sent": "Depends on the hash function you're using, of course, but yes, it's basically linear in the number of hash function.",
                    "label": 0
                },
                {
                    "sent": "Typically for this example I had to use 5 hash function for each filter.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Since we have to compute the distance to reach out to each squad, yes.",
                    "label": 0
                },
                {
                    "sent": "Both Frozen 30 stream and for the relation stream.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And in terms of computation time.",
                    "label": 0
                },
                {
                    "sent": "So this we actually, I'm actually finishing testing the algorithm on dynamic data sets, so have not computed the full results yet and I'm confident it them to you.",
                    "label": 0
                },
                {
                    "sent": "Uh, no, but still with continue with tackled.",
                    "label": 0
                },
                {
                    "sent": "I've presented you two things first and you and very fast method to summarize one single data stream.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this method is very efficient and very fast and gives good results for one data stream.",
                    "label": 0
                },
                {
                    "sent": "And an oven method that interest in self in summarizing several data streams, showing relational information with one another.",
                    "label": 1
                },
                {
                    "sent": "This is a problem that hasn't been much stood up to now at least, so miss is quite new kind of algorithm and you kind of problematics, which is why we've had difficulties with testing because we don't have any reference algorithm to compare ourselves to.",
                    "label": 0
                },
                {
                    "sent": "Nor is it easy to build data sets to test this method, or in the case of real data, to assess the quality of the summaries will pain.",
                    "label": 0
                },
                {
                    "sent": "So we've had them quite a lot of problem in the testing part.",
                    "label": 0
                },
                {
                    "sent": "Or perspective include extending question to deal with more complex relational models, and now we're dealing with stress streams and would like to extend it to star models and eagerly to any kind of relational models such as the one with using for instance at France Telecom for our houses.",
                    "label": 0
                },
                {
                    "sent": "And we'd like to integrate both methods to and in twin existing database management data stream management system so that they can be used directly as part of the toolkits present in this in this systems.",
                    "label": 0
                },
                {
                    "sent": "Got any question?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Transitive.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat the properties that's supposed to preserve?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can't.",
                    "label": 0
                },
                {
                    "sent": "I don't have theoretical arguments that had wanted that.",
                    "label": 0
                },
                {
                    "sent": "However, the way the summary is built want is that basically she got information that's worked together in one stream.",
                    "label": 0
                },
                {
                    "sent": "Which is related strongly to the elements that are grouped together.",
                    "label": 0
                },
                {
                    "sent": "Unless a stream will have only will have a strong micro clusters appearing in the cross table, so you'll be able to identify that these two groups have strong connection to each other.",
                    "label": 0
                },
                {
                    "sent": "Basically what happens, at least in the test we've done so far, is that of course this matrices this matrix is actually very sparse, so.",
                    "label": 0
                },
                {
                    "sent": "When you Jackie look at it, you were able to locate.",
                    "label": 0
                },
                {
                    "sent": "All the big points that actually corresponds to elements strongly linked with one another in the streams.",
                    "label": 0
                },
                {
                    "sent": "I'm actually one of the further one work one to do is do.",
                    "label": 0
                },
                {
                    "sent": "Oh, clustering on this table so that directly by looking at the result of the cross clustering, you'll be able to infer this type of properties in the summary.",
                    "label": 0
                }
            ]
        }
    }
}