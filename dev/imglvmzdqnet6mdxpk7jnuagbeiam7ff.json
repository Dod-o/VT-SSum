{
    "id": "imglvmzdqnet6mdxpk7jnuagbeiam7ff",
    "title": "Mixed Regression: Minimax Optimal Rates",
    "info": {
        "author": [
            "Constantine Caramanis, Department of Electrical and Computer Engineering, University of Texas at Austin"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_caramanis_rates/",
    "segmentation": [
        [
            "This is joint work with you don't Chen who's at Berkeley, and he's actually here in the audience and qinyang you, who's who's a student at Utah Austin?",
            "Um?"
        ],
        [
            "So very simple problem, but that we know quite well is a simple regression problem.",
            "I said simple because we understand both sides of it.",
            "We understand the statistical aspects, how error goes down as we get more samples, piece the dimension and his number of samples.",
            "And also we understand how to optimally compute this.",
            "So we understand both facets of the problem very well, and in fact it's so simple that we understand very well numerous, very interesting extensions of it that have been the subject of work.",
            "Quite a bit of work over the last decade, so we under."
        ],
        [
            "And the setting when the when we want to recover is sparse.",
            "We understand the matrix setting where we want to recover something that's low rank and various extensions low rank plus sparse etc.",
            "So the problem of mixed regression, which is which is the topic here is when we have a different kind of superposition, not something like low rank, plus sparse.",
            "But when every sample we get comes from either beta one or beta two and we and we don't know which one.",
            "Another reason I put up the slide is that these these extensions that many people here have been working on.",
            "One common feature that they have is is that either the either the condition itself like L + S is the truth.",
            "That condition is either exactly checkable by a convex operation, namely addition, or we have a proxy which is approximately, which is which is also checkable via convex optimization, such as a matrix being low rank.",
            "So the point is that if you have candidate solutions in order to check whether they are correct, you just need to add L + S and see if it's equal to see if it's equal, and that's the basis for essentially this family of convex optimization approaches that have led to our live in large part led to our great understanding of these problems, statistical and computational.",
            "So let me contrast that with the mixture problem.",
            "This is if I give you beta one and beta two it's.",
            "Again, it's simple to check, you just go through every single constraint that you have.",
            "So every single sample and you see does beta one work, or does beta two work?",
            "And if you seem to get low air on all of them, then that's correct.",
            "But this is this is an or operation and it's not a convex operation, so there's sort of this fundamental roadblock to plugging it into the usual to the usual machinery, namely convex optimization, which has really been at the core of a lot of the results that have given us.",
            "Our understanding of optimal computation and also optimal statistical rates."
        ],
        [
            "OK so again the stated explicitly the problem I consider is the following.",
            "You get responses why I and they either come from X beta one or X beta two.",
            "So ZI is just a selector, so this is this is the simple mixture problem and I'm only considering 2.",
            "To a mixture of two things.",
            "So to stick with sort of a theme that that was in Mike Stockon has been in several other talks.",
            "Yeah, sure.",
            "So X comes from the same same model.",
            "Effects comes from a different model, so yeah, so you get to see X and you get to see why.",
            "The more difference there are an ex is that that's not covered here, but perhaps or other things that you could do because you have some information here you have no information about.",
            "That would indicate what the true labels are."
        ],
        [
            "So if we don't care about computational complexity, a theme which is true here is that problems are often easier.",
            "And here again I could look at brute force, all possible subsets and do something.",
            "Also, if we don't care bout sample complexity, then again in many cases, including this one, we know what to do.",
            "But but here of course we care about we care about both.",
            "So."
        ],
        [
            "The first of all the exact solution is hard, and there's a simple reduction there.",
            "Classically, what's done, but there are no guarantees that I'm aware of in the noisy setting is an alternating alternating maximization approach, so recently just last year there was.",
            "There was a paper by Chaganti and gang.",
            "That basically uses the tensor approach to solve this problem, But this needs order wise.",
            "Many more samples if the dimension is P, we'd like to do this and end samples, but they need sort of Peter the 4th to Peter the."
        ],
        [
            "Examples.",
            "So here's our approach is is giving a convex formulation that avoids going to higher order tensors and our basic results are as follows.",
            "We get minimax optimal rates when the noise E is either arbitrary or it's sub Gaussian."
        ],
        [
            "And well, you can see more details at the at the poster, but these are.",
            "We look at basically not higher order tensors.",
            "We just look at what you might call it 2 tensor space and."
        ],
        [
            "Key results are which are interesting I think, because if something unexpected are as follows and the arbitrary noise case, the minimax error rate that you get is exactly the same as what you would expect in a standard regression.",
            "So it's you pay no, no penalty.",
            "Essentially for having this mixture now in the stochastic case, or basically the noise is Gaussian, you'd like to do better.",
            "You don't want to have this norm of the noise which has which scales with the dimension and the numerator, and indeed you don't get that.",
            "But you have actually changing rates depending on SNR, so this is not a feature that you see in ordinary regression.",
            "So in ordinary regression, if you have more noise while that affects your error rate, but just in terms of the Sigma, so this scaling term on the Left Sigma is the variance of your noise, P is your dimension, and as a number of samples you get what you see on the left is what you would expect and standard regression.",
            "But it turns out that in the low SNR rate which you can show and what's minimax optimal.",
            "Um?",
            "So our convex optimization has this rate and also we have information theoretic lower bound.",
            "Showing this is that actually this rate changes.",
            "So as you go from high SNR to Lois and R there is an interpolation between between those two rates.",
            "So please come to the poster for more, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with you don't Chen who's at Berkeley, and he's actually here in the audience and qinyang you, who's who's a student at Utah Austin?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very simple problem, but that we know quite well is a simple regression problem.",
                    "label": 1
                },
                {
                    "sent": "I said simple because we understand both sides of it.",
                    "label": 0
                },
                {
                    "sent": "We understand the statistical aspects, how error goes down as we get more samples, piece the dimension and his number of samples.",
                    "label": 0
                },
                {
                    "sent": "And also we understand how to optimally compute this.",
                    "label": 0
                },
                {
                    "sent": "So we understand both facets of the problem very well, and in fact it's so simple that we understand very well numerous, very interesting extensions of it that have been the subject of work.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit of work over the last decade, so we under.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the setting when the when we want to recover is sparse.",
                    "label": 0
                },
                {
                    "sent": "We understand the matrix setting where we want to recover something that's low rank and various extensions low rank plus sparse etc.",
                    "label": 1
                },
                {
                    "sent": "So the problem of mixed regression, which is which is the topic here is when we have a different kind of superposition, not something like low rank, plus sparse.",
                    "label": 1
                },
                {
                    "sent": "But when every sample we get comes from either beta one or beta two and we and we don't know which one.",
                    "label": 0
                },
                {
                    "sent": "Another reason I put up the slide is that these these extensions that many people here have been working on.",
                    "label": 0
                },
                {
                    "sent": "One common feature that they have is is that either the either the condition itself like L + S is the truth.",
                    "label": 0
                },
                {
                    "sent": "That condition is either exactly checkable by a convex operation, namely addition, or we have a proxy which is approximately, which is which is also checkable via convex optimization, such as a matrix being low rank.",
                    "label": 0
                },
                {
                    "sent": "So the point is that if you have candidate solutions in order to check whether they are correct, you just need to add L + S and see if it's equal to see if it's equal, and that's the basis for essentially this family of convex optimization approaches that have led to our live in large part led to our great understanding of these problems, statistical and computational.",
                    "label": 0
                },
                {
                    "sent": "So let me contrast that with the mixture problem.",
                    "label": 0
                },
                {
                    "sent": "This is if I give you beta one and beta two it's.",
                    "label": 0
                },
                {
                    "sent": "Again, it's simple to check, you just go through every single constraint that you have.",
                    "label": 0
                },
                {
                    "sent": "So every single sample and you see does beta one work, or does beta two work?",
                    "label": 0
                },
                {
                    "sent": "And if you seem to get low air on all of them, then that's correct.",
                    "label": 0
                },
                {
                    "sent": "But this is this is an or operation and it's not a convex operation, so there's sort of this fundamental roadblock to plugging it into the usual to the usual machinery, namely convex optimization, which has really been at the core of a lot of the results that have given us.",
                    "label": 0
                },
                {
                    "sent": "Our understanding of optimal computation and also optimal statistical rates.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so again the stated explicitly the problem I consider is the following.",
                    "label": 0
                },
                {
                    "sent": "You get responses why I and they either come from X beta one or X beta two.",
                    "label": 0
                },
                {
                    "sent": "So ZI is just a selector, so this is this is the simple mixture problem and I'm only considering 2.",
                    "label": 1
                },
                {
                    "sent": "To a mixture of two things.",
                    "label": 1
                },
                {
                    "sent": "So to stick with sort of a theme that that was in Mike Stockon has been in several other talks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "So X comes from the same same model.",
                    "label": 0
                },
                {
                    "sent": "Effects comes from a different model, so yeah, so you get to see X and you get to see why.",
                    "label": 0
                },
                {
                    "sent": "The more difference there are an ex is that that's not covered here, but perhaps or other things that you could do because you have some information here you have no information about.",
                    "label": 0
                },
                {
                    "sent": "That would indicate what the true labels are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we don't care about computational complexity, a theme which is true here is that problems are often easier.",
                    "label": 1
                },
                {
                    "sent": "And here again I could look at brute force, all possible subsets and do something.",
                    "label": 1
                },
                {
                    "sent": "Also, if we don't care bout sample complexity, then again in many cases, including this one, we know what to do.",
                    "label": 1
                },
                {
                    "sent": "But but here of course we care about we care about both.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first of all the exact solution is hard, and there's a simple reduction there.",
                    "label": 1
                },
                {
                    "sent": "Classically, what's done, but there are no guarantees that I'm aware of in the noisy setting is an alternating alternating maximization approach, so recently just last year there was.",
                    "label": 0
                },
                {
                    "sent": "There was a paper by Chaganti and gang.",
                    "label": 1
                },
                {
                    "sent": "That basically uses the tensor approach to solve this problem, But this needs order wise.",
                    "label": 0
                },
                {
                    "sent": "Many more samples if the dimension is P, we'd like to do this and end samples, but they need sort of Peter the 4th to Peter the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "So here's our approach is is giving a convex formulation that avoids going to higher order tensors and our basic results are as follows.",
                    "label": 1
                },
                {
                    "sent": "We get minimax optimal rates when the noise E is either arbitrary or it's sub Gaussian.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well, you can see more details at the at the poster, but these are.",
                    "label": 0
                },
                {
                    "sent": "We look at basically not higher order tensors.",
                    "label": 0
                },
                {
                    "sent": "We just look at what you might call it 2 tensor space and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Key results are which are interesting I think, because if something unexpected are as follows and the arbitrary noise case, the minimax error rate that you get is exactly the same as what you would expect in a standard regression.",
                    "label": 1
                },
                {
                    "sent": "So it's you pay no, no penalty.",
                    "label": 0
                },
                {
                    "sent": "Essentially for having this mixture now in the stochastic case, or basically the noise is Gaussian, you'd like to do better.",
                    "label": 0
                },
                {
                    "sent": "You don't want to have this norm of the noise which has which scales with the dimension and the numerator, and indeed you don't get that.",
                    "label": 0
                },
                {
                    "sent": "But you have actually changing rates depending on SNR, so this is not a feature that you see in ordinary regression.",
                    "label": 0
                },
                {
                    "sent": "So in ordinary regression, if you have more noise while that affects your error rate, but just in terms of the Sigma, so this scaling term on the Left Sigma is the variance of your noise, P is your dimension, and as a number of samples you get what you see on the left is what you would expect and standard regression.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that in the low SNR rate which you can show and what's minimax optimal.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So our convex optimization has this rate and also we have information theoretic lower bound.",
                    "label": 0
                },
                {
                    "sent": "Showing this is that actually this rate changes.",
                    "label": 0
                },
                {
                    "sent": "So as you go from high SNR to Lois and R there is an interpolation between between those two rates.",
                    "label": 0
                },
                {
                    "sent": "So please come to the poster for more, thanks.",
                    "label": 0
                }
            ]
        }
    }
}