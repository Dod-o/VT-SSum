{
    "id": "znhau4ytq3mtebvo4b3beommildjouok",
    "title": "Flexible Priors for Exemplar-based Clustering",
    "info": {
        "author": [
            "Daniel Tarlow, Department of Computer Science, University of Toronto"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/uai08_tarlow_fp/",
    "segmentation": [
        [
            "Unlike the previous two speakers, I'm not going to talk about topic models, but instead I'm going to focus on clustering, and in particular I'm going to talk about how to formulate a clustering model in a natural way that lets you express the clustering problem in the way that you want to.",
            "So the topic is going to be flexible.",
            "Priors.",
            "For example, are based clustering and it's with my advisor, Richard Zemel, and Brendan Frey."
        ],
        [
            "So I want to start off at a high level by talking about why we cluster data to begin with, and so if you have data that looks like this, you might be interested just on the left side in representing the density of the data points of your data.",
            "A second reason that you might want to cluster is that you actually believe that there are underlying classes in the data, and these are two slightly different thinking about the problem in these two slightly different ways is going to give you a different way of formulating the clustering problem."
        ],
        [
            "And so, in particular, we're going to talk about the second example in this work."
        ],
        [
            "So just I want to start off by convincing you that all of the clustering algorithms that you know and love might not be as powerful and as expressive as you might think they would be.",
            "So in particular, I want to think about a problem where if you have a personal photo collection and you're interested in clustering the faces that you find in that photo collection.",
            "So the first thing to notice about this problem is that when you have data points that are complex entities such as images.",
            "I just embedding them in a Gaussian space or in a Euclidean space.",
            "Probably isn't going to be as expressive as you'd like to be.",
            "In particular, if you are interested in clustering while maintaining translation invariants, for example, it's much easier to just think about a similarity measure between between two of your objects, and that will allow you to.",
            "Come up with more complex but also more natural ways of representing how your clustering problem should be formulated."
        ],
        [
            "The second thing that I want you to notice is that.",
            "If your photo collections are anything like mine, for example, I have a lot of photos of certain people, so if, for example, my coauthors I spend a lot of time with them and as a result I'm going to have a lot of pictures of them in my photo collection while."
        ],
        [
            "And in contrast, there's also going to be a large number of people that I see very infrequently in my photo collection, so these are my graduate student friends.",
            "More seriously, you could imagine that in a photo collection you have your friends and family that you see often, but then there's also going to be a decay of how often people occur, and you'll have images where you just capture a random stranger in the background."
        ],
        [
            "Example.",
            "So these are the two ideas that motivate this work.",
            "The first is that we want to be able to express more complex measures between our data points.",
            "So in particular similarity measures possibly aren't even metric.",
            "And then second we want to instead of having to specify some arbitrary model selection parameters such as specifying K or choosing a cost for creating each cluster, we want to express this information in a more natural way."
        ],
        [
            "So I'm going to talk about the building blocks that we've chosen to work with."
        ],
        [
            "And.",
            "In particular there are there other choices of how you may choose to do this, But this is the one that we're going to focus on, so we're going to work in an exemplar based clustering framework.",
            "The idea of an exemplar based clustering method is that instead of trying to estimate latent continuous parameters, we can instead just try to find a data point that is representative of each cluster and the benefit of doing this is that you no longer have to estimate any latent parameters, and instead it's just a combinatorial problem of choosing the exemplar for each data point."
        ],
        [
            "So the comment Orks are a little bit tricky, but they can also be dealt with efficiently, so if you haven't seen the affinity propagation algorithm, this is what you get just by applying Max product belief propagation to the exemplar based clustering problem, and you can see an example intuitively of how it works is that the data points past messages to each other, and the goal is for them to reach an agreement on which the exemplars are.",
            "And so you see at the bottom right upon convergence.",
            "Each of the clusters agree on who their exemplar should be.",
            "Now, one of the drawbacks of this is that to choose how many clusters you have in the data, you have to set a parameter which they call the preference, and it's not a very intuitive parameter to set and it doesn't have a natural a justification for what it means by choosing a specif."
        ],
        [
            "Value.",
            "So instead what we think is a more natural way to specify the model selection aspect of the problem is to deal with distributions over cluster sizes.",
            "So given a particular application, you might have a rough idea about how big the clusters should be.",
            "So in particular in the photo collection application, you might be able to say that there are going to be a small number of or they're going to be a large number of very small clusters.",
            "These represent the strangers or the people you see infrequently.",
            "And then there's also going to be the close friends and family that corresponds to this heavier tail.",
            "That you see here.",
            "And so this is a more general form of specifying the model selection problem than Dursley process priors, for example.",
            "Or the Pitman yor process, in that you don't have to maintain the statistical properties of the Dursley process, for example.",
            "And this can be used in place of specifying."
        ],
        [
            "For example.",
            "OK, so just to make this blatantly clear, there are other works that are able to take into account parts of this, so in particular we're looking at automatic model selection.",
            "Being able to specify these flexible priors and being able to express the similarity measures that are more complex and so, for example, Dursley process mixture models can take into account the priors, but they don't deal well with the.",
            "Nonmetric or."
        ],
        [
            "Complicated likelihoods.",
            "So in order to explain how this works, I'm going to start by explaining a more specific model than we're able to handle, but one that gets across the point pretty well, and So what we're going to do is we're going to use this exemplar based setting, and we're going to develop a Dursley process mixture MoD."
        ],
        [
            "So just to get the notation set, there's going to be three sets of variables for each point we're going to maintain which cluster it's assigned to, so this is, this is just a representation of a partition.",
            "So for example, if C1C2C and C5 are given the same #7, they belong to Cluster 7 and that just says that they belong to the same class.",
            "You could instead give them label 12 and it would be exactly the same.",
            "Partition.",
            "On top of that, we're going to represent, for each point, just a binary variable saying whether it's an example or not.",
            "And then we're also going to obviously represent the parameter vectors describing each point."
        ],
        [
            "The X is.",
            "So the generative model at a high level, how it's going to work is we're going to start by pulling a partition by using a Chinese restaurant process, and then what we're going to do is before we draw parameters for any of the points, we're going to choose which we're going to choose a single point from each cluster to be its exemplar.",
            "Once we have the exemplar, we're going to draw the parameters for it from a base distribution, and then only after we've drawn the exemplar parameters are we going to draw the parameters for the remainder of the points.",
            "From there."
        ],
        [
            "Examplar to put a little more concreteness to this, the way that the Chinese restaurant process works very briefly is you imagine an infinite number of tables and you sequentially have diners come in and they decide to sit at a table with probability proportional to how many people are already sitting at the table.",
            "And then there's also a concentration parameter Alpha which tells you the probability of creating a new table to sit at, and so this is a stochastic process, but it defines a specific.",
            "Prior distribution over the cluster sizes."
        ],
        [
            "You can see on the bottom.",
            "The second step we're going to choose an exemplar for each of the nonempty clusters, and we're going to do this just uniformly, so the only constraint is that there's a single exemplar for each group."
        ],
        [
            "After that you can see this switching behavior is going to come in where if a point is an exemplar, then you're going to draw its parameters from a base distribution, which you can think of roughly as a very broad Gaussian centered at the sample mean.",
            "If you're in some sort of Euclidian space, but in general it's just a generative.",
            "Model of the points that you have.",
            "And then the second thing is, if a point is drawn.",
            "If a point is not an exemplar, then it's going to be drawn from a distribution parameterized by its exemplary.",
            "And so.",
            "I right here down the probability distributions, but it's going to turn out an inference.",
            "We don't need to explicitly know these distributions.",
            "We don't need to be able to sample from them or evaluate the normalization constant.",
            "We just need to evaluate the."
        ],
        [
            "Unnormalized probabilities.",
            "To give you an example of what this would look like in a very simple case, if your base distribution is Gaussian and each of the conditional likelihoods are Gaussian in a 2 dimensional space, you're going to have datasets that look roughly like this.",
            "The thing to notice about these is first that across the different datasets, there's a large number of different.",
            "There's a large variance in the number of clusters, so there's some that have only two clusters, and then some that has seven or eight here and then also.",
            "Within any given data set, the difference in size of the clusters is a lot as well.",
            "You can see for example up there you can see that there's a single cluster, a single point in one cluster, and then there's two very large clusters relatively."
        ],
        [
            "To do inference in our model, we're going to follow in the spirit of affinity propagation, and we're going to develop a Max product belief propagation algorithm."
        ],
        [
            "And there's a little bit of.",
            "Representational change that goes from the generative model that I explained to get to the point that we use in inference, you can see the paper for the exact details.",
            "But what you come up with is you're going to have three sets of factors in a factor graph, and each of these H is are going to be a binary variable where hij represents weather point.",
            "I belongs to cluster J and work.",
            "The exemplar of cluster J is going to be point J so.",
            "Along the rows, you're going to have factors.",
            "These are binary variables where they represent a multinomial variable that I had before and there's going to be end states along the row and what you're just going to have along the rows is a factor that says only one of these can be on at any given time, and this just says that each point can be assigned to only one cluster.",
            "Along the column, we're going to have factors that define the prior distribution over cluster sizes.",
            "You can see that in the second term there and then also attached to each point, we're going to have the similarity of point I to point J and so this is going to be a factor graph representation of exactly the generative model that I disc."
        ],
        [
            "Did earlier.",
            "So if you're looking at this in terms of inference algorithms for Dursley processes, the representation is slightly different than other mean field based approximations that you'll see elsewhere.",
            "So in particular, the labels that we're going to give these C values are always going to lie in the range of 1 to N, But they're not going to necessarily be contiguous.",
            "It's not going to be stacked up to be the first K digits, and also this isn't a. Truncated model, or a finite approximation.",
            "So there's no guarantee, for example, that these end clusters are these end clusters that you observe are the first N sticks in the stick breaking representation."
        ],
        [
            "To review, belief propagation and factor graphs, there's two types of messages.",
            "There's going to be messages from variables to factors that sum up the messages from the neighbors of each of the variables, and then from factors to variables it's going to add in the local potential to all of the incoming messages, and then it's going to maximize over the rest of the variables.",
            "In R model, there are only binary variables, so this lets us do a little bit of a trick that will simplify things later.",
            "Where the only messages that we're going to we're going to normalize messages in a specific way so that the messages corresponding to a value of 0 are always zero, and so the way that we do this is we compute the messages as we originally would, but then we subtract off the message corresponding to 0 from both of them."
        ],
        [
            "It is.",
            "So if you then take the factors that I showed you and you try to stick them into the Max product updates that I also showed you, then you're going to get stuck trying to compute the outgoing messages from these column factors and so this is the column factors that represent the distribution over the cluster sizes that we have.",
            "And the problem that you're going."
        ],
        [
            "To notice is that you're going to have a maximization that looks like this.",
            "In particular, there's going to be 2 terms.",
            "There's going to be a contribution from the incoming messages that tells you for each point whether or not it likes to be in a given cluster or not, and we're going to try to be looking over all points.",
            "And trying to decide which points should belong to a specific cluster, and so there's going to be this term from each point that says whether it likes to be in a cluster or not.",
            "And then there's going to be the second term which says that each cluster would like to have more points in it, regardless of which point it is, and so the problem with this maximization is to balance those two forces."
        ],
        [
            "And.",
            "If you look at it a little bit harder, the thing that you'll notice is that if you only knew how many points were going to be in each cluster, then these two terms would be couple, and you could greedily maximize it, so this is going to be the heart of the algorithm that we used to compute the messages and what we're going to do is we're going to condition on there being a certain number of points in each cluster, and then we're going to take the points that are most we're going to gradually take the points that should belong to that cluster.",
            "And then we're going to look over all possible.",
            "Sizes of the clusters and choose."
        ],
        [
            "Largest value.",
            "So once we've done that, then the rest of the messages are relatively easy to compute and we can.",
            "We have a Max product belief."
        ],
        [
            "An algorithm."
        ],
        [
            "The first set of experiments that we're going to look at is we're going to take some synthetic data similar to the ones I showed you, but in a higher dimensional space, and we're going to generate 1000 different datasets of 100 points each, and then what I'm showing here is a histogram that shows the frequency of different cluster sizes and so the thing to note in this synthetic example is that there's a large number of very small clusters, But then there's also this heavy tail where you have very large clust."
        ],
        [
            "And the algorithms that we're going to run are the original affinity propagation algorithm.",
            "This model that I just explained that we're going to call it Dursley, process affinity, propagation.",
            "And then we also developed and greedy iterated conditional modes algorithm on the same model."
        ],
        [
            "So this is one of the simpler experiments, but probably one of the more interesting ones that motivated this work, and this is if we take the original affinity propagation algorithm and we vary the free parameter in a number of ways to express the full range of its capabilities.",
            "What you see is that no matter how you set the parameters, it's not going to be able to represent this distribution over cluster sizes that we see in our synthetic data, and in particular, if you get this.",
            "Density of small clusters.",
            "It's not going to be able to keep the large one.",
            "So there's this implicit prior."
        ],
        [
            "Knowing the clusters to be roughly the same sizes.",
            "If you look at any of our algorithms, at least qualitatively, they're able to represent this same distribute."
        ],
        [
            "And that we wanted.",
            "Looking a little bit more quantitatively, we compare our Max product algorithm to our greedy algorithms along the.",
            "Likelihood access and we show that we are able to get slightly higher likelihoods and then comparing in the Rand index, which is a measure of how closely the labels we find matched the true labels.",
            "We also are able to help."
        ],
        [
            "Form that.",
            "For real experiments we then look at an image segmentation task where these similarity measures between data points becomes a more natural thing to use, and we're going to start by simplifying the problem by discretizing our images into a super pixel representation.",
            "And then what we're going to do is we're going to come up with a similarity measure that first finds an edge detector.",
            "Basically a local boundary detector.",
            "Across the image and then in order to measure the similarity between 2 super pixels, we're going to find shortest paths in this graph."
        ],
        [
            "And so you can see that this is a more complex similarity measure, but it's also a natural one to encode, and so then we're also going to add in some color information."
        ],
        [
            "Just to get that in.",
            "We're going to then compare a few algorithms.",
            "Affinity propagation, our model, and then a normalized cut segmentation algorithm."
        ],
        [
            "And what you can see very quickly here is that the main thing to notice is that in the Dursley process, affinity propagation the Christmas tree is going to be able to stay as one big cluster while you still have a very fine granularity on a lot of the other clusters, whereas the other algorithms when you try to get them to that fine of a granularity, they're going to break apart the."
        ],
        [
            "Mystery."
        ],
        [
            "I have other examples that demonstrate the same thing, just to wrap up.",
            "First off, the image segmentation algorithm is should be taken with a grain of salt.",
            "It was not a very well developed similarity measure, but it does show you the implicit.",
            "The implicit priors here in the normalized cut and the affinity propagation algorithm, and it shows a way to make that prior explicit, and it shows the effect that we have on that application.",
            "One caveat that we should make is that the algorithm as it is is fairly slow.",
            "It takes order N cubed per iteration to run, but we have some ideas for how to speed that up and are looking forward to that.",
            "And then finally for the image segmentation task, we played around with a number of different parameters or different settings of the concentration parameter for the Jersey process, but for any reasonable setting of the parameters, it still was too extreme of a prior.",
            "It tried to make the large clusters too large and then it was too willing to just have clusters of a single superpixel, so it would be interesting to look to see if we could learn application specific priors over these cluster size distributions."
        ],
        [
            "That is all, thank you.",
            "Question.",
            "Well, I have one I'm wondering about how you were talking about at the very end about trying to actually learn the prior of the cluster distributions, and I guess I've got it's a two part question.",
            "One part is, is it actually possible to go Bayesian over the parameters in the in the display process?",
            "You know, could you try to actually put up?",
            "Put a prior on then you know and then and then Secondly.",
            "Yeah, I mean.",
            "It's also it's.",
            "It's certainly possible.",
            "I don't know off the top of my head if the inference would workout in as nice of a way as it would if you had the priors over the priors.",
            "And then I was also wondering how the complexity of the prior impacted the computational complexity is.",
            "Um?",
            "If you mean so the way that we represent the priors is basically just as a histogram.",
            "So for each size of a cluster, what is the probability associated with that?",
            "So there's no.",
            "There's no difference in any shape of the histogram if you don't have a prior at all, then you can speed up the inference.",
            "One thing that we process mixture models is that.",
            "But it would seem that maybe putting him the exemplar stepped into the process would eliminate that.",
            "Yeah, it's certainly not infinitely exchangeable.",
            "We're not doing Gibbs sampling, so it's questionable of how important that is and this application, but certainly the priors, for example, are dependent on the size of the data set, so it's not going to have the nice statistical properties that a lot of the Dursley processes will."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unlike the previous two speakers, I'm not going to talk about topic models, but instead I'm going to focus on clustering, and in particular I'm going to talk about how to formulate a clustering model in a natural way that lets you express the clustering problem in the way that you want to.",
                    "label": 0
                },
                {
                    "sent": "So the topic is going to be flexible.",
                    "label": 0
                },
                {
                    "sent": "Priors.",
                    "label": 0
                },
                {
                    "sent": "For example, are based clustering and it's with my advisor, Richard Zemel, and Brendan Frey.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to start off at a high level by talking about why we cluster data to begin with, and so if you have data that looks like this, you might be interested just on the left side in representing the density of the data points of your data.",
                    "label": 0
                },
                {
                    "sent": "A second reason that you might want to cluster is that you actually believe that there are underlying classes in the data, and these are two slightly different thinking about the problem in these two slightly different ways is going to give you a different way of formulating the clustering problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, in particular, we're going to talk about the second example in this work.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just I want to start off by convincing you that all of the clustering algorithms that you know and love might not be as powerful and as expressive as you might think they would be.",
                    "label": 0
                },
                {
                    "sent": "So in particular, I want to think about a problem where if you have a personal photo collection and you're interested in clustering the faces that you find in that photo collection.",
                    "label": 1
                },
                {
                    "sent": "So the first thing to notice about this problem is that when you have data points that are complex entities such as images.",
                    "label": 1
                },
                {
                    "sent": "I just embedding them in a Gaussian space or in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "Probably isn't going to be as expressive as you'd like to be.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you are interested in clustering while maintaining translation invariants, for example, it's much easier to just think about a similarity measure between between two of your objects, and that will allow you to.",
                    "label": 0
                },
                {
                    "sent": "Come up with more complex but also more natural ways of representing how your clustering problem should be formulated.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second thing that I want you to notice is that.",
                    "label": 0
                },
                {
                    "sent": "If your photo collections are anything like mine, for example, I have a lot of photos of certain people, so if, for example, my coauthors I spend a lot of time with them and as a result I'm going to have a lot of pictures of them in my photo collection while.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in contrast, there's also going to be a large number of people that I see very infrequently in my photo collection, so these are my graduate student friends.",
                    "label": 0
                },
                {
                    "sent": "More seriously, you could imagine that in a photo collection you have your friends and family that you see often, but then there's also going to be a decay of how often people occur, and you'll have images where you just capture a random stranger in the background.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "So these are the two ideas that motivate this work.",
                    "label": 1
                },
                {
                    "sent": "The first is that we want to be able to express more complex measures between our data points.",
                    "label": 1
                },
                {
                    "sent": "So in particular similarity measures possibly aren't even metric.",
                    "label": 0
                },
                {
                    "sent": "And then second we want to instead of having to specify some arbitrary model selection parameters such as specifying K or choosing a cost for creating each cluster, we want to express this information in a more natural way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the building blocks that we've chosen to work with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In particular there are there other choices of how you may choose to do this, But this is the one that we're going to focus on, so we're going to work in an exemplar based clustering framework.",
                    "label": 0
                },
                {
                    "sent": "The idea of an exemplar based clustering method is that instead of trying to estimate latent continuous parameters, we can instead just try to find a data point that is representative of each cluster and the benefit of doing this is that you no longer have to estimate any latent parameters, and instead it's just a combinatorial problem of choosing the exemplar for each data point.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the comment Orks are a little bit tricky, but they can also be dealt with efficiently, so if you haven't seen the affinity propagation algorithm, this is what you get just by applying Max product belief propagation to the exemplar based clustering problem, and you can see an example intuitively of how it works is that the data points past messages to each other, and the goal is for them to reach an agreement on which the exemplars are.",
                    "label": 0
                },
                {
                    "sent": "And so you see at the bottom right upon convergence.",
                    "label": 0
                },
                {
                    "sent": "Each of the clusters agree on who their exemplar should be.",
                    "label": 0
                },
                {
                    "sent": "Now, one of the drawbacks of this is that to choose how many clusters you have in the data, you have to set a parameter which they call the preference, and it's not a very intuitive parameter to set and it doesn't have a natural a justification for what it means by choosing a specif.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value.",
                    "label": 0
                },
                {
                    "sent": "So instead what we think is a more natural way to specify the model selection aspect of the problem is to deal with distributions over cluster sizes.",
                    "label": 0
                },
                {
                    "sent": "So given a particular application, you might have a rough idea about how big the clusters should be.",
                    "label": 0
                },
                {
                    "sent": "So in particular in the photo collection application, you might be able to say that there are going to be a small number of or they're going to be a large number of very small clusters.",
                    "label": 0
                },
                {
                    "sent": "These represent the strangers or the people you see infrequently.",
                    "label": 0
                },
                {
                    "sent": "And then there's also going to be the close friends and family that corresponds to this heavier tail.",
                    "label": 0
                },
                {
                    "sent": "That you see here.",
                    "label": 0
                },
                {
                    "sent": "And so this is a more general form of specifying the model selection problem than Dursley process priors, for example.",
                    "label": 1
                },
                {
                    "sent": "Or the Pitman yor process, in that you don't have to maintain the statistical properties of the Dursley process, for example.",
                    "label": 0
                },
                {
                    "sent": "And this can be used in place of specifying.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to make this blatantly clear, there are other works that are able to take into account parts of this, so in particular we're looking at automatic model selection.",
                    "label": 0
                },
                {
                    "sent": "Being able to specify these flexible priors and being able to express the similarity measures that are more complex and so, for example, Dursley process mixture models can take into account the priors, but they don't deal well with the.",
                    "label": 1
                },
                {
                    "sent": "Nonmetric or.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated likelihoods.",
                    "label": 0
                },
                {
                    "sent": "So in order to explain how this works, I'm going to start by explaining a more specific model than we're able to handle, but one that gets across the point pretty well, and So what we're going to do is we're going to use this exemplar based setting, and we're going to develop a Dursley process mixture MoD.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to get the notation set, there's going to be three sets of variables for each point we're going to maintain which cluster it's assigned to, so this is, this is just a representation of a partition.",
                    "label": 0
                },
                {
                    "sent": "So for example, if C1C2C and C5 are given the same #7, they belong to Cluster 7 and that just says that they belong to the same class.",
                    "label": 1
                },
                {
                    "sent": "You could instead give them label 12 and it would be exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Partition.",
                    "label": 0
                },
                {
                    "sent": "On top of that, we're going to represent, for each point, just a binary variable saying whether it's an example or not.",
                    "label": 0
                },
                {
                    "sent": "And then we're also going to obviously represent the parameter vectors describing each point.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The X is.",
                    "label": 0
                },
                {
                    "sent": "So the generative model at a high level, how it's going to work is we're going to start by pulling a partition by using a Chinese restaurant process, and then what we're going to do is before we draw parameters for any of the points, we're going to choose which we're going to choose a single point from each cluster to be its exemplar.",
                    "label": 1
                },
                {
                    "sent": "Once we have the exemplar, we're going to draw the parameters for it from a base distribution, and then only after we've drawn the exemplar parameters are we going to draw the parameters for the remainder of the points.",
                    "label": 0
                },
                {
                    "sent": "From there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examplar to put a little more concreteness to this, the way that the Chinese restaurant process works very briefly is you imagine an infinite number of tables and you sequentially have diners come in and they decide to sit at a table with probability proportional to how many people are already sitting at the table.",
                    "label": 0
                },
                {
                    "sent": "And then there's also a concentration parameter Alpha which tells you the probability of creating a new table to sit at, and so this is a stochastic process, but it defines a specific.",
                    "label": 0
                },
                {
                    "sent": "Prior distribution over the cluster sizes.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see on the bottom.",
                    "label": 0
                },
                {
                    "sent": "The second step we're going to choose an exemplar for each of the nonempty clusters, and we're going to do this just uniformly, so the only constraint is that there's a single exemplar for each group.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After that you can see this switching behavior is going to come in where if a point is an exemplar, then you're going to draw its parameters from a base distribution, which you can think of roughly as a very broad Gaussian centered at the sample mean.",
                    "label": 0
                },
                {
                    "sent": "If you're in some sort of Euclidian space, but in general it's just a generative.",
                    "label": 0
                },
                {
                    "sent": "Model of the points that you have.",
                    "label": 0
                },
                {
                    "sent": "And then the second thing is, if a point is drawn.",
                    "label": 0
                },
                {
                    "sent": "If a point is not an exemplar, then it's going to be drawn from a distribution parameterized by its exemplary.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I right here down the probability distributions, but it's going to turn out an inference.",
                    "label": 0
                },
                {
                    "sent": "We don't need to explicitly know these distributions.",
                    "label": 1
                },
                {
                    "sent": "We don't need to be able to sample from them or evaluate the normalization constant.",
                    "label": 0
                },
                {
                    "sent": "We just need to evaluate the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unnormalized probabilities.",
                    "label": 0
                },
                {
                    "sent": "To give you an example of what this would look like in a very simple case, if your base distribution is Gaussian and each of the conditional likelihoods are Gaussian in a 2 dimensional space, you're going to have datasets that look roughly like this.",
                    "label": 0
                },
                {
                    "sent": "The thing to notice about these is first that across the different datasets, there's a large number of different.",
                    "label": 0
                },
                {
                    "sent": "There's a large variance in the number of clusters, so there's some that have only two clusters, and then some that has seven or eight here and then also.",
                    "label": 0
                },
                {
                    "sent": "Within any given data set, the difference in size of the clusters is a lot as well.",
                    "label": 0
                },
                {
                    "sent": "You can see for example up there you can see that there's a single cluster, a single point in one cluster, and then there's two very large clusters relatively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do inference in our model, we're going to follow in the spirit of affinity propagation, and we're going to develop a Max product belief propagation algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Representational change that goes from the generative model that I explained to get to the point that we use in inference, you can see the paper for the exact details.",
                    "label": 0
                },
                {
                    "sent": "But what you come up with is you're going to have three sets of factors in a factor graph, and each of these H is are going to be a binary variable where hij represents weather point.",
                    "label": 0
                },
                {
                    "sent": "I belongs to cluster J and work.",
                    "label": 0
                },
                {
                    "sent": "The exemplar of cluster J is going to be point J so.",
                    "label": 0
                },
                {
                    "sent": "Along the rows, you're going to have factors.",
                    "label": 0
                },
                {
                    "sent": "These are binary variables where they represent a multinomial variable that I had before and there's going to be end states along the row and what you're just going to have along the rows is a factor that says only one of these can be on at any given time, and this just says that each point can be assigned to only one cluster.",
                    "label": 0
                },
                {
                    "sent": "Along the column, we're going to have factors that define the prior distribution over cluster sizes.",
                    "label": 0
                },
                {
                    "sent": "You can see that in the second term there and then also attached to each point, we're going to have the similarity of point I to point J and so this is going to be a factor graph representation of exactly the generative model that I disc.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did earlier.",
                    "label": 0
                },
                {
                    "sent": "So if you're looking at this in terms of inference algorithms for Dursley processes, the representation is slightly different than other mean field based approximations that you'll see elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So in particular, the labels that we're going to give these C values are always going to lie in the range of 1 to N, But they're not going to necessarily be contiguous.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be stacked up to be the first K digits, and also this isn't a. Truncated model, or a finite approximation.",
                    "label": 0
                },
                {
                    "sent": "So there's no guarantee, for example, that these end clusters are these end clusters that you observe are the first N sticks in the stick breaking representation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To review, belief propagation and factor graphs, there's two types of messages.",
                    "label": 1
                },
                {
                    "sent": "There's going to be messages from variables to factors that sum up the messages from the neighbors of each of the variables, and then from factors to variables it's going to add in the local potential to all of the incoming messages, and then it's going to maximize over the rest of the variables.",
                    "label": 0
                },
                {
                    "sent": "In R model, there are only binary variables, so this lets us do a little bit of a trick that will simplify things later.",
                    "label": 0
                },
                {
                    "sent": "Where the only messages that we're going to we're going to normalize messages in a specific way so that the messages corresponding to a value of 0 are always zero, and so the way that we do this is we compute the messages as we originally would, but then we subtract off the message corresponding to 0 from both of them.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "So if you then take the factors that I showed you and you try to stick them into the Max product updates that I also showed you, then you're going to get stuck trying to compute the outgoing messages from these column factors and so this is the column factors that represent the distribution over the cluster sizes that we have.",
                    "label": 0
                },
                {
                    "sent": "And the problem that you're going.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To notice is that you're going to have a maximization that looks like this.",
                    "label": 0
                },
                {
                    "sent": "In particular, there's going to be 2 terms.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a contribution from the incoming messages that tells you for each point whether or not it likes to be in a given cluster or not, and we're going to try to be looking over all points.",
                    "label": 0
                },
                {
                    "sent": "And trying to decide which points should belong to a specific cluster, and so there's going to be this term from each point that says whether it likes to be in a cluster or not.",
                    "label": 0
                },
                {
                    "sent": "And then there's going to be the second term which says that each cluster would like to have more points in it, regardless of which point it is, and so the problem with this maximization is to balance those two forces.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you look at it a little bit harder, the thing that you'll notice is that if you only knew how many points were going to be in each cluster, then these two terms would be couple, and you could greedily maximize it, so this is going to be the heart of the algorithm that we used to compute the messages and what we're going to do is we're going to condition on there being a certain number of points in each cluster, and then we're going to take the points that are most we're going to gradually take the points that should belong to that cluster.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to look over all possible.",
                    "label": 0
                },
                {
                    "sent": "Sizes of the clusters and choose.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Largest value.",
                    "label": 0
                },
                {
                    "sent": "So once we've done that, then the rest of the messages are relatively easy to compute and we can.",
                    "label": 1
                },
                {
                    "sent": "We have a Max product belief.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first set of experiments that we're going to look at is we're going to take some synthetic data similar to the ones I showed you, but in a higher dimensional space, and we're going to generate 1000 different datasets of 100 points each, and then what I'm showing here is a histogram that shows the frequency of different cluster sizes and so the thing to note in this synthetic example is that there's a large number of very small clusters, But then there's also this heavy tail where you have very large clust.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the algorithms that we're going to run are the original affinity propagation algorithm.",
                    "label": 1
                },
                {
                    "sent": "This model that I just explained that we're going to call it Dursley, process affinity, propagation.",
                    "label": 1
                },
                {
                    "sent": "And then we also developed and greedy iterated conditional modes algorithm on the same model.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is one of the simpler experiments, but probably one of the more interesting ones that motivated this work, and this is if we take the original affinity propagation algorithm and we vary the free parameter in a number of ways to express the full range of its capabilities.",
                    "label": 0
                },
                {
                    "sent": "What you see is that no matter how you set the parameters, it's not going to be able to represent this distribution over cluster sizes that we see in our synthetic data, and in particular, if you get this.",
                    "label": 0
                },
                {
                    "sent": "Density of small clusters.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be able to keep the large one.",
                    "label": 0
                },
                {
                    "sent": "So there's this implicit prior.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knowing the clusters to be roughly the same sizes.",
                    "label": 0
                },
                {
                    "sent": "If you look at any of our algorithms, at least qualitatively, they're able to represent this same distribute.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that we wanted.",
                    "label": 0
                },
                {
                    "sent": "Looking a little bit more quantitatively, we compare our Max product algorithm to our greedy algorithms along the.",
                    "label": 0
                },
                {
                    "sent": "Likelihood access and we show that we are able to get slightly higher likelihoods and then comparing in the Rand index, which is a measure of how closely the labels we find matched the true labels.",
                    "label": 0
                },
                {
                    "sent": "We also are able to help.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Form that.",
                    "label": 0
                },
                {
                    "sent": "For real experiments we then look at an image segmentation task where these similarity measures between data points becomes a more natural thing to use, and we're going to start by simplifying the problem by discretizing our images into a super pixel representation.",
                    "label": 1
                },
                {
                    "sent": "And then what we're going to do is we're going to come up with a similarity measure that first finds an edge detector.",
                    "label": 0
                },
                {
                    "sent": "Basically a local boundary detector.",
                    "label": 0
                },
                {
                    "sent": "Across the image and then in order to measure the similarity between 2 super pixels, we're going to find shortest paths in this graph.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you can see that this is a more complex similarity measure, but it's also a natural one to encode, and so then we're also going to add in some color information.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to get that in.",
                    "label": 0
                },
                {
                    "sent": "We're going to then compare a few algorithms.",
                    "label": 0
                },
                {
                    "sent": "Affinity propagation, our model, and then a normalized cut segmentation algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you can see very quickly here is that the main thing to notice is that in the Dursley process, affinity propagation the Christmas tree is going to be able to stay as one big cluster while you still have a very fine granularity on a lot of the other clusters, whereas the other algorithms when you try to get them to that fine of a granularity, they're going to break apart the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mystery.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have other examples that demonstrate the same thing, just to wrap up.",
                    "label": 0
                },
                {
                    "sent": "First off, the image segmentation algorithm is should be taken with a grain of salt.",
                    "label": 1
                },
                {
                    "sent": "It was not a very well developed similarity measure, but it does show you the implicit.",
                    "label": 0
                },
                {
                    "sent": "The implicit priors here in the normalized cut and the affinity propagation algorithm, and it shows a way to make that prior explicit, and it shows the effect that we have on that application.",
                    "label": 0
                },
                {
                    "sent": "One caveat that we should make is that the algorithm as it is is fairly slow.",
                    "label": 0
                },
                {
                    "sent": "It takes order N cubed per iteration to run, but we have some ideas for how to speed that up and are looking forward to that.",
                    "label": 1
                },
                {
                    "sent": "And then finally for the image segmentation task, we played around with a number of different parameters or different settings of the concentration parameter for the Jersey process, but for any reasonable setting of the parameters, it still was too extreme of a prior.",
                    "label": 0
                },
                {
                    "sent": "It tried to make the large clusters too large and then it was too willing to just have clusters of a single superpixel, so it would be interesting to look to see if we could learn application specific priors over these cluster size distributions.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is all, thank you.",
                    "label": 1
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Well, I have one I'm wondering about how you were talking about at the very end about trying to actually learn the prior of the cluster distributions, and I guess I've got it's a two part question.",
                    "label": 0
                },
                {
                    "sent": "One part is, is it actually possible to go Bayesian over the parameters in the in the display process?",
                    "label": 0
                },
                {
                    "sent": "You know, could you try to actually put up?",
                    "label": 0
                },
                {
                    "sent": "Put a prior on then you know and then and then Secondly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "It's also it's.",
                    "label": 0
                },
                {
                    "sent": "It's certainly possible.",
                    "label": 0
                },
                {
                    "sent": "I don't know off the top of my head if the inference would workout in as nice of a way as it would if you had the priors over the priors.",
                    "label": 0
                },
                {
                    "sent": "And then I was also wondering how the complexity of the prior impacted the computational complexity is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If you mean so the way that we represent the priors is basically just as a histogram.",
                    "label": 0
                },
                {
                    "sent": "So for each size of a cluster, what is the probability associated with that?",
                    "label": 0
                },
                {
                    "sent": "So there's no.",
                    "label": 0
                },
                {
                    "sent": "There's no difference in any shape of the histogram if you don't have a prior at all, then you can speed up the inference.",
                    "label": 0
                },
                {
                    "sent": "One thing that we process mixture models is that.",
                    "label": 0
                },
                {
                    "sent": "But it would seem that maybe putting him the exemplar stepped into the process would eliminate that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's certainly not infinitely exchangeable.",
                    "label": 0
                },
                {
                    "sent": "We're not doing Gibbs sampling, so it's questionable of how important that is and this application, but certainly the priors, for example, are dependent on the size of the data set, so it's not going to have the nice statistical properties that a lot of the Dursley processes will.",
                    "label": 0
                }
            ]
        }
    }
}