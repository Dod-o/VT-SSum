{
    "id": "r5t2e27k3by6z3wepdjzo4e3jka5ogof",
    "title": "Hierarchical Kernel Stick-Breaking Process for Multi-Task Image Analysis",
    "info": {
        "author": [
            "Lawrence Carin, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Image Analysis",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_carin_hks/",
    "segmentation": [
        [
            "OK, good afternoon.",
            "So I'm Larry, Karen from Duke University.",
            "And I'm going to talk to you about something called a hierarchical kernel stick breaking process, which we've applied to multitask image analysis.",
            "I'll."
        ],
        [
            "I'll explain all of that.",
            "OK, so so the problem that we're trying to examine his image analysis an as a as a component of that we have the problem of image segmentation.",
            "So some of the various techniques that have been developed over the years for this problem include K means, statistical mixture modeling, spectral clustering, etc.",
            "So there definitely techniques out there to do this.",
            "Our objective here is to process, but each of those is performed on each image one by one.",
            "Each image is analyzed in isolation.",
            "So what we want to do here is to do simultaneous processing.",
            "Of a whole bunch of images.",
            "The idea is that if we if we do that effectively, we can.",
            "We can learn when we analyze any one image we can learn from the other images and Moreover the sharing mechanisms that we infer between the different images tell us something about the relationship of the images, which is useful for ranking and sorting and things of that nature.",
            "So that kind of motivate."
        ],
        [
            "The multi task aspect of this we're going to process multiple images simultaneously."
        ],
        [
            "The way we're going to do this is via an extension of the Dursley process or a particular form of the Dursley process, which can be represented as a kernel stick breaking process as.",
            "Sorry, the DP can be represented as a stick breaking process, and we're going to."
        ],
        [
            "Stand that to a kernel stick breaking process.",
            "So one of them.",
            "So why are we doing that?",
            "In other words, we could we could do the clustering.",
            "The segmentation of the images with the DP.",
            "But the reason that we don't want to do that is because their sleep process assumes that all of the data samples in a given image would be exchangeable.",
            "In other words, the order doesn't matter and order absolutely matters for images in the sense that whenever two pixels are near each other, you would expect them to be in the same same cluster, and that information would be ignored by dearsley process representation.",
            "So some of the other issues with typical segmentation techniques.",
            "Why do we want to do this?",
            "Nonparametrically we don't.",
            "We do not have to apriori set the number of segments the data will tell us that by itself, and Moreover, by putting this in one level more of hierarchy, we can process a bunch of images simultaneously and therefore infer into relationships and therefore do things like sort."
        ],
        [
            "Of images.",
            "OK, so so I assume most of this audience is pretty familiar with their sleep process, but let me just remind you a little bit about it.",
            "So it's a process is represented by two parameters, a positive scalar Alpha and a base measure G not.",
            "And a draw from a DP is in fact a measure, so it's it's.",
            "It's a statistical measure as well, and the stick breaking representation and so the DP is characterized by two things by precision and a base measure, and the way we draw G from a DP is to draw parameters V sub H. And please pay attention to the notation 'cause we're going to extend this, so this may not be the way everybody looks at VP, but it's the way we're going to and will help you to understand K SBP, which is probably new to most people.",
            "So in any case we're going to draw, we're going to draw V sub H ID from a beta distribution and then we're going to use it to drawl off, break off pieces of a stick of unit length.",
            "So what we're doing here is this is a stick of.",
            "Like one, we're going to break off a piece of that which is a fractional length V1, and we're going to call that Pi one and then 1 -- V one will be left behind were going to draw another V2 from the beta distribution, break off another stick, call that Pi 2, etc.",
            "And then these are are going to be the weight and then these atoms.",
            "The Theta H is are drawn IID from the base measure G not and so.",
            "Therefore G is represented in.",
            "So called stick breaking form as shown here.",
            "OK so and then typically we would draw parameters from G. Since this is basically a mixture model and infinite mixture model, what's going to happen is that those types of H is with large amplitude, are going to dominate, and the associated atoms are going to be preferentially selected.",
            "This is going to yield a mixture model.",
            "It's nonparametric in the sense that the number of mixtures is not set apriori.",
            "The problem with this and the problem that we're trying to address, or one of the problems we're trying to address, is that if you draw parameters from G, the order at which you draw those parameters is irrelevant, there's no."
        ],
        [
            "Order information at all, and so oftentimes we do have order information in the previous talk we talked about time that data is collected sequentially in time, and we know that and you would expect that things should cluster more prominently whenever they're nearby in time.",
            "So you can use a K SBP, which I'll describe to model time involving data.",
            "Here we're going to be looking at.",
            "Data in an image, and so the idea is, is that if you draw features from an image and you wish to segment or cluster those features, it's reasonable to assume that features that are close together within the image are more likely to be clustered together than otherwise.",
            "That information would be lost in a fiercely."
        ],
        [
            "Process representation, so this motivates K SBP.",
            "OK, so this is it.",
            "Hopefully I can convey convey this to you.",
            "OK, so the idea here is that we're going to draw the visa batches are the exactly the same.",
            "Visa batches that we drew before from the beta distribution.",
            "This is no longer ADP, so we're free to say AB.",
            "You don't have to say 1A could be.",
            "Maybe this is something different than DP, but in any case we're going to draw.",
            "These visa pages from a beta distribution we're going to draw an infinite number of these IID.",
            "And so that that is the same.",
            "And if you look at this in the."
        ],
        [
            "And the DP the weights on the sticks are given by this, so you just have a product of those draws from them."
        ],
        [
            "So what is what is different here?",
            "Is that we now want to take into account the location, not just so X of N represents the feature of the NTH Patch in the image and our sub N locates defines its location.",
            "Let's say in two XYRIJ of the Pixel.",
            "OK, so we're using both the feature and its location.",
            "And so R is a general location, identifies a general location within a within within image.",
            "So the thing to notice here is that the weight on our stick breaking representation.",
            "So Ji sub R which is our draw from a K SBP, has weights which are function of where you are in the image.",
            "So notice that the weights are now function of R. The way that we set that up is using a kernel or K can be any kernel you like, which has a maximum value of 1 and minimum of 0.",
            "So a radial basis function would be a good example.",
            "So R is a general location in the image an gamma sub L. Is a basis function which locates basically locate source, enters this kernel in space.",
            "OK, So what we're doing here is that these guys are drawn from the beta distribution in the usual way, but now we're going to multiply these by a kernel and what's going to happen is is that whenever R is.",
            "Close to gamma sub L, This kernel is going to be near one and whenever R is far away from gammas of L, this kernel is going to go to zero OK and so now what's going to happen is is that we're going to a draw from a kernel stick.",
            "Breaking process is also a stick breaking representation.",
            "Atoms we have an infinite set of atoms and what's happening is is that the weights on those atoms are going to be spatially.",
            "Spatially located."
        ],
        [
            "Within within the image.",
            "OK so."
        ],
        [
            "The model graphically looks like this, so on the left is a DP.",
            "So in a DP we draw from a beta distribution with parameter Alpha.",
            "We draw atoms from a base measure and then indicator variables are selected via a multinomial characterized by the stick weights and then with the associated parameter we draw data.",
            "The distinction with the kernel stick breaking processes everything.",
            "Looks the same except for we now are going to draw these gamma H is which are going to center or localize each one of those atoms within the image.",
            "So Gamma said H is a 2 dimensional IJ which is going to locate the.",
            "The Atom or localize the Atom in space, and So what happens here now is that the observed data the observed data is a function of where you are.",
            "So our sub N is the location of the NTH sample annexa Ben is associated feature vector.",
            "Are there any questions that make any sense?",
            "Is that clear?"
        ],
        [
            "OK, so So what are the properties of this kernel stick breaking process?",
            "So let's assume that you draw from that distribution and \u03c0 sub H is are the weights of that stick breaking representation, which are now a function of R. And now let us draw parameters we're going to draw from the stick breaking representation Colonel Stick breaking representation, and now we're going to draw parameters at the location R&R Prime and the question is how are they correlated?",
            "With each other.",
            "So what you would expect is that as R&R prime get near each other, the correlation between the parameters is going to be high and whenever our in our prime are distant from each other, their correlation goes to zero.",
            "We can write it down.",
            "It turns out analytically.",
            "We can prove that this is this is true.",
            "So basically what the kernel stick breaking processes doing again is saying that if you have an image, that image is characterized by a bunch of features.",
            "What this is saying.",
            "Is that it is likely that if 2 features are nearby in the image that they will be drawn from the model with the same parameter and therefore be in the same cluster and whenever 2 features are more separated from each other within the image, the correlation between the atoms is going to diminish and therefore it's likely that they will be indistinct segm."
        ],
        [
            "OK, OK, so that's that's the kernel stick breaking process applied to images.",
            "We can apply this to one image at a time, but we oftentimes have many images and it would be useful if we can process or segment all of the images simultaneously, because we're able to share information between the different images and Moreover were able to learn the interrelationships between between the different images as well.",
            "OK, so this is going to be a technique we're going to use too.",
            "Not only segment images, but sort images and do that simultaneously."
        ],
        [
            "OK, so it turns out that this is actually pretty simple if you know about the HTP, then doing this is actually not a big deal, and the HTP, what we do is we have two levels.",
            "We first draw G from ADP with base measure G not.",
            "Which tells us with probability 1G is discrete.",
            "And so in the HDP this measure G is then used as the base measure for for DP at the next level, the only thing we're doing here, which is different, which is why we call this a hierarchical Colonel Sacred hierarchical kernel stick breaking representation is that instead of, it's really basically the same thing as HTP, except that this G is now put into the into the H."
        ],
        [
            "And into the CCP.",
            "OK, so the model looks like this so that complicated thing.",
            "And now I'm just going to call that KSP.",
            "And I'm going to draw a CSB.",
            "P I'm going to draw a G from a case PP for each of my images.",
            "So each image I'm going to do this on each of my images.",
            "The base measure of each of those case CPS will be drawn from a DP which will allow me."
        ],
        [
            "To share, OK, this is what the graphical representation looks like.",
            "I'm running out of time, so I'll kind of I'll skip this, but it's."
        ],
        [
            "Something that is something that you can perform inference on relatively straightforwardly every every level in the hierarchy is in the conjugate exponential family, except for one.",
            "So we want to do this fast, so we have a variational based solution for every step in the hierarchy except for one, at which point we sample.",
            "OK, so we have analytic updates at every point except for one, where we do Monte Carlo expectation Monte Carlo type of."
        ],
        [
            "Of sampling, but the inversion."
        ],
        [
            "The processing is actually quite fast.",
            "We're able to process a lot of images, I'll show you.",
            "OK, so to show you how this works we processed some data from the Microsoft Research came."
        ],
        [
            "Which image database?",
            "So hopefully that is you can see that.",
            "So what we're looking at here is different types of data.",
            "Clouds, buildings, countryside, etc and we're looking at which atoms are dominant for each type of image.",
            "So there are seven different types of images.",
            "I'll show you what they look like an for each of them.",
            "We're going to represent them as a mixture mixture model and these are the atoms that we find on that mixture model, and we're also imposing that spatial structure from the CCP.",
            "OK, so by looking at that you can see that the atoms are fairly distinct for the diff."
        ],
        [
            "Types of images which you would think would allow us to do a fairly good job of not only clustering the images, but clustering across images, which is what we really want to do.",
            "OK, so this is just to show you visually what the atoms look like, so these are.",
            "What you're looking at here is.",
            "This is the 4th 4th Atom.",
            "And what I'm doing is I'm showing you a particular image where it goes black.",
            "That means that it was not included in that Atom.",
            "So what this tells you is the 4th Atom is kind of a Sky Atom.",
            "The 31st Atom is a cloud Atom.",
            "What we're kind of pleased with is like, for example the 38th Atom does a very nice job of representing buildings and houses, so you can see that if you look at where the 38th Adam goes to it does a really nice job of pulling out these buildings even under fairly complicated situations the.",
            "33rd Adam seems to be a face Adam."
        ],
        [
            "OK, OK. And then this is an example of the of the segmentation that you get.",
            "So what we're looking at here and I'm just about done, is a particular image and its associated segmentation as constituted through HK SPP.",
            "An HTP.",
            "So the only distinction between these is that in the HK SPP were imposing the belief that proximate pixels should be in the same cluster, whereas an HTP we do not.",
            "HTP does perform fairly well, but there are several cases for which it really does inferior job relative to HBP.",
            "So for here example, the face with HBP.",
            "Does it really seem to have like a skin Atom that it pulls out the face pretty nicely?",
            "Where HDP does not OK so so there are two products of this work.",
            "This is 1 product, one product is we can cluster the images and Moreover we can learn the atoms of that clustering.",
            "On the fly.",
            "So we're learning the atoms and clustering."
        ],
        [
            "Simultaneously.",
            "But what we really want to do is you give me and.",
            "And then I will find 10 other images that I say you should also find interesting.",
            "In other words, I want to cluster images, not just segment them, and So what you're looking at here is for those seven different classes of.",
            "Image is one of the products that we get is how interrelated the different data are.",
            "The way that we this is what this is a representation of is the sharing mechanism that was uncovered within the inference process.",
            "So this comes directly is not a post processing step.",
            "This comes directly as a product of the HKS BP and you can see that it does a pretty nice job.",
            "What we would like to see is this to be blocked diagonal, which would imply that each of the images were shared within each of the 7.",
            "Classes of images, but it generally does well, and if I showed you each of the images where we were that doesn't occur."
        ],
        [
            "It actually is.",
            "Is is reasonable.",
            "OK, so this is the last slide so that the kernel stick breaking processes.",
            "A new technique developed by David Duke and extended here for image segmentation.",
            "It removes the exchangeability assumption in the daresay process.",
            "We are interested in not only segmenting a single image, but in segmenting multiple images.",
            "So a natural extension of this is the hierarchical KSP, which we've implemented, and it seems to perform well.",
            "And as I said, the inference is fast.",
            "We want to solve a lot of images.",
            "We want to analyze a lot of images, so we really don't do not want to use an MCMC type of a solution which we have, by the way, but we wanted to use a variational base solution.",
            "And with the model, every step in the hierarchy satisfies the requirements of VB, except for one at which we sample and still get a pretty efficient code.",
            "So anyway, thank you for your attention.",
            "Questions.",
            "Can you you showed a plate representation of the KSB?"
        ],
        [
            "He way at the beginning anyway made me think of mixtures of experts, right so that they could have a direct arrow from our to the outputs, right?",
            "Yeah, so yes.",
            "So that's right.",
            "So we have applied this.",
            "A mixture of experts.",
            "So this is a way to do nonparametric mixture of experts and we were doing that.",
            "But you're right, quite right.",
            "It's a good.",
            "It's a good way to do that.",
            "That's a good way to do that.",
            "There's some we could talk offline where there are better ways of doing it.",
            "But anyway, you're right.",
            "So I might be missing something to do show that your kernel stick breaking process results in sticks that sum to one?",
            "Yes, I didn't.",
            "I didn't, I didn't.",
            "I saw I had five minutes with about 10 minutes of material, so I kind of skipped that."
        ],
        [
            "But yeah, all the sticks sum to one.",
            "And the reason that that's true is because the kernel whenever whenever R equals gamma L. In other words, when you're on the center location, the kernel is 1 and whenever you move off it goes to 0.",
            "Because of that property, the sticks sum to one.",
            "And then you can use any kernel you like to make that happen.",
            "OK, thanks.",
            "I have a question.",
            "Can you contrast this with Jim Griffin's auto based dependent richly processes?",
            "Since I'm not that familiar with it, I want you want to tell me a little bit about it, and then I will.",
            "I mean, we're I. I'm not familiar with that.",
            "OK so?",
            "You so the atoms are also associated with locations.",
            "Anne, this you have a set of these as well associated with each Atom.",
            "But the ordering of the atoms in the stick breaking construction depends on the location of the atoms and where you are in the space.",
            "So you want to have atoms which are close by to be put into the sticking construction.",
            "First there is there there there are other techniques I think I know what you're talking about there.",
            "There are other people who have tried to remove the exchange ability and DP and.",
            "Let me just say this, David Johnson, who was the inventor of KSP, who's looked at as you know, look at this stuff very deeply.",
            "Thinks that the KSP P is.",
            "And I do agree with him that it's it's a really nice way to do it.",
            "But it's not the only way to do it, that's for sure.",
            "It's not the only way, but it's it's a very nice nice way to do it, but I think I know what you're referring to.",
            "What year was that?",
            "Is it recent?",
            "2000 Six 1500 Jessup paper.",
            "I think Griff Griffin and still I'm not, I I should I take a look at it?",
            "Add any questions.",
            "Let's thank the speaker then."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good afternoon.",
                    "label": 0
                },
                {
                    "sent": "So I'm Larry, Karen from Duke University.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to talk to you about something called a hierarchical kernel stick breaking process, which we've applied to multitask image analysis.",
                    "label": 1
                },
                {
                    "sent": "I'll.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll explain all of that.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the problem that we're trying to examine his image analysis an as a as a component of that we have the problem of image segmentation.",
                    "label": 0
                },
                {
                    "sent": "So some of the various techniques that have been developed over the years for this problem include K means, statistical mixture modeling, spectral clustering, etc.",
                    "label": 1
                },
                {
                    "sent": "So there definitely techniques out there to do this.",
                    "label": 0
                },
                {
                    "sent": "Our objective here is to process, but each of those is performed on each image one by one.",
                    "label": 0
                },
                {
                    "sent": "Each image is analyzed in isolation.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do here is to do simultaneous processing.",
                    "label": 0
                },
                {
                    "sent": "Of a whole bunch of images.",
                    "label": 0
                },
                {
                    "sent": "The idea is that if we if we do that effectively, we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn when we analyze any one image we can learn from the other images and Moreover the sharing mechanisms that we infer between the different images tell us something about the relationship of the images, which is useful for ranking and sorting and things of that nature.",
                    "label": 0
                },
                {
                    "sent": "So that kind of motivate.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The multi task aspect of this we're going to process multiple images simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way we're going to do this is via an extension of the Dursley process or a particular form of the Dursley process, which can be represented as a kernel stick breaking process as.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the DP can be represented as a stick breaking process, and we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stand that to a kernel stick breaking process.",
                    "label": 0
                },
                {
                    "sent": "So one of them.",
                    "label": 0
                },
                {
                    "sent": "So why are we doing that?",
                    "label": 0
                },
                {
                    "sent": "In other words, we could we could do the clustering.",
                    "label": 0
                },
                {
                    "sent": "The segmentation of the images with the DP.",
                    "label": 1
                },
                {
                    "sent": "But the reason that we don't want to do that is because their sleep process assumes that all of the data samples in a given image would be exchangeable.",
                    "label": 0
                },
                {
                    "sent": "In other words, the order doesn't matter and order absolutely matters for images in the sense that whenever two pixels are near each other, you would expect them to be in the same same cluster, and that information would be ignored by dearsley process representation.",
                    "label": 0
                },
                {
                    "sent": "So some of the other issues with typical segmentation techniques.",
                    "label": 0
                },
                {
                    "sent": "Why do we want to do this?",
                    "label": 0
                },
                {
                    "sent": "Nonparametrically we don't.",
                    "label": 0
                },
                {
                    "sent": "We do not have to apriori set the number of segments the data will tell us that by itself, and Moreover, by putting this in one level more of hierarchy, we can process a bunch of images simultaneously and therefore infer into relationships and therefore do things like sort.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of images.",
                    "label": 0
                },
                {
                    "sent": "OK, so so I assume most of this audience is pretty familiar with their sleep process, but let me just remind you a little bit about it.",
                    "label": 0
                },
                {
                    "sent": "So it's a process is represented by two parameters, a positive scalar Alpha and a base measure G not.",
                    "label": 0
                },
                {
                    "sent": "And a draw from a DP is in fact a measure, so it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's a statistical measure as well, and the stick breaking representation and so the DP is characterized by two things by precision and a base measure, and the way we draw G from a DP is to draw parameters V sub H. And please pay attention to the notation 'cause we're going to extend this, so this may not be the way everybody looks at VP, but it's the way we're going to and will help you to understand K SBP, which is probably new to most people.",
                    "label": 0
                },
                {
                    "sent": "So in any case we're going to draw, we're going to draw V sub H ID from a beta distribution and then we're going to use it to drawl off, break off pieces of a stick of unit length.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is this is a stick of.",
                    "label": 0
                },
                {
                    "sent": "Like one, we're going to break off a piece of that which is a fractional length V1, and we're going to call that Pi one and then 1 -- V one will be left behind were going to draw another V2 from the beta distribution, break off another stick, call that Pi 2, etc.",
                    "label": 0
                },
                {
                    "sent": "And then these are are going to be the weight and then these atoms.",
                    "label": 0
                },
                {
                    "sent": "The Theta H is are drawn IID from the base measure G not and so.",
                    "label": 0
                },
                {
                    "sent": "Therefore G is represented in.",
                    "label": 0
                },
                {
                    "sent": "So called stick breaking form as shown here.",
                    "label": 0
                },
                {
                    "sent": "OK so and then typically we would draw parameters from G. Since this is basically a mixture model and infinite mixture model, what's going to happen is that those types of H is with large amplitude, are going to dominate, and the associated atoms are going to be preferentially selected.",
                    "label": 0
                },
                {
                    "sent": "This is going to yield a mixture model.",
                    "label": 0
                },
                {
                    "sent": "It's nonparametric in the sense that the number of mixtures is not set apriori.",
                    "label": 0
                },
                {
                    "sent": "The problem with this and the problem that we're trying to address, or one of the problems we're trying to address, is that if you draw parameters from G, the order at which you draw those parameters is irrelevant, there's no.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order information at all, and so oftentimes we do have order information in the previous talk we talked about time that data is collected sequentially in time, and we know that and you would expect that things should cluster more prominently whenever they're nearby in time.",
                    "label": 0
                },
                {
                    "sent": "So you can use a K SBP, which I'll describe to model time involving data.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to be looking at.",
                    "label": 0
                },
                {
                    "sent": "Data in an image, and so the idea is, is that if you draw features from an image and you wish to segment or cluster those features, it's reasonable to assume that features that are close together within the image are more likely to be clustered together than otherwise.",
                    "label": 0
                },
                {
                    "sent": "That information would be lost in a fiercely.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process representation, so this motivates K SBP.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is it.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I can convey convey this to you.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea here is that we're going to draw the visa batches are the exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Visa batches that we drew before from the beta distribution.",
                    "label": 0
                },
                {
                    "sent": "This is no longer ADP, so we're free to say AB.",
                    "label": 0
                },
                {
                    "sent": "You don't have to say 1A could be.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is something different than DP, but in any case we're going to draw.",
                    "label": 0
                },
                {
                    "sent": "These visa pages from a beta distribution we're going to draw an infinite number of these IID.",
                    "label": 0
                },
                {
                    "sent": "And so that that is the same.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this in the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the DP the weights on the sticks are given by this, so you just have a product of those draws from them.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is what is different here?",
                    "label": 0
                },
                {
                    "sent": "Is that we now want to take into account the location, not just so X of N represents the feature of the NTH Patch in the image and our sub N locates defines its location.",
                    "label": 0
                },
                {
                    "sent": "Let's say in two XYRIJ of the Pixel.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're using both the feature and its location.",
                    "label": 0
                },
                {
                    "sent": "And so R is a general location, identifies a general location within a within within image.",
                    "label": 0
                },
                {
                    "sent": "So the thing to notice here is that the weight on our stick breaking representation.",
                    "label": 0
                },
                {
                    "sent": "So Ji sub R which is our draw from a K SBP, has weights which are function of where you are in the image.",
                    "label": 0
                },
                {
                    "sent": "So notice that the weights are now function of R. The way that we set that up is using a kernel or K can be any kernel you like, which has a maximum value of 1 and minimum of 0.",
                    "label": 0
                },
                {
                    "sent": "So a radial basis function would be a good example.",
                    "label": 0
                },
                {
                    "sent": "So R is a general location in the image an gamma sub L. Is a basis function which locates basically locate source, enters this kernel in space.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're doing here is that these guys are drawn from the beta distribution in the usual way, but now we're going to multiply these by a kernel and what's going to happen is is that whenever R is.",
                    "label": 0
                },
                {
                    "sent": "Close to gamma sub L, This kernel is going to be near one and whenever R is far away from gammas of L, this kernel is going to go to zero OK and so now what's going to happen is is that we're going to a draw from a kernel stick.",
                    "label": 0
                },
                {
                    "sent": "Breaking process is also a stick breaking representation.",
                    "label": 0
                },
                {
                    "sent": "Atoms we have an infinite set of atoms and what's happening is is that the weights on those atoms are going to be spatially.",
                    "label": 0
                },
                {
                    "sent": "Spatially located.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within within the image.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model graphically looks like this, so on the left is a DP.",
                    "label": 0
                },
                {
                    "sent": "So in a DP we draw from a beta distribution with parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "We draw atoms from a base measure and then indicator variables are selected via a multinomial characterized by the stick weights and then with the associated parameter we draw data.",
                    "label": 0
                },
                {
                    "sent": "The distinction with the kernel stick breaking processes everything.",
                    "label": 0
                },
                {
                    "sent": "Looks the same except for we now are going to draw these gamma H is which are going to center or localize each one of those atoms within the image.",
                    "label": 0
                },
                {
                    "sent": "So Gamma said H is a 2 dimensional IJ which is going to locate the.",
                    "label": 0
                },
                {
                    "sent": "The Atom or localize the Atom in space, and So what happens here now is that the observed data the observed data is a function of where you are.",
                    "label": 0
                },
                {
                    "sent": "So our sub N is the location of the NTH sample annexa Ben is associated feature vector.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions that make any sense?",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so So what are the properties of this kernel stick breaking process?",
                    "label": 0
                },
                {
                    "sent": "So let's assume that you draw from that distribution and \u03c0 sub H is are the weights of that stick breaking representation, which are now a function of R. And now let us draw parameters we're going to draw from the stick breaking representation Colonel Stick breaking representation, and now we're going to draw parameters at the location R&R Prime and the question is how are they correlated?",
                    "label": 0
                },
                {
                    "sent": "With each other.",
                    "label": 0
                },
                {
                    "sent": "So what you would expect is that as R&R prime get near each other, the correlation between the parameters is going to be high and whenever our in our prime are distant from each other, their correlation goes to zero.",
                    "label": 0
                },
                {
                    "sent": "We can write it down.",
                    "label": 0
                },
                {
                    "sent": "It turns out analytically.",
                    "label": 0
                },
                {
                    "sent": "We can prove that this is this is true.",
                    "label": 0
                },
                {
                    "sent": "So basically what the kernel stick breaking processes doing again is saying that if you have an image, that image is characterized by a bunch of features.",
                    "label": 0
                },
                {
                    "sent": "What this is saying.",
                    "label": 0
                },
                {
                    "sent": "Is that it is likely that if 2 features are nearby in the image that they will be drawn from the model with the same parameter and therefore be in the same cluster and whenever 2 features are more separated from each other within the image, the correlation between the atoms is going to diminish and therefore it's likely that they will be indistinct segm.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, OK, so that's that's the kernel stick breaking process applied to images.",
                    "label": 0
                },
                {
                    "sent": "We can apply this to one image at a time, but we oftentimes have many images and it would be useful if we can process or segment all of the images simultaneously, because we're able to share information between the different images and Moreover were able to learn the interrelationships between between the different images as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is going to be a technique we're going to use too.",
                    "label": 0
                },
                {
                    "sent": "Not only segment images, but sort images and do that simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it turns out that this is actually pretty simple if you know about the HTP, then doing this is actually not a big deal, and the HTP, what we do is we have two levels.",
                    "label": 0
                },
                {
                    "sent": "We first draw G from ADP with base measure G not.",
                    "label": 0
                },
                {
                    "sent": "Which tells us with probability 1G is discrete.",
                    "label": 0
                },
                {
                    "sent": "And so in the HDP this measure G is then used as the base measure for for DP at the next level, the only thing we're doing here, which is different, which is why we call this a hierarchical Colonel Sacred hierarchical kernel stick breaking representation is that instead of, it's really basically the same thing as HTP, except that this G is now put into the into the H.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And into the CCP.",
                    "label": 0
                },
                {
                    "sent": "OK, so the model looks like this so that complicated thing.",
                    "label": 0
                },
                {
                    "sent": "And now I'm just going to call that KSP.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to draw a CSB.",
                    "label": 0
                },
                {
                    "sent": "P I'm going to draw a G from a case PP for each of my images.",
                    "label": 0
                },
                {
                    "sent": "So each image I'm going to do this on each of my images.",
                    "label": 0
                },
                {
                    "sent": "The base measure of each of those case CPS will be drawn from a DP which will allow me.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To share, OK, this is what the graphical representation looks like.",
                    "label": 0
                },
                {
                    "sent": "I'm running out of time, so I'll kind of I'll skip this, but it's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something that is something that you can perform inference on relatively straightforwardly every every level in the hierarchy is in the conjugate exponential family, except for one.",
                    "label": 0
                },
                {
                    "sent": "So we want to do this fast, so we have a variational based solution for every step in the hierarchy except for one, at which point we sample.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have analytic updates at every point except for one, where we do Monte Carlo expectation Monte Carlo type of.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of sampling, but the inversion.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The processing is actually quite fast.",
                    "label": 0
                },
                {
                    "sent": "We're able to process a lot of images, I'll show you.",
                    "label": 0
                },
                {
                    "sent": "OK, so to show you how this works we processed some data from the Microsoft Research came.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which image database?",
                    "label": 0
                },
                {
                    "sent": "So hopefully that is you can see that.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking at here is different types of data.",
                    "label": 0
                },
                {
                    "sent": "Clouds, buildings, countryside, etc and we're looking at which atoms are dominant for each type of image.",
                    "label": 0
                },
                {
                    "sent": "So there are seven different types of images.",
                    "label": 0
                },
                {
                    "sent": "I'll show you what they look like an for each of them.",
                    "label": 0
                },
                {
                    "sent": "We're going to represent them as a mixture mixture model and these are the atoms that we find on that mixture model, and we're also imposing that spatial structure from the CCP.",
                    "label": 0
                },
                {
                    "sent": "OK, so by looking at that you can see that the atoms are fairly distinct for the diff.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Types of images which you would think would allow us to do a fairly good job of not only clustering the images, but clustering across images, which is what we really want to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just to show you visually what the atoms look like, so these are.",
                    "label": 0
                },
                {
                    "sent": "What you're looking at here is.",
                    "label": 0
                },
                {
                    "sent": "This is the 4th 4th Atom.",
                    "label": 0
                },
                {
                    "sent": "And what I'm doing is I'm showing you a particular image where it goes black.",
                    "label": 0
                },
                {
                    "sent": "That means that it was not included in that Atom.",
                    "label": 0
                },
                {
                    "sent": "So what this tells you is the 4th Atom is kind of a Sky Atom.",
                    "label": 0
                },
                {
                    "sent": "The 31st Atom is a cloud Atom.",
                    "label": 0
                },
                {
                    "sent": "What we're kind of pleased with is like, for example the 38th Atom does a very nice job of representing buildings and houses, so you can see that if you look at where the 38th Adam goes to it does a really nice job of pulling out these buildings even under fairly complicated situations the.",
                    "label": 0
                },
                {
                    "sent": "33rd Adam seems to be a face Adam.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, OK. And then this is an example of the of the segmentation that you get.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking at here and I'm just about done, is a particular image and its associated segmentation as constituted through HK SPP.",
                    "label": 0
                },
                {
                    "sent": "An HTP.",
                    "label": 0
                },
                {
                    "sent": "So the only distinction between these is that in the HK SPP were imposing the belief that proximate pixels should be in the same cluster, whereas an HTP we do not.",
                    "label": 0
                },
                {
                    "sent": "HTP does perform fairly well, but there are several cases for which it really does inferior job relative to HBP.",
                    "label": 0
                },
                {
                    "sent": "So for here example, the face with HBP.",
                    "label": 0
                },
                {
                    "sent": "Does it really seem to have like a skin Atom that it pulls out the face pretty nicely?",
                    "label": 0
                },
                {
                    "sent": "Where HDP does not OK so so there are two products of this work.",
                    "label": 0
                },
                {
                    "sent": "This is 1 product, one product is we can cluster the images and Moreover we can learn the atoms of that clustering.",
                    "label": 0
                },
                {
                    "sent": "On the fly.",
                    "label": 0
                },
                {
                    "sent": "So we're learning the atoms and clustering.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simultaneously.",
                    "label": 0
                },
                {
                    "sent": "But what we really want to do is you give me and.",
                    "label": 0
                },
                {
                    "sent": "And then I will find 10 other images that I say you should also find interesting.",
                    "label": 0
                },
                {
                    "sent": "In other words, I want to cluster images, not just segment them, and So what you're looking at here is for those seven different classes of.",
                    "label": 0
                },
                {
                    "sent": "Image is one of the products that we get is how interrelated the different data are.",
                    "label": 0
                },
                {
                    "sent": "The way that we this is what this is a representation of is the sharing mechanism that was uncovered within the inference process.",
                    "label": 0
                },
                {
                    "sent": "So this comes directly is not a post processing step.",
                    "label": 0
                },
                {
                    "sent": "This comes directly as a product of the HKS BP and you can see that it does a pretty nice job.",
                    "label": 0
                },
                {
                    "sent": "What we would like to see is this to be blocked diagonal, which would imply that each of the images were shared within each of the 7.",
                    "label": 0
                },
                {
                    "sent": "Classes of images, but it generally does well, and if I showed you each of the images where we were that doesn't occur.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It actually is.",
                    "label": 0
                },
                {
                    "sent": "Is is reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the last slide so that the kernel stick breaking processes.",
                    "label": 1
                },
                {
                    "sent": "A new technique developed by David Duke and extended here for image segmentation.",
                    "label": 1
                },
                {
                    "sent": "It removes the exchangeability assumption in the daresay process.",
                    "label": 0
                },
                {
                    "sent": "We are interested in not only segmenting a single image, but in segmenting multiple images.",
                    "label": 0
                },
                {
                    "sent": "So a natural extension of this is the hierarchical KSP, which we've implemented, and it seems to perform well.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the inference is fast.",
                    "label": 1
                },
                {
                    "sent": "We want to solve a lot of images.",
                    "label": 0
                },
                {
                    "sent": "We want to analyze a lot of images, so we really don't do not want to use an MCMC type of a solution which we have, by the way, but we wanted to use a variational base solution.",
                    "label": 1
                },
                {
                    "sent": "And with the model, every step in the hierarchy satisfies the requirements of VB, except for one at which we sample and still get a pretty efficient code.",
                    "label": 0
                },
                {
                    "sent": "So anyway, thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Can you you showed a plate representation of the KSB?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He way at the beginning anyway made me think of mixtures of experts, right so that they could have a direct arrow from our to the outputs, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yes.",
                    "label": 0
                },
                {
                    "sent": "So that's right.",
                    "label": 0
                },
                {
                    "sent": "So we have applied this.",
                    "label": 0
                },
                {
                    "sent": "A mixture of experts.",
                    "label": 0
                },
                {
                    "sent": "So this is a way to do nonparametric mixture of experts and we were doing that.",
                    "label": 0
                },
                {
                    "sent": "But you're right, quite right.",
                    "label": 0
                },
                {
                    "sent": "It's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good way to do that.",
                    "label": 0
                },
                {
                    "sent": "That's a good way to do that.",
                    "label": 0
                },
                {
                    "sent": "There's some we could talk offline where there are better ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "But anyway, you're right.",
                    "label": 0
                },
                {
                    "sent": "So I might be missing something to do show that your kernel stick breaking process results in sticks that sum to one?",
                    "label": 0
                },
                {
                    "sent": "Yes, I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't, I didn't.",
                    "label": 0
                },
                {
                    "sent": "I saw I had five minutes with about 10 minutes of material, so I kind of skipped that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But yeah, all the sticks sum to one.",
                    "label": 0
                },
                {
                    "sent": "And the reason that that's true is because the kernel whenever whenever R equals gamma L. In other words, when you're on the center location, the kernel is 1 and whenever you move off it goes to 0.",
                    "label": 0
                },
                {
                    "sent": "Because of that property, the sticks sum to one.",
                    "label": 0
                },
                {
                    "sent": "And then you can use any kernel you like to make that happen.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "Can you contrast this with Jim Griffin's auto based dependent richly processes?",
                    "label": 0
                },
                {
                    "sent": "Since I'm not that familiar with it, I want you want to tell me a little bit about it, and then I will.",
                    "label": 0
                },
                {
                    "sent": "I mean, we're I. I'm not familiar with that.",
                    "label": 0
                },
                {
                    "sent": "OK so?",
                    "label": 0
                },
                {
                    "sent": "You so the atoms are also associated with locations.",
                    "label": 0
                },
                {
                    "sent": "Anne, this you have a set of these as well associated with each Atom.",
                    "label": 0
                },
                {
                    "sent": "But the ordering of the atoms in the stick breaking construction depends on the location of the atoms and where you are in the space.",
                    "label": 0
                },
                {
                    "sent": "So you want to have atoms which are close by to be put into the sticking construction.",
                    "label": 0
                },
                {
                    "sent": "First there is there there there are other techniques I think I know what you're talking about there.",
                    "label": 0
                },
                {
                    "sent": "There are other people who have tried to remove the exchange ability and DP and.",
                    "label": 0
                },
                {
                    "sent": "Let me just say this, David Johnson, who was the inventor of KSP, who's looked at as you know, look at this stuff very deeply.",
                    "label": 0
                },
                {
                    "sent": "Thinks that the KSP P is.",
                    "label": 0
                },
                {
                    "sent": "And I do agree with him that it's it's a really nice way to do it.",
                    "label": 0
                },
                {
                    "sent": "But it's not the only way to do it, that's for sure.",
                    "label": 0
                },
                {
                    "sent": "It's not the only way, but it's it's a very nice nice way to do it, but I think I know what you're referring to.",
                    "label": 0
                },
                {
                    "sent": "What year was that?",
                    "label": 0
                },
                {
                    "sent": "Is it recent?",
                    "label": 0
                },
                {
                    "sent": "2000 Six 1500 Jessup paper.",
                    "label": 0
                },
                {
                    "sent": "I think Griff Griffin and still I'm not, I I should I take a look at it?",
                    "label": 0
                },
                {
                    "sent": "Add any questions.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker then.",
                    "label": 0
                }
            ]
        }
    }
}