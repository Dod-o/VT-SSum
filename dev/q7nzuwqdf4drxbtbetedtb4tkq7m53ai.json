{
    "id": "q7nzuwqdf4drxbtbetedtb4tkq7m53ai",
    "title": "Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering",
    "info": {
        "author": [
            "Justin Eldridge, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_eldridge_hierarchical_clustering/",
    "segmentation": [
        [
            "Hello, I'm Justin Eldridge.",
            "This is work done with Mikhail Belkin and you see Wong."
        ],
        [
            "So at a very high level, clustering right is identifying structure and data by grouping it into clusters, and a typical assumption that we may have is that our data is drawn from some density.",
            "And in this setting where we have it and see it's through clustering that we hope to recover some structure of the density right?",
            "And so you might immediately ask."
        ],
        [
            "What do we mean precisely by structure?",
            "We have to define this in a more rigorous way.",
            "But supposing we have some definition in mind, we can ask three very fundamental questions about clustering, and the first is what properties ensure that an algorithm captures this structure.",
            "OK, so we want to say that if a clustering method has properties XY and Z, then in the limit of a lot of data, the clustering will start to resemble the structure that we want.",
            "#2 how close is a clustering to this ideal structure?",
            "OK, we want to measure the distance between this finite discrete object, this clustering and this potentially infinite continuous object, which is the structure.",
            "And Lastly, and certainly not least, do algorithms with these properties that we have in mind from.",
            "Right question one, the algorithms actually exist.",
            "Or if we have a distance in mind from question two to algorithms exist which converge in this distance.",
            "So in this talk, when I speak of structure, I'm going to be referring specifically to an object called the density cluster tree, and I'll define this a bit more in a second.",
            "But for announce Vice is to say that this is a infinite continuous hierarchy that captures the clusters of a density at every level.",
            "As far as.",
            "Question one goes, what properties ensure that an algorithm captures the density cluster tree well previously Hartigan consistency was proposed as an answer to this question.",
            "And as I'll show in this talk, and it has been discussed a bit in the literature, Hartigan consistency is in fact an insufficient notion.",
            "It's it's not a sufficient answer to this question.",
            "So our first contribution is going to be to introduce 2 new notions which replace hard again consistency, called minimality and separation.",
            "And so these will sort of clean up some of the issues that surround her to get inconsistency.",
            "As far as question two goes, well previously there wasn't really a formalism for discussing the distance between the density cluster tree an an estimate.",
            "So will we will introduce a merge distortion metric for quantifying the distance between these two objects and the nice thing about our metric is that it will be equivalent to convergence in this metric will be equivalent to our properties of minimality and separation.",
            "And Lastly, do algorithms with these properties of minimality and separation exist?",
            "Yeah, they do."
        ],
        [
            "So.",
            "Let's get started on defining this object that we want to recover the density cluster tree.",
            "So this goes back to work by Hartigan and Wishart before him.",
            "On what a density or what a cluster of a density actually is.",
            "So they say a cluster of a density is a very natural idea, is just a region of high probability so."
        ],
        [
            "What that looks like is we'll take our density, will pick some level Lambda one, and we'll just slice our density at this level and will look at the points where the density is at is at least Lambda one.",
            "And then we'll look at the connected components of this set, and those will be our clusters.",
            "So right the the high density clusters at level Lambda one are the connected components of this super level set an for this choice of Lambda one we get two clusters.",
            "Here in here of course Lambda one was arbitrary.",
            "We could have chosen landed."
        ],
        [
            "Two and we get these two clusters.",
            "Or even Lambda."
        ],
        [
            "Three, and we get a single cluster, and so we kind of conclude is that."
        ],
        [
            "A cluster is a connected component of a super level set for any level Lambda, so this density has infinitely many high density clusters."
        ],
        [
            "And what we immediately see is that these high density clusters have a hierarchical structure, meaning that clusters from higher levels nest within clusters from lower levels.",
            "OK, and."
        ],
        [
            "This leads us to think of this infinite collection of clusters as a tree, and we call this tree the density cluster tree of the density F. OK, so specifically, this green tree is what we have in mind when we speak of the density cluster tree.",
            "We can also think of it as a sort of function that takes the level Lambda and Maps it to the connected components of the Super level set F at least Lambda.",
            "Anne."
        ],
        [
            "To be clear, it's this tree that we hope to recover by clustering data sampled from F. So."
        ],
        [
            "In practice, what we have we don't have access to the density itself, but we have access to samples from the density and so we draw a set of samples XN of endpoints from F, and we apply a clustering algorithm.",
            "So we have our points here on the surface of FX1A1, and so forth, and our clustering algorithm returns a collection of so-called empirical clusters."
        ],
        [
            "And by the assumption that we're using a hierarchical clustering algorithm, these clusters will have a nested structure.",
            "So they'll nest within one another."
        ],
        [
            "Again, will want to."
        ],
        [
            "Of this as a tree and empirical cluster tree.",
            "Right, so we'll have a tree where each node in this tree is a cluster.",
            "Right?",
            "So a quick note about how."
        ],
        [
            "We represent trees.",
            "In this talk, you'll notice that, for example A3 here it appears not only in this cluster, but in this cluster in this cluster.",
            "In here, in here, in any ancestor of this cluster, A3 appears, and so that."
        ],
        [
            "It's kind of redundant and clumsy, so I'm just going to omit these redundant labels and just show.",
            "Points."
        ],
        [
            "Where they sort of 1st appear, so that's.",
            "What I'm deleting here?",
            "Right so.",
            "So just to be very clear, this is a cluster and it contains not only a one but A1A2 and A3.",
            "Right so."
        ],
        [
            "It will be useful to talk about the height of a cluster or the height of a node in this tree and how will define the height is as the density of the lowest point in the cluster.",
            "So for this cluster it contains a 18283.",
            "We look over here the density of the lowest one is a one, so the height of this node is a one.",
            "And will draw this tree with its nodes placed according to height and in fact."
        ],
        [
            "You can.",
            "And for all the trees in this talk, you'll be able to cross reference the height of the tree with the density."
        ],
        [
            "OK, so the goal of an algorithm that tries to recover the density cluster trees as we get more and more data.",
            "This empirical tree on the right should start to resemble this green ideal tree on the left right?",
            "So we gather."
        ],
        [
            "More and more data and this tree on the right starts to become more and more."
        ],
        [
            "Fine OK."
        ],
        [
            "So back to question one, what properties ensure that an algorithm does this?",
            "That as we gather more and more data, it starts to capture the density cluster tree?",
            "And John Hartigan, statistician in 1981, while he raised this question, then he answered it.",
            "He said, it's hard to get consistency.",
            "Well, you just said it's consistency and we call it Hartigan consistency, right?",
            "So informally, what this says is that clusters which are disjoint in the density in the true tree.",
            "These natural clusters, they should be separated in the empirical tree.",
            "As we gather more and more data.",
            "So let's make this."
        ],
        [
            "Can create.",
            "We have a density here and we've labeled clusters A&B, and they're disjoint.",
            "At some level, Lambda here.",
            "And we have our empirical tree over here to the right.",
            "So."
        ],
        [
            "Hartigan says find a N which we defined to be the smallest empirical cluster containing a restricted to our sample.",
            "So a restricted to the samples just A1A2 and A3.",
            "We look at the empirical tree.",
            "We find the smallest cluster containing those three points, at least those three points.",
            "And it's this cluster right here."
        ],
        [
            "We do the same for B.",
            "Find the small."
        ],
        [
            "Cluster containing B1B2B3.",
            "It's this cluster."
        ],
        [
            "And harder."
        ],
        [
            "It says that as you gather more and more data, the probability that AN&B in these two clusters here are disjoint should tend to one.",
            "So will gather more data."
        ],
        [
            "So this is the limit tree.",
            "We find a N we find BN."
        ],
        [
            "There did."
        ],
        [
            "Joint and so we can say that."
        ],
        [
            "History."
        ],
        [
            "Does not violate Hartigan consistency?",
            "As an example of a tree which does violate Hartigan consistency, let's swap these two point."
        ],
        [
            "Tear.",
            "We find a in."
        ],
        [
            "Now it has to be done here because to include a two we have to include this branch.",
            "For being."
        ],
        [
            "We have to include this branch as well as this branch.",
            "They ova."
        ],
        [
            "Lap and so this tree violates hard again consistency.",
            "OK, so."
        ],
        [
            "Hartigan proposed this as an answer to this first question.",
            "This first fundamental question about clustering.",
            "Unfortunately, we'll see in a second and has been and has been discussed a bit in the literature.",
            "Hartigan consistency is not sufficient.",
            "But I want to note that it's still a nice property to have.",
            "It's just not strong enough.",
            "Does Hartigan consistency address question two?",
            "No, it's a limit property.",
            "It doesn't quantify anything as such.",
            "And the algorithms exist which are hard.",
            "Again consistent.",
            "It's a little bit of an interesting history.",
            "Hard again, right?",
            "He introduced this notion of consistency and immediately he asked if single linkage clustering is consistent and he showed that it's not in dimensions larger than one.",
            "And equals one.",
            "It is consistent in larger dimensions.",
            "It's only fractionally consistent, but that's not quite what we want.",
            "Then 30 years went by with crickets, and several algorithms showed to be consistent or shown to be consistent.",
            "In the recent years, including this robust single leakage algorithm of children Dasgupta and that reprinting algorithm of puthuff in Von Luxburg.",
            "Bye."
        ],
        [
            "But as I've said, Hartigan consistency is not sufficient, and the reason is, it lacks a strong notion of connectedness.",
            "So to show what I mean by that, we look at this empirical tree that we just saw is sort of nice.",
            "It looks like the true tree that we want to recover.",
            "But will sort of start."
        ],
        [
            "Unzipping this tree.",
            "I'm."
        ],
        [
            "We're doing sort of it."
        ],
        [
            "Is.",
            "Deepening this gap between A&B and so this is, say, a tree that our clustering algorithm has returned in the limit."
        ],
        [
            "Um?"
        ],
        [
            "And the claim is that your algorithm could return this tree.",
            "After sampling infinite infinitely many points and it's fine, Hartigan says it's great.",
            "It's still hard.",
            "Again, it doesn't violate Hartigan consistency, even though it looks very different than the true tree."
        ],
        [
            "An right to support this claim?",
            "Just look and find."
        ],
        [
            "Am find BN they're disjoint.",
            "Hartigan says everything is great.",
            "What about this tree?"
        ],
        [
            "It's also badly over segmented, but we've kind of blown up a."
        ],
        [
            "But again, we find ANBN they're disjoint, everything's fine according to heart again, so something's not quite right with Hartigan consistency that take."
        ],
        [
            "The way is that a tree or an algorithm can be hard, again consistent, but be very different from the true tree.",
            "OK, so we need something stronger."
        ],
        [
            "The reason that these issues are permitted intuitively is that Hartigan consistency lacks connectedness, so we need something stronger, and what we'll do is we will introduce a property called minimality to address connectedness directly.",
            "Will also introduce the notion called separation, which is slightly weaker than hard against notion, but is sort of the symmetric definition to minimality.",
            "Together, these will imply Hartigan consistency will be stronger and they will clean up some of the issues with over segmentation that Hartigan consistency permits."
        ],
        [
            "So our first property is minimality.",
            "And the intuition behind Maletis say we have a cluster C in our density and this cluster exists at level Lambda OK.",
            "In our empirical clustering.",
            "We want C to exist at about level Lambda."
        ],
        [
            "But if we look at our empirical clustering, it doesn't.",
            "This is badly over segmented as we saw before and the problem as we know is that C is connected at some level Delta less than Lambda.",
            "So what minimality says is that C restricted to the data should be connected at.",
            "We allow some slack, Lambda minus Delta, but Delta should go to zero as we gather more points.",
            "So what connectedness is saying is we're going to take this.",
            "Level at which she is connected and we're going to force it."
        ],
        [
            "Be pushed up."
        ],
        [
            "To about the right level."
        ],
        [
            "In the limit of infinite data.",
            "So this I want to point out that this.",
            "Is designed to address this over segmentation issue directly."
        ],
        [
            "And this is the sort of formal definition that I can come back to if there are questions.",
            "Separation goes from the other direction.",
            "It says we have clusters A&B and they merge, or they are separated above level mu.",
            "In the density they should be separated above level mu.",
            "In the empirical tree, and again we allow some slackness."
        ],
        [
            "Delta.",
            "And."
        ],
        [
            "Delta should go to zero as we go."
        ],
        [
            "More data."
        ],
        [
            "Here A&B have been merged too soon, but as we gather more data.",
            "We're going to force this to go down.",
            "OK.",
            "So the theorem we have is that minimality and separation together imply Hartigan consistency there strictly stronger than it, because Hartigan consistency does not imply minimality and separation."
        ],
        [
            "So we've proposed minimality and separation as properties which answer this first question.",
            "Minimality addresses the shortcomings of Hartigan consistency directly and together they imply Hartigan consistency.",
            "Now we turn our attention to Question 2.",
            "How close is a clustering to the ideal clustering tree?",
            "How do we measure this distance and to do this will introduce a merge distortion metric and it will in fact imply minimally and separation.",
            "So the."
        ],
        [
            "The intuition behind this metric is we look at the merge height.",
            "So we look at any two points in the density in this part of the density and we look at the height at which they merge.",
            "We call this M of a BK, so they merged about here.",
            "And that's the ideal merge height.",
            "In the empirical tree."
        ],
        [
            "We have the empirical merge height which is way down here called em hat.",
            "And what minimality?"
        ],
        [
            "And separation together will imply."
        ],
        [
            "Is that the empirical merge height converge to the true merge height as we gather more data, so minimality pushes it up?"
        ],
        [
            "Separation pushes."
        ],
        [
            "Down and so as we get."
        ],
        [
            "Our data this is distance here with this disparity."
        ],
        [
            "Will converge to 0.",
            "So we define a disk."
        ],
        [
            "Between the tree based on this we look at every pair of points in our sample.",
            "We look at the disparity between the true merge height and the empirical merge height, and we define the distance between these two objects.",
            "The empirical tree and the true tree to be the maximum such disparity.",
            "OK, so this is the distortion of a correspondence basically.",
            "And so the theorem that we have."
        ],
        [
            "That convergence in this metric is equivalent to the uniform versions of minimality and separation.",
            "Just sort of nice."
        ],
        [
            "So we've introduced minimally separation this merge distortion metric.",
            "Do algorithms actually exist which converge in this metric, or which have properties of minimally in separation?",
            "And they do we analyze two in our paper this robust single linkage algorithm from Chuck Berry and US Gupta and a split tree place based clustering algorithms from computational topology?"
        ],
        [
            "Robust single linkage is inelegant generalization of single linkage which incorporates density information so it combats this chaining effect that single linkage just notorious for it is hard to get consistent.",
            "An authors in fact went above that showed that it has some sort of connectedness and separation guarantees, and we do a little bit of footwork to put it in our formalism and show that given some smoothness conditions on the density robust single linkage converges to the true cluster tree in our metrics."
        ],
        [
            "There's some future work that I think I'll.",
            "Skip for now.",
            "In the interest of time."
        ],
        [
            "And leave you with this summary."
        ],
        [
            "We've replaced Hartigan consistency with two notions which are more comprehensive.",
            "We introduce emerge, distortion, metric, and the nice thing is that these two things are equivalent, and this allows us to measure the distance between these objects.",
            "An are these definitions actually feasable?",
            "Yeah, they are so."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, I'm Justin Eldridge.",
                    "label": 0
                },
                {
                    "sent": "This is work done with Mikhail Belkin and you see Wong.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at a very high level, clustering right is identifying structure and data by grouping it into clusters, and a typical assumption that we may have is that our data is drawn from some density.",
                    "label": 1
                },
                {
                    "sent": "And in this setting where we have it and see it's through clustering that we hope to recover some structure of the density right?",
                    "label": 0
                },
                {
                    "sent": "And so you might immediately ask.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we mean precisely by structure?",
                    "label": 1
                },
                {
                    "sent": "We have to define this in a more rigorous way.",
                    "label": 0
                },
                {
                    "sent": "But supposing we have some definition in mind, we can ask three very fundamental questions about clustering, and the first is what properties ensure that an algorithm captures this structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to say that if a clustering method has properties XY and Z, then in the limit of a lot of data, the clustering will start to resemble the structure that we want.",
                    "label": 1
                },
                {
                    "sent": "#2 how close is a clustering to this ideal structure?",
                    "label": 0
                },
                {
                    "sent": "OK, we want to measure the distance between this finite discrete object, this clustering and this potentially infinite continuous object, which is the structure.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, and certainly not least, do algorithms with these properties that we have in mind from.",
                    "label": 0
                },
                {
                    "sent": "Right question one, the algorithms actually exist.",
                    "label": 0
                },
                {
                    "sent": "Or if we have a distance in mind from question two to algorithms exist which converge in this distance.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, when I speak of structure, I'm going to be referring specifically to an object called the density cluster tree, and I'll define this a bit more in a second.",
                    "label": 0
                },
                {
                    "sent": "But for announce Vice is to say that this is a infinite continuous hierarchy that captures the clusters of a density at every level.",
                    "label": 0
                },
                {
                    "sent": "As far as.",
                    "label": 0
                },
                {
                    "sent": "Question one goes, what properties ensure that an algorithm captures the density cluster tree well previously Hartigan consistency was proposed as an answer to this question.",
                    "label": 1
                },
                {
                    "sent": "And as I'll show in this talk, and it has been discussed a bit in the literature, Hartigan consistency is in fact an insufficient notion.",
                    "label": 0
                },
                {
                    "sent": "It's it's not a sufficient answer to this question.",
                    "label": 0
                },
                {
                    "sent": "So our first contribution is going to be to introduce 2 new notions which replace hard again consistency, called minimality and separation.",
                    "label": 0
                },
                {
                    "sent": "And so these will sort of clean up some of the issues that surround her to get inconsistency.",
                    "label": 1
                },
                {
                    "sent": "As far as question two goes, well previously there wasn't really a formalism for discussing the distance between the density cluster tree an an estimate.",
                    "label": 0
                },
                {
                    "sent": "So will we will introduce a merge distortion metric for quantifying the distance between these two objects and the nice thing about our metric is that it will be equivalent to convergence in this metric will be equivalent to our properties of minimality and separation.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, do algorithms with these properties of minimality and separation exist?",
                    "label": 0
                },
                {
                    "sent": "Yeah, they do.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's get started on defining this object that we want to recover the density cluster tree.",
                    "label": 1
                },
                {
                    "sent": "So this goes back to work by Hartigan and Wishart before him.",
                    "label": 0
                },
                {
                    "sent": "On what a density or what a cluster of a density actually is.",
                    "label": 0
                },
                {
                    "sent": "So they say a cluster of a density is a very natural idea, is just a region of high probability so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What that looks like is we'll take our density, will pick some level Lambda one, and we'll just slice our density at this level and will look at the points where the density is at is at least Lambda one.",
                    "label": 0
                },
                {
                    "sent": "And then we'll look at the connected components of this set, and those will be our clusters.",
                    "label": 1
                },
                {
                    "sent": "So right the the high density clusters at level Lambda one are the connected components of this super level set an for this choice of Lambda one we get two clusters.",
                    "label": 0
                },
                {
                    "sent": "Here in here of course Lambda one was arbitrary.",
                    "label": 0
                },
                {
                    "sent": "We could have chosen landed.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two and we get these two clusters.",
                    "label": 0
                },
                {
                    "sent": "Or even Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three, and we get a single cluster, and so we kind of conclude is that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A cluster is a connected component of a super level set for any level Lambda, so this density has infinitely many high density clusters.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we immediately see is that these high density clusters have a hierarchical structure, meaning that clusters from higher levels nest within clusters from lower levels.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This leads us to think of this infinite collection of clusters as a tree, and we call this tree the density cluster tree of the density F. OK, so specifically, this green tree is what we have in mind when we speak of the density cluster tree.",
                    "label": 1
                },
                {
                    "sent": "We can also think of it as a sort of function that takes the level Lambda and Maps it to the connected components of the Super level set F at least Lambda.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be clear, it's this tree that we hope to recover by clustering data sampled from F. So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, what we have we don't have access to the density itself, but we have access to samples from the density and so we draw a set of samples XN of endpoints from F, and we apply a clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have our points here on the surface of FX1A1, and so forth, and our clustering algorithm returns a collection of so-called empirical clusters.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by the assumption that we're using a hierarchical clustering algorithm, these clusters will have a nested structure.",
                    "label": 0
                },
                {
                    "sent": "So they'll nest within one another.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, will want to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this as a tree and empirical cluster tree.",
                    "label": 1
                },
                {
                    "sent": "Right, so we'll have a tree where each node in this tree is a cluster.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So a quick note about how.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We represent trees.",
                    "label": 0
                },
                {
                    "sent": "In this talk, you'll notice that, for example A3 here it appears not only in this cluster, but in this cluster in this cluster.",
                    "label": 1
                },
                {
                    "sent": "In here, in here, in any ancestor of this cluster, A3 appears, and so that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's kind of redundant and clumsy, so I'm just going to omit these redundant labels and just show.",
                    "label": 0
                },
                {
                    "sent": "Points.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where they sort of 1st appear, so that's.",
                    "label": 0
                },
                {
                    "sent": "What I'm deleting here?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So just to be very clear, this is a cluster and it contains not only a one but A1A2 and A3.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It will be useful to talk about the height of a cluster or the height of a node in this tree and how will define the height is as the density of the lowest point in the cluster.",
                    "label": 1
                },
                {
                    "sent": "So for this cluster it contains a 18283.",
                    "label": 0
                },
                {
                    "sent": "We look over here the density of the lowest one is a one, so the height of this node is a one.",
                    "label": 0
                },
                {
                    "sent": "And will draw this tree with its nodes placed according to height and in fact.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "And for all the trees in this talk, you'll be able to cross reference the height of the tree with the density.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the goal of an algorithm that tries to recover the density cluster trees as we get more and more data.",
                    "label": 1
                },
                {
                    "sent": "This empirical tree on the right should start to resemble this green ideal tree on the left right?",
                    "label": 0
                },
                {
                    "sent": "So we gather.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More and more data and this tree on the right starts to become more and more.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fine OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So back to question one, what properties ensure that an algorithm does this?",
                    "label": 1
                },
                {
                    "sent": "That as we gather more and more data, it starts to capture the density cluster tree?",
                    "label": 0
                },
                {
                    "sent": "And John Hartigan, statistician in 1981, while he raised this question, then he answered it.",
                    "label": 0
                },
                {
                    "sent": "He said, it's hard to get consistency.",
                    "label": 0
                },
                {
                    "sent": "Well, you just said it's consistency and we call it Hartigan consistency, right?",
                    "label": 0
                },
                {
                    "sent": "So informally, what this says is that clusters which are disjoint in the density in the true tree.",
                    "label": 1
                },
                {
                    "sent": "These natural clusters, they should be separated in the empirical tree.",
                    "label": 0
                },
                {
                    "sent": "As we gather more and more data.",
                    "label": 0
                },
                {
                    "sent": "So let's make this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can create.",
                    "label": 0
                },
                {
                    "sent": "We have a density here and we've labeled clusters A&B, and they're disjoint.",
                    "label": 0
                },
                {
                    "sent": "At some level, Lambda here.",
                    "label": 0
                },
                {
                    "sent": "And we have our empirical tree over here to the right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hartigan says find a N which we defined to be the smallest empirical cluster containing a restricted to our sample.",
                    "label": 1
                },
                {
                    "sent": "So a restricted to the samples just A1A2 and A3.",
                    "label": 0
                },
                {
                    "sent": "We look at the empirical tree.",
                    "label": 0
                },
                {
                    "sent": "We find the smallest cluster containing those three points, at least those three points.",
                    "label": 0
                },
                {
                    "sent": "And it's this cluster right here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do the same for B.",
                    "label": 0
                },
                {
                    "sent": "Find the small.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cluster containing B1B2B3.",
                    "label": 0
                },
                {
                    "sent": "It's this cluster.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And harder.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It says that as you gather more and more data, the probability that AN&B in these two clusters here are disjoint should tend to one.",
                    "label": 0
                },
                {
                    "sent": "So will gather more data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the limit tree.",
                    "label": 0
                },
                {
                    "sent": "We find a N we find BN.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There did.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Joint and so we can say that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "History.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does not violate Hartigan consistency?",
                    "label": 0
                },
                {
                    "sent": "As an example of a tree which does violate Hartigan consistency, let's swap these two point.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tear.",
                    "label": 0
                },
                {
                    "sent": "We find a in.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it has to be done here because to include a two we have to include this branch.",
                    "label": 0
                },
                {
                    "sent": "For being.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have to include this branch as well as this branch.",
                    "label": 0
                },
                {
                    "sent": "They ova.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lap and so this tree violates hard again consistency.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hartigan proposed this as an answer to this first question.",
                    "label": 0
                },
                {
                    "sent": "This first fundamental question about clustering.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we'll see in a second and has been and has been discussed a bit in the literature.",
                    "label": 0
                },
                {
                    "sent": "Hartigan consistency is not sufficient.",
                    "label": 1
                },
                {
                    "sent": "But I want to note that it's still a nice property to have.",
                    "label": 0
                },
                {
                    "sent": "It's just not strong enough.",
                    "label": 0
                },
                {
                    "sent": "Does Hartigan consistency address question two?",
                    "label": 1
                },
                {
                    "sent": "No, it's a limit property.",
                    "label": 0
                },
                {
                    "sent": "It doesn't quantify anything as such.",
                    "label": 1
                },
                {
                    "sent": "And the algorithms exist which are hard.",
                    "label": 0
                },
                {
                    "sent": "Again consistent.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit of an interesting history.",
                    "label": 0
                },
                {
                    "sent": "Hard again, right?",
                    "label": 1
                },
                {
                    "sent": "He introduced this notion of consistency and immediately he asked if single linkage clustering is consistent and he showed that it's not in dimensions larger than one.",
                    "label": 0
                },
                {
                    "sent": "And equals one.",
                    "label": 0
                },
                {
                    "sent": "It is consistent in larger dimensions.",
                    "label": 0
                },
                {
                    "sent": "It's only fractionally consistent, but that's not quite what we want.",
                    "label": 0
                },
                {
                    "sent": "Then 30 years went by with crickets, and several algorithms showed to be consistent or shown to be consistent.",
                    "label": 1
                },
                {
                    "sent": "In the recent years, including this robust single leakage algorithm of children Dasgupta and that reprinting algorithm of puthuff in Von Luxburg.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But as I've said, Hartigan consistency is not sufficient, and the reason is, it lacks a strong notion of connectedness.",
                    "label": 1
                },
                {
                    "sent": "So to show what I mean by that, we look at this empirical tree that we just saw is sort of nice.",
                    "label": 0
                },
                {
                    "sent": "It looks like the true tree that we want to recover.",
                    "label": 0
                },
                {
                    "sent": "But will sort of start.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unzipping this tree.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're doing sort of it.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Deepening this gap between A&B and so this is, say, a tree that our clustering algorithm has returned in the limit.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the claim is that your algorithm could return this tree.",
                    "label": 0
                },
                {
                    "sent": "After sampling infinite infinitely many points and it's fine, Hartigan says it's great.",
                    "label": 0
                },
                {
                    "sent": "It's still hard.",
                    "label": 0
                },
                {
                    "sent": "Again, it doesn't violate Hartigan consistency, even though it looks very different than the true tree.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An right to support this claim?",
                    "label": 0
                },
                {
                    "sent": "Just look and find.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Am find BN they're disjoint.",
                    "label": 0
                },
                {
                    "sent": "Hartigan says everything is great.",
                    "label": 0
                },
                {
                    "sent": "What about this tree?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also badly over segmented, but we've kind of blown up a.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But again, we find ANBN they're disjoint, everything's fine according to heart again, so something's not quite right with Hartigan consistency that take.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way is that a tree or an algorithm can be hard, again consistent, but be very different from the true tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need something stronger.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason that these issues are permitted intuitively is that Hartigan consistency lacks connectedness, so we need something stronger, and what we'll do is we will introduce a property called minimality to address connectedness directly.",
                    "label": 1
                },
                {
                    "sent": "Will also introduce the notion called separation, which is slightly weaker than hard against notion, but is sort of the symmetric definition to minimality.",
                    "label": 0
                },
                {
                    "sent": "Together, these will imply Hartigan consistency will be stronger and they will clean up some of the issues with over segmentation that Hartigan consistency permits.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our first property is minimality.",
                    "label": 0
                },
                {
                    "sent": "And the intuition behind Maletis say we have a cluster C in our density and this cluster exists at level Lambda OK.",
                    "label": 0
                },
                {
                    "sent": "In our empirical clustering.",
                    "label": 0
                },
                {
                    "sent": "We want C to exist at about level Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if we look at our empirical clustering, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "This is badly over segmented as we saw before and the problem as we know is that C is connected at some level Delta less than Lambda.",
                    "label": 0
                },
                {
                    "sent": "So what minimality says is that C restricted to the data should be connected at.",
                    "label": 1
                },
                {
                    "sent": "We allow some slack, Lambda minus Delta, but Delta should go to zero as we gather more points.",
                    "label": 0
                },
                {
                    "sent": "So what connectedness is saying is we're going to take this.",
                    "label": 0
                },
                {
                    "sent": "Level at which she is connected and we're going to force it.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be pushed up.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To about the right level.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the limit of infinite data.",
                    "label": 0
                },
                {
                    "sent": "So this I want to point out that this.",
                    "label": 0
                },
                {
                    "sent": "Is designed to address this over segmentation issue directly.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the sort of formal definition that I can come back to if there are questions.",
                    "label": 0
                },
                {
                    "sent": "Separation goes from the other direction.",
                    "label": 0
                },
                {
                    "sent": "It says we have clusters A&B and they merge, or they are separated above level mu.",
                    "label": 0
                },
                {
                    "sent": "In the density they should be separated above level mu.",
                    "label": 1
                },
                {
                    "sent": "In the empirical tree, and again we allow some slackness.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Delta.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Delta should go to zero as we go.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More data.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here A&B have been merged too soon, but as we gather more data.",
                    "label": 0
                },
                {
                    "sent": "We're going to force this to go down.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the theorem we have is that minimality and separation together imply Hartigan consistency there strictly stronger than it, because Hartigan consistency does not imply minimality and separation.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've proposed minimality and separation as properties which answer this first question.",
                    "label": 0
                },
                {
                    "sent": "Minimality addresses the shortcomings of Hartigan consistency directly and together they imply Hartigan consistency.",
                    "label": 1
                },
                {
                    "sent": "Now we turn our attention to Question 2.",
                    "label": 0
                },
                {
                    "sent": "How close is a clustering to the ideal clustering tree?",
                    "label": 1
                },
                {
                    "sent": "How do we measure this distance and to do this will introduce a merge distortion metric and it will in fact imply minimally and separation.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The intuition behind this metric is we look at the merge height.",
                    "label": 0
                },
                {
                    "sent": "So we look at any two points in the density in this part of the density and we look at the height at which they merge.",
                    "label": 0
                },
                {
                    "sent": "We call this M of a BK, so they merged about here.",
                    "label": 0
                },
                {
                    "sent": "And that's the ideal merge height.",
                    "label": 1
                },
                {
                    "sent": "In the empirical tree.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have the empirical merge height which is way down here called em hat.",
                    "label": 0
                },
                {
                    "sent": "And what minimality?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And separation together will imply.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the empirical merge height converge to the true merge height as we gather more data, so minimality pushes it up?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Separation pushes.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down and so as we get.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our data this is distance here with this disparity.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will converge to 0.",
                    "label": 0
                },
                {
                    "sent": "So we define a disk.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Between the tree based on this we look at every pair of points in our sample.",
                    "label": 0
                },
                {
                    "sent": "We look at the disparity between the true merge height and the empirical merge height, and we define the distance between these two objects.",
                    "label": 1
                },
                {
                    "sent": "The empirical tree and the true tree to be the maximum such disparity.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the distortion of a correspondence basically.",
                    "label": 0
                },
                {
                    "sent": "And so the theorem that we have.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That convergence in this metric is equivalent to the uniform versions of minimality and separation.",
                    "label": 0
                },
                {
                    "sent": "Just sort of nice.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've introduced minimally separation this merge distortion metric.",
                    "label": 0
                },
                {
                    "sent": "Do algorithms actually exist which converge in this metric, or which have properties of minimally in separation?",
                    "label": 0
                },
                {
                    "sent": "And they do we analyze two in our paper this robust single linkage algorithm from Chuck Berry and US Gupta and a split tree place based clustering algorithms from computational topology?",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Robust single linkage is inelegant generalization of single linkage which incorporates density information so it combats this chaining effect that single linkage just notorious for it is hard to get consistent.",
                    "label": 0
                },
                {
                    "sent": "An authors in fact went above that showed that it has some sort of connectedness and separation guarantees, and we do a little bit of footwork to put it in our formalism and show that given some smoothness conditions on the density robust single linkage converges to the true cluster tree in our metrics.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some future work that I think I'll.",
                    "label": 0
                },
                {
                    "sent": "Skip for now.",
                    "label": 0
                },
                {
                    "sent": "In the interest of time.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And leave you with this summary.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've replaced Hartigan consistency with two notions which are more comprehensive.",
                    "label": 1
                },
                {
                    "sent": "We introduce emerge, distortion, metric, and the nice thing is that these two things are equivalent, and this allows us to measure the distance between these objects.",
                    "label": 0
                },
                {
                    "sent": "An are these definitions actually feasable?",
                    "label": 0
                },
                {
                    "sent": "Yeah, they are so.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}