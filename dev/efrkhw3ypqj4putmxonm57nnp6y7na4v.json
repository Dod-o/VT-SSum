{
    "id": "efrkhw3ypqj4putmxonm57nnp6y7na4v",
    "title": "A tutorial on deep and unsupervised feature learning for activity recognition",
    "info": {
        "author": [
            "Graham Taylor, School of Engineering, University of Guelph"
        ],
        "published": "Aug. 24, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Computer Vision->Video Analysis"
        ]
    },
    "url": "http://videolectures.net/gesturerecognition2011_taylor_tutorial/",
    "segmentation": [
        [
            "I'm going to be giving a tutorial today on deep learning for activity recognition an when I think about tutorial.",
            "Generally I think you know, in an hour at least or two hours to survey the field, but I just have 1/2 an hour so this is going to be a very brief non exhaustive survey of some of the key ideas in deep learning.",
            "And I'll also touch on some recent applications to the area of activity recognition."
        ],
        [
            "So when I talk about activity recognition, I'd like to start with one of the existing pipelines that's been very successful.",
            "I point out some of Ivan laptops work and the folks at INRIA.",
            "There are various flavors of this type of approach.",
            "Mainly in terms of choosing certain feature detectors and interest point detectors and so forth.",
            "But this general approach is to take a video, run an interest point detector on it and extract spacetime patches around these interest points and then use engineer descriptors.",
            "As David pointed out, they can be quite simple like gradient based features or flow based features, and because we may have differing number of descriptors per video.",
            "We then typically run a clustering techniques to quantize them.",
            "Typically it's K means and then represent the entire video by a histogram of such descriptors.",
            "So now the video is represented.",
            "As's account of the various types of descriptors have shown up, and then we take that vector per video and run a classifier on it.",
            "Usually it's a nonlinear SVM.",
            "Chi squared SPS have been popular recently and so this approach is worked quite well.",
            "In practice, on many of the standard datasets and we ask ourselves how could we improve on this approach?",
            "And one of the things we might like to do is learn the features instead of use the engineer descriptors.",
            "The reason for this is because we might not know on the outset what types of features are important for a particular task at hand, and so This is why we might want to turn to deep learning and the idea of extracting."
        ],
        [
            "Hierarchical features for our problem.",
            "So the idea behind deep learning is that we propose to learn.",
            "Hierarchical representations that are more salient for high level visual reasoning tasks.",
            "And typically these methods learn one layer at a time, and we build more abstract, higher level representations on top of existing representations and compose them.",
            "And typically such methods have also used unsupervised learning.",
            "In recent years there's been many advances in various unsupervised learning algorithms, and these have played a big part in deep learning.",
            "But as we'll see today, there's also various supervised algorithms as well, so it's not all just about unsupervised learning.",
            "These learned representations are often used as input to classifiers, but we could also use different other applications as well.",
            "Doesn't have to be classification.",
            "So on the right here I'm showing 11 example of deep learning.",
            "This is something called Deconvolutional network and it's an unsupervised mechanism of building increasingly abstract representation of images.",
            "This is some work that will appear later this year at ICC V and we see at the very top.",
            "Hopefully it's readable by people in the fourth layer of this network.",
            "We've extracted fairly complete representations of entire objects, like faces.",
            "Chairs, cars and so forth.",
            "At the third layer we have large receptive fields and we're seeing a range of complex combinations of features at the second layer, we see things like edge junctions and curves, and these are really compositions of the first layer features which are gabors.",
            "This has been built completely unsupervised.",
            "This particular network has been trained on the Caltech data set and the idea here is to use the representations of the various layers as input to a classifier."
        ],
        [
            "So just to cover some of the motivations of the field of deep learning.",
            "There are several motivations, but perhaps most importantly, the representations produced by deep learning methods of representation.",
            "Lee officient.",
            "And as in a concrete example of this, I showed this example on the right, which is an example from Yoshua Bengio and this is a polynomial network and this example really illustrates this sort of implicit factorization that comes from such deep learning methods.",
            "So we see this network has products that odd layers and some that even layers, and we're able to efficiently express this polynomial here.",
            "With a number of layers, whereas if we use the shallow representation, more concretely, a two layer network where we explicitly wrote out such an expression.",
            "If we see the bold expression, the X 2X3 product, this would need to explicitly appear a number of times, and this number of times is actually exponential in the depth of the network, so we can achieve a very efficient representation due to the sort of natural hierarchy inherent in this problem.",
            "Another aspect that's nice about such hierarchical representations as is that they are intuitive, so humans we organize our thoughts and hierarchies.",
            "And another nice aspect of them is that these methods permit nonlocal generalization, so this is the idea that prototype based methods, like clustering techniques or mixture models.",
            "They associate examples with a single.",
            "Single example or single prototype and when we base our comparisons on features.",
            "In other words, distributed representations, we can make these comparisons in a much more efficient manner.",
            "Actually, we need an exponentially less number of prototypes to represent a problem, so we're comparing based on features or ABS aspects of the problem as opposed to prototypes, perhaps in a more sort of hand WAVY nature.",
            "These these methods are also biologically motivated.",
            "So of course our brains do use unsupervised learning.",
            "We've taken a lot of video and don't necessarily have a supervisor always there to inform us of what's going on, and also our brains use distributed representations.",
            "So it's not like one neuron is on at any one time, typically about 1% of the brain is firing."
        ],
        [
            "As a brief outline of some of the popular deep learning approaches that are out there, I just like to say that deep neural networks have actually been around for a long time, and lately there's been a resurgence of interest in such methods.",
            "I think in part due to the progress of unsupervised learning and being able to connect these these mechanisms to deep neural networks.",
            "For example, pre training and addressing the difficult optimization problem, there's actually been.",
            "Recent progress to in optimization methods.",
            "2nd order efficient optimization methods that can train such architectures more efficiently.",
            "And finally, I think that changes in hardware have also made a big impact on the field.",
            "For example, massively parallel architectures such as GPU's.",
            "Deep belief networks were introduced in 2006 and they sort of led the way in terms of the recent interest in deep learning and more recently, there's been sort of more scalable architectures.",
            "Convolutional type of deep belief networks also proposed convolutional networks on their own will see some examples of them today.",
            "They've been actually around for quite a long time there, perhaps best known for handwriting recognition.",
            "But we're also seeing a resurgence of interest in convolutional networks or confidence as well due to new applications.",
            "Again, unsupervised learning algorithms for convolutional networks and also hardware changes as well as stacked denoising autoencoders there a alternative to deep belief networks?",
            "Another type of general purpose unsupervised learning algorithm that's deep and then hierarchical sparse coding and convolutional sparse coding.",
            "I've worked myself in this area again.",
            "It's a scalable unsupervised learning algorithm for large image and video data, and finally deep Boltzmann machines.",
            "This is an alternative to deep belief networks.",
            "If you know directed and undirected graphical models.",
            "The deep belief network is sort of an undirected directed hybrid model, but people, some machines or the undirected analog, and these are a little bit more difficult to train, but they perhaps produce better representations even then deep belief networks."
        ],
        [
            "So for the remainder of the talk, as I said, I'd like to introduce some concrete applications to the area of activity recognition.",
            "This is a gesture recognition workshop and time permitting I'd like to talk about four different methods, one of which I've worked on myself.",
            "And of course I'd like to take questions, but I will deflect you if they're very detailed to the authors, some of whom are already at this workshop.",
            "So we'll start with."
        ],
        [
            "3D convolutional neural networks and 1st I'll give a brief introduction to what a convolutional neural network is to those who may be unfamiliar with it.",
            "So continents are a good example of an architecture that is both biologically motivated and also engineered from a practical point of view to perform well in real world tasks.",
            "And it comes with the basic idea of stacking multiple stages of filterbank plus non linearity plus pooling operator.",
            "And this particular this particular module actually shares properties with a lot of existing features.",
            "Detector systems from computer vision literature.",
            "So you may know of sift or or hog features and so forth.",
            "The difference between the convolutional net module and these other type of feature extractors is that the filter banks are learned rather than set by hands and.",
            "This make this these methods and manage in terms of adapting to different problems or adapting to the."
        ],
        [
            "Certain problem that we're looking at.",
            "Comments are also biologically motivated.",
            "The idea here is to learn representations that are increasingly abstract, global and invariant, and they're inspired by the model of the visual cortex proposed by Hubel and Wiesel.",
            "So the idea here is that feature Maps are corresponding to simple cells which are tuned to different frequencies and orientations, and then the pool Maps correspond to these complex cells which pool over a local retinotopic neighborhood."
        ],
        [
            "So when we think about applying comnets to activity recognition or to video problems, perhaps the simplest thing to come that comes to mind is to treat each frame of the video as independent and just apply a standard 2D convolutional net to the frames, and this has been done before.",
            "But the problem here is that we miss the type of relationships that's inherent between blocks of contiguous frames.",
            "So we like to perform 3D convolutions and this has been done by Gia Dal where we are able to capture motion sensitive features in 3D filters.",
            "So we see on the left at the top just a standard 2D convolution, illustrating that a filter is shared across all regions in a 2D image.",
            "But now we live, we look at 3D convolution.",
            "We're applying these filters to contiguous frames and we've used color coding here.",
            "To indicate the sharing of weights so this will happen to multiple blocks of frames in time.",
            "In practice, we actually not just.",
            "We don't just apply a single 3D filter through the video, but we apply multiple ones to be able to extract multiple features, and this is what's being shown on the right.",
            "Now."
        ],
        [
            "That were armed with this idea of 3D filtering and pooling operations.",
            "Or you may say subsampling.",
            "There are many different architectural choices that we could make here this particular convolutional neural net architecture that's been proposed by G. It all starts with an input video that is 60 by 40 pixels and seven frames in the temporal dimension.",
            "Interestingly enough, the first layer is hardwired to extract 5 different channels.",
            "So they extract first to Grayscale Channel, which is just basically the raw pixels.",
            "We also extract horizontal and vertical gradients.",
            "And then finally, there's two channels which correspond to the horizontal and vertical optical flow.",
            "Once these five channels have been extracted, then a learned 7 by 7 by three filter is applied to these five channels independently there, then subsample to reduce the spatial resolution, and then filtered again, and this time three filters are applied to the two groups of five channels, and ultimately, we.",
            "Ultimately we produce 78 different feature Maps.",
            "478 is spread out in terms of the number of features as well as the temporal dimension, and these are fully connected to a final layer of 128 numbers, and these 128 numbers represent what's going on in the original input, so everything except the first layer here has been has been learned.",
            "This final 128 D representation is connected to a number of units which represent activities, and this depends on the actual problem whether the type of activities is some sort of surveillance activity like cell phone to head or putting bag down or something more simple like walking or running and so forth."
        ],
        [
            "So this particular architecture has been applied to the Trek video surveillance data and Institute.",
            "Fairly good results in this data set.",
            "It's also performed well on the KTH accidents data set, which we've already talked about a bit today, and many people know about in terms of commenting on such an architecture, there's still a fair amount of engineering presence, so it requires a person detection to be done for the Trek video ahead of time.",
            "Foreground extraction to be performed for the KTH data.",
            "And as we've seen, there's a hard coded first layer."
        ],
        [
            "The second type of deep architecture I'd like to talk about is something that I've been involved with myself, and compared to the first method we looked at, which was a fully discriminative approach.",
            "This is a generative model of image transformations, so there's a lot of work.",
            "As I mentioned, that's going into unsupervised learning of images, but less attention has been devoted to learning about the way that images change in time.",
            "So the way that this type of model works is that we show it pairs of images.",
            "So here we have an image pair that's.",
            "Undergoing some type of transformation and then we extract from it a number of feature Maps that describe the type of transformation present in those images.",
            "Other people have looked at a similar problem, so rolling misiewicz and Jeff Hinton, Chuck Audio, and Bruno's House and have models that extract transformation features.",
            "But there they have used so-called fully connected networks where they are very highly parameterized and thus have been limited limited to modeling only small image patches.",
            "So we adopted convolutional architecture much like a convolutional net, which allows us to scale to image is much larger than say 64 by 64."
        ],
        [
            "So before I actually show you this model architecture, I'd like to introduce the gated restricted Boltzmann machine, which is a model on which this our model is based essentially.",
            "There's two views that you can use to look at this model, depending on what type of background you're from.",
            "If you know about linear autoregressive models, then the first view may be more appropriate.",
            "We have some variables representing an input, for example, A-frame at time T -- 1 and some variables representing an output.",
            "For example the frame at time T. And then we also have a series of latent variables denoted Z here, and the idea is that these latent variables are going to model the transformation between the input and the output.",
            "So if you're familiar with auto regressive models, if we were to get rid of the Z variables, the latent variables, this would just be a linear autoregressive model between input and output, so it's fully parameterized by a weight matrix.",
            "But what's happening here is we're introducing these latent variables, which allow us to switch in different types of linear autoregressive models to create an overall nonlinear model.",
            "So it's a sort of very rich type of autoregressive model.",
            "The other view for people that are actually familiar with restricted Boltzmann machines.",
            "Is a type of gated restricted Boltzmann machines, so we have some output units and some latent variables.",
            "Those are the standard types of units you have in a restricted Boltzmann machine, but the weights in this restricted Boltzmann machine are gated by the input and these model views are equivalent, But the second view is nice because it shows that if you fix the input, which is what we usually do, it just defines the weights in a restricted Boltzmann machine.",
            "So learning and inference or just as easy as they are in a standard RBM.",
            "OK, so this is a gated restricted Boltzmann machine.",
            "The problem with such a model is that there are three types of variables in this model, and they're all connected to one another.",
            "So I've shown some restricted connections here just for illustration purposes, but in practice all of these variables are connected to one another, so this model is really parameterized by a 3D weight tensor between all inputs, outputs and latent variables.",
            "So if you have a large input, for example.",
            "This is effects both the input and output, and so the parameters of this model quickly blow up."
        ],
        [
            "So one thing that we can do is use this idea of weight sharing and feature pooling that convolutional Nets do and define a convolutional version of the gated restricted Boltzmann machine.",
            "So like as in a convolutional net we have filters which are shared at all regions of the input and output.",
            "And now we extract instead of a single vector as our latent variables.",
            "We extract feature Maps and multiple feature Maps which describe the type of transformation going on in the image and where it's happening.",
            "Again, inference and reconstruction.",
            "These are the steps necessary for learning or as easy as in a standard RBM."
        ],
        [
            "OK, so let's look at an example of what type of features this model learns.",
            "One thing that we can do is take an actual input and output pair."
        ],
        [
            "And infer the latent feature Maps so."
        ],
        [
            "This will give US series of Maps."
        ],
        [
            "Which show is basically where the activity is happening in a particular image, but they don't really give insight an idea of what type of feature this this feature map describes."
        ],
        [
            "So something we can do is take the input and output pair as we see on the left, infer the latent feature Maps, and then look at the induced reconstruction distribution happening on the output.",
            "In other words, for each pixel in the input, look and see where it would like to send its ink as defined by the latent variables, and we get a little picture like this, so we see that there's a sort of induced flow field in terms of rotational motion when we observe these.",
            "Input and output pair.",
            "Another interesting that thing that we can do with this is we can take this in Ferd transformation and apply it to a novel input.",
            "So we give the model and input here.",
            "This little car that hasn't seen before and we say apply this transformation that's in your latent variables that you just saw between this original input and output, and you see that it's able to take this car and do a little transformation of it.",
            "And we can actually compare this because it is a synthetic data set.",
            "We have the ground truth and we see that it is a reasonable job of actually rotating the."
        ],
        [
            "Object.",
            "OK, so if we apply this to a more realistic data set, this is the KTH actions which we've already discussed.",
            "We can look at a subset here.",
            "I have six of the 32 features that have been extracted by this model and we have a feature for each pair of inputs.",
            "So as we move from left to right across here, we have these temporal features that are being extracted and we can see that some of the features are quite motion sensitive As for example.",
            "Features one and three you can see in the hand clapping example up above there highlighting the hands and there are localized in the hands that are as they move.",
            "You also have static features.",
            "For example the 4th feature and this is able to also capture sort of a segmentation operator as we see in the 6th row of these features.",
            "So the good thing here is that we can capture both motion sensitive features an static features which are important for contextual information.",
            "As David pointed out in his talk."
        ],
        [
            "We do reasonably well on on Cth, we're just behind sort of a leading methods.",
            "There's a method that's presented here at CPR, which is a deep learning method, achieving almost 94% now in the KTH actions data database, but I think you need to take these numbers again, as with sort of with a grain of salt, because KCH really has been hammered to death."
        ],
        [
            "There is a more challenging activity recognition data set Hollywood 2 which has been put forth and again, we're doing reasonably well against other methods that use sort of fine tuned handcrafted features.",
            "Our method, again, showing sort of 46.8 accuracy on On this date."
        ],
        [
            "Set.",
            "The next method I'd like to talk about briefly is the idea of space time, deep belief, network network.",
            "So this was put forth by some researchers at the University of British Columbia at the last NIPS Deep Learning Workshop, and the two approaches that we just saw used some sort of discriminative learning.",
            "So in the first example it was completely discriminative, and in the second example we had a generative feature extractor which was this convolutional gated restricted.",
            "Also machine, but then we use a discriminative learning procedure after the features have been extracted.",
            "So this space time deep belief network is a fully generative method, which means it opens it up to other applications beyond activity recognition.",
            "For example in painting or denoising.",
            "So another key aspect of this work is it's demonstrated learn invariants, which we will see, and it uses as a basic module.",
            "The convolutional restricted Boltzmann machine, which was proposed by Hong likely and coauthors a couple years ago.",
            "So on the right is the convolutional restricted Boltzmann machine.",
            "It's much like a convolutional network, but it's able to infer up to the feature Maps as well as do a reconstruction back down to the image, making it a fully generative model.",
            "It also incorporates a pooling operator as as is in standard convolutional network."
        ],
        [
            "OK, so the basic idea behind the space time, this deep belief network is to alternate layers of spatial and temporal pooling.",
            "Weight sharing is done across all convolutional restricted Boltzmann machines in a particular layer.",
            "So we have some video frames as inputs and we apply a convolutional restricted Boltzmann machine to each frame.",
            "So we see in this layer down here we might have multiple channels, for example corresponding to different color channels and then time proceeds to the right here.",
            "So we're just applying this static model, the PBM, to each of these frames.",
            "But it's the same CRBN that's applying.",
            "Being applied to each frames.",
            "The Serapeum extracts a varying number of features which you see in this little stack here and then their pooled spatially to gain some a little bit of.",
            "Translational in."
        ],
        [
            "Variance?",
            "So now that these little feature Maps have been extracted for each frame, we now apply a different type of CRB M that pools in the temporal dimension so you can see in red.",
            "We've highlighted each pixel each pixel is collected into a little sequence corresponding to the number of feature Maps that were extracted and a temporal.",
            "See RBM is applied.",
            "This reduces the temporal resolution and even more so when it is temporarily pools this stack that's been extracted for each pixel.",
            "And then and then applied through the CR BM is then reshaped into an image structure and again we were then arriving at a spatially arranged feature map which could then be processed by say another spatial pooling layer, and so on.",
            "And this it can be stacked basically in this way."
        ],
        [
            "So one thing that the others have done is measured the invariants in such a network.",
            "So this is the idea of taking an image or video where there's a specified translation or rotation happening and measuring the firing rate of a particular particular unit under this type of transformation.",
            "So we can see that some units that we saw on the right here could be overly selective, as in their red, not selective at all in the blue or invariant.",
            "Over a particular degree of transformation, that's the type of thing we'd like to see.",
            "So on the right were comparing three different layers of such a module, so there's S. One layer is the first layer, it's a spatial pooling layer.",
            "There's no temporal pooling layer for the for the first layer in the network, so we go right into the layer S2, which is a spec.",
            "Second spatial pooling layer, and then there's T1, which is the third layer of the network, and that's a temporal pooling layer.",
            "And what we see here.",
            "Is increased in variance as we move higher in the network so higher in this picture is better, showing more invariants.",
            "So all of the three layers exhibit fairly good translation invariants, but we see the higher layers exhibiting better invariants to zooming 2D and 3D rotation."
        ],
        [
            "As I mentioned, because this is a fully generative model, we can do operations like denoising as we see in the in the very top here, because we can go right to the top layer representation and then come all the way back down.",
            "Another interesting operation that can be done in such a network is give the model observed cases, so only partial input and then acts get to reconstruct that input as well as the region outside that input.",
            "So here is doing in paging reasonably well."
        ],
        [
            "The 4th method I'm going to very very briefly mention is this idea of stacked convolutional independent subspace analysis, and the reason I'm going to mention it very briefly is because it's a paper here at CPR and instant oral, so you'll be able to hear about it on your own.",
            "But this is another way of using unsupervised learning module in a stacked approach.",
            "And it's also been scaled as the other methods have convolutionally."
        ],
        [
            "OK, so in summary, I've told you about a few different ways of applying deep learning to the activity recognition problem.",
            "We talked about 3D convolutional neural Nets, convolutional gated restricted Boltzmann machines, spacetime, deep belief networks, and I gave you just basically a pointer to this last work here at CPR this year."
        ],
        [
            "In conclusion, deep learning methods have shown quite amount of a good amount of progress.",
            "Promise in this area of activity recognition.",
            "But to this point in terms of their performance, I think it's still very neck and neck against traditional computer vision algorithms.",
            "the Holy Grail, I guess in this field is going towards homogeneous networks built by simple, trainable modules, and I guess there's a desire here in this field of future improvements in activity recognition being made by.",
            "Progress in efficient and robust unsupervised learning as opposed to relying a lot on a supervision signal.",
            "Because the data of course is much more much more limited, an more costly to obtain.",
            "The last thing I'd like to mention is, as we said several times in this talk, we've tried to learn invariant representations and I have some work actually tomorrow talking about learning invariant representations, but some very interesting work that's come out of the Hinton lab in the last year.",
            "Questions the idea of learning invariant representations.",
            "This work suggests that we should actually represent things like translations and rotations and lighting changes.",
            "These things that we have been trying to build invariants too.",
            "As latent features within our model, so they they proposed this method called the transforming autoencoder.",
            "You can view this paper if you're interested and explicitly tries to reason about the things we have already been trying to gain invariants too.",
            "So with that, I'd like to thank you very much for your attention and thank my various collaborators at NYU, faculty students funding, and then maybe just give a very quick pointer to a poster I have, which is not an activity recognition but tomorrow morning.",
            "Basically you have computer vision researchers in a Dutch progressive Electro band can team up to solve computer vision so that interests you.",
            "Please come to my poster, thanks.",
            "Thank you Carl.",
            "It's quick questions.",
            "Yeah, I think it was the 3rd method.",
            "You said you can reconstruct down to the original image using the generative model.",
            "How do you go backwards from the pooling operation?",
            "Great points.",
            "So if you see in those slides I'll just jump back.",
            "There it's a little bit blurry in certain regions, and that's an artifact of the pooling, so you can't perfectly reconstruct once you've done the pooling, you can basically approximate sort of maybe smooth out from where you from where you've pulled, but you're going to get blurry reconstruction that's just a limitation, and I think there's a lot of interest in deep learning researchers to build better pooling mechanisms.",
            "It's very simple.",
            "Right now, average or Max pooling is all we do, but I think various forms of adaptive pooling could really assist these methods.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to be giving a tutorial today on deep learning for activity recognition an when I think about tutorial.",
                    "label": 1
                },
                {
                    "sent": "Generally I think you know, in an hour at least or two hours to survey the field, but I just have 1/2 an hour so this is going to be a very brief non exhaustive survey of some of the key ideas in deep learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll also touch on some recent applications to the area of activity recognition.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when I talk about activity recognition, I'd like to start with one of the existing pipelines that's been very successful.",
                    "label": 1
                },
                {
                    "sent": "I point out some of Ivan laptops work and the folks at INRIA.",
                    "label": 0
                },
                {
                    "sent": "There are various flavors of this type of approach.",
                    "label": 0
                },
                {
                    "sent": "Mainly in terms of choosing certain feature detectors and interest point detectors and so forth.",
                    "label": 1
                },
                {
                    "sent": "But this general approach is to take a video, run an interest point detector on it and extract spacetime patches around these interest points and then use engineer descriptors.",
                    "label": 0
                },
                {
                    "sent": "As David pointed out, they can be quite simple like gradient based features or flow based features, and because we may have differing number of descriptors per video.",
                    "label": 0
                },
                {
                    "sent": "We then typically run a clustering techniques to quantize them.",
                    "label": 0
                },
                {
                    "sent": "Typically it's K means and then represent the entire video by a histogram of such descriptors.",
                    "label": 1
                },
                {
                    "sent": "So now the video is represented.",
                    "label": 0
                },
                {
                    "sent": "As's account of the various types of descriptors have shown up, and then we take that vector per video and run a classifier on it.",
                    "label": 0
                },
                {
                    "sent": "Usually it's a nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "Chi squared SPS have been popular recently and so this approach is worked quite well.",
                    "label": 0
                },
                {
                    "sent": "In practice, on many of the standard datasets and we ask ourselves how could we improve on this approach?",
                    "label": 0
                },
                {
                    "sent": "And one of the things we might like to do is learn the features instead of use the engineer descriptors.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is because we might not know on the outset what types of features are important for a particular task at hand, and so This is why we might want to turn to deep learning and the idea of extracting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hierarchical features for our problem.",
                    "label": 1
                },
                {
                    "sent": "So the idea behind deep learning is that we propose to learn.",
                    "label": 1
                },
                {
                    "sent": "Hierarchical representations that are more salient for high level visual reasoning tasks.",
                    "label": 1
                },
                {
                    "sent": "And typically these methods learn one layer at a time, and we build more abstract, higher level representations on top of existing representations and compose them.",
                    "label": 1
                },
                {
                    "sent": "And typically such methods have also used unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "In recent years there's been many advances in various unsupervised learning algorithms, and these have played a big part in deep learning.",
                    "label": 1
                },
                {
                    "sent": "But as we'll see today, there's also various supervised algorithms as well, so it's not all just about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "These learned representations are often used as input to classifiers, but we could also use different other applications as well.",
                    "label": 1
                },
                {
                    "sent": "Doesn't have to be classification.",
                    "label": 0
                },
                {
                    "sent": "So on the right here I'm showing 11 example of deep learning.",
                    "label": 0
                },
                {
                    "sent": "This is something called Deconvolutional network and it's an unsupervised mechanism of building increasingly abstract representation of images.",
                    "label": 0
                },
                {
                    "sent": "This is some work that will appear later this year at ICC V and we see at the very top.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it's readable by people in the fourth layer of this network.",
                    "label": 0
                },
                {
                    "sent": "We've extracted fairly complete representations of entire objects, like faces.",
                    "label": 0
                },
                {
                    "sent": "Chairs, cars and so forth.",
                    "label": 0
                },
                {
                    "sent": "At the third layer we have large receptive fields and we're seeing a range of complex combinations of features at the second layer, we see things like edge junctions and curves, and these are really compositions of the first layer features which are gabors.",
                    "label": 0
                },
                {
                    "sent": "This has been built completely unsupervised.",
                    "label": 0
                },
                {
                    "sent": "This particular network has been trained on the Caltech data set and the idea here is to use the representations of the various layers as input to a classifier.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to cover some of the motivations of the field of deep learning.",
                    "label": 1
                },
                {
                    "sent": "There are several motivations, but perhaps most importantly, the representations produced by deep learning methods of representation.",
                    "label": 0
                },
                {
                    "sent": "Lee officient.",
                    "label": 0
                },
                {
                    "sent": "And as in a concrete example of this, I showed this example on the right, which is an example from Yoshua Bengio and this is a polynomial network and this example really illustrates this sort of implicit factorization that comes from such deep learning methods.",
                    "label": 0
                },
                {
                    "sent": "So we see this network has products that odd layers and some that even layers, and we're able to efficiently express this polynomial here.",
                    "label": 0
                },
                {
                    "sent": "With a number of layers, whereas if we use the shallow representation, more concretely, a two layer network where we explicitly wrote out such an expression.",
                    "label": 0
                },
                {
                    "sent": "If we see the bold expression, the X 2X3 product, this would need to explicitly appear a number of times, and this number of times is actually exponential in the depth of the network, so we can achieve a very efficient representation due to the sort of natural hierarchy inherent in this problem.",
                    "label": 0
                },
                {
                    "sent": "Another aspect that's nice about such hierarchical representations as is that they are intuitive, so humans we organize our thoughts and hierarchies.",
                    "label": 0
                },
                {
                    "sent": "And another nice aspect of them is that these methods permit nonlocal generalization, so this is the idea that prototype based methods, like clustering techniques or mixture models.",
                    "label": 1
                },
                {
                    "sent": "They associate examples with a single.",
                    "label": 0
                },
                {
                    "sent": "Single example or single prototype and when we base our comparisons on features.",
                    "label": 0
                },
                {
                    "sent": "In other words, distributed representations, we can make these comparisons in a much more efficient manner.",
                    "label": 0
                },
                {
                    "sent": "Actually, we need an exponentially less number of prototypes to represent a problem, so we're comparing based on features or ABS aspects of the problem as opposed to prototypes, perhaps in a more sort of hand WAVY nature.",
                    "label": 0
                },
                {
                    "sent": "These these methods are also biologically motivated.",
                    "label": 1
                },
                {
                    "sent": "So of course our brains do use unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "We've taken a lot of video and don't necessarily have a supervisor always there to inform us of what's going on, and also our brains use distributed representations.",
                    "label": 1
                },
                {
                    "sent": "So it's not like one neuron is on at any one time, typically about 1% of the brain is firing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a brief outline of some of the popular deep learning approaches that are out there, I just like to say that deep neural networks have actually been around for a long time, and lately there's been a resurgence of interest in such methods.",
                    "label": 1
                },
                {
                    "sent": "I think in part due to the progress of unsupervised learning and being able to connect these these mechanisms to deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "For example, pre training and addressing the difficult optimization problem, there's actually been.",
                    "label": 0
                },
                {
                    "sent": "Recent progress to in optimization methods.",
                    "label": 0
                },
                {
                    "sent": "2nd order efficient optimization methods that can train such architectures more efficiently.",
                    "label": 0
                },
                {
                    "sent": "And finally, I think that changes in hardware have also made a big impact on the field.",
                    "label": 0
                },
                {
                    "sent": "For example, massively parallel architectures such as GPU's.",
                    "label": 0
                },
                {
                    "sent": "Deep belief networks were introduced in 2006 and they sort of led the way in terms of the recent interest in deep learning and more recently, there's been sort of more scalable architectures.",
                    "label": 0
                },
                {
                    "sent": "Convolutional type of deep belief networks also proposed convolutional networks on their own will see some examples of them today.",
                    "label": 0
                },
                {
                    "sent": "They've been actually around for quite a long time there, perhaps best known for handwriting recognition.",
                    "label": 0
                },
                {
                    "sent": "But we're also seeing a resurgence of interest in convolutional networks or confidence as well due to new applications.",
                    "label": 0
                },
                {
                    "sent": "Again, unsupervised learning algorithms for convolutional networks and also hardware changes as well as stacked denoising autoencoders there a alternative to deep belief networks?",
                    "label": 1
                },
                {
                    "sent": "Another type of general purpose unsupervised learning algorithm that's deep and then hierarchical sparse coding and convolutional sparse coding.",
                    "label": 0
                },
                {
                    "sent": "I've worked myself in this area again.",
                    "label": 0
                },
                {
                    "sent": "It's a scalable unsupervised learning algorithm for large image and video data, and finally deep Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "This is an alternative to deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "If you know directed and undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "The deep belief network is sort of an undirected directed hybrid model, but people, some machines or the undirected analog, and these are a little bit more difficult to train, but they perhaps produce better representations even then deep belief networks.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the remainder of the talk, as I said, I'd like to introduce some concrete applications to the area of activity recognition.",
                    "label": 0
                },
                {
                    "sent": "This is a gesture recognition workshop and time permitting I'd like to talk about four different methods, one of which I've worked on myself.",
                    "label": 0
                },
                {
                    "sent": "And of course I'd like to take questions, but I will deflect you if they're very detailed to the authors, some of whom are already at this workshop.",
                    "label": 0
                },
                {
                    "sent": "So we'll start with.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "3D convolutional neural networks and 1st I'll give a brief introduction to what a convolutional neural network is to those who may be unfamiliar with it.",
                    "label": 0
                },
                {
                    "sent": "So continents are a good example of an architecture that is both biologically motivated and also engineered from a practical point of view to perform well in real world tasks.",
                    "label": 0
                },
                {
                    "sent": "And it comes with the basic idea of stacking multiple stages of filterbank plus non linearity plus pooling operator.",
                    "label": 1
                },
                {
                    "sent": "And this particular this particular module actually shares properties with a lot of existing features.",
                    "label": 0
                },
                {
                    "sent": "Detector systems from computer vision literature.",
                    "label": 0
                },
                {
                    "sent": "So you may know of sift or or hog features and so forth.",
                    "label": 1
                },
                {
                    "sent": "The difference between the convolutional net module and these other type of feature extractors is that the filter banks are learned rather than set by hands and.",
                    "label": 0
                },
                {
                    "sent": "This make this these methods and manage in terms of adapting to different problems or adapting to the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Certain problem that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Comments are also biologically motivated.",
                    "label": 0
                },
                {
                    "sent": "The idea here is to learn representations that are increasingly abstract, global and invariant, and they're inspired by the model of the visual cortex proposed by Hubel and Wiesel.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that feature Maps are corresponding to simple cells which are tuned to different frequencies and orientations, and then the pool Maps correspond to these complex cells which pool over a local retinotopic neighborhood.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we think about applying comnets to activity recognition or to video problems, perhaps the simplest thing to come that comes to mind is to treat each frame of the video as independent and just apply a standard 2D convolutional net to the frames, and this has been done before.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that we miss the type of relationships that's inherent between blocks of contiguous frames.",
                    "label": 0
                },
                {
                    "sent": "So we like to perform 3D convolutions and this has been done by Gia Dal where we are able to capture motion sensitive features in 3D filters.",
                    "label": 0
                },
                {
                    "sent": "So we see on the left at the top just a standard 2D convolution, illustrating that a filter is shared across all regions in a 2D image.",
                    "label": 0
                },
                {
                    "sent": "But now we live, we look at 3D convolution.",
                    "label": 0
                },
                {
                    "sent": "We're applying these filters to contiguous frames and we've used color coding here.",
                    "label": 0
                },
                {
                    "sent": "To indicate the sharing of weights so this will happen to multiple blocks of frames in time.",
                    "label": 0
                },
                {
                    "sent": "In practice, we actually not just.",
                    "label": 0
                },
                {
                    "sent": "We don't just apply a single 3D filter through the video, but we apply multiple ones to be able to extract multiple features, and this is what's being shown on the right.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That were armed with this idea of 3D filtering and pooling operations.",
                    "label": 0
                },
                {
                    "sent": "Or you may say subsampling.",
                    "label": 0
                },
                {
                    "sent": "There are many different architectural choices that we could make here this particular convolutional neural net architecture that's been proposed by G. It all starts with an input video that is 60 by 40 pixels and seven frames in the temporal dimension.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, the first layer is hardwired to extract 5 different channels.",
                    "label": 1
                },
                {
                    "sent": "So they extract first to Grayscale Channel, which is just basically the raw pixels.",
                    "label": 0
                },
                {
                    "sent": "We also extract horizontal and vertical gradients.",
                    "label": 0
                },
                {
                    "sent": "And then finally, there's two channels which correspond to the horizontal and vertical optical flow.",
                    "label": 1
                },
                {
                    "sent": "Once these five channels have been extracted, then a learned 7 by 7 by three filter is applied to these five channels independently there, then subsample to reduce the spatial resolution, and then filtered again, and this time three filters are applied to the two groups of five channels, and ultimately, we.",
                    "label": 1
                },
                {
                    "sent": "Ultimately we produce 78 different feature Maps.",
                    "label": 0
                },
                {
                    "sent": "478 is spread out in terms of the number of features as well as the temporal dimension, and these are fully connected to a final layer of 128 numbers, and these 128 numbers represent what's going on in the original input, so everything except the first layer here has been has been learned.",
                    "label": 1
                },
                {
                    "sent": "This final 128 D representation is connected to a number of units which represent activities, and this depends on the actual problem whether the type of activities is some sort of surveillance activity like cell phone to head or putting bag down or something more simple like walking or running and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this particular architecture has been applied to the Trek video surveillance data and Institute.",
                    "label": 0
                },
                {
                    "sent": "Fairly good results in this data set.",
                    "label": 0
                },
                {
                    "sent": "It's also performed well on the KTH accidents data set, which we've already talked about a bit today, and many people know about in terms of commenting on such an architecture, there's still a fair amount of engineering presence, so it requires a person detection to be done for the Trek video ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Foreground extraction to be performed for the KTH data.",
                    "label": 0
                },
                {
                    "sent": "And as we've seen, there's a hard coded first layer.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second type of deep architecture I'd like to talk about is something that I've been involved with myself, and compared to the first method we looked at, which was a fully discriminative approach.",
                    "label": 0
                },
                {
                    "sent": "This is a generative model of image transformations, so there's a lot of work.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, that's going into unsupervised learning of images, but less attention has been devoted to learning about the way that images change in time.",
                    "label": 0
                },
                {
                    "sent": "So the way that this type of model works is that we show it pairs of images.",
                    "label": 1
                },
                {
                    "sent": "So here we have an image pair that's.",
                    "label": 0
                },
                {
                    "sent": "Undergoing some type of transformation and then we extract from it a number of feature Maps that describe the type of transformation present in those images.",
                    "label": 0
                },
                {
                    "sent": "Other people have looked at a similar problem, so rolling misiewicz and Jeff Hinton, Chuck Audio, and Bruno's House and have models that extract transformation features.",
                    "label": 0
                },
                {
                    "sent": "But there they have used so-called fully connected networks where they are very highly parameterized and thus have been limited limited to modeling only small image patches.",
                    "label": 1
                },
                {
                    "sent": "So we adopted convolutional architecture much like a convolutional net, which allows us to scale to image is much larger than say 64 by 64.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I actually show you this model architecture, I'd like to introduce the gated restricted Boltzmann machine, which is a model on which this our model is based essentially.",
                    "label": 0
                },
                {
                    "sent": "There's two views that you can use to look at this model, depending on what type of background you're from.",
                    "label": 0
                },
                {
                    "sent": "If you know about linear autoregressive models, then the first view may be more appropriate.",
                    "label": 0
                },
                {
                    "sent": "We have some variables representing an input, for example, A-frame at time T -- 1 and some variables representing an output.",
                    "label": 0
                },
                {
                    "sent": "For example the frame at time T. And then we also have a series of latent variables denoted Z here, and the idea is that these latent variables are going to model the transformation between the input and the output.",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar with auto regressive models, if we were to get rid of the Z variables, the latent variables, this would just be a linear autoregressive model between input and output, so it's fully parameterized by a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "But what's happening here is we're introducing these latent variables, which allow us to switch in different types of linear autoregressive models to create an overall nonlinear model.",
                    "label": 0
                },
                {
                    "sent": "So it's a sort of very rich type of autoregressive model.",
                    "label": 0
                },
                {
                    "sent": "The other view for people that are actually familiar with restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "Is a type of gated restricted Boltzmann machines, so we have some output units and some latent variables.",
                    "label": 1
                },
                {
                    "sent": "Those are the standard types of units you have in a restricted Boltzmann machine, but the weights in this restricted Boltzmann machine are gated by the input and these model views are equivalent, But the second view is nice because it shows that if you fix the input, which is what we usually do, it just defines the weights in a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "So learning and inference or just as easy as they are in a standard RBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a gated restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "The problem with such a model is that there are three types of variables in this model, and they're all connected to one another.",
                    "label": 0
                },
                {
                    "sent": "So I've shown some restricted connections here just for illustration purposes, but in practice all of these variables are connected to one another, so this model is really parameterized by a 3D weight tensor between all inputs, outputs and latent variables.",
                    "label": 0
                },
                {
                    "sent": "So if you have a large input, for example.",
                    "label": 0
                },
                {
                    "sent": "This is effects both the input and output, and so the parameters of this model quickly blow up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing that we can do is use this idea of weight sharing and feature pooling that convolutional Nets do and define a convolutional version of the gated restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "So like as in a convolutional net we have filters which are shared at all regions of the input and output.",
                    "label": 0
                },
                {
                    "sent": "And now we extract instead of a single vector as our latent variables.",
                    "label": 0
                },
                {
                    "sent": "We extract feature Maps and multiple feature Maps which describe the type of transformation going on in the image and where it's happening.",
                    "label": 0
                },
                {
                    "sent": "Again, inference and reconstruction.",
                    "label": 0
                },
                {
                    "sent": "These are the steps necessary for learning or as easy as in a standard RBM.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at an example of what type of features this model learns.",
                    "label": 0
                },
                {
                    "sent": "One thing that we can do is take an actual input and output pair.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And infer the latent feature Maps so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This will give US series of Maps.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which show is basically where the activity is happening in a particular image, but they don't really give insight an idea of what type of feature this this feature map describes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So something we can do is take the input and output pair as we see on the left, infer the latent feature Maps, and then look at the induced reconstruction distribution happening on the output.",
                    "label": 0
                },
                {
                    "sent": "In other words, for each pixel in the input, look and see where it would like to send its ink as defined by the latent variables, and we get a little picture like this, so we see that there's a sort of induced flow field in terms of rotational motion when we observe these.",
                    "label": 0
                },
                {
                    "sent": "Input and output pair.",
                    "label": 0
                },
                {
                    "sent": "Another interesting that thing that we can do with this is we can take this in Ferd transformation and apply it to a novel input.",
                    "label": 1
                },
                {
                    "sent": "So we give the model and input here.",
                    "label": 0
                },
                {
                    "sent": "This little car that hasn't seen before and we say apply this transformation that's in your latent variables that you just saw between this original input and output, and you see that it's able to take this car and do a little transformation of it.",
                    "label": 0
                },
                {
                    "sent": "And we can actually compare this because it is a synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "We have the ground truth and we see that it is a reasonable job of actually rotating the.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Object.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we apply this to a more realistic data set, this is the KTH actions which we've already discussed.",
                    "label": 1
                },
                {
                    "sent": "We can look at a subset here.",
                    "label": 0
                },
                {
                    "sent": "I have six of the 32 features that have been extracted by this model and we have a feature for each pair of inputs.",
                    "label": 0
                },
                {
                    "sent": "So as we move from left to right across here, we have these temporal features that are being extracted and we can see that some of the features are quite motion sensitive As for example.",
                    "label": 0
                },
                {
                    "sent": "Features one and three you can see in the hand clapping example up above there highlighting the hands and there are localized in the hands that are as they move.",
                    "label": 0
                },
                {
                    "sent": "You also have static features.",
                    "label": 0
                },
                {
                    "sent": "For example the 4th feature and this is able to also capture sort of a segmentation operator as we see in the 6th row of these features.",
                    "label": 0
                },
                {
                    "sent": "So the good thing here is that we can capture both motion sensitive features an static features which are important for contextual information.",
                    "label": 1
                },
                {
                    "sent": "As David pointed out in his talk.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do reasonably well on on Cth, we're just behind sort of a leading methods.",
                    "label": 0
                },
                {
                    "sent": "There's a method that's presented here at CPR, which is a deep learning method, achieving almost 94% now in the KTH actions data database, but I think you need to take these numbers again, as with sort of with a grain of salt, because KCH really has been hammered to death.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a more challenging activity recognition data set Hollywood 2 which has been put forth and again, we're doing reasonably well against other methods that use sort of fine tuned handcrafted features.",
                    "label": 0
                },
                {
                    "sent": "Our method, again, showing sort of 46.8 accuracy on On this date.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "The next method I'd like to talk about briefly is the idea of space time, deep belief, network network.",
                    "label": 0
                },
                {
                    "sent": "So this was put forth by some researchers at the University of British Columbia at the last NIPS Deep Learning Workshop, and the two approaches that we just saw used some sort of discriminative learning.",
                    "label": 1
                },
                {
                    "sent": "So in the first example it was completely discriminative, and in the second example we had a generative feature extractor which was this convolutional gated restricted.",
                    "label": 0
                },
                {
                    "sent": "Also machine, but then we use a discriminative learning procedure after the features have been extracted.",
                    "label": 0
                },
                {
                    "sent": "So this space time deep belief network is a fully generative method, which means it opens it up to other applications beyond activity recognition.",
                    "label": 0
                },
                {
                    "sent": "For example in painting or denoising.",
                    "label": 0
                },
                {
                    "sent": "So another key aspect of this work is it's demonstrated learn invariants, which we will see, and it uses as a basic module.",
                    "label": 1
                },
                {
                    "sent": "The convolutional restricted Boltzmann machine, which was proposed by Hong likely and coauthors a couple years ago.",
                    "label": 0
                },
                {
                    "sent": "So on the right is the convolutional restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "It's much like a convolutional network, but it's able to infer up to the feature Maps as well as do a reconstruction back down to the image, making it a fully generative model.",
                    "label": 0
                },
                {
                    "sent": "It also incorporates a pooling operator as as is in standard convolutional network.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the basic idea behind the space time, this deep belief network is to alternate layers of spatial and temporal pooling.",
                    "label": 1
                },
                {
                    "sent": "Weight sharing is done across all convolutional restricted Boltzmann machines in a particular layer.",
                    "label": 0
                },
                {
                    "sent": "So we have some video frames as inputs and we apply a convolutional restricted Boltzmann machine to each frame.",
                    "label": 0
                },
                {
                    "sent": "So we see in this layer down here we might have multiple channels, for example corresponding to different color channels and then time proceeds to the right here.",
                    "label": 0
                },
                {
                    "sent": "So we're just applying this static model, the PBM, to each of these frames.",
                    "label": 0
                },
                {
                    "sent": "But it's the same CRBN that's applying.",
                    "label": 0
                },
                {
                    "sent": "Being applied to each frames.",
                    "label": 0
                },
                {
                    "sent": "The Serapeum extracts a varying number of features which you see in this little stack here and then their pooled spatially to gain some a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Translational in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance?",
                    "label": 0
                },
                {
                    "sent": "So now that these little feature Maps have been extracted for each frame, we now apply a different type of CRB M that pools in the temporal dimension so you can see in red.",
                    "label": 0
                },
                {
                    "sent": "We've highlighted each pixel each pixel is collected into a little sequence corresponding to the number of feature Maps that were extracted and a temporal.",
                    "label": 0
                },
                {
                    "sent": "See RBM is applied.",
                    "label": 0
                },
                {
                    "sent": "This reduces the temporal resolution and even more so when it is temporarily pools this stack that's been extracted for each pixel.",
                    "label": 0
                },
                {
                    "sent": "And then and then applied through the CR BM is then reshaped into an image structure and again we were then arriving at a spatially arranged feature map which could then be processed by say another spatial pooling layer, and so on.",
                    "label": 0
                },
                {
                    "sent": "And this it can be stacked basically in this way.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing that the others have done is measured the invariants in such a network.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of taking an image or video where there's a specified translation or rotation happening and measuring the firing rate of a particular particular unit under this type of transformation.",
                    "label": 1
                },
                {
                    "sent": "So we can see that some units that we saw on the right here could be overly selective, as in their red, not selective at all in the blue or invariant.",
                    "label": 0
                },
                {
                    "sent": "Over a particular degree of transformation, that's the type of thing we'd like to see.",
                    "label": 1
                },
                {
                    "sent": "So on the right were comparing three different layers of such a module, so there's S. One layer is the first layer, it's a spatial pooling layer.",
                    "label": 1
                },
                {
                    "sent": "There's no temporal pooling layer for the for the first layer in the network, so we go right into the layer S2, which is a spec.",
                    "label": 0
                },
                {
                    "sent": "Second spatial pooling layer, and then there's T1, which is the third layer of the network, and that's a temporal pooling layer.",
                    "label": 1
                },
                {
                    "sent": "And what we see here.",
                    "label": 0
                },
                {
                    "sent": "Is increased in variance as we move higher in the network so higher in this picture is better, showing more invariants.",
                    "label": 0
                },
                {
                    "sent": "So all of the three layers exhibit fairly good translation invariants, but we see the higher layers exhibiting better invariants to zooming 2D and 3D rotation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I mentioned, because this is a fully generative model, we can do operations like denoising as we see in the in the very top here, because we can go right to the top layer representation and then come all the way back down.",
                    "label": 0
                },
                {
                    "sent": "Another interesting operation that can be done in such a network is give the model observed cases, so only partial input and then acts get to reconstruct that input as well as the region outside that input.",
                    "label": 0
                },
                {
                    "sent": "So here is doing in paging reasonably well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 4th method I'm going to very very briefly mention is this idea of stacked convolutional independent subspace analysis, and the reason I'm going to mention it very briefly is because it's a paper here at CPR and instant oral, so you'll be able to hear about it on your own.",
                    "label": 0
                },
                {
                    "sent": "But this is another way of using unsupervised learning module in a stacked approach.",
                    "label": 0
                },
                {
                    "sent": "And it's also been scaled as the other methods have convolutionally.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in summary, I've told you about a few different ways of applying deep learning to the activity recognition problem.",
                    "label": 0
                },
                {
                    "sent": "We talked about 3D convolutional neural Nets, convolutional gated restricted Boltzmann machines, spacetime, deep belief networks, and I gave you just basically a pointer to this last work here at CPR this year.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, deep learning methods have shown quite amount of a good amount of progress.",
                    "label": 1
                },
                {
                    "sent": "Promise in this area of activity recognition.",
                    "label": 1
                },
                {
                    "sent": "But to this point in terms of their performance, I think it's still very neck and neck against traditional computer vision algorithms.",
                    "label": 0
                },
                {
                    "sent": "the Holy Grail, I guess in this field is going towards homogeneous networks built by simple, trainable modules, and I guess there's a desire here in this field of future improvements in activity recognition being made by.",
                    "label": 1
                },
                {
                    "sent": "Progress in efficient and robust unsupervised learning as opposed to relying a lot on a supervision signal.",
                    "label": 1
                },
                {
                    "sent": "Because the data of course is much more much more limited, an more costly to obtain.",
                    "label": 0
                },
                {
                    "sent": "The last thing I'd like to mention is, as we said several times in this talk, we've tried to learn invariant representations and I have some work actually tomorrow talking about learning invariant representations, but some very interesting work that's come out of the Hinton lab in the last year.",
                    "label": 0
                },
                {
                    "sent": "Questions the idea of learning invariant representations.",
                    "label": 0
                },
                {
                    "sent": "This work suggests that we should actually represent things like translations and rotations and lighting changes.",
                    "label": 0
                },
                {
                    "sent": "These things that we have been trying to build invariants too.",
                    "label": 0
                },
                {
                    "sent": "As latent features within our model, so they they proposed this method called the transforming autoencoder.",
                    "label": 0
                },
                {
                    "sent": "You can view this paper if you're interested and explicitly tries to reason about the things we have already been trying to gain invariants too.",
                    "label": 0
                },
                {
                    "sent": "So with that, I'd like to thank you very much for your attention and thank my various collaborators at NYU, faculty students funding, and then maybe just give a very quick pointer to a poster I have, which is not an activity recognition but tomorrow morning.",
                    "label": 0
                },
                {
                    "sent": "Basically you have computer vision researchers in a Dutch progressive Electro band can team up to solve computer vision so that interests you.",
                    "label": 0
                },
                {
                    "sent": "Please come to my poster, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thank you Carl.",
                    "label": 0
                },
                {
                    "sent": "It's quick questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it was the 3rd method.",
                    "label": 0
                },
                {
                    "sent": "You said you can reconstruct down to the original image using the generative model.",
                    "label": 0
                },
                {
                    "sent": "How do you go backwards from the pooling operation?",
                    "label": 0
                },
                {
                    "sent": "Great points.",
                    "label": 0
                },
                {
                    "sent": "So if you see in those slides I'll just jump back.",
                    "label": 0
                },
                {
                    "sent": "There it's a little bit blurry in certain regions, and that's an artifact of the pooling, so you can't perfectly reconstruct once you've done the pooling, you can basically approximate sort of maybe smooth out from where you from where you've pulled, but you're going to get blurry reconstruction that's just a limitation, and I think there's a lot of interest in deep learning researchers to build better pooling mechanisms.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "Right now, average or Max pooling is all we do, but I think various forms of adaptive pooling could really assist these methods.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}