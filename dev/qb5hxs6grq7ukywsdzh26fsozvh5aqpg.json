{
    "id": "qb5hxs6grq7ukywsdzh26fsozvh5aqpg",
    "title": "Preference-based policy iteration: Leveraging preference learning for reinforcement learning",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Johannes F\u00fcrnkranz, Department of Computer Science, Darmstadt University of Technology"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_furnkranz_iteration/",
    "segmentation": [
        [
            "OK, I tried to start while this boots up my computer.",
            "Recent parents terribly slow.",
            "And try to talk about preference based policy iteration and if you are somehow you will somehow notice that you've heard that term before in this session.",
            "And there are some really.",
            "And there's some similarities in what we've done and with.",
            "What has been presented in the first group and then the presentation of the first group?",
            "Oh, now it shows all my spending.",
            "OK."
        ],
        [
            "Sorry about that.",
            "Oh God, I don't want to know anything is happening anyways, so.",
            "I notice I spent half of my talk already.",
            "We start with classical reinforcement learning.",
            "I have given very high level view in that because I'm also not really an expert in reinforcement learning, but you usually have his action phrases, observations of some sequences of actions and you get some numerical reward for the action phrases, and then you learn something like a value function or a Q function that is a function that allows you to predict how good it is to take an action in a certain state, and then you use that function to actually act, usually by taking the action that maximizes that function in your current statement.",
            "And also make different choices and that is standard policy.",
            "The method for making actions out of this.",
            "In certain states, so they're classically reinforcement learning algorithms, Q learning, TD lender, and all these things.",
            "They all work in that."
        ],
        [
            "Gamers, then there are methods that do with this quote policy learning where you actually just skip that step.",
            "You do not directly learn.",
            "Where are you function or a Q function?",
            "You just try to learn the policy, so whenever you're in a state you get an action that you want to perform without really knowing how good is this?",
            "Actually, what is my expected reward for this action?",
            "You don't really need that too, it correcting.",
            "You just need to know.",
            "I mean the state.",
            "What action should I take?",
            "There are several methods in the reinforcement learning literature that follow that Rd.",
            "What we want to do.",
            "Is we ask the question, what about this numerical rewards?",
            "In many applications, this is sort of artificial.",
            "You assume some reward because you just need a numerical reward signal.",
            "Can we try to reformulate the problem in a way that you do not need anymore numerical?"
        ],
        [
            "Reward signal and.",
            "Framework that we're aiming at is using preferences for that, so you have something like state preferences or action preferences.",
            "You get information.",
            "In certain states it is better to use ActionAid an action B or you get information like this state is better than that state.",
            "So pairwise comparisons between states preferences between states.",
            "I show you an example."
        ],
        [
            "This comes up very naturally.",
            "This is chess annotated chess games, so this is a chess game from I don't know.",
            "It says here Aug 14 has complete between two very good chess players and it has been annotated by a third chess player.",
            "The annotations are something like.",
            "Here is some text.",
            "This is a bad move here.",
            "But they are also in a very formal way that you can interpret.",
            "You see here.",
            "Or maybe you don't see it in the back, but there's a question mark behind that move.",
            "That means this is a bad move.",
            "This is a mistake.",
            "And from that you can of course choose."
        ],
        [
            "Preferences, so for example, in the 13 smooth for black you have a move that was played.",
            "That had the.",
            "For black, sorry slightly.",
            "Yeah, you have a regular move.",
            "You have a move with a question mark and!",
            "You have another alternative that has a question mark and you have another alternative that is 2 question marks just simply means this move is better than that move.",
            "This move is better than that move and this move is better than that move according to the opinion of this annotator, it doesn't say how good these moves are and it's really impossible to say this move is.",
            "Equalizes this move.",
            "Costs.",
            "You want pawn.",
            "Discuss 22 calls or something like that or this cost you half opponent.",
            "Discuss one point.",
            "You can say that you can't quantify that, but you know this is better than that.",
            "This is better than that.",
            "This is better than that.",
            "You can get that from this annotator and there are millions of games that provide these sorts."
        ],
        [
            "Imitation is freely available.",
            "There are also state annotations.",
            "They use different symbols for that you have here for example, plus minus.",
            "This means this position after this move is winning for white and then you have something like a plus over an = This means the position after this move is slightly better for light, so these are annotations that apply throughout the game there not refering to the same action sequence.",
            "Or not to the same state, but.",
            "Or not to the same trajectory, but two arbitrary states that you can find in this sequence.",
            "But you know this position is better than this position in this position is better than this position, and they both are better than this position.",
            "For example, position is a state, so you have state preferences here and we wanted to use these state and action sequences.",
            "We have not yet solved this for chess.",
            "I want to do that, but.",
            "This is."
        ],
        [
            "In the future, I hope.",
            "We try to start with the simplest scenario and there's some work that that is called approximate policy iteration with rollouts by Lam.",
            "Talk is in part at assume L 2003.",
            "The key idea is quite simple.",
            "You do what is called a rollout, and the rollout is in a state.",
            "I have to go one step back.",
            "You assume something like a generative model.",
            "That means you can sample the action space you can.",
            "You can make actions and you get some reward so you can just experiment with the environment and then you can do something like what is called the rollout rollout is just in order to evaluate an action you take this action and then from from the state you end up.",
            "In you try multiple alternative action sequences, observe how much we want to give and if you have tried 100 of them, you can average the result and you get an estimate for the average reward in that state.",
            "That's the key idea of rollouts, and these are used for policy."
        ],
        [
            "Iteration process that means you start with some policy in the beginning some.",
            "Some procedure that assigns actions to States and then you visit each state and try out all actions in that state and you try to estimate by a rollout switch is the best action in this state and then you have sort of a supervised classification problem.",
            "You have states that are annotated with features and you know which is the best action in this state.",
            "And then you train a classifier, and expressive fire assigns actions to state.",
            "You can use this classifier as your policy, meaning when you are in the state you look up in the decision tree over there where you have it gives you an effect in action, and then you take this action and you can iterate that.",
            "So after the first policy you do one iteration of learning, you get another policy, then you repeat and you do that for until you converge or you do it up to.",
            "Maybe 10 times."
        ],
        [
            "So the idea of what we have done is that we replaced this.",
            "Classifier with the Rancor with label Rancor label Rancor does not assign as in classification.",
            "One label to an object, so this is supposed to mean you have some object and you want to know the color the classifier would say this object is red label ranker would say for this object.",
            "I think Red is the better color than yellow and yellow is a better color than green and these are trained on comparisons between these labels.",
            "So you could have a training example that says for this particular object green is a better color than red but you don't know anything about yellow and you don't know whether green is really the best color photos.",
            "Object so this is this basic scenario of label ranking learning from labeled prep preferences and you predict a ranking.",
            "In our case, the labels are the actions and you want to rank the actions depending On this date in a given state you want to have a ranking of all actions.",
            "And the training information is given the state and information like this action is better than that action in this state."
        ],
        [
            "So we when we use that we end up with a version of this approximate policy iteration that we call preference based policy iteration.",
            "Almost the same as in the first talk, and it is also very similar idea.",
            "The basic loop is the same you visit all states, but in each state you do not try to find out the best action, because in many states it's even there is no clear best action.",
            "For example, if you know in games.",
            "And in many chess positions you can take many different moves, and they're all good.",
            "But for some moves you know that they are worse than others, and for others you may know that they're better than others.",
            "So the idea is that we can make use of this.",
            "Information by replacing the classifier that we had here in the preference based.",
            "In the approximate policy iteration with the label Rancor.",
            "That can make use of more."
        ],
        [
            "Training.",
            "Information why is that a good idea?",
            "First, some often there is no numerical value.",
            "As I said in this chest positions, sometimes you get a qualitative annotation, but you don't really know how good is that.",
            "Another case is if you have multiple objectives and I'll show you an example.",
            "At the end of the talk, it's.",
            "Difficult to define what is better if you might have some things that end up better according to criteria, May and other actions lead you to a better result in Criterion B and it's hard to come back with a reward signal from that.",
            "And it might also be infeasible to determine the best action, impossible or infeasible, too expensive or whatever.",
            "For example, in this preference, based in this approximate policy iteration.",
            "If you have a situation like this, you have a statement.",
            "You have three actions.",
            "This are the better actions.",
            "These are diverse actions and you have had two actions that are very close together.",
            "And if you do a significance test, the intervals would overlap, so you cannot really clearly determine which action is the best, what the language pack is algorithm would do.",
            "In this case it would say in this state I can determine the best action.",
            "I ignore this if you use a preference based approach, we can still say we don't know which one is better, a one or a do we don't know it, but we whoops.",
            "But we do know a one is better than a three and a two is better than a 3, so we have some information, although we don't know the best action, but we can use that if we use a label ranker instead of the."
        ],
        [
            "Classifier.",
            "So we did two case studies.",
            "The first one was focusing on this third part.",
            "The comparison of the actions and the second case study was focusing on the other two important things that multi objective criteria and the fact that it's not always feasible to assign a numerical value.",
            "So in this first place study, we.",
            "Trite.",
            "In reinforcement learning you don't have UCI, but you have these many little toy problems that you can play around this, and two of them are inverted.",
            "Pendulum and Mountain cogg inverted pendulum.",
            "They've all this you have recurrent.",
            "You want to you have a pole and you can shift the car and you want to balance.",
            "The bullet shouldn't fall down and in the mountain car example you have a card you're in the middle of the Valley and you can drive up, but you can't really make it to the top so you will go down again and then you have to apply force to go up to the other side to take some.",
            "Speed and then you can go up again, and if you repeat that one point you will get out.",
            "We use these two problems with 359 and 17 actions.",
            "Three actions is apply force to the left apply force to the right and apply for no force and five actions is a little bit to the left or strong to the left and a little bit to the right frontal right and the same for 9 and 17.",
            "The evaluation that we took was basically following the original paper with approximate policy theory."
        ],
        [
            "Asian.",
            "And we compared three different systems.",
            "The Red one is too original.",
            "Approximately policy ploration the blue one is the one that I just described, and the black one is a version of this sort of in the middle where we replace the classification.",
            "That is used in.",
            "The classifier that is used in this approach with the pairwise classifier, so that is a version that transforms the classification problem into a preference learning problem, but it does not make use of these additional preferences.",
            "So for three actions, you don't see much difference.",
            "The red and the blue ones are almost identical.",
            "Oh, I should explain what these curves actually tell you these are.",
            "OK, these are cumulative curves, meaning we did a large number of experiments with also different parameter settings and then we measured the success rate, which means in how many percentage of the?",
            "Configurations did we succeed?",
            "And this is the number of actions that you had to take cumulatively to reach that level of success.",
            "The cursor have different length because success is defined.",
            "Either you have a successful policy and you can reach that with a different number of actions, so there might be if the blue one is shorter, this is already a little bit better than the red one, 'cause it used fewer actions.",
            "In total, over the entire experiment.",
            "So if you repeat that with five, you see that the.",
            "Ruben races with nine through 17.",
            "There's a big advantage already, so the more actions you have, the more you can gain from these preferences that you can obtain that you would not get when you only use a classifier or formulated in a different way.",
            "If you have many.",
            "Actions it's hard to determine the best action, and the more you have, the harder it becomes, and this is what puts this red curve down."
        ],
        [
            "In cases where you have more actions, the same for the mountain carp."
        ],
        [
            "Both."
        ],
        [
            "I will skip this.",
            "I'll briefly tell you about the second case study.",
            "This is a domain for clinical trials of cancer treatment.",
            "The goal here is the actions are you apply a certain dose of.",
            "For many comments too.",
            "A patient and of course, there's this tradeoff.",
            "The more you give, the better it will kill the cancer, but diverse the patient will fit will feel, and so you have two objectives here.",
            "You want the patient to feel well, but you also want to kill cancer.",
            "And the way we formulated this is that we defined a preference over action traces and we say treatment plan is better treatment.",
            "Plan A is better than treatment Plan B.",
            "If the patient feels better in a in all of the states.",
            "And if the two more in the end is smaller, or at least the same size as in the other treatment only if there is a strict dominance.",
            "So if the curves cross in one week, if the patient feels better in one treatment plan and the other in the other treatment plan, we do not.",
            "Make.",
            "Any?"
        ],
        [
            "Reference and this also worked.",
            "You would you see here are four points with fixed treatment.",
            "This is the size of the tumor in the end of the treatment.",
            "This is the toxic toxicity of the drug that is given.",
            "So the higher here diversity patient feels.",
            "So if you are here, the patient doesn't have cancer and feels well.",
            "This is good and learn policy is here.",
            "It's outside of the convex Hull of the.",
            "Fixed policies random policy."
        ],
        [
            "See.",
            "Would be here?",
            "OK, the conclusion is just repeat what I just said."
        ],
        [
            "It is already getting."
        ],
        [
            "Angry, I would still use this for a shameless plug.",
            "It maybe there's one time for a quick little time for question.",
            "Question.",
            "I just have a reminder person an compared additional regional rollout sampling.",
            "Or they proved wrong.",
            "Something 100.",
            "The original.",
            "We we we looked at some of the improvements and we did some experiments, but I'm not exactly sure what sign do you know?",
            "He did the experiments here, but he is also quite so.",
            "We compare the results.",
            "Here are definitely against the original version."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I tried to start while this boots up my computer.",
                    "label": 0
                },
                {
                    "sent": "Recent parents terribly slow.",
                    "label": 0
                },
                {
                    "sent": "And try to talk about preference based policy iteration and if you are somehow you will somehow notice that you've heard that term before in this session.",
                    "label": 0
                },
                {
                    "sent": "And there are some really.",
                    "label": 0
                },
                {
                    "sent": "And there's some similarities in what we've done and with.",
                    "label": 0
                },
                {
                    "sent": "What has been presented in the first group and then the presentation of the first group?",
                    "label": 0
                },
                {
                    "sent": "Oh, now it shows all my spending.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Oh God, I don't want to know anything is happening anyways, so.",
                    "label": 0
                },
                {
                    "sent": "I notice I spent half of my talk already.",
                    "label": 0
                },
                {
                    "sent": "We start with classical reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "I have given very high level view in that because I'm also not really an expert in reinforcement learning, but you usually have his action phrases, observations of some sequences of actions and you get some numerical reward for the action phrases, and then you learn something like a value function or a Q function that is a function that allows you to predict how good it is to take an action in a certain state, and then you use that function to actually act, usually by taking the action that maximizes that function in your current statement.",
                    "label": 0
                },
                {
                    "sent": "And also make different choices and that is standard policy.",
                    "label": 1
                },
                {
                    "sent": "The method for making actions out of this.",
                    "label": 0
                },
                {
                    "sent": "In certain states, so they're classically reinforcement learning algorithms, Q learning, TD lender, and all these things.",
                    "label": 0
                },
                {
                    "sent": "They all work in that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gamers, then there are methods that do with this quote policy learning where you actually just skip that step.",
                    "label": 1
                },
                {
                    "sent": "You do not directly learn.",
                    "label": 0
                },
                {
                    "sent": "Where are you function or a Q function?",
                    "label": 1
                },
                {
                    "sent": "You just try to learn the policy, so whenever you're in a state you get an action that you want to perform without really knowing how good is this?",
                    "label": 0
                },
                {
                    "sent": "Actually, what is my expected reward for this action?",
                    "label": 1
                },
                {
                    "sent": "You don't really need that too, it correcting.",
                    "label": 0
                },
                {
                    "sent": "You just need to know.",
                    "label": 0
                },
                {
                    "sent": "I mean the state.",
                    "label": 0
                },
                {
                    "sent": "What action should I take?",
                    "label": 1
                },
                {
                    "sent": "There are several methods in the reinforcement learning literature that follow that Rd.",
                    "label": 0
                },
                {
                    "sent": "What we want to do.",
                    "label": 0
                },
                {
                    "sent": "Is we ask the question, what about this numerical rewards?",
                    "label": 0
                },
                {
                    "sent": "In many applications, this is sort of artificial.",
                    "label": 1
                },
                {
                    "sent": "You assume some reward because you just need a numerical reward signal.",
                    "label": 0
                },
                {
                    "sent": "Can we try to reformulate the problem in a way that you do not need anymore numerical?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reward signal and.",
                    "label": 0
                },
                {
                    "sent": "Framework that we're aiming at is using preferences for that, so you have something like state preferences or action preferences.",
                    "label": 0
                },
                {
                    "sent": "You get information.",
                    "label": 0
                },
                {
                    "sent": "In certain states it is better to use ActionAid an action B or you get information like this state is better than that state.",
                    "label": 0
                },
                {
                    "sent": "So pairwise comparisons between states preferences between states.",
                    "label": 0
                },
                {
                    "sent": "I show you an example.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This comes up very naturally.",
                    "label": 0
                },
                {
                    "sent": "This is chess annotated chess games, so this is a chess game from I don't know.",
                    "label": 1
                },
                {
                    "sent": "It says here Aug 14 has complete between two very good chess players and it has been annotated by a third chess player.",
                    "label": 0
                },
                {
                    "sent": "The annotations are something like.",
                    "label": 0
                },
                {
                    "sent": "Here is some text.",
                    "label": 0
                },
                {
                    "sent": "This is a bad move here.",
                    "label": 0
                },
                {
                    "sent": "But they are also in a very formal way that you can interpret.",
                    "label": 0
                },
                {
                    "sent": "You see here.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you don't see it in the back, but there's a question mark behind that move.",
                    "label": 0
                },
                {
                    "sent": "That means this is a bad move.",
                    "label": 0
                },
                {
                    "sent": "This is a mistake.",
                    "label": 0
                },
                {
                    "sent": "And from that you can of course choose.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Preferences, so for example, in the 13 smooth for black you have a move that was played.",
                    "label": 1
                },
                {
                    "sent": "That had the.",
                    "label": 0
                },
                {
                    "sent": "For black, sorry slightly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have a regular move.",
                    "label": 0
                },
                {
                    "sent": "You have a move with a question mark and!",
                    "label": 0
                },
                {
                    "sent": "You have another alternative that has a question mark and you have another alternative that is 2 question marks just simply means this move is better than that move.",
                    "label": 0
                },
                {
                    "sent": "This move is better than that move and this move is better than that move according to the opinion of this annotator, it doesn't say how good these moves are and it's really impossible to say this move is.",
                    "label": 0
                },
                {
                    "sent": "Equalizes this move.",
                    "label": 0
                },
                {
                    "sent": "Costs.",
                    "label": 0
                },
                {
                    "sent": "You want pawn.",
                    "label": 0
                },
                {
                    "sent": "Discuss 22 calls or something like that or this cost you half opponent.",
                    "label": 0
                },
                {
                    "sent": "Discuss one point.",
                    "label": 0
                },
                {
                    "sent": "You can say that you can't quantify that, but you know this is better than that.",
                    "label": 1
                },
                {
                    "sent": "This is better than that.",
                    "label": 0
                },
                {
                    "sent": "This is better than that.",
                    "label": 0
                },
                {
                    "sent": "You can get that from this annotator and there are millions of games that provide these sorts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imitation is freely available.",
                    "label": 0
                },
                {
                    "sent": "There are also state annotations.",
                    "label": 0
                },
                {
                    "sent": "They use different symbols for that you have here for example, plus minus.",
                    "label": 0
                },
                {
                    "sent": "This means this position after this move is winning for white and then you have something like a plus over an = This means the position after this move is slightly better for light, so these are annotations that apply throughout the game there not refering to the same action sequence.",
                    "label": 0
                },
                {
                    "sent": "Or not to the same state, but.",
                    "label": 0
                },
                {
                    "sent": "Or not to the same trajectory, but two arbitrary states that you can find in this sequence.",
                    "label": 0
                },
                {
                    "sent": "But you know this position is better than this position in this position is better than this position, and they both are better than this position.",
                    "label": 0
                },
                {
                    "sent": "For example, position is a state, so you have state preferences here and we wanted to use these state and action sequences.",
                    "label": 0
                },
                {
                    "sent": "We have not yet solved this for chess.",
                    "label": 0
                },
                {
                    "sent": "I want to do that, but.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the future, I hope.",
                    "label": 0
                },
                {
                    "sent": "We try to start with the simplest scenario and there's some work that that is called approximate policy iteration with rollouts by Lam.",
                    "label": 1
                },
                {
                    "sent": "Talk is in part at assume L 2003.",
                    "label": 0
                },
                {
                    "sent": "The key idea is quite simple.",
                    "label": 0
                },
                {
                    "sent": "You do what is called a rollout, and the rollout is in a state.",
                    "label": 0
                },
                {
                    "sent": "I have to go one step back.",
                    "label": 1
                },
                {
                    "sent": "You assume something like a generative model.",
                    "label": 0
                },
                {
                    "sent": "That means you can sample the action space you can.",
                    "label": 0
                },
                {
                    "sent": "You can make actions and you get some reward so you can just experiment with the environment and then you can do something like what is called the rollout rollout is just in order to evaluate an action you take this action and then from from the state you end up.",
                    "label": 0
                },
                {
                    "sent": "In you try multiple alternative action sequences, observe how much we want to give and if you have tried 100 of them, you can average the result and you get an estimate for the average reward in that state.",
                    "label": 0
                },
                {
                    "sent": "That's the key idea of rollouts, and these are used for policy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Iteration process that means you start with some policy in the beginning some.",
                    "label": 1
                },
                {
                    "sent": "Some procedure that assigns actions to States and then you visit each state and try out all actions in that state and you try to estimate by a rollout switch is the best action in this state and then you have sort of a supervised classification problem.",
                    "label": 1
                },
                {
                    "sent": "You have states that are annotated with features and you know which is the best action in this state.",
                    "label": 0
                },
                {
                    "sent": "And then you train a classifier, and expressive fire assigns actions to state.",
                    "label": 0
                },
                {
                    "sent": "You can use this classifier as your policy, meaning when you are in the state you look up in the decision tree over there where you have it gives you an effect in action, and then you take this action and you can iterate that.",
                    "label": 0
                },
                {
                    "sent": "So after the first policy you do one iteration of learning, you get another policy, then you repeat and you do that for until you converge or you do it up to.",
                    "label": 0
                },
                {
                    "sent": "Maybe 10 times.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea of what we have done is that we replaced this.",
                    "label": 0
                },
                {
                    "sent": "Classifier with the Rancor with label Rancor label Rancor does not assign as in classification.",
                    "label": 1
                },
                {
                    "sent": "One label to an object, so this is supposed to mean you have some object and you want to know the color the classifier would say this object is red label ranker would say for this object.",
                    "label": 0
                },
                {
                    "sent": "I think Red is the better color than yellow and yellow is a better color than green and these are trained on comparisons between these labels.",
                    "label": 0
                },
                {
                    "sent": "So you could have a training example that says for this particular object green is a better color than red but you don't know anything about yellow and you don't know whether green is really the best color photos.",
                    "label": 1
                },
                {
                    "sent": "Object so this is this basic scenario of label ranking learning from labeled prep preferences and you predict a ranking.",
                    "label": 1
                },
                {
                    "sent": "In our case, the labels are the actions and you want to rank the actions depending On this date in a given state you want to have a ranking of all actions.",
                    "label": 1
                },
                {
                    "sent": "And the training information is given the state and information like this action is better than that action in this state.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we when we use that we end up with a version of this approximate policy iteration that we call preference based policy iteration.",
                    "label": 1
                },
                {
                    "sent": "Almost the same as in the first talk, and it is also very similar idea.",
                    "label": 0
                },
                {
                    "sent": "The basic loop is the same you visit all states, but in each state you do not try to find out the best action, because in many states it's even there is no clear best action.",
                    "label": 0
                },
                {
                    "sent": "For example, if you know in games.",
                    "label": 1
                },
                {
                    "sent": "And in many chess positions you can take many different moves, and they're all good.",
                    "label": 0
                },
                {
                    "sent": "But for some moves you know that they are worse than others, and for others you may know that they're better than others.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we can make use of this.",
                    "label": 0
                },
                {
                    "sent": "Information by replacing the classifier that we had here in the preference based.",
                    "label": 0
                },
                {
                    "sent": "In the approximate policy iteration with the label Rancor.",
                    "label": 0
                },
                {
                    "sent": "That can make use of more.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Information why is that a good idea?",
                    "label": 0
                },
                {
                    "sent": "First, some often there is no numerical value.",
                    "label": 1
                },
                {
                    "sent": "As I said in this chest positions, sometimes you get a qualitative annotation, but you don't really know how good is that.",
                    "label": 0
                },
                {
                    "sent": "Another case is if you have multiple objectives and I'll show you an example.",
                    "label": 0
                },
                {
                    "sent": "At the end of the talk, it's.",
                    "label": 0
                },
                {
                    "sent": "Difficult to define what is better if you might have some things that end up better according to criteria, May and other actions lead you to a better result in Criterion B and it's hard to come back with a reward signal from that.",
                    "label": 1
                },
                {
                    "sent": "And it might also be infeasible to determine the best action, impossible or infeasible, too expensive or whatever.",
                    "label": 1
                },
                {
                    "sent": "For example, in this preference, based in this approximate policy iteration.",
                    "label": 0
                },
                {
                    "sent": "If you have a situation like this, you have a statement.",
                    "label": 0
                },
                {
                    "sent": "You have three actions.",
                    "label": 0
                },
                {
                    "sent": "This are the better actions.",
                    "label": 0
                },
                {
                    "sent": "These are diverse actions and you have had two actions that are very close together.",
                    "label": 0
                },
                {
                    "sent": "And if you do a significance test, the intervals would overlap, so you cannot really clearly determine which action is the best, what the language pack is algorithm would do.",
                    "label": 0
                },
                {
                    "sent": "In this case it would say in this state I can determine the best action.",
                    "label": 0
                },
                {
                    "sent": "I ignore this if you use a preference based approach, we can still say we don't know which one is better, a one or a do we don't know it, but we whoops.",
                    "label": 0
                },
                {
                    "sent": "But we do know a one is better than a three and a two is better than a 3, so we have some information, although we don't know the best action, but we can use that if we use a label ranker instead of the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifier.",
                    "label": 0
                },
                {
                    "sent": "So we did two case studies.",
                    "label": 0
                },
                {
                    "sent": "The first one was focusing on this third part.",
                    "label": 0
                },
                {
                    "sent": "The comparison of the actions and the second case study was focusing on the other two important things that multi objective criteria and the fact that it's not always feasible to assign a numerical value.",
                    "label": 0
                },
                {
                    "sent": "So in this first place study, we.",
                    "label": 0
                },
                {
                    "sent": "Trite.",
                    "label": 0
                },
                {
                    "sent": "In reinforcement learning you don't have UCI, but you have these many little toy problems that you can play around this, and two of them are inverted.",
                    "label": 0
                },
                {
                    "sent": "Pendulum and Mountain cogg inverted pendulum.",
                    "label": 0
                },
                {
                    "sent": "They've all this you have recurrent.",
                    "label": 0
                },
                {
                    "sent": "You want to you have a pole and you can shift the car and you want to balance.",
                    "label": 0
                },
                {
                    "sent": "The bullet shouldn't fall down and in the mountain car example you have a card you're in the middle of the Valley and you can drive up, but you can't really make it to the top so you will go down again and then you have to apply force to go up to the other side to take some.",
                    "label": 0
                },
                {
                    "sent": "Speed and then you can go up again, and if you repeat that one point you will get out.",
                    "label": 0
                },
                {
                    "sent": "We use these two problems with 359 and 17 actions.",
                    "label": 0
                },
                {
                    "sent": "Three actions is apply force to the left apply force to the right and apply for no force and five actions is a little bit to the left or strong to the left and a little bit to the right frontal right and the same for 9 and 17.",
                    "label": 0
                },
                {
                    "sent": "The evaluation that we took was basically following the original paper with approximate policy theory.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian.",
                    "label": 0
                },
                {
                    "sent": "And we compared three different systems.",
                    "label": 0
                },
                {
                    "sent": "The Red one is too original.",
                    "label": 0
                },
                {
                    "sent": "Approximately policy ploration the blue one is the one that I just described, and the black one is a version of this sort of in the middle where we replace the classification.",
                    "label": 0
                },
                {
                    "sent": "That is used in.",
                    "label": 0
                },
                {
                    "sent": "The classifier that is used in this approach with the pairwise classifier, so that is a version that transforms the classification problem into a preference learning problem, but it does not make use of these additional preferences.",
                    "label": 0
                },
                {
                    "sent": "So for three actions, you don't see much difference.",
                    "label": 0
                },
                {
                    "sent": "The red and the blue ones are almost identical.",
                    "label": 0
                },
                {
                    "sent": "Oh, I should explain what these curves actually tell you these are.",
                    "label": 0
                },
                {
                    "sent": "OK, these are cumulative curves, meaning we did a large number of experiments with also different parameter settings and then we measured the success rate, which means in how many percentage of the?",
                    "label": 0
                },
                {
                    "sent": "Configurations did we succeed?",
                    "label": 0
                },
                {
                    "sent": "And this is the number of actions that you had to take cumulatively to reach that level of success.",
                    "label": 0
                },
                {
                    "sent": "The cursor have different length because success is defined.",
                    "label": 0
                },
                {
                    "sent": "Either you have a successful policy and you can reach that with a different number of actions, so there might be if the blue one is shorter, this is already a little bit better than the red one, 'cause it used fewer actions.",
                    "label": 0
                },
                {
                    "sent": "In total, over the entire experiment.",
                    "label": 0
                },
                {
                    "sent": "So if you repeat that with five, you see that the.",
                    "label": 0
                },
                {
                    "sent": "Ruben races with nine through 17.",
                    "label": 0
                },
                {
                    "sent": "There's a big advantage already, so the more actions you have, the more you can gain from these preferences that you can obtain that you would not get when you only use a classifier or formulated in a different way.",
                    "label": 0
                },
                {
                    "sent": "If you have many.",
                    "label": 0
                },
                {
                    "sent": "Actions it's hard to determine the best action, and the more you have, the harder it becomes, and this is what puts this red curve down.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In cases where you have more actions, the same for the mountain carp.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will skip this.",
                    "label": 0
                },
                {
                    "sent": "I'll briefly tell you about the second case study.",
                    "label": 0
                },
                {
                    "sent": "This is a domain for clinical trials of cancer treatment.",
                    "label": 1
                },
                {
                    "sent": "The goal here is the actions are you apply a certain dose of.",
                    "label": 0
                },
                {
                    "sent": "For many comments too.",
                    "label": 1
                },
                {
                    "sent": "A patient and of course, there's this tradeoff.",
                    "label": 0
                },
                {
                    "sent": "The more you give, the better it will kill the cancer, but diverse the patient will fit will feel, and so you have two objectives here.",
                    "label": 0
                },
                {
                    "sent": "You want the patient to feel well, but you also want to kill cancer.",
                    "label": 0
                },
                {
                    "sent": "And the way we formulated this is that we defined a preference over action traces and we say treatment plan is better treatment.",
                    "label": 0
                },
                {
                    "sent": "Plan A is better than treatment Plan B.",
                    "label": 1
                },
                {
                    "sent": "If the patient feels better in a in all of the states.",
                    "label": 0
                },
                {
                    "sent": "And if the two more in the end is smaller, or at least the same size as in the other treatment only if there is a strict dominance.",
                    "label": 0
                },
                {
                    "sent": "So if the curves cross in one week, if the patient feels better in one treatment plan and the other in the other treatment plan, we do not.",
                    "label": 0
                },
                {
                    "sent": "Make.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reference and this also worked.",
                    "label": 0
                },
                {
                    "sent": "You would you see here are four points with fixed treatment.",
                    "label": 0
                },
                {
                    "sent": "This is the size of the tumor in the end of the treatment.",
                    "label": 0
                },
                {
                    "sent": "This is the toxic toxicity of the drug that is given.",
                    "label": 0
                },
                {
                    "sent": "So the higher here diversity patient feels.",
                    "label": 0
                },
                {
                    "sent": "So if you are here, the patient doesn't have cancer and feels well.",
                    "label": 0
                },
                {
                    "sent": "This is good and learn policy is here.",
                    "label": 0
                },
                {
                    "sent": "It's outside of the convex Hull of the.",
                    "label": 1
                },
                {
                    "sent": "Fixed policies random policy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Would be here?",
                    "label": 0
                },
                {
                    "sent": "OK, the conclusion is just repeat what I just said.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is already getting.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Angry, I would still use this for a shameless plug.",
                    "label": 0
                },
                {
                    "sent": "It maybe there's one time for a quick little time for question.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "I just have a reminder person an compared additional regional rollout sampling.",
                    "label": 0
                },
                {
                    "sent": "Or they proved wrong.",
                    "label": 0
                },
                {
                    "sent": "Something 100.",
                    "label": 0
                },
                {
                    "sent": "The original.",
                    "label": 0
                },
                {
                    "sent": "We we we looked at some of the improvements and we did some experiments, but I'm not exactly sure what sign do you know?",
                    "label": 0
                },
                {
                    "sent": "He did the experiments here, but he is also quite so.",
                    "label": 0
                },
                {
                    "sent": "We compare the results.",
                    "label": 0
                },
                {
                    "sent": "Here are definitely against the original version.",
                    "label": 0
                }
            ]
        }
    }
}